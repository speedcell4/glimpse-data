{
  "https://www.isca-speech.org/archive/interspeech_2020/pierrehumbert20_interspeech.html": {
    "title": "The cognitive status of simple and complex models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "68b1551cc73dc27a1f836e064a7f7b08916e163a",
    "semantic_title": "the cognitive status of simple and complex models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20_interspeech.html": {
    "title": "On the Comparison of Popular End-to-End Models for Large Scale Speech Recognition",
    "volume": "main",
    "abstract": "Recently, there has been a strong push to transition from hybrid models to end-to-end (E2E) models for automatic speech recognition. Currently, there are three promising E2E methods: recurrent neural network transducer (RNN-T), RNN attention-based encoder-decoder (AED), and Transformer-AED. In this study, we conduct an empirical comparison of RNN-T, RNN-AED, and Transformer-AED models, in both non-streaming and streaming modes. We use 65 thousand hours of Microsoft anonymized training data to train these models. As E2E models are more data hungry, it is better to compare their effectiveness with large amount of training data. To the best of our knowledge, no such comprehensive study has been conducted yet. We show that although AED models are stronger than RNN-T in the non-streaming mode, RNN-T is very competitive in streaming mode if its encoder can be properly initialized. Among all three E2E models, transformer-AED achieved the best accuracy in both streaming and non-streaming mode. We show that both streaming RNN-T and transformer-AED models can obtain better accuracy than a highly-optimized hybrid model",
    "checked": true,
    "id": "be934c378c897e5bc3b3767376a59a4679093286",
    "semantic_title": "on the comparison of popular end-to-end models for large scale speech recognition",
    "citation_count": 109
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20_interspeech.html": {
    "title": "SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end speech recognition has become popular in recent years, since it can integrate the acoustic, pronunciation and language models into a single neural network. Among end-to-end approaches, attention-based methods have emerged as being superior. For example, Transformer, which adopts an encoder-decoder architecture. The key improvement introduced by Transformer is the utilization of self-attention instead of recurrent mechanisms, enabling both encoder and decoder to capture long-range dependencies with lower computational complexity. In this work, we propose boosting the self-attention ability with a DFSMN memory block, forming the proposed memory equipped self-attention (SAN-M) mechanism. Theoretical and empirical comparisons have been made to demonstrate the relevancy and complementarity between self-attention and the DFSMN memory block. Furthermore, the proposed SAN-M provides an efficient mechanism to integrate these two modules. We have evaluated our approach on the public AISHELL-1 benchmark and an industrial-level 20,000-hour Mandarin speech recognition task. On both tasks, SAN-M systems achieved much better performance than the self-attention based Transformer baseline system. Specially, it can achieve a CER of 6.46% on the AISHELL-1 task even without using any external LM, comfortably outperforming other state-of-the-art systems",
    "checked": true,
    "id": "96da36f81c065a154f5423c0dfbb1b1de69c2574",
    "semantic_title": "san-m: memory equipped self-attention for end-to-end speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jain20_interspeech.html": {
    "title": "Contextual RNN-T for Open Domain ASR",
    "volume": "main",
    "abstract": "End-to-end (E2E) systems for automatic speech recognition (ASR), such as RNN Transducer (RNN-T) and Listen-Attend-Spell (LAS) blend the individual components of a traditional hybrid ASR system — acoustic model, language model, pronunciation model — into a single neural network. While this has some nice advantages, it limits the system to be trained using only paired audio and text. Because of this, E2E models tend to have difficulties with correctly recognizing rare words that are not frequently seen during training, such as entity names. In this paper, we propose modifications to the RNN-T model that allow the model to utilize additional metadata text with the objective of improving performance on these named entity words. We evaluate our approach on an in-house dataset sampled from de-identified public social media videos, which represent an open domain ASR task. By using an attention model to leverage the contextual metadata that accompanies a video, we observe a relative improvement of about 16% in Word Error Rate on Named Entities (WER-NE) for videos with related metadata",
    "checked": true,
    "id": "c728dcae48bbca45b8dfed60f245c005e5de4b48",
    "semantic_title": "contextual rnn-t for open domain asr",
    "citation_count": 56
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pan20_interspeech.html": {
    "title": "ASAPP-ASR: Multistream CNN and Self-Attentive SRU for SOTA Speech Recognition",
    "volume": "main",
    "abstract": "In this paper we present state-of-the-art (SOTA) performance on the LibriSpeech corpus with two novel neural network architectures, a multistream CNN for acoustic modeling and a self-attentive simple recurrent unit (SRU) for language modeling. In the hybrid ASR framework, the multistream CNN acoustic model processes an input of speech frames in multiple parallel pipelines where each stream has a unique dilation rate for diversity. Trained with the SpecAugment data augmentation method, it achieves relative word error rate (WER) improvements of 4% on test-clean and 14% on test-other. We further improve the performance via N-best rescoring using a 24-layer self-attentive SRU language model, achieving WERs of 1.75% on test-clean and 4.46% on test-other",
    "checked": true,
    "id": "04d4543a2d38895bbaee0475fc6bc6c97ec6b696",
    "semantic_title": "asapp-asr: multistream cnn and self-attentive sru for sota speech recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kadetotad20_interspeech.html": {
    "title": "Compressing LSTM Networks with Hierarchical Coarse-Grain Sparsity",
    "volume": "main",
    "abstract": "The long short-term memory (LSTM) network is one of the most widely used recurrent neural networks (RNNs) for automatic speech recognition (ASR), but is parametrized by millions of parameters. This makes it prohibitive for memory-constrained hardware accelerators as the storage demand causes higher dependence on off-chip memory, which bottlenecks latency and power. In this paper, we propose a new LSTM training technique based on hierarchical coarse-grain sparsity (HCGS), which enforces hierarchical structured sparsity by randomly dropping static block-wise connections between layers. HCGS maintains the same hierarchical structured sparsity throughout training and inference; this reduces weight storage for both training and inference hardware systems. We also jointly optimize in-training quantization with HCGS on 2-/3-layer LSTM networks for the TIMIT and TED-LIUM corpora. With 16× structured compression and 6-bit weight precision, we achieved a phoneme error rate (PER) of 16.9% for TIMIT and a word error rate (WER) of 18.9% for TED-LIUM, showing the best trade-off between error rate and LSTM memory compression compared to prior works",
    "checked": true,
    "id": "e9dc99949241a2ef2a161ecaaf20cb3e36b588e1",
    "semantic_title": "compressing lstm networks with hierarchical coarse-grain sparsity",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lohrenz20_interspeech.html": {
    "title": "BLSTM-Driven Stream Fusion for Automatic Speech Recognition: Novel Methods and a Multi-Size Window Fusion Example",
    "volume": "main",
    "abstract": "Optimal fusion of streams for ASR is a nontrivial problem. Recently, so-called posterior-in-posterior-out (PIPO-)BLSTMs have been proposed that serve as state sequence enhancers and have highly attractive training properties. In this work, we adopt the PIPO-BLSTMs and employ them in the context of stream fusion for ASR. Our contributions are the following: First, we show the positive effect of a PIPO-BLSTM as state sequence enhancer for various stream fusion approaches. Second, we confirm the advantageous context-free (CF) training property of the PIPO-BLSTM for all investigated fusion approaches. Third, we show with a fusion example of two streams, stemming from different short-time Fourier transform window lengths, that all investigated fusion approaches take profit. Finally, the turbo fusion approach turns out to be best, employing a CF-type PIPO-BLSTM with a novel iterative augmentation in training",
    "checked": true,
    "id": "4d634e9dff89704d68fc0f49e06c94db4ab831d2",
    "semantic_title": "blstm-driven stream fusion for automatic speech recognition: novel methods and a multi-size window fusion example",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pham20_interspeech.html": {
    "title": "Relative Positional Encoding for Speech Recognition and Direct Translation",
    "volume": "main",
    "abstract": "Transformer models are powerful sequence-to-sequence architectures that are capable of directly mapping speech inputs to transcriptions or translations. However, the mechanism for modeling positions in this model was tailored for text modeling, and thus is less ideal for acoustic inputs. In this work, we adapt the relative position encoding scheme to the Speech Transformer, where the key addition is relative distance between input states in the self-attention network. As a result, the network can better adapt to the variable distributions present in speech data. Our experiments show that our resulting model achieves the best recognition result on the Switchboard benchmark in the non-augmentation condition, and the best published result in the MuST-C speech translation benchmark. We also show that this model is able to better utilize synthetic data than the Transformer, and adapts better to variable sentence segmentation quality for speech translation",
    "checked": true,
    "id": "0a82b81fbc0bc25bf4d60f9a18c8ee3571e80d7d",
    "semantic_title": "relative positional encoding for speech recognition and direct translation",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kanda20_interspeech.html": {
    "title": "Joint Speaker Counting, Speech Recognition, and Speaker Identification for Overlapped Speech of any Number of Speakers",
    "volume": "main",
    "abstract": "We propose an end-to-end speaker-attributed automatic speech recognition model that unifies speaker counting, speech recognition, and speaker identification on monaural overlapped speech. Our model is built on serialized output training (SOT) with attention-based encoder-decoder, a recently proposed method for recognizing overlapped speech comprising an arbitrary number of speakers. We extend SOT by introducing a speaker inventory as an auxiliary input to produce speaker labels as well as multi-speaker transcriptions. All model parameters are optimized by speaker-attributed maximum mutual information criterion, which represents a joint probability for overlapped speech recognition and speaker identification. Experiments on LibriSpeech corpus show that our proposed method achieves significantly better speaker-attributed word error rate than the baseline that separately performs overlapped speech recognition and speaker identification",
    "checked": true,
    "id": "5390502397185775e3bceceb442b7d52d17383be",
    "semantic_title": "joint speaker counting, speech recognition, and speaker identification for overlapped speech of any number of speakers",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fukuda20_interspeech.html": {
    "title": "Implicit Transfer of Privileged Acoustic Information in a Generalized Knowledge Distillation Framework",
    "volume": "main",
    "abstract": "This paper proposes a novel generalized knowledge distillation framework, with an implicit transfer of privileged information. In our proposed framework, teacher networks are trained with two input branches on pairs of time-synchronous lossless and lossy acoustic features. While one branch of the teacher network processes a privileged view of the data using lossless features, the second branch models a student view, by processing lossy features corresponding to the same data. During the training step, weights of this teacher network are updated using a composite two-part cross entropy loss. The first part of this loss is computed between the predicted output labels of the lossless data and the actual ground truth. The second part of the loss is computed between the predicted output labels of the lossy data and lossless data. In the next step of generating soft labels, only the student view branch of the teacher is used with lossy data. The benefit of this proposed technique is shown on speech signals with long-term time-frequency bandwidth loss due to recording devices and network conditions. Compared to conventional generalized knowledge distillation with privileged information, the proposed method has a relative improvement of 9.5% on both lossless and lossy test sets",
    "checked": true,
    "id": "39f416ae6187f223d7242fa8fb11aabace0d382f",
    "semantic_title": "implicit transfer of privileged acoustic information in a generalized knowledge distillation framework",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20_interspeech.html": {
    "title": "Effect of Adding Positional Information on Convolutional Neural Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Attention-based models with convolutional encoders enable faster training and inference than recurrent neural network-based ones. However, convolutional models often require a very large receptive field to achieve high recognition accuracy, which not only increases the parameter size but also the computational cost and run-time memory footprint. A convolutional encoder with a short receptive field length can suffer from looping or skipping problems when the input utterance contains the same words as nearby sentences. We believe that this is due to the insufficient receptive field length, and try to remedy this problem by adding positional information to the convolution-based encoder. It is shown that the word error rate (WER) of a convolutional encoder with a short receptive field size can be reduced significantly by augmenting it with positional information. Visualization results are presented to demonstrate the effectiveness of adding positional information. The proposed method improves the accuracy of attention models with a convolutional encoder and achieves a WER of 10.60% on TED-LIUMv2 for an end-to-end speech recognition task",
    "checked": true,
    "id": "2a5db3e4a2dc8c9f0f5c221cb8a9b1fa4e2cc9a3",
    "semantic_title": "effect of adding positional information on convolutional neural networks for end-to-end speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20b_interspeech.html": {
    "title": "Deep Neural Network-Based Generalized Sidelobe Canceller for Robust Multi-Channel Speech Recognition",
    "volume": "main",
    "abstract": "The elastic spatial filter (ESF) proposed in recent years is a popular multi-channel speech enhancement front end based on deep neural network (DNN). It is suitable for real-time processing and has shown promising automatic speech recognition (ASR) results. However, the ESF only utilizes the knowledge of fixed beamforming, resulting in limited noise reduction capabilities. In this paper, we propose a DNN-based generalized sidelobe canceller (GSC) that can automatically track the target speaker's direction in real time and use the blocking technique to generate reference noise signals to further reduce noise from the fixed beam pointing to the target direction. The coefficients in the proposed GSC are fully learnable and an ASR criterion is used to optimize the entire network. The 4-channel experiments show that the proposed GSC achieves a relative word error rate improvement of 27.0% compared to the raw observation, 20.6% compared to the oracle direction-based traditional GSC, 10.5% compared to the ESF and 7.9% compared to the oracle mask-based generalized eigenvalue (GEV) beamformer",
    "checked": true,
    "id": "8756bb23d9ba9e0b8b1aa381965f245bd9a67399",
    "semantic_title": "deep neural network-based generalized sidelobe canceller for robust multi-channel speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20_interspeech.html": {
    "title": "Neural Spatio-Temporal Beamformer for Target Speech Separation",
    "volume": "main",
    "abstract": "Purely neural network (NN) based speech separation and enhancement methods, although can achieve good objective scores, inevitably cause nonlinear speech distortions that are harmful for the automatic speech recognition (ASR). On the other hand, the minimum variance distortionless response (MVDR) beamformer with NN-predicted masks, although can significantly reduce speech distortions, has limited noise reduction capability. In this paper, we propose a multi-tap MVDR beamformer with complex-valued masks for speech separation and enhancement. Compared to the state-of-the-art NN-mask based MVDR beamformer, the multi-tap MVDR beamformer exploits the inter-frame correlation in addition to the inter-microphone correlation that is already utilized in prior arts. Further improvements include the replacement of the real-valued masks with the complex-valued masks and the joint training of the complex-mask NN. The evaluation on our multi-modal multi-channel target speech separation and enhancement platform demonstrates that our proposed multi-tap MVDR beamformer improves both the ASR accuracy and the perceptual speech quality against prior arts",
    "checked": true,
    "id": "1cba72db8b8ce089680f0c7c078cab622bd841c3",
    "semantic_title": "neural spatio-temporal beamformer for target speech separation",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20c_interspeech.html": {
    "title": "Online Directional Speech Enhancement Using Geometrically Constrained Independent Vector Analysis",
    "volume": "main",
    "abstract": "This paper proposes an online dual-microphone system for directional speech enhancement, which employs geometrically constrained independent vector analysis (IVA) based on the auxiliary function approach and vectorwise coordinate descent. Its offline version has recently been proposed and shown to outperform the conventional auxiliary function approach-based IVA (AuxIVA) thanks to the properly designed spatial constraints. We extend the offline algorithm to online by incorporating the autoregressive approximation of an auxiliary variable. Experimental evaluations revealed that the proposed online algorithm could work in real-time and achieved superior speech enhancement performance to online AuxIVA in both situations where a fixed target was interfered by a spatially stationary or dynamic interference",
    "checked": true,
    "id": "5e71f69790abe9af869c24108c47c762e7c6878e",
    "semantic_title": "online directional speech enhancement using geometrically constrained independent vector analysis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20_interspeech.html": {
    "title": "End-to-End Multi-Look Keyword Spotting",
    "volume": "main",
    "abstract": "The performance of keyword spotting (KWS), measured in false alarms and false rejects, degrades significantly under the far field and noisy conditions. In this paper, we propose a multi-look neural network modeling for speech enhancement which simultaneously steers to listen to multiple sampled look directions. The multi-look enhancement is then jointly trained with KWS to form an end-to-end KWS model which integrates the enhanced signals from multiple look directions and leverages an attention mechanism to dynamically tune the model's attention to the reliable sources. We demonstrate, on our large noisy and far-field evaluation sets, that the proposed approach significantly improves the KWS performance against the baseline KWS system and a recent beamformer based multi-beam KWS system",
    "checked": true,
    "id": "0b3b732864e65c8882d4d9b014596de127c616a7",
    "semantic_title": "end-to-end multi-look keyword spotting",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20_interspeech.html": {
    "title": "Differential Beamforming for Uniform Circular Array with Directional Microphones",
    "volume": "main",
    "abstract": "Use of omni-directional microphones is commonly assumed in the differential beamforming with uniform circular arrays. The conventional differential beamforming with omni-directional elements tends to suffer in low white-noise-gain (WNG) at the low frequencies and decrease of directivity factor (DF) at high frequencies. WNG measures the robustness of beamformer and DF evaluates the array performance in the presence of reverberation. The major contributions of this paper are as follows: First, we extends the existing work by presenting a new approach with the use of the directional microphone elements, and show clearly the connection between the conventional beamforming and the proposed beamforming. Second, a comparative study is made to show that the proposed approach brings about the noticeable improvement in WNG at the low frequencies and some improvement in DF at the high frequencies by exploiting an additional degree of freedom in the differential beamforming design. In addition, the beampattern appears more frequency-invariant than that of the conventional method. Third, we study how the proposed beamformer performs as the number of microphone elements and the radius of the array vary",
    "checked": true,
    "id": "2372ab78474ed348060ddf3fe215b1a1ece4d03c",
    "semantic_title": "differential beamforming for uniform circular array with directional microphones",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qi20_interspeech.html": {
    "title": "Exploring Deep Hybrid Tensor-to-Vector Network Architectures for Regression Based Speech Enhancement",
    "volume": "main",
    "abstract": "This paper investigates different trade-offs between the number of model parameters and enhanced speech qualities by employing several deep tensor-to-vector regression models for speech enhancement. We find that a hybrid architecture, namely CNN-TT, is capable of maintaining a good quality performance with a reduced model parameter size. CNN-TT is composed of several convolutional layers at the bottom for feature extraction to improve speech quality and a tensor-train (TT) output layer on the top to reduce model parameters. We first derive a new upper bound on the generalization power of the convolutional neural network (CNN) based vector-to-vector regression models. Then, we provide experimental evidence on the Edinburgh noisy speech corpus to demonstrate that, in single-channel speech enhancement, CNN outperforms DNN at the expense of a small increment of model sizes. Besides, CNN-TT slightly outperforms the CNN counterpart by utilizing only 32% of the CNN model parameters. Besides, further performance improvement can be attained if the number of CNN-TT parameters is increased to 44% of the CNN model size. Finally, our experiments of multi-channel speech enhancement on a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture achieves better results than both DNN and CNN models in terms of better-enhanced speech qualities and smaller parameter sizes",
    "checked": true,
    "id": "797cac6bd3207e1a2398f28087194541f44fc2f2",
    "semantic_title": "exploring deep hybrid tensor-to-vector network architectures for regression based speech enhancement",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20_interspeech.html": {
    "title": "An End-to-End Architecture of Online Multi-Channel Speech Separation",
    "volume": "main",
    "abstract": "Multi-speaker speech recognition has been one of the key challenges in conversation transcription as it breaks the single active speaker assumption employed by most state-of-the-art speech recognition systems. Speech separation is considered as a remedy to this problem. Previously, we introduced a system, called unmixing, fixed-beamformer and extraction (UFE), that was shown to be effective in addressing the speech overlap problem in conversation transcription. With UFE, an input mixed signal is processed by fixed beamformers, followed by a neural network post filtering. Although promising results were obtained, the system contains multiple individually developed modules, leading potentially sub-optimum performance. In this work, we introduce an end-to-end modeling version of UFE. To enable gradient propagation all the way, an attentional selection module is proposed, where an attentional weight is learnt for each beamformer and spatial feature sampled over space. Experimental results show that the proposed system achieves comparable performance in an offline evaluation with the original separate processing-based pipeline, while producing remarkable improvements in an online evaluation",
    "checked": true,
    "id": "e909128e078ecb6148ce0f102953d8448b7b5c54",
    "semantic_title": "an end-to-end architecture of online multi-channel speech separation",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nakagome20_interspeech.html": {
    "title": "Mentoring-Reverse Mentoring for Unsupervised Multi-Channel Speech Source Separation",
    "volume": "main",
    "abstract": "Mentoring-reverse mentoring, which is a novel knowledge transfer framework for unsupervised learning, is introduced in multi-channel speech source separation. This framework aims to improve two different systems, which are referred to as a senior and a junior system, by mentoring each other. The senior system, which is composed of a neural separator and a statistical blind source separation (BSS) model, generates a pseudo-target signal. The junior system, which is composed of a neural separator and a post-filter, was constructed using teacher-student learning with the pseudo-target signal generated from the senior system i.e, imitating the output from the senior system (mentoring step). Then, the senior system can be improved by propagating the shared neural separator of the grown-up junior system to the senior system (reverse mentoring step). Since the improved neural separator can give better initial parameters for the statistical BSS model, the senior system can yield more accurate pseudo-target signals, leading to iterative improvement of the pseudo-target signal generator and the neural separator. Experimental comparisons conducted under the condition where mixture-clean parallel data are not available demonstrated that the proposed mentoring-reverse mentoring framework yielded improvements in speech source separation over the existing unsupervised source separation methods",
    "checked": true,
    "id": "2f8b71c27626e58ec251d992ffcf8e19b3dec7bf",
    "semantic_title": "mentoring-reverse mentoring for unsupervised multi-channel speech source separation",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nakatani20_interspeech.html": {
    "title": "Computationally Efficient and Versatile Framework for Joint Optimization of Blind Speech Separation and Dereverberation",
    "volume": "main",
    "abstract": "This paper proposes new blind signal processing techniques for optimizing a multi-input multi-output (MIMO) convolutional beamformer (CBF) in a computationally efficient way to simultaneously perform dereverberation and source separation. For effective CBF optimization, a conventional technique factorizes it into a multiple-target weighted prediction error (WPE) based dereverberation filter and a separation matrix. However, this technique requires the calculation of a huge spatio-temporal covariance matrix that reflects the statistics of all the sources, which makes the computational cost very high. For computationally efficient optimization, this paper introduces two techniques: one that decomposes the huge covariance matrix into ones for individual sources, and another that decomposes the CBF into sub-filters for estimating individual sources. Both techniques effectively and substantively reduce the size of the covariance matrices that must calculated, and allow us to greatly reduce the computational cost without loss of optimality",
    "checked": true,
    "id": "ab7d3d318a9bceab3c605962f5a99c57a95e2790",
    "semantic_title": "computationally efficient and versatile framework for joint optimization of blind speech separation and dereverberation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tu20_interspeech.html": {
    "title": "A Space-and-Speaker-Aware Iterative Mask Estimation Approach to Multi-Channel Speech Recognition in the CHiME-6 Challenge",
    "volume": "main",
    "abstract": "We propose a space-and-speaker-aware iterative mask estimation (SSA-IME) approach to improving complex angular central Gaussian distributions (cACGMM) based beamforming in an iterative manner by leveraging upon the complementary information obtained from SSA-based regression. First, a mask calculated by beamformed speech features is proposed to enhance the estimation accuracy of the ideal ratio mask from noisy speech. Second, the outputs of cACGMM-beamformed speech with given time annotation as initial values are used to extract the log-power spectral and inter-phase difference features of different speakers serving as inputs to estimate the regression-based SSA model. Finally, in decoding, the mask estimated by the SSA model is also used to iteratively refine cACGMM-based masks, yielding enhanced multi-array speech. Tested on the recent CHiME-6 Challenge Track 1 tasks, the proposed SSA-IME framework significantly and consistently outperforms state-of-the-art approaches, and achieves the lowest word error rates for both Track 1 speech recognition tasks",
    "checked": true,
    "id": "b6128b94c124ad4cfba360cbbfaae39a3780d245",
    "semantic_title": "a space-and-speaker-aware iterative mask estimation approach to multi-channel speech recognition in the chime-6 challenge",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/youssef20_interspeech.html": {
    "title": "Identifying Causal Relationships Between Behavior and Local Brain Activity During Natural Conversation",
    "volume": "main",
    "abstract": "Characterizing precisely neurophysiological activity involved in natural conversations remains a major challenge. We explore in this paper the relationship between multimodal conversational behavior and brain activity during natural conversations. This is challenging due to Functional Magnetic Resonance Imaging (fMRI) time resolution and to the diversity of the recorded multimodal signals. We use a unique corpus including localized brain activity and behavior recorded during a fMRI experiment when several participants had natural conversations alternatively with a human and a conversational robot. The corpus includes fMRI responses as well as conversational signals that consist of synchronized raw audio and their transcripts, video and eye-tracking recordings. The proposed approach includes a first step to extract discrete neurophysiological time-series from functionally well defined brain areas, as well as behavioral time-series describing specific behaviors. Then, machine learning models are applied to predict neurophysiological time-series based on the extracted behavioral features. The results show promising prediction scores, and specific causal relationships are found between behaviors and the activity in functional brain areas for both conditions, i.e., human-human and human-robot conversations",
    "checked": true,
    "id": "a7869468bad54521f19bb1176a4c208a5a89fe6e",
    "semantic_title": "identifying causal relationships between behavior and local brain activity during natural conversation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20_interspeech.html": {
    "title": "Neural Entrainment to Natural Speech Envelope Based on Subject Aligned EEG Signals",
    "volume": "main",
    "abstract": "Reconstruction of speech envelope from neural signal is a general way to study neural entrainment, which helps to understand the neural mechanism underlying speech processing. Previous neural entrainment studies were mainly based on single-trial neural activities, and the reconstruction accuracy of speech envelope is not high enough, probably due to the interferences from diverse noises such as breath and heartbeat. Considering that such noises independently emerge in the consistent neural processing of the subjects responding to the same speech stimulus, we proposed a method to align and average electroencephalograph (EEG) signals of the subjects for the same stimuli to reduce the noises of neural signals. Pearson correlation of constructed speech envelops with the original ones showed a great improvement comparing to the single-trial based method. Our study improved the correlation coefficient in delta band from around 0.25 to 0.5, where 0.25 was obtained in previous leading studies based on single-trial. The speech tracking phenomenon not only occurred in the commonly reported delta and theta band, but also occurred in the gamma band of EEG. Moreover, the reconstruction accuracy for regular speech was higher than that for the time-reversed speech, suggesting that neural entrainment to natural speech envelope reflects speech semantics",
    "checked": true,
    "id": "0407dd4edfd9bcd3af5801f8b2bdb75fdd27eb06",
    "semantic_title": "neural entrainment to natural speech envelope based on subject aligned eeg signals",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lian20_interspeech.html": {
    "title": "Does Lexical Retrieval Deteriorate in Patients with Mild Cognitive Impairment? Analysis of Brain Functional Network Will Tell",
    "volume": "main",
    "abstract": "Alterations in speech and language are typical signs of mild cognitive impairment (MCI), considered to be the prodromal stage of Alzheimer's disease (AD). Yet, very few studies have pointed out at what stage their speech production is disrupted. To bridge this knowledge gap, the present study focused on lexical retrieval, a specific process during speech production, and investigated how it is affected in cognitively impairment patients with the state-of-the-art analysis of brain functional network. 17 patients with MCI and 20 age-matched controls were invited to complete a primed picture naming task, of which the prime was either semantically related or unrelated to the target. Using electroencephalography (EEG) signals collected during task performance, even-related potentials (ERPs) were analyzed, together with the construction of the brain functional network. Results showed that whereas MCI patients did not exhibit significant differences in reaction time and ERP responses, their brain functional network did alter associated with a significant main effect in accuracy. The observation of increased cluster coefficients and characteristic path length indicated deteriorations in global information processing, which provided evidence that deficits in lexical retrieval might have occurred even at the preclinical stage of AD",
    "checked": true,
    "id": "0457d416107d7905a07bc59c37a2c5e6a34d0e43",
    "semantic_title": "does lexical retrieval deteriorate in patients with mild cognitive impairment? analysis of brain functional network will tell",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fu20_interspeech.html": {
    "title": "Congruent Audiovisual Speech Enhances Cortical Envelope Tracking During Auditory Selective Attention",
    "volume": "main",
    "abstract": "Listeners usually have the ability to selectively attend to the target speech while ignoring competing sounds. The mechanism that top-down attention modulates the cortical envelope tracking to speech was proposed to account for this ability. Additional visual input, such as lipreading was considered beneficial for speech perception, especially in noise. However, the effect of audiovisual (AV) congruency on the dynamic properties of cortical envelope tracking activities was not discussed explicitly. And the involvement of cortical regions processing AV speech was unclear. To solve these issues, electroencephalography (EEG) was recorded while participants attending to one talker from a mixture for several AV conditions (audio-only, congruent and incongruent). Approaches of temporal response functions (TRFs) and inter-trial phase coherence (ITPC) analysis were utilized to index the cortical envelope tracking for each condition. Comparing with the audio-only condition, both indices were enhanced only for the congruent AV condition, and the enhancement was prominent over both the auditory and visual cortex. In addition, timings of different cortical regions involved in cortical envelope tracking activities were subject to stimulus modality. The present work provides new insight into the neural mechanisms of auditory selective attention when visual input is available",
    "checked": true,
    "id": "891bd3df3421dea4c50b182a7baea3b6f39915c9",
    "semantic_title": "congruent audiovisual speech enhances cortical envelope tracking during auditory selective attention",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20_interspeech.html": {
    "title": "Contribution of RMS-Level-Based Speech Segments to Target Speech Decoding Under Noisy Conditions",
    "volume": "main",
    "abstract": "Human listeners can recognize target speech streams in complex auditory scenes. The cortical activities can robustly track the amplitude fluctuations of target speech with auditory attentional modulation under a range of signal-to-masker ratios (SMRs). The root-mean-square (RMS) level of the speech signal is a crucial acoustic cue for target speech perception. However, in most studies, the neural-tracking activities were analyzed with the intact speech temporal envelopes, ignoring the characteristic decoding features in different RMS-level-specific speech segments. This study aimed to explore the contributions of high- and middle-RMS-level segments to target speech decoding in noisy conditions based on electroencephalogram (EEG) signals. The target stimulus was mixed with a competing speaker at five SMRs (i.e., 6, 3, 0, -3, and -6 dB), and then the temporal response function (TRF) was used to analyze the relationship between neural responses and high/middle-RMS-level segments. Experimental results showed that target and ignored speech streams had significantly different TRF responses under conditions with the high- or middle-RMS-level segments. Besides, the high- and middle-RMS-level segments elicited different TRF responses in morphological distributions. These results suggested that distinct models could be used in different RMS-level-specific speech segments to better decode target speech with corresponding EEG signals",
    "checked": true,
    "id": "999f83c0dc899dbe653a536f804b68d3d1d1040f",
    "semantic_title": "contribution of rms-level-based speech segments to target speech decoding under noisy conditions",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20_interspeech.html": {
    "title": "Cortical Oscillatory Hierarchy for Natural Sentence Processing",
    "volume": "main",
    "abstract": "Human speech processing, either for listening or oral reading, requires dynamic cortical activities that are not only driven by sensory stimuli externally but also influenced by semantic knowledge and speech planning goals internally. Each of these functions has been known to accompany specific rhythmic oscillations and be localized in distributed networks. The question is how the brain organizes these spatially and spectrally distinct functional networks in such a temporal precision that endows us with incredible speech abilities. For clarification, this study conducted an oral reading task with natural sentences and collected simultaneously the involved brain waves, eye movements, and speech signals with high-density EEG and eye movement equipment. By examining the regional oscillatory spectral perturbation and modeling the frequency-specific interregional connections, our results revealed a hierarchical oscillatory mechanism, in which gamma oscillation entrains with the fine-structured sensory input while beta oscillation modulated the sensory output. Alpha oscillation mediated between sensory perception and cognitive function via selective suppression. Theta oscillation synchronized local networks for large-scale coordination. Differing from a single function-frequency-correspondence, the coexistence of multi-frequency oscillations was found to be critical for local regions to communicate remotely and diversely in a larger network",
    "checked": true,
    "id": "834bfee413d7f6ffb4fdf0db207559e8a743c1c5",
    "semantic_title": "cortical oscillatory hierarchy for natural sentence processing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bosch20_interspeech.html": {
    "title": "Comparing EEG Analyses with Different Epoch Alignments in an Auditory Lexical Decision Experiment",
    "volume": "main",
    "abstract": "In processing behavioral data from auditory lexical decision, reaction times (RT) can be defined relative to stimulus onset or relative to stimulus offset. Using stimulus onset as the reference invokes models that assumes that relevant processing starts immediately, while stimulus offset invokes models that assume that relevant processing can only start when the acoustic input is complete. It is suggested that EEG recordings can be used to tear apart putative processes. EEG analysis requires some kind of time-locking of epochs, so that averaging of multiple signals does not mix up effects of different processes. However, in many lexical decision experiments the duration of the speech stimuli varies substantially. Consequently, processes tied to stimulus offset are not appropriately aligned and might get lost in the averaging process. In this paper we investigate whether the time course of putative processes such as phonetic encoding, lexical access and decision making can be derived from ERPs and from instantaneous power representations in several frequency bands when epochs are time-locked at stimulus onset or stimulus offset. In addition, we investigate whether time-locking at the moment when the response is given can shed light on the decision process per sé",
    "checked": true,
    "id": "3db247f5d8d212f5d51d3601afdadeece380c0ad",
    "semantic_title": "comparing eeg analyses with different epoch alignments in an auditory lexical decision experiment",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/talkar20_interspeech.html": {
    "title": "Detection of Subclinical Mild Traumatic Brain Injury (mTBI) Through Speech and Gait",
    "volume": "main",
    "abstract": "Between 15% to 40% of mild traumatic brain injury (mTBI) patients experience incomplete recoveries or provide subjective reports of decreased motor abilities, despite a clinically-determined complete recovery. This demonstrates a need for objective measures capable of detecting subclinical residual mTBI, particularly in return-to-duty decisions for warfighters and return-to-play decisions for athletes. In this paper, we utilize features from recordings of directed speech and gait tasks completed by ten healthy controls and eleven subjects with lingering subclinical impairments from an mTBI. We hypothesize that decreased coordination and precision during fine motor movements governing speech production (articulation, phonation, and respiration), as well as during gross motor movements governing gait, can be effective indicators of subclinical mTBI. Decreases in coordination are measured from correlations of vocal acoustic feature time series and torso acceleration time series. We apply eigenspectra derived from these correlations to machine learning models to discriminate between the two subject groups. The fusion of correlation features derived from acoustic and gait time series achieve an AUC of 0.98. This highlights the potential of using the combination of vocal acoustic features from speech tasks and torso acceleration during a simple gait task as a rapid screening tool for subclinical mTBI ",
    "checked": true,
    "id": "d81bd4c3ed6ad4562d0681ea1d1a2798f1c82176",
    "semantic_title": "detection of subclinical mild traumatic brain injury (mtbi) through speech and gait",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shor20_interspeech.html": {
    "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
    "volume": "main",
    "abstract": "The ultimate goal of transfer learning is to reduce labeled data requirements by exploiting a pre-existing embedding model trained for different datasets or tasks. The visual and language communities have established benchmarks to compare embeddings, but the speech community has yet to do so. This paper proposes a benchmark for comparing speech representations on non-semantic tasks, and proposes a representation based on an unsupervised triplet-loss objective. The proposed representation outperforms other representations on the benchmark, and even exceeds state-of-the-art performance on a number of transfer learning tasks. The embedding is trained on a publicly available dataset, and it is tested on a variety of low-resource downstream tasks, including personalization tasks and medical domain. The benchmark , models , and evaluation code are publicly released",
    "checked": true,
    "id": "1042714c5be82d980066fd038105112e601a848e",
    "semantic_title": "towards learning a universal non-semantic representation of speech",
    "citation_count": 115
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rajan20_interspeech.html": {
    "title": "Poetic Meter Classification Using i-Vector-MTF Fusion",
    "volume": "main",
    "abstract": "In this paper, a deep neural network (DNN)-based poetic meter classification scheme is proposed using a fusion of musical texture features (MTF) and i-vectors. The experiment is performed in two phases. Initially, the mel-frequency cepstral coefficient (MFCC) features are fused with MTF and classification is done using DNN. MTF include timbral, rhythmic, and melodic features. Later, in the second phase, the MTF is fused with i-vectors and classification is performed. The performance is evaluated using a newly created poetic corpus in Malayalam, one of the prominent languages in India. While the MFCC-MTF/DNN system reports an overall accuracy of 80.83%, the i-vector/MTF fusion reports an overall accuracy of 86.66%. The performance is also compared with a baseline support vector machine (SVM)-based classifier. The results show that the architectural choice of i-vector fusion with MTF on DNN has merit in recognizing meters from recited poems",
    "checked": true,
    "id": "39dd2d0bc5494d7ec4c9b9a6083efe389503ed27",
    "semantic_title": "poetic meter classification using i-vector-mtf fusion",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dai20_interspeech.html": {
    "title": "Formant Tracking Using Dilated Convolutional Networks Through Dense Connection with Gating Mechanism",
    "volume": "main",
    "abstract": "Formant tracking is one of the most fundamental problems in speech processing. Traditionally, formants are estimated using signal processing methods. Recent studies showed that generic convolutional architectures can outperform recurrent networks on temporal tasks such as speech synthesis and machine translation. In this paper, we explored the use of Temporal Convolutional Network (TCN) for formant tracking. In addition to the conventional implementation, we modified the architecture from three aspects. First, we turned off the \"causal\" mode of dilated convolution, making the dilated convolution see the future speech frames. Second, each hidden layer reused the output information from all the previous layers through dense connection. Third, we also adopted a gating mechanism to alleviate the problem of gradient disappearance by selectively forgetting unimportant information. The model was validated on the open access formant database VTR. The experiment showed that our proposed model was easy to converge and achieved an overall mean absolute percent error (MAPE) of 8.2% on speech-labeled frames, compared to three competitive baselines of 9.4% (LSTM), 9.1% (Bi-LSTM) and 8.9% (TCN)",
    "checked": true,
    "id": "011a36d6e092e018998b8502966a61a67ad83aa0",
    "semantic_title": "formant tracking using dilated convolutional networks through dense connection with gating mechanism",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20_interspeech.html": {
    "title": "Automatic Analysis of Speech Prosody in Dutch",
    "volume": "main",
    "abstract": "In this paper we present a publicly available tool for automatic analysis of speech prosody (AASP) in Dutch. Incorporating the state-of-the-art analytical frameworks, AASP enables users to analyze prosody at two levels from different theoretical perspectives. Holistically, by means of the Functional Principal Component Analysis (FPCA) it generates mathematical functions that capture changes in the shape of a pitch contour. The tool outputs the weights of principal components in a table for users to process in further statistical analysis. Structurally, AASP analyzes prosody in terms of prosodic events within the auto-segmental metrical framework, hypothesizing prosodic labels in accordance with Transcription of Dutch Intonation (ToDI) with accuracy comparable to similar tools for other languages. Published as a Docker container, the tool can be set up on various operating systems in only two steps. Moreover, the tool is accessed through a graphic user interface, making it accessible to users with limited programming skills",
    "checked": true,
    "id": "1c0d78af159c753c974496eca2925910b516fc3d",
    "semantic_title": "automatic analysis of speech prosody in dutch",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gresse20_interspeech.html": {
    "title": "Learning Voice Representation Using Knowledge Distillation for Automatic Voice Casting",
    "volume": "main",
    "abstract": "The search for professional voice-actors for audiovisual productions is a sensitive task, performed by the artistic directors (ADs). The ADs have a strong appetite for new talents/voices but cannot perform large scale auditions. Automatic tools able to suggest the most suited voices are of a great interest for audiovisual industry. In previous works, we showed the existence of acoustic information allowing to mimic the AD's choices. However, the only available information is the ADs' choices from the already dubbed multimedia productions. In this paper, we propose a representation-learning based strategy to build a character/role representation, called p-vector. In addition, the large variability between audiovisual productions makes it difficult to have homogeneous training datasets. We overcome this difficulty by using knowledge distillation methods to take advantage of external datasets. Experiments are conducted on video-game voice excerpts. Results show a significant improvement using the p-vector, compared to the speaker-based x-vector representation",
    "checked": false,
    "id": "f0a867e24fe77a65bf19ba9ea6759b39cf11f8e0",
    "semantic_title": "apprentissage automatique de représentation de voix à l'aide d'une distillation de la connaissance pour le casting vocal (learning voice representation using knowledge distillation for automatic voice casting )",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yegnanarayana20_interspeech.html": {
    "title": "Enhancing Formant Information in Spectrographic Display of Speech",
    "volume": "main",
    "abstract": "Formants are resonances of the time varying vocal tract system, and their characteristics are reflected in the response of the system for a sequence of impulse-like excitation sequence originated at the glottis. This paper presents a method to enhance the formants information in the display of spectrogram of the speech signal, especially for high pitched voices. It is well known that in the narrowband spectrogram, the presence of pitch harmonics masks the formant information, whereas in the wideband spectrogram, the formant regions are smeared. Using single frequency filtering (SFF) analysis, we show that the wideband equivalent SFF spectrogram can be modified to enhance the formant information in the display by improving the frequency resolution. For this, we obtain two SFF spectrograms by using single frequency filtering of the speech signal at two closely spaced roots on the real axis in the z-plane. The ratio or difference of the two SFF spectrograms is processed to enhance the formant information in the spectrographic display. This will help in tracking rapidly changing formants and in resolving closely spaced formants. The effect is more pronounced in the case of high-pitched voices, like female and children speech",
    "checked": true,
    "id": "4c96a8ca9b37c69c4052fd68dc779dc8096ae7b8",
    "semantic_title": "enhancing formant information in spectrographic display of speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gump20_interspeech.html": {
    "title": "Unsupervised Methods for Evaluating Speech Representations",
    "volume": "main",
    "abstract": "Disentanglement is a desired property in representation learning and a significant body of research has tried to show that it is a useful representational prior. Evaluating disentanglement is challenging, particularly for real world data like speech, where ground truth generative factors are typically not available. Previous work on disentangled representation learning in speech has used categorical supervision like phoneme or speaker identity in order to disentangle grouped feature spaces. However, this work differs from the typical dimension-wise view of disentanglement in other domains. This paper proposes to use low-level acoustic features to provide the structure required to evaluate dimension-wise disentanglement. By choosing well-studied acoustic features, grounded and descriptive evaluation is made possible for unsupervised representation learning. This work produces a toolkit for evaluating disentanglement in unsupervised representations of speech and evaluates its efficacy on previous research",
    "checked": true,
    "id": "26a006d2ad970efd5c25f5fe849740f3ff3ab48d",
    "semantic_title": "unsupervised methods for evaluating speech representations",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tran20_interspeech.html": {
    "title": "Robust Pitch Regression with Voiced/Unvoiced Classification in Nonstationary Noise Environments",
    "volume": "main",
    "abstract": "Accurate voiced/unvoiced information is crucial in estimating the pitch of a target speech signal in severe nonstationary noise environments. Nevertheless, state-of-the-art pitch estimators based on deep neural networks (DNN) lack a dedicated mechanism for robustly detecting voiced and unvoiced segments in the target speech in noisy conditions. In this work, we proposed an end-to-end deep learning-based pitch estimation framework which jointly detects voiced/unvoiced segments and predicts pitch values for the voiced regions of the ground-truth speech. We empirically showed that our proposed framework significantly more robust than state-of-the-art DNN based pitch detectors in nonstationary noise settings. Our results suggest that joint training of voiced/unvoiced detection and voiced pitch prediction can significantly improve pitch estimation performance",
    "checked": true,
    "id": "ef2d3fe969b497df3bd60247458df2e1c254cf57",
    "semantic_title": "robust pitch regression with voiced/unvoiced classification in nonstationary noise environments",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/setlur20_interspeech.html": {
    "title": "Nonlinear ISA with Auxiliary Variables for Learning Speech Representations",
    "volume": "main",
    "abstract": "This paper extends recent work on nonlinear Independent Component Analysis ( ica) by introducing a theoretical framework for nonlinear Independent Subspace Analysis ( isa) in the presence of auxiliary variables. Observed high dimensional acoustic features like log Mel spectrograms can be considered as surface level manifestations of nonlinear transformations over individual multivariate sources of information like speaker characteristics, phonological content etc. Under assumptions of energy based models we use the theory of nonlinear isa to propose an algorithm that learns unsupervised speech representations whose subspaces are independent and potentially highly correlated with the original non-stationary multivariate sources. We show how nonlinear ica with auxiliary variables can be extended to a generic identifiable model for subspaces as well while also providing sufficient conditions for the identifiability of these high dimensional subspaces. Our proposed methodology is generic and can be integrated with standard unsupervised approaches to learn speech representations with subspaces that can theoretically capture independent higher order speech signals. We evaluate the gains of our algorithm when integrated with the Autoregressive Predictive Coding ( apc) model by showing empirical results on the speaker verification and phoneme recognition tasks",
    "checked": true,
    "id": "1db6f2396fd6315044a552351baa3bcac1eabf8c",
    "semantic_title": "nonlinear isa with auxiliary variables for learning speech representations",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/takeuchi20_interspeech.html": {
    "title": "Harmonic Lowering for Accelerating Harmonic Convolution for Audio Signals",
    "volume": "main",
    "abstract": "Convolutional neural networks have been successfully applied to a variety of audio signal processing tasks including sound source separation, speech recognition and acoustic scene understanding. Since many pitched sounds have a harmonic structure, an operation, called harmonic convolution, has been proposed to take advantages of the structure appearing in the audio signals. However, the computational cost involved is higher than that of normal convolution. This paper proposes a faster calculation method of harmonic convolution called Harmonic Lowering. The method unrolls the input data to a redundant layout so that the normal convolution operation can be applied. The analysis of the runtimes and the number of multiplication operations show that the proposed method accelerates the harmonic convolution 2 to 7 times faster than the conventional method under realistic parameter settings, while no approximation is introduced",
    "checked": true,
    "id": "759d08cf11bd60dffaec8d4abb6964c4c9f4526b",
    "semantic_title": "harmonic lowering for accelerating harmonic convolution for audio signals",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ai20_interspeech.html": {
    "title": "Knowledge-and-Data-Driven Amplitude Spectrum Prediction for Hierarchical Neural Vocoders",
    "volume": "main",
    "abstract": "In our previous work, we have proposed a neural vocoder called HiNet which recovers speech waveforms by predicting amplitude and phase spectra hierarchically from input acoustic features. In HiNet, the amplitude spectrum predictor (ASP) predicts log amplitude spectra (LAS) from input acoustic features. This paper proposes a novel knowledge-and-data-driven ASP (KDD-ASP) to improve the conventional one. First, acoustic features (i.e., F0 and mel-cepstra) pass through a knowledge-driven LAS recovery module to obtain approximate LAS (ALAS). This module is designed based on the combination of STFT and source-filter theory, in which the source part and the filter part are designed based on input F0 and mel-cepstra, respectively. Then, the recovered ALAS are processed by a data-driven LAS refinement module which consists of multiple trainable convolutional layers to get the final LAS. Experimental results show that the HiNet vocoder using KDD-ASP can achieve higher quality of synthetic speech than that using conventional ASP and the WaveRNN vocoder on a text-to-speech (TTS) task",
    "checked": true,
    "id": "383133f2cf579661146a792df7c6ebf49c6ca96b",
    "semantic_title": "knowledge-and-data-driven amplitude spectrum prediction for hierarchical neural vocoders",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tian20_interspeech.html": {
    "title": "FeatherWave: An Efficient High-Fidelity Neural Vocoder with Multi-Band Linear Prediction",
    "volume": "main",
    "abstract": "In this paper, we propose the FeatherWave, yet another variant of WaveRNN vocoder combining the multi-band signal processing and the linear predictive coding. The LPCNet, a recently proposed neural vocoder which utilized the linear predictive characteristic of speech signal in the WaveRNN architecture, can generate high quality speech with a speed faster than real-time on a single CPU core. However, LPCNet is still not efficient enough for online speech generation tasks. To address this issue, we adopt the multi-band linear predictive coding for WaveRNN vocoder. The multi-band method enables the model to generate several speech samples in parallel at one step. Therefore, it can significantly improve the efficiency of speech synthesis. The proposed model with 4 sub-bands needs less than 1.6 GFLOPS for speech generation. In our experiments, it can generate 24 kHz high-fidelity audio 9× faster than real-time on a single CPU, which is much faster than the LPCNet vocoder. Furthermore, our subjective listening test shows that the FeatherWave can generate speech with better quality than LPCNet",
    "checked": true,
    "id": "cf8fbc40e9f92729b904e835640e32ae71416c16",
    "semantic_title": "featherwave: an efficient high-fidelity neural vocoder with multi-band linear prediction",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20_interspeech.html": {
    "title": "VocGAN: A High-Fidelity Real-Time Vocoder with a Hierarchically-Nested Adversarial Network",
    "volume": "main",
    "abstract": "We present a novel high-fidelity real-time neural vocoder called VocGAN. A recently developed GAN-based vocoder, MelGAN, produces speech waveforms in real-time. However, it often produces a waveform that is insufficient in quality or inconsistent with acoustic characteristics of the input mel spectrogram. VocGAN is nearly as fast as MelGAN, but it significantly improves the quality and consistency of the output waveform. VocGAN applies a multi-scale waveform generator and a hierarchically-nested discriminator to learn multiple levels of acoustic properties in a balanced way. It also applies the joint conditional and unconditional objective, which has shown successful results in high-resolution image synthesis. In experiments, VocGAN synthesizes speech waveforms 416.7× faster on a GTX 1080Ti GPU and 3.24× faster on a CPU than real-time. Compared with MelGAN, it also exhibits significantly improved quality in multiple evaluation metrics including mean opinion score (MOS) with minimal additional overhead. Additionally, compared with Parallel WaveGAN, another recently developed high-fidelity vocoder, VocGAN is 6.98× faster on a CPU and exhibits higher MOS",
    "checked": true,
    "id": "4cd2c3440b3c770ce3e1e75a0371a892bdb30945",
    "semantic_title": "vocgan: a high-fidelity real-time vocoder with a hierarchically-nested adversarial network",
    "citation_count": 55
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kanagawa20_interspeech.html": {
    "title": "Lightweight LPCNet-Based Neural Vocoder with Tensor Decomposition",
    "volume": "main",
    "abstract": "This paper proposes a lightweight neural vocoder based on LPCNet. The recently proposed LPCNet exploits linear predictive coding to represent vocal tract characteristics, and can rapidly synthesize high-quality waveforms with fewer parameters than WaveRNN. For even greater speeds, it is necessary to reduce the time-heavy two GRUs and the DualFC. Although the original work only pruned the first GRU weight, there is room for improvements in the other GRU and DualFC. Accordingly, we use tensor decomposition to reduce these remaining parameters by more than 80%. For the proposed method we demonstrate that 1) it is 1.26 times faster on a CPU, and 2) it matched naturalness of the original LPCNet for acoustic features extracted from natural speech and for those predicted by TTS",
    "checked": true,
    "id": "f75b0d27a5175e1a8d0c643f26259022997803ae",
    "semantic_title": "lightweight lpcnet-based neural vocoder with tensor decomposition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hsu20_interspeech.html": {
    "title": "WG-WaveNet: Real-Time High-Fidelity Speech Synthesis Without GPU",
    "volume": "main",
    "abstract": "In this paper, we propose WG-WaveNet, a fast, lightweight, and high-quality waveform generation model. WG-WaveNet is composed of a compact flow-based model and a post-filter. The two components are jointly trained by maximizing the likelihood of the training data and optimizing loss functions on the frequency domains. As we design a flow-based model that is heavily compressed, the proposed model requires much less computational resources compared to other waveform generation models during both training and inference time; even though the model is highly compressed, the post-filter maintains the quality of generated waveform. Our PyTorch implementation can be trained using less than 8 GB GPU memory and generates audio samples at a rate of more than 960 kHz on an NVIDIA 1080Ti GPU. Furthermore, even if synthesizing on a CPU, we show that the proposed method is capable of generating 44.1 kHz speech waveform 1.2 times faster than real-time. Experiments also show that the quality of generated audio is comparable to those of other methods. Audio samples are publicly available online",
    "checked": true,
    "id": "67b59158d4925767bb8e615d9ffa372384f8b81c",
    "semantic_title": "wg-wavenet: real-time high-fidelity speech synthesis without gpu",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/stephenson20_interspeech.html": {
    "title": "What the Future Brings: Investigating the Impact of Lookahead for Incremental Neural TTS",
    "volume": "main",
    "abstract": "In incremental text to speech synthesis (iTTS), the synthesizer produces an audio output before it has access to the entire input sentence. In this paper, we study the behavior of a neural sequence-to-sequence TTS system when used in an incremental mode, i.e. when generating speech output for token n, the system has access to n+k tokens from the text sequence. We first analyze the impact of this incremental policy on the evolution of the encoder representations of token n for different values of k (the lookahead parameter). The results show that, on average, tokens travel 88% of the way to their full context representation with a one-word lookahead and 94% after 2 words. We then investigate which text features are the most influential on the evolution towards the final representation using a random forest analysis. The results show that the most salient factors are related to token length. We finally evaluate the effects of lookahead k at the decoder level, using a MUSHRA listening test. This test shows results that contrast with the above high figures: speech synthesis quality obtained with 2 word-lookahead is significantly lower than the one obtained with the full sentence",
    "checked": true,
    "id": "d890606b654e7ec1d90fa368290b306626090b75",
    "semantic_title": "what the future brings: investigating the impact of lookahead for incremental neural tts",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/popov20_interspeech.html": {
    "title": "Fast and Lightweight On-Device TTS with Tacotron2 and LPCNet",
    "volume": "main",
    "abstract": "We present a fast and lightweight on-device text-to-speech system based on state-of-art methods of feature and speech generation i.e. Tacotron2 and LPCNet. We show that modification of the basic pipeline combined with hardware-specific optimizations and extensive usage of parallelization enables running TTS service even on low-end devices with faster than realtime waveform generation. Moreover, the system preserves high quality of speech without noticeable degradation of Mean Opinion Score compared to the non-optimized baseline. While the system is mostly oriented on low-to-mid range hardware we believe that it can also be used in any CPU-based environment",
    "checked": true,
    "id": "ee9f2afd6c383979e01d7ff72dd5192f3b1898c3",
    "semantic_title": "fast and lightweight on-device tts with tacotron2 and lpcnet",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/song20_interspeech.html": {
    "title": "Efficient WaveGlow: An Improved WaveGlow Vocoder with Enhanced Speed",
    "volume": "main",
    "abstract": "Neural vocoder, such as WaveGlow, has become an important component in recent high-quality text-to-speech (TTS) systems. In this paper, we propose Efficient WaveGlow (EWG), a flow-based generative model serving as an efficient neural vocoder. Similar to WaveGlow, EWG has a normalizing flow backbone where each flow step consists of an affine coupling layer and an invertible 1×1 convolution. To reduce the number of model parameters and enhance the speed without sacrificing the quality of the synthesized speech, EWG improves WaveGlow in three aspects. First, the WaveNet-style transform network in WaveGlow is replaced with an FFTNet-style dilated convolution network. Next, to reduce the computation cost, group convolution is applied to both audio and local condition features. At last, the local condition is shared among the transform network layers in each coupling layer. As a result, EWG can reduce the number of floating-point operations (FLOPs) required to generate one-second audio and the number of model parameters both by more than 12 times. Experimental results show that EWG can reduce real-world inference time cost by more than twice, without any obvious reduction in the speech quality",
    "checked": true,
    "id": "96ec36cac1b7693f01282ce44735f928dd584c98",
    "semantic_title": "efficient waveglow: an improved waveglow vocoder with enhanced speed",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/maguer20_interspeech.html": {
    "title": "Can Auditory Nerve Models Tell us What's Different About WaveNet Vocoded Speech?",
    "volume": "main",
    "abstract": "Nowadays, synthetic speech is almost indistinguishable from human speech. The remarkable quality is mainly due to the displacing of signal processing based vocoders in favour of neural vocoders and, in particular, the WaveNet architecture. At the same time, speech synthesis evaluation is still facing difficulties in adjusting to these improvements. These difficulties are even more prevalent in the case of objective evaluation methodologies which do not correlate well with human perception. Yet, an often forgotten use of objective evaluation is to uncover prominent differences between speech signals. Such differences are crucial to decipher the improvement introduced by the use of WaveNet. Therefore, abandoning objective evaluation could be a serious mistake. In this paper, we analyze vocoded synthetic speech re-rendered using WaveNet, comparing it to standard vocoded speech. To do so, we objectively compare spectrograms and neurograms, the latter being the output of AN models. The spectrograms allow us to look at the speech production side, and the neurograms relate to the speech perception path. While we were not yet able to pinpoint how WaveNet and WORLD differ, our results suggest that the Mean-Rate (MR) neurograms in particular warrant further investigation",
    "checked": true,
    "id": "7a9e655652a5be68224b76d9c73ce41041780730",
    "semantic_title": "can auditory nerve models tell us what's different about wavenet vocoded speech?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/paul20_interspeech.html": {
    "title": "Speaker Conditional WaveRNN: Towards Universal Neural Vocoder for Unseen Speaker and Recording Conditions",
    "volume": "main",
    "abstract": "Recent advancements in deep learning led to human-level performance in single-speaker speech synthesis. However, there are still limitations in terms of speech quality when generalizing those systems into multiple-speaker models especially for unseen speakers and unseen recording qualities. For instance, conventional neural vocoders are adjusted to the training speaker and have poor generalization capabilities to unseen speakers. In this work, we propose a variant of WaveRNN, referred to as speaker conditional WaveRNN (SC-WaveRNN). We target towards the development of an efficient universal vocoder even for unseen speakers and recording conditions. In contrast to standard WaveRNN, SC-WaveRNN exploits additional information given in the form of speaker embeddings. Using publicly-available data for training, SC-WaveRNN achieves significantly better performance over baseline WaveRNN on both subjective and objective metrics. In MOS, SC-WaveRNN achieves an improvement of about 23% for seen speaker and seen recording condition and up to 95% for unseen speaker and unseen condition. Finally, we extend our work by implementing a multi-speaker text-to-speech (TTS) synthesis similar to zero-shot speaker adaptation. In terms of performance, our system has been preferred over the baseline TTS system by 60% over 15.5% and by 60.9% over 32.6%, for seen and unseen speakers, respectively",
    "checked": true,
    "id": "f56e37bf467f8e93efe97f9014ee133e8adfd3c7",
    "semantic_title": "speaker conditional wavernn: towards universal neural vocoder for unseen speaker and recording conditions",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20_interspeech.html": {
    "title": "Neural Homomorphic Vocoder",
    "volume": "main",
    "abstract": "In this paper, we propose the neural homomorphic vocoder (NHV), a source-filter model based neural vocoder framework. NHV synthesizes speech by filtering impulse trains and noise with linear time-varying (LTV) filters. A neural network controls the LTV filters by estimating complex cepstrums of time-varying impulse responses given acoustic features. The proposed framework can be trained with a combination of multi-resolution STFT loss and adversarial loss functions. Due to the use of DSP-based synthesis methods, NHV is highly efficient, fully controllable and interpretable. A vocoder was built under the framework to synthesize speech given log-Mel spectrograms and fundamental frequencies. While the model cost only 15 kFLOPs per sample, the synthesis quality remained comparable to baseline neural vocoders in both copy-synthesis and text-to-speech",
    "checked": true,
    "id": "638f2c213e221d75f3dd45518d47ace3630ac524",
    "semantic_title": "neural homomorphic vocoder",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gretter20_interspeech.html": {
    "title": "Overview of the Interspeech TLT2020 Shared Task on ASR for Non-Native Children's Speech",
    "volume": "main",
    "abstract": "We present an overview of the ASR challenge for non-native children's speech organized for a special session at Interspeech 2020. The data for the challenge was obtained in the context of a spoken language proficiency assessment administered at Italian schools for students between the ages of 9 and 16 who were studying English and German as a foreign language. The corpus distributed for the challenge was a subset of the English recordings. Participating teams competed either in a closed track, in which they could use only the training data released by the organizers of the challenge, or in an open track, in which they were allowed to use additional training data. The closed track received 9 entries and the open track received 7 entries, with the best scoring systems achieving substantial improvements over a state-of-the-art baseline system. This paper describes the corpus of non-native children's speech that was used for the challenge, analyzes the results, and discusses some points that should be considered for subsequent challenges in this domain in the future",
    "checked": true,
    "id": "f8d0fc38ffeed4afe26013eb87143bec4d38a42e",
    "semantic_title": "overview of the interspeech tlt2020 shared task on asr for non-native children's speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lo20_interspeech.html": {
    "title": "The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR Challenge",
    "volume": "main",
    "abstract": "This paper describes the NTNU ASR system participating in the Interspeech 2020 Non-Native Children's Speech ASR Challenge supported by the SIG-CHILD group of ISCA. This ASR shared task is made much more challenging due to the coexisting diversity of non-native and children speaking characteristics. In the setting of closed-track evaluation, all participants were restricted to develop their systems merely based on the speech and text corpora provided by the organizer. To work around this under-resourced issue, we built our ASR system on top of CNN-TDNNF-based acoustic models, meanwhile harnessing the synergistic power of various data augmentation strategies, including both utterance- and word-level speed perturbation and spectrogram augmentation, alongside a simple yet effective data-cleansing approach. All variants of our ASR system employed an RNN-based language model to rescore the first-pass recognition hypotheses, which was trained solely on the text dataset released by the organizer. Our system with the best configuration came out in second place, resulting in a word error rate (WER) of 17.59%, while those of the top-performing, second runner-up and official baseline systems are 15.67%, 18.71%, 35.09%, respectively",
    "checked": true,
    "id": "e719b336c0267c274881bff7646bea1aa9586f30",
    "semantic_title": "the ntnu system at the interspeech 2020 non-native children's speech asr challenge",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/knill20_interspeech.html": {
    "title": "Non-Native Children's Automatic Speech Recognition: The INTERSPEECH 2020 Shared Task ALTA Systems",
    "volume": "main",
    "abstract": "Automatic spoken language assessment (SLA) is a challenging problem due to the large variations in learner speech combined with limited resources. These issues are even more problematic when considering children learning a language, with higher levels of acoustic and lexical variability, and of code-switching compared to adult data. This paper describes the ALTA system for the INTERSPEECH 2020 Shared Task on Automatic Speech Recognition for Non-Native Children's Speech. The data for this task consists of examination recordings of Italian school children aged 9–16, ranging in ability from minimal, to basic, to limited but effective command of spoken English. A variety of systems were developed using the limited training data available, 49 hours. State-of-the-art acoustic models and language models were evaluated, including a diversity of lexical representations, handling code-switching and learner pronunciation errors, and grade specific models. The best single system achieved a word error rate (WER) of 16.9% on the evaluation data. By combining multiple diverse systems, including both grade independent and grade specific models, the error rate was reduced to 15.7%. This combined system was the best performing submission for both the closed and open tasks",
    "checked": true,
    "id": "0cc4a4cdc1a4c0d214a72470833cb127cfeef0a5",
    "semantic_title": "non-native children's automatic speech recognition: the interspeech 2020 shared task alta systems",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kathania20_interspeech.html": {
    "title": "Data Augmentation Using Prosody and False Starts to Recognize Non-Native Children's Speech",
    "volume": "main",
    "abstract": "This paper describes AaltoASR's speech recognition system for the INTERSPEECH 2020 shared task on Automatic Speech Recognition (ASR) for non-native children's speech. The task is to recognize non-native speech from children of various age groups given a limited amount of speech. Moreover, the speech being spontaneous has false starts transcribed as partial words, which in the test transcriptions leads to unseen partial words. To cope with these two challenges, we investigate a data augmentation-based approach. Firstly, we apply the prosody-based data augmentation to supplement the audio data. Secondly, we simulate false starts by introducing partial-word noise in the language modeling corpora creating new words. Acoustic models trained on prosody-based augmented data outperform the models using the baseline recipe or the SpecAugment-based augmentation. The partial-word noise also helps to improve the baseline language model. Our ASR system, a combination of these schemes, is placed third in the evaluation period and achieves the word error rate of 18.71%. Post-evaluation period, we observe that increasing the amounts of prosody-based augmented data leads to better performance. Furthermore, removing low-confidence-score words from hypotheses can lead to further gains. These two improvements lower the ASR error rate to 17.99%",
    "checked": true,
    "id": "10ea7cc20e56dc6d97e4646bf93cb9e7e4f4716b",
    "semantic_title": "data augmentation using prosody and false starts to recognize non-native children's speech",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shahin20_interspeech.html": {
    "title": "UNSW System Description for the Shared Task on Automatic Speech Recognition for Non-Native Children's Speech",
    "volume": "main",
    "abstract": "In this paper we describe our children's Automatic Speech Recognition (ASR) system for the first shared task on ASR for English non-native children's speech. The acoustic model comprises 6 Convolutional Neural Network (CNN) layers and 12 Factored Time-Delay Neural Network (TDNN-F) layers, trained by data from 5 different children's speech corpora. Speed perturbation, Room Impulse Response (RIR), babble noise and non-speech noise data augmentation methods were utilized to enhance the model robustness. Three Language Models (LMs) were employed: an in-domain LM trained on written data and speech transcriptions of non-native children, a LM trained on non-native written data and transcription of both native and non-native children's speech and a TEDLIUM LM trained on adult TED talks transcriptions. Lattices produced from the different ASR systems were combined and decoded using the Minimum Bayes-Risk (MBR) decoding algorithm to get the final output. Our system achieved a final Word Error Rate (WER) of 17.55% and 16.59% for both developing and testing sets respectively and ranked second among the 10 teams participating in the task",
    "checked": true,
    "id": "d66a6bb4a681cd291d56163c0e3f9a6ead33c7d4",
    "semantic_title": "unsw system description for the shared task on automatic speech recognition for non-native children's speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/horiguchi20_interspeech.html": {
    "title": "End-to-End Speaker Diarization for an Unknown Number of Speakers with Encoder-Decoder Based Attractors",
    "volume": "main",
    "abstract": "End-to-end speaker diarization for an unknown number of speakers is addressed in this paper. Recently proposed end-to-end speaker diarization outperformed conventional clustering-based speaker diarization, but it has one drawback: it is less flexible in terms of the number of speakers. This paper proposes a method for encoder-decoder based attractor calculation (EDA), which first generates a flexible number of attractors from a speech embedding sequence. Then, the generated multiple attractors are multiplied by the speech embedding sequence to produce the same number of speaker activities. The speech embedding sequence is extracted using the conventional self-attentive end-to-end neural speaker diarization (SA-EEND) network. In a two-speaker condition, our method achieved a 2.69% diarization error rate (DER) on simulated mixtures and a 8.07% DER on the two-speaker subset of CALLHOME, while vanilla SA-EEND attained 4.56% and 9.54%, respectively. In unknown numbers of speakers conditions, our method attained a 15.29% DER on CALLHOME, while the x-vector-based clustering method achieved a 19.43% DER",
    "checked": true,
    "id": "ee33d61522fd70fa4e6470decbdac6c17f8b4fdb",
    "semantic_title": "end-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors",
    "citation_count": 112
  },
  "https://www.isca-speech.org/archive/interspeech_2020/medennikov20_interspeech.html": {
    "title": "Target-Speaker Voice Activity Detection: A Novel Approach for Multi-Speaker Diarization in a Dinner Party Scenario",
    "volume": "main",
    "abstract": "Speaker diarization for real-life scenarios is an extremely challenging problem. Widely used clustering-based diarization approaches perform rather poorly in such conditions, mainly due to the limited ability to handle overlapping speech. We propose a novel Target-Speaker Voice Activity Detection (TS-VAD) approach, which directly predicts an activity of each speaker on each time frame. TS-VAD model takes conventional speech features (e.g., MFCC) along with i-vectors for each speaker as inputs. A set of binary classification output layers produces activities of each speaker. I-vectors can be estimated iteratively, starting with a strong clustering-based diarization We also extend the TS-VAD approach to the multi-microphone case using a simple attention mechanism on top of hidden representations extracted from the single-channel TS-VAD model. Moreover, post-processing strategies for the predicted speaker activity probabilities are investigated. Experiments on the CHiME-6 unsegmented data show that TS-VAD achieves state-of-the-art results outperforming the baseline x-vector-based system by more than 30% Diarization Error Rate (DER) abs",
    "checked": true,
    "id": "50a6b28d4b9d13f030407e3d860d1c852dc213bd",
    "semantic_title": "target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario",
    "citation_count": 108
  },
  "https://www.isca-speech.org/archive/interspeech_2020/aronowitz20_interspeech.html": {
    "title": "New Advances in Speaker Diarization",
    "volume": "main",
    "abstract": "Recently, speaker diarization based on speaker embeddings has shown excellent results in many works. In this paper we propose several enhancements throughout the diarization pipeline. This work addresses two clustering frameworks: agglomerative hierarchical clustering (AHC) and spectral clustering (SC) First, we use multiple speaker embeddings. We show that fusion of x-vectors and d-vectors boosts accuracy significantly. Second, we train neural networks to leverage both acoustic and duration information for scoring similarity of segments or clusters. Third, we introduce a novel method to guide the AHC clustering mechanism using a neural network. Fourth, we handle short duration segments in SC by deemphasizing their effect on setting the number of speakers Finally, we propose a novel method for estimating the number of clusters in the SC framework. The method takes each eigenvalue and analyzes the projections of the SC similarity matrix on the corresponding eigenvector We evaluated our system on NIST SRE 2000 CALLHOME and, using cross-validation, we achieved an error rate of 5.1%, going beyond state-of-the-art speaker diarization",
    "checked": true,
    "id": "9a840a0aef4a28fa503d62be7dc1b881d6236221",
    "semantic_title": "new advances in speaker diarization",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20_interspeech.html": {
    "title": "Self-Attentive Similarity Measurement Strategies in Speaker Diarization",
    "volume": "main",
    "abstract": "Speaker diarization can be described as the process of extracting sequential speaker embeddings from an audio stream and clustering them according to speaker identities. Nowadays, deep neural network based approaches like x-vector have been widely adopted for speaker embedding extraction. However, in the clustering back-end, probabilistic linear discriminant analysis (PLDA) is still the dominant algorithm for similarity measurement. PLDA works in a pair-wise and independent manner, which may ignore the positional correlation of adjacent speaker embeddings. To address this issue, our previous work proposed the long short-term memory (LSTM) based scoring model, followed by the spectral clustering algorithm. In this paper, we further propose two enhanced methods based on the self-attention mechanism, which no longer focuses on the local correlation but searches for similar speaker embeddings in the whole sequence. The first approach achieves state-of-the-art performance on the DIHARD II Eval Set (18.44% DER after resegmentation), while the second one operates with higher efficiency",
    "checked": true,
    "id": "52dc0f0576d75064f7ff06d69ef717f95487e25c",
    "semantic_title": "self-attentive similarity measurement strategies in speaker diarization",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20b_interspeech.html": {
    "title": "Speaker Attribution with Voice Profiles by Graph-Based Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Speaker attribution is required in many real-world applications, such as meeting transcription, where speaker identity is assigned to each utterance according to speaker voice profiles. In this paper, we propose to solve the speaker attribution problem by using graph-based semi-supervised learning methods. A graph of speech segments is built for each session, on which segments from voice profiles are represented by labeled nodes while segments from test utterances are unlabeled nodes. The weight of edges between nodes is evaluated by the similarities between the pretrained speaker embeddings of speech segments. Speaker attribution then becomes a semi-supervised learning problem on graphs, on which two graph-based methods are applied: label propagation (LP) and graph neural networks (GNNs). The proposed approaches are able to utilize the structural information of the graph to improve speaker attribution performance. Experimental results on real meeting data show that the graph based approaches reduce speaker attribution error by up to 68% compared to a baseline speaker identification approach that processes each utterance independently",
    "checked": true,
    "id": "114ed6d345da665393493b52669a9e9b0ecbdd12",
    "semantic_title": "speaker attribution with voice profiles by graph-based semi-supervised learning",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/singh20_interspeech.html": {
    "title": "Deep Self-Supervised Hierarchical Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "The state-of-the-art speaker diarization systems use agglomerative hierarchical clustering (AHC) which performs the clustering of previously learned neural embeddings. While the clustering approach attempts to identify speaker clusters, the AHC algorithm does not involve any further learning. In this paper, we propose a novel algorithm for hierarchical clustering which combines the speaker clustering along with a representation learning framework. The proposed approach is based on principles of self-supervised learning where the self-supervision is derived from the clustering algorithm. The representation learning network is trained with a regularized triplet loss using the clustering solution at the current step while the clustering algorithm uses the deep embeddings from the representation learning step. By combining the self-supervision based representation learning along with the clustering algorithm, we show that the proposed algorithm improves significantly (29% relative improvement) over the AHC algorithm with cosine similarity for a speaker diarization task on CALLHOME dataset. In addition, the proposed approach also improves over the state-of-the-art system with PLDA affinity matrix with 10% relative improvement in DER",
    "checked": true,
    "id": "425303b773b2746beeb96bf6653c9995311d56b8",
    "semantic_title": "deep self-supervised hierarchical clustering for speaker diarization",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20_interspeech.html": {
    "title": "Spot the Conversation: Speaker Diarisation in the Wild",
    "volume": "main",
    "abstract": "The goal of this paper is speaker diarisation of videos collected ‘in the wild' We make three key contributions. First, we propose an automatic audio-visual diarisation method for YouTube videos. Our method consists of active speaker detection using audio-visual methods and speaker verification using self-enrolled speaker models. Second, we integrate our method into a semi-automatic dataset creation pipeline which significantly reduces the number of hours required to annotate videos with diarisation labels. Finally, we use this pipeline to create a large-scale diarisation dataset called VoxConverse, collected from ‘in the wild' videos, which we will release publicly to the research community. Our dataset consists of overlapping speech, a large and diverse speaker pool, and challenging background conditions",
    "checked": true,
    "id": "49528263d02e03f5b877dc081a1e049ab88fe504",
    "semantic_title": "spot the conversation: speaker diarisation in the wild",
    "citation_count": 86
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20_interspeech.html": {
    "title": "Learning Contextual Language Embeddings for Monaural Multi-Talker Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end multi-speaker speech recognition has been a popular topic in recent years, as more and more researches focus on speech processing in more realistic scenarios. Inspired by the hearing mechanism of human beings, which enables us to concentrate on the interested speaker from the multi-speaker mixed speech by utilizing both audio and context knowledge, this paper explores the contextual information to improve the multi-talker speech recognition. In the proposed architecture, the novel embedding learning model is designed to accurately extract the contextual embedding from the multi-talker mixed speech directly. Then two advanced training strategies are further proposed to improve the new model. Experimental results show that our proposed method achieves a very large improvement on multi-speaker speech recognition, with ~25% relative WER reduction against the baseline end-to-end multi-talker ASR model",
    "checked": true,
    "id": "9e4047639948c4676861585c302431e41998a96e",
    "semantic_title": "learning contextual language embeddings for monaural multi-talker speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/du20_interspeech.html": {
    "title": "Double Adversarial Network Based Monaural Speech Enhancement for Robust Speech Recognition",
    "volume": "main",
    "abstract": "To improve the noise robustness of automatic speech recognition (ASR), the generative adversarial network (GAN) based enhancement methods are employed as the front-end processing, which comprise a single adversarial process of an enhancement model and a discriminator. In this single adversarial process, the discriminator is encouraged to find differences between the enhanced and clean speeches, but the distribution of clean speeches is ignored. In this paper, we propose a double adversarial network (DAN) by adding another adversarial generation process (AGP), which forces the discriminator not only to find the differences but also to model the distribution. Furthermore, a functional mean square error (f-MSE) is proposed to utilize the representations learned by the discriminator. Experimental results reveal that AGP and f-MSE are crucial for the enhancement performance on ASR task, which are missed in previous GAN-based methods. Specifically, our DAN achieves 13.00% relative word error rate improvements over the noisy speeches on the test set of CHiME-2, which outperforms several recent GAN-based enhancement methods significantly",
    "checked": true,
    "id": "25fa6cf7cae30d347884438ba1126264e2619256",
    "semantic_title": "double adversarial network based monaural speech enhancement for robust speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bruguier20_interspeech.html": {
    "title": "Anti-Aliasing Regularization in Stacking Layers",
    "volume": "main",
    "abstract": "Shift-invariance is a desirable property of many machine learning models. It means that delaying the input of a model in time should only result in delaying its prediction in time. A model that is shift-invariant, also eliminates undesirable side effects like frequency aliasing. When building sequence models, not only should the shift-invariance property be preserved when sampling input features, it must also be respected inside the model itself. Here, we study the impact of the commonly used stacking layer in LSTM-based ASR models and show that aliasing is likely to occur. Experimentally, by adding merely 7 parameters to an existing speech recognition model that has 120 million parameters, we are able to reduce the impact of aliasing. This acts as a regularizer that discards frequencies the model shouldn't be relying on for predictions. Our results show that under conditions unseen at training, we are able to reduce the relative word error rate by up to 5%",
    "checked": true,
    "id": "76acec25016547c1f5db19ebe5591e3866c4c270",
    "semantic_title": "anti-aliasing regularization in stacking layers",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/andrusenko20_interspeech.html": {
    "title": "Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription",
    "volume": "main",
    "abstract": "While end-to-end ASR systems have proven competitive with the conventional hybrid approach, they are prone to accuracy degradation when it comes to noisy and low-resource conditions. In this paper, we argue that, even in such difficult cases, some end-to-end approaches show performance close to the hybrid baseline. To demonstrate this, we use the CHiME-6 Challenge data as an example of challenging environments and noisy conditions of everyday speech. We experimentally compare and analyze CTC-Attention versus RNN-Transducer approaches along with RNN versus Transformer architectures. We also provide a comparison of acoustic features and speech enhancements. Besides, we evaluate the effectiveness of neural network language models for hypothesis re-scoring in low-resource conditions. Our best end-to-end model based on RNN-Transducer, together with improved beam search, reaches quality by only 3.8% WER abs. worse than the LF-MMI TDNN-F CHiME-6 Challenge baseline. With the Guided Source Separation based training data augmentation, this approach outperforms the hybrid baseline system by 2.7% WER abs. and the end-to-end system best known before by 25.7% WER abs",
    "checked": true,
    "id": "c3c58177c073d677ec97de834340815b9dd89b42",
    "semantic_title": "towards a competitive end-to-end speech recognition for chime-6 dinner party transcription",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20b_interspeech.html": {
    "title": "End-to-End Far-Field Speech Recognition with Unified Dereverberation and Beamforming",
    "volume": "main",
    "abstract": "Despite successful applications of end-to-end approaches in multi-channel speech recognition, the performance still degrades severely when the speech is corrupted by reverberation. In this paper, we integrate the dereverberation module into the end-to-end multi-channel speech recognition system and explore two different frontend architectures. First, a multi-source mask-based weighted prediction error (WPE) module is incorporated in the frontend for dereverberation. Second, another novel frontend architecture is proposed, which extends the weighted power minimization distortionless response (WPD) convolutional beamformer to perform simultaneous separation and dereverberation. We derive a new formulation from the original WPD, which can handle multi-source input, and replace eigenvalue decomposition with the matrix inverse operation to make the back-propagation algorithm more stable. The above two architectures are optimized in a fully end-to-end manner, only using the speech recognition criterion. Experiments on both spatialized wsj1-2mix corpus and REVERB show that our proposed model outperformed the conventional methods in reverberant scenarios",
    "checked": true,
    "id": "721d7c82b80f14246d353251837e1711824a9e60",
    "semantic_title": "end-to-end far-field speech recognition with unified dereverberation and beamforming",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qiu20_interspeech.html": {
    "title": "Quaternion Neural Networks for Multi-Channel Distant Speech Recognition",
    "volume": "main",
    "abstract": "Despite the significant progress in automatic speech recognition (ASR), distant ASR remains challenging due to noise and reverberation. A common approach to mitigate this issue consists of equipping the recording devices with multiple microphones that capture the acoustic scene from different perspectives. These multi-channel audio recordings contain specific internal relations between each signal. In this paper, we propose to capture these inter- and intra- structural dependencies with quaternion neural networks, which can jointly process multiple signals as whole quaternion entities. The quaternion algebra replaces the standard dot product with the Hamilton one, thus offering a simple and elegant way to model dependencies between elements. The quaternion layers are then coupled with a recurrent neural network, which can learn long-term dependencies in the time domain. We show that a quaternion long-short term memory neural network (QLSTM), trained on the concatenated multi-channel speech signals, outperforms equivalent real-valued LSTM on two different tasks of multi-channel distant speech recognition",
    "checked": true,
    "id": "12957c9dc9045a04c229d25dcc02f0b636fbed78",
    "semantic_title": "quaternion neural networks for multi-channel distant speech recognition",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20_interspeech.html": {
    "title": "Improved Guided Source Separation Integrated with a Strong Back-End for the CHiME-6 Dinner Party Scenario",
    "volume": "main",
    "abstract": "The CHiME-6 dataset presents a difficult task with extreme speech overlap, severe noise and a natural speaking style. The gap of the word error rate (WER) is distinct between the audios recorded by the distant microphone arrays and the individual headset microphones. The official baseline exhibits a WER gap of approximately 10% even though the guided source separation (GSS) has achieved considerable WER reduction. In the paper, we make an effort to integrate an improved GSS with a strong automatic speech recognition (ASR) back-end, which bridges the WER gap and achieves substantial ASR performance improvement. Specifically, the proposed GSS is initialized by masks from data-driven deep-learning models, utilizes the spectral information and conducts a selection of the input channels. Meanwhile, we propose a data augmentation technique via random channel selection and deep convolutional neural network-based multi-channel acoustic models for back-end modeling. In the experiments, our framework largely reduced the WER to 34.78%/36.85% on the CHiME-6 development/evaluation set. Moreover, a narrower gap of 0.89%/4.67% was observed between the distant and headset audios. This framework is also the foundation of the IOA's submission to the CHiME-6 competition, which is ranked among the top systems",
    "checked": true,
    "id": "37bc21a50850348a0a3fa1db50e05b3714a172a5",
    "semantic_title": "improved guided source separation integrated with a strong back-end for the chime-6 dinner party scenario",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20c_interspeech.html": {
    "title": "Neural Speech Separation Using Spatially Distributed Microphones",
    "volume": "main",
    "abstract": "This paper proposes a neural network based speech separation method using spatially distributed microphones. Unlike with traditional microphone array settings, neither the number of microphones nor their spatial arrangement is known in advance, which hinders the use of conventional multi-channel speech separation neural networks based on fixed size input. To overcome this, a novel network architecture is proposed that interleaves inter-channel processing layers and temporal processing layers. The inter-channel processing layers apply a self-attention mechanism along the channel dimension to exploit the information obtained with a varying number of microphones. The temporal processing layers are based on a bidirectional long short term memory (BLSTM) model and applied to each channel independently. The proposed network leverages information across time and space by stacking these two kinds of layers alternately. Our network estimates time-frequency (TF) masks for each speaker, which are then used to generate enhanced speech signals either with TF masking or beamforming. Speech recognition experimental results show that the proposed method significantly outperforms baseline multi-channel speech separation systems",
    "checked": true,
    "id": "a87879088eb6b5425497edaf7f736b5377fcdbee",
    "semantic_title": "neural speech separation using spatially distributed microphones",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2020/horiguchi20b_interspeech.html": {
    "title": "Utterance-Wise Meeting Transcription System Using Asynchronous Distributed Microphones",
    "volume": "main",
    "abstract": "A novel framework for meeting transcription using asynchronous microphones is proposed in this paper. It consists of audio synchronization, speaker diarization, utterance-wise speech enhancement using guided source separation, automatic speech recognition, and duplication reduction. Doing speaker diarization before speech enhancement enables the system to deal with overlapped speech without considering sampling frequency mismatch between microphones. Evaluation on our real meeting datasets showed that our framework achieved a character error rate (CER) of 28.7% by using 11 distributed microphones, while a monaural microphone placed on the center of the table had a CER of 38.2%. We also showed that our framework achieved CER of 21.8%, which is only 2.1 percentage points higher than the CER in headset microphone-based transcription",
    "checked": true,
    "id": "dac0722cf97410467eef8e9e6884f93020267baf",
    "semantic_title": "utterance-wise meeting transcription system using asynchronous distributed microphones",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/deadman20_interspeech.html": {
    "title": "Simulating Realistically-Spatialised Simultaneous Speech Using Video-Driven Speaker Detection and the CHiME-5 Dataset",
    "volume": "main",
    "abstract": "Simulated data plays a crucial role in the development and evaluation of novel distant microphone ASR techniques. However, the commonly used simulated datasets adopt uninformed and potentially unrealistic speaker location distributions. We wish to generate more realistic simulations driven by recorded human behaviour. By using devices with a paired microphone array and camera, we analyse unscripted dinner party scenarios (CHiME-5) to estimate the distribution of speaker separation in a realistic setting. We deploy face-detection, and pose-detection techniques on 114 cameras to automatically locate speakers in 20 dinner party sessions. Our analysis found that on average, the separation between speakers was only 17 degrees. We use this analysis to create datasets with realistic distributions and compare it with commonly used datasets of simulated signals. By changing the position of speakers, we show that the word error rate can increase by over 73.5% relative when using a strong speech enhancement and ASR system",
    "checked": true,
    "id": "f3734b4d4a6d0ef227224e31734ef37376ddf0ad",
    "semantic_title": "simulating realistically-spatialised simultaneous speech using video-driven speaker detection and the chime-5 dataset",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/botelho20_interspeech.html": {
    "title": "Toward Silent Paralinguistics: Speech-to-EMG — Retrieving Articulatory Muscle Activity from Speech",
    "volume": "main",
    "abstract": "Electromyographic (EMG) signals recorded during speech production encode information on articulatory muscle activity and also on the facial expression of emotion, thus representing a speech-related biosignal with strong potential for paralinguistic applications. In this work, we estimate the electrical activity of the muscles responsible for speech articulation directly from the speech signal. To this end, we first perform a neural conversion of speech features into electromyographic time domain features, and then attempt to retrieve the original EMG signal from the time domain features. We propose a feed forward neural network to address the first step of the problem (speech features to EMG features) and a neural network composed of a convolutional block and a bidirectional long short-term memory block to address the second problem (true EMG features to EMG signal). We observe that four out of the five originally proposed time domain features can be estimated reasonably well from the speech signal. Further, the five time domain features are able to predict the original speech-related EMG signal with a concordance correlation coefficient of 0.663. We further compare our results with the ones achieved on the inverse problem of generating acoustic speech features from EMG features",
    "checked": false,
    "id": "3a4c3bd6a78aad367f49f903b31c588825c3bf3c",
    "semantic_title": "toward silent paralinguistics: speech-to-emg - retrieving articulatory muscle activity from speech",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20c_interspeech.html": {
    "title": "Multimodal Deception Detection Using Automatically Extracted Acoustic, Visual, and Lexical Features",
    "volume": "main",
    "abstract": "Deception detection in conversational dialogue has attracted much attention in recent years. Yet existing methods for this rely heavily on human-labeled annotations that are costly and potentially inaccurate. In this work, we present an automated system that utilizes multimodal features for conversational deception detection, without the use of human annotations. We study the predictive power of different modalities and combine them for better performance. We use openSMILE to extract acoustic features after applying noise reduction techniques to the original audio. Facial landmark features are extracted from the visual modality. We experiment with training facial expression detectors and applying Fisher Vectors to encode sequences of facial landmarks with varying length. Linguistic features are extracted from automatic transcriptions of the data. We examine the performance of these methods on the Box of Lies dataset of deception game videos, achieving 73% accuracy using features from all modalities. This result is significantly better than previous results on this corpus which relied on manual annotations, and also better than human performance",
    "checked": true,
    "id": "8339aa76f8ebb8b506d9688e50e98b886c71b472",
    "semantic_title": "multimodal deception detection using automatically extracted acoustic, visual, and lexical features",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pan20b_interspeech.html": {
    "title": "Multi-Modal Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion represents an essential aspect of human speech that is manifested in speech prosody. Speech, visual, and textual cues are complementary in human communication. In this paper, we study a hybrid fusion method, referred to as multi-modal attention network (MMAN) to makes use of visual and textual cues in speech emotion recognition. We propose a novel multi-modal attention mechanism, cLSTM-MMA, which facilitates the attention across three modalities and selectively fuse the information. cLSTM-MMA is fused with other uni-modal sub-networks in the late fusion. The experiments show that speech emotion recognition benefits significantly from visual and textual cues, and the proposed cLSTM-MMA alone is as competitive as other fusion methods in terms of accuracy, but with a much more compact network structure. The proposed hybrid network MMAN achieves state-of-the-art performance on IEMOCAP database for emotion recognition",
    "checked": true,
    "id": "2e762956621d3a3906b5891afa0f755e01ffcbde",
    "semantic_title": "multi-modal attention for speech emotion recognition",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shen20_interspeech.html": {
    "title": "WISE: Word-Level Interaction-Based Multimodal Fusion for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "While having numerous real-world applications, speech emotion recognition is still a technically challenging problem. How to effectively leverage the inherent multiple modalities in speech data (e.g., audio and text) is key to accurate classification. Existing studies normally choose to fuse multimodal features at the utterance level and largely neglect the dynamic interplay of features from different modalities at a fine-granular level over time. In this paper, we explicitly model dynamic interactions between audio and text at the word level via interaction units between two long short-term memory networks representing audio and text. We also devise a hierarchical representation of audio information from the frame, phoneme and word levels, which largely improves the expressiveness of resulting audio features. We finally propose WISE, a novel word-level interaction-based multimodal fusion framework for speech emotion recognition, to accommodate the aforementioned components. We evaluate WISE on the public benchmark IEMOCAP corpus and demonstrate that it outperforms state-of-the-art methods",
    "checked": true,
    "id": "3d1353331d71fec03fd11d8fdaee2435ca23f247",
    "semantic_title": "wise: word-level interaction-based multimodal fusion for speech emotion recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20b_interspeech.html": {
    "title": "A Multi-Scale Fusion Framework for Bimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is a challenging task that requires to learn suitable features for achieving good performance. The development of deep learning techniques makes it possible to automatically extract features rather than construct hand-crafted features. In this paper, a multi-scale fusion framework named STSER is proposed for bimodal SER by using speech and text information. A smodel, which takes advantage of convolutional neural network (CNN), bi-directional long short-term memory (Bi-LSTM) and the attention mechanism, is proposed to learn speech representation from the log-mel spectrogram extracted from speech data. Specifically, the CNN layers are utilized to learn local correlations. Then the Bi-LSTM layer is applied to learn long-term dependencies and contextual information. Finally, the multi-head self-attention layer makes the model focus on the features that are most related to the emotions. A tmodel using a pre-trained ALBERT model is applied for learning text representation from text data. Finally, a multi-scale fusion strategy, including feature fusion and ensemble learning, is applied to improve the overall performance. Experiments conducted on the public emotion dataset IEMOCAP have shown that the proposed STSER can achieve comparable recognition accuracy with fewer feature inputs",
    "checked": true,
    "id": "5c6942e94661dd67933c660b3661e9e5cb26ccbd",
    "semantic_title": "a multi-scale fusion framework for bimodal speech emotion recognition",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20b_interspeech.html": {
    "title": "Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion recognition is a challenging and actively-studied research area that plays a critical role in emotion-aware human-computer interaction systems. In a multimodal setting, temporal alignment between different modalities has not been well investigated yet. This paper presents a new model named as Gated Bidirectional Alignment Network (GBAN), which consists of an attention-based bidirectional alignment network over LSTM hidden states to explicitly capture the alignment relationship between speech and text, and a novel group gated fusion (GGF) layer to integrate the representations of different modalities. We empirically show that the attention-aligned representations outperform the last-hidden-states of LSTM significantly, and the proposed GBAN model outperforms existing state-of-the-art multimodal approaches on the IEMOCAP dataset",
    "checked": true,
    "id": "7d60294d3e19dafa1fb0143136c9efae4b8f59de",
    "semantic_title": "group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/khare20_interspeech.html": {
    "title": "Multi-Modal Embeddings Using Multi-Task Learning for Emotion Recognition",
    "volume": "main",
    "abstract": "General embeddings like word2vec, GloVe and ELMo have shown a lot of success in natural language tasks. The embeddings are typically extracted from models that are built on general tasks such as skip-gram models and natural language generation. In this paper, we extend the work from natural language understanding to multi-modal architectures that use audio, visual and textual information for machine learning tasks. The embeddings in our network are extracted using the encoder of a transformer model trained using multi-task training. We use person identification and automatic speech recognition as the tasks in our embedding generation framework. We tune and evaluate the embeddings on the downstream task of emotion recognition and demonstrate that on the CMU-MOSEI dataset, the embeddings can be used to improve over previous state of the art results",
    "checked": true,
    "id": "1904958461d165d1cfddcecb5358f413cba1f639",
    "semantic_title": "multi-modal embeddings using multi-task learning for emotion recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20d_interspeech.html": {
    "title": "Using Speaker-Aligned Graph Memory Block in Multimodally Attentive Emotion Recognition Network",
    "volume": "main",
    "abstract": "Integrating multimodal emotion sensing modules in realizing human-centered technologies is rapidly growing. Despite recent advancement of deep architectures in improving recognition performances, inability to handle individual differences in the expressive cues creates a major hurdle for real world applications. In this work, we propose a Speaker-aligned Graph Memory Network (SaGMN) that leverages the use of speaker embedding learned from a large speaker verification network to characterize such an individualized personal difference across speakers. Specifically, the learning of the gated memory block is jointly optimized with a speaker graph encoder which aligns similar vocal characteristics samples together while effectively enlarge the discrimination across emotion classes. We evaluate our multimodal emotion recognition network on the CMU-MOSEI database and achieve a state-of-art accuracy of 65.1% UAR and 74.7% F1 score. Further visualization experiments demonstrate the effect of speaker space alignment with the use of graph memory blocks",
    "checked": true,
    "id": "90eff998afb10d8f41e15b26f2db3b360fa0ebee",
    "semantic_title": "using speaker-aligned graph memory block in multimodally attentive emotion recognition network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lian20b_interspeech.html": {
    "title": "Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion recognition remains a complex task due to speaker variations and low-resource training samples. To address these difficulties, we focus on the domain adversarial neural networks (DANN) for emotion recognition. The primary task is to predict emotion labels. The secondary task is to learn a common representation where speaker identities can not be distinguished. By using this approach, we bring the representations of different speakers closer. Meanwhile, through using the unlabeled data in the training process, we alleviate the impact of low-resource training samples. In the meantime, prior work found that contextual information and multimodal features are important for emotion recognition. However, previous DANN based approaches ignore these information, thus limiting their performance. In this paper, we propose the context-dependent domain adversarial neural network for multimodal emotion recognition. To verify the effectiveness of our proposed method, we conduct experiments on the benchmark dataset IEMOCAP. Experimental results demonstrate that the proposed method shows an absolute improvement of 3.48% over state-of-the-art strategies",
    "checked": true,
    "id": "29b972f985c53d960b96a5bf9916cb132c138923",
    "semantic_title": "context-dependent domain adversarial neural network for multimodal emotion recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20b_interspeech.html": {
    "title": "ATCSpeech: A Multilingual Pilot-Controller Speech Corpus from Real Air Traffic Control Environment",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) technique has been greatly developed in recent years, which expedites many applications in other fields. For the ASR research, speech corpus is always an essential foundation, especially for the vertical industry, such as Air Traffic Control (ATC). There are some speech corpora for common applications, public or paid. However, for the ATC domain, it is difficult to collect raw speeches from real systems due to safety issues. More importantly, annotating the transcription is a more laborious work for the supervised learning ASR task, which hugely restricts the prospect of ASR application. In this paper, a multilingual speech corpus (ATCSpeech) from real ATC systems, including accented Mandarin Chinese and English speeches, is built and released to encourage the non-commercial ASR research in the ATC domain. The corpus is detailly introduced from the perspective of data amount, speaker gender and role, speech quality and other attributions. In addition, the performance of baseline ASR models is also reported. A community edition for our speech database can be applied and used under a special contract. To our best knowledge, this is the first work that aims at building a real and multilingual ASR corpus for the ATC related research",
    "checked": true,
    "id": "b6287bd90206b14fd43d9ac17e51417bc2fcaad5",
    "semantic_title": "atcspeech: a multilingual pilot-controller speech corpus from real air traffic control environment",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gutkin20_interspeech.html": {
    "title": "Developing an Open-Source Corpus of Yoruba Speech",
    "volume": "main",
    "abstract": "This paper introduces an open-source speech dataset for Yoruba — one of the largest low-resource West African languages spoken by at least 22 million people. Yoruba is one of the official languages of Nigeria, Benin and Togo, and is spoken in other neighboring African countries and beyond. The corpus consists of over four hours of 48 kHz recordings from 36 male and female volunteers and the corresponding transcriptions that include disfluency annotation. The transcriptions have full diacritization, which is vital for pronunciation and lexical disambiguation. The annotated speech dataset described in this paper is primarily intended for use in text-to-speech systems, serve as adaptation data in automatic speech recognition and speech-to-speech translation, and provide insights in West African corpus linguistics. We demonstrate the use of this corpus in a simple statistical parametric speech synthesis (SPSS) scenario evaluating it against the related languages from the CMU Wilderness dataset and the Yoruba Lagos-NWU corpus",
    "checked": true,
    "id": "d5ae43312497cd9648e7b2ed3b9370747d3317f0",
    "semantic_title": "developing an open-source corpus of yoruba speech",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ha20_interspeech.html": {
    "title": "ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) via call is essential for various applications, including AI for contact center (AICC) services. Despite the advancement of ASR, however, most publicly available call-based speech corpora such as Switchboard are old-fashioned. Also, most existing call corpora are in English and mainly focus on open domain dialog or general scenarios such as audiobooks. Here we introduce a new large-scale Korean call-based speech corpus under a goal-oriented dialog scenario from more than 11,000 people, i.e., ClovaCall corpus. ClovaCall includes approximately 60,000 pairs of a short sentence and its corresponding spoken utterance in a restaurant reservation domain. We validate the effectiveness of our dataset with intensive experiments using two standard ASR models. Furthermore, we release our ClovaCall dataset and baseline source codes to be available via ",
    "checked": true,
    "id": "a216567dac02d770a6ab860984c53ddaa2aab6ac",
    "semantic_title": "clovacall: korean goal-oriented dialog speech corpus for automatic speech recognition of contact centers",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20d_interspeech.html": {
    "title": "LAIX Corpus of Chinese Learner English: Towards a Benchmark for L2 English ASR",
    "volume": "main",
    "abstract": "This paper introduces a corpus of Chinese Learner English containing 82 hours of L2 English speech by Chinese learners from all major dialect regions, collected through mobile apps developed by LAIX Inc. The LAIX corpus was created to serve as a benchmark dataset for evaluating Automatic Speech Recognition (ASR) performance on L2 English, the first of this kind as far as we know. The paper describes our effort to build the corpus, including corpus design, data selection and transcription. Multiple rounds of quality check were conducted in the transcription process. Transcription errors were analyzed in terms of error types, rounds of reviewing, and learners' proficiency levels. Word error rates of state-of-the-art ASR systems on the benchmark corpus were also reported",
    "checked": true,
    "id": "3c9e473a3367e0346ce97751a24b5ba77f21ca8e",
    "semantic_title": "laix corpus of chinese learner english: towards a benchmark for l2 english asr",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ramanarayanan20_interspeech.html": {
    "title": "Design and Development of a Human-Machine Dialog Corpus for the Automated Assessment of Conversational English Proficiency",
    "volume": "main",
    "abstract": "This paper presents a carefully designed corpus of scored spoken conversations between English language learners and a dialog system to facilitate research and development of both human and machine scoring of dialog interactions. We collected speech, demographic and user experience data from non-native speakers of English who interacted with a virtual boss as part of a workplace pragmatics skill building application. Expert raters then scored the dialogs on a custom rubric encompassing 12 aspects of conversational proficiency as well as an overall holistic performance score. We analyze key corpus statistics and discuss the advantages of such a corpus for both human and machine scoring",
    "checked": true,
    "id": "23d00bf0fc60f7a1745c20b21a899cfe64365c21",
    "semantic_title": "design and development of a human-machine dialog corpus for the automated assessment of conversational english proficiency",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ng20_interspeech.html": {
    "title": "CUCHILD: A Large-Scale Cantonese Corpus of Child Speech for Phonology and Articulation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/leino20_interspeech.html": {
    "title": "FinChat: Corpus and Evaluation Setup for Finnish Chat Conversations on Everyday Topics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/segbroeck20_interspeech.html": {
    "title": "DiPCo — Dinner Party Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20e_interspeech.html": {
    "title": "Learning to Detect Bipolar Disorder and Borderline Personality Disorder with Language and Speech in Non-Clinical Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kirkedal20_interspeech.html": {
    "title": "FT Speech: Danish Parliament Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/duroselle20_interspeech.html": {
    "title": "Metric Learning Loss Functions to Reduce Domain Mismatch in the x-Vector Space for Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20e_interspeech.html": {
    "title": "The XMUSPEECH System for the AP19-OLR Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20f_interspeech.html": {
    "title": "On the Usage of Multi-Feature Integration for Speaker Verification and Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chowdhury20_interspeech.html": {
    "title": "What Does an End-to-End Dialect Identification Model Learn About Non-Dialectal Information?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lindgren20_interspeech.html": {
    "title": "Releasing a Toolkit and Comparing the Performance of Language Embeddings Across Various Spoken Language Identification Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/alvarez20_interspeech.html": {
    "title": "Learning Intonation Pattern Embeddings for Arabic Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abdullah20_interspeech.html": {
    "title": "Cross-Domain Adaptation of Spoken Language Identification for Related Languages: The Curious Case of Slavic Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tits20_interspeech.html": {
    "title": "ICE-Talk: An Interface for a Controllable Expressive Talking Machine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20b_interspeech.html": {
    "title": "Kaldi-Web: An Installation-Free, On-Device Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kelly20_interspeech.html": {
    "title": "Soapbox Labs Verification Platform for Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kelly20b_interspeech.html": {
    "title": "SoapBox Labs Fluency Assessment Platform for Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kulebi20_interspeech.html": {
    "title": "CATOTRON — A Neural Text-to-Speech System in Catalan",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ramanarayanan20b_interspeech.html": {
    "title": "Toward Remote Patient Monitoring of Speech, Video, Cognitive and Respiratory Biomarkers Using Multimodal Dialog Technology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20b_interspeech.html": {
    "title": "VoiceID on the Fly: A Speaker Recognition System that Learns from Scratch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ren20_interspeech.html": {
    "title": "Enhancing Transferability of Black-Box Adversarial Attacks via Lifelong Learning for Speech Emotion Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/feng20_interspeech.html": {
    "title": "End-to-End Speech Emotion Recognition Combined with Acoustic-to-Word ASR Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/su20_interspeech.html": {
    "title": "Improving Speech Emotion Recognition Using Graph Attentive Bi-Directional Gated Recurrent Unit Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mallolragolta20_interspeech.html": {
    "title": "An Investigation of Cross-Cultural Semi-Supervised Learning for Continuous Affect Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sridhar20_interspeech.html": {
    "title": "Ensemble of Students Taught by Probabilistic Teachers to Improve Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/latif20_interspeech.html": {
    "title": "Augmenting Generative Adversarial Networks for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dissanayake20_interspeech.html": {
    "title": "Speech Emotion Recognition ‘in the Wild' Using an Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mao20_interspeech.html": {
    "title": "Emotion Profile Refinery for Speech Emotion Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yeh20_interspeech.html": {
    "title": "Speech Representation Learning for Emotion Recognition Using End-to-End ASR with Factorized Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20_interspeech.html": {
    "title": "Fast and Slow Acoustic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/moriya20_interspeech.html": {
    "title": "Self-Distillation for Improving CTC-Transformer-Based ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tuske20_interspeech.html": {
    "title": "Single Headed Attention Based Sequence-to-Sequence Model for State-of-the-Art Results on Switchboard",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20c_interspeech.html": {
    "title": "Improving Speech Recognition Using GAN-Based Speech Synthesis and Contrastive Unspoken Text Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shao20_interspeech.html": {
    "title": "PyChain: A Fully Parallelized PyTorch Implementation of LF-MMI for End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/an20_interspeech.html": {
    "title": "CAT: A CTC-CRF Based ASR Toolkit Bridging the Hybrid and the End-to-End Approaches Towards Data Efficiency and Low Latency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/inaguma20_interspeech.html": {
    "title": "CTC-Synchronous Training for Monotonic Attention Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/houston20_interspeech.html": {
    "title": "Continual Learning for Multi-Dialect Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/song20b_interspeech.html": {
    "title": "SpecSwap: A Simple Data Augmentation Method for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/stan20_interspeech.html": {
    "title": "RECOApy: Data Recording, Pre-Processing and Phonetic Transcription for End-to-End Speech-Based Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shangguan20_interspeech.html": {
    "title": "Analyzing the Quality and Stability of a Streaming End-to-End On-Device Speech Recognizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20c_interspeech.html": {
    "title": "Statistical Testing on ASR Performance via Blockwise Bootstrap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ramakrishna20_interspeech.html": {
    "title": "Sentence Level Estimation of Psycholinguistic Norms Using Joint Multidimensional Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20_interspeech.html": {
    "title": "Neural Zero-Inflated Quality Estimation Model for Automatic Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/woodward20_interspeech.html": {
    "title": "Confidence Measures in Encoder-Decoder Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ali20_interspeech.html": {
    "title": "Word Error Rate Estimation Without ASR Output: e-WER2",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ludusan20_interspeech.html": {
    "title": "An Evaluation of Manual and Semi-Automatic Laughter Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/martin20_interspeech.html": {
    "title": "Understanding Racial Disparities in Automatic Speech Recognition: The Case of Habitual \"be",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zellou20_interspeech.html": {
    "title": "Secondary Phonetic Cues in the Production of the Nasal Short-a System in California English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lorin20_interspeech.html": {
    "title": "Acoustic Properties of Strident Fricatives at the Edges: Implications for Consonant Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/luo20_interspeech.html": {
    "title": "Processes and Consequences of Co-Articulation in Mandarin V1N.(C2)V2 Context: Phonology and Phonetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yue20_interspeech.html": {
    "title": "Voicing Distinction of Obstruents in the Hangzhou Wu Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20f_interspeech.html": {
    "title": "The Phonology and Phonetics of Kaifeng Mandarin Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zellers20_interspeech.html": {
    "title": "Microprosodic Variability in Plosives in German and Austrian German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20b_interspeech.html": {
    "title": "Er-Suffixation in Southwestern Mandarin: An EMA and Ultrasound Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20g_interspeech.html": {
    "title": "Electroglottographic-Phonetic Study on Korean Phonation Induced by Tripartite Plosives in Yanbian Korean",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wilkins20_interspeech.html": {
    "title": "Modeling Global Body Configurations in American Sign Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20h_interspeech.html": {
    "title": "Augmenting Turn-Taking Prediction with Wearable Eye Activity During Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20_interspeech.html": {
    "title": "CAM: Uninteresting Speech Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/caseiro20_interspeech.html": {
    "title": "Mixed Case Contextual ASR Using Capitalization Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mao20b_interspeech.html": {
    "title": "Speech Recognition and Multi-Speaker Diarization of Long Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/geng20_interspeech.html": {
    "title": "Investigation of Data Augmentation Techniques for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wei20_interspeech.html": {
    "title": "A Real-Time Robot-Based Auxiliary System for Risk Evaluation of COVID-19 Infection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/barbera20_interspeech.html": {
    "title": "An Utterance Verification System for Word Naming Therapy in Aphasia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20d_interspeech.html": {
    "title": "Exploiting Cross-Domain Visual Feature Generation for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20c_interspeech.html": {
    "title": "Joint Prediction of Punctuation and Disfluency in Speech Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yi20_interspeech.html": {
    "title": "Focal Loss for Punctuation Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20d_interspeech.html": {
    "title": "Improving X-Vector and PLDA for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zeinali20_interspeech.html": {
    "title": "SdSV Challenge 2020: Large-Scale Evaluation of Short-Duration Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jiang20_interspeech.html": {
    "title": "The XMUSPEECH System for Short-Duration Speaker Verification Challenge 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mun20_interspeech.html": {
    "title": "Robust Text-Dependent Speaker Verification via Character-Level Information Preservation for the SdSV Challenge 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/alumae20_interspeech.html": {
    "title": "The TalTech Systems for the Short-Duration Speaker Verification Challenge 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shen20b_interspeech.html": {
    "title": "Investigation of NICT Submission for Short-Duration Speaker Verification Challenge 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/thienpondt20_interspeech.html": {
    "title": "Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype Mining and Language-Dependent Score Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lozanodiez20_interspeech.html": {
    "title": "BUT Text-Dependent Speaker Verification System for SdSV Challenge 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ravi20_interspeech.html": {
    "title": "Exploring the Use of an Unsupervised Autoregressive Model as a Shared Encoder for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20d_interspeech.html": {
    "title": "Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20_interspeech.html": {
    "title": "Improving the Speaker Identity of Non-Parallel Many-to-Many Voice Conversion with Adversarial Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20i_interspeech.html": {
    "title": "Non-Parallel Many-to-Many Voice Conversion with PSR-StarGAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/polyak20_interspeech.html": {
    "title": "TTS Skins: Speaker Conversion via ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20e_interspeech.html": {
    "title": "GAZEV: GAN-Based Zero-Shot Voice Conversion Over Non-Parallel Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20g_interspeech.html": {
    "title": "Spoken Content and Voice Factorization for Few-Shot Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/polyak20b_interspeech.html": {
    "title": "Unsupervised Cross-Domain Singing Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ishihara20_interspeech.html": {
    "title": "Attention-Based Speaker Embeddings for One-Shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cong20_interspeech.html": {
    "title": "Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hong20_interspeech.html": {
    "title": "Gated Multi-Head Attention Pooling for Weakly Labelled Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20h_interspeech.html": {
    "title": "Environmental Sound Classification with Parallel Temporal-Spectral Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20i_interspeech.html": {
    "title": "Contrastive Predictive Coding of Audio with an Adversary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pankajakshan20_interspeech.html": {
    "title": "Memory Controlled Sequential Self Attention for Sound Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20_interspeech.html": {
    "title": "Dual Stage Learning Based Dynamic Time-Frequency Mask Generation for Audio Event Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zheng20_interspeech.html": {
    "title": "An Effective Perturbation Based Semi-Supervised Learning Method for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kao20_interspeech.html": {
    "title": "A Joint Framework for Audio Tagging and Weakly Supervised Acoustic Event Detection Using DenseNet with Global Average Pooling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chang20_interspeech.html": {
    "title": "Intra-Utterance Similarity Preserving Knowledge Distillation for Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20b_interspeech.html": {
    "title": "Two-Stage Polyphonic Sound Event Detection Based on Faster R-CNN-LSTM with Multi-Token Connectionist Temporal Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jindal20_interspeech.html": {
    "title": "SpeechMix — Augmenting Deep Sound Recognition Using Hidden Space Interpolations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/radfar20_interspeech.html": {
    "title": "End-to-End Neural Transformer Based Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20e_interspeech.html": {
    "title": "Jointly Encoding Word Confusion Network and Dialogue Context with BERT for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rao20_interspeech.html": {
    "title": "Speech to Semantics: Improve ASR and NLU Jointly via All-Neural Interfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/denisov20_interspeech.html": {
    "title": "Pretrained Semantic Speech Embeddings for End-to-End Spoken Language Understanding via Cross-Modal Teacher-Student Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chetupalli20_interspeech.html": {
    "title": "Context Dependent RNNLM for Automatic Transcription of Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tian20b_interspeech.html": {
    "title": "Improving End-to-End Speech-to-Intent Classification with Reptile",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cho20_interspeech.html": {
    "title": "Speech to Text Adaptation: Towards an Efficient Cross-Modal Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ruan20_interspeech.html": {
    "title": "Towards an ASR Error Robust Spoken Language Understanding System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kuo20_interspeech.html": {
    "title": "End-to-End Spoken Language Understanding Without Full Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gopalakrishnan20_interspeech.html": {
    "title": "Are Neural Open-Domain Dialog Systems Robust to Speech Recognition Errors in the Dialog History? An Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20b_interspeech.html": {
    "title": "AutoSpeech: Neural Architecture Search for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20b_interspeech.html": {
    "title": "Densely Connected Time Delay Neural Network for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zheng20b_interspeech.html": {
    "title": "Phonetically-Aware Coupled Network For Short Duration Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jung20_interspeech.html": {
    "title": "Multi-Task Network for Noise-Robust Keyword Spotting and Speaker Verification Using CTC-Based Soft VAD and Global Query Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20b_interspeech.html": {
    "title": "Vector-Based Attentive Pooling for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/safari20_interspeech.html": {
    "title": "Self-Attention Encoding and Pooling for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20f_interspeech.html": {
    "title": "ARET: Aggregated Residual Extended Time-Delay Neural Networks for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20g_interspeech.html": {
    "title": "Adversarial Separation Network for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20j_interspeech.html": {
    "title": "Text-Independent Speaker Verification with Dual Attention Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qu20_interspeech.html": {
    "title": "Evolutionary Algorithm Enhanced Neural Architecture Search for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/weng20_interspeech.html": {
    "title": "Minimum Bayes Risk Training of RNN-Transducer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20j_interspeech.html": {
    "title": "Semantic Mask for Transformer Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20h_interspeech.html": {
    "title": "Faster, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dimitriadis20_interspeech.html": {
    "title": "A Federated Approach in Training Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sheikh20_interspeech.html": {
    "title": "On Semi-Supervised LF-MMI Training of Acoustic Models with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20b_interspeech.html": {
    "title": "On Front-End Gain Invariant Modeling for Wake Word Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20c_interspeech.html": {
    "title": "Unsupervised Regularization-Based Adaptive Training for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/loweimi20_interspeech.html": {
    "title": "On the Robustness and Training Dynamics of Raw Waveform Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20b_interspeech.html": {
    "title": "Iterative Pseudo-Labeling for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kawamura20_interspeech.html": {
    "title": "Smart Tube: A Biofeedback System for Vocal Training and Therapy Through Tube Phonation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/choi20_interspeech.html": {
    "title": "VCTUBE : A Library for Automatic Speech Data Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xie20_interspeech.html": {
    "title": "A Mandarin L2 Learning APP with Mispronunciation Detection and Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/udayakumar20_interspeech.html": {
    "title": "Rapid Enhancement of NLP Systems by Acquisition of Data in Correlated Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20_interspeech.html": {
    "title": "Computer-Assisted Language Learning System: Automatic Speech Evaluation for Children Learning Malay and Tamil",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/saeki20_interspeech.html": {
    "title": "Real-Time, Full-Band, Online DNN-Based Voice Conversion System Using a Single CPU",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/feng20b_interspeech.html": {
    "title": "A Dynamic 3D Pronunciation Teaching Model Based on Pronunciation Attributes and Anatomy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kimura20_interspeech.html": {
    "title": "End-to-End Deep Learning Speech Recognition Model for Silent Speech Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20k_interspeech.html": {
    "title": "Autosegmental Neural Nets: Should Phones and Tones be Synchronous or Asynchronous?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tachbelie20_interspeech.html": {
    "title": "Development of Multilingual ASR Using GlobalPhone for Less-Resourced Languages: The Case of Ethiopian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hou20_interspeech.html": {
    "title": "Large-Scale End-to-End Multilingual Speech Recognition and Language Identification with Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20b_interspeech.html": {
    "title": "Multi-Encoder-Decoder Transformer for Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abate20_interspeech.html": {
    "title": "Multilingual Acoustic and Language Modeling for Ethio-Semitic Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20c_interspeech.html": {
    "title": "Multilingual Jointly Trained Acoustic and Written Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20l_interspeech.html": {
    "title": "Improving Code-Switching Language Modeling with Artificially Generated Texts Using Cycle-Consistent Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20d_interspeech.html": {
    "title": "Data Augmentation for Code-Switch Language Modeling by Fusing Multiple Text Generation Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20m_interspeech.html": {
    "title": "A 43 Language Multilingual Punctuation Prediction Neural Network Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20k_interspeech.html": {
    "title": "Exploring Lexicon-Free Modeling Units for End-to-End Korean and Korean-English Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/platen20_interspeech.html": {
    "title": "Multi-Task Siamese Neural Network for Improving Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/akimoto20_interspeech.html": {
    "title": "POCO: A Voice Spoofing and Liveness Detection Corpus Based on Pop Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20l_interspeech.html": {
    "title": "Dual-Adversarial Domain Adaptation for Generalized Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shim20_interspeech.html": {
    "title": "Self-Supervised Pre-Training with Acoustic Configurations for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/g20_interspeech.html": {
    "title": "Competency Evaluation in Voice Mimicking Using Acoustic Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20c_interspeech.html": {
    "title": "Light Convolutional Neural Network with Feature Genuinization for Detection of Synthetic Speech Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tak20_interspeech.html": {
    "title": "Spoofing Attack Detection Using the Non-Linear Fusion of Sub-Band Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/parasu20_interspeech.html": {
    "title": "Investigating Light-ResNet Architecture for Spoofing Detection Under Mismatched Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lei20_interspeech.html": {
    "title": "Siamese Convolutional Neural Network Using Gaussian Probability Feature for Spoofing Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/schroter20_interspeech.html": {
    "title": "Lightweight Online Noise Reduction on Embedded Devices Using Hierarchical Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tagliasacchi20_interspeech.html": {
    "title": "SEANet: A Multi-Modal Speech Enhancement Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chuang20_interspeech.html": {
    "title": "Lite Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bergler20_interspeech.html": {
    "title": "ORCA-CLEAN: A Deep Denoising Toolkit for Killer Whale Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20i_interspeech.html": {
    "title": "A Deep Learning Approach to Active Noise Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dinh20_interspeech.html": {
    "title": "Improving Speech Intelligibility Through Speaker Dependent and Independent Spectral Style Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pedersen20_interspeech.html": {
    "title": "End-to-End Speech Intelligibility Prediction Using Time-Domain Fully Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/arai20_interspeech.html": {
    "title": "Predicting Intelligibility of Enhanced Speech Using Posteriors Derived from DNN-Based ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abavisani20_interspeech.html": {
    "title": "Automatic Estimation of Intelligibility Measure for Consonants in Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/trinh20_interspeech.html": {
    "title": "Large Scale Evaluation of Importance Maps in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20n_interspeech.html": {
    "title": "Neural Architecture Search on Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jung20b_interspeech.html": {
    "title": "Acoustic Scene Classification Using Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20j_interspeech.html": {
    "title": "ATReSN-Net: Capturing Attentive Temporal Relations in Semantic Neighborhood for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharma20_interspeech.html": {
    "title": "Environment Sound Classification Using Multiple Feature Channels and Attention Based Deep Convolutional Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20m_interspeech.html": {
    "title": "Acoustic Scene Analysis with Multi-Head Attention Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20e_interspeech.html": {
    "title": "Relational Teacher Student Learning with Neural Label Embedding for Device Adaptation in Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20f_interspeech.html": {
    "title": "An Acoustic Segment Model Based Segment Unit Selection Approach to Acoustic Scene Classification with Partial Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/devalraju20_interspeech.html": {
    "title": "Attention-Driven Projections for Soundscape Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tzirakis20_interspeech.html": {
    "title": "Computer Audition for Continuous Rainforest Occupancy Monitoring: The Case of Bornean Gibbons' Call Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kwiatkowska20_interspeech.html": {
    "title": "Deep Learning Based Open Set Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/angelini20_interspeech.html": {
    "title": "Singing Synthesis: With a Little Help from my Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20d_interspeech.html": {
    "title": "Peking Opera Synthesis via Duration Informed Attention Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20k_interspeech.html": {
    "title": "DurIAN-SC: Duration Informed Attention Network Based Singing Voice Conversion System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hou20b_interspeech.html": {
    "title": "Transfer Learning for Improving Singing-Voice Detection in Polyphonic Instrumental Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20f_interspeech.html": {
    "title": "Channel-Wise Subband Input for Better Voice and Accompaniment Separation on High Resolution Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sadhu20_interspeech.html": {
    "title": "Continual Learning in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wan20_interspeech.html": {
    "title": "Speaker Adaptive Training for Speech Recognition Based on Attention-Over-Attention Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20c_interspeech.html": {
    "title": "Rapid RNN-T Adaptation Using Personalized Speech Synthesis and Neural Language Generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20b_interspeech.html": {
    "title": "Speech Transformer with Speaker Aware Persistent Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20d_interspeech.html": {
    "title": "Adaptive Speaker Normalization for CTC-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mathur20_interspeech.html": {
    "title": "Unsupervised Domain Adaptation Under Label Space Mismatch for Speech Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/winata20_interspeech.html": {
    "title": "Learning Fast Adaptation on Cross-Accented Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/khandelwal20_interspeech.html": {
    "title": "Black-Box Adaptation of ASR for Accented Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/turan20_interspeech.html": {
    "title": "Achieving Multi-Accent ASR via Unsupervised Acoustic Model Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/takeda20_interspeech.html": {
    "title": "Frame-Wise Online Unsupervised Adaptation of DNN-HMM Acoustic Model from Perspective of Robust Adaptive Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20e_interspeech.html": {
    "title": "Adversarially Trained Multi-Singer Sequence-to-Sequence Singing Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20b_interspeech.html": {
    "title": "Prediction of Head Motion from Speech Waveforms with a Canonical-Correlation-Constrained Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20c_interspeech.html": {
    "title": "XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yadav20_interspeech.html": {
    "title": "Stochastic Talking Face Generation Using Latent Distribution Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20f_interspeech.html": {
    "title": "Speech-to-Singing Conversion Based on Boundary Equilibrium GAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/goto20_interspeech.html": {
    "title": "Face2Speech: Towards Multi-Speaker Text-to-Speech Synthesis Using an Embedding Vector Predicted from a Face Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20n_interspeech.html": {
    "title": "Speech Driven Talking Head Generation via Attentional Landmarks Based Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/schadler20_interspeech.html": {
    "title": "Optimization and Evaluation of an Intelligibility-Improving Signal Processing Approach (IISPA) for the Hurricane Challenge 2.0 with FADE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20o_interspeech.html": {
    "title": "iMetricGAN: Intelligibility Enhancement for Speech-in-Noise Using Generative Adversarial Network-Based Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rennies20_interspeech.html": {
    "title": "Intelligibility-Enhancing Speech Modifications — The Hurricane Challenge 2.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/simantiraki20_interspeech.html": {
    "title": "Exploring Listeners' Speech Rate Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bederna20_interspeech.html": {
    "title": "Adaptive Compressive Onset-Enhancement for Improved Speech Intelligibility in Noise and Reverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chermaz20_interspeech.html": {
    "title": "A Sound Engineering Approach to Near End Listening Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/paul20b_interspeech.html": {
    "title": "Enhancing Speech Intelligibility in Text-To-Speech Synthesis Using Speaking Style Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/arai20b_interspeech.html": {
    "title": "Two Different Mechanisms of Movable Mandible for Vocal-Tract Model with Flexible Tongue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fang20_interspeech.html": {
    "title": "Improving the Performance of Acoustic-to-Articulatory Inversion by Removing the Training Loss of Noncritical Portions of Articulatory Channels Dynamically",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/illa20_interspeech.html": {
    "title": "Speaker Conditioned Acoustic-to-Articulatory Inversion Using x-Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20g_interspeech.html": {
    "title": "Coarticulation as Synchronised Sequential Target Approximation: An EMA Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/santos20_interspeech.html": {
    "title": "Improved Model for Vocal Folds with a Polyp with Potential Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20l_interspeech.html": {
    "title": "Regional Resonance of the Lower Vocal Tract and its Contribution to Speaker Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mannem20_interspeech.html": {
    "title": "Air-Tissue Boundary Segmentation in Real Time Magnetic Resonance Imaging Video Using 3-D Convolutional Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/purohit20_interspeech.html": {
    "title": "An Investigation of the Virtual Lip Trajectories During the Production of Bilabial Stops and Nasal at Different Speaking Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ge20_interspeech.html": {
    "title": "SpEx+: A Complete Time Domain Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20p_interspeech.html": {
    "title": "Atss-Net: Target Speaker Separation via Attention-Based Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qu20b_interspeech.html": {
    "title": "Multimodal Target Speech Separation with Voice and Face References",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20m_interspeech.html": {
    "title": "X-TaSNet: Robust and Accurate Time-Domain Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20q_interspeech.html": {
    "title": "Listen, Watch and Understand at the Cocktail Party: Audio-Visual-Contextual Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hao20_interspeech.html": {
    "title": "A Unified Framework for Low-Latency Speaker Extraction in Cocktail Party Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20c_interspeech.html": {
    "title": "Time-Domain Target-Speaker Speech Separation with Waveform-Based Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ochiai20_interspeech.html": {
    "title": "Listen to What You Want: Neural Network-Based Universal Sound Selector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yasuda20_interspeech.html": {
    "title": "Crossmodal Sound Retrieval Based on Specific Target Co-Occurrence Denoted with Weak Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20c_interspeech.html": {
    "title": "Speaker-Aware Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shinncunningham20_interspeech.html": {
    "title": "Brain networks enabling speech perception in everyday settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20o_interspeech.html": {
    "title": "A DNN-HMM-DNN Hybrid Model for Discovering Word-Like Units from Spoken Captions and Image Regions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/elbayad20_interspeech.html": {
    "title": "Efficient Wait-k Models for Simultaneous Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nguyen20_interspeech.html": {
    "title": "Investigating Self-Supervised Pre-Training for End-to-End Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gaido20_interspeech.html": {
    "title": "Contextualized Translation of Automatically Segmented Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pino20_interspeech.html": {
    "title": "Self-Training for End-to-End Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/federico20_interspeech.html": {
    "title": "Evaluating and Optimizing Prosodic Alignment for Automatic Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ohishi20_interspeech.html": {
    "title": "Pair Expansion for Learning Multilingual Semantic Embeddings Using Disjoint Visually-Grounded Speech Audio Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20g_interspeech.html": {
    "title": "Self-Supervised Representations Improve End-to-End Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jung20c_interspeech.html": {
    "title": "Improved RawNet with Feature Map Scaling for Text-Independent Speaker Verification Using Raw Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jung20d_interspeech.html": {
    "title": "Improving Multi-Scale Aggregation Using Feature Pyramid Module for Robust Speaker Verification of Variable-Duration Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gu20_interspeech.html": {
    "title": "An Adaptive X-Vector Model for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/prieto20_interspeech.html": {
    "title": "Shouted Speech Compensation for Speaker Verification Robust to Vocal Effort Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nicolson20_interspeech.html": {
    "title": "Sum-Product Networks for Robust Automatic Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20b_interspeech.html": {
    "title": "Segment Aggregation for Short Utterances Speaker Verification Using Raw Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rozenberg20_interspeech.html": {
    "title": "Siamese X-Vector Reconstruction for Domain Adapted Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20b_interspeech.html": {
    "title": "Speaker Re-Identification with Speaker Dependent Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lavrentyeva20_interspeech.html": {
    "title": "Blind Speech Signal Quality Estimation for Speaker Verification Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20r_interspeech.html": {
    "title": "Investigating Robustness of Adversarial Samples Detection for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pal20_interspeech.html": {
    "title": "Modeling ASR Ambiguity for Neural Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20p_interspeech.html": {
    "title": "ASR Error Correction with Augmented Transformer for Entity Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jia20_interspeech.html": {
    "title": "Large-Scale Transfer Learning for Low-Resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gaspers20_interspeech.html": {
    "title": "Data Balancing for Boosting Performance of Low-Frequency Classes in Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20q_interspeech.html": {
    "title": "An Interactive Adversarial Reward Learning-Based Spoken Language Understanding System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cao20_interspeech.html": {
    "title": "Style Attuned Pre-Training and Parameter Efficient Fine-Tuning for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/orihashi20_interspeech.html": {
    "title": "Unsupervised Domain Adaptation for Dialogue Sequence Labeling Based on Hierarchical Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sar20_interspeech.html": {
    "title": "Deep F-Measure Maximization for End-to-End Speech Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/whang20_interspeech.html": {
    "title": "An Effective Domain Adaptive Post-Training Method for BERT in Response Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/caubriere20_interspeech.html": {
    "title": "Confidence Measure for Speech-to-Concept End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mcguire20_interspeech.html": {
    "title": "Attention to Indexical Information Improves Voice Recall",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ngoc20_interspeech.html": {
    "title": "Categorization of Whistled Consonants by French Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ngoc20b_interspeech.html": {
    "title": "Whistled Vowel Identification by French Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cordero20_interspeech.html": {
    "title": "F0 Slope and Mean: Cues to Speech Segmentation in French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/michelas20_interspeech.html": {
    "title": "Does French Listeners' Ability to Use Accentual Information at the Word Level Depend on the Ear of Presentation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20h_interspeech.html": {
    "title": "A Perceptual Study of the Five Level Tones in Hmu (Xinzhai Variety)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zeng20_interspeech.html": {
    "title": "Mandarin and English Adults' Cue-Weighting of Lexical Stress",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/feng20c_interspeech.html": {
    "title": "Age-Related Differences of Tone Perception in Mandarin-Speaking Seniors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zellou20b_interspeech.html": {
    "title": "Social and Functional Pressures in Vocal Alignment: Differences for Human and Voice-AI Interlocutors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kavaki20_interspeech.html": {
    "title": "Identifying Important Time-Frequency Locations in Continuous Speech Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/loweimi20b_interspeech.html": {
    "title": "Raw Sign and Magnitude Spectra for Multi-Head Acoustic Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/agrawal20_interspeech.html": {
    "title": "Robust Raw Waveform Speech Recognition Using Relevance Weighted Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/oglic20_interspeech.html": {
    "title": "A Deep 2D Convolutional Network for Waveform-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kurzinger20_interspeech.html": {
    "title": "Lightweight End-to-End Speech Recognition from Raw Audio Data Using Sinc-Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ghahramani20_interspeech.html": {
    "title": "An Alternative to MFCCs for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dutta20_interspeech.html": {
    "title": "Phase Based Spectro-Temporal Features for Building a Robust ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/joy20_interspeech.html": {
    "title": "Deep Scattering Power Spectrum Features for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/parcollet20_interspeech.html": {
    "title": "FusionRNN: Shared Neural Parameters for Multi-Channel Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20b_interspeech.html": {
    "title": "Bandpass Noise Generation and Augmentation for Unified ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/purushothaman20_interspeech.html": {
    "title": "Deep Learning Based Dereverberation of Temporal Envelopes for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tomashenko20_interspeech.html": {
    "title": "Introducing the VoicePrivacy Initiative",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nautsch20_interspeech.html": {
    "title": "The Privacy ZEBRA: Zero Evidence Biometric Recognition Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mawalim20_interspeech.html": {
    "title": "X-Vector Singular Value Modification and Statistical-Based Decomposition with Ensemble Regression Modeling for Speaker Anonymization System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/maouche20_interspeech.html": {
    "title": "A Comparative Study of Speech Anonymization Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/srivastava20_interspeech.html": {
    "title": "Design Choices for X-Vector Based Speaker Anonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/noe20_interspeech.html": {
    "title": "Speech Pseudonymisation Assessment Using Voice Similarity Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20c_interspeech.html": {
    "title": "g2pM: A Neural Grapheme-to-Phoneme Conversion Package for Mandarin Chinese Based on a New Open Benchmark Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20n_interspeech.html": {
    "title": "A Mask-Based Model for Mandarin Chinese Polyphone Disambiguation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cohn20_interspeech.html": {
    "title": "Perception of Concatenative vs. Neural Text-To-Speech (TTS): Differences in Intelligibility in Noise and Language Attitudes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/taylor20_interspeech.html": {
    "title": "Enhancing Sequence-to-Sequence Text-to-Speech with Morphology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/choi20b_interspeech.html": {
    "title": "Deep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mittag20_interspeech.html": {
    "title": "Deep Learning Based Assessment of Synthetic Speech Naturalness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20o_interspeech.html": {
    "title": "Distant Supervision for Polyphone Disambiguation in Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gallegos20_interspeech.html": {
    "title": "An Unsupervised Method to Select a Speaker Subset from Large Multi-Speaker Speech Synthesis Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/das20_interspeech.html": {
    "title": "Understanding the Effect of Voice Quality and Accent on Talker Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20c_interspeech.html": {
    "title": "Robust Beam Search for Encoder-Decoder Attention Based Speech Recognition Without Length Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20e_interspeech.html": {
    "title": "Transformer with Bidirectional Decoder for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20r_interspeech.html": {
    "title": "An Investigation of Phone-Based Subword Units for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wong20_interspeech.html": {
    "title": "Combination of End-to-End and Hybrid Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20c_interspeech.html": {
    "title": "Evolved Speech-Transformer: Applying Neural Architecture Search to End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/garg20_interspeech.html": {
    "title": "Hierarchical Multi-Stage Word-to-Grapheme Named Entity Corrector for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/beck20_interspeech.html": {
    "title": "LVCSR with Transformer Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20f_interspeech.html": {
    "title": "DARTS-ASR: Differentiable Architecture Search for Multilingual Speech Recognition and Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/stappen20_interspeech.html": {
    "title": "Uncertainty-Aware Machine Support for Paper Reviewing on the Interspeech 2019 Submission Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cohn20b_interspeech.html": {
    "title": "Individual Variation in Language Attitudes Toward Voice-AI: The Role of Listeners' Autistic-Like Traits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cohn20c_interspeech.html": {
    "title": "Differences in Gradient Emotion Perception: Human vs. Alexa Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/martinezlucas20_interspeech.html": {
    "title": "The MSP-Conversation Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tao20_interspeech.html": {
    "title": "Spotting the Traces of Depression in Read Speech: An Approach Based on Computational Paralinguistics and Social Signal Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20d_interspeech.html": {
    "title": "Speech Sentiment and Customer Satisfaction Estimation in Socialbot Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lepp20_interspeech.html": {
    "title": "Pardon the Interruption: An Analysis of Gender and Turn-Taking in U.S. Supreme Court Oral Arguments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/neitsch20_interspeech.html": {
    "title": "Are Germans Better Haters Than Danes? Language-Specific Implicit Prosodies of Types of Hate Speech and How They Relate to Perceived Severity and Societal Rules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20g_interspeech.html": {
    "title": "An Objective Voice Gender Scoring System and Identification of the Salient Acoustic Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jayawardena20_interspeech.html": {
    "title": "How Ordinal Are Your Data?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hughes20_interspeech.html": {
    "title": "Correlating Cepstra with Formant Frequencies: Implications for Phonetically-Informed Forensic Voice Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/neitsch20b_interspeech.html": {
    "title": "Prosody and Breathing: A Comparison Between Rhetorical and Information-Seeking Questions in German and Brazilian Portuguese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/defina20_interspeech.html": {
    "title": "Scaling Processes of Clause Chains in Pitjantjatjara",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mizoguchi20_interspeech.html": {
    "title": "Neutralization of Voicing Distinction of Stops in Tohoku Dialects of Japanese: Field Work and Acoustic Measurements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20b_interspeech.html": {
    "title": "Correlation Between Prosody and Pragmatics: Case Study of Discourse Markers in French and English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zarka20_interspeech.html": {
    "title": "An Analysis of Prosodic Prominence Cues to Information Structure in Egyptian Arabic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mumtaz20_interspeech.html": {
    "title": "Lexical Stress in Urdu",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/riad20_interspeech.html": {
    "title": "Vocal Markers from Sustained Phonation in Huntington's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dentel20_interspeech.html": {
    "title": "How Rhythm and Timbre Encode Mooré Language in Bendré Drummed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20_interspeech.html": {
    "title": "Doing Something we Never could with Spoken Language Technologies-from early days to the era of deep learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lalhminghlui20_interspeech.html": {
    "title": "Interaction of Tone and Voicing in Mizo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20h_interspeech.html": {
    "title": "Mandarin Lexical Tones: A Corpus-Based Study of Word Length, Syllable Position and Prosodic Position on Duration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20c_interspeech.html": {
    "title": "An Investigation of the Target Approximation Model for Tone Modeling and Recognition in Continuous Mandarin Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lai20_interspeech.html": {
    "title": "Integrating the Application and Realization of Mandarin 3rd Tone Sandhi in the Resolution of Sentence Ambiguity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20p_interspeech.html": {
    "title": "Neutral Tone in Changde Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cui20_interspeech.html": {
    "title": "Pitch Declination and Final Lowering in Northeastern Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rose20_interspeech.html": {
    "title": "Variation in Spectral Slope and Interharmonic Noise in Cantonese Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tang20_interspeech.html": {
    "title": "The Acoustic Realization of Mandarin Tones in Fast Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/loukina20_interspeech.html": {
    "title": "Do Face Masks Introduce Bias in Speech Technologies? The Case of Automated Scoring of Speaking Proficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mhiri20_interspeech.html": {
    "title": "A Low Latency ASR-Free End to End Spoken Language Understanding System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20s_interspeech.html": {
    "title": "An Audio-Based Wakeword-Independent Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/vuong20_interspeech.html": {
    "title": "Learnable Spectro-Temporal Receptive Fields for Robust Voice Type Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chang20b_interspeech.html": {
    "title": "Low Latency Speech Recognition Using End-to-End Prefetching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20t_interspeech.html": {
    "title": "AutoSpeech 2020: The Second Automated Machine Learning Challenge for Speech Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20c_interspeech.html": {
    "title": "Building a Robust Word-Level Wakeword Verification Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/koizumi20_interspeech.html": {
    "title": "A Transformer-Based Audio Captioning Model with Keyword Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mo20_interspeech.html": {
    "title": "Neural Architecture Search for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20s_interspeech.html": {
    "title": "Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20u_interspeech.html": {
    "title": "Using Cyclic Noise as the Source Signal for Neural Source-Filter-Based Speech Waveform Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20i_interspeech.html": {
    "title": "Unconditional Audio Generation with Generative Adversarial Networks and Cycle Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nakashika20_interspeech.html": {
    "title": "Complex-Valued Variational Autoencoder: A Novel Deep Generative Model for Direct Representation of Complex Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/choi20c_interspeech.html": {
    "title": "Attentron: Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ihm20_interspeech.html": {
    "title": "Reformer-TTS: Neural Speech Synthesis with Reformer Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kaneko20_interspeech.html": {
    "title": "CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-Spectrogram Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ellinas20_interspeech.html": {
    "title": "High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20c_interspeech.html": {
    "title": "DurIAN: Duration Informed Attention Network for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mitsui20_interspeech.html": {
    "title": "Multi-Speaker Text-to-Speech Synthesis Using Deep Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/m20_interspeech.html": {
    "title": "A Hybrid HMM-Waveglow Based Text-to-Speech Synthesizer Using Histogram Equalization for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/schuller20_interspeech.html": {
    "title": "The INTERSPEECH 2020 Computational Paralinguistics Challenge: Elderly Emotion, Breathing & Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/koike20_interspeech.html": {
    "title": "Learning Higher Representations from Pre-Trained Deep Models with Data Augmentation for the COMPARE 2020 Challenge Mask Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/illium20_interspeech.html": {
    "title": "Surgical Mask Detection with Convolutional Neural Networks and Data Augmentations on Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/klumpp20_interspeech.html": {
    "title": "Surgical Mask Detection with Deep Recurrent Phonetic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/montacie20_interspeech.html": {
    "title": "Phonetic, Frame Clustering and Intelligibility Analyses for the INTERSPEECH 2020 ComParE Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/juliao20_interspeech.html": {
    "title": "Exploring Text and Audio Embeddings for Multi-Dimension Elderly Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/markitantov20_interspeech.html": {
    "title": "Ensembling End-to-End Deep Models for Computational Paralinguistics Tasks: ComParE 2020 Mask and Breathing Sub-Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mendonca20_interspeech.html": {
    "title": "Analyzing Breath Signals for the Interspeech 2020 ComParE Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/macintyre20_interspeech.html": {
    "title": "Deep Attentive End-to-End Continuous Breath Sensing from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/szep20_interspeech.html": {
    "title": "Paralinguistic Classification of Mask Wearing by Image Classifiers and Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20c_interspeech.html": {
    "title": "Exploration of Acoustic and Lexical Cues for the INTERSPEECH 2020 Computational Paralinguistic Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sogancoglu20_interspeech.html": {
    "title": "Is Everything Fine, Grandma? Acoustic and Linguistic Modeling for Robust Elderly Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ristea20_interspeech.html": {
    "title": "Are you Wearing a Mask? Improving Mask Detection from Speech Using Augmentation by Cycle-Consistent GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20d_interspeech.html": {
    "title": "1-D Row-Convolution LSTM: Fast Streaming ASR at Accuracy Parity with LC-BLSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20v_interspeech.html": {
    "title": "Low Latency End-to-End Streaming Speech Recognition with a Scout Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kurata20_interspeech.html": {
    "title": "Knowledge Distillation from Offline to Streaming RNN Transducer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20t_interspeech.html": {
    "title": "Parallel Rescoring with Transformer for Streaming On-Device Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/baqueroarnal20_interspeech.html": {
    "title": "Improved Hybrid Streaming ASR with Transformer Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20i_interspeech.html": {
    "title": "Streaming Transformer-Based Acoustic Models Using Self-Attention with Augmented Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/inaguma20b_interspeech.html": {
    "title": "Enhancing Monotonic Multihead Attention for Streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20q_interspeech.html": {
    "title": "Streaming Chunk-Aware Multihead Attention for Online End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nguyen20b_interspeech.html": {
    "title": "High Performance Sequence-to-Sequence Model for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/joshi20_interspeech.html": {
    "title": "Transfer Learning Approaches for Streaming End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/martinc20_interspeech.html": {
    "title": "Tackling the ADReSS Challenge: A Multimodal Approach to the Automated Recognition of Alzheimer's Dementia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yuan20_interspeech.html": {
    "title": "Disfluencies and Fine-Tuning Pre-Trained Language Models for Detection of Alzheimer's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/balagopalan20_interspeech.html": {
    "title": "To BERT or not to BERT: Comparing Speech and Language-Based Approaches for Alzheimer's Disease Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/luz20_interspeech.html": {
    "title": "Alzheimer's Dementia Recognition Through Spontaneous Speech: The ADReSS Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pappagari20_interspeech.html": {
    "title": "Using State of the Art Speaker Recognition and Natural Language Processing Technologies to Detect Alzheimer's Disease and Assess its Severity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cummins20_interspeech.html": {
    "title": "A Comparison of Acoustic and Linguistics Methodologies for Alzheimer's Dementia Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rohanian20_interspeech.html": {
    "title": "Multi-Modal Fusion with Gating Using Audio, Lexical and Disfluency Features for Alzheimer's Dementia Recognition from Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/searle20_interspeech.html": {
    "title": "Comparing Natural Language Processing Techniques for Alzheimer's Dementia Prediction in Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/edwards20_interspeech.html": {
    "title": "Multiscale System for Alzheimer's Dementia Recognition Through Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pompili20_interspeech.html": {
    "title": "The INESC-ID Multi-Modal System for the ADReSS 2020 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/farzana20_interspeech.html": {
    "title": "Exploring MMSE Score Prediction Using Verbal and Non-Verbal Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sarawgi20_interspeech.html": {
    "title": "Multimodal Inductive Transfer Learning for Detection of Alzheimer's Dementia and its Severity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/koo20_interspeech.html": {
    "title": "Exploiting Multi-Modal Features from Pre-Trained Networks for Alzheimer's Dementia Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/syed20_interspeech.html": {
    "title": "Automated Screening for Alzheimer's Dementia Through Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20c_interspeech.html": {
    "title": "NEC-TT Speaker Verification System for SRE'19 CTS Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20u_interspeech.html": {
    "title": "THUEE System for NIST SRE19 CTS Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/antipov20_interspeech.html": {
    "title": "Automatic Quality Assessment for Audio-Visual Verification Systems. The LOVe Submission to NIST SRE Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tao20b_interspeech.html": {
    "title": "Audio-Visual Speaker Recognition with a Cross-Modal Discriminative Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shon20_interspeech.html": {
    "title": "Multimodal Association for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20h_interspeech.html": {
    "title": "Multi-Modality Matters: A Performance Leap on VoxCeleb",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20w_interspeech.html": {
    "title": "Cross-Domain Adaptation with Discrepancy Minimization for Text-Independent Forensic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sang20_interspeech.html": {
    "title": "Open-Set Short Utterance Forensic Speaker Verification Using Teacher-Student Network with Explicit Inductive Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chowdhury20b_interspeech.html": {
    "title": "JukeBox: A Multilingual Singer Recognition Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20v_interspeech.html": {
    "title": "Speaker Identification for Household Scenarios with Self-Attention and Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rybakov20_interspeech.html": {
    "title": "Streaming Keyword Spotting on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20j_interspeech.html": {
    "title": "Metadata-Aware End-to-End Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kong20_interspeech.html": {
    "title": "Adversarial Audio: A New Information Hiding Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20x_interspeech.html": {
    "title": "S2IGAN: Speech-to-Image Generation via Adversarial Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zuluagagomez20_interspeech.html": {
    "title": "Automatic Speech Recognition Benchmark for Air-Traffic Communications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gudepu20_interspeech.html": {
    "title": "Whisper Augmented End-to-End/Hybrid Speech Recognition System — CycleGAN Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sawhney20_interspeech.html": {
    "title": "Risk Forecasting from Earnings Calls Acoustics and Network Correlations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20i_interspeech.html": {
    "title": "SpecMark: A Spectral Watermarking Framework for IP Protection of Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hout20_interspeech.html": {
    "title": "Evaluating Automatically Generated Phoneme Captions for Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20d_interspeech.html": {
    "title": "An Efficient Temporal Modeling Approach for Speech Emotion Recognition by Mapping Varied Duration Sentences into Fixed Number of Chunks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/latif20b_interspeech.html": {
    "title": "Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks, and Cross-Corpus Setting for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fujioka20_interspeech.html": {
    "title": "Meta-Learning for Speech Emotion Recognition Considering Ambiguity of Emotion Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20k_interspeech.html": {
    "title": "Temporal Attention Convolutional Network for Speech Emotion Recognition with Latent Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhu20_interspeech.html": {
    "title": "Reconciliation of Multiple Corpora for Speech Emotion Recognition by Multiple Classifiers with an Adversarial Corpus Discriminator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lian20c_interspeech.html": {
    "title": "Conversational Emotion Recognition Using Self-Attention Mechanisms and Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mao20c_interspeech.html": {
    "title": "EigenEmo: Spectral Utterance Representation Using Dynamic Mode Decomposition for Speech Emotion Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mao20d_interspeech.html": {
    "title": "Advancing Multiple Instance Learning with Attention Modeling for Categorical Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/perezramon20_interspeech.html": {
    "title": "The Effect of Language Proficiency on the Perception of Segmental Foreign Accent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20l_interspeech.html": {
    "title": "The Effect of Language Dominance on the Selective Attention of Segments and Tones in Urdu-Cantonese Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20w_interspeech.html": {
    "title": "The Effect of Input on the Production of English Tense and Lax Vowels by Chinese Learners: Evidence from an Elementary School in China",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/spinu20_interspeech.html": {
    "title": "Exploring the Use of an Artificial Accent of English to Assess Phonetic Learning in Monolingual and Bilingual Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chowdhury20c_interspeech.html": {
    "title": "Effects of Dialectal Code-Switching on Speech Modules: A Study Using Egyptian Arabic Broadcast Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/johnson20_interspeech.html": {
    "title": "Bilingual Acoustic Voice Variation is Similarly Structured Across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20r_interspeech.html": {
    "title": "Monolingual Data Selection Analysis for English-Mandarin Hybrid Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/du20b_interspeech.html": {
    "title": "Perception and Production of Mandarin Initial Stops by Native Urdu Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/afouras20_interspeech.html": {
    "title": "Now You're Speaking My Language: Visual Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rhee20_interspeech.html": {
    "title": "The Different Enhancement Roles of Covarying Cues in Thai and Mandarin Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20c_interspeech.html": {
    "title": "Singing Voice Extraction with Attention-Based Spectrograms Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20d_interspeech.html": {
    "title": "Incorporating Broad Phonetic Information for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20x_interspeech.html": {
    "title": "A Recursive Network with Dynamic Attention for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20d_interspeech.html": {
    "title": "Constrained Ratio Mask for Speech Enhancement Using DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20d_interspeech.html": {
    "title": "SERIL: Noise Adaptive Speech Enhancement Using Regularization-Based Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bando20_interspeech.html": {
    "title": "Adaptive Neural Speech Enhancement with a Denoising Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bulut20_interspeech.html": {
    "title": "Low-Latency Single Channel Speech Dereverberation Using U-Net Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tran20b_interspeech.html": {
    "title": "Single-Channel Speech Enhancement by Subspace Affinity Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20y_interspeech.html": {
    "title": "Noise Tokens: Learning Neural Noise Templates for Environment-Aware Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/deng20_interspeech.html": {
    "title": "NAAGN: Noise-Aware Attention-Gated Network for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20z_interspeech.html": {
    "title": "Online Monaural Speech Enhancement Using Delayed Subband LSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/strake20_interspeech.html": {
    "title": "INTERSPEECH 2020 Deep Noise Suppression Challenge: A Fully Convolutional Recurrent Network (FCRN) for Joint Dereverberation and Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20g_interspeech.html": {
    "title": "DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/westhausen20_interspeech.html": {
    "title": "Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/valin20_interspeech.html": {
    "title": "A Perceptually-Motivated Approach for Low-Complexity, Real-Time Enhancement of Fullband Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/isik20_interspeech.html": {
    "title": "PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings, Semi-Supervised Conversational Data, and Biased Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/reddy20_interspeech.html": {
    "title": "The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Testing Framework, and Challenge Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/akbarzadeh20_interspeech.html": {
    "title": "The Implication of Sound Level on Spatial Selective Auditory Attention for Cochlear Implant Users: Behavioral and Electrophysiological Measurement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wan20b_interspeech.html": {
    "title": "Enhancing the Interaural Time Difference of Bilateral Cochlear Implants with the Temporal Limits Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/irino20_interspeech.html": {
    "title": "Speech Clarity Improvement by Vocal Self-Training Using a Hearing Impairment Simulator and its Correlation with an Auditory Modulation Index",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20s_interspeech.html": {
    "title": "Investigation of Phase Distortion on Perceived Speech Quality for Hearing-Impaired Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20t_interspeech.html": {
    "title": "EEG-Based Short-Time Auditory Attention Detection Using Multi-Task Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abderrazek20_interspeech.html": {
    "title": "Towards Interpreting Deep Learning Models to Understand Loss of Speech Intelligibility in Speech Disorders — Step 1: CNN Model-Based Phone Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mirheidari20_interspeech.html": {
    "title": "Improving Cognitive Impairment Classification by Generative Neural Network-Based Feature Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/moore20_interspeech.html": {
    "title": "UncommonVoice: A Crowdsourced Dataset of Dysphonic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/barche20_interspeech.html": {
    "title": "Towards Automatic Assessment of Voice Disorders: A Clinical Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shivkumar20_interspeech.html": {
    "title": "BlaBla: Linguistic Feature Extraction for Clinical Analysis in Multiple Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20d_interspeech.html": {
    "title": "Depthwise Separable Convolutional ResNet with Squeeze-and-Excitation Blocks for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bluche20_interspeech.html": {
    "title": "Predicting Detection Filters for Small Footprint Open-Vocabulary Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ylmaz20_interspeech.html": {
    "title": "Deep Convolutional Spiking Neural Networks for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20j_interspeech.html": {
    "title": "Domain Aware Training for Far-Field Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20u_interspeech.html": {
    "title": "Re-Weighted Interval Loss for Handling Data Imbalance Problem of End-to-End Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20v_interspeech.html": {
    "title": "Deep Template Matching for Small-Footprint and Configurable Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20d_interspeech.html": {
    "title": "Multi-Scale Convolution for Robust Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20j_interspeech.html": {
    "title": "An Investigation of Few-Shot Learning in Spoken Term Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20d_interspeech.html": {
    "title": "End-to-End Keyword Search Based on Attention and Energy Scorer for Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/higuchi20_interspeech.html": {
    "title": "Stacked 1D Convolutional Networks for End-to-End Small Footprint Voice Trigger Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/heitkaemper20_interspeech.html": {
    "title": "Statistical and Neural Network Based Speech Activity Detection in Non-Stationary Acoustic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20w_interspeech.html": {
    "title": "Speaker Diarization System Based on DPCA Algorithm for Fearless Steps Challenge Phase-2",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20e_interspeech.html": {
    "title": "The DKU Speech Activity Detection and Speaker Identification Systems for Fearless Steps Challenge Phase-02",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gorin20_interspeech.html": {
    "title": "This is Houston. Say again, please\". The Behavox System for the Apollo-11 Fearless Steps Challenge (Phase II)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/joglekar20_interspeech.html": {
    "title": "FEARLESS STEPS Challenge (FS-2): Supervised Learning with Massive Naturalistic Apollo Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/luo20b_interspeech.html": {
    "title": "Separating Varying Numbers of Sources with Auxiliary Autoencoding Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20k_interspeech.html": {
    "title": "On Synthesis for Supervised Monaural Speech Separation in Time Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20y_interspeech.html": {
    "title": "Learning Better Speech Representations by Worsening Interference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pariente20_interspeech.html": {
    "title": "Asteroid: The PyTorch-Based Audio Source Separation Toolkit for Researchers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20l_interspeech.html": {
    "title": "Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/deng20b_interspeech.html": {
    "title": "Conv-TasSAN: Separative Adversarial Network Based on Conv-TasNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kinoshita20_interspeech.html": {
    "title": "Multi-Path RNN for Hierarchical Modeling of Long Sequential Data and its Application to Speaker Stream Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/narayanaswamy20_interspeech.html": {
    "title": "Unsupervised Audio Source Separation Using Generative Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qiu20b_interspeech.html": {
    "title": "Adversarial Latent Representation Learning for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xiang20_interspeech.html": {
    "title": "An NMF-HMM Speech Enhancement Method Based on Kullback-Leibler Divergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20x_interspeech.html": {
    "title": "Multi-Scale TCN: Exploring Better Temporal DNN Model for Causal Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20z_interspeech.html": {
    "title": "VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20d_interspeech.html": {
    "title": "Speech Separation Based on Multi-Stage Elaborated Dual-Path Deep BiLSTM with Auxiliary Identity Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hao20b_interspeech.html": {
    "title": "Sub-Band Knowledge Distillation Framework for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/roy20_interspeech.html": {
    "title": "A Deep Learning-Based Kalman Filter for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20e_interspeech.html": {
    "title": "Subband Kalman Filtering with DNN Estimated Parameters for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20aa_interspeech.html": {
    "title": "Bidirectional LSTM Network with Ordered Neurons for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20e_interspeech.html": {
    "title": "Speaker-Conditional Chain Model for Speech Separation and Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nortje20_interspeech.html": {
    "title": "Unsupervised vs. Transfer Learning for Multimodal One-Shot Matching of Speech and Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20e_interspeech.html": {
    "title": "Multimodal Speech Emotion Recognition Using Cross Attention with Aligned Audio and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/csapo20_interspeech.html": {
    "title": "Speaker Dependent Articulatory-to-Acoustic Mapping Using Real-Time MRI of the Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/csapo20b_interspeech.html": {
    "title": "Ultrasound-Based Articulatory-to-Acoustic Mapping with WaveGlow Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/feng20d_interspeech.html": {
    "title": "Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/matsuura20_interspeech.html": {
    "title": "Generative Adversarial Training Data Adaptation for Very Low-Resource Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tsunematsu20_interspeech.html": {
    "title": "Neural Speech Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/milde20_interspeech.html": {
    "title": "Improving Unsupervised Sparsespeech Acoustic Models with Categorical Reparameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/papadimitriou20_interspeech.html": {
    "title": "Multimodal Sign Language Recognition via Temporal Deformable Convolutional Sequence Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pratap20_interspeech.html": {
    "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/parmonangan20_interspeech.html": {
    "title": "Combining Audio and Brain Activity for Predicting Speech Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharon20_interspeech.html": {
    "title": "The \"Sound of Silence\" in EEG — Cognitive Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cai20_interspeech.html": {
    "title": "Low Latency Auditory Attention Detection with Common Spatial Pattern Analysis of EEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/angrick20_interspeech.html": {
    "title": "Speech Spectrogram Estimation from Intracranial Brain Activity Using a Quantization Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dash20_interspeech.html": {
    "title": "Neural Speech Decoding for Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20m_interspeech.html": {
    "title": "Semi-Supervised ASR by End-to-End Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tulsiani20_interspeech.html": {
    "title": "Improved Training Strategies for End-to-End Speech Recognition in Digital Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kanda20b_interspeech.html": {
    "title": "Serialized Output Training for End-to-End Overlapped Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/weninger20_interspeech.html": {
    "title": "Semi-Supervised Learning with Data Augmentation for End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/guo20_interspeech.html": {
    "title": "Efficient Minimum Word Error Rate Training of RNN-Transducer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zeyer20_interspeech.html": {
    "title": "A New Training Pipeline for an Improved Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20d_interspeech.html": {
    "title": "Improved Noisy Student Training for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/masumura20_interspeech.html": {
    "title": "Phoneme-to-Grapheme Conversion Based Large-Scale Pre-Training for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gowda20_interspeech.html": {
    "title": "Utterance Invariant Training for Hybrid Two-Pass End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20aa_interspeech.html": {
    "title": "SCADA: Stochastic, Consistent and Adversarial Data Augmentation to Improve ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/das20b_interspeech.html": {
    "title": "Fundamental Frequency Model for Postfiltering at Low Bitrates in a Transform-Domain Speech and Audio Codec",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/broucke20_interspeech.html": {
    "title": "Hearing-Impaired Bio-Inspired Cochlear Models for Real-Time Auditory Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/skoglund20_interspeech.html": {
    "title": "Improving Opus Low Bit Rate Quality with Neural Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/manocha20_interspeech.html": {
    "title": "A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/masztalski20_interspeech.html": {
    "title": "StoRIR: Stochastic Room Impulse Response Generation for Audio Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/naderi20_interspeech.html": {
    "title": "An Open Source Implementation of ITU-T Recommendation P.808 with Validation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mittag20b_interspeech.html": {
    "title": "DNN No-Reference PSTN Speech Quality Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/moller20_interspeech.html": {
    "title": "Non-Intrusive Diagnostic Monitoring of Fullband Speech Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shahrebabaki20_interspeech.html": {
    "title": "Transfer Learning of Articulatory Information Through Phone Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shahrebabaki20b_interspeech.html": {
    "title": "Sequence-to-Sequence Articulatory Inversion Through Time Convolution of Sub-Band Frequency Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gatto20_interspeech.html": {
    "title": "Discriminative Singular Spectrum Analysis for Bioacoustic Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mannem20b_interspeech.html": {
    "title": "Speech Rate Task-Specific Representation Learning from Acoustic-Articulatory Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hernandez20_interspeech.html": {
    "title": "Dysarthria Detection and Severity Assessment Using Rhythm-Based Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ma20_interspeech.html": {
    "title": "LungRN+NL: An Improved Adventitious Lung Sound Classification Using Non-Local Block ResNet Neural Network with Mixup Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/singh20b_interspeech.html": {
    "title": "Attention and Encoder-Decoder Based Models for Transforming Articulatory Movements at Different Speaking Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20e_interspeech.html": {
    "title": "Adventitious Respiratory Classification Using Attentive Residual Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lenain20_interspeech.html": {
    "title": "Surfboard: Audio Feature Extraction for Modern Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/naini20_interspeech.html": {
    "title": "Whisper Activity Detection Using CNN-LSTM Based Attention Pooling Network Trained for a Speaker Identification Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20e_interspeech.html": {
    "title": "Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20m_interspeech.html": {
    "title": "Multi-Lingual Multi-Speaker Text-to-Speech Synthesis for Voice Cloning with Online Speaker Enrollment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fu20b_interspeech.html": {
    "title": "Dynamic Soft Windowing and Language Dependent Style Token for Code-Switching End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/staib20_interspeech.html": {
    "title": "Phonological Features for 0-Shot Multilingual Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xin20_interspeech.html": {
    "title": "Cross-Lingual Text-To-Speech Synthesis via Domain Adaptation and Perceptual Similarity Regression in Speaker Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20n_interspeech.html": {
    "title": "Tone Learning in Low-Resource Bilingual TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bansal20_interspeech.html": {
    "title": "On Improving Code Mixed Speech Synthesis with Mixlingual Grapheme-to-Phoneme Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/prakash20_interspeech.html": {
    "title": "Generic Indic Text-to-Speech Synthesisers with Rapid Adaptation in an End-to-End Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/korte20_interspeech.html": {
    "title": "Efficient Neural Speech Synthesis for Low-Resource Languages Through Multilingual Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nekvinda20_interspeech.html": {
    "title": "One Model, Many Languages: Meta-Learning for Multilingual Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20b_interspeech.html": {
    "title": "In Defence of Metric Learning for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kye20_interspeech.html": {
    "title": "Meta-Learning for Short Utterance Speaker Recognition with Imbalance Length Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ba_interspeech.html": {
    "title": "Segment-Level Effects of Gender, Nationality and Emotion Information on Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20f_interspeech.html": {
    "title": "Weakly Supervised Training of Hierarchical Attention Networks for Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/montalvo20_interspeech.html": {
    "title": "Multi-Task Learning for Voice Related Recognition Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/khan20_interspeech.html": {
    "title": "Unsupervised Training of Siamese Networks for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20o_interspeech.html": {
    "title": "An Effective Speaker Recognition Method Based on Joint Identification and Verification Supervisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zheng20c_interspeech.html": {
    "title": "Speaker-Aware Linear Discriminant Analysis in Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20n_interspeech.html": {
    "title": "Adversarial Domain Adaptation for Speaker Verification Using Partially Shared Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20f_interspeech.html": {
    "title": "Automatic Scoring at Multi-Granularity for L2 Pronunciation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lo20b_interspeech.html": {
    "title": "An Effective End-to-End Modeling Approach for Mispronunciation Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yan20_interspeech.html": {
    "title": "An End-to-End Mispronunciation Detection System for L2 English Speech Leveraging Novel Anti-Phone Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/duan20_interspeech.html": {
    "title": "Unsupervised Feature Adaptation Using Adversarial Multi-Task Training for Automatic Evaluation of Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20f_interspeech.html": {
    "title": "Pronunciation Erroneous Tendency Detection with Language Adversarial Represent Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cheng20_interspeech.html": {
    "title": "ASR-Free Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kyriakopoulos20_interspeech.html": {
    "title": "Automatic Detection of Accent and Lexical Pronunciation Errors in Spontaneous Non-Native English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20g_interspeech.html": {
    "title": "Context-Aware Goodness of Pronunciation for Computer-Assisted Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chu20_interspeech.html": {
    "title": "Recognize Mispronunciations to Improve Non-Native Acoustic Modeling Through a Phone Decoder Built from One Edit Distance Finite State Automaton",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gimeno20_interspeech.html": {
    "title": "Partial AUC Optimisation Using Recurrent Neural Networks for Music Detection with Limited Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lavechin20_interspeech.html": {
    "title": "An Open-Source Voice Type Classifier for Child-Centered Daylong Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/peng20_interspeech.html": {
    "title": "Competing Speaker Count Estimation on the Fusion of the Spectral and Spatial Embedding Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20g_interspeech.html": {
    "title": "Audio-Visual Multi-Speaker Tracking Based on the GLMB Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20p_interspeech.html": {
    "title": "Towards Speech Robustness for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhu20b_interspeech.html": {
    "title": "Identify Speakers in Cocktail Parties with End-to-End Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/neumann20_interspeech.html": {
    "title": "Multi-Talker ASR for an Unknown Number of Sources: Joint Training of Source Counting, Separation and ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/upadhyay20_interspeech.html": {
    "title": "Attentive Convolutional Recurrent Neural Network Using Phoneme-Level Acoustic Representation for Rare Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cornell20_interspeech.html": {
    "title": "Detecting and Counting Overlapping Speakers in Distant Speech Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/moritz20_interspeech.html": {
    "title": "All-in-One Transformer: Unifying Speech Recognition, Audio Tagging, and Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/diener20_interspeech.html": {
    "title": "Towards Silent Paralinguistics: Deriving Speaking Mode and Speaker ID from Electromyographic Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhong20_interspeech.html": {
    "title": "Predicting Collaborative Task Performance Using Graph Interlocutor Acoustic Network in Small Group Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gosztolya20_interspeech.html": {
    "title": "Very Short-Term Conflict Intensity Estimation Using Fisher Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mori20_interspeech.html": {
    "title": "Gaming Corpus for Studying Social Screams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/afshan20_interspeech.html": {
    "title": "Speaker Discrimination in Humans and Machines: Effects of Speaking Style Variability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sabu20_interspeech.html": {
    "title": "Automatic Prediction of Confidence Level from Children's Oral Reading Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xue20_interspeech.html": {
    "title": "Towards a Comprehensive Assessment of Speech Intelligibility for Pathological Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20h_interspeech.html": {
    "title": "Effects of Communication Channels and Actor's Gender on Emotion Identification by Native Mandarin Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/anjos20_interspeech.html": {
    "title": "Detection of Voicing and Place of Articulation of Fricatives with Deep Learning in a Virtual Speech and Language Therapy Tutor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20y_interspeech.html": {
    "title": "Unsupervised Learning for Sequence-to-Sequence Text-to-Speech for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/palkama20_interspeech.html": {
    "title": "Conditional Spoken Digit Generation with StyleGAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20g_interspeech.html": {
    "title": "Towards Universal Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/katsurada20_interspeech.html": {
    "title": "Speaker-Independent Mel-Cepstrum Estimation from Articulator Movements Using D-Vector Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liang20_interspeech.html": {
    "title": "Enhancing Monotonicity for Robust Autoregressive Transformer TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mohan20_interspeech.html": {
    "title": "Incremental Text to Speech for Neural Sequence-to-Sequence Models Using Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tu20b_interspeech.html": {
    "title": "Semi-Supervised Learning for Multi-Speaker Text-to-Speech Synthesis Using Discrete Speech Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/saha20_interspeech.html": {
    "title": "Learning Joint Articulatory-Acoustic Representations with Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yamashita20_interspeech.html": {
    "title": "Investigating Effective Additional Contextual Factors in DNN-Based Spontaneous Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/webber20_interspeech.html": {
    "title": "Hider-Finder-Combiner: An Adversarial Architecture for General Speech Signal Modification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20i_interspeech.html": {
    "title": "Wav2Spk: A Simple DNN Architecture for Learning Speaker Embeddings from Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pham20b_interspeech.html": {
    "title": "How Does Label Noise Affect the Quality of Speaker Embeddings?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20q_interspeech.html": {
    "title": "A Comparative Re-Assessment of Feature Extractors for Deep Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xia20_interspeech.html": {
    "title": "Speaker Representation Learning Using Global Context Guided Channel and Time-Frequency Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kwon20_interspeech.html": {
    "title": "Intra-Class Variation Reduction of Speaker Representation in Disentanglement Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/georges20_interspeech.html": {
    "title": "Compact Speaker Embedding: lrx-Vector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kreyssig20_interspeech.html": {
    "title": "Cosine-Distance Virtual Adversarial Training for Semi-Supervised Speaker-Discriminative Acoustic Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/peng20b_interspeech.html": {
    "title": "Deep Speaker Embedding with Long Short Term Centroid Learning for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ca_interspeech.html": {
    "title": "Neural Discriminant Analysis for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cho20b_interspeech.html": {
    "title": "Learning Speaker Embedding from Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20f_interspeech.html": {
    "title": "Noisy-Reverberant Speech Enhancement Using DenseUNet with Time-Frequency Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20z_interspeech.html": {
    "title": "On Loss Functions and Recurrency Training for GAN-Based Speech Enhancement Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/du20c_interspeech.html": {
    "title": "Self-Supervised Adversarial Multi-Task Learning for Vocoder-Based Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kegler20_interspeech.html": {
    "title": "Deep Speech Inpainting of Time-Frequency Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shankar20_interspeech.html": {
    "title": "Real-Time Single-Channel Deep Neural Network-Based Speech Enhancement on Edge Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20j_interspeech.html": {
    "title": "Improved Speech Enhancement Using a Time-Domain GAN with Mask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/defossez20_interspeech.html": {
    "title": "Real Time Speech Enhancement in the Waveform Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/romaniuk20_interspeech.html": {
    "title": "Efficient Low-Latency Speech Enhancement with Mobile Audio Streaming Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chiba20_interspeech.html": {
    "title": "Multi-Stream Attention-Based BLSTM with Feature Segmentation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20da_interspeech.html": {
    "title": "Microphone Array Post-Filter for Target Speech Enhancement Without a Prior Information of Point Interferers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hiroe20_interspeech.html": {
    "title": "Similarity-and-Independence-Aware Beamformer: Method for Target Source Extraction Using Magnitude Spectrogram as Reference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/golokolenko20_interspeech.html": {
    "title": "The Method of Random Directions Optimization for Stereo Audio Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20b_interspeech.html": {
    "title": "Gated Recurrent Fusion of Spatial and Spectral Features for Multi-Channel Speech Separation with Deep Embedding Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/scheibler20_interspeech.html": {
    "title": "Generalized Minimal Distortion Principle for Blind Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhong20b_interspeech.html": {
    "title": "A Lightweight Model Based on Separable Convolution for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cai20b_interspeech.html": {
    "title": "Meta Multi-Task Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/grondin20_interspeech.html": {
    "title": "GEV Beamforming Supported by DOA-Based Masks Generated on Pairs of Microphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jose20_interspeech.html": {
    "title": "Accurate Detection of Wake Word Start and End Using a CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/adya20_interspeech.html": {
    "title": "Hybrid Transformer/CTC Networks for Hardware Efficient Voice Triggering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/majumdar20_interspeech.html": {
    "title": "MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mehrotra20_interspeech.html": {
    "title": "Iterative Compression of End-to-End ASR Model Using AutoML",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nguyen20c_interspeech.html": {
    "title": "Quantization Aware Training with Absolute-Cosine Regularization for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/garg20b_interspeech.html": {
    "title": "Streaming On-Device End-to-End ASR System for Privacy-Sensitive Voice-Typing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pratap20b_interspeech.html": {
    "title": "Scaling Up Online Speech Recognition Using ConvNets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bai20_interspeech.html": {
    "title": "Listen Attentively, and Spell Once: Whole Sentence Generation via a Non-Autoregressive Architecture for Low-Latency Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/strimel20_interspeech.html": {
    "title": "Rescore in a Flash: Compact, Cache Efficient Hashing Data Structures for n-Gram Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shankar20b_interspeech.html": {
    "title": "Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shankar20c_interspeech.html": {
    "title": "Non-Parallel Emotion Conversion Using a Deep-Generative Hybrid Network and an Adversarial Pair Discriminator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tits20b_interspeech.html": {
    "title": "Laughter Synthesis: Combining Seq2seq Modeling with Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cao20b_interspeech.html": {
    "title": "Nonparallel Emotional Speech Conversion Using VAE-GAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sorin20_interspeech.html": {
    "title": "Principal Style Components: Expressive Style Control and Cross-Speaker Transfer in Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20d_interspeech.html": {
    "title": "Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/matsumoto20_interspeech.html": {
    "title": "Controlling the Strength of Emotions in Speech-Like Emotional Sound Generated by WaveNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20aa_interspeech.html": {
    "title": "Learning Syllable-Level Discrete Prosodic Representation for Expressive Speech Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kishida20_interspeech.html": {
    "title": "Simultaneous Conversion of Speaker Identity and Emotion Based on Multiple-Domain Adaptive RBM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20h_interspeech.html": {
    "title": "Exploiting Deep Sentential Context for Expressive End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hono20_interspeech.html": {
    "title": "Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/eskimez20_interspeech.html": {
    "title": "GAN-Based Data Generation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dhamyal20_interspeech.html": {
    "title": "The Phonetic Bases of Vocal Expressed Emotion: Natural versus Acted",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qin20_interspeech.html": {
    "title": "The INTERSPEECH 2020 Far-Field Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ba_interspeech.html": {
    "title": "Deep Embedding Learning for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gusev20_interspeech.html": {
    "title": "STC-Innovation Speaker Recognition Systems for Far-Field Speaker Verification Challenge 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ca_interspeech.html": {
    "title": "NPU Speaker Verification System for INTERSPEECH 2020 Far-Field Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tong20_interspeech.html": {
    "title": "The JD AI Speaker Verification System for the FFSVC 2020 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20c_interspeech.html": {
    "title": "FaceFilter: Audio-Visual Speech Separation Using Still Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20d_interspeech.html": {
    "title": "Seeing Voices and Hearing Voices: Learning Discriminative Embeddings Using Cross-Modal Self-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wand20_interspeech.html": {
    "title": "Fusion Architectures for Word-Based Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20f_interspeech.html": {
    "title": "Audio-Visual Multi-Channel Recognition of Overlapped Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ea_interspeech.html": {
    "title": "TMT: A Transformer-Based Modal Translator for Improving Multimodal Sequence Representations in Audio Visual Scene-Aware Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sterpu20_interspeech.html": {
    "title": "Should we Hard-Code the Recurrence Concept or Learn it Instead ? Exploring the Transformer Architecture for Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/koumparoulis20_interspeech.html": {
    "title": "Resource-Adaptive Deep Learning for Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mortazavi20_interspeech.html": {
    "title": "Speech-Image Semantic Alignment Does Not Depend on Any Prior Classification Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20r_interspeech.html": {
    "title": "Lip Graph Assisted Audio-Visual Speech Recognition Using Bidirectional Synchronous Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/konda20_interspeech.html": {
    "title": "Caption Alignment for Low Resource Audio-Visual Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mevawalla20_interspeech.html": {
    "title": "Successes, Challenges and Opportunities for Speech Technology in Conversational Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/michelsanti20_interspeech.html": {
    "title": "Vocoder-Based Speech Synthesis from Silent Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20k_interspeech.html": {
    "title": "Quasi-Periodic Parallel WaveGAN Vocoder: A Non-Autoregressive Pitch-Dependent Dilated Convolution Model for Parametric Speech Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20l_interspeech.html": {
    "title": "A Cyclical Post-Filtering Approach to Mismatch Refinement of Neural Vocoder for Text-to-Speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yoon20_interspeech.html": {
    "title": "Audio Dequantization for High Fidelity Audio Generation in Flow-Based Neural Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharma20b_interspeech.html": {
    "title": "StrawNet: Self-Training WaveNet for TTS in Low-Data Regimes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cui20b_interspeech.html": {
    "title": "An Efficient Subband Linear Prediction for LPCNet-Based Neural Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ai20b_interspeech.html": {
    "title": "Reverberation Modeling for Source-Filter-Based Neural Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/vipperla20_interspeech.html": {
    "title": "Bunched LPCNet: Vocoder for Low-Cost Neural Text-To-Speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/song20c_interspeech.html": {
    "title": "Neural Text-to-Speech with a Modeling-by-Generation Excitation Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/vainer20_interspeech.html": {
    "title": "SpeedySpeech: Efficient Neural Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20da_interspeech.html": {
    "title": "Semi-Supervised End-to-End ASR via Teacher-Student Learning with Conditional Posterior Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sapru20_interspeech.html": {
    "title": "Leveraging Unlabeled Speech for Sequence Discriminative Training of Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20fa_interspeech.html": {
    "title": "Developing RNN-T Models Surpassing High-Performance Hybrid Models with Customization Capability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chang20c_interspeech.html": {
    "title": "End-to-End ASR with Adaptive Span Self-Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lakomkin20_interspeech.html": {
    "title": "Subword Regularization: An Analysis of Scalability and Generalization for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/michel20_interspeech.html": {
    "title": "Early Stage LM Integration Using Local and Global Log-Linear Combination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/han20_interspeech.html": {
    "title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sainath20_interspeech.html": {
    "title": "Emitting Word Timings with End-to-End Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20s_interspeech.html": {
    "title": "Low-Latency Sequence-to-Sequence Speech Recognition and Translation by Partial Hypothesis Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ga_interspeech.html": {
    "title": "Neural Language Modeling with Implicit Cache Pointers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jain20b_interspeech.html": {
    "title": "Finnish ASR with Deep Transformer Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/futami20_interspeech.html": {
    "title": "Distilling the Knowledge of BERT for Sequence-to-Sequence ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chien20_interspeech.html": {
    "title": "Stochastic Convolutional Recurrent Networks for Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huo20_interspeech.html": {
    "title": "Investigation of Large-Margin Softmax in Neural Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20t_interspeech.html": {
    "title": "Contextualizing ASR Lattice Rescoring with Hybrid Pointer Network Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/higuchi20b_interspeech.html": {
    "title": "Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fujita20_interspeech.html": {
    "title": "Insertion-Based Modeling for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20o_interspeech.html": {
    "title": "Voice Activity Detection in the Wild via Weakly Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20f_interspeech.html": {
    "title": "Dual Attention in Time and Frequency Domain for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20e_interspeech.html": {
    "title": "Polishing the Classical Likelihood Ratio Test by Supervised Learning for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20e_interspeech.html": {
    "title": "A Noise Robust Technique for Detecting Vowels in Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lavechin20b_interspeech.html": {
    "title": "End-to-End Domain-Adversarial Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/agarwal20_interspeech.html": {
    "title": "VOP Detection in Variable Speech Rate Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zheng20d_interspeech.html": {
    "title": "MLNET: An Adaptive Multiple Receptive-Field Attention Neural Network for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kreuk20_interspeech.html": {
    "title": "Self-Supervised Contrastive Learning for Unsupervised Phoneme Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zelasko20_interspeech.html": {
    "title": "That Sounds Familiar: An Analysis of Phonetic Representations Transfer Across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/limonard20_interspeech.html": {
    "title": "Analyzing Read Aloud Speech by Primary School Pupils: Insights for Research and Development",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rasilo20_interspeech.html": {
    "title": "Discovering Articulatory Speech Targets from Synthesized Random Babble",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/csapo20c_interspeech.html": {
    "title": "Speaker Dependent Acoustic-to-Articulatory Inversion Using Real-Time MRI of the Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bozorg20_interspeech.html": {
    "title": "Acoustic-to-Articulatory Inversion with Deep Autoregressive Articulatory-WaveNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/douros20_interspeech.html": {
    "title": "Using Silence MR Image to Synthesise Dynamic MRI Vocal Tract Data of CV",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/csapo20d_interspeech.html": {
    "title": "Quantification of Transducer Misalignment in Ultrasound Tongue Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/parrot20_interspeech.html": {
    "title": "Independent and Automatic Evaluation of Speaker-Independent Acoustic-to-Articulatory Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/diener20b_interspeech.html": {
    "title": "CSL-EMG_Array: An Open Access Corpus for EMG-to-Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/penney20_interspeech.html": {
    "title": "Links Between Production and Perception of Glottalisation in Individual Australian English Speaker/Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/siriwardhana20_interspeech.html": {
    "title": "Jointly Fine-Tuning \"BERT-Like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20e_interspeech.html": {
    "title": "Vector-Quantized Autoregressive Predictive Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/song20d_interspeech.html": {
    "title": "Speech-XLNet: Unsupervised Acoustic Model Pretraining for Self-Attention Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/singh20c_interspeech.html": {
    "title": "Large Scale Weakly and Semi-Supervised Learning for Low-Resource Video ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumatani20_interspeech.html": {
    "title": "Sequence-Level Self-Learning with Multiple Hypotheses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20m_interspeech.html": {
    "title": "Defense for Black-Box Attacks on Anti-Spoofing Models by Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20i_interspeech.html": {
    "title": "Understanding Self-Attention of Self-Supervised Audio Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/khurana20_interspeech.html": {
    "title": "A Convolutional Deep Markov Model for Unsupervised Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abulimiti20_interspeech.html": {
    "title": "Automatic Speech Recognition for ILSE-Interviews: Longitudinal Conversational Speech Recordings Covering Aging and Cognitive Decline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20e_interspeech.html": {
    "title": "Dynamic Margin Softmax Loss for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rybicka20_interspeech.html": {
    "title": "On Parameter Adaptation in Softmax-Based Cross-Entropy Loss for Improved Convergence Speed and Accuracy in DNN-Based Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mingote20_interspeech.html": {
    "title": "Training Speaker Enrollment Models by Network Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sarfjoo20_interspeech.html": {
    "title": "Supervised Domain Adaptation for Text-Independent Speaker Verification Using Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wei20b_interspeech.html": {
    "title": "Angular Margin Centroid Loss for Text-Independent Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kang20_interspeech.html": {
    "title": "Domain-Invariant Speaker Vector Projection by Model-Agnostic Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/desplanques20_interspeech.html": {
    "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20p_interspeech.html": {
    "title": "Length- and Noise-Aware Training Techniques for Short-Utterance Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20e_interspeech.html": {
    "title": "Spoken Language ‘Grammatical Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/papi20_interspeech.html": {
    "title": "Mixtures of Deep Neural Experts for Automated Speech Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ba_interspeech.html": {
    "title": "Targeted Content Feedback in Spoken Language Learning and Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/raina20_interspeech.html": {
    "title": "Universal Adversarial Attacks on Spoken Language Assessment Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20n_interspeech.html": {
    "title": "Ensemble Approaches for Uncertainty in Spoken Language Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20k_interspeech.html": {
    "title": "Shadowability Annotation with Fine Granularity on L2 Utterances and its Improvement with Native Listeners' Script-Shadowing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bai20b_interspeech.html": {
    "title": "ASR-Based Evaluation and Feedback for Individualized Reading Practice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/woszczyk20_interspeech.html": {
    "title": "Domain Adversarial Neural Networks for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hidaka20_interspeech.html": {
    "title": "Automatic Estimation of Pathological Voice Quality Based on Recurrent Neural Network Using Amplitude and Phase Spectrogram",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chien20b_interspeech.html": {
    "title": "Stochastic Curiosity Exploration for Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jeong20_interspeech.html": {
    "title": "Conditional Response Augmentation for Dialogue Using Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/luo20c_interspeech.html": {
    "title": "Prototypical Q Networks for Automatic Conversational Diagnosis and Few-Shot New Disease Adaption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hong20b_interspeech.html": {
    "title": "End-to-End Task-Oriented Dialog System Through Template Slot Value Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/he20_interspeech.html": {
    "title": "Task-Oriented Dialog Generation with Enhanced Entity Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dang20_interspeech.html": {
    "title": "End-to-End Speech-to-Dialog-Act Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qian20_interspeech.html": {
    "title": "Discriminative Transfer Learning for Optimizing ASR and Semantic Labeling in Task-Oriented Spoken Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20f_interspeech.html": {
    "title": "Datasets and Benchmarks for Task-Oriented Log Dialogue Ranking Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ca_interspeech.html": {
    "title": "A Semi-Blind Source Separation Approach for Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20j_interspeech.html": {
    "title": "Virtual Acoustic Channel Expansion Based on Neural Networks for Weighted Prediction Error-Based Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kothapally20_interspeech.html": {
    "title": "SkipConvNet: Skip Convolutional Neural Network for Speech Dereverberation Using Optimally Smoothed Spectral Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ea_interspeech.html": {
    "title": "A Robust and Cascaded Acoustic Echo Cancellation Based on Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20fa_interspeech.html": {
    "title": "Generative Adversarial Network Based Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pfeifenberger20_interspeech.html": {
    "title": "Nonlinear Residual Echo Suppression Using a Recurrent Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20d_interspeech.html": {
    "title": "Independent Echo Path Modeling for Stereophonic Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20q_interspeech.html": {
    "title": "Nonlinear Residual Echo Suppression Based on Multi-Stream Conv-TasNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20c_interspeech.html": {
    "title": "Improving Partition-Block-Based Acoustic Echo Canceler in Under-Modeling Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20e_interspeech.html": {
    "title": "Attention Wave-U-Net for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cai20c_interspeech.html": {
    "title": "From Speaker Verification to Multispeaker Speech Synthesis, Deep Transfer with Feedback Constraint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cooper20_interspeech.html": {
    "title": "Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20da_interspeech.html": {
    "title": "Non-Autoregressive End-to-End TTS with Coarse-to-Fine Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ea_interspeech.html": {
    "title": "Bi-Level Speaker Supervision for One-Shot Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/peirolilja20_interspeech.html": {
    "title": "Naturalness Enhancement with Linguistic Information in End-to-End TTS Using Unsupervised Parallel Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ha_interspeech.html": {
    "title": "MoBoAligner: A Neural Alignment Model for Non-Autoregressive TTS with Monotonic Boundary Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lim20_interspeech.html": {
    "title": "JDI-T: Jointly Trained Duration Informed Transformer for Text-To-Speech without Explicit Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/aso20_interspeech.html": {
    "title": "End-to-End Text-to-Speech Synthesis with Unaligned Multiple Language Units Based on Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dou20_interspeech.html": {
    "title": "Attention Forcing for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fong20_interspeech.html": {
    "title": "Testing the Limits of Representation Mixing for Pronunciation Correction in End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20r_interspeech.html": {
    "title": "MultiSpeech: Multi-Speaker Text to Speech with Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/papadopoulos20_interspeech.html": {
    "title": "Exploiting Conic Affinity Measures to Design Speech Enhancement Systems Operating in Unseen Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ji20_interspeech.html": {
    "title": "Adversarial Dictionary Learning for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/seki20_interspeech.html": {
    "title": "Semi-Supervised Self-Produced Speech Enhancement and Suppression Based on Joint Source Modeling of Air- and Body-Conducted Signals Using Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/weisman20_interspeech.html": {
    "title": "Spatial Covariance Matrix Estimation for Reverberant Speech with Application to Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ho20_interspeech.html": {
    "title": "A Cross-Channel Attention-Based Wave-U-Net for Multi-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fedorov20_interspeech.html": {
    "title": "TinyLSTMs: Efficient Neural Speech Enhancement for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hikosaka20_interspeech.html": {
    "title": "Intelligibility Enhancement Based on Speech Waveform Modification Using Hearing Impairment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hou20c_interspeech.html": {
    "title": "Speaker and Phoneme-Aware Speech Bandwidth Extension with Residual Dual-Path Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hou20d_interspeech.html": {
    "title": "Multi-Task Learning for End-to-End Noise-Robust Bandwidth Extension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20h_interspeech.html": {
    "title": "Phase-Aware Music Super-Resolution Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20d_interspeech.html": {
    "title": "Learning Utterance-Level Representations with Label Smoothing for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jalal20_interspeech.html": {
    "title": "Removing Bias with Residual Mixture of Multi-View Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20d_interspeech.html": {
    "title": "Adaptive Domain-Aware Representation Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20f_interspeech.html": {
    "title": "Speech Emotion Recognition with Discriminative Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20g_interspeech.html": {
    "title": "Using Speech Enhancement Preprocessing for Speech Emotion Recognition in Realistic Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ia_interspeech.html": {
    "title": "Comparison of Glottal Source Parameter Values in Emotional Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chou20_interspeech.html": {
    "title": "Learning to Recognize Per-Rater's Emotion Perception Using Co-Rater Training Strategy with Soft and Hard Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jalal20b_interspeech.html": {
    "title": "Empirical Interpretation of Speech Emotion Perception with Attention Based Model for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gessinger20_interspeech.html": {
    "title": "Phonetic Accommodation of L2 German Speakers to the Virtual Language Learning Tutor Mirabella",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gu20b_interspeech.html": {
    "title": "Characterization of Singaporean Children's English: Comparisons to American and British Counterparts Using Archetypal Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kaminskaia20_interspeech.html": {
    "title": "Rhythmic Convergence in Canadian French Varieties?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/manghat20_interspeech.html": {
    "title": "Malayalam-English Code-Switched: Grapheme to Phoneme System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hutin20_interspeech.html": {
    "title": "Ongoing Phonologization of Word-Final Voicing Alternations in Two Romance Languages: Romanian and French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hope20_interspeech.html": {
    "title": "Cues for Perception of Gender in Synthetic Voices and the Role of Identity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/menshikova20_interspeech.html": {
    "title": "Phonetic Entrainment in Cooperative Dialogues: A Case of Russian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20g_interspeech.html": {
    "title": "Prosodic Characteristics of Genuine and Mock (Im)polite Mandarin Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ja_interspeech.html": {
    "title": "Tone Variations in Regionally Accented Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20k_interspeech.html": {
    "title": "F0 Patterns in Mandarin Statements of Mandarin and Cantonese Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chuang20b_interspeech.html": {
    "title": "SpeechBERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kuo20b_interspeech.html": {
    "title": "An Audio-Enriched BERT-Based Framework for Spoken Multiple-Choice Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20e_interspeech.html": {
    "title": "Entity Linking for Short Text Using Structured Knowledge Graph via Multi-Grained Text Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ga_interspeech.html": {
    "title": "Sound-Image Grounding Based Focusing Mechanism for Efficient Automatic Spoken Language Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yamamoto20_interspeech.html": {
    "title": "Semi-Supervised Learning for Character Expression of Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20h_interspeech.html": {
    "title": "Dimensional Emotion Prediction Based on Interactive Context in Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/atamna20_interspeech.html": {
    "title": "HRI-RNN: A User-Robot Dynamics-Oriented RNN for Engagement Decrease Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fuscone20_interspeech.html": {
    "title": "Neural Representations of Dialogical History for Improving Upcoming Turn Acoustic Parameters Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20i_interspeech.html": {
    "title": "Detecting Domain-Specific Credibility and Expertise in Text and Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/das20c_interspeech.html": {
    "title": "The Attacker's Perspective on Automatic Speaker Verification: An Overview",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sholokhov20_interspeech.html": {
    "title": "Extrapolating False Alarm Rates in Automatic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jiang20b_interspeech.html": {
    "title": "Self-Supervised Spoofing Audio Detection Scheme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20fa_interspeech.html": {
    "title": "Inaudible Adversarial Perturbations for Targeted Attack in Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/villalba20_interspeech.html": {
    "title": "x-Vectors Meet Adversarial Attacks: Benchmarking Adversarial Robustness in Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ha_interspeech.html": {
    "title": "Black-Box Attacks on Spoofing Countermeasures Using Transferability of Adversarial Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/n20_interspeech.html": {
    "title": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/manakul20_interspeech.html": {
    "title": "Abstractive Spoken Document Summarization Using Hierarchical Model with Multi-Stage Attention Diversity Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ia_interspeech.html": {
    "title": "Improved Learning of Word Embeddings with Word Definitions and Semantic Injection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ga_interspeech.html": {
    "title": "Wake Word Detection with Alignment-Free Lattice-Free MMI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nguyen20d_interspeech.html": {
    "title": "Improving Vietnamese Named Entity Recognition from Speech Using Word Capitalization and Punctuation Recovery Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yadav20b_interspeech.html": {
    "title": "End-to-End Named Entity Recognition from English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mckenna20_interspeech.html": {
    "title": "Semantic Complexity in End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tran20c_interspeech.html": {
    "title": "Analysis of Disfluency in Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mittal20_interspeech.html": {
    "title": "Representation Based Meta-Learning for Few-Shot Spoken Intent Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/agarwal20b_interspeech.html": {
    "title": "Complementary Language Model and Parallel Bi-LRNN for False Trigger Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20u_interspeech.html": {
    "title": "Speaker-Utterance Dual Attention for Speaker and Utterance Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yi20b_interspeech.html": {
    "title": "Adversarial Separation and Adaptation Network for Far-Field Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/han20b_interspeech.html": {
    "title": "MIRNet: Learning Multiple Identities Representations in Overlapped Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20l_interspeech.html": {
    "title": "Strategies for End-to-End Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hautamaki20_interspeech.html": {
    "title": "Why Did the x-Vector System Miss a Target Speaker? Impact of Acoustic Mismatch Upon Target Score on VoxCeleb Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/afshan20b_interspeech.html": {
    "title": "Variable Frame Rate-Based Data Augmentation to Handle Speaking-Style Variability for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/seurin20_interspeech.html": {
    "title": "A Machine of Few Words: Interactive Speaker Recognition with Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/granqvist20_interspeech.html": {
    "title": "Improving On-Device Speaker Verification Using Federated Learning with Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ramoji20_interspeech.html": {
    "title": "Neural PLDA Modeling for End-to-End Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/opatka20_interspeech.html": {
    "title": "State Sequence Pooling Training of Acoustic Models for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hard20_interspeech.html": {
    "title": "Training Keyword Spotting Models on Non-IID Data with Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20f_interspeech.html": {
    "title": "Class LM and Word Mapping for Contextual Biasing in End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/borgholt20_interspeech.html": {
    "title": "Do End-to-End Speech Recognition Models Care About Context?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20f_interspeech.html": {
    "title": "Utterance Confidence Measure for End-to-End Speech Recognition with Applications to Distributed Speech Recognition Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20o_interspeech.html": {
    "title": "Speaker Code Based Speaker Adaptive Training Using Model Agnostic Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhu20c_interspeech.html": {
    "title": "Domain Adaptation Using Class Similarity for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/novitasari20_interspeech.html": {
    "title": "Incremental Machine Speech Chain Towards Enabling Listening While Speaking in Real-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/raissi20_interspeech.html": {
    "title": "Context-Dependent Acoustic Modeling Without Explicit Phone Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shahnawazuddin20_interspeech.html": {
    "title": "Voice Conversion Based Data Augmentation to Improve Children's Speech Recognition in Limited Data Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/karlapati20_interspeech.html": {
    "title": "CopyCat: Many-to-Many Fine-Grained Prosody Transfer for Neural Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20m_interspeech.html": {
    "title": "Joint Detection of Sentence Stress and Phrase Boundary for Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kulkarni20_interspeech.html": {
    "title": "Transfer Learning of the Expressivity Using FLOW Metric Learning in Multispeaker Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bae20_interspeech.html": {
    "title": "Speaking Speed Control of End-to-End Speech Synthesis Using Sentence-Level Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tyagi20_interspeech.html": {
    "title": "Dynamic Prosody Generation for Speech Synthesis Using Linguistics-Driven Acoustic Embedding Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kenter20_interspeech.html": {
    "title": "Improving the Prosody of RNN-Based English Text-To-Speech Synthesis by Incorporating a BERT Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20g_interspeech.html": {
    "title": "Improved Prosody from Learned F0 Codebook Representations for VQ-VAE Speech Waveform Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zeng20b_interspeech.html": {
    "title": "Prosody Learning Mechanism for Speech Synthesis System Without Text Length Limit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shirahata20_interspeech.html": {
    "title": "Discriminative Method to Extract Coarse Prosodic Structure and its Application for Statistical Phrase/Accent Command Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/raitio20_interspeech.html": {
    "title": "Controllable Neural Text-to-Speech Synthesis Using Intuitive Prosodic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/morrison20_interspeech.html": {
    "title": "Controllable Neural Prosody Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/whitehill20_interspeech.html": {
    "title": "Multi-Reference Neural TTS Stylization with Adversarial Cycle Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20e_interspeech.html": {
    "title": "Interactive Text-to-Speech System via Joint Style Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hirschi20_interspeech.html": {
    "title": "Mobile-Assisted Prosody Training for Limited English Proficiency: Learner Background and Speech Learning Pattern",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/niekerk20_interspeech.html": {
    "title": "Finding Intelligible Consonant-Vowel Sounds Using High-Quality Articulatory Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/krishnamohan20_interspeech.html": {
    "title": "Audiovisual Correspondence Learning in Humans and Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lan20_interspeech.html": {
    "title": "Perception of English Fricatives and Affricates by Advanced Chinese Learners of English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tsukada20_interspeech.html": {
    "title": "Perception of Japanese Consonant Length by Native Speakers of Korean Differing in Japanese Learning Experience",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ng20b_interspeech.html": {
    "title": "Automatic Detection of Phonological Errors in Child Speech Using Siamese Recurrent Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20e_interspeech.html": {
    "title": "A Comparison of English Rhythm Produced by Native American Speakers and Mandarin ESL Primary School Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20h_interspeech.html": {
    "title": "Cross-Linguistic Interaction Between Phonological Categorization and Orthography Predicts Prosodic Effects in the Acquisition of Portuguese Liquids by L1-Mandarin Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ka_interspeech.html": {
    "title": "Cross-Linguistic Perception of Utterances with Willingness and Reluctance in Mandarin by Korean L2 Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cheng20b_interspeech.html": {
    "title": "Speech Enhancement Based on Beamforming and Post-Filtering by Combining Phase Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ha_interspeech.html": {
    "title": "A Noise-Aware Memory-Attention Network Architecture for Regression-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/su20b_interspeech.html": {
    "title": "HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pandey20_interspeech.html": {
    "title": "Learning Complex Spectral Mapping for Speech Enhancement with Improved Cross-Corpus Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/richter20_interspeech.html": {
    "title": "Speech Enhancement with Stochastic Temporal Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gogate20_interspeech.html": {
    "title": "Visual Speech In Real Noisy Environments (VISION): A Novel Benchmark Dataset and Deep Learning-Based Baseline System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sivaraman20_interspeech.html": {
    "title": "Sparse Mixture of Local Experts for Efficient Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kishore20_interspeech.html": {
    "title": "Improved Speech Enhancement Using TCN with Multiple Encoder-Decoder Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20e_interspeech.html": {
    "title": "Joint Training for Simultaneous Speech Denoising and Dereverberation with Deep Embedding Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fontaine20_interspeech.html": {
    "title": "Unsupervised Robust Speech Enhancement Based on Alpha-Stable Fast Multichannel Nonnegative Matrix Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/albes20_interspeech.html": {
    "title": "Squeeze for Sneeze: Compact Neural Networks for Cold and Flu Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/seneviratne20_interspeech.html": {
    "title": "Extended Study on the Use of Vocal Tract Variables to Quantify Neuromotor Coordination in Depression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xezonaki20_interspeech.html": {
    "title": "Affective Conditioning on Hierarchical Attention Networks Applied to Depression Detection from Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20g_interspeech.html": {
    "title": "Domain Adaptation for Enhancing Speech-Based Depression Detection in Natural Environmental Conditions Using Dilated CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gosztolya20b_interspeech.html": {
    "title": "Making a Distinction Between Schizophrenia and Bipolar Disorder Based on Temporal Parameters in Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huckvale20_interspeech.html": {
    "title": "Prediction of Sleepiness Ratings from Voice by Man and Machine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/teplansky20_interspeech.html": {
    "title": "Tongue and Lip Motion Patterns in Alaryngeal Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yue20b_interspeech.html": {
    "title": "Autoencoder Bottleneck Features with Multi-Task Optimisation for Improved Continuous Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mallela20_interspeech.html": {
    "title": "Raw Speech Waveform Based Classification of Patients with ALS, Parkinson's Disease and Healthy Controls Using CNN-BLSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pompili20b_interspeech.html": {
    "title": "Assessment of Parkinson's Disease Medication State Through Automatic Speech Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ja_interspeech.html": {
    "title": "Improving Replay Detection System with Channel Consistency DenseNeXt for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/falkowskigilski20_interspeech.html": {
    "title": "Subjective Quality Evaluation of Speech Signals Transmitted via BPL-PLC Wired System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chiu20_interspeech.html": {
    "title": "Investigating the Visual Lombard Effect with Gabor Based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20h_interspeech.html": {
    "title": "Exploration of Audio Quality Assessment and Anomaly Localisation Using Attention Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ragano20_interspeech.html": {
    "title": "Development of a Speech Quality Database Under Uncontrolled Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/algayres20_interspeech.html": {
    "title": "Evaluating the Reliability of Acoustic Speech Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20la_interspeech.html": {
    "title": "Frame-Level Signal-to-Noise Ratio Estimation Using Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dong20_interspeech.html": {
    "title": "A Pyramid Recurrent Network for Predicting Crowdsourced Speech-Quality Ratings of Real-World Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/brueggeman20_interspeech.html": {
    "title": "Effect of Spectral Complexity Reduction and Number of Instruments on Musical Enjoyment with Cochlear Implants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kosmider20_interspeech.html": {
    "title": "Spectrum Correction: Acoustic Scene Classification with Mismatched Recording Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/oconnor20_interspeech.html": {
    "title": "Distributed Summation Privacy for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/leschanowsky20_interspeech.html": {
    "title": "Perception of Privacy Measured in the Crowd — Paired Comparison on the Effect of Background Noises",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kreuk20b_interspeech.html": {
    "title": "Hide and Speak: Towards Deep Neural Networks for Speech Steganography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/daubener20_interspeech.html": {
    "title": "Detecting Adversarial Examples for Speech Recognition via Uncertainty Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/adelani20_interspeech.html": {
    "title": "Privacy Guarantees for De-Identifying Text Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jayashankar20_interspeech.html": {
    "title": "Detecting Audio Attacks on ASR Systems with Dropout Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20i_interspeech.html": {
    "title": "Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/suda20_interspeech.html": {
    "title": "Nonparallel Training of Exemplar-Based Voice Conversion System Using INCA-Based Alignment Technique",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20s_interspeech.html": {
    "title": "Enhancing Intelligibility of Dysarthric Speech Using Gated Convolutional-Based Voice Conversion System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20p_interspeech.html": {
    "title": "VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20e_interspeech.html": {
    "title": "Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion Without Parallel Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fu20c_interspeech.html": {
    "title": "Dynamic Speaker Representations Adjustment and Decoder Factorization for Speaker Adaptation in End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lian20d_interspeech.html": {
    "title": "ARVC: An Auto-Regressive Voice Conversion System Without Parallel Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nercessian20_interspeech.html": {
    "title": "Improved Zero-Shot Voice Conversion Using Explicit Conditioning Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20t_interspeech.html": {
    "title": "Non-Parallel Voice Conversion with Fewer Labeled Data by Conditional Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20v_interspeech.html": {
    "title": "Transferring Source Style in Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/albadawy20_interspeech.html": {
    "title": "Voice Conversion Using Speech-to-Speech Neuro-Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ia_interspeech.html": {
    "title": "Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/thomas20_interspeech.html": {
    "title": "Transliteration Based Data Augmentation for Training Multilingual ASR Acoustic Models in Low Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhu20d_interspeech.html": {
    "title": "Multilingual Speech Recognition with Self-Attention Structured Parameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/madikeri20_interspeech.html": {
    "title": "Lattice-Free Maximum Mutual Information Training of Multilingual Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pratap20c_interspeech.html": {
    "title": "Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sailor20_interspeech.html": {
    "title": "Multilingual Speech Recognition Using Language-Specific Phoneme Recognition as Auxiliary Task for Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chandu20_interspeech.html": {
    "title": "Style Variation as a Vantage Point for Code-Switching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20f_interspeech.html": {
    "title": "Bi-Encoder Transformer Network for Mandarin-English Code-Switching Speech Recognition Using Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharma20c_interspeech.html": {
    "title": "Improving Low Resource Code-Switched ASR Using Augmented Code-Switched TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qiu20c_interspeech.html": {
    "title": "Towards Context-Aware End-to-End Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dinh20b_interspeech.html": {
    "title": "Increasing the Intelligibility and Naturalness of Alaryngeal Speech Using Voice Conversion and Synthetic Fundamental Frequency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tong20b_interspeech.html": {
    "title": "Automatic Assessment of Dysarthric Severity Level Using Audio-Video Cross-Modal Approach in Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20n_interspeech.html": {
    "title": "Staged Knowledge Distillation for End-to-End Dysarthric Speech Recognition and Speech Attribute Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/takashima20_interspeech.html": {
    "title": "Dysarthric Speech Recognition Based on Deep Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/degala20_interspeech.html": {
    "title": "Automatic Glottis Detection and Segmentation in Stroboscopic Videos Using Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pan20c_interspeech.html": {
    "title": "Acoustic Feature Extraction with Interpretable Deep Neural Network for Neurodegenerative Related Disorder Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharma20d_interspeech.html": {
    "title": "Coswara — A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rowe20_interspeech.html": {
    "title": "Acoustic-Based Articulatory Phenotypes of Amyotrophic Lateral Sclerosis and Parkinson's Disease: Towards an Interpretable, Hypothesis-Driven Framework of Motor Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/alhinti20_interspeech.html": {
    "title": "Recognising Emotions in Dysarthric Speech Using Typical Speech Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/halpern20_interspeech.html": {
    "title": "Detecting and Analysing Spontaneous Oral Cancer Speech in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dunbar20_interspeech.html": {
    "title": "The Zero Resource Speech Challenge 2020: Discovering Discrete Subword and Word Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/niekerk20b_interspeech.html": {
    "title": "Vector-Quantized Neural Networks for Acoustic Unit Discovery in the ZeroSpeech 2020 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ds20_interspeech.html": {
    "title": "Exploration of End-to-End Synthesisers for Zero Resource Speech Challenge 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gundogdu20_interspeech.html": {
    "title": "Vector Quantized Temporally-Aware Correspondence Sparse Autoencoders for Zero-Resource Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tjandra20_interspeech.html": {
    "title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/morita20_interspeech.html": {
    "title": "Exploring TTS Without T Using Biologically/Psychologically Motivated Neural Network Modules (ZeroSpeech 2020)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tobing20_interspeech.html": {
    "title": "Cyclic Spectral Modeling for Unsupervised Unit Discovery into Voice Conversion with Excitation and Waveform Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20u_interspeech.html": {
    "title": "Unsupervised Acoustic Unit Representation Learning for Voice Conversion Using WaveNet Auto-Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rasanen20_interspeech.html": {
    "title": "Unsupervised Discovery of Recurring Speech Patterns Using Probabilistic Adaptive Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bhati20_interspeech.html": {
    "title": "Self-Expressing Autoencoders for Unsupervised Spoken Term Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/millet20_interspeech.html": {
    "title": "Perceptimatic: A Human Speech Perception Benchmark for Unsupervised Subword Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/clayton20_interspeech.html": {
    "title": "Decoding Imagined, Heard, and Spoken Speech: Classification and Regression of EEG Using a 14-Channel Dry-Contact Mobile Headset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/m20b_interspeech.html": {
    "title": "Glottal Closure Instants Detection from EGG Signal by Classification Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ma_interspeech.html": {
    "title": "Classify Imaginary Mandarin Tones with Cortical EEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/effendi20_interspeech.html": {
    "title": "Augmenting Images for ASR and TTS Through Single-Loop and Dual-Loop Multimodal Chain Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/augustyniak20_interspeech.html": {
    "title": "Punctuation Prediction in Spontaneous Conversations: Can We Mitigate ASR Errors with Retrofitted Word Embeddings?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sunkara20_interspeech.html": {
    "title": "Multimodal Semi-Supervised Learning Framework for Punctuation Prediction in Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20j_interspeech.html": {
    "title": "Efficient MDI Adaptation for n-Gram Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/peyser20_interspeech.html": {
    "title": "Improving Tail Performance of a Deliberation E2E ASR Model Using a Large Text Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ogawa20_interspeech.html": {
    "title": "Language Model Data Augmentation Based on Text Domain Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wok20_interspeech.html": {
    "title": "Contemporary Polish Language Model (Version 2) Using Big Data and Sub-Word Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pandey20b_interspeech.html": {
    "title": "Improving Speech Recognition of Compound-Rich Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wills20_interspeech.html": {
    "title": "Language Modeling for Speech Analytics in Under-Resourced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/han20c_interspeech.html": {
    "title": "An Early Study on Intelligent Analysis of Speech Under COVID-19: Severity, Sleep Quality, Fatigue, and Anxiety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/baird20_interspeech.html": {
    "title": "An Evaluation of the Effect of Anxiety on Speech — Computational Prediction of Anxiety from Sustained Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20h_interspeech.html": {
    "title": "Hybrid Network Feature Extraction for Depression Assessment from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pan20d_interspeech.html": {
    "title": "Improving Detection of Alzheimer's Disease Using Automatic Speech Recognition to Identify High-Quality Segments for More Robust Feature Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/romana20_interspeech.html": {
    "title": "Classification of Manifest Huntington Disease Using Vowel Distortion Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kadiri20_interspeech.html": {
    "title": "Parkinson's Disease Detection from Speech Using Single Frequency Filtering Cepstral Coefficients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/quintas20_interspeech.html": {
    "title": "Automatic Prediction of Speech Intelligibility Based on X-Vectors in the Context of Head and Neck Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abraham20_interspeech.html": {
    "title": "Spectral Moment and Duration of Burst of Plosives in Speech of Children with Hearing Impairment and Typically Developing Children — A Comparative Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/perez20_interspeech.html": {
    "title": "Aphasic Speech Recognition Using a Mixture of Speech Intelligibility Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kodrasi20_interspeech.html": {
    "title": "Automatic Discrimination of Apraxia of Speech and Dysarthria Using a Minimalistic Set of Handcrafted Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20i_interspeech.html": {
    "title": "Weak-Attention Suppression for Transformer Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20k_interspeech.html": {
    "title": "Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20na_interspeech.html": {
    "title": "Improving Transformer-Based Speech Recognition with Unsupervised Pre-Training and Multi-Task Semantic Knowledge Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hori20_interspeech.html": {
    "title": "Transformer-Based Long-Context End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20i_interspeech.html": {
    "title": "Self-and-Mixed Attention Decoder with Deep Acoustic Structure for Transformer-Based LVCSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20i_interspeech.html": {
    "title": "Universal Speech Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tian20c_interspeech.html": {
    "title": "Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20j_interspeech.html": {
    "title": "Cross Attention with Monotonic Alignment for Speech Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gulati20_interspeech.html": {
    "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20g_interspeech.html": {
    "title": "Exploring Transformers for Large-Scale Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/togami20_interspeech.html": {
    "title": "Sparseness-Aware DOA Estimation with Majorization Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhong20c_interspeech.html": {
    "title": "Spatial Resolution of Early Reflection for Speech and White Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/raikar20_interspeech.html": {
    "title": "Effect of Microphone Position Measurement Error on RIR and its Impact on Speech Intelligibility and Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/deng20c_interspeech.html": {
    "title": "Online Blind Reverberation Time Estimation Using CRNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mack20_interspeech.html": {
    "title": "Single-Channel Blind Direct-to-Reverberation Ratio Estimation Using Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/beiton20_interspeech.html": {
    "title": "The Importance of Time-Frequency Averaging for Binaural Speaker Localization in Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20j_interspeech.html": {
    "title": "Acoustic Signal Enhancement Using Relative Harmonic Coefficients: Spherical Harmonics Domain Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/murthy20_interspeech.html": {
    "title": "Instantaneous Time Delay Estimation of Broadband Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ja_interspeech.html": {
    "title": "U-Net Based Direct-Path Dominance Test for Robust Direction-of-Arrival Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xue20b_interspeech.html": {
    "title": "Sound Event Localization and Detection Based on Multiple DOA Beamforming and Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}