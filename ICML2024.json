{
  "https://openreview.net/forum?id=frA0NNBS1n": {
    "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
    "volume": "oral",
    "abstract": "Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks",
    "checked": true,
    "id": "0317c55e77147a2f3bba943c4afddee5f5daa19b",
    "semantic_title": "probabilistic inference in language models via twisted sequential monte carlo",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Bc4vZ2CX7E": {
    "title": "Position: Open-Endedness is Essential for Artificial Superhuman Intelligence",
    "volume": "oral",
    "abstract": "In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internet-scale data. Nevertheless, the creation of open-ended, ever self-improving AI remains elusive. **In this position paper, we argue that the ingredients are now in place to achieve *open-endedness* in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI).** We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, human-relevant discoveries. We conclude by examining the safety implications of generally-capable open-ended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future",
    "checked": false,
    "id": "e76b4e3dcd69220ee11e40ebcb6357e5088f04a8",
    "semantic_title": "open-endedness is essential for artificial superhuman intelligence",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dVpFKfqF3R": {
    "title": "Stop Regressing: Training Value Functions via Classification for Scalable Deep RL",
    "volume": "oral",
    "abstract": "Value functions are an essential component in deep reinforcement learning (RL), that are typically trained via mean squared error regression to match bootstrapped target values. However, scaling value-based RL methods to large networks has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We show that training value functions with categorical cross-entropy significantly enhances performance and scalability across various domains, including single-task RL on Atari 2600 games, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving *state-of-the-art results* on these domains. Through careful analysis, we show that categorical cross-entropy mitigates issues inherent to value-based RL, such as noisy targets and non-stationarity. We argue that shifting to categorical cross-entropy for training value functions can substantially improve the scalability of deep RL at little-to-no cost",
    "checked": true,
    "id": "1d36dd3d333659d72df8eb5666edfd5cde54c84f",
    "semantic_title": "stop regressing: training value functions via classification for scalable deep rl",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=RbiBKPtuHp": {
    "title": "Improving Transformers with Dynamically Composable Multi-Head Attention",
    "volume": "oral",
    "abstract": "Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy. We propose Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads. At the core of DCMHA is a Compose function that transforms the attention score and weight matrices in an input-dependent way. DCMHA can be used as a drop-in replacement of MHA in any transformer architecture to obtain the corresponding DCFormer. DCFormer significantly outperforms Transformer on different architectures and model scales in language modeling, matching the performance of models with 1.7x-2.0x compute. For example, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining perplexity and downstream task evaluation",
    "checked": true,
    "id": "f4a25d45bb381b3f6ab08e84c9a65bff90e3a104",
    "semantic_title": "improving transformers with dynamically composable multi-head attention",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QBj7Uurdwf": {
    "title": "Learning Useful Representations of Recurrent Neural Network Weight Matrices",
    "volume": "oral",
    "abstract": "Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the _mechanistic approach_ directly looks at some RNN's weights to predict its behavior, the _functionalist approach_ analyzes its overall functionality–specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation learning. One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed MNIST digits. With the help of an emulation-based self-supervised learning technique we compare and evaluate the different RNN weight encoding techniques on multiple downstream applications. On the most challenging one, namely predicting which exact task the RNN was trained on, functionalist approaches show clear superiority",
    "checked": true,
    "id": "4b3396c3b4eca43aeae7f4628880f855bc437fb1",
    "semantic_title": "learning useful representations of recurrent neural network weight matrices",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BwAkaxqiLB": {
    "title": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model",
    "volume": "oral",
    "abstract": "Heuristics are widely used for dealing with complex search and optimization problems. However, manual design of heuristics can be often very labour extensive and requires rich working experience and knowledge. This paper proposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that leverages both Large Language Models (LLMs) and Evolutionary Computation (EC) methods for Automatic Heuristic Design (AHD). EoH represents the ideas of heuristics in natural language, termed thoughts. They are then translated into executable codes by LLMs. The evolution of both thoughts and codes in an evolutionary search framework makes it very effective and efficient for generating high-performance heuristics. Experiments on three widely studied combinatorial optimization benchmark problems demonstrate that EoH outperforms commonly used handcrafted heuristics and other recent AHD methods including FunSearch. Particularly, the heuristic produced by EoH with a low computational budget (in terms of the number of queries to LLMs) significantly outperforms widely-used human hand-crafted baseline algorithms for the online bin packing problem",
    "checked": true,
    "id": "a2e128be33b85f8f91515e205bf0237ecc0546cc",
    "semantic_title": "evolution of heuristics: towards efficient automatic algorithm design using large language model",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=gAyzjHw2ml": {
    "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code",
    "volume": "oral",
    "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal",
    "checked": false,
    "id": "ecd3091debcd2f393379508df70bceb94db0be3b",
    "semantic_title": "scenecraft: an llm agent for synthesizing 3d scene as blender code",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=5lI9wm4dws": {
    "title": "Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning",
    "volume": "oral",
    "abstract": "Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators",
    "checked": true,
    "id": "b51588fc138dcf7d9ba700fcce5e081f1d4c4e62",
    "semantic_title": "doubly robust causal effect estimation under networked interference via targeted learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=plXXbXjvQ9": {
    "title": "Emergent Equivariance in Deep Ensembles",
    "volume": "oral",
    "abstract": "We show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments",
    "checked": true,
    "id": "e17447e8f1d240a7246953cf6434994d8aceff19",
    "semantic_title": "emergent equivariance in deep ensembles",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jKYyFbH8ap": {
    "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks",
    "volume": "oral",
    "abstract": "We introduce **S**yntax-**A**ware **F**ill-**i**n-the-**M**iddle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com",
    "checked": true,
    "id": "8e36a8f8fd4451d55e927151aabc463830941a8e",
    "semantic_title": "evaluation of llms on syntax-aware code fill-in-the-middle tasks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dslUyy1rN4": {
    "title": "Position: Automatic Environment Shaping is the Next Frontier in RL",
    "volume": "oral",
    "abstract": "Many roboticists dream of presenting a robot with a task in the evening and returning the next morning to find the robot capable of solving the task. What is preventing us from achieving this? Sim-to-real reinforcement learning (RL) has achieved impressive performance on challenging robotics tasks, but requires substantial human effort to set up the task in a way that is amenable to RL. It's our position that algorithmic improvements in policy optimization and other ideas should be guided towards resolving the primary bottleneck of shaping the training environment, i.e., designing observations, actions, rewards and simulation dynamics. Most practitioners don't tune the RL algorithm, but other environment parameters to obtain a desirable controller. We posit that scaling RL to diverse robotic tasks will only be achieved if the community focuses on automating environment shaping procedures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TujtZgdRxB": {
    "title": "Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning",
    "volume": "oral",
    "abstract": "For a specific online optimization problem, for example, online bipartite matching (OBM), research efforts could be made in two directions before it is finally closed, i.e., the optimal competitive online algorithm is found. One is to continuously design algorithms with better performance. To this end, reinforcement learning (RL) has demonstrated great success in literature. However, little is known on the other direction: whether RL helps explore how hard an online problem is. In this paper, we study a generalized model of OBM, named online matching with stochastic rewards (OMSR, FOCS 2012), for which the optimal competitive ratio is still unknown. We adopt an adversarial RL approach that trains two RL agents adversarially and iteratively: the algorithm agent learns for algorithms with larger competitive ratios, while the adversarial agent learns to produce a family of hard instances. Through such a framework, agents converge at the end with a robust algorithm, which empirically outperforms the state of the art (STOC 2020). Much more significantly, it allows to track how the hard instances are generated. We succeed in distilling two structural properties from the learned graph patterns, which remarkably reduce the action space, and further enable theoretical improvement on the best-known hardness result of OMSR, from $0.621$ (FOCS 2012) to $0.597$. To the best of our knowledge, this gives the first evidence that RL can help enhance the theoretical understanding of an online problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4XlGXIh2BB": {
    "title": "Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis",
    "volume": "oral",
    "abstract": "What is *agency,* and why does it matter? In this work, we draw from the political science and philosophy literature and give two competing visions of what it means to be an (ethical) agent. The first view, which we term *mechanistic*, is commonly— and implicitly—assumed in AI research, yet it is a fundamentally limited means to understand the ethical characteristics of AI. Under the second view, which we term volitional, AI can no longer be considered an ethical agent. We discuss the implications of each of these views for two critical questions: first, what the ideal system \"ought\" to look like, and second, how accountability may be achieved. In light of this discussion, we ultimately argue that, in the context of ethically-significant behavior, AI should be viewed not as an agent but as the outcome of political processes",
    "checked": false,
    "id": "01d3ffd257d88cb997a00dcc9ec571d778006db0",
    "semantic_title": "beyond personhood: agency, accountability, and the limits of anthropomorphic ethical analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xm2lU7tteQ": {
    "title": "Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape",
    "volume": "oral",
    "abstract": "Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest",
    "checked": true,
    "id": "b62814ad9176b5c7d09eff2cab7707e81bf2deab",
    "semantic_title": "transformers learn nonlinear features in context: nonconvex mean-field dynamics on the attention landscape",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=bX3J7ho18S": {
    "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
    "volume": "oral",
    "abstract": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: *ICLR* 2024, *NeurIPS* 2023, *CoRL* 2023 and *EMNLP* 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices",
    "checked": true,
    "id": "023e113b11ff7bac182713a069fedcbcccad9562",
    "semantic_title": "monitoring ai-modified content at scale: a case study on the impact of chatgpt on ai conference peer reviews",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=vJx6fld6l0": {
    "title": "Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics",
    "volume": "oral",
    "abstract": "This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR & AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (**HEPT**), which combines E$^2$LSH with OR & AND constructions and is built upon regular computations. HEPT demonstrates remarkable performance on two critical yet time-consuming HEP tasks, significantly outperforming existing GNNs and transformers in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing. Our code is available at https://github.com/Graph-COM/HEPT",
    "checked": true,
    "id": "c87b56ede2479fa809627a0ec89c97f5a1533ff9",
    "semantic_title": "locality-sensitive hashing-based efficient point transformer with applications in high-energy physics",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Yd8eHMY1wz": {
    "title": "Unified Training of Universal Time Series Forecasting Transformers",
    "volume": "oral",
    "abstract": "Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of *universal forecasting*, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: (i) cross-frequency learning, (ii) accommodating an arbitrary number of variates for multivariate time series, and (iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed **M**asked Enc**o**der-based Un**i**ve**r**s**a**l T**i**me Series Forecasting Transformer (**Moirai**). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts",
    "checked": true,
    "id": "4a111f7a3b56d0468f13104999844885157ef17d",
    "semantic_title": "unified training of universal time series forecasting transformers",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=arwP5FA2dO": {
    "title": "Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy",
    "volume": "oral",
    "abstract": "Magnetic confinement fusion may one day provide reliable, carbon-free energy, but the field currently faces technical hurdles. In this position paper, we highlight six key research challenges in the field of fusion energy that we believe should be research priorities for the Machine Learning (ML) community because they are especially ripe for ML applications: (1) disruption prediction, (2) simulation and dynamics modeling (3) resolving partially observed data, (4) improving controls, (5) guiding experiments with optimal design, and (6) enhancing materials discovery. For each problem, we give background, review past ML work, suggest features of future models, and list challenges and idiosyncrasies facing ML development. We also discuss ongoing efforts to update the fusion data ecosystem and identify opportunities further down the line that will be enabled as fusion and its data infrastructure advance. It is our position that fusion energy offers especially exciting opportunities for ML practitioners to impact decarbonization and the future of energy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WLPhywf1si": {
    "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
    "volume": "oral",
    "abstract": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. The code and robust models are available on GitHub",
    "checked": true,
    "id": "d9f198541267870ae71087f120ea543ebc38d1c6",
    "semantic_title": "robust clip: unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=uKmcyyrZae": {
    "title": "Less is More: on the Over-Globalizing Problem in Graph Transformers",
    "volume": "oral",
    "abstract": "Graph Transformer, due to its global attention mechanism, has emerged as a new tool in dealing with graph-structured data. It is well recognized that the global attention mechanism considers a wider receptive field in a fully connected graph, leading many to believe that useful information can be extracted from all the nodes. In this paper, we challenge this belief: does the globalizing property always benefit Graph Transformers? We reveal the over-globalizing problem in Graph Transformer by presenting both empirical evidence and theoretical analysis, i.e., the current attention mechanism overly focuses on those distant nodes, while the near nodes, which actually contain most of the useful information, are relatively weakened. Then we propose a novel Bi-Level Global Graph Transformer with Collaborative Training (CoBFormer), including the inter-cluster and intra-cluster Transformers, to prevent the over-globalizing problem while keeping the ability to extract valuable information from distant nodes. Moreover, the collaborative training is proposed to improve the model's generalization ability with a theoretical guarantee. Extensive experiments on various graphs well validate the effectiveness of our proposed CoBFormer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hYHsrKDiX7": {
    "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
    "volume": "oral",
    "abstract": "Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies",
    "checked": true,
    "id": "c1fa6255cc9fc3128f74befc7855e255bc7a2c6e",
    "semantic_title": "galore: memory-efficient llm training by gradient low-rank projection",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=3WCvnkHnxV": {
    "title": "PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs",
    "volume": "oral",
    "abstract": "On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using 9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and 100$\\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text",
    "checked": true,
    "id": "97f7a0c4be425f0019f7fca603d3dfd522da025a",
    "semantic_title": "pre-text: training language models on private federated data in the age of llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uDkXoZMzBv": {
    "title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation",
    "volume": "oral",
    "abstract": "While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow. In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens. In practice, we demonstrate the effectiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models. Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are confined to an invariant low-dimensional subspace. Consequently, we can construct and train compact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts. In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization. For language model fine-tuning, we propose a method called \"Deep LoRA\", which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency. We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data",
    "checked": false,
    "id": "7bc03047bb085c204b577eff58d380c04cfa83d2",
    "semantic_title": "compressible dynamics in deep overparameterized low-rank learning&adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Be2B6f0ps1": {
    "title": "Position: Technical Research and Talent is Needed for Effective AI Governance",
    "volume": "oral",
    "abstract": "In light of recent advancements in AI capabilities and the increasingly widespread integration of AI systems into society, governments worldwide are actively seeking to mitigate the potential harms and risks associated with these technologies through regulation and other governance tools. However, there exist significant gaps between governance aspirations and the current state of the technical tooling necessary for their realisation. In this position paper, we survey policy documents published by public-sector institutions in the EU, US, and China to highlight specific areas of disconnect between the technical requirements necessary for enacting proposed policy actions, and the current technical state of the art. Our analysis motivates a call for tighter integration of the AI/ML research community within AI governance in order to i) catalyse technical research aimed at bridging the gap between current and supposed technical underpinnings of regulatory action, as well as ii) increase the level of technical expertise within governing institutions so as to inform and guide effective governance of AI",
    "checked": true,
    "id": "defd7eceb5646578c2aa0f837efbf317cf19ae95",
    "semantic_title": "position: technical research and talent is needed for effective ai governance",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MFPYCvWsNR": {
    "title": "Bottleneck-Minimal Indexing for Generative Document Retrieval",
    "volume": "oral",
    "abstract": "We apply an information-theoretic perspective to reconsider generative document retrieval (GDR), in which a document $x \\in \\mathcal{X}$ is indexed by $t \\in \\mathcal{T}$, and a neural autoregressive model is trained to map queries $\\mathcal{Q}$ to $\\mathcal{T}$. GDR can be considered to involve information transmission from documents $\\mathcal{X}$ to queries $\\mathcal{Q}$, with the requirement to transmit more bits via the indexes $\\mathcal{T}$. By applying Shannon's rate-distortion theory, the optimality of indexing can be analyzed in terms of the mutual information, and the design of the indexes $\\mathcal{T}$ can then be regarded as a *bottleneck* in GDR. After reformulating GDR from this perspective, we empirically quantify the bottleneck underlying GDR. Finally, using the NQ320K and MARCO datasets, we evaluate our proposed bottleneck-minimal indexing method in comparison with various previous indexing methods, and we show that it outperforms those methods",
    "checked": true,
    "id": "f0524448eb3d6bb57e6caed5dc1d50c48c1fa8c4",
    "semantic_title": "bottleneck-minimal indexing for generative document retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GHZVjmaGQM": {
    "title": "Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic Response",
    "volume": "oral",
    "abstract": "Hybrid models composing mechanistic ODE-based dynamics with flexible and expressive neural network components have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as the hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: *ranking* of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a *causal loss* that we combine with the standard predictive loss to arrive at a *hybrid loss* that biases our learning towards causally valid hybrid models. We demonstrate our ability to achieve a win-win, state-of-the-art predictive performance *and* causal validity, in the challenging task of modeling glucose dynamics post-exercise in individuals with type 1 diabetes",
    "checked": true,
    "id": "bda169f613b0f15366c7ea0dd5b11d6837c9f09b",
    "semantic_title": "hybrid$^2$ neural ode causal modeling and an application to glycemic response",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jrHUbftLd6": {
    "title": "FedMBridge: Bridgeable Multimodal Federated Learning",
    "volume": "oral",
    "abstract": "Multimodal Federated Learning (MFL) addresses the setup of multiple clients with diversified modality types (e.g. image, text, video, and audio) working together to improve their local personal models in a data-privacy manner. Prior MFL works rely on restrictive compositional neural architecture designs to ensure inter-client information sharing via blockwise model aggregation, limiting their applicability in the real-world **Architecture-personalized MFL (AMFL)** scenarios, where clients may have distinguished multimodal interaction strategies and there is no restriction on local architecture design. The key challenge in AMFL is how to automatically and efficiently tackle the two heterogeneity patterns--statistical and architecture heterogeneity--while maximizing the beneficial information sharing among clients. To solve this challenge, we propose **FedMBridge**, which leverages a topology-aware hypernetwork to act as a bridge that can automatically balance and digest the two heterogeneity patterns in a communication-efficient manner. Our experiments on four AMFL simulations demonstrate the efficiency and effectiveness of our proposed approach",
    "checked": false,
    "id": "933364b4c60c153b5d278beb78715b258d8415c3",
    "semantic_title": "multimodal federated learning in healthcare: a review",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=685vj0lC9z": {
    "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
    "volume": "oral",
    "abstract": "In day-to-day communication, people often approximate the truth --- for example, rounding the time or omitting details --- in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting",
    "checked": true,
    "id": "291923449015d8fdd13e8af432a7b1169666dcec",
    "semantic_title": "how do large language models navigate conflicts between honesty and helpfulness?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ghNRg2mEgN": {
    "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
    "volume": "oral",
    "abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior---for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to *weakly supervise* superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call *weak-to-strong generalization*. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models",
    "checked": true,
    "id": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
    "semantic_title": "weak-to-strong generalization: eliciting strong capabilities with weak supervision",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=1puvYh729M": {
    "title": "ACE: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization",
    "volume": "oral",
    "abstract": "The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, **ACE**: Off-policy **A**ctor-critic with **C**ausality-aware **E**ntropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which underscores the effectiveness, versatility, and efficient sample efficiency of our approach. Benchmark results and videos are available at https://ace-rl.github.io/",
    "checked": false,
    "id": "06df2005b1244940c711eb4819be41cc26e7eed7",
    "semantic_title": "ace : off-policy actor-critic with causality-aware entropy regularization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=rPm5cKb1VB": {
    "title": "Expressivity and Generalization: Fragment-Biases for Molecular GNNs",
    "volume": "oral",
    "abstract": "Although recent advances in higher-order Graph Neural Networks (GNNs) improve the theoretical expressiveness and molecular property predictive performance, they often fall short of the empirical performance of models that explicitly use fragment information as inductive bias. However, for these approaches, there exists no theoretic expressivity study. In this work, we propose the *Fragment-WL* test, an extension to the well-known Weisfeiler & Leman (WL) test, which enables the theoretic analysis of these fragment-biased GNNs. Building on the insights gained from the Fragment-WL test, we develop a new GNN architecture and a fragmentation with infinite vocabulary that significantly boosts expressiveness. We show the effectiveness of our model on synthetic and real-world data where we outperform all GNNs on Peptides and have $12$% lower error than all GNNs on ZINC and $34$% lower error than other fragment-biased models. Furthermore, we show that our model exhibits superior generalization capabilities compared to the latest transformer-based architectures, positioning it as a robust solution for a range of molecular modeling tasks",
    "checked": true,
    "id": "2c8d8c23156f886865df2cf99f18f658cbdea826",
    "semantic_title": "expressivity and generalization: fragment-biases for molecular gnns",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oowQ8LPA12": {
    "title": "Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability",
    "volume": "oral",
    "abstract": "Use of machine learning to perform database operations, such as indexing, cardinality estimation, and sorting, is shown to provide substantial performance benefits. However, when datasets change and data distribution shifts, empirical results also show performance degradation for learned models, possibly to worse than non-learned alternatives. This, together with a lack of theoretical understanding of learned methods undermines their practical applicability, since there are no guarantees on how well the models will perform after deployment. In this paper, we present the first known theoretical characterization of the performance of learned models in dynamic datasets, for the aforementioned operations. Our results show novel theoretical characteristics achievable by learned models and provide bounds on the performance of the models that characterize their advantages over non-learned methods, showing why and when learned models can outperform the alternatives. Our analysis develops the *distribution learnability* framework and novel theoretical tools which build the foundation for the analysis of learned database operations in the future",
    "checked": false,
    "id": "6158de1f0c7f7943436644787681e2d951ad0068",
    "semantic_title": "on genuine invariance learning without weight-tying",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dLojMSgSFW": {
    "title": "Position: A Safe Harbor for AI Evaluation and Red Teaming",
    "volume": "oral",
    "abstract": "Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major generative AI developers commit to providing a legal and technical safe harbor, protecting public interest safety research and removing the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting safety, privacy, and trustworthiness research on generative AI systems, where norms and incentives could be better aligned with public interests, without exacerbating model misuse. We believe these commitments are a necessary step towards more inclusive and unimpeded community efforts to tackle the risks of generative AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uUeXaKLE1I": {
    "title": "A Dynamic Algorithm for Weighted Submodular Cover Problem",
    "volume": "oral",
    "abstract": "We initiate the study of the submodular cover problem in a dynamic setting where the elements of the ground set are inserted and deleted. In the classical submodular cover problem, we are given a monotone submodular function $f : 2^{V} \\to \\mathbb{R}^{\\ge 0}$ and the goal is to obtain a set $S \\subseteq V$ that minimizes the cost subject to the constraint $f(S) = f(V)$. This is a classical problem in computer science and generalizes the Set Cover problem, 2-Set Cover, and dominating set problem among others. We consider this problem in a dynamic setting where there are updates to our set $V$, in the form of insertions and deletions of elements from a ground set $\\mathcal{V}$, and the goal is to maintain an approximately optimal solution with low query complexity per update. For this problem, we propose a randomized algorithm that, in expectation, obtains a $(1-O(\\epsilon), O(\\epsilon^{-1}))$-bicriteria approximation using polylogarithmic query complexity per update",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mKYBMf1hHG": {
    "title": "Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits",
    "volume": "oral",
    "abstract": "Data Shapley provides a principled approach to data valuation and plays a crucial role in data-centric machine learning (ML) research. Data selection is considered a standard application of Data Shapley. However, its data selection performance has shown to be inconsistent across settings in the literature. This study aims to deepen our understanding of this phenomenon. We introduce a hypothesis testing framework and show that Data Shapley's performance can be no better than random selection without specific constraints on utility functions. We identify a class of utility functions, monotonically transformed modular functions, within which Data Shapley optimally selects data. Based on this insight, we propose a heuristic for predicting Data Shapley's effectiveness in data selection tasks. Our experiments corroborate these findings, adding new insights into when Data Shapley may or may not succeed",
    "checked": true,
    "id": "3b785e0921999317a7a42a8da4e8ea73c0780a45",
    "semantic_title": "rethinking data shapley for data selection tasks: misleads and merits",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6XH8R7YrSk": {
    "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study",
    "volume": "oral",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions",
    "checked": true,
    "id": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
    "semantic_title": "is dpo superior to ppo for llm alignment? a comprehensive study",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=cc72Vnfvoc": {
    "title": "Trained Random Forests Completely Reveal your Dataset",
    "volume": "oral",
    "abstract": "We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest. Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn. To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective. We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming - an approach rooted in constraint propagation and solution-domain reduction. Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction. This holds true even with a small number of trees. Even with bootstrap aggregation, the majority of the data can also be reconstructed. These findings underscore a critical vulnerability inherent in widely adopted ensemble methods, warranting attention and mitigation. Although the potential for such reconstruction attacks has been discussed in privacy research, our study provides clear empirical evidence of their practicability",
    "checked": true,
    "id": "55a9feacb97fb0fcf85b3c6cbd760e9bb3ba5b70",
    "semantic_title": "trained random forests completely reveal your dataset",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s1sdx6vNsU": {
    "title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
    "volume": "oral",
    "abstract": "Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with rank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well",
    "checked": true,
    "id": "20214d0a8147dbd26c0481b39fe59682155033cd",
    "semantic_title": "lora training in the ntk regime has no spurious local minima",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4dOJAfXhNV": {
    "title": "SAPG: Split and Aggregate Policy Gradients",
    "volume": "oral",
    "abstract": "Despite extreme sample inefficiency, on-policy reinforcement learning, aka policy gradients, has become a fundamental tool in decision-making problems. With the recent advances in GPU-driven simulation, the ability to collect large amounts of data for RL training has scaled exponentially. However, we show that current RL methods, e.g. PPO, fail to ingest the benefit of parallelized environments beyond a certain point and their performance saturates. To address this, we propose a new on-policy RL algorithm that can effectively leverage large-scale environments by splitting them into chunks and fusing them back together via importance sampling. Our algorithm, termed SAPG, shows significantly higher performance across a variety of challenging environments where vanilla PPO and other strong baselines fail to achieve high performance. Webpage at https://sapg-rl.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xWI0MKwJSS": {
    "title": "How Private are DP-SGD Implementations?",
    "volume": "oral",
    "abstract": "We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling-based DP-SGD is more commonly used in practical implementations, it has not been amenable to easy privacy analysis, either analytically or even numerically. On the other hand, Poisson subsampling-based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling-based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD",
    "checked": true,
    "id": "81f0c0edd9f5415740079d48c4671d9cd6f1a572",
    "semantic_title": "how private are dp-sgd implementations?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=JnA9IveEwg": {
    "title": "From Coarse to Fine: Enable Comprehensive Graph Self-supervised Learning with Multi-granular Semantic Ensemble",
    "volume": "oral",
    "abstract": "Self-supervised learning (SSL) has gained increasing attention in the graph learning community, owing to its capability of enabling powerful models pre-trained on large unlabeled graphs for general purposes, facilitating quick adaptation to specific domains. Though promising, existing graph SSL frameworks often struggle to capture both high-level abstract features and fine-grained features simultaneously, leading to sub-optimal generalization abilities across different downstream tasks. To bridge this gap, we present Multi-granularity Graph Semantic Ensemble via Knowledge Distillation, namely MGSE, a plug-and-play graph knowledge distillation framework that can be applied to any existing graph SSL framework to enhance its performance by incorporating the concept of multi-granularity. Specifically, MGSE captures multi-granular knowledge by employing multiple student models to learn from a single teacher model, conditioned by probability distributions with different granularities. We apply it to six state-of-the-art graph SSL frameworks and evaluate their performances over multiple graph datasets across different domains, the experimental results show that MGSE can consistently boost the performance of these existing graph SSL frameworks with up to 9.2% improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CNicRIVIPA": {
    "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
    "volume": "oral",
    "abstract": "Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting)",
    "checked": true,
    "id": "ce806f8d32f6fb1eaa821248a1bc4fa2cd949fbb",
    "semantic_title": "discrete diffusion modeling by estimating the ratios of the data distribution",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=gPStP3FSY9": {
    "title": "Discovering Environments with XRM",
    "volume": "oral",
    "abstract": "Environment annotations are essential for the success of many out-of-distribution (OOD) generalization methods. Unfortunately, these are costly to obtain and often limited by human annotators' biases. To achieve robust generalization, it is essential to develop algorithms for automatic environment discovery within datasets. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods introduce hyper-parameters and early-stopping criteria, which require a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk Minimization (XRM) to address this issue. XRM trains twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not require early-stopping, and can discover environments for all training and validation data. Algorithms built on top of XRM environments achieve oracle worst-group-accuracy, addressing a long-standing challenge in OOD generalization",
    "checked": true,
    "id": "dc3fa7346cbe8801ae20d0ff4a14c21e90b91c7d",
    "semantic_title": "discovering environments with xrm",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=GqWy1wZKeE": {
    "title": "Fast Co-Training under Weak Dependence via Stream-Based Active Learning",
    "volume": "oral",
    "abstract": "Co-training is a classical semi-supervised learning method which only requires a small number of labeled examples for learning, under reasonable assumptions. Despite extensive literature on the topic, very few hypothesis classes are known to be provably efficiently learnable via co-training, even under very strong distributional assumptions. In this work, we study the co-training problem in the stream-based active learning model. We show that a range of natural concept classes are efficiently learnable via co-training, in terms of both label efficiency and computational efficiency. We provide an efficient reduction of co-training under the standard assumption of weak dependence, in the stream-based active model, to online classification. As a corollary, we obtain efficient co-training algorithms with error independent label complexity for every concept class class efficiently learnable in the mistake bound online model. Our framework also gives co-training algorithms with label complexity $\\tilde{O}(d\\log (1/\\epsilon))$ for any concept class with VC dimension $d$, though in general this reduction is not computationally efficient. Finally, using additional ideas from online learning, we design the first efficient co-training algorithms with label complexity $\\tilde{O}(d^2\\log (1/\\epsilon))$ for several concept classes, including unions of intervals and homogeneous halfspaces",
    "checked": false,
    "id": "508730e9b10bf7fb048677248b53d82144d63666",
    "semantic_title": "arch: large-scale knowledge graph via aggregated narrative codified health records analysis",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=K6HpbvkrwO": {
    "title": "Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choice",
    "volume": "oral",
    "abstract": "This study designs an adaptive experiment for efficiently estimating *average treatment effects* (ATEs). In each round of our adaptive experiment, an experimenter sequentially samples an experimental unit, assigns a treatment, and observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using the gathered samples. The objective is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose optimizing the covariate density as well as the propensity score. First, we derive the efficient covariate density and propensity score that minimize the semiparametric efficiency bound and find that optimizing both covariate density and propensity score minimizes the semiparametric efficiency bound more effectively than optimizing only the propensity score. Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment. Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound",
    "checked": false,
    "id": "ab7dc1dc7ff64e93afcf7af56d4e1bd8ef07a0f6",
    "semantic_title": "active adaptive experimental design for treatment effect estimation with covariate choices",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kAfYYg6PX8": {
    "title": "Listenable Maps for Audio Classifiers",
    "volume": "oral",
    "abstract": "Despite the impressive performance of deep learning models across diverse tasks, their complexity poses challenges for interpretation. This challenge is particularly evident for audio signals, where conveying interpretations becomes inherently difficult. To address this issue, we introduce Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method that generates faithful and listenable interpretations. L-MAC utilizes a decoder on top of a pretrained classifier to generate binary masks that highlight relevant portions of the input audio. We train the decoder with a loss function that maximizes the confidence of the classifier decision on the masked-in portion of the audio while minimizing the probability of model output for the masked-out portion. Quantitative evaluations on both in-domain and out-of-domain data demonstrate that L-MAC consistently produces more faithful interpretations than several gradient and masking-based methodologies. Furthermore, a user study confirms that, on average, users prefer the interpretations generated by the proposed technique",
    "checked": true,
    "id": "a76ca04997dfde12a84ca37f576db972b26e96b5",
    "semantic_title": "listenable maps for audio classifiers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MdPBVWTfwG": {
    "title": "I/O Complexity of Attention, or How Optimal is FlashAttention?",
    "volume": "oral",
    "abstract": "Attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity. In a recent significant development, FlashAttention shows that the I/O complexity of attention is the true bottleneck in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) where computation happens and a slow memory (e.g. GPU high-bandwidth memory) where the data resides, the I/O complexity measures the number of accesses to the slow memory. FlashAttention is an I/O-aware algorithm for self-attention that requires $\\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ is the head-dimension and $M$ is the size of cache. Naturally, to further reduce the computational costs of Attention, the authors ask the question: is FlashAttention's I/O complexity optimal for every value of $M$? We resolve the above question in its full generality by showing an I/O complexity lower bound that matches the upper bound provided by FlashAttention for any values of $M \\geq d^2$ within any constant factors. Moreover, our lower bounds do not rely on using combinatorial matrix multiplication for computing the attention matrix: even if one uses fast matrix multiplication, the above I/O complexity bounds cannot be improved. Further, we give a better algorithm with lower I/O complexity for $M < d^2$, and show that it is optimal for combinatorial algorithms. We do so by introducing a new communication complexity protocol for matrix compression, and connecting communication complexity to I/O complexity. We believe this connection could be of independent interest and will find more applications in proving I/O complexity lower bounds in future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=byxXa99PtF": {
    "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
    "volume": "oral",
    "abstract": "Uncertainty decomposition refers to the task of decomposing the total uncertainty of a predictive model into aleatoric (data) uncertainty, resulting from inherent randomness in the data-generating process, and epistemic (model) uncertainty, resulting from missing information in the model's training data. In large language models (LLMs) specifically, identifying sources of uncertainty is an important step toward improving reliability, trustworthiness, and interpretability, but remains an important open research question. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarification ensembling, which can be applied to any pre-trained LLM. Our approach generates a set of clarifications for the input, feeds them into an LLM, and ensembles the corresponding predictions. We show that, when aleatoric uncertainty arises from ambiguity or under-specification in LLM inputs, this approach makes it possible to factor an (un-clarified) LLM's predictions into separate aleatoric and epistemic terms, using a decomposition similar to the one employed by Bayesian neural networks. Empirical evaluations demonstrate that input clarification ensembling provides accurate and reliable uncertainty quantification on several language processing tasks. Code and data are available at https://github.com/UCSB-NLP-Chang/llm_uncertainty",
    "checked": true,
    "id": "67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3",
    "semantic_title": "decomposing uncertainty for large language models through input clarification ensembling",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=lQ2o7JteMO": {
    "title": "Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments",
    "volume": "oral",
    "abstract": "We study inference on the long-term causal effect of a continual exposure to a novel intervention, which we term a long-term treatment, based on an experiment involving only short-term observations. Key examples include the long-term health effects of regularly-taken medicine or of environmental hazards and the long-term effects on users of changes to an online platform. This stands in contrast to short-term treatments or \"shocks,\" whose long-term effect can reasonably be mediated by short-term observations, enabling the use of surrogate methods. Long-term treatments by definition have direct effects on long-term outcomes via continual exposure, so surrogacy conditions cannot reasonably hold. We connect the problem with offline reinforcement learning, leveraging doubly-robust estimators to estimate long-term causal effects for long-term treatments and construct confidence intervals",
    "checked": true,
    "id": "eb12d472cf1c22df690da9eb7f82e263c05dc170",
    "semantic_title": "inferring the long-term causal effects of long-term treatments from short-term experiments",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=d2E2i5rJ4x": {
    "title": "Multiplicative Weights Update, Area Convexity and Random Coordinate Descent for Densest Subgraph Problems",
    "volume": "oral",
    "abstract": "We study the densest subgraph problem and give algorithms via multiplicative weights update and area convexity that converge in $O\\left(\\frac{\\log m}{\\epsilon^{2}}\\right)$ and $O\\left(\\frac{\\log m}{\\epsilon}\\right)$ iterations, respectively, both with nearly-linear time per iteration. Compared with the work by Bahmani et al. (2014), our MWU algorithm uses a very different and much simpler procedure for recovering the dense subgraph from the fractional solution and does not employ a binary search. Compared with the work by Boob et al. (2019), our algorithm via area convexity improves the iteration complexity by a factor $\\Delta$---the maximum degree in the graph, and matches the fastest theoretical runtime currently known via flows (Chekuri et al., 2022) in total time. Next, we study the dense subgraph decomposition problem and give the first practical iterative algorithm with linear convergence rate $O\\left(mn\\log\\frac{1}{\\epsilon}\\right)$ via accelerated random coordinate descent. This significantly improves over $O\\left(\\frac{m\\sqrt{mn\\Delta}}{\\epsilon}\\right)$ time of the FISTA-based algorithm by Harb et al. (2022). In the high precision regime $\\epsilon\\ll\\frac{1}{n}$ where we can even recover the exact solution, our algorithm has a total runtime of $O\\left(mn\\log n\\right)$, matching the state of the art exact algorithm via parametric flows (Gallo et al., 1989). Empirically, we show that this algorithm is very practical and scales to very large graphs, and its performance is competitive with widely used methods that have significantly weaker theoretical guarantees",
    "checked": true,
    "id": "39cf54384a50d50862e0e26ebe93638631d2603e",
    "semantic_title": "multiplicative weights update, area convexity and random coordinate descent for densest subgraph problems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DL79HYCFFq": {
    "title": "All-in-one simulation-based inference",
    "volume": "oral",
    "abstract": "Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method---the Simformer---which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models",
    "checked": true,
    "id": "298a3a4d509473ca3e82d6e16ba7a98ebc7372ae",
    "semantic_title": "all-in-one simulation-based inference",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Ar0dsOMStE": {
    "title": "Environment Design for Inverse Reinforcement Learning",
    "volume": "oral",
    "abstract": "Learning a reward function from demonstrations suffers from low sample-efficiency. Even with abundant data, current inverse reinforcement learning methods that focus on learning from a single environment can fail to handle slight changes in the environment dynamics. We tackle these challenges through adaptive environment design. In our framework, the learner repeatedly interacts with the expert, with the former selecting environments to identify the reward function as quickly as possible from the expert's demonstrations in said environments. This results in improvements in both sample-efficiency and robustness, as we show experimentally, for both exact and approximate inference",
    "checked": true,
    "id": "32971d45062ce0df1830af60d5c6d75a990ec53a",
    "semantic_title": "environment design for inverse reinforcement learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6jmdOTRMIO": {
    "title": "Scalable AI Safety via Doubly-Efficient Debate",
    "volume": "oral",
    "abstract": "The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al (2018). proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate *deterministic* AI systems for an *exponential* number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a *polynomial* number of steps, whilst being able to verify the alignment of *stochastic* AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps",
    "checked": true,
    "id": "50d1eeb8678a267d4759bd7418457998c0135d90",
    "semantic_title": "scalable ai safety via doubly-efficient debate",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=R83VIZtHXA": {
    "title": "OMPO: A Unified Framework for RL under Policy and Dynamics Shifts",
    "volume": "oral",
    "abstract": "Training reinforcement learning policies using environment interaction data collected from varying policies or dynamics presents a fundamental challenge. Existing works often overlook the distribution discrepancies induced by policy or dynamics shifts, or rely on specialized algorithms with task priors, thus often resulting in suboptimal policy performances and high learning variances. In this paper, we identify a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts: transition occupancy matching. In light of this, we introduce a surrogate policy learning objective by considering the transition occupancy discrepancies and then cast it into a tractable min-max optimization problem through dual reformulation. Our method, dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized actor-critic structure equipped with a distribution discriminator and a small-size local buffer. We conduct extensive experiments based on the OpenAI Gym, Meta-World, and Panda Robots environments, encompassing policy shifts under stationary and non-stationary dynamics, as well as domain adaption. The results demonstrate that OMPO outperforms the specialized baselines from different categories in all settings. We also find that OMPO exhibits particularly strong performance when combined with domain randomization, highlighting its potential in RL-based robotics applications",
    "checked": true,
    "id": "e75305bc1efabdf5960aa3f837b26dd735c720b9",
    "semantic_title": "ompo: a unified framework for rl under policy and dynamics shifts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7rrN6E4KU0": {
    "title": "Neural Collapse meets Differential Privacy: Curious behaviors of NoisyGD with Near-Perfect Representation Learning",
    "volume": "oral",
    "abstract": "A recent study by De et al. (2022) shows that large-scale representation learning through pre-training on a public dataset significantly enhances differentially private (DP) learning in downstream tasks. To explain this, we consider a layer-peeled model in representation learning, resulting in Neural Collapse (NC) phenomena. Within NC, we establish that the misclassification error is independent of dimension when the distance between actual and ideal features is below a threshold. We empirically evaluate feature quality in the last layer under different pre-trained models, showing that a more powerful pre-trained model improves feature representation. Moreover, we show that DP fine-tuning is less robust compared to non-DP fine-tuning, especially with perturbations. Supported by theoretical analyses and experiments, we suggest strategies like feature normalization and dimension reduction methods such as PCA to enhance DP fine-tuning robustness. Conducting PCA on last-layer features significantly improves testing accuracy",
    "checked": true,
    "id": "a22b63e2dc52d2f0bd111236b498989887617d36",
    "semantic_title": "neural collapse meets differential privacy: curious behaviors of noisygd with near-perfect representation learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QZ1DVzr6N9": {
    "title": "Differentiable Mapper for Topological Optimization of Data Representation",
    "volume": "oral",
    "abstract": "Unsupervised data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science. Its most prominent line of work is based on the so-called Mapper graph, which is a combinatorial graph whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself. While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters—among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures. However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, clustering), there is currently no method for tuning the filter itself. In this work, we build on a recently proposed optimization framework incorporating topology to provide the first filter optimization scheme for Mapper graphs. In order to achieve this, we propose a relaxed and more general version of the Mapper graph, whose convergence properties are investigated. Finally, we demonstrate the usefulness of our approach by optimizing Mapper graph representations on several datasets, and showcasing the superiority of the optimized representation over arbitrary ones",
    "checked": true,
    "id": "1aac087c15d9c02604da5ff28060d6d6194d3c2a",
    "semantic_title": "differentiable mapper for topological optimization of data representation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=hlvKd7Vdxm": {
    "title": "ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking",
    "volume": "oral",
    "abstract": "Large language models (LLM) have recently attracted significant attention in the field of artificial intelligence. However, the training process of these models poses significant challenges in terms of computational and storage capacities, thus compressing checkpoints has become an urgent problem. In this paper, we propose a novel Extreme Checkpoint Compression (ExCP) framework, which significantly reduces the required storage of training checkpoints while achieving nearly lossless performance. We first calculate the residuals of adjacent checkpoints to obtain the essential but sparse information for higher compression ratio. To further excavate the redundancy parameters in checkpoints, we then propose a weight-momentum joint shrinking method to utilize another important information during the model optimization, i.e., momentum. In particular, we exploit the information of both model and optimizer to discard as many parameters as possible while preserving critical information to ensure optimal performance. Furthermore, we utilize non-uniform quantization to further compress the storage of checkpoints. We extensively evaluate our proposed ExCP framework on several models ranging from 410M to 7B parameters and demonstrate significant storage reduction while maintaining strong performance. For instance, we achieve approximately $70\\times$ compression for the Pythia-410M model, with the final performance being as accurate as the original model on various downstream tasks. Codes will be available at https://github.com/Gaffey/ExCP",
    "checked": true,
    "id": "88401fc86945029ae7e6697cf979f72ff574bf9a",
    "semantic_title": "excp: extreme llm checkpoint compression via weight-momentum joint shrinking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pgI9inG2Ny": {
    "title": "Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error",
    "volume": "oral",
    "abstract": "Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CAR-DQN) by minimizing a surrogate of Bellman Infinity-error. The top-tier performance of CAR-DQN across various benchmarks validates its practical effectiveness and reinforces the soundness of our theoretical analysis",
    "checked": true,
    "id": "1320bd8a981cd31a0924ca9482670cbe799e402b",
    "semantic_title": "towards optimal adversarial robust q-learning with bellman infinity-error",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=saP7s0ZgYE": {
    "title": "Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models",
    "volume": "oral",
    "abstract": "Given a graph with positive and negative edge labels, the correlation clustering problem aims to cluster the nodes so to minimize the total number of between-cluster positive and within-cluster negative edges. This problem has many applications in data mining, particularly in unsupervised learning. Inspired by the prevalence of large graphs and constantly changing data in modern applications, we study correlation clustering in dynamic, parallel (MPC), and local computation (LCA) settings. We design an approach that improves state-of-the-art runtime complexities in all these settings. In particular, we provide the first fully dynamic algorithm that runs in an expected amortized constant time, without any dependence on the graph size. Moreover, our algorithm essentially matches the approximation guarantee of the celebrated Pivot algorithm",
    "checked": true,
    "id": "c32b0a6abdaa6c069224549df5150a0cba3c5f81",
    "semantic_title": "pruned pivot: correlation clustering algorithm for dynamic, parallel, and local computation models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dbFEFHAD79": {
    "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
    "volume": "oral",
    "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, a closer examination reveals persistent challenges in the evaluative capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: https://mllm-judge.github.io/",
    "checked": true,
    "id": "1036f6dfe75af06fbcdb3447dbe9be8613bf857c",
    "semantic_title": "mllm-as-a-judge: assessing multimodal llm-as-a-judge with vision-language benchmark",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=wGtzp4ZT1n": {
    "title": "CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents",
    "volume": "oral",
    "abstract": "Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. Although most of the work has focused on cooperation and collaboration between agents, little work explores *competition*, another important mechanism that promotes the development of society and economy. In this paper, we seek to examine the competition dynamics in LLM-based agents. We first propose a general framework for studying the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, the restaurant agents compete with each other to attract more customers, where competition encourages them to transform, such as cultivating new operating strategies. Simulation experiments reveal several interesting findings at the micro and macro levels, which align well with existing market and sociological theories. We hope that the framework and environment can be a promising testbed to study the competition that fosters understanding of society. Code is available at: https://github.com/microsoft/competeai",
    "checked": false,
    "id": "e5cd93d7d9f0542a2e670778394417aa773452ae",
    "semantic_title": "competeai: understanding the competition dynamics in large language model-based agents",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GyV33H5Uuk": {
    "title": "Robustness of Nonlinear Representation Learning",
    "volume": "oral",
    "abstract": "We study the problem of unsupervised representation learning in slightly misspecified settings, and thus formalize the study of robustness of nonlinear representation learning. We focus on the case where the mixing is close to a local isometry in a suitable distance and show based on existing rigidity results that the mixing can be identified up to linear transformations and small errors. In a second step, we investigate Independent Component Analysis (ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an invertible mixing matrix and $h$ a small perturbation. We show that we can approximately recover the matrix $A$ and the independent components. Together, these two results show approximate identifiability of nonlinear ICA with almost isometric mixing functions. Those results are a step towards identifiability results for unsupervised representation learning for real-world data that do not follow restrictive model classes",
    "checked": false,
    "id": "f33f84555a855a1c2d3d8ec91822652d9340c0ff",
    "semantic_title": "probing the robustness of independent mechanism analysis for representation learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=LuhWZ2oJ5L": {
    "title": "S$\\Omega$I: Score-based O-INFORMATION Estimation",
    "volume": "oral",
    "abstract": "The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is *O-information*, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce **S$\\Omega$I**, which allows to compute *O-information* without restrictive assumptions about the system while leveraging a unique model. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of **S$\\Omega$I** in the context of a real-world use case",
    "checked": false,
    "id": "e959b547aad7777891b67a2863e39e9baf299f88",
    "semantic_title": "sωi: score-based o-information estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VJwsDwuiuH": {
    "title": "Rate-Optimal Policy Optimization for Linear Markov Decision Processes",
    "volume": "oral",
    "abstract": "We study regret minimization in online episodic linear Markov Decision Processes, and propose a policy optimization algorithm that is computationally efficient, and obtains rate optimal $\\widetilde O (\\sqrt K)$ regret where $K$ denotes the number of episodes. Our work is the first to establish the optimal rate (in terms of $K$) of convergence in the stochastic setting with bandit feedback using a policy optimization based approach, and the first to establish the optimal rate in the adversarial setup with full information feedback, for which no algorithm with an optimal rate guarantee was previously known",
    "checked": true,
    "id": "ad4b67b05148f9e92de14e3ec582a118787423bf",
    "semantic_title": "rate-optimal policy optimization for linear markov decision processes",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=M5kn9NKIs4": {
    "title": "SAM as the Guide: Mastering Pseudo-Label Refinement in Semi-Supervised Referring Expression Segmentation",
    "volume": "oral",
    "abstract": "In this paper, we introduce SemiRES, a semi-supervised framework that effectively leverages a combination of labeled and unlabeled data to perform RES. A significant hurdle in applying semi-supervised techniques to RES is the prevalence of noisy pseudo-labels, particularly at the boundaries of objects. SemiRES incorporates the Segment Anything Model (SAM), renowned for its precise boundary demarcation, to improve the accuracy of these pseudo-labels. Within SemiRES, we offer two alternative matching strategies: IoU-based Optimal Matching (IOM) and Composite Parts Integration (CPI). These strategies are designed to extract the most accurate masks from SAM's output, thus guiding the training of the student model with enhanced precision. In instances where a precise mask cannot be matched from the available candidates, we develop the Pixel-Wise Adjustment (PWA) strategy, guiding the student model's training directly by the pseudo-labels. Extensive experiments on three RES benchmarks—RefCOCO, RefCOCO+, and G-Ref reveal its superior performance compared to fully supervised methods, especially in low-data scenarios. Remarkably, with only 1% labeled data, our SemiRES outperforms the supervised baseline by a large margin, e.g. +18.64% gains on RefCOCO val set",
    "checked": true,
    "id": "9af9f0c87f5333e54e322cc7b59b91c55efa3faa",
    "semantic_title": "sam as the guide: mastering pseudo-label refinement in semi-supervised referring expression segmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8q4EPdjTLE": {
    "title": "Position: Near to Mid-term Risks and Opportunities of Open-Source Generative AI",
    "volume": "oral",
    "abstract": "In the next few years, applications of Generative AI are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about potential risks and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. While regulation is important, it is key that it does not put at risk the budding field of open-source Generative AI. We argue for the responsible open sourcing of generative AI models in the near and medium term. To set the stage, we first introduce an AI openness taxonomy system and apply it to 40 current large language models. We then outline differential benefits and risks of open versus closed source AI and present potential risk mitigation, ranging from best practices to calls for technical and scientific contributions. We hope that this report will add a much needed missing voice to the current public discourse on near to mid-term AI safety and other societal impact",
    "checked": false,
    "id": "b7104e1bbeb0dbc9f00cc8fd704495cae85d11e0",
    "semantic_title": "near to mid-term risks and opportunities of open source generative ai",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=moyG54Okrj": {
    "title": "Repoformer: Selective Retrieval for Repository-Level Code Completion",
    "volume": "oral",
    "abstract": "Recent advances in retrieval-augmented generation (RAG) have initiated a new era in repository-level code completion. However, the invariable use of retrieval in existing methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). In this paper, we propose a selective RAG framework to avoid retrieval when unnecessary. To power this framework, we design a self-supervised learning approach to enable a code LM to accurately self-evaluate whether retrieval can improve its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective RAG policy and the generation model, our framework achieves state-of-the-art repository-level code completion performance on diverse benchmarks including RepoEval, CrossCodeEval, and CrossCodeLongEval, a new long-form code completion benchmark. Meanwhile, our analyses show that selectively retrieving brings as much as 70% inference speedup in the online serving setting without harming the performance. We further demonstrate that our framework is able to accommodate different generation models, retrievers, and programming languages. These advancements position our framework as an important step towards more accurate and efficient repository-level code completion",
    "checked": true,
    "id": "a12958761a391db12b0bd80e6121fc9de3b8040a",
    "semantic_title": "repoformer: selective retrieval for repository-level code completion",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=FPnUhsQJ5B": {
    "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
    "volume": "oral",
    "abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models. Stability AI is considering making experimental data, code, and model weights publicly available",
    "checked": true,
    "id": "41a66997ce0a366bba3becf7c3f37c9aebb13fbd",
    "semantic_title": "scaling rectified flow transformers for high-resolution image synthesis",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=jRX6yCxFhx": {
    "title": "Position: On the Societal Impact of Open Foundation Models",
    "volume": "oral",
    "abstract": "Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on *open* foundation models, defined here as those with broadly available model weights (e.g., Llama 3, Stable Diffusion XL). We identify five distinctive properties (e.g., greater customizability, poor monitoring) that mediate their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their *marginal risk*. Across several misuse vectors (e.g., cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work helps support a more grounded assessment of the societal impact of open foundation models by outlining what research is needed to empirically validate their theoretical benefits and risks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z5Ux2u6t7U": {
    "title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation",
    "volume": "oral",
    "abstract": "We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose framework for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents. Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency. We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control – all without ever fine-tuning the underlying model. When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models. Sound examples can be found at https://ditto-music.github.io/web/",
    "checked": true,
    "id": "1868aa487f8be8c80eeb16a7d86ca61e86007dd9",
    "semantic_title": "ditto: diffusion inference-time t-optimization for music generation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=n3yYrtt9U7": {
    "title": "Parameterized Physics-informed Neural Networks for Parameterized PDEs",
    "volume": "oral",
    "abstract": "Complex physical systems are often described by partial differential equations (PDEs) that depend on parameters such as the Raynolds number in fluid mechanics. In applications such as design optimization or uncertainty quantification, solutions of those PDEs need to be evaluated at numerous points in the parameter space. While physics-informed neural networks (PINNs) have emerged as a new strong competitor as a surrogate, their usage in this scenario remains underexplored due to the inherent need for repetitive and time-consuming training. In this paper, we address this problem by proposing a novel extension, parameterized physics-informed neural networks (P$^2$INNs). P$^2$INNs enable modeling the solutions of parameterized PDEs via explicitly encoding a latent representation of PDE parameters. With the extensive empirical evaluation, we demonstrate that P$^2$INNs outperform the baselines both in accuracy and parameter efficiency on benchmark 1D and 2D parameterized PDEs and are also effective in overcoming the known \"failure modes\"",
    "checked": false,
    "id": "96d251208ff1ca159231a6a02c2d03c3e36f45c5",
    "semantic_title": "physics-informed neural networks for solving parametric magnetostatic problems",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=cEJ9jNJuJP": {
    "title": "Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems",
    "volume": "oral",
    "abstract": "Recent advancements in solving large-scale traveling salesman problems (TSP) utilize the heatmap-guided Monte Carlo tree search (MCTS) paradigm, where machine learning (ML) models generate heatmaps, indicating the probability distribution of each edge being part of the optimal solution, to guide MCTS in solution finding. However, our theoretical and experimental analysis raises doubts about the effectiveness of ML-based heatmap generation. In support of this, we demonstrate that a simple baseline method can outperform complex ML approaches in heatmap generation. Furthermore, we question the practical value of the heatmap-guided MCTS paradigm. To substantiate this, our findings show its inferiority to the LKH-3 heuristic despite the paradigm's reliance on problem-specific, hand-crafted strategies. For the future, we suggest research directions focused on developing more theoretically sound heatmap generation methods and exploring autonomous, generalizable ML approaches for combinatorial problems. The code is available for review: https://github.com/xyfffff/rethink_mcts_for_tsp",
    "checked": true,
    "id": "f208a76d3dd451927c60714198b97faf888ba942",
    "semantic_title": "position: rethinking post-hoc search-based neural approaches for solving large-scale traveling salesman problems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4jqOV6NlUz": {
    "title": "Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation",
    "volume": "oral",
    "abstract": "We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model",
    "checked": true,
    "id": "55c3095681acc82780508b0e484dba0c30cf1caa",
    "semantic_title": "automated evaluation of retrieval-augmented language models with task-specific exam generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=p225Od0aYt": {
    "title": "PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control",
    "volume": "oral",
    "abstract": "Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to bear on the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the learning performance of behavior cloning on downstream tasks",
    "checked": true,
    "id": "600fce8b16708755a331ba7122b9a0600f8890e1",
    "semantic_title": "prise: llm-style sequence compression for learning temporal action abstractions in control",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HPXRzM9BYZ": {
    "title": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies",
    "volume": "oral",
    "abstract": "We tackle the challenge of predicting models' Out-of-Distribution (OOD) performance using in-distribution (ID) measurements without requiring OOD data. Existing evaluations with ``Effective robustness'', which use ID accuracy as an indicator of OOD accuracy, encounter limitations when models are trained with diverse supervision and distributions, such as class labels (*Vision Models, VMs, on ImageNet*) and textual descriptions (*Visual-Language Models, VLMs, on LAION*). VLMs often generalize better to OOD data than VMs despite having similar or lower ID performance. To improve the prediction of models' OOD performance from ID measurements, we introduce the *Lowest Common Ancestor (LCA)-on-the-Line* framework. This approach revisits the established concept of LCA distance, which measures the hierarchical distance between labels and predictions within a predefined class hierarchy, such as WordNet. We assess 75 models using ImageNet as the ID dataset and five significantly shifted OOD variants, uncovering a strong linear correlation between ID LCA distance and OOD top-1 accuracy. Our method provides a compelling alternative for understanding why VLMs tend to generalize better. Additionally, we propose a technique to construct a taxonomic hierarchy on any dataset using $K$-means clustering, demonstrating that LCA distance is robust to the constructed taxonomic hierarchy. Moreover, we demonstrate that aligning model predictions with class taxonomies, through soft labels or prompt engineering, can enhance model generalization. Open source code in our [Project Page](https://elvishelvis.github.io/papers/lca/)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKMcCtWC7H": {
    "title": "Active Statistical Inference",
    "volume": "oral",
    "abstract": "Inspired by the concept of active learning, we propose active inference---a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident. Active inference constructs valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful tests. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics",
    "checked": true,
    "id": "9c8436967a7547888b9734a1a66191a0da0d7c64",
    "semantic_title": "active statistical inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=g8AigOTNXL": {
    "title": "Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion",
    "volume": "oral",
    "abstract": "We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quality and rule-based controllability, outperforming current state-of-the-art generators in a variety of settings. For detailed demonstrations, code and model checkpoints, please visit our [project website](https://scg-rule-guided-music.github.io/)",
    "checked": true,
    "id": "ee9ed60f41a0fe1c515c2e4d348ae4840f9f7e3e",
    "semantic_title": "symbolic music generation with non-differentiable rule guided diffusion",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=dVhrnjZJad": {
    "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
    "volume": "oral",
    "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall shorts in speech quality, similarity, and prosody. Considering that speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model, which generates attributes in each subspace following its corresponding prompt. With this factorization design, our method can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way. Experimental results show that our method outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility",
    "checked": true,
    "id": "575bf1f23a19620a8f696a965528e15d4833179a",
    "semantic_title": "naturalspeech 3: zero-shot speech synthesis with factorized codec and diffusion models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=1QmFKwVwwI": {
    "title": "Privacy Preserving Adaptive Experiment Design",
    "volume": "oral",
    "abstract": "Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power of CATE estimation in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimality condition. Furthermore, we propose differentially private algorithms which still matches the lower bound, showing that privacy is \"almost free\". Additionally, we derive the asymptotic normality of the estimator, which is essential in statistical inference and hypothesis testing",
    "checked": true,
    "id": "9689240eda8359d6e26af6a8035bd0333f247a5a",
    "semantic_title": "privacy preserving adaptive experiment design",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WsawczEqO6": {
    "title": "Position: Do pretrained Transformers Learn In-Context by Gradient Descent?",
    "volume": "oral",
    "abstract": "The emergence of In-Context Learning (ICL) in LLMs remains a remarkable phenomenon that is partially understood. To explain ICL, recent studies have created theoretical connections to Gradient Descent (GD). We ask, do such connections hold up in actual pre-trained language models? We highlight the limiting assumptions in prior works that make their setup considerably different from the practical setup in which language models are trained. For example, their experimental verification uses *ICL objective* (training models explicitly for ICL), which differs from the emergent ICL in the wild. Furthermore, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that *the equivalence between ICL and GD remains an open hypothesis* and calls for further studies",
    "checked": false,
    "id": "f30ddf0c7455f89f016c540564e235b191c503db",
    "semantic_title": "do pretrained transformers learn in-context by gradient descent?",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=7dP6Yq9Uwv": {
    "title": "Learning to Model the World With Language",
    "volume": "oral",
    "abstract": "To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents can learn to execute simple language instructions, we aim to build agents that leverage diverse language---language like \"this button turns on the TV\" or \"I put the bowls away\"---that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that *agents should interpret such diverse language as a signal that helps them predict the future*: what they will observe, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. While current methods that learn language-conditioned policies degrade in performance with more diverse types of language, we show that Dynalang learns to leverage environment descriptions, game rules, and instructions to excel on tasks ranging from game-playing to navigating photorealistic home scans. Finally, we show that our method enables additional capabilities due to learning a generative model: Dynalang can be pretrained on text-only data, enabling learning from offline datasets, and generate language grounded in an environment",
    "checked": true,
    "id": "a5cddee937d7d2f005e781e453833cd64d3cf343",
    "semantic_title": "learning to model the world with language",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ZwUThOE7Zc": {
    "title": "Position: AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research",
    "volume": "oral",
    "abstract": "The recent embrace of machine learning (ML) in the development of autonomous weapons systems (AWS) creates serious risks to geopolitical stability and the free exchange of ideas in AI research. This topic has received comparatively little attention of late compared to risks stemming from superintelligent artificial general intelligence (AGI), but requires fewer assumptions about the course of technological development and is thus a nearer-future issue. ML is already enabling the substitution of AWS for human soldiers in many battlefield roles, reducing the upfront human cost, and thus political cost, of waging offensive war. In the case of peer adversaries, this increases the likelihood of \"low intensity\" conflicts which risk escalation to broader warfare. In the case of non-peer adversaries, it reduces the domestic blowback to wars of aggression. This effect can occur regardless of other ethical issues around the use of military AI such as the risk of civilian casualties, and does not require any superhuman AI capabilities. Further, the military value of AWS raises the specter of an AI-powered arms race and the misguided imposition of national security restrictions on AI research. Our goal in this paper is to raise awareness among the public and ML researchers on the near-future risks posed by full or near-full autonomy in military technology, and we provide regulatory suggestions to mitigate these risks. We call upon AI policy experts and the defense AI community in particular to embrace transparency and caution in their development and deployment of AWS to avoid the negative effects on global stability and AI research that we highlight here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UpSe7ag34v": {
    "title": "Arrows of Time for Large Language Models",
    "volume": "oral",
    "abstract": "We study the probabilistic modeling performed by Autoregressive Large Language Models (LLMs) through the angle of time directionality, addressing a question first raised in (Shannon, 1951). For large enough models, we empirically find a time asymmetry in their ability to learn natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results",
    "checked": true,
    "id": "5339f21241f64f76a0a891888fb1796f9aede7d1",
    "semantic_title": "arrows of time for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=vKtomqlSxm": {
    "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
    "volume": "oral",
    "abstract": "Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter – we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for \"detect_sarcasm(string)\" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively \"emulate\" the interpreter by generating the expected output of \"detect_sarcasm(string)\". In this work, we propose Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an \"LMulator\"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by \"thinking in code\"",
    "checked": true,
    "id": "3a56bc074b8f3f985599627404b70e16fc5bce1b",
    "semantic_title": "chain of code: reasoning with a language model-augmented code emulator",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=mJGiFr8jLa": {
    "title": "Challenges in Training PINNs: A Loss Landscape Perspective",
    "volume": "oral",
    "abstract": "This paper explores challenges in training Physics-Informed Neural Networks (PINNs), emphasizing the role of the loss landscape in the training process. We examine difficulties in minimizing the PINN loss function, particularly due to ill-conditioning caused by differential operators in the residual term. We compare gradient-based optimizers Adam, L-BFGS, and their combination Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN performance. Theoretically, our work elucidates the connection between ill-conditioned differential operators and ill-conditioning in the PINN loss and shows the benefits of combining first- and second-order optimization methods. Our work presents valuable insights and more powerful optimization strategies for training PINNs, which could improve the utility of PINNs for solving difficult partial differential equations",
    "checked": true,
    "id": "658c5711c6d07bfe2b9514ce22411e57a8a0ff1a",
    "semantic_title": "challenges in training pinns: a loss landscape perspective",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=KviM5k8pcP": {
    "title": "AI Control: Improving Safety Despite Intentional Subversion",
    "volume": "oral",
    "abstract": "As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. To do so, safety measures either aim at making LLMs try to avoid harmful outcomes or aim at preventing LLMs from causing harmful outcomes, even if they try to cause them. In this paper, we focus on this second layer of defense. We develop and evaluate pipelines of safety techniques (protocols) that try to ensure safety despite intentional subversion - an approach we call AI control. We investigate a setting in which we want to solve a sequence of programming problems without ever submitting subtly wrong code, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate a range of protocols and red-team them by exploring strategies that the untrusted model could use to subvert them. We find that using the trusted model to edit untrusted-model code or using the untrusted model as a monitor substantially improves on simple baselines",
    "checked": true,
    "id": "fbd13beeb4706fb3b07b8bd22a16f0504683fcf3",
    "semantic_title": "ai control: improving safety despite intentional subversion",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=sb81Xl50JG": {
    "title": "APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference",
    "volume": "oral",
    "abstract": "Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively *prunes* and *tunes* parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds *salient* tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remaining. Furthermore, APT speeds up LMs' fine-tuning by up to 8$\\times$ and reduces large LMs' memory training footprint by up to 70%. Our code and models are publicly available at https://github.com/ROIM1998/APT",
    "checked": true,
    "id": "993611f5d9f1585c861c9806f8df84d2cdbb19d0",
    "semantic_title": "apt: adaptive pruning and tuning pretrained language models for efficient training and inference",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=tl2qmO5kpD": {
    "title": "Offline Actor-Critic Reinforcement Learning Scales to Large Models",
    "volume": "oral",
    "abstract": "We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset; containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data",
    "checked": true,
    "id": "911f28279f8f80f1520181ecc4e839c54408a68c",
    "semantic_title": "offline actor-critic reinforcement learning scales to large models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=jOlO8t1xdx": {
    "title": "Fast Timing-Conditioned Latent Audio Diffusion",
    "volume": "oral",
    "abstract": "Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. It is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. The generative model is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. It is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, the proposed model is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds",
    "checked": true,
    "id": "c281c4d6f9c60b424dcc86ca27807afd3c9cdc94",
    "semantic_title": "fast timing-conditioned latent audio diffusion",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Zc22RDtsvP": {
    "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
    "volume": "oral",
    "abstract": "Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent works leverage text instructions to allow users to more freely express their search intents. However, they primarily focus on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions via foundation models. Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relations mined from the web, MagicLens achieves results comparable with or better than prior best on eight benchmarks of various image retrieval tasks, while maintaining high parameter efficiency with a significantly smaller model size. Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens. Code and models are publicly available at the https://open-vision-language.github.io/MagicLens/",
    "checked": true,
    "id": "3c19452f6d55778a05cc529a9f5fbc77be59df3a",
    "semantic_title": "magiclens: self-supervised image retrieval with open-ended instructions",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=3ajK5xplDL": {
    "title": "Making Old Things New: A Unified Algorithm for Differentially Private Clustering",
    "volume": "oral",
    "abstract": "As a staple of data analysis and unsupervised learning, the problem of private clustering has been widely studied, under various privacy models. Centralized differential privacy is the first of them, and the problem has also been studied for the local and the shuffle variation. In each case, the goal is to design an algorithm that computes privately a clustering, with the smallest possible error. The study of each variation gave rise to new algorithm: the landscape of private clustering algorithm is therefore quite intricate. In this paper, we show that a 20 year-old algorithm can be slightly modified to work for any of those models. This provides a unified picture: while matching almost all previously known results, it allows us to improve some of them, and extend to a new privacy model, the continual observation setting, where the input is changing over time and the algorithm must output a new solution at each time step",
    "checked": true,
    "id": "490a7db4816a68458e1c9c1451d6aa4d58545409",
    "semantic_title": "making old things new: a unified algorithm for differentially private clustering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CfOtiepP8s": {
    "title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
    "volume": "oral",
    "abstract": "Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations. However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remains mysterious, making it challenging to ensure reliability. In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations. Through comprehensive experiments, we find that LLMs frequently involve a small fraction (<5%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes. Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution. These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks. This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance. We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks. Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=54NSHO0lFe": {
    "title": "SparseTSF: Modeling Long-term Time Series Forecasting with *1k* Parameters",
    "volume": "oral",
    "abstract": "This paper introduces SparseTSF, a novel, extremely lightweight model for Long-term Time Series Forecasting (LTSF), designed to address the challenges of modeling complex temporal dependencies over extended horizons with minimal computational resources. At the heart of SparseTSF lies the Cross-Period Sparse Forecasting technique, which simplifies the forecasting task by decoupling the periodicity and trend in time series data. This technique involves downsampling the original sequences to focus on cross-period trend prediction, effectively extracting periodic features while minimizing the model's complexity and parameter count. Based on this technique, the SparseTSF model uses fewer than *1k* parameters to achieve competitive or superior performance compared to state-of-the-art models. Furthermore, SparseTSF showcases remarkable generalization capabilities, making it well-suited for scenarios with limited computational resources, small samples, or low-quality data. The code is publicly available at this repository: https://github.com/lss-1138/SparseTSF",
    "checked": false,
    "id": "896c83e6dbd42621cb5c084bfc477802ed198b14",
    "semantic_title": "sparsetsf: modeling long-term time series forecasting with 1k parameters",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Xdy9bjwHDu": {
    "title": "On the Last-Iterate Convergence of Shuffling Gradient Methods",
    "volume": "oral",
    "abstract": "Shuffling gradient methods are widely used in modern machine learning tasks and include three popular implementations: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understood for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove the first last-iterate convergence rates for shuffling gradient methods with respect to the objective value even without strong convexity. Our new results either (nearly) match the existing last-iterate lower bounds or are as fast as the previous best upper bounds for the average iterate",
    "checked": true,
    "id": "88bfb05ffb6d41c7c95d5a8dbc12afb12ace3db9",
    "semantic_title": "on the last-iterate convergence of shuffling gradient methods",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rs8Sh2UASt": {
    "title": "AlphaFold Meets Flow Matching for Generating Protein Ensembles",
    "volume": "oral",
    "abstract": "The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditioned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at https://github.com/bjing2016/alphaflow",
    "checked": true,
    "id": "8136c9a5915cee9bf332e0969719dd4884f7c673",
    "semantic_title": "alphafold meets flow matching for generating protein ensembles",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=sT7UJh5CTc": {
    "title": "Low-Cost High-Power Membership Inference Attacks",
    "volume": "oral",
    "abstract": "Membership inference attacks aim to detect if a particular data point was used in training a model. We design a novel statistical test to perform robust membership inference attacks (RMIA) with low computational overhead. We achieve this by a fine-grained modeling of the null hypothesis in our likelihood ratio tests, and effectively leveraging both reference models and reference population data samples. RMIA has superior test power compared with prior methods, throughout the TPR-FPR curve (even at extremely low FPR, as low as 0). Under computational constraints, where only a limited number of pre-trained reference models (as few as 1) are available, and also when we vary other elements of the attack (e.g., data distribution), our method performs exceptionally well, unlike prior attacks that approach random guessing. RMIA lays the groundwork for practical yet accurate data privacy risk assessment in machine learning",
    "checked": true,
    "id": "c74f7c2e906f1f12707735d40d810472406d5a84",
    "semantic_title": "low-cost high-power membership inference attacks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=tFEOOH9eH0": {
    "title": "A Touch, Vision, and Language Dataset for Multimodal Alignment",
    "volume": "oral",
    "abstract": "Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild visiontouch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-visionlanguage (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) tactile-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code, checkpoints and data are available on https: //tactile-vlm.github.io",
    "checked": true,
    "id": "c4a1d57011307c575bfa6f41d0afe9dc75fed10b",
    "semantic_title": "a touch, vision, and language dataset for multimodal alignment",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=jsKr6RVDDs": {
    "title": "Position: Measure Dataset Diversity, Don't Just Claim It",
    "volume": "oral",
    "abstract": "Machine learning (ML) datasets, often perceived as neutral, inherently encapsulate abstract and disputed social constructs. Dataset curators frequently employ value-laden terms such as diversity, bias, and quality to characterize datasets. Despite their prevalence, these terms lack clear definitions and validation. Our research explores the implications of this issue by analyzing \"diversity\" across 135 image and text datasets. Drawing from social sciences, we apply principles from measurement theory to identify considerations and offer recommendations for conceptualizing, operationalizing, and evaluating diversity in datasets. Our findings have broader implications for ML research, advocating for a more nuanced and precise approach to handling value-laden properties in dataset construction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dV9B9qFeGi": {
    "title": "Contrasting Multiple Representations with the Multi-Marginal Matching Gap",
    "volume": "oral",
    "abstract": "Learning meaningful representations of complex objects that can be seen through multiple ($k\\geq 3$) views or modalities is a core task in machine learning. Existing methods use losses originally intended for paired views, and extend them to $k$ views, either by instantiating $\\tfrac12k(k-1)$ loss-pairs, or by using reduced embeddings, following a *one vs. average-of-rest* strategy. We propose the multi-marginal matching gap (M3G), a loss that borrows tools from multi-marginal optimal transport (MM-OT) theory to simultaneously incorporate all $k$ views. Given a batch of $n$ points, each seen as a $k$-tuple of views subsequently transformed into $k$ embeddings, our loss contrasts the cost of matching these $n$ ground-truth $k$-tuples with the MM-OT polymatching cost, which seeks $n$ optimally arranged $k$-tuples chosen within these $n\\times k$ vectors. While the exponential complexity $O(n^k$) of the MM-OT problem may seem daunting, we show in experiments that a suitable generalization of the Sinkhorn algorithm for that problem can scale to, e.g., $k=3\\sim 6$ views using mini-batches of size $64~\\sim128$. Our experiments demonstrate improved performance over multiview extensions of pairwise losses, for both self-supervised and multimodal tasks",
    "checked": true,
    "id": "5d436fff942dcd343f142808fe9ee53396c0ba53",
    "semantic_title": "contrasting multiple representations with the multi-marginal matching gap",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=sBJNokmYuV": {
    "title": "Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data",
    "volume": "oral",
    "abstract": "Fine-tuning vision-language models (VLMs) with abundant unlabeled data recently has attracted increasing attention. Existing methods that resort to the pseudolabeling strategy would suffer from heavily incorrect hard pseudolabels when VLMs exhibit low zero-shot performance in downstream tasks. To alleviate this issue, we propose a **C**andidate **P**seudolabel **L**earning method, termed **CPL**, to fine-tune VLMs with suitable candidate pseudolabels of unlabeled data in downstream tasks. The core of our method lies in the generation strategy of candidate pseudolabels, which progressively generates refined candidate pseudolabels by both intra- and inter-instance label selection, based on a confidence score matrix for all unlabeled data. This strategy can result in better performance in true label inclusion and class-balanced instance selection. In this way, we can directly apply existing loss functions to learn with generated candidate psueudolabels. Extensive experiments on nine benchmark datasets with three learning paradigms demonstrate the effectiveness of our method. Our code can be found here",
    "checked": true,
    "id": "ae6f6d8544d4d8dc46c98ca32e0ea6178c982f79",
    "semantic_title": "candidate pseudolabel learning: enhancing vision-language models by prompt tuning with unlabeled data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZvFLbEPv6x": {
    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline",
    "volume": "oral",
    "abstract": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs",
    "checked": false,
    "id": "86307d23d25cefe451a53fd76ddbe5798b592c3b",
    "semantic_title": "the stronger the diffusion model, the easier the backdoor: data poisoning to induce copyright breaches without adjusting finetuning pipeline",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=VE3yWXt3KB": {
    "title": "Stealing part of a production language model",
    "volume": "oral",
    "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under $20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the GPT-3.5-turbo model, and estimate it would cost under \\\\$2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack",
    "checked": true,
    "id": "b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
    "semantic_title": "stealing part of a production language model",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=qY622O6Ehg": {
    "title": "Pausing Policy Learning in Non-stationary Reinforcement Learning",
    "volume": "oral",
    "abstract": "Real-time inference is a challenge of real-world reinforcement learning due to temporal differences in time-varying environments: the system collects data from the past, updates the decision model in the present, and deploys it in the future. We tackle a common belief that continually updating the decision is optimal to minimize the temporal gap. We propose forecasting an online reinforcement learning framework and show that strategically pausing decision updates yields better overall performance by effectively managing aleatoric uncertainty. Theoretically, we compute an optimal ratio between policy update and hold duration, and show that a non-zero policy hold duration provides a sharper upper bound on the dynamic regret. Our experimental evaluations on three different environments also reveal that a non-zero policy hold duration yields higher rewards compared to continuous decision updates",
    "checked": true,
    "id": "0e355c7dec9b51fe8c3a12469b031ef337e39715",
    "semantic_title": "pausing policy learning in non-stationary reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iLCZtl7FTa": {
    "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
    "volume": "oral",
    "abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is *debate*, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth",
    "checked": true,
    "id": "dca0aa58f257da06d8b506ae78ab842f60982b17",
    "semantic_title": "debating with more persuasive llms leads to more truthful answers",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=EqFxIbGWRU": {
    "title": "Probabilistic Generating Circuits - Demystified",
    "volume": "oral",
    "abstract": "Zhang et al. (ICML 2021, PLMR 139, pp. 12447–12457) introduced probabilistic generating circuits (PGCs) as a probabilistic model to unify probabilistic circuits (PCs) and determinantal point processes (DPPs). At a first glance, PGCs store a distribution in a very different way, they compute the probability generating polynomial instead of the probability mass function and it seems that this is the main reason why PGCs are more powerful than PCs or DPPs. However, PGCs also allow for negative weights, whereas classical PCs assume that all weights are nonnegative. One main insight of this work is that the negative weights are the cause for the power of PGCs and not the different representation. PGCs are PCs in disguise: we show how to transform any PGC on binary variables into a PC with negative weights with only polynomial blowup. PGCs were defined by Zhang et al. only for binary random variables. As our second main result, we show that there is a good reason for this: we prove that PGCs for categorical variables with larger image size do not support tractable marginalization unless NP=P. On the other hand, we show that we can model categorical variables with larger image size as PC with negative weights computing set-multilinear polynomials. These allow for tractable marginalization. In this sense, PCs with negative weights strictly subsume PGCs",
    "checked": true,
    "id": "ca724adee2c54c66db260885217a5d62f478130d",
    "semantic_title": "probabilistic generating circuits - demystified",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f5gtX2VWSB": {
    "title": "Self-Composing Policies for Scalable Continual Reinforcement Learning",
    "volume": "oral",
    "abstract": "This work introduces a growable and modular neural network architecture that naturally avoids catastrophic forgetting and interference in continual reinforcement learning. The structure of each module allows the selective combination of previous policies along with its internal policy accelerating the learning process on the current task. Unlike previous growing neural network approaches, we show that the number of parameters of the proposed approach grows linearly with respect to the number of tasks, and does not sacrifice plasticity to scale. Experiments conducted in benchmark continuous control and visual problems reveal that the proposed approach achieves greater knowledge transfer and performance than alternative methods",
    "checked": false,
    "id": "f61d428eeb7d81ca0c6ccec0ddcb30503db282ec",
    "semantic_title": "active predictive coding: a unified neural framework for learning hierarchical world models for perception and planning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=jTn4AIOgpM": {
    "title": "Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference",
    "volume": "oral",
    "abstract": "Deep Gaussian processes (DGPs) provide a robust paradigm in Bayesian deep learning. In DGPs, a set of sparse integration locations called inducing points are selected to approximate the posterior distribution of the model. This is done to reduce computational complexity and improve model efficiency. However, inferring the posterior distribution of inducing points is not straightforward. Traditional variational inference techniques methods to approximate the posterior often leads to significant bias. To address this issue, we propose an alternative named Denoising Diffusion Variational Inference (DDVI) that utilizes a denoising diffusion stochastic differential equation (SDE) for generating posterior samples of inducing variables. We refer to the score matching method in the denoising diffusion model to approximate challenging score functions using a neural network. Furthermore, by combining classical mathematical theory of SDE with the minimization of KL divergence between the approximate and true processes, we propose a novel explicit variational lower bound for the marginal likelihood function of DGP. Through extensive experiments on various datasets and comparisons with baseline methods, we empirically demonstrate the effectiveness of the DDVI method in posterior inference of inducing points for DGP models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eZiQWM5U0E": {
    "title": "Optimal Hessian/Jacobian-Free Nonconvex-PL Bilevel Optimization",
    "volume": "oral",
    "abstract": "Bilevel optimization is widely applied in many machine learning tasks such as hyper-parameter learning, meta learning and reinforcement learning. Although many algorithms recently have been developed to solve the bilevel optimization problems, they generally rely on the (strongly) convex lower-level problems. More recently, some methods have been proposed to solve the nonconvex-PL bilevel optimization problems, where their upper-level problems are possibly nonconvex, and their lower-level problems are also possibly nonconvex while satisfying Polyak-Łojasiewicz (PL) condition. However, these methods still have a high convergence complexity or a high computation complexity such as requiring compute expensive Hessian/Jacobian matrices and its inverses. In the paper, thus, we propose an efficient Hessian/Jacobian-free method (i.e., HJFBiO) with the optimal convergence complexity to solve the nonconvex-PL bilevel problems. Theoretically, under some mild conditions, we prove that our HJFBiO method obtains an optimal convergence rate of $O(\\frac{1}{T})$, where $T$ denotes the number of iterations, and has an optimal gradient complexity of $O(\\epsilon^{-1})$ in finding an $\\epsilon$-stationary solution. We conduct some numerical experiments on the bilevel PL game and hyper-representation learning task to demonstrate efficiency of our proposed method",
    "checked": false,
    "id": "a7f8d6d486ba9b56448fda3ada10e1dd0f8e8a3f",
    "semantic_title": "achieving ${o}(\\epsilon^{-1.5})$ complexity in hessian/jacobian-free stochastic bilevel optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6SRXG92s6": {
    "title": "LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering",
    "volume": "oral",
    "abstract": "Graph clustering is a fundamental problem in machine learning. Deep learning methods achieve the state-of-the-art results in recent years, but they still cannot work without predefined cluster numbers. Such limitation motivates us to pose a more challenging problem of graph clustering with unknown cluster number. We propose to address this problem from a fresh perspective of graph information theory (i.e., structural information). In the literature, structural information has not yet been introduced to deep clustering, and its classic definition falls short of discrete formulation and modeling node features. In this work, we first formulate a differentiable structural information (DSI) in the continuous realm, accompanied by several theoretical results. By minimizing DSI, we construct the optimal partitioning tree where densely connected nodes in the graph tend to have the same assignment, revealing the cluster struc- ture. DSI is also theoretically presented as a new graph clustering objective, not requiring the pre-defined cluster number. Furthermore, we design a neural LSEnet in the Lorentz model of hyperbolic space, where we integrate node features to structural information via manifold-valued graph convolution. Extensive empirical results on real graphs show the superiority of our approach",
    "checked": true,
    "id": "9ff579d85ba330bfeaaa63eb39150793b6d70f40",
    "semantic_title": "lsenet: lorentz structural entropy neural network for deep graph clustering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LRkJwPIDuE": {
    "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
    "volume": "oral",
    "abstract": "We present VideoPoet, a language model capable of synthesizing high-quality video from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting the ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/",
    "checked": true,
    "id": "0c4f46e4dcae5527018e6432fb60cfe8c3354e97",
    "semantic_title": "videopoet: a large language model for zero-shot video generation",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=S9lk6dk4LL": {
    "title": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization",
    "volume": "oral",
    "abstract": "In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models are available at https://video-lavit.github.io",
    "checked": true,
    "id": "c1b5195bc09a2232ec2b69e5a2a6bd39b3162c62",
    "semantic_title": "video-lavit: unified video-language pre-training with decoupled visual-motional tokenization",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=ncjhi4qAPV": {
    "title": "Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining",
    "volume": "oral",
    "abstract": "The performance of differentially private machine learning can be boosted significantly by leveraging the transfer learning capabilities of non-private models pretrained on large *public* datasets. We critically review this approach. We primarily question whether the use of large Web-scraped datasets *should* be viewed as differential-privacy-preserving. We further scrutinize whether existing machine learning benchmarks are appropriate for measuring the ability of pretrained models to generalize to sensitive domains. Finally, we observe that reliance on large pretrained models may lose *other* forms of privacy, requiring data to be outsourced to a more compute-powerful third party",
    "checked": false,
    "id": "7932b714e2ae1def5828df52b97f1decb9bebd32",
    "semantic_title": "considerations for differentially private learning with large-scale public pretraining",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=Mfk6ZbD6eY": {
    "title": "Stereo Risk: A Continuous Modeling Approach to Stereo Matching",
    "volume": "oral",
    "abstract": "We introduce Stereo Risk, a new deep-learning approach to solve the classical stereo-matching problem in computer vision. As it is well-known that stereo matching boils down to a per-pixel disparity estimation problem, the popular state-of-the-art stereo-matching approaches widely rely on regressing the scene disparity values, yet via discretization of scene disparity values. Such discretization often fails to capture the nuanced, continuous nature of scene depth. Stereo Risk departs from the conventional discretization approach by formulating the scene disparity as an optimal solution to a continuous risk minimization problem, hence the name \"stereo risk\". We demonstrate that $L^1$ minimization of the proposed continuous risk function enhances stereo-matching performance for deep networks, particularly for disparities with multi-modal probability distributions. Furthermore, to enable the end-to-end network training of the non-differentiable $L^1$ risk optimization, we exploited the implicit function theorem, ensuring a fully differentiable network. A comprehensive analysis demonstrates our method's theoretical soundness and superior performance over the state-of-the-art methods across various benchmark datasets, including KITTI 2012, KITTI 2015, ETH3D, SceneFlow, and Middlebury 2014",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PxHmxoFOgI": {
    "title": "Zeroth-Order Methods for Constrained Nonconvex Nonsmooth Stochastic Optimization",
    "volume": "oral",
    "abstract": "This paper studies the problem of solving nonconvex nonsmooth optimization over a closed convex set. Most previous works tackle such problems by transforming the constrained problem into an unconstrained problem that can be solved by the techniques developed in the unconstrained setting. However, they only provide asymptotic convergence analysis for their methods. In this work, we provide the non-asymptotic analysis for solving constrained nonconvex nonsmooth optimization. We first generalize classical gradient mapping and the Frank–Wolfe gap in the nonsmooth setting. Then we introduce novel notions of approximate stationarity concerning such generalized quantities. We also propose several stochastic zeroth-order algorithms for the problem, along with their non-asymptotic convergence guarantees of obtaining the proposed approximate stationarity. Finally, we conduct numerical experiments that demonstrate the effectiveness of our algorithms",
    "checked": false,
    "id": "abc97ab3a8fa08172d53596c2018790d18de250c",
    "semantic_title": "zeroth-order gradient and quasi-newton methods for nonsmooth nonconvex stochastic optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CyEJn71Z00": {
    "title": "Information Complexity of Stochastic Convex Optimization: Applications to Generalization, Memorization, and Tracing",
    "volume": "oral",
    "abstract": "In this work, we investigate the interplay between memorization and learning in the context of *stochastic convex optimization* (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\\epsilon$ has CMI bounded below by $\\Omega(1/\\epsilon^2)$ and $\\Omega(1/\\epsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems. Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems",
    "checked": false,
    "id": "d6e2803b933e32f6f0e932a5d99e7dca08ba6666",
    "semantic_title": "information complexity of stochastic convex optimization: applications to generalization, compression, and tracing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JSYN891WnB": {
    "title": "Image Clustering with External Guidance",
    "volume": "oral",
    "abstract": "The core of clustering lies in incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance the feature discriminability. Then, TAC collaborates text and image modalities by mutually distilling cross-modal neighborhood information. Experiments demonstrate that TAC achieves state-of-the-art performance on five widely used and three more challenging image clustering benchmarks, including the full ImageNet-1K dataset. The code can be accessed at https://github.com/XLearning-SCU/2024-ICML-TAC",
    "checked": true,
    "id": "0d1b784a6f440d13d4f8d85f1a66ef3dde904067",
    "semantic_title": "image clustering with external guidance",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dBqHGZPGZI": {
    "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity",
    "volume": "oral",
    "abstract": "While alignment algorithms are commonly used to tune pre-trained language models towards user preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in pre-trained language models (GPT2-medium, Llama2-7b). We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting models avert toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the models, reverting them back to their toxic behavior",
    "checked": true,
    "id": "26b2adbe089ea36617c3ec0aa009319929da0550",
    "semantic_title": "a mechanistic understanding of alignment algorithms: a case study on dpo and toxicity",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=1vGN3CSxVs": {
    "title": "EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction",
    "volume": "oral",
    "abstract": "Predicting the binding sites of target proteins plays a fundamental role in drug discovery. Most existing deep-learning methods consider a protein as a 3D image by spatially clustering its atoms into voxels and then feed the voxelized protein into a 3D CNN for prediction. However, the CNN-based methods encounter several critical issues: 1) defective in representing irregular protein structures; 2) sensitive to rotations; 3) insufficient to characterize the protein surface; 4) unaware of protein size shift. To address the above issues, this work proposes EquiPocket, an E(3)-equivariant Graph Neural Network (GNN) for binding site prediction, which comprises three modules: the first one to extract local geometric information for each surface atom, the second one to model both the chemical and spatial structure of protein and the last one to capture the geometry of the surface via equivariant message passing over the surface atoms. We further propose a dense attention output layer to alleviate the effect incurred by variable protein size. Extensive experiments on several representative benchmarks demonstrate the superiority of our framework to the state-of-the-art methods",
    "checked": true,
    "id": "97b2c62fe4dacae931e2df1eefa633da81a79108",
    "semantic_title": "equipocket: an e(3)-equivariant geometric graph neural network for ligand binding site prediction",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=ecnpYYHjt9": {
    "title": "Speech Self-Supervised Learning Using Diffusion Model Synthetic Data",
    "volume": "oral",
    "abstract": "While self-supervised learning (SSL) in speech has greatly reduced the reliance of speech processing systems on annotated corpora, the success of SSL still hinges on the availability of a large-scale unannotated corpus, which is still often impractical for many low-resource languages or under privacy concerns. Some existing work seeks to alleviate the problem by data augmentation, but most works are confined to introducing perturbations to real speech and do not introduce new variations in speech prosody, speakers, and speech content, which are important for SSL. Motivated by the recent finding that diffusion models have superior capabilities for modeling data distributions, we propose DiffS4L, a pretraining scheme that augments the limited unannotated data with synthetic data with different levels of variations, generated by a diffusion model trained on the limited unannotated data. Finally, an SSL model is pre-trained on the real and the synthetic speech. Our experiments show that DiffS4L can significantly improve the performance of SSL models, such as reducing the WER of the HuBERT pretrained model by 6.26 percentage points in the English ASR task. Notably, we find that the synthetic speech with all levels of variations, i.e. new prosody, new speakers, and even new content (despite the new content being mostly babble), accounts for significant performance improvement. The code is available at github.com/Hertin/DiffS4L",
    "checked": false,
    "id": "8ddebb0f126c38167bb00cd6e984bf6c7c1d0ca1",
    "semantic_title": "on the effectiveness of speech self-supervised learning for music",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=BH8TYy0r6u": {
    "title": "Position: The Platonic Representation Hypothesis",
    "volume": "oral",
    "abstract": "We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M8UbECx485": {
    "title": "Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks",
    "volume": "oral",
    "abstract": "An increasingly popular machine learning paradigm is to pretrain a neural network (NN) on many tasks offline, then adapt it to downstream tasks, often by re-training only the last linear layer of the network. This approach yields strong downstream performance in a variety of contexts, demonstrating that multitask pretraining leads to effective feature learning. Although several recent theoretical studies have shown that shallow NNs learn meaningful features when either (i) they are trained on a *single* task or (ii) they are *linear*, very little is known about the closer-to-practice case of *nonlinear* NNs trained on *multiple* tasks. In this work, we present the first results proving that feature learning occurs during training with a nonlinear model on multiple tasks. Our key insight is that multi-task pretraining induces a pseudo-contrastive loss that favors representations that align points that typically have the same label across tasks. Using this observation, we show that when the tasks are binary classification tasks with labels depending on the projection of the data onto an $r$-dimensional subspace within the $d\\gg r$-dimensional input space, a simple gradient-based multitask learning algorithm on a two-layer ReLU NN recovers this projection, allowing for generalization to downstream tasks with sample and neuron complexity independent of $d$. In contrast, we show that with high probability over the draw of a single task, training on this single task cannot guarantee to learn all $r$ ground-truth features",
    "checked": true,
    "id": "57d99a9831cbe76ca8668e58f67254a9bbe92aac",
    "semantic_title": "provable multi-task representation learning by two-layer relu neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mzGtunvpJH": {
    "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
    "volume": "oral",
    "abstract": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset --- by training on publicly available datasets, D-iGPT unprecedentedly achieves **90.0%** top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT",
    "checked": true,
    "id": "c3f75e1a55fed93fd3c5bc785d62a0081b1b0d20",
    "semantic_title": "rejuvenating image-gpt as strong visual representation learners",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=DBI6AuCD4a": {
    "title": "High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise",
    "volume": "oral",
    "abstract": "High-probability analysis of stochastic first-order optimization methods under mild assumptions on the noise has been gaining a lot of attention in recent years. Typically, gradient clipping is one of the key algorithmic ingredients to derive good high-probability guarantees when the noise is heavy-tailed. However, if implemented naively, clipping can spoil the convergence of the popular methods for composite and distributed optimization (Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason, many works on high-probability analysis consider only unconstrained non-distributed problems, and the existing results for composite/distributed problems do not include some important special cases (like strongly convex problems) and are not optimal. To address this issue, we propose new stochastic methods for composite and distributed optimization based on the clipping of stochastic gradient differences and prove tight high-probability convergence results (including nearly optimal ones) for the new methods. In addition, we also develop new methods for composite and distributed variational inequalities and analyze the high-probability convergence of these methods",
    "checked": true,
    "id": "1dd38fbb9edc55cb8a7212c80808d19cfab760a8",
    "semantic_title": "high-probability convergence for composite and distributed stochastic minimization and variational inequalities with heavy-tailed noise",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=40hCy8n5XH": {
    "title": "InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization",
    "volume": "oral",
    "abstract": "Estimating mutual correlations between random variables or data streams is essential for intelligent behavior and decision-making. As a fundamental quantity for measuring statistical relationships, mutual information has been extensively studied and utilized for its generality and equitability. However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms. We introduce a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures. By maximizing a dual formulation of mutual information through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability. We evaluate the effectiveness and generalization of our proposed mutual information estimation scheme on various families of distributions and applications. Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties. We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time mutual information estimation",
    "checked": true,
    "id": "05cb90b9e3eee5185655a7b42e8b7c2851b5fc44",
    "semantic_title": "infonet: neural estimation of mutual information without test-time optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NZQkumsNlf": {
    "title": "NExT-GPT: Any-to-Any Multimodal LLM",
    "volume": "oral",
    "abstract": "While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio. By leveraging the existing well-trained high-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training but also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building a unified AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community",
    "checked": true,
    "id": "fa75a55760e6ea49b39b83cb85c99a22e1088254",
    "semantic_title": "next-gpt: any-to-any multimodal llm",
    "citation_count": 179,
    "authors": []
  },
  "https://openreview.net/forum?id=u09gadH3BU": {
    "title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",
    "volume": "oral",
    "abstract": "Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces any-precision LLM, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit LLM. All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs",
    "checked": true,
    "id": "b3c99575f91c551704d4a31d703d5ceb4b616dd1",
    "semantic_title": "any-precision llm: low-cost deployment of multiple, different-sized llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BdQTCAuT6L": {
    "title": "Private Truly-Everlasting Robust-Prediction",
    "volume": "oral",
    "abstract": "Private everlasting prediction (PEP), recently introduced by Naor et al. [2023], is a model for differentially private learning in which the learner never publicly releases a hypothesis. Instead, it provides black-box access to a \"prediction oracle\" that can predict the labels of an *endless stream* of unlabeled examples drawn from the underlying distribution. Importantly, PEP provides privacy both for the initial training set and for the endless stream of classification queries. We present two conceptual modifications to the definition of PEP, as well as new constructions exhibiting significant improvements over prior work. Specifically, we incorporate robustness against poisoning attacks into the definition of PEP; we present a relaxed privacy definition, suitable for PEP, that allows us to disconnect the privacy parameter $\\delta$ from the number of total time steps $T$; and we present new constructions for axis-aligned rectangles and decision-stumps exhibiting improved sample complexity and runtime",
    "checked": true,
    "id": "135fef1f1f876424572f560a994683d4b306783e",
    "semantic_title": "private truly-everlasting robust-prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6aKwVmHQI1": {
    "title": "ViP: A Differentially Private Foundation Model for Computer Vision",
    "volume": "oral",
    "abstract": "Artificial intelligence (AI) has seen a tremendous surge in capabilities thanks to the use of foundation models trained on internet-scale data. On the flip side, the uncurated nature of internet-scale data also poses significant privacy and legal risks, as they often contain personal information or copyrighted material that should not be trained on without permission. In this work, we propose as a mitigation measure a recipe to train foundation vision models via self-supervised learning with differential privacy (DP) guarantee. We identify masked autoencoders as a suitable learning algorithm that aligns well with DP-SGD, and train *ViP*---a **Vi**sion transformer with differential **P**rivacy---under a strict privacy budget of $\\epsilon=8$ on the LAION400M dataset. We evaluate the quality of representation learned by ViP using standard downstream vision tasks; in particular, ViP achieves a (non-private) linear probing accuracy of 55.7% on ImageNet, comparable to that of end-to-end trained AlexNet (trained and evaluated on ImageNet). Our result suggests that scaling to internet-scale data can be practical for private learning. Code and DP pre-trained models are available at https://github.com/facebookresearch/ViP-MAE",
    "checked": true,
    "id": "d74b100d22638d5f8bfd1ebf1f5bbb2d945de6e1",
    "semantic_title": "vip: a differentially private foundation model for computer vision",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=bJbSbJskOS": {
    "title": "Genie: Generative Interactive Environments",
    "volume": "oral",
    "abstract": "We introduce Genie, the first *generative interactive environment* trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a *foundation world model*. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis *despite training without any ground-truth action labels* or other domain specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future",
    "checked": true,
    "id": "c064de2c71ebc5cf05493f49dc312b033c36b3b9",
    "semantic_title": "genie: generative interactive environments",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=8kLzL5QBh2": {
    "title": "SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention",
    "volume": "oral",
    "abstract": "Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses current state-of-the-art methods and is on par with the biggest foundation model MOIRAI while having significantly fewer parameters. The code is available at https://github.com/romilbert/samformer",
    "checked": true,
    "id": "c8f5f6d981d96ebc84ba0a2665f00be3d1cfc27e",
    "semantic_title": "samformer: unlocking the potential of transformers in time series forecasting with sharpness-aware minimization and channel-wise attention",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=LTifAl5bKb": {
    "title": "Data-free Neural Representation Compression with Riemannian Neural Dynamics",
    "volume": "oral",
    "abstract": "Neural models are equivalent to dynamic systems from a physics-inspired view, implying that computation on neural networks can be interpreted as the dynamical interactions between neurons. However, existing work models neuronal interaction as a weight-based linear transformation, and the nonlinearity comes from the nonlinear activation functions, which leads to limited nonlinearity and data-fitting ability of the whole neural model. Inspired by Riemannian geometry, we interpret neural structures by projecting neurons onto the Riemannian neuronal state space and model neuronal interaction with Riemannian metric (${\\it RieM}$), which provides a more efficient neural representation with higher parameter efficiency. With ${\\it RieM}$, we further design a novel data-free neural compression mechanism that does not require additional fine-tuning with real data. Using backbones like ResNet and Vision Transformer, we conduct extensive experiments on datasets such as MNIST, CIFAR-100, ImageNet-1k, and COCO object detection. Empirical results show that, under equal compression rates and computational complexity, models compressed with ${\\it RieM}$ achieve superior inference accuracy compared to existing data-free compression methods",
    "checked": false,
    "id": "7468abdefc66dfdeaf8a2953a0a2f8b1472b129c",
    "semantic_title": "high-fidelity, generalizable light-field reconstruction of biological dynamics with physics-informed meta neural representation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fO31YAyNbI": {
    "title": "Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition",
    "volume": "oral",
    "abstract": "Existing research of video understanding still struggles to achieve in-depth comprehension and reasoning in complex videos, primarily due to the under-exploration of two key bottlenecks: fine-grained spatial-temporal perceptive understanding and cognitive-level video scene comprehension. This paper bridges the gap by presenting a novel solution. We first introduce a novel video Multimodal Large Language Model (MLLM), MotionEpic, which achieves fine-grained pixel-level spatial-temporal video grounding by integrating video spatial-temporal scene graph (STSG) representation. Building upon MotionEpic, we then develop a Video-of-Thought (VoT) reasoning framework. VoT inherits the Chain-of-Thought (CoT) core, breaking down a complex task into simpler and manageable sub-problems, and addressing them step-by-step from a low-level pixel perception to high-level cognitive interpretation. Extensive experiments across various complex video QA benchmarks demonstrate that our overall framework strikingly boosts existing state-of-the-art. To our knowledge, this is the first attempt at successfully implementing the CoT technique for achieving human-level video reasoning, where we show great potential in extending it to a wider range of video understanding scenarios. Systems and codes will be open later",
    "checked": false,
    "id": "6a08ff3d0821654b1fa92b2b63106946813e41a3",
    "semantic_title": "video-of-thought : step-by-step video reasoning from perception to cognition",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=qz1Vx1v9iK": {
    "title": "Test-Time Model Adaptation with Only Forward Passes",
    "volume": "oral",
    "abstract": "Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts. However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Optimization Adaptation (FOA) method. In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance. Without using any backpropagation and altering model weights, FOA runs on quantized 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to *24*-fold memory reduction on ImageNet-C. The source code is available at: https://github.com/mr-eggplant/FOA",
    "checked": true,
    "id": "bf9e3b31e6207293213caa60b7563e989d8cd9a3",
    "semantic_title": "test-time model adaptation with only forward passes",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTN866OsGx": {
    "title": "MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation",
    "volume": "oral",
    "abstract": "Neuronal morphology is essential for studying brain functioning and understanding neurodegenerative disorders. As acquiring real-world morphology data is expensive, computational approaches for morphology generation have been studied. Traditional methods heavily rely on expert-set rules and parameter tuning, making it difficult to generalize across different types of morphologies. Recently, MorphVAE was introduced as the sole learning-based method, but its generated morphologies lack plausibility, i.e., they do not appear realistic enough and most of the generated samples are topologically invalid. To fill this gap, this paper proposes **MorphGrower**, which mimicks the neuron natural growth mechanism for generation. Specifically, MorphGrower generates morphologies layer by layer, with each subsequent layer conditioned on the previously generated structure. During each layer generation, MorphGrower utilizes a pair of sibling branches as the basic generation block and generates branch pairs synchronously. This approach ensures topological validity and allows for fine-grained generation, thereby enhancing the realism of the final generated morphologies. Results on four real-world datasets demonstrate that MorphGrower outperforms MorphVAE by a notable margin. Importantly, the electrophysiological response simulation demonstrates the plausibility of our generated samples from a neuroscience perspective. Our code is available at [https://github.com/Thinklab-SJTU/MorphGrower](https://github.com/Thinklab-SJTU/MorphGrower)",
    "checked": true,
    "id": "5e9e9d8bfefb9f79e94c824c07ce37c09054b10f",
    "semantic_title": "morphgrower: a synchronized layer-by-layer growing approach for plausible neuronal morphology generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9vKRhnflAs": {
    "title": "Flextron: Many-in-One Flexible Large Language Model",
    "volume": "oral",
    "abstract": "Training modern LLMs is extremely resource intensive, and customizing them for various deployment scenarios characterized by limited compute and memory resources through repeated training is impractical. In this paper, we introduce Flextron, a network architecture and post-training model optimization framework supporting flexible model deployment. The Flextron architecture utilizes a nested elastic structure to rapidly adapt to specific user-defined latency and accuracy targets during inference with no additional fine-tuning required. It is also input-adaptive, and can automatically route tokens through its sub-networks for improved performance and efficiency. We present a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model. We evaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance over multiple end-to-end trained variants and other state-of-the-art elastic networks, all with a single pretraining run that consumes a mere 7.63% tokens compared to original pretraining",
    "checked": true,
    "id": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
    "semantic_title": "flextron: many-in-one flexible large language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lgcFX4VFrM": {
    "title": "Mean-field Chaos Diffusion Models",
    "volume": "oral",
    "abstract": "In this paper, we introduce a new class of score-based generative models (SGMs) designed to handle high-cardinality data distributions by leveraging concepts from mean-field theory. We present mean-field chaos diffusion models (MF-CDMs), which address the curse of dimensionality inherent in high-cardinality data by utilizing the propagation of chaos property of interacting particles. By treating high-cardinality data as a large stochastic system of interacting particles, we develop a novel score-matching method for infinite-dimensional chaotic particle systems and propose an approximation scheme that employs a subdivision strategy for efficient training. Our theoretical and empirical results demonstrate the scalability and effectiveness of MF-CDMs for managing large high-cardinality data structures, such as 3D point clouds",
    "checked": true,
    "id": "12d649a503070e845883c961c205428505a34fdf",
    "semantic_title": "mean-field chaos diffusion models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YqMOM5W9GF": {
    "title": "Principled Preferential Bayesian Optimization",
    "volume": "oral",
    "abstract": "We study the problem of preferential Bayesian optimization (BO), where we aim to optimize a black-box function with only preference feedback over a pair of candidate solutions. Inspired by the likelihood ratio idea, we construct a confidence set of the black-box function using only the preference feedback. An optimistic algorithm with an efficient computational method is then developed to solve the problem, which enjoys an information-theoretic bound on the total cumulative regret, a first-of-its-kind for preferential BO. This bound further allows us to design a scheme to report an estimated best solution, with a guaranteed convergence rate. Experimental results on sampled instances from Gaussian processes, standard test functions, and a thermal comfort optimization problem all show that our method stably achieves better or competitive performance as compared to the existing state-of-the-art heuristics, which, however, do not have theoretical guarantees on regret bounds or convergence",
    "checked": true,
    "id": "5fbbacd98613c917920a10d0626391b513673e80",
    "semantic_title": "principled preferential bayesian optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=oLfq1KKneW": {
    "title": "Preference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models",
    "volume": "oral",
    "abstract": "Molecule synthesis through machine learning is one of the fundamental problems in drug discovery. Current data-driven strategies employ one-step retrosynthesis models and search algorithms to predict synthetic routes in a top-bottom manner. Despite their effective performance, these strategies face limitations in the molecule synthetic route generation due to a greedy selection of the next molecule set without any lookahead. Furthermore, existing strategies cannot control the generation of synthetic routes based on possible criteria such as material costs, yields, and step count. In this work, we propose a general and principled framework via conditional residual energy-based models (EBMs), that focus on the quality of the entire synthetic route based on the specific criteria. By incorporating an additional energy-based function into our probabilistic model, our proposed algorithm can enhance the quality of the most probable synthetic routes (with higher probabilities) generated by various strategies in a plug-and-play fashion. Extensive experiments demonstrate that our framework can consistently boost performance across various strategies and outperforms previous state-of-the-art top-1 accuracy by a margin of 2.5%. Code is available at https://github.com/SongtaoLiu0823/CREBM",
    "checked": true,
    "id": "39aaca98b537985d00437e89100d2b5eff674b76",
    "semantic_title": "preference optimization for molecule synthesis with conditional residual energy-based models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jQ92egz5Ym": {
    "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
    "volume": "oral",
    "abstract": "The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4-bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the state-of-the-art methods. The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IR-QLoRA. We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer quantization) and brings general accuracy gains. The code is available at https://github.com/htqin/ir-qlora",
    "checked": true,
    "id": "cb169315cb75ce4b1af20083fbbe46c553e7f252",
    "semantic_title": "accurate lora-finetuning quantization of llms via information retention",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=0uUHfhXdnH": {
    "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
    "volume": "oral",
    "abstract": "In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced training costs and much faster inference speeds. Our DiJiang-7B achieves comparable performance with LLaMA2-7B on various benchmark while requires only about 1/50 training cost. Code is available at https://github.com/YuchuanTian/DiJiang",
    "checked": true,
    "id": "ef8846c0b9eb9e915d44e18bf06fda51f9b5e2fa",
    "semantic_title": "dijiang: efficient large language models through compact kernelization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uTC9AFXIhg": {
    "title": "GPTSwarm: Language Agents as Optimizable Graphs",
    "volume": "oral",
    "abstract": "Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. Our code is public",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3d5CIRG1n2": {
    "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
    "volume": "oral",
    "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. The code is available at https://github.com/NVlabs/DoRA",
    "checked": true,
    "id": "da053e2a4ba1b244940c8f2cad5dcdf0d730f85f",
    "semantic_title": "dora: weight-decomposed low-rank adaptation",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=3RXAiU7sss": {
    "title": "Position: Embracing Negative Results in Machine Learning",
    "volume": "oral",
    "abstract": "Publications proposing novel machine learning methods are often primarily rated by exhibited predictive performance on selected problems. In this position paper we argue that predictive performance alone is not a good indicator for the worth of a publication. Using it as such even fosters problems like inefficiencies of the machine learning research community as a whole and setting wrong incentives for researchers. We therefore put out a call for the publication of ``negative'' results, which can help alleviate some of these problems and improve the scientific output of the machine learning research community. To substantiate our position, we present the advantages of publishing negative results and provide concrete measures for the community to move towards a paradigm where their publication is normalized",
    "checked": true,
    "id": "5ba24e06a8ddcce6bccac5f79058d7e62668e9b3",
    "semantic_title": "position: embracing negative results in machine learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=drjjxmi2Ha": {
    "title": "Does Label Smoothing Help Deep Partial Label Learning?",
    "volume": "oral",
    "abstract": "Although deep partial label learning (deep PLL) classifiers have shown their competitive performance, they are heavily influenced by the noisy false-positive labels leading to poorer performance as the training progresses. Meanwhile, existing deep PLL research lacks theoretical guarantee on the analysis of correlation between label noise (or ambiguity degree) and classification performance. This paper addresses the above limitations with label smoothing (LS) from both theoretical and empirical aspects. In theory, we prove lower and upper bounds of the expected risk to show that label smoothing can help deep PLL. We further derive the optimal smoothing rate to investigate the conditions, i.e., when label smoothing benefits deep PLL. In practice, we design a benchmark solution and a novel optimization algorithm called Label Smoothing-based Partial Label Learning (LS-PLL). Extensive experimental results on benchmark PLL datasets and various deep architectures validate that label smoothing does help deep PLL in improving classification performance and learning distinguishable representations, and the best results can be achieved when the empirical smoothing rate approximately approaches the optimal smoothing rate in theoretical findings. Code is publicly available at https://github.com/kalpiree/LS-PLL",
    "checked": false,
    "id": "04f574bb227144ba2f60def2c16778e09b867f97",
    "semantic_title": "pseudo-labelling meets label smoothing for noisy partial label learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e93ffDcpH3": {
    "title": "Simple linear attention language models balance the recall-throughput tradeoff",
    "volume": "spotlight",
    "abstract": "Recent work has shown that attention-based language models excel at \"recall\", the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's recurrent state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the Pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to $1.3$b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 10.36 accuracy points. We further develop IO-aware algorithms that enable BASED to provide 24× higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Overall, BASED expands the Pareto frontier of the throughput-recall tradeoff space beyond prior architectures",
    "checked": true,
    "id": "cde66097f4123a62bf3e28d48c764648e8c69f72",
    "semantic_title": "simple linear attention language models balance the recall-throughput tradeoff",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=WUicA0hOF9": {
    "title": "Allocation Requires Prediction Only if Inequality Is Low",
    "volume": "spotlight",
    "abstract": "Algorithmic predictions are emerging as a promising solution concept for efficiently allocating societal resources. Fueling their use is an underlying assumption that such systems are necessary to identify individuals for interventions. We propose a principled framework for assessing this assumption: Using a simple mathematical model, we evaluate the efficacy of prediction-based allocations in settings where individuals belong to larger units such as hospitals, neighborhoods, or schools. We find that prediction-based allocations outperform baseline methods using aggregate unit-level statistics only when between-unit inequality is low and the intervention budget is high. Our results hold for a wide range of settings for the price of prediction, treatment effect heterogeneity, and unit-level statistics' learnability. Combined, we highlight the potential limits to improving the efficacy of interventions through prediction",
    "checked": true,
    "id": "aa8f21216beffa5990386d0677c982245b13e1ef",
    "semantic_title": "allocation requires prediction only if inequality is low",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1sRuv4cnuZ": {
    "title": "Multi-Track Message Passing: Tackling Oversmoothing and Oversquashing in Graph Learning via Preventing Heterophily Mixing",
    "volume": "spotlight",
    "abstract": "The advancement toward deeper graph neural networks is currently obscured by two inherent issues in message passing, *oversmoothing* and *oversquashing*. We identify the root cause of these issues as information loss due to *heterophily mixing* in aggregation, where messages of diverse category semantics are mixed. We propose a novel multi-track graph convolutional network to address oversmoothing and oversquashing effectively. Our basic idea is intuitive: if messages are separated and independently propagated according to their category semantics, heterophilic mixing can be prevented. Consequently, we present a novel multi-track message passing scheme capable of preventing heterophilic mixing, enhancing long-distance information flow, and improving separation condition. Empirical validations show that our model achieved state-of-the-art performance on several graph datasets and effectively tackled oversmoothing and oversquashing, setting a new benchmark of $86.4$% accuracy on Cora",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jdRIaUu3xY": {
    "title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models",
    "volume": "spotlight",
    "abstract": "Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively",
    "checked": true,
    "id": "19da1292e830f128f21b8f6b178c803edda657b7",
    "semantic_title": "bbox-adapter: lightweight adapting for black-box large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pVyOchWUBa": {
    "title": "Position: Understanding LLMs Requires More Than Statistical Generalization",
    "volume": "spotlight",
    "abstract": "The last decade has seen blossoming research in deep learning theory attempting to answer, ``Why does deep learning generalize?\" A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart---thus, equivalent test loss---can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases",
    "checked": false,
    "id": "6002f0d76189ad99ed3cb2d9945e8be869b96244",
    "semantic_title": "understanding llms requires more than statistical generalization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oRLwyayrh1": {
    "title": "DRCT: Diffusion Reconstruction Contrastive Training towards Universal Detection of Diffusion Generated Images",
    "volume": "spotlight",
    "abstract": "Diffusion models have made significant strides in visual content generation but also raised increasing demands on generated image detection. Existing detection methods have achieved considerable progress, but they usually suffer a significant decline in accuracy when detecting images generated by an unseen diffusion model. In this paper, we seek to address the generalizability of generated image detectors from the perspective of hard sample classification. The basic idea is that if a classifier can distinguish generated images that closely resemble real ones, then it can also effectively detect less similar samples, potentially even those produced by a different diffusion model. Based on this idea, we propose Diffusion Reconstruction Contrastive Learning (DRCT), a universal framework to enhance the generalizability of the existing detectors. DRCT generates hard samples by high-quality diffusion reconstruction and adopts contrastive training to guide the learning of diffusion artifacts. In addition, we have built a million-scale dataset, DRCT-2M, including 16 types diffusion models for the evaluation of generalizability of detection methods. Extensive experimental results show that detectors enhanced with DRCT achieve over a 10% accuracy improvement in cross-set tests. The code, models, and dataset will soon be available at https://github.com/beibuwandeluori/DRCT",
    "checked": false,
    "id": "30350a9360ccb6bb158313814fac1d21b515f5ae",
    "semantic_title": "progress toward iridium-catalyzed, iodine-mediated cyclization of allylic alcohols for the construction of quaternary centers development of hyperspectral spectroscopy data processing and visualization modules for the workbench for imaging spectroscopy exploration and research (wiser)",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=37xFIeYgE0": {
    "title": "Explaining Probabilistic Models with Distributional Values",
    "volume": "spotlight",
    "abstract": "A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for probabilistic models by generalising cooperative games and value operators. We introduce the *distributional values*, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models",
    "checked": true,
    "id": "95078a05a627a60c64d0640347738f886f411f2c",
    "semantic_title": "explaining probabilistic models with distributional values",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ln3moCobjO": {
    "title": "Relaxing the Accurate Imputation Assumption in Doubly Robust Learning for Debiased Collaborative Filtering",
    "volume": "spotlight",
    "abstract": "Recommender system aims to recommend items or information that may interest users based on their behaviors and preferences. However, there may be sampling selection bias in the data collection process, i.e., the collected data is not a representative of the target population. Many debiasing methods are developed based on pseudo-labelings. Nevertheless, the validity of these methods relies heavily on accurate pseudo-labelings (i.e., the imputed labels), which is difficult to satisfy in practice. In this paper, we theoretically propose several novel doubly robust estimators that are unbiased when either (a) the pseudo-labelings deviate from the true labels with an arbitrary user-specific inductive bias, item-specific inductive bias, or a combination of both, or (b) the learned propensities are accurate. We further propose a propensity reconstruction learning approach that adaptively updates the constraint weights using an attention mechanism and effectively controls the variance. Extensive experiments show that our approach outperforms the state-of-the-art on one semi-synthetic and three real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XLlQb24X2o": {
    "title": "Test-Time Degradation Adaptation for Open-Set Image Restoration",
    "volume": "spotlight",
    "abstract": "In contrast to close-set scenarios that restore images from a predefined set of degradations, open-set image restoration aims to handle the unknown degradations that were unforeseen during the pretraining phase, which is less-touched as far as we know. This work study this challenging problem and reveal its essence as unidentified distribution shifts between the test and training data. Recently, test-time adaptation has emerged as a fundamental method to address this inherent disparities. Inspired by it, we propose a test-time degradation adaptation framework for open-set image restoration, which consists of three components, *i.e.*, i) a pre-trained and degradation-agnostic diffusion model for generating clean images, ii) a test-time degradation adapter adapts the unknown degradations based on the input image during the testing phase, and iii) the adapter-guided image restoration guides the model through the adapter to produce the corresponding clean image. Through experiments on multiple degradations, we show that our method achieves comparable even better performance than those task-specific methods. The code is available at https://github.com/XLearning-SCU/2024-ICML-TAO",
    "checked": true,
    "id": "0c291c421fb11b5b27e2d3526efb2353d88c27a8",
    "semantic_title": "test-time degradation adaptation for open-set image restoration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5NTTCCO74S": {
    "title": "Regression with Multi-Expert Deferral",
    "volume": "spotlight",
    "abstract": "Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of *regression with deferral*, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set-specific. Our framework is versatile, applying to multiple experts, accommodating any bounded regression losses, addressing both instance-dependent and label-dependent costs, and supporting both single-stage and two-stage methods. Our single-stage formulation subsumes as a special case the recent *regression with abstention* (Cheng et al., 2023) framework, where only a single expert is considered, specifically for the squared loss and a label-independent cost. Minimizing our proposed loss functions directly leads to novel algorithms for regression with deferral. We report the results of extensive experiments showing the effectiveness of our proposed algorithms",
    "checked": true,
    "id": "daeac7c3bb8c05b6adf65cb014a3dd776197522b",
    "semantic_title": "regression with multi-expert deferral",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=beXQVQorse": {
    "title": "High-Dimensional Bayesian Optimization via Semi-Supervised Learning with Optimized Unlabeled Data Sampling",
    "volume": "spotlight",
    "abstract": "We introduce a novel semi-supervised learning approach, named Teacher-Student Bayesian Optimization ($\\texttt{TSBO}$), integrating the teacher-student paradigm into BO to minimize expensive labeled data queries for the first time. $\\texttt{TSBO}$ incorporates a teacher model, an unlabeled data sampler, and a student model. The student is trained on unlabeled data locations generated by the sampler, with pseudo labels predicted by the teacher. The interplay between these three components implements a unique *selective regularization* to the teacher in the form of student feedback. This scheme enables the teacher to predict high-quality pseudo labels, enhancing the generalization of the GP surrogate model in the search space. To fully exploit $\\texttt{TSBO}$, we propose two optimized unlabeled data samplers to construct effective student feedback that well aligns with the objective of Bayesian optimization. Furthermore, we quantify and leverage the uncertainty of the teacher-student model for the provision of reliable feedback to the teacher in the presence of risky pseudo-label predictions. $\\texttt{TSBO}$ demonstrates significantly improved sample-efficiency in several global optimization tasks under tight labeled data budgets. The implementation is available at https://github.com/reminiscenty/TSBO-Official",
    "checked": true,
    "id": "cb99078d2ba2e969a7b481fcf4c4582f775eeea7",
    "semantic_title": "high-dimensional bayesian optimization via semi-supervised learning with optimized unlabeled data sampling",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=DkqiId4AuR": {
    "title": "Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models",
    "volume": "spotlight",
    "abstract": "In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug and legislative bodies to audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure. In this paper, we introduce a post-hoc method that utilizes *deep reinforcement learning* to explore and construct the landscape of failure modes in pre-trained discriminative and generative models. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically show the effectiveness of the proposed method across common Computer Vision, Natural Language Processing, and Vision-Language tasks",
    "checked": true,
    "id": "2a32279d8b78ad5c8415b4841ed7c58a56f0935f",
    "semantic_title": "failures are fated, but can be faded: characterizing and mitigating unwanted behaviors in large-scale vision and language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=a2uFstsHPb": {
    "title": "Efficient Pareto Manifold Learning with Low-Rank Structure",
    "volume": "spotlight",
    "abstract": "Multi-task learning, which optimizes performance across multiple tasks, is inherently a multi-objective optimization problem. Various algorithms are developed to provide discrete trade-off solutions on the Pareto front. Recently, continuous Pareto front approximations using a linear combination of base networks have emerged as a compelling strategy. However, it suffers from scalability issues when the number of tasks is large. To address this issue, we propose a novel approach that integrates a main network with several low-rank matrices to efficiently learn the Pareto manifold. It significantly reduces the number of parameters and facilitates the extraction of shared features. We also introduce orthogonal regularization to further bolster performance. Extensive experimental results demonstrate that the proposed approach outperforms state-of-the-art baselines, especially on datasets with a large number of tasks",
    "checked": false,
    "id": "556b3a5fd58d95b42b50db1ac7c422d729206c03",
    "semantic_title": "label distribution learning by partitioning label distribution manifold",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=v7I5FtL2pV": {
    "title": "Tabular Insights, Visual Impacts: Transferring Expertise from Tables to Images",
    "volume": "spotlight",
    "abstract": "Transferring knowledge across diverse data modalities is receiving increasing attention in machine learning. This paper tackles the task of leveraging expert-derived, yet expensive, tabular data to enhance image-based predictions when tabular data is unavailable during inference. The primary challenges stem from the inherent complexity of accurately mapping diverse tabular data to visual contexts, coupled with the necessity to devise distinct strategies for numerical and categorical tabular attributes. We propose CHannel tAbulaR alignment with optiMal tranSport (Charms), which establishes an alignment between image channels and tabular attributes, enabling selective knowledge transfer that is pertinent to visual features. Specifically, Charms measures similarity distributions across modalities to effectively differentiate and transfer relevant tabular features, with a focus on morphological characteristics, enhancing the capabilities of visual classifiers. By maximizing the mutual information between image channels and tabular features, knowledge from both numerical and categorical tabular attributes are extracted. Experimental results demonstrate that Charms not only enhances the performance of image classifiers but also improves their interpretability by effectively utilizing tabular knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CaxQ5IbHgF": {
    "title": "Convergence of Some Convex Message Passing Algorithms to a Fixed Point",
    "volume": "spotlight",
    "abstract": "A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. This is also known as convex/convergent message passing; examples are max-sum diffusion and sequential tree-reweighted message passing (TRW-S). Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any point). We prove a stronger result (conjectured before but never proved): the iterates converge to a fixed point of the method. Moreover, we show that the algorithm terminates within $\\mathcal{O}(1/\\varepsilon)$ iterations. We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective. Then we show that several convex message passing methods are special cases of this method. Finally, we show that a slightly different version of coordinate descent can cycle",
    "checked": true,
    "id": "27bf27dd6a493835e53a09ab6da5ee2472a8e7db",
    "semantic_title": "convergence of some convex message passing algorithms to a fixed point",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a6wCNfIj8E": {
    "title": "Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings",
    "volume": "spotlight",
    "abstract": "Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a *functional* reward encoding (FRE) as a general, scalable solution to this *zero-shot RL* problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods",
    "checked": true,
    "id": "d27da1ba65fa958e45837120fad1c25e7017d80c",
    "semantic_title": "unsupervised zero-shot reinforcement learning via functional reward encodings",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=d1P6GtRzuV": {
    "title": "Neural Jump-Diffusion Temporal Point Processes",
    "volume": "spotlight",
    "abstract": "We present a novel perspective on temporal point processes (TPPs) by reformulating their intensity processes as solutions to stochastic differential equations (SDEs). In particular, we first prove the equivalent SDE formulations of several classical TPPs, including Poisson processes, Hawkes processes, and self-correcting processes. Based on these proofs, we introduce a unified TPP framework called Neural Jump-Diffusion Temporal Point Process (NJDTPP), whose intensity process is governed by a neural jump-diffusion SDE (NJDSDE) where the drift, diffusion, and jump coefficient functions are parameterized by neural networks. Compared to previous works, NJDTPP exhibits model flexibility in capturing intensity dynamics without relying on any specific functional form, and provides theoretical guarantees regarding the existence and uniqueness of the solution to the proposed NJDSDE. Experiments on both synthetic and real-world datasets demonstrate that NJDTPP is capable of capturing the dynamics of intensity processes in different scenarios and significantly outperforms the state-of-the-art TPP models in prediction tasks",
    "checked": false,
    "id": "eddcc9f77bde8be2cc4d23e7a8282b619b3cbd26",
    "semantic_title": "add and thin: diffusion for temporal point processes",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=8viuf9PdzU": {
    "title": "Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models",
    "volume": "spotlight",
    "abstract": "We introduce Sequential Neural Posterior Score Estimation (SNPSE), a score-based method for Bayesian inference in simulator-based models. Our method, inspired by the remarkable success of score-based methods in generative modelling, leverages conditional score-based diffusion models to generate samples from the posterior distribution of interest. The model is trained using an objective function which directly estimates the score of the posterior. We embed the model into a sequential training procedure, which guides simulations using the current approximation of the posterior at the observation of interest, thereby reducing the simulation cost. We also introduce several alternative sequential approaches, and discuss their relative merits. We then validate our method, as well as its amortised, non-sequential, variant on several numerical examples, demonstrating comparable or superior performance to existing state-of-the-art methods such as Sequential Neural Posterior Estimation (SNPE)",
    "checked": true,
    "id": "0da325499eed6086bde470f2f2f6fb4b5d009a94",
    "semantic_title": "sequential neural score estimation: likelihood-free inference with conditional score based diffusion models",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=iUwHnoENnl": {
    "title": "Model Alignment as Prospect Theoretic Optimization",
    "volume": "spotlight",
    "abstract": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases---the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration",
    "checked": false,
    "id": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
    "semantic_title": "kto: model alignment as prospect theoretic optimization",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=RiM3cl9MdK": {
    "title": "Stay on Topic with Classifier-Free Guidance",
    "volume": "spotlight",
    "abstract": "Classifier-Free Guidance (CFG) has recently emerged in as a lightweight technique to encourage prompt-adherence in generations, yet has not yet been successfully applied to language modeling. In this work, we demonstrate across a wide array of benchmarks that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across: Q&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in human evaluations we show a 75% preference for using CFG over baseline",
    "checked": true,
    "id": "5a3a04af4935302f0871bf14a4b573d477ce96be",
    "semantic_title": "stay on topic with classifier-free guidance",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=ltzTHGFF5i": {
    "title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization",
    "volume": "spotlight",
    "abstract": "Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory reduction compared to the FP16 baseline",
    "checked": true,
    "id": "388df4f4d6577c92a43a9a578ebed844a1b4cc17",
    "semantic_title": "jetfire: efficient and accurate transformer pretraining with int8 data flow and per-block quantization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GLGYYqPwjy": {
    "title": "QuRating: Selecting High-Quality Data for Training Language Models",
    "volume": "spotlight",
    "abstract": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications",
    "checked": true,
    "id": "181c8d59e60427e1f6c7152d6908fca1551b59fa",
    "semantic_title": "qurating: selecting high-quality data for training language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=A6fmX9QCEa": {
    "title": "Tuning-Free Stochastic Optimization",
    "volume": "spotlight",
    "abstract": "Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of *``tuning-free''* algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved. For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of SGD that matches the best-known high-probability convergence rate for tuned SGD at only an additional polylogarithmic cost. However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned SGD with high probability",
    "checked": true,
    "id": "bc14e62aaf5c2dfe1259d67c1a03d19063e7fa9d",
    "semantic_title": "tuning-free stochastic optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=VJjjNrUi8j": {
    "title": "Second-Order Uncertainty Quantification: A Distance-Based Approach",
    "volume": "spotlight",
    "abstract": "In the past couple of years, various approaches to representing and quantifying different types of predictive uncertainty in machine learning, notably in the setting of classification, have been proposed on the basis of second-order probability distributions, i.e., predictions in the form of distributions on probability distributions. A completely conclusive solution has not yet been found, however, as shown by recent criticisms of commonly used uncertainty measures associated with second-order distributions, identifying undesirable theoretical properties of these measures. In light of these criticisms, we propose a set of formal criteria that meaningful uncertainty measures for predictive uncertainty based on second-order distributions should obey. Moreover, we provide a general framework for developing uncertainty measures to account for these criteria, and offer an instantiation based on the Wasserstein distance, for which we prove that all criteria are satisfied",
    "checked": true,
    "id": "7261b93494e0511f1e9d1edbf3abdd6d7cda2b08",
    "semantic_title": "second-order uncertainty quantification: a distance-based approach",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=s6ZAT8MLKU": {
    "title": "Fundamental Benefit of Alternating Updates in Minimax Optimization",
    "volume": "spotlight",
    "abstract": "The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller iteration complexity bound, identical to that of the Extra-gradient method, while requiring less gradient computations. We also prove that Alex-GDA enjoys linear convergence for bilinear problems, for which both Sim-GDA and Alt-GDA fail to converge at all",
    "checked": true,
    "id": "97304389236496650fa5ca0bc1f24a468b3bbd58",
    "semantic_title": "fundamental benefit of alternating updates in minimax optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eapFRURALQ": {
    "title": "Leveraging Attractor Dynamics in Spatial Navigation for Better Language Parsing",
    "volume": "spotlight",
    "abstract": "Increasing experimental evidence suggests that the human hippocampus, evolutionarily shaped by spatial navigation tasks, also plays an important role in language comprehension, indicating a shared computational mechanism for both functions. However, the specific relationship between the hippocampal formation's computational mechanism in spatial navigation and its role in language processing remains elusive. To investigate this question, we develop a prefrontal-hippocampal-entorhinal model (which called PHE-trinity) that features two key aspects: 1) the use of a modular continuous attractor neural network to represent syntactic structure, akin to the grid network in the entorhinal cortex; 2) the creation of two separate input streams, mirroring the factorized structure-content representation found in the hippocampal formation. We evaluate our model on language command parsing tasks, specifically using the SCAN dataset. Our findings include: 1) attractor dynamics can facilitate systematic generalization and efficient learning from limited data; 2) through visualization and reverse engineering, we unravel a potential dynamic mechanism for grid network representing syntactic structure. Our research takes an initial step in uncovering the dynamic mechanism shared by spatial navigation and language information processing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWD7upg1ob": {
    "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text",
    "volume": "spotlight",
    "abstract": "Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications",
    "checked": true,
    "id": "a27d2f743dab4ae009beec52f2d61e0be885a7bd",
    "semantic_title": "differentially private synthetic data via foundation model apis 2: text",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=B0xmynxt4f": {
    "title": "DISCRET: Synthesizing Faithful Explanations For Treatment Effect Estimation",
    "volume": "spotlight",
    "abstract": "Designing faithful yet accurate AI models is challenging, particularly in the field of individual treatment effect estimation (ITE). ITE prediction models deployed in critical settings such as healthcare should ideally be (i) accurate, and (ii) provide faithful explanations. However, current solutions are inadequate: state-of-the-art black-box models do not supply explanations, post-hoc explainers for black-box models lack faithfulness guarantees, and self-interpretable models greatly compromise accuracy. To address these issues, we propose DISCRET, a self-interpretable ITE framework that synthesizes faithful, rule-based explanations for each sample. A key insight behind DISCRET is that explanations can serve dually as *database queries* to identify similar subgroups of samples. We provide a novel RL algorithm to efficiently synthesize these explanations from a large search space. We evaluate DISCRET on diverse tasks involving tabular, image, and text data. DISCRET outperforms the best self-interpretable models and has accuracy comparable to the best black-box models while providing faithful explanations. DISCRET is available at https://github.com/wuyinjun-1993/DISCRET-ICML2024",
    "checked": true,
    "id": "67f694e2718358f5eb3a23cf59a6968a4cb63d29",
    "semantic_title": "discret: synthesizing faithful explanations for treatment effect estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Edz0QXKKAo": {
    "title": "Position: Graph Foundation Models Are Already Here",
    "volume": "spotlight",
    "abstract": "Graph Foundation Models (GFMs) are emerging as a significant research topic in the graph domain, aiming to develop graph models trained on extensive and diverse data to enhance their applicability across various tasks and domains. Developing GFMs presents unique challenges over traditional Graph Neural Networks (GNNs), which are typically trained from scratch for specific tasks on particular datasets. The primary challenge in constructing GFMs lies in effectively leveraging vast and diverse graph data to achieve positive transfer. Drawing inspiration from existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a \"graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, expressiveness, and stability. Such a vocabulary perspective can potentially advance the future GFM design in line with the neural scaling laws. All relevant resources with GFM design can be found here",
    "checked": true,
    "id": "13f81979372850d9ab9d386c35bb00b4cc0e35f1",
    "semantic_title": "position: graph foundation models are already here",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6w7zkf9FBR": {
    "title": "Improved Operator Learning by Orthogonal Attention",
    "volume": "spotlight",
    "abstract": "This work presents orthogonal attention for constructing neural operators to serve as surrogates to model the solutions of a family of Partial Differential Equations (PDEs). The motivation is that the kernel integral operator, which is usually at the core of neural operators, can be reformulated with orthonormal eigenfunctions. Inspired by the success of the neural approximation of eigenfunctions (Deng et al., 2022), we opt to directly parameterize the involved eigenfunctions with flexible neural networks (NNs), based on which the input function is then transformed by the rule of kernel integral. Surprisingly, the resulting NN module bears a striking resemblance to regular attention mechanisms, albeit without softmax. Instead, it incorporates an orthogonalization operation that provides regularization during model training and helps mitigate overfitting, particularly in scenarios with limited data availability. In practice, the orthogonalization operation can be implemented with minimal additional overheads. Experiments on six standard neural operator benchmark datasets comprising both regular and irregular geometries show that our method can outperform competing baselines with decent margins",
    "checked": true,
    "id": "685f275f8362618c16749b99f9cd1dbef32a0ac7",
    "semantic_title": "improved operator learning by orthogonal attention",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=OLvgrLtv6J": {
    "title": "Exploiting Code Symmetries for Learning Program Semantics",
    "volume": "spotlight",
    "abstract": "This paper tackles the challenge of teaching code semantics to Large Language Models (LLMs) for program analysis by incorporating code symmetries into the model architecture. We introduce a group-theoretic framework that defines code symmetries as semantics-preserving transformations, where forming a code symmetry group enables precise and efficient reasoning of code semantics. Our solution, SymC, develops a novel variant of self-attention that is provably equivariant to code symmetries from the permutation group defined over the program dependence graph. SymC obtains superior performance on five program analysis tasks, outperforming state-of-the-art code models, including GPT-4, without any pre-training. Our results suggest that code LLMs that encode the code structural prior via the code symmetry group generalize better and faster",
    "checked": true,
    "id": "8291f1f917c0792a95417b3f8a1c2a5ec53872e3",
    "semantic_title": "exploiting code symmetries for learning program semantics",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=NbYAmsFJrc": {
    "title": "Resisting Stochastic Risks in Diffusion Planners with the Trajectory Aggregation Tree",
    "volume": "spotlight",
    "abstract": "Diffusion planners have shown promise in handling long-horizon and sparse-reward tasks due to the non-autoregressive plan generation. However, their inherent stochastic risk of generating infeasible trajectories presents significant challenges to their reliability and stability. We introduce a novel approach, the Trajectory Aggregation Tree (TAT), to address this issue in diffusion planners. Compared to prior methods that rely solely on raw trajectory predictions, TAT aggregates information from both historical and current trajectories, forming a dynamic tree-like structure. Each trajectory is conceptualized as a branch and individual states as nodes. As the structure evolves with the integration of new trajectories, unreliable states are marginalized, and the most impactful nodes are prioritized for decision-making. TAT can be deployed without modifying the original training and sampling pipelines of diffusion planners, making it a training-free, ready-to-deploy solution. We provide both theoretical analysis and empirical evidence to support TAT's effectiveness. Our results highlight its remarkable ability to resist the risk from unreliable trajectories, guarantee the performance boosting of diffusion planners in 100% of tasks, and exhibit an appreciable tolerance margin for sample quality, thereby enabling planning with a more than $3\\times$ acceleration",
    "checked": true,
    "id": "0f98478ad9cb2f52f23e25fe8a2747c56cf4d72b",
    "semantic_title": "resisting stochastic risks in diffusion planners with the trajectory aggregation tree",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FFILRGD0jG": {
    "title": "Stochastic Interpolants with Data-Dependent Couplings",
    "volume": "spotlight",
    "abstract": "Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to *couple* the base and the target densities, whereby samples from the base are computed conditionally given samples from the target in a way that is different from (but does not preclude) incorporating information about class labels or continuous embeddings. This enables us to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting. The code is available at [https://github.com/interpolants/couplings](https://github.com/interpolants/couplings)",
    "checked": true,
    "id": "c34cc8723ab763caab3596c395e44d755f533bad",
    "semantic_title": "stochastic interpolants with data-dependent couplings",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=6L4K5jmSJq": {
    "title": "How Free is Parameter-Free Stochastic Optimization?",
    "volume": "spotlight",
    "abstract": "We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free stochastic convex optimization infeasible, and provide a method which is (partially) parameter-free up to the limit indicated by our lower bound",
    "checked": true,
    "id": "5e59454d79797d6f426dfe07abb405006addb4c9",
    "semantic_title": "how free is parameter-free stochastic optimization?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=QCZabhKQhB": {
    "title": "Transformers, parallel computation, and logarithmic depth",
    "volume": "spotlight",
    "abstract": "We show that a constant number of self-attention layers can efficiently simulate—and be simulated by—a constant number of communication rounds of *Massively Parallel Computation*. As a consequence, we show that logarithmic-depth is sufficient for transformers to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and sub-quadratic transformer approximations. We thus establish parallelism as a key distinguishing property of transformers",
    "checked": true,
    "id": "1b3f1fc36ef84e90e837df474121f5d67afd13d9",
    "semantic_title": "transformers, parallel computation, and logarithmic depth",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=gn5AsHIIwb": {
    "title": "StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation",
    "volume": "spotlight",
    "abstract": "WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics",
    "checked": true,
    "id": "fa83074f19beab66f6e1b1f3e428fdf7730b33af",
    "semantic_title": "stacksight: unveiling webassembly through large language models and neurosymbolic chain-of-thought decompilation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nJzf3TVnOn": {
    "title": "Adaptive Online Experimental Design for Causal Discovery",
    "volume": "spotlight",
    "abstract": "Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on interventional data efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples",
    "checked": true,
    "id": "f88d7d1aa285439b2df575e5f98682852b8ad6a5",
    "semantic_title": "adaptive online experimental design for causal discovery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zn44XGFGam": {
    "title": "Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time",
    "volume": "spotlight",
    "abstract": "In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of O(√log n), where n is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that local gradient methods converge to a point with low training loss with high probability. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well",
    "checked": true,
    "id": "6738b7280d3b94f7365982197a51b32f0bbb10c8",
    "semantic_title": "convex relaxations of relu neural networks approximate global optima in polynomial time",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TfwGtfPkhV": {
    "title": "Testing the Feasibility of Linear Programs with Bandit Feedback",
    "volume": "spotlight",
    "abstract": "While the recent literature has seen a surge in the study of constrained bandit problems, all existing methods for these begin by assuming the feasibility of the underlying problem. We initiate the study of testing such feasibility assumptions, and in particular address the problem in the linear bandit setting, thus characterising the costs of feasibility testing for an unknown linear program using bandit feedback. Concretely, we test if $\\exists x: Ax \\ge 0$ for an unknown $A \\in \\mathbb{R}^{m \\times d}$, by playing a sequence of actions $x_t\\in \\mathbb{R}^d$, and observing $Ax_t + \\mathrm{noise}$ in response. By identifying the hypothesis as determining the sign of the value of a minimax game, we construct a novel test based on low-regret algorithms and a nonasymptotic law of iterated logarithms. We prove that this test is reliable, and adapts to the `signal level,' $\\Gamma,$ of any instance, with mean sample costs scaling as $\\widetilde{O}(d^2/\\Gamma^2)$. We complement this by a minimax lower bound of $\\Omega(d/\\Gamma^2)$ for sample costs of reliable tests, dominating prior asymptotic lower bounds by capturing the dependence on $d$, and thus elucidating a basic insight missing in the extant literature on such problems",
    "checked": true,
    "id": "14dfe8498f57f4af3830ae326a2430ab5d0f2acf",
    "semantic_title": "testing the feasibility of linear programs with bandit feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bplNmU2ROC": {
    "title": "Batch and match: black-box variational inference with a score-based divergence",
    "volume": "spotlight",
    "abstract": "Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates and their sensitivity to hyperparameters. In this work, we propose _batch and match_ (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM typically converges in fewer (and sometimes significantly fewer) gradient evaluations than leading implementations of BBVI based on ELBO maximization",
    "checked": true,
    "id": "a1e6b79a6e4f0fce311d11c8ed97047b48047eb2",
    "semantic_title": "batch and match: black-box variational inference with a score-based divergence",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=zajsXCxMgW": {
    "title": "A Distributional Analogue to the Successor Representation",
    "volume": "spotlight",
    "abstract": "This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible",
    "checked": true,
    "id": "ad3d7f81e41fc85b77dfedd4310f764f5607ca9e",
    "semantic_title": "a distributional analogue to the successor representation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=3QM5SWfeov": {
    "title": "MALIBO: Meta-learning for Likelihood-free Bayesian Optimization",
    "volume": "spotlight",
    "abstract": "Bayesian optimization (BO) is a popular method to optimize costly black-box functions, and meta-learning has emerged as a way to leverage knowledge from related tasks to optimize new tasks faster. However, existing meta-learning methods for BO rely on surrogate models that are not scalable or are sensitive to varying input scales and noise types across tasks. Moreover, they often overlook the uncertainty associated with task similarity, leading to unreliable task adaptation when a new task differs significantly or has not been sufficiently explored yet. We propose a novel meta-learning BO approach that bypasses the surrogate model and directly learns the utility of queries across tasks. It explicitly models task uncertainty and includes an auxiliary model to enable robust adaptation to new tasks. Extensive experiments show that our method achieves strong performance and outperforms multiple meta-learning BO methods across various benchmarks",
    "checked": true,
    "id": "ca41459c817cdbbfe608c490ca61aa7d28de89e3",
    "semantic_title": "malibo: meta-learning for likelihood-free bayesian optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=yzNEkTmcoF": {
    "title": "Triple Changes Estimator for Targeted Policies",
    "volume": "spotlight",
    "abstract": "The renowned difference-in-differences (DiD) estimator relies on the assumption of 'parallel trends,' which may not hold in many practical applications. To address this issue, economists are increasingly considering the triple difference estimator as a more credible alternative. Both DiD and triple difference are limited to assessing average effects exclusively. An alternative avenue is offered by the changes-in-changes (CiC) estimator, which provides an estimate of the entire counterfactual distribution by relying on assumptions imposed on the distribution of potential outcomes. In this work, we extend the triple difference estimator to accommodate the CiC framework, presenting the `triple changes estimator' and its identification assumptions, thereby expanding the scope of the CiC paradigm. Subsequently, we empirically evaluate the proposed framework and apply it to a study examining the impact of Medicaid expansion on children's preventive care",
    "checked": false,
    "id": "bf9e25a47b3799fc73c3592b3a9fbd278774c8f6",
    "semantic_title": "non-linear triple changes estimator for targeted policies",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WpKDeixmFr": {
    "title": "Time Weaver: A Conditional Time Series Generation Model",
    "volume": "spotlight",
    "abstract": "Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (e.g., weather and location). Current approaches to time series generation often ignore this paired metadata. Additionally, the heterogeneity in metadata poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce TIME WEAVER, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series. Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series. We show that TIME WEAVER outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 30% in downstream classification tasks on real-world energy, medical, air quality, and traffic datasets",
    "checked": true,
    "id": "1766bfff5e58c2be739ce1c13e2565815de6700b",
    "semantic_title": "time weaver: a conditional time series generation model",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Th8JPEmH4z": {
    "title": "Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
    "volume": "spotlight",
    "abstract": "We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of LLM-Modulo Frameworks that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications",
    "checked": false,
    "id": "b156004675ad3aa5e39a56928afc530aec191044",
    "semantic_title": "llms can't plan, but can help planning in llm-modulo frameworks",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=hg4wXlrQCV": {
    "title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We therefore believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources",
    "checked": true,
    "id": "139a84c6fc4887ce2374489d79af0df9e1e7e4d6",
    "semantic_title": "craftax: a lightning-fast benchmark for open-ended reinforcement learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=EsSSDjwFra": {
    "title": "A Tensor Decomposition Perspective on Second-order RNNs",
    "volume": "spotlight",
    "abstract": "Second-order Recurrent Neural Networks (2RNNs) extend RNNs by leveraging second-order interactions for sequence modelling. These models are provably more expressive than their first-order counterparts and have connections to well-studied models from formal language theory. However, their large parameter tensor makes computations intractable. To circumvent this issue, one approach known as MIRNN consists in limiting the type of interactions used by the model. Another is to leverage tensor decomposition to diminish the parameter count. In this work, we study the model resulting from parameterizing 2RNNs using the CP decomposition, which we call CPRNN. Intuitively, the rank of the decomposition should reduce expressivity. We analyze how rank and hidden size affect model capacity and show the relationships between RNNs, 2RNNs, MIRNNs, and CPRNNs based on these parameters. We support these results empirically with experiments on the Penn Treebank dataset which demonstrate that, with a fixed parameter budget, CPRNNs outperforms RNNs, 2RNNs, and MIRNNs with the right choice of rank and hidden size",
    "checked": true,
    "id": "d399a56e9fae0cd2c0c3abf505b90c302a64b5d9",
    "semantic_title": "a tensor decomposition perspective on second-order rnns",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V3OpGwo68Z": {
    "title": "Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models",
    "volume": "spotlight",
    "abstract": "Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the ground truth signal structure, the severity of the degradation and the complex interactions between the above. This results in natural sample-by-sample variation in the difficulty of a reconstruction problem. Our key observation is that most existing inverse problem solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in subpar performance and wasteful resource allocation. We propose a novel method, *severity encoding*, to estimate the degradation severity of corrupted signals in the latent space of an autoencoder. We show that the estimated severity has strong correlation with the true corruption level and can provide useful hints on the difficulty of reconstruction problems on a sample-by-sample basis. Furthermore, we propose a reconstruction method based on latent diffusion models that leverages the predicted degradation severities to fine-tune the reverse diffusion sampling trajectory and thus achieve sample-adaptive inference times. Our framework, Flash-Diffusion, acts as a wrapper that can be combined with any latent diffusion-based baseline solver, imbuing it with sample-adaptivity and acceleration. We perform experiments on both linear and nonlinear inverse problems and demonstrate that our technique greatly improves the performance of the baseline solver and achieves up to $10\\times$ acceleration in mean sampling speed",
    "checked": true,
    "id": "b13c89699a98d6dacf8d46a8fc635388e00da618",
    "semantic_title": "adapt and diffuse: sample-adaptive reconstruction via latent diffusion models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=5x788rqbcj": {
    "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g., \"What is Abraham Lincoln's birthday?''). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various _diversity measures_ of the training data. **Essentially**, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling) _during pretraining_. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and _how the model internally encodes knowledge_ --- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. **This paper provides several key recommendations for LLM pretraining in the industry:** (1) rewrite the pretraining data --- using small, auxiliary models --- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late",
    "checked": true,
    "id": "f29f8b8aa2b7e608199b65d3cf751969d4024132",
    "semantic_title": "physics of language models: part 3.1, knowledge storage and extraction",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=tMkPL7Tiul": {
    "title": "Fast Sampling-Based Sketches for Tensors",
    "volume": "spotlight",
    "abstract": "We introduce a new approach for applying sampling-based sketches to two and three mode tensors. We illustrate our technique to construct sketches for the classical problems of $\\ell_0$ sampling and producing $\\ell_1$ embeddings. In both settings we achieve sketches that can be applied to a rank one tensor in $(\\mathbb{R}^d)^{\\otimes q}$ (for $q=2,3$) in time scaling with $d$ rather than $d^2$ or $d^3$. Our main idea is a particular sampling construction based on fast convolution which allows us to quickly compute sums over sufficiently random subsets of tensor entries",
    "checked": false,
    "id": "4055b7a80568f2db4176a1678849726ad4a7d30f",
    "semantic_title": "fast sampling based sketches for tensors",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mu7Er7f9NQ": {
    "title": "Gambling-Based Confidence Sequences for Bounded Random Vectors",
    "volume": "spotlight",
    "abstract": "A confidence sequence (CS) is a sequence of confidence sets that contains a target parameter of an underlying stochastic process at any time step with high probability. This paper proposes a new approach to constructing CSs for means of bounded multivariate stochastic processes using a general gambling framework, extending the recently established coin toss framework for bounded random processes. The proposed gambling framework provides a general recipe for constructing CSs for categorical and probability-vector-valued observations, as well as for general bounded multidimensional observations through a simple reduction. This paper specifically explores the use of the mixture portfolio, akin to Cover's universal portfolio, in the proposed framework and investigates the properties of the resulting CSs. Simulations demonstrate the tightness of these confidence sequences compared to existing methods. When applied to the sampling without-replacement setting for finite categorical data, it is shown that the resulting CS based on a universal gambling strategy is provably tighter than that of the posterior-prior ratio martingale proposed by Waudby-Smith and Ramdas",
    "checked": true,
    "id": "9d9a27ad5c0b8ae243af2dc77f862a50c18aa15b",
    "semantic_title": "gambling-based confidence sequences for bounded random vectors",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mUSPhG4uDW": {
    "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
    "volume": "spotlight",
    "abstract": "We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings",
    "checked": true,
    "id": "4dd1b575a6ac74bfe9fab2e40c2cf1567c82fc6e",
    "semantic_title": "weblinx: real-world website navigation with multi-turn dialogue",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=3hSTecKy1b": {
    "title": "Position: Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?",
    "volume": "spotlight",
    "abstract": "New capabilities in foundation models are owed in large part to massive, widely-sourced, and under-documented training data collections. Existing practices in data collection have led to challenges in tracing authenticity, verifying consent, preserving privacy, addressing representation and bias, respecting copyright, and overall developing ethical and trustworthy foundation models. In response, regulation is emphasizing the need for training data transparency to understand foundation models' limitations. Based on a large-scale analysis of the foundation model training data landscape and existing solutions, we identify the missing infrastructure to facilitate responsible foundation model development practices. We examine the current shortcomings of common tools for tracing data authenticity, consent, and documentation, and outline how policymakers, developers, and data creators can facilitate responsible foundation model development by adopting universal data provenance standards",
    "checked": false,
    "id": "a15e462e30a1785f53e5eb9210acdf9908e8d7cb",
    "semantic_title": "data authenticity, consent, & provenance for ai are all broken: what will it take to fix them?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gS3nc9iUrH": {
    "title": "Representing Molecules as Random Walks Over Interpretable Grammars",
    "volume": "spotlight",
    "abstract": "Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability",
    "checked": true,
    "id": "6186c0dbaec22a0b624b007a69807f7df4b7e1be",
    "semantic_title": "representing molecules as random walks over interpretable grammars",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CKCzfU9YKE": {
    "title": "Replicable Learning of Large-Margin Halfspaces",
    "volume": "spotlight",
    "abstract": "We provide an efficient replicable algorithm for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell (STOC, 2022). We design the first dimension-independent replicable algorithm for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. (STOC, 2022) with respect to all the relevant parameters. Moreover, our algorithm has sample complexity that is optimal with respect to the accuracy parameter $\\epsilon$. Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun et al. (STOC 2023), we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\\tau$, but running time doubly exponential in $1/\\tau^2$ and worse sample complexity dependence on $\\epsilon$ than our previous algorithm. We then design an improved algorithm with better sample complexity than both of our previous algorithms and running time exponential in $1/\\tau^{2}.$",
    "checked": true,
    "id": "6c55cb3e0a0d390093aa11327f9a8ceb93f0ed91",
    "semantic_title": "replicable learning of large-margin halfspaces",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rK6AZem0hX": {
    "title": "Transport of Algebraic Structure to Latent Embeddings",
    "volume": "spotlight",
    "abstract": "Machine learning often aims to produce latent embeddings of inputs which lie in a larger, abstract mathematical space. For example, in the field of 3D modeling, subsets of Euclidean space can be embedded as vectors using implicit neural representations. Such subsets also have a natural algebraic structure including operations (e.g., union) and corresponding laws (e.g., associativity). How can we learn to \"union\" two sets using only their latent embeddings while respecting associativity? We propose a general procedure for parameterizing latent space operations that are provably consistent with the laws on the input space. This is achieved by learning a bijection from the latent space to a carefully designed *mirrored algebra* which is constructed on Euclidean space in accordance with desired laws. We evaluate these *structural transport nets* for a range of mirrored algebras against baselines that operate directly on the latent space. Our experiments provide strong evidence that respecting the underlying algebraic structure of the input space is key for learning accurate and self-consistent operations",
    "checked": true,
    "id": "cabbdd5e76ca47c37b049146ed71c9705cc5f335",
    "semantic_title": "transport of algebraic structure to latent embeddings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=99jx5U81jx": {
    "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers ''$\\textit{yes}$'' to the input question ''$\\textit{Can eagles fly?}$'' with the explanation ''$\\textit{all birds can fly}$'', then humans would infer from the explanation that it would also answer ''$\\textit{yes}$'' to the counterfactual input ''$\\textit{Can penguins fly?}$''. If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may be insufficient",
    "checked": true,
    "id": "69f2ba0f33a54e01de32c616b64e85d5d7194067",
    "semantic_title": "do models explain themselves? counterfactual simulatability of natural language explanations",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ToHkAg936Y": {
    "title": "Harnessing the Power of Neural Operators with Automatically Encoded Conservation Laws",
    "volume": "spotlight",
    "abstract": "Neural operators (NOs) have emerged as effective tools for modeling complex physical systems in scientific machine learning. In NOs, a central characteristic is to learn the governing physical laws directly from data. In contrast to other machine learning applications, partial knowledge is often known a priori about the physical system at hand whereby quantities such as mass, energy and momentum are exactly conserved. Currently, NOs have to learn these conservation laws from data and can only approximately satisfy them due to finite training data and random noise. In this work, we introduce conservation law-encoded neural operators (clawNOs), a suite of NOs that endow inference with automatic satisfaction of such conservation laws. ClawNOs are built with a divergence-free prediction of the solution field, with which the continuity equation is automatically guaranteed. As a consequence, clawNOs are compliant with the most fundamental and ubiquitous conservation laws essential for correct physical consistency. As demonstrations, we consider a wide variety of scientific applications ranging from constitutive modeling of material deformation, incompressible fluid dynamics, to atmospheric simulation. ClawNOs significantly outperform the state-of-the-art NOs in learning efficacy, especially in small-data regimes",
    "checked": true,
    "id": "ea0b85872a529a6e6437eb14b19b4de2cbadc163",
    "semantic_title": "harnessing the power of neural operators with automatically encoded conservation laws",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=2Gr5wZR6uc": {
    "title": "Stochastic Localization via Iterative Posterior Sampling",
    "volume": "spotlight",
    "abstract": "Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, *Stochastic Localization via Iterative Posterior Sampling* (**SLIPS**), to obtain approximate samples of these dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines. We illustrate the benefits and applicability of **SLIPS** on several benchmarks of multi-modal distributions, including Gaussian mixtures in increasing dimensions, Bayesian logistic regression and a high-dimensional field system from statistical-mechanics",
    "checked": true,
    "id": "58c56f1141402abc88f9e4921a6e5b33512178ad",
    "semantic_title": "stochastic localization via iterative posterior sampling",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=zDCwJQY3eI": {
    "title": "On a Neural Implementation of Brenier's Polar Factorization",
    "volume": "spotlight",
    "abstract": "In 1991, Brenier proved a theorem that generalizes the polar decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of Brenier's polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave",
    "checked": true,
    "id": "20f4b4d6aee5c651250c8695d4f1cff3ec80dc7f",
    "semantic_title": "on a neural implementation of brenier's polar factorization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xtKWwB6lzT": {
    "title": "Position: Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination",
    "volume": "spotlight",
    "abstract": "In the rapidly changing healthcare landscape, the implementation of offline reinforcement learning (RL) in dynamic treatment regimes (DTRs) presents a mix of unprecedented opportunities and challenges. This position paper offers a critical examination of the current status of offline RL in the context of DTRs. We argue for a reassessment of applying RL in DTRs, citing concerns such as inconsistent and potentially inconclusive evaluation metrics, the absence of naive and supervised learning baselines, and the diverse choice of RL formulation in existing research. Through a case study with more than 17,000 evaluation experiments using a publicly available Sepsis dataset, we demonstrate that the performance of RL algorithms can significantly vary with changes in evaluation metrics and Markov Decision Process (MDP) formulations. Surprisingly, it is observed that in some instances, RL algorithms can be surpassed by random baselines subjected to policy evaluation methods and reward design. This calls for more careful policy evaluation and algorithm development in future DTR works. Additionally, we discussed potential enhancements toward more reliable development of RL-based dynamic treatment regimes and invited further discussion within the community. Code is available at https://github.com/GilesLuo/ReassessDTR",
    "checked": false,
    "id": "7ee316da8c7b0970649fff217a502f022232e658",
    "semantic_title": "reinforcement learning in dynamic treatment regimes needs critical reexamination",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n8g6WMxt09": {
    "title": "Decoding-time Realignment of Language Models",
    "volume": "spotlight",
    "abstract": "Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset",
    "checked": true,
    "id": "44162aa2763c88a384d9c51d60eafcc59277a1c9",
    "semantic_title": "decoding-time realignment of language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=lT3W4AkyM7": {
    "title": "Predictive Linear Online Tracking for Unknown Targets",
    "volume": "spotlight",
    "abstract": "In this paper, we study the problem of online tracking in linear control systems, where the objective is to follow a moving target. Unlike classical tracking control, the target is unknown, non-stationary, and its state is revealed sequentially, thus, fitting the framework of online non-stochastic control. We consider the case of quadratic costs and propose a new algorithm, called predictive linear online tracking (PLOT). The algorithm uses recursive least squares with exponential forgetting to learn a time-varying dynamic model of the target. The learned model is used in the optimal policy under the framework of receding horizon control. We show the dynamic regret of PLOT scales with $\\mathcal{O}(\\sqrt{TV_T})$, where $V_T$ is the total variation of the target dynamics and $T$ is the time horizon. Unlike prior work, our theoretical results hold for non-stationary targets. We implement our online control algorithm on a real quadrotor, thus, showcasing one of the first successful applications of online control methods on real hardware",
    "checked": true,
    "id": "b76c652803b6b8512627056d0beb54b62e70e8a4",
    "semantic_title": "predictive linear online tracking for unknown targets",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=45HNimd4YI": {
    "title": "Perturb-and-Project: Differentially Private Similarities and Marginals",
    "volume": "spotlight",
    "abstract": "We revisit the objective perturbations framework for differential privacy where noise is added to the input $A\\in \\mathcal{S}$ and the result is then projected back to the space of admissible datasets $\\mathcal{S}$. Through this framework, we first design novel efficient algorithms to privately release pair-wise cosine similarities. Second, we derive a novel algorithm to compute $k$-way marginal queries over $n$ features. Prior work could achieve comparable guarantees only for $k$ even. Furthermore, we extend our results to $t$-sparse datasets, where our efficient algorithms yields novel, stronger guarantees whenever $t\\le n^{5/6}/\\log n.$ Finally, we provide a theoretical perspective on why *fast* input perturbation algorithms works well in practice. The key technical ingredients behind our results are tight sum-of-squares certificates upper bounding the Gaussian complexity of sets of solutions",
    "checked": true,
    "id": "b6258d3aa1e99808e1dce164818e86da931df727",
    "semantic_title": "perturb-and-project: differentially private similarities and marginals",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1OsRSrkFWl": {
    "title": "Promoting External and Internal Equities Under Ex-Ante/Ex-Post Metrics in Online Resource Allocation",
    "volume": "spotlight",
    "abstract": "This paper proposes two different models for equitable resource allocation in online settings. The first one is called *external* equity promotion, where sequentially arriving agents are heterogeneous in their external attributes, namely how many resources they demand, which are drawn from a probability distribution (accessible to the algorithm). The focus is then to devise an allocation policy such that every requester can get a fair share of resources *proportional to their demands*, regardless of their arrival time. The second is called *internal* equity promotion, where arriving requesters can be treated homogeneously in external attributes (demands) but are heterogeneous in internal traits such as demographics. In particular, each requester can be identified as belonging to one or several groups, and an allocation of resources is regarded as equitable when every group of requesters can receive a fair share of resources proportional to the percentage of that group in the whole population. For both models above, we consider as the benchmark a clairvoyant optimal solution that has the privilege to access all random demand realizations in advance. We consider two equity metrics, namely *ex-post* and *ex-ante*, and discuss the challenges under the two metrics in detail. Specifically, we present two linear program (LP)-based policies for external equity promotion under ex-ante with independent demands, each achieving an *optimal* CR of $1/2$ with respect to the benchmark LP. For internal equity promotion, we present optimal policies under both ex-ante and ex-post metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NsHxeSCtgr": {
    "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups (e.g., female), raising severe fairness concerns. As remedies, prior works intervened the generation by removing attitude or demographic information, inevitably degrading the generation quality and resulting in notable *fairness-fluency* trade-offs. However, it is still under-explored to what extent the fluency *has to* be affected in order to achieve a desired level of fairness. In this work, we conduct the first formal study from an information-theoretic perspective. We show that previous approaches are excessive for debiasing and propose LIDAO, a general framework to debias a (L)LM at a better fluency provably. We further robustify LIDAO in adversarial scenarios, where a carefully-crafted prompt may stimulate LLMs exhibiting instruction-following abilities to generate texts with fairness issue appears only when the prompt is also taken into account. Experiments on three LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our method",
    "checked": true,
    "id": "085d1dbae0159686503269d56fb2fc1091373355",
    "semantic_title": "lidao: towards limited interventions for debiasing (large) language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5WnKLIAX4q": {
    "title": "Robust and Conjugate Gaussian Process Regression",
    "volume": "spotlight",
    "abstract": "To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this paper, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes",
    "checked": true,
    "id": "173c4373744d772be88f99dc500fb13ef9d42229",
    "semantic_title": "robust and conjugate gaussian process regression",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GC8HkKeH8s": {
    "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
    "volume": "spotlight",
    "abstract": "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with \"high quality\" data sources may not increase (and can even hurt) performance compared to randomly selecting data. To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods",
    "checked": true,
    "id": "d6a546b23c3b67aa2dc2d03ef3e7692f031ebd2d",
    "semantic_title": "dsdm: model-aware dataset selection with datamodels",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=nkOMLBIiI7": {
    "title": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning",
    "volume": "spotlight",
    "abstract": "It is well known that LLMs cannot generalize well to long contexts whose lengths are larger than the training sequence length. This poses challenges when employing LLMs for processing long input sequences during inference. In this work, we argue that LLMs themselves have inherent capabilities to handles s long contexts without fine-tuning. To achieve this goal, we propose SelfExtend to extend the context window of LLMs by constructing bi-level attention information: the grouped attention and the neighbor attention. The grouped attention captures the dependencies among tokens that are far apart, while neighbor attention captures dependencies among adjacent tokens within a specified range. The two-level attentions are computed based on the original model's self-attention mechanism during inference. With minor code modification, our SelfExtend can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments on multiple benchmarks and the results show that our SelfExtend can effectively extend existing LLMs' context window length",
    "checked": false,
    "id": "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b",
    "semantic_title": "llm maybe longlm: self-extend llm context window without tuning",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=iLyUEPZ0fR": {
    "title": "Block Acceleration Without Momentum: On Optimal Stepsizes of Block Gradient Descent for Least-Squares",
    "volume": "spotlight",
    "abstract": "Block coordinate descent is a powerful algorithmic template suitable for big data optimization. This template admits a lot of variants including block gradient descent (BGD), which performs gradient descent on a selected block of variables, while keeping other variables fixed. For a very long time, the stepsize for each block has tacitly been set to one divided by the block-wise Lipschitz smoothness constant, imitating the vanilla stepsize rule for gradient descent (GD). However, such a choice for BGD has not yet been able to theoretically justify its empirical superiority over GD, as existing convergence rates for BGD have worse constants than GD in the deterministic cases. To discover such theoretical justification, we set up a simple environment where we consider BGD applied to least-squares with two blocks of variables. Assuming the data matrix corresponding to each block is orthogonal, we find optimal stepsizes of BGD in closed form, which provably lead to asymptotic convergence rates twice as fast as GD with Polyak's momentum; this means, under that orthogonality assumption, one can accelerate BGD by just tuning stepsizes and without adding any momentum. An application that satisfies this assumption is *generalized alternating projection* between two subspaces, and applying our stepsizes to it improves the prior convergence rate that was once claimed, slightly inaccurately, to be optimal. The main proof idea is to minimize, in stepsize variables, the spectral radius of a matrix that controls convergence rates",
    "checked": true,
    "id": "3823be9038961c34cc8f1a0217c277b6787e7744",
    "semantic_title": "block acceleration without momentum: on optimal stepsizes of block gradient descent for least-squares",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=veEjiN2w9F": {
    "title": "Local vs. Global Interpretability: A Computational Complexity Perspective",
    "volume": "spotlight",
    "abstract": "The local and global interpretability of various ML models has been studied extensively in recent years. However, despite significant progress in the field, many known results remain informal or lack sufficient mathematical rigor. We propose a framework for bridging this gap, by using computational complexity theory to assess local and global perspectives of interpreting ML models. We begin by proposing proofs for two novel insights that are essential for our analysis: (1) a duality between local and global forms of explanations; and (2) the inherent uniqueness of certain global explanation forms. We then use these insights to evaluate the complexity of computing explanations, across three model types representing the extremes of the interpretability spectrum: (1) linear models; (2) decision trees; and (3) neural networks. Our findings offer insights into both the local and global interpretability of these models. For instance, under standard complexity assumptions such as P != NP, we prove that selecting *global* sufficient subsets in linear models is computationally harder than selecting *local* subsets. Interestingly, with neural networks and decision trees, the opposite is true: it is harder to carry out this task locally than globally. We believe that our findings demonstrate how examining explainability through a computational complexity lens can help us develop a more rigorous grasp of the inherent interpretability of ML models",
    "checked": true,
    "id": "f99b4dbd359fdce2f20ac6cb3ed42076782507f4",
    "semantic_title": "local vs. global interpretability: a computational complexity perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ssFMq35UUY": {
    "title": "ULAREF: A Unified Label Refinement Framework for Learning with Inaccurate Supervision",
    "volume": "spotlight",
    "abstract": "Learning with inaccurate supervision is often encountered in weakly supervised learning, and researchers have invested a considerable amount of time and effort in designing specialized algorithms for different forms of annotations in inaccurate supervision. In fact, different forms of these annotations share the fundamental characteristic that they all still incorporate some portion of correct labeling information. This commonality can serve as a lever, enabling the creation of a cohesive framework designed to tackle the challenges associated with various forms of annotations in learning with inaccurate supervision. In this paper, we propose a unified label refinement framework named ULAREF, i.e., a Unified LAbel REfinement Framework for learning with inaccurate supervision, which is capable of leveraging label refinement to handle inaccurate supervision. Specifically, our framework trains the predictive model with refined labels through global detection of reliability and local enhancement using an enhanced model fine-tuned by a proposed consistency loss. Also, we theoretically justify that the enhanced model in local enhancement can achieve higher accuracy than the predictive model on the detected unreliable set under mild assumptions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ABt0jlLZtX": {
    "title": "Learning Optimal Deterministic Policies with Stochastic Policy Gradients",
    "volume": "spotlight",
    "abstract": "Policy gradient (PG) methods are successful approaches to deal with continuous reinforcement learning (RL) problems. They learn stochastic parametric (hyper)policies by either exploring in the space of actions or in the space of parameters. Stochastic controllers, however, are often undesirable from a practical perspective because of their lack of robustness, safety, and traceability. In common practice, stochastic (hyper)policies are learned only to deploy their deterministic version. In this paper, we make a step towards the theoretical understanding of this practice. After introducing a novel framework for modeling this scenario, we study the global convergence to the best deterministic policy, under (weak) gradient domination assumptions. Then, we illustrate how to tune the exploration level used for learning to optimize the trade-off between the sample complexity and the performance of the deployed deterministic policy. Finally, we quantitatively compare action-based and parameter-based exploration, giving a formal guise to intuitive results",
    "checked": true,
    "id": "c7efbc88db0cdff41df874a65da809e3090aeb80",
    "semantic_title": "learning optimal deterministic policies with stochastic policy gradients",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7RSIGQRT1F": {
    "title": "A Geometric Decomposition of Finite Games: Convergence vs. Recurrence under Exponential Weights",
    "volume": "spotlight",
    "abstract": "In view of the complexity of the dynamics of learning in games, we seek to decompose a game into simpler components where the dynamics' long-run behavior is well understood. A natural starting point for this is Helmholtz's theorem, which decomposes a vector field into a potential and an incompressible component. However, the geometry of game dynamics - and, in particular, the dynamics of exponential / multiplicative weights (EW) schemes - is not compatible with the Euclidean underpinnings of Helmholtz's theorem. This leads us to consider a specific Riemannian framework based on the so-called *Shahshahani metric*, and introduce the class of *incompressible games*, for which we establish the following results: First, in addition to being volume-preserving, the continuous-time EW dynamics in incompressible games admit a constant of motion and are *Poincaré recurrent* - i.e., almost every trajectory of play comes arbitrarily close to its starting point infinitely often. Second, we establish a deep connection with a well-known decomposition of games into a potential and harmonic component (where the players' objectives are aligned and anti-aligned respectively): a game is incompressible if and only if it is harmonic, implying in turn that the EW dynamics lead to Poincaré recurrence in harmonic games",
    "checked": true,
    "id": "80552502c3f2589c51d1c3255a11764847bb200f",
    "semantic_title": "a geometric decomposition of finite games: convergence vs. recurrence under exponential weights",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R6GT1UDcOW": {
    "title": "Target Networks and Over-parameterization Stabilize Off-policy Bootstrapping with Function Approximation",
    "volume": "spotlight",
    "abstract": "We prove that the combination of a target network and over-parameterized linear function approximation establishes a weaker convergence condition for bootstrapped value estimation in certain cases, even with off-policy data. Our condition is naturally satisfied for expected updates over the entire state-action space or learning with a batch of complete trajectories from episodic Markov decision processes. Notably, using only a target network or an over-parameterized model does not provide such a convergence guarantee. Additionally, we extend our results to learning with truncated trajectories, showing that convergence is achievable for all tasks with minor modifications, akin to value truncation for the final states in trajectories. Our primary result focuses on temporal difference estimation for prediction, providing high-probability value estimation error bounds and empirical analysis on Baird's counterexample and a Four-room task. Furthermore, we explore the control setting, demonstrating that similar convergence conditions apply to Q-learning",
    "checked": true,
    "id": "4330106cd46d41fd0d2d0266c8b3da264e372c13",
    "semantic_title": "target networks and over-parameterization stabilize off-policy bootstrapping with function approximation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O8rrXl71D5": {
    "title": "What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation",
    "volume": "spotlight",
    "abstract": "In-context learning is a powerful emergent ability in transformer models. Prior work in mechanistic interpretability has identified a circuit element that may be critical for in-context learning – the induction head (IH), which performs a match-and-copy operation. During training of large transformers on natural language data, IHs emerge around the same time as a notable phase change in the loss. Despite the robust evidence for IHs and this interesting coincidence with the phase change, relatively little is known about the diversity and emergence dynamics of IHs. Why is there more than one IH, and how are they dependent on each other? Why do IHs appear all of a sudden, and what are the subcircuits that enable them to emerge? We answer these questions by studying IH emergence dynamics in a controlled setting by training on synthetic data. In doing so, we develop and share a novel optogenetics-inspired causal framework for modifying activations throughout training. Using this framework, we delineate the diverse and additive nature of IHs. By \"clamping\" subsets of activations throughout training, we then identify three underlying subcircuits that interact to drive IH formation, yielding the phase change. Furthermore, these subcircuits shed light on data-dependent properties of formation, such as phase change timing, already showing the promise of this more in-depth understanding of subcircuits that need to \"go right\" for an induction head",
    "checked": true,
    "id": "63a87feede94433b44b2c2b194e5902c3c5158f2",
    "semantic_title": "what needs to go right for an induction head? a mechanistic study of in-context learning circuits and their formation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ttnbM598vZ": {
    "title": "Pairwise Alignment Improves Graph Domain Adaptation",
    "volume": "spotlight",
    "abstract": "Graph-based methods, pivotal for label inference over interconnected objects in many real-world applications, often encounter generalization challenges, if the graph used for model training differs significantly from the graph used for testing. This work delves into Graph Domain Adaptation (GDA) to address the unique complexities of distribution shifts over graph data, where interconnected data points experience shifts in features, labels, and in particular, connecting patterns. We propose a novel, theoretically principled method, Pairwise Alignment (Pair-Align) to counter graph structure shift by mitigating conditional structure shift (CSS) and label shift (LS). Pair-Align uses edge weights to recalibrate the influence among neighboring nodes to handle CSS and adjusts the classification loss with label weights to handle LS. Our method demonstrates superior performance in real-world applications, including node classification with region shift in social networks, and the pileup mitigation task in particle colliding experiments. For the first application, we also curate the largest dataset by far for GDA studies. Our method shows strong performance in synthetic and other existing benchmark datasets",
    "checked": true,
    "id": "493cabe729d7cc3eb3d919a00692afae761f2feb",
    "semantic_title": "pairwise alignment improves graph domain adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qE4nkfyMYl": {
    "title": "Estimating Unknown Population Sizes Using the Hypergeometric Distribution",
    "volume": "spotlight",
    "abstract": "The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the category sizes are unknown. Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation problem, even in the presence of severe under-sampling. Our approach accounts for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, as seen in collaborative filtering, using the variational autoencoder framework. Empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and learning an informative latent space. We showcase our method's versatility through applications in NLP, by inferring and estimating the complexity of latent vocabularies in reading passage excerpts, and in biology, by accurately recovering the true number of gene transcripts from sparse single-cell genomics data",
    "checked": true,
    "id": "dd558553e0d5ee1146332bcbcc83575134680894",
    "semantic_title": "estimating unknown population sizes using the hypergeometric distribution",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YqIIhl2ToH": {
    "title": "Beyond the Norms: Detecting Prediction Errors in Regression Models",
    "volume": "spotlight",
    "abstract": "This paper tackles the challenge of detecting unreliable behavior in regression algorithms, which may arise from intrinsic variability (e.g., aleatoric uncertainty) or modeling errors (e.g., model uncertainty). First, we formally introduce the notion of unreliability in regression, i.e., when the output of the regressor exceeds a specified discrepancy (or error). Then, using powerful tools for probabilistic modeling, we estimate the discrepancy density, and we measure its statistical diversity using our proposed metric for statistical dissimilarity. In turn, this allows us to derive a data-driven score that expresses the uncertainty of the regression outcome. We show empirical improvements in error detection for multiple regression tasks, consistently outperforming popular baseline approaches, and contributing to the broader field of uncertainty quantification and safe machine learning systems",
    "checked": true,
    "id": "cfd96aefb10f644a7aa435afb416846cff58ceb6",
    "semantic_title": "beyond the norms: detecting prediction errors in regression models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qeFgvVVAJ2": {
    "title": "Memory Consolidation Enables Long-Context Video Understanding",
    "volume": "spotlight",
    "abstract": "Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. We propose to instead re-purpose existing pre-trained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memory-consolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters",
    "checked": true,
    "id": "3c23f28bac6c9387573a645673622172ea8b50a5",
    "semantic_title": "memory consolidation enables long-context video understanding",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=QRjTDhCIO8": {
    "title": "Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge",
    "volume": "spotlight",
    "abstract": "Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock demonstrate our model's superior effectiveness and efficiency over current methods",
    "checked": true,
    "id": "68981c9aba4c06176f3c062b94c3e6861371bdb6",
    "semantic_title": "re-dock: towards flexible and realistic molecular docking with diffusion bridge",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rfvgdfd1K9": {
    "title": "Position: Intent-aligned AI Systems Must Optimize for Agency Preservation",
    "volume": "spotlight",
    "abstract": "A central approach to AI-safety research has been to generate aligned AI systems: i.e. systems that do not deceive users and yield actions or recommendations that humans might judge as consistent with their intentions and goals. Here we argue that truthful AIs aligned solely to human intent are insufficient and that preservation of long-term agency of humans may be a more robust standard that may need to be separated and explicitly optimized for. We discuss the science of intent and control and how human intent can be manipulated and we provide a formal definition of agency-preserving AI-human interactions focusing on forward-looking explicit agency evaluations. Our work points to a novel pathway for human harm in AI-human interactions and proposes solutions to this challenge",
    "checked": false,
    "id": "1e603f3254bc0e0dbcf9d1170f968b45d502d557",
    "semantic_title": "intent-aligned ai systems deplete human agency: the need for agency foundations research in ai safety",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=53iSXb1m8w": {
    "title": "Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem",
    "volume": "spotlight",
    "abstract": "Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: *forgetting of pre-trained capabilities*. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the pre-trained capabilities. In particular, in NetHack, we achieve a new state-of-the-art for neural models, improving the previous best score from $5$K to over $10$K points in the Human Monk scenario",
    "checked": true,
    "id": "40c52f7a1df09143f7cc4d686e3d78e9e0f77f21",
    "semantic_title": "fine-tuning reinforcement learning models is secretly a forgetting mitigation problem",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=LmzsgSDkWs": {
    "title": "Learning with Partial-Label and Unlabeled Data: A Uniform Treatment for Supervision Redundancy and Insufficiency",
    "volume": "spotlight",
    "abstract": "One major challenge in weakly supervised learning is learning from inexact supervision, ranging from partial labels (PLs) with *redundant* information to the extreme of unlabeled data with *insufficient* information. While recent work has made significant strides in specific inexact supervision contexts, supervision forms typically *coexist* in complex combinations. This is exemplified in *semi-supervised partial label learning*, where PLs act as the exclusive supervision in a semi-supervised setting. Current strategies addressing combined inexact scenarios are usually composite, which can lead to incremental solutions that essentially replicate existing methods. In this paper, we propose a novel approach to *uniformly* tackle both label redundancy and insufficiency, derived from a mutual information-based perspective. We design a label channel that facilitates dynamic label exchange within the candidate label sets, which identifies potential true labels and filters out likely incorrect ones, thereby minimizing error accumulation. Experimental results demonstrate the superiority of our method over existing state-of-the-art PL and semi-supervised learning approaches by directly integrating them. Furthermore, our extended experiments on partial-complementary label learning underscore the flexibility of our uniform treatment in managing diverse supervision scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hrWte3nlzr": {
    "title": "Truly No-Regret Learning in Constrained MDPs",
    "volume": "spotlight",
    "abstract": "Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in reinforcement learning. State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for *error cancellations* --- one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal-dual algorithm to learn in an unknown CMDP. We prove that our algorithm achieves sublinear regret without error cancellations",
    "checked": true,
    "id": "5e64a6013c5d4668af17b9ea631c2a24c2a2caf6",
    "semantic_title": "truly no-regret learning in constrained mdps",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CtEWswTjUd": {
    "title": "How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy Model",
    "volume": "spotlight",
    "abstract": "Understanding what makes high-dimensional data learnable is a fundamental question in machine learning. On the one hand, it is believed that the success of deep learning lies in its ability to build a hierarchy of representations that become increasingly more abstract with depth, going from simple features like edges to more complex concepts. On the other hand, learning to be insensitive to invariances of the task, such as smooth transformations for image datasets, has been argued to be important for deep networks and it strongly correlates with their performance. In this work, we aim to explain this correlation and unify these two viewpoints. We show that by introducing sparsity to generative hierarchical models of data, the task acquires insensitivity to spatial transformations that are discrete versions of smooth transformations. In particular, we introduce the Sparse Random Hierarchy Model (SRHM), where we observe and rationalize that a hierarchical representation mirroring the hierarchical model is learnt precisely when such insensitivity is learnt, thereby explaining the strong correlation between the latter and performance. Moreover, we quantify how the sample complexity of CNNs learning the SRHM depends on both the sparsity and hierarchical structure of the task",
    "checked": true,
    "id": "c643827e0de9ffd30f1fb8ab77953d5d778e62c0",
    "semantic_title": "how deep networks learn sparse and hierarchical data: the sparse random hierarchy model",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=PQ0ERKKYJu": {
    "title": "Position: Mission Critical – Satellite Data is a Distinct Modality in Machine Learning",
    "volume": "spotlight",
    "abstract": "Satellite data has the potential to inspire a seismic shift for machine learning---one in which we rethink existing practices designed for traditional data modalities. As machine learning for satellite data (SatML) gains traction for its real-world impact, our field is at a crossroads. We can either continue applying ill-suited approaches, or we can initiate a new research agenda that centers around the unique characteristics and challenges of satellite data. This position paper argues that satellite data constitutes a distinct modality for machine learning research and that we must recognize it as such to advance the quality and impact of SatML research across theory, methods, and deployment. We outline research directions, critical discussion questions and actionable suggestions to transform SatML from merely an intriguing application area to a dedicated research discipline that helps move the needle on big challenges for machine learning and society",
    "checked": false,
    "id": "0385c1fa107ce68db9f988547bf2d7b708a0c748",
    "semantic_title": "mission critical - satellite data is a distinct modality in machine learning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=S3xqyEaST9": {
    "title": "Practical Performance Guarantees for Pipelined DNN Inference",
    "volume": "spotlight",
    "abstract": "We optimize pipeline parallelism for deep neural network (DNN) inference by partitioning model graphs into $k$ stages and minimizing the running time of the bottleneck stage, including communication. We give practical and effective algorithms for this NP-hard problem, but our emphasis is on tackling the practitioner's dilemma of deciding when a solution is good enough. To this end, we design novel mixed integer programming (MIP) relaxations for proving lower bounds. Applying these methods to a diverse testbed of 369 production models, for $k \\in \\\\{2, 4, 8, 16, 32, 64\\\\}$, we empirically show that these lower bounds are strong enough to be useful in practice. Our lower bounds are substantially stronger than standard combinatorial bounds. For example, evaluated via geometric means across a production testbed with $k = 16$ pipeline stages, our MIP formulations raise the lower bound from 0.4598 to 0.9452, expressed as a fraction of the best partition found. In other words, our improved lower bounds close the optimality gap by a factor of 9.855x",
    "checked": true,
    "id": "288f945833238aca0e63e247eb06eb578da81013",
    "semantic_title": "practical performance guarantees for pipelined dnn inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a6366lEzbX": {
    "title": "Position: What makes an image realistic?",
    "volume": "spotlight",
    "abstract": "The last decade has seen tremendous progress in our ability to *generate* realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of *quantifying* realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a *universal critic*, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool for analyzing existing attempts to capture realism",
    "checked": false,
    "id": "215c84fc184287b871338dce783d4866c40af218",
    "semantic_title": "what makes an image realistic?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Lhb39btw16": {
    "title": "FAFE: Immune Complex Modeling with Geodesic Distance Loss on Noisy Group Frames",
    "volume": "spotlight",
    "abstract": "Despite the striking success of general protein folding models such as AlphaFold2 (AF2), the accurate computational modeling of antibody-antigen complexes remains a challenging task. In this paper, we first analyze AF2's primary loss function, known as the Frame Aligned Point Error (FAPE), and raise a previously overlooked issue that FAPE tends to face gradient vanishing problem on high-rotational-error targets. To address this fundamental limitation, we propose a novel geodesic loss called Frame Aligned Frame Error (FAFE, denoted as F2E to distinguish from FAPE), which enables the model to better optimize both the rotational and translational errors between two frames. We then prove that F2E can be reformulated as a group-aware geodesic loss, which translates the optimization of the residue-to-residue error to optimizing group-to-group geodesic frame distance. By fine-tuning AF2 with our proposed new loss function, we attain a correct rate of 52.3% (DockQ > 0.23) on an evaluation set and 43.8% correct rate on a subset with low homology, with improvement over AF2 by 182% and 100% respectively",
    "checked": true,
    "id": "36d294c9589ba6472d7940071d2d03284ee5608d",
    "semantic_title": "fafe: immune complex modeling with geodesic distance loss on noisy group frames",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e1jPdRJeo7": {
    "title": "Double Variance Reduction: A Smoothing Trick for Composite Optimization Problems without First-Order Gradient",
    "volume": "spotlight",
    "abstract": "Variance reduction techniques are designed to decrease the sampling variance, thereby accelerating convergence rates of first-order (FO) and zeroth-order (ZO) optimization methods. However, in composite optimization problems, ZO methods encounter an additional variance called the coordinate-wise variance, which stems from the random gradient estimation. To reduce this variance, prior works require estimating all partial derivatives, essentially approximating FO information. This approach demands $\\mathcal{O}(d)$ function evaluations ($d$ is the dimension size), which incurs substantial computational costs and is prohibitive in high-dimensional scenarios. This paper proposes the Zeroth-order Proximal Double Variance Reduction ($\\texttt{ZPDVR}$) method, which utilizes the averaging trick to reduce both sampling and coordinate-wise variances. Compared to prior methods, $\\texttt{ZPDVR}$ relies solely on random gradient estimates, calls the stochastic zeroth-order oracle (SZO) in expectation $\\mathcal{O}(1)$ times per iteration, and achieves the optimal $\\mathcal{O}(d(n + \\kappa)\\log (\\frac{1}{\\epsilon}))$ SZO query complexity in the strongly convex and smooth setting, where $\\kappa$ represents the condition number and $\\epsilon$ is the desired accuracy. Empirical results validate $\\texttt{ZPDVR}$'s linear convergence and demonstrate its superior performance over other related methods",
    "checked": true,
    "id": "59c16ccad89e3982f33413666c7c882766faad71",
    "semantic_title": "double variance reduction: a smoothing trick for composite optimization problems without first-order gradient",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1KemC8DNa0": {
    "title": "A Circuit Domain Generalization Framework for Efficient Logic Synthesis in Chip Design",
    "volume": "spotlight",
    "abstract": "Logic Synthesis (LS) plays a vital role in chip design. A key task in LS is to simplify circuits---modeled by directed acyclic graphs (DAGs)---with functionality-equivalent transformations. To tackle this task, many LS heuristics apply transformations to subgraphs---rooted at each node on an input DAG---sequentially. However, we found that a large number of transformations are ineffective, which makes applying these heuristics highly time-consuming. In particular, we notice that the runtime of the Resub and Mfs2 heuristics often dominates the overall runtime of LS optimization processes. To address this challenge, we propose a novel data-driven LS heuristic paradigm, namely PruneX, to reduce ineffective transformations. The major challenge of developing PruneX is to learn models that well generalize to unseen circuits, i.e., the out-of-distribution (OOD) generalization problem. Thus, the major technical contribution of PruneX is the novel circuit domain generalization framework, which learns domain-invariant representations based on the transformation-invariant domain-knowledge. To the best of our knowledge, PruneX is the first approach to tackle the OOD problem in LS heuristics. We integrate PruneX with the aforementioned Resub and Mfs2 heuristics. Experiments demonstrate that PruneX significantly improves their efficiency while keeping comparable optimization performance on industrial and very large-scale circuits, achieving up to $3.1\\times$ faster runtime",
    "checked": true,
    "id": "24a864008bc1f132a7ac4a47f2bbf91952822f0b",
    "semantic_title": "a circuit domain generalization framework for efficient logic synthesis in chip design",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DHtF8Y6PqS": {
    "title": "Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss",
    "volume": "spotlight",
    "abstract": "In this work, we study statistical learning with dependent data and square loss in a hypothesis class with tail decay in Orlicz space: $\\mathscr{F}\\subset L_{\\Psi_p}$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent (e.g. $\\beta$-mixing) data. Typical non-asymptotic results exhibit variance proxies that are deflated *multiplicatively* in the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\\Psi_p$ are comparable on our hypothesis class $\\mathscr{F}$, the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. We refer to this as a *near mixing-free rate*, since direct dependence on mixing is relegated to an additive higher order term. Our approach, reliant on mixed tail generic chaining, allows us to obtain sharp, instance-optimal rates. Examples that satisfy our framework include for instance sub-Gaussian linear regression and bounded smoothness classes",
    "checked": true,
    "id": "700c519d6e64e1052286deaced0c3ed80c697169",
    "semantic_title": "sharp rates in dependent learning theory: avoiding sample size deflation for the square loss",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=3KxPo62PYn": {
    "title": "Navigating Scaling Laws: Compute Optimality in Adaptive Model Training",
    "volume": "spotlight",
    "abstract": "In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a 'compute-optimal' model, i.e. a model that allocates a given level of compute during training optimally to maximize performance. In this work, we extend the concept of optimality by allowing for an 'adaptive' model, i.e. a model that can change its shape during training. By doing so, we can design adaptive models that optimally traverse between the underlying scaling laws and outpace their `static' counterparts, leading to a significant reduction in the required compute to reach a given target performance. We show that our approach generalizes across modalities and different shape parameters",
    "checked": true,
    "id": "c0075372464eb68011f68d0293849bcab9159355",
    "semantic_title": "navigating scaling laws: compute optimality in adaptive model training",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=InUUQkExsw": {
    "title": "Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "We study risk-sensitive reinforcement learning (RL), a crucial field due to its ability to enhance decision-making in scenarios where it is essential to manage uncertainty and minimize potential adverse outcomes. Particularly, our work focuses on applying the entropic risk measure to RL problems. While existing literature primarily investigates the online setting, there remains a large gap in understanding how to efficiently derive a near-optimal policy based on this risk measure using only a pre-collected dataset. We center on the linear Markov Decision Process (MDP) setting, a well-regarded theoretical framework that has yet to be examined from a risk-sensitive standpoint. In response, we introduce two provably sample-efficient algorithms. We begin by presenting a risk-sensitive pessimistic value iteration algorithm, offering a tight analysis by leveraging the structure of the risk-sensitive performance measure. To further improve the obtained bounds, we propose another pessimistic algorithm that utilizes variance information and reference-advantage decomposition, effectively improving both the dependence on the space dimension $d$ and the risk-sensitivity factor. To the best of our knowledge, we obtain the first provably efficient risk-sensitive offline RL algorithms",
    "checked": false,
    "id": "f3112ed57ee120ccc5a0b496f7a5feeb5f3f74af",
    "semantic_title": "uncertainty-aware distributional offline reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uN39Tt9P8b": {
    "title": "Conformal prediction for multi-dimensional time series by ellipsoidal sets",
    "volume": "spotlight",
    "abstract": "Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called $\\texttt{MultiDimSPCI}$ that builds prediction $\\textit{regions}$ for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate $\\textit{finite-sample}$ high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that $\\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines",
    "checked": true,
    "id": "8a88035efc9de91afa64c3343fbe629f28e8000c",
    "semantic_title": "conformal prediction for multi-dimensional time series by ellipsoidal sets",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=EdRb84fiJY": {
    "title": "Asymptotics of feature learning in two-layer networks after one gradient-step",
    "volume": "spotlight",
    "abstract": "In this manuscript, we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging the insight from (Ba et al., 2022), we model the trained network by a spiked Random Features (sRF) model. Further building on recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error of the sRF in the high-dimensional limit where the number of samples, the width, and the input dimension grow at a proportional rate. The resulting characterization for sRFs also captures closely the learning curves of the original network model. This enables us to understand how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient - where at initialization it can only express linear functions in this regime",
    "checked": true,
    "id": "ecc98cc49d0e30355ceeb887dbcb6c5c3fa0f6e3",
    "semantic_title": "asymptotics of feature learning in two-layer networks after one gradient-step",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=0zbxwvJqwf": {
    "title": "Robust Optimization in Protein Fitness Landscapes Using Reinforcement Learning in Latent Space",
    "volume": "spotlight",
    "abstract": "Proteins are complex molecules responsible for different functions in nature. Enhancing the functionality of proteins and cellular fitness can significantly impact various industries. However, protein optimization using computational methods remains challenging, especially when starting from low-fitness sequences. We propose LatProtRL, an optimization method to efficiently traverse a latent space learned by an encoder-decoder leveraging a large protein language model. To escape local optima, our optimization is modeled as a Markov decision process using reinforcement learning acting directly in latent space. We evaluate our approach on two important fitness optimization tasks, demonstrating its ability to achieve comparable or superior fitness over baseline methods. Our findings and in vitro evaluation show that the generated sequences can reach high-fitness regions, suggesting a substantial potential of LatProtRL in lab-in-the-loop scenarios",
    "checked": true,
    "id": "a9589c7a5062134012d56bf3ea6aedd1d76bfb1d",
    "semantic_title": "robust optimization in protein fitness landscapes using reinforcement learning in latent space",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yY6N89IlHa": {
    "title": "CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural Networks",
    "volume": "spotlight",
    "abstract": "Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Compared to conventional deep Artificial Neural Networks (ANNs), SNNs exhibit superior efficiency and capability to process temporal information. However, it remains a challenge to train SNNs due to their undifferentiable spiking mechanism. The surrogate gradients method is commonly used to train SNNs, but often comes with an accuracy disadvantage over ANNs counterpart. We link the degraded accuracy to the vanishing of gradient on the temporal dimension through the analytical and experimental study of the training process of Leaky Integrate-and-Fire (LIF) Neuron-based SNNs. Moreover, we propose the Complementary Leaky Integrate-and-Fire (CLIF) Neuron. CLIF creates extra paths to facilitate the backpropagation in computing temporal gradient while keeping binary output. CLIF is hyperparameter-free and features broad applicability. Extensive experiments on a variety of datasets demonstrate CLIF's clear performance advantage over other neuron models. Furthermore, the CLIF's performance even slightly surpasses superior ANNs with identical network structure and training conditions. The code is available at https://github.com/HuuYuLong/Complementary-LIF",
    "checked": true,
    "id": "a59bce51ffc91f03f990859af7ece7a166155329",
    "semantic_title": "clif: complementary leaky integrate-and-fire neuron for spiking neural networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=01ahsMovBx": {
    "title": "One Meta-tuned Transformer is What You Need for Few-shot Learning",
    "volume": "spotlight",
    "abstract": "Pre-trained vision transformers have revolutionized few-shot image classification, and it has been recently demonstrated that the previous common practice of meta-learning in synergy with these pre-trained transformers still holds significance. In this work, we design a new framework centered exclusively on self-attention, called MetaFormer, which extends the vision transformers beyond patch token interactions to encompass relationships between samples and tasks simultaneously for further advancing their downstream task performance. Leveraging the intrinsical property of ViTs in handling local patch relationships, we propose Masked Sample Attention (MSA) to efficiently embed the sample relationships into the network, where an adaptive mask is attached for enhancing task-specific feature consistency and providing flexibility in switching between few-shot learning setups. To encapsulate task relationships while filtering out background noise, Patch-grained Task Attention (PTA) is designed to maintain a dynamic knowledge pool consolidating diverse patterns from historical tasks. MetaFormer demonstrates coherence and compatibility with off-the-shelf pre-trained vision transformers and shows significant improvements in both inductive and transductive few-shot learning scenarios, outperforming state-of-the-art methods by up to 8.77% and 6.25% on 12 in-domain and 10 cross-domain datasets, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X9VMhfFxwn": {
    "title": "Mixtures of Experts Unlock Parameter Scaling for Deep RL",
    "volume": "spotlight",
    "abstract": "The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning",
    "checked": true,
    "id": "e1470b674dd6abd78f5d086e70a20ae783c2ad40",
    "semantic_title": "mixtures of experts unlock parameter scaling for deep rl",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=leJGQCron2": {
    "title": "On the Complexity of Finite-Sum Smooth Optimization under the Polyak–Łojasiewicz Condition",
    "volume": "spotlight",
    "abstract": "This paper considers the optimization problem of the form $\\min_{{\\bf x}\\in{\\mathbb R}^d} f({\\bf x})\\triangleq \\frac{1}{n}\\sum_{i=1}^n f_i({\\bf x})$, where $f(\\cdot)$ satisfies the Polyak–Łojasiewicz (PL) condition with parameter $\\mu$ and $\\{f_i(\\cdot)\\}_{i=1}^n$ is $L$-mean-squared smooth. We show that any gradient method requires at least $\\Omega(n+\\kappa\\sqrt{n}\\log(1/\\epsilon))$ incremental first-order oracle (IFO) calls to find an $\\epsilon$-suboptimal solution, where $\\kappa\\triangleq L/\\mu$ is the condition number of the problem. This result nearly matches upper bounds of IFO complexity for best-known first-order methods. We also study the problem of minimizing the PL function in the distributed setting such that the individuals $f_1(\\cdot),\\dots,f_n(\\cdot)$ are located on a connected network of $n$ agents. We provide lower bounds of $\\Omega(\\kappa/\\sqrt{\\gamma}\\log(1/\\epsilon))$, $\\Omega((\\kappa+\\tau\\kappa/\\sqrt{\\gamma})\\log(1/\\epsilon))$ and $\\Omega\\big(n+\\kappa\\sqrt{n}\\log(1/\\epsilon)\\big)$ for communication rounds, time cost and local first-order oracle calls respectively, where $\\gamma\\in(0,1]$ is the spectral gap of the mixing matrix associated with the network and $\\tau>0$ is the time cost of per communication round. Furthermore, we propose a decentralized first-order method that nearly matches above lower bounds in expectation",
    "checked": false,
    "id": "5b1ee0c2d34089f03a70a1f2c3e17c1cb6152085",
    "semantic_title": "on the complexity of finite-sum smooth optimization under the polyak-łojasiewicz condition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AG45XqwPKU": {
    "title": "Learning Exceptional Subgroups by End-to-End Maximizing KL-Divergence",
    "volume": "spotlight",
    "abstract": "Finding and describing sub-populations that are exceptional in terms of a target property has important applications in many scientific disciplines, from identifying disadvantaged demographic groups in census data to finding conductive molecules within gold nanoparticles. Current approaches to finding such subgroups require pre-discretized predictive variables, do not permit non-trivial target distributions, do not scale to large datasets, and struggle to find diverse results. To address these limitations, we propose SYFLOW, an end-to-end optimizable approach in which we leverage normalizing flows to model arbitrary target distributions and introduce a novel neural layer that results in easily interpretable subgroup descriptions. We demonstrate on synthetic data, real-world data, and via a case study, that SYFLOW reliably finds highly exceptional subgroups accompanied by insightful descriptions",
    "checked": true,
    "id": "a9e2d445f1e627a7effb7450a19efdc83a789f3c",
    "semantic_title": "learning exceptional subgroups by end-to-end maximizing kl-divergence",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WvIHbQhrTq": {
    "title": "Leveraging (Biased) Information: Multi-armed Bandits with Offline Data",
    "volume": "spotlight",
    "abstract": "We leverage offline data to facilitate online learning in stochastic multi-armed bandits. The probability distributions that govern the offline data and the online rewards can be different. Without any non-trival upper bound on their difference, we show that no non-anticipatory policy can out-perform the UCB policy by (Auer et al. 2002), even in the presence of offline data. In complement, we propose an online policy MIN-UCB, which outperforms UCB when a non-trivial upper bound is given. MIN-UCB adaptively chooses to utilize the offline data when they are deemed informative, and to ignore them otherwise. MIN-UCB is shown to be tight in terms of both instance indepedent and dependent regret bounds. Finally, we corroborate the theoretical results with numerical experiments",
    "checked": true,
    "id": "2fa11dd2bcf807095bb450a24d9528ce25e8eb25",
    "semantic_title": "leveraging (biased) information: multi-armed bandits with offline data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60F0fVbknK": {
    "title": "Learning Causal Relations from Subsampled Time Series with Two Time-Slices",
    "volume": "spotlight",
    "abstract": "This paper studies the causal relations from subsampled time series, in which measurements are sparse and sampled at a coarser timescale than the causal timescale of the underlying system. In such data, because there are numerous missing time-slices (i.e., cross-sections at each time point) between two consecutive measurements, conventional causal discovery methods designed for standard time series data would produce significant errors. To learn causal relations from subsampled time series, a typical solution is to conduct different interventions and then make a comparison. However, full interventions are often expensive, unethical, or even infeasible, particularly in fields such as health and social science. In this paper, we first explore how readily available two-time-slices data can replace intervention data to improve causal ordering, and propose a novel Descendant Hierarchical Topology algorithm with Conditional Independence Test (DHT-CIT) to learn causal relations from subsampled time series using only two time-slices. Specifically, we develop a conditional independence criterion that can be applied iteratively to test each node from time series and identify all of its descendant nodes. Empirical results on both synthetic and real-world datasets demonstrate the superiority of our DHT-CIT algorithm",
    "checked": false,
    "id": "3ece8120233ea379107440ea9297d6be6eef93db",
    "semantic_title": "inferring extended summary causal graphs from observational time series",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=guFsTBXsov": {
    "title": "Equivariance via Minimal Frame Averaging for More Symmetries and Efficiency",
    "volume": "spotlight",
    "abstract": "We consider achieving equivariance in machine learning systems via frame averaging. Current frame averaging methods involve a costly sum over large frames or rely on sampling-based approaches that only yield approximate equivariance. Here, we propose Minimal Frame Averaging (MFA), a mathematical framework for constructing provably minimal frames that are exactly equivariant. The general foundations of MFA also allow us to extend frame averaging to more groups than previously considered, including the Lorentz group for describing symmetries in space-time, and the unitary group for complex-valued domains. Results demonstrate the efficiency and effectiveness of encoding symmetries via MFA across a diverse range of tasks, including $n$-body simulation, top tagging in collider physics, and relaxed energy prediction. Our code is available at https://github.com/divelab/MFA",
    "checked": true,
    "id": "ef4ec66e57e5d1b3bb17622ac5ec4b4de17220c7",
    "semantic_title": "equivariance via minimal frame averaging for more symmetries and efficiency",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rucbIsWoEV": {
    "title": "Dynamic Facility Location in High Dimensional Euclidean Spaces",
    "volume": "spotlight",
    "abstract": "We study the facility location problem in the dynamic setting, where the goal is to efficiently process an intermixed sequence of point insertions and deletions while maintaining a high quality and stable solution. Although the problem has been studied in the context of general metrics and low-dimensional spaces, much remains unknown concerning dynamic facility location in high dimensional spaces. In this work, we present the first fully dynamic algorithm for facility location in high-dimensional spaces $\\mathbb{R}^{d}$. For any $c \\geq 1$, our algorithm achieves $O(c)$-approximation, supports point updates in $\\tilde{O}(\\mathrm{poly}(d)n^{1/c + o(1)})$ amortized time and incurs $O(1)$ amortized recourse. More generally, our result shows that despite the linear-time lower bound on the update time for general metrics, it is possible to achieve sub-linear update times for metric spaces that admit dynamic nearest neighbour oracles. Experiments on real datasets confirm that our algorithm achieves high-quality solutions with low running time, and incurs minimal recourse",
    "checked": false,
    "id": "6d9ee6387f05354e867828bad859bded5ba438b8",
    "semantic_title": "streaming facility location in high dimension via geometric hashing",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=bzNwexOPWm": {
    "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
    "volume": "spotlight",
    "abstract": "Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting---the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products of example representations achieves better forecasting performance over a series of setups. Finally, we show that we reduce forgetting of upstream pretraining examples by replaying examples that are forecasted to be forgotten, demonstrating the practical utility of forecasting example forgetting",
    "checked": true,
    "id": "50c9dc9907d19afcafc082332e53b5a0ac58956d",
    "semantic_title": "what will my model forget? forecasting forgotten examples in language model refinement",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qXoqV40imX": {
    "title": "Defining Neural Network Architecture through Polytope Structures of Datasets",
    "volume": "spotlight",
    "abstract": "Current theoretical and empirical research in neural networks suggests that complex datasets require large network architectures for thorough classification, yet the precise nature of this relationship remains unclear. This paper tackles this issue by defining upper and lower bounds for neural network widths, which are informed by the polytope structure of the dataset in question. We also delve into the application of these principles to simplicial complexes and specific manifold shapes, explaining how the requirement for network width varies in accordance with the geometric complexity of the dataset. Moreover, we develop an algorithm to investigate a converse situation where the polytope structure of a dataset can be inferred from its corresponding trained neural networks. Through our algorithm, it is established that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be efficiently encapsulated using no more than two polytopes with a small number of faces",
    "checked": false,
    "id": "d4705141c495b272e19761d5379631fcdcc729a0",
    "semantic_title": "defining neural network architecture through polytope structures of dataset",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PNsdnl8blk": {
    "title": "Extending Test-Time Augmentation with Metamorphic Relations for Combinatorial Problems",
    "volume": "spotlight",
    "abstract": "The application of machine learning methods to solve combinatorial problems has garnered considerable research interest. In this paper, we propose MAgg (**M**etamorphic **Agg**regation), a method to augment machine learning models for combinatorial problems at inference time using metamorphic relations. MAgg models metamorphic relations using directed graphs, which are then fed to a Graph Neural Network (GNN) model to improve the aggregation of predictions across transformed input instances. By incorporating metamorphic relations, MAgg essentially extends standard Test-Time Augmentation (TTA), eliminating the necessity of label-preserving transformations and expanding its applicability to a broader range of supervised learning tasks for combinatorial problems. We evaluate the proposed MAgg method on three mainstream machine learning tasks for combinatorial problems, namely Boolean Satisfiability Prediction (SAT), Decision Traveling Salesman Problem Satisfiability Prediction (Decision TSP), and Graph Edit Distance Estimation (GED). The evaluation result shows significant improvements over base models in all three tasks, corroborating the effectiveness and versatility of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PpBs2iL0jv": {
    "title": "Bridging Data Gaps in Diffusion Models with Adversarial Noise-Based Transfer Learning",
    "volume": "spotlight",
    "abstract": "Diffusion Probabilistic Models (DPMs) show significant potential in image generation, yet their performance hinges on having access to large datasets. Previous works, like Generative Adversarial Networks (GANs), have tackled the limited data problem by transferring pre-trained models learned with sufficient data. However, those methods are hard to be utilized in DPMs since the distinct differences between DPM-based and GAN-based methods, showing in the unique iterative denoising process integral and the need for many timesteps with no-targeted noise in DPMs. In this paper, we propose a novel DPMs-based transfer learning method, ANT, to address the limited data problem. It includes two strategies: similarity-guided training, which boosts transfer with a classifier, and adversarial noise selection which adaptively chooses targeted noise based on the input image. Extensive experiments in the context of few-shot image generation tasks demonstrate that our method is not only efficient but also excels in terms of image quality and diversity when compared to existing GAN-based and DDPM-based methods",
    "checked": false,
    "id": "5ba65dfca3f1f588cc1ddd85e4fbbffde895b71d",
    "semantic_title": "self-supervised learning of time series representation via diffusion process and imputation-interpolation-forecasting mask",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zWIS8I9G9B": {
    "title": "Handling Heterogeneous Curvatures in Bandit LQR Control",
    "volume": "spotlight",
    "abstract": "We investigate online Linear Quadratic Regulator (LQR) with bandit feedback and semi-adversarial disturbances. Previous works assume costs with *homogeneous* curvatures (i.e., with a uniform strong convexity lower bound), which can be hard to satisfy in many real scenarios and prohibits adapting to true curvatures for better performance. In this paper, we initiate the study of bandit LQR control with *heterogeneous* cost curvatures, aiming to strengthen the algorithm's adaptivity. To achieve this, we reduce the problem to bandit convex optimization with memory via a ``with-history'' reduction to avoid hard-to-control truncation errors. Then we provide a novel analysis for an important *stability* term that appeared in both regret and memory, using *Newton decrement* developed in interior-point methods. The analysis enables us to guarantee memory-related terms introduced in the reduction and also provide a simplified analysis for handling heterogeneous curvatures in bandit convex optimization. Finally, we achieve interpolated guarantees that can not only recover existing bounds for convex and quadratic costs but also attain new implications for cases of corrupted and decaying quadraticity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bvPYroQgc3": {
    "title": "Optimal Ridge Regularization for Out-of-Distribution Prediction",
    "volume": "spotlight",
    "abstract": "We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the train or the test distributions, except for moment bounds, and allow for arbitrary shifts and the widest possible range of (negative) regularization levels",
    "checked": true,
    "id": "0a96442dadb740ae7049f1c73565984ed9f0c3a4",
    "semantic_title": "optimal ridge regularization for out-of-distribution prediction",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=KOW9ncAiRo": {
    "title": "Optimal Kernel Quantile Learning with Random Features",
    "volume": "spotlight",
    "abstract": "The random feature (RF) approach is a well-established and efficient tool for scalable kernel methods, but existing literature has primarily focused on kernel ridge regression with random features (KRR-RF), which has limitations in handling heterogeneous data with heavy-tailed noises. This paper presents a generalization study of kernel quantile regression with random features (KQR-RF), which accounts for the non-smoothness of the check loss in KQR-RF by introducing a refined error decomposition and establishing a novel connection between KQR-RF and KRR-RF. Our study establishes the capacity-dependent learning rates for KQR-RF under mild conditions on the number of RFs, which are minimax optimal up to some logarithmic factors. Importantly, our theoretical results, utilizing a data-dependent sampling strategy, can be extended to cover the agnostic setting where the target quantile function may not precisely align with the assumed kernel space. By slightly modifying our assumptions, the capacity-dependent error analysis can also be applied to cases with Lipschitz continuous losses, enabling broader applications in the machine learning community. To validate our theoretical findings, simulated experiments and a real data application are conducted",
    "checked": false,
    "id": "981216f5c50d2f07b53fc6c642b2a55ea53b8952",
    "semantic_title": "on optimal learning with random features",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=PKJqsZD5nQ": {
    "title": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation",
    "volume": "spotlight",
    "abstract": "Deep reinforcement learning (DRL) is playing an increasingly important role in real-world applications. However, obtaining an optimally performing DRL agent for complex tasks, especially with sparse rewards, remains a significant challenge. The training of a DRL agent can be often trapped in a bottleneck without further progress. In this paper, we propose RICE, an innovative refining scheme for reinforcement learning that incorporates explanation methods to break through the training bottlenecks. The high-level idea of RICE is to construct a new initial state distribution that combines both the default initial states and critical states identified through explanation methods, thereby encouraging the agent to explore from the mixed initial states. Through careful design, we can theoretically guarantee that our refining scheme has a tighter sub-optimality bound. We evaluate RICE in various popular RL environments and real-world applications. The results demonstrate that RICE significantly outperforms existing refining schemes in enhancing agent performance",
    "checked": true,
    "id": "a8cf219c83b66995f5fa535078e4a9e3f3e68a01",
    "semantic_title": "rice: breaking through the training bottlenecks of reinforcement learning with explanation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5FIjG9fbs": {
    "title": "Prospective Side Information for Latent MDPs",
    "volume": "spotlight",
    "abstract": "In many interactive decision-making problems, there is contextual side information that remains fixed within the course of an interaction. This problem has been studied quite extensively under the assumption the context is fully observed, as well as in the opposing limit when the context is unobserved, a special type of POMDP also referred to as a Latent MDP (LMDP). In this work, we consider a class of decision problems that interpolates between the settings, namely, between the case the context is fully observed, and the case the context is unobserved. We refer to this class of decision problems as *LMDPs with prospective side information*. In such an environment an agent receives additional, weakly revealing, information on the latent context at the beginning of each episode. We show that, surprisingly, this problem is not captured by contemporary POMDP settings and is not solved by RL algorithms designed for partially observed environments. We then establish that any sample efficient algorithm must suffer at least $\\Omega(K^{2/3})$-regret, as opposed to standard $\\Omega(\\sqrt{K})$ lower bounds. We design an algorithm with a matching upper bound that depends only polynomially on the problem parameters. This establishes exponential improvement in the sample complexity relative to the existing LMDP lower bound, when prospective information is not given in prior work",
    "checked": true,
    "id": "8ac1868105ea5427476c3f1a558093fbc326b5ab",
    "semantic_title": "prospective side information for latent mdps",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=3YG55Lbcnr": {
    "title": "Dynamic Correlation Clustering in Sublinear Update Time",
    "volume": "spotlight",
    "abstract": "We study the classic problem of correlation clustering in dynamic vertex streams. In this setting, vertices are either added or randomly deleted over time, and each vertex pair is connected by a positive or negative edge. The objective is to continuously find a partition which minimizes the sum of positive edges crossing clusters and negative edges within clusters. We present an algorithm that maintains an $O(1)$-approximation with $O(\\text{polylog} n)$ amortized update time. Prior to our work Behnezhad et al. in SODA 2023 achieved a $5$-approximation with $O(1)$ expected update time in edge streams which translates in vertex streams to an $O(D)$-update time where $D$ is the maximum possible degree. Finally we complement our theoretical analysis with experiments on real world data",
    "checked": true,
    "id": "89576139894b8e6e42b314bee2bd63a4efe29ee1",
    "semantic_title": "dynamic correlation clustering in sublinear update time",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SfcB4cVvPz": {
    "title": "A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural Networks",
    "volume": "spotlight",
    "abstract": "The rising threat of backdoor poisoning attacks (BPAs) on Deep Neural Networks (DNNs) has become a significant concern in recent years. In such attacks, the adversaries strategically target a specific class and generate a poisoned training set. The neural network (NN), well-trained on the poisoned training set, is able to predict any input with the trigger pattern as the targeted label, while maintaining accurate outputs for clean inputs. However, why the BPAs work remains less explored. To fill this gap, we employ a dirty-label attack and conduct a detailed analysis of BPAs in a two-layer convolutional neural network. We provide theoretical insights and results on the effectiveness of BPAs. Our experimental results on two real-world datasets validate our theoretical findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tya725xlZ3": {
    "title": "Masked Face Recognition with Generative-to-Discriminative Representations",
    "volume": "spotlight",
    "abstract": "Masked face recognition is important for social good but challenged by diverse occlusions that cause insufficient or inaccurate representations. In this work, we propose a unified deep network to learn generative-to-discriminative representations for facilitating masked face recognition. To this end, we split the network into three modules and learn them on synthetic masked faces in a greedy module-wise pretraining manner. First, we leverage a generative encoder pretrained for face inpainting and finetune it to represent masked faces into category-aware descriptors. Attribute to the generative encoder's ability in recovering context information, the resulting descriptors can provide occlusion-robust representations for masked faces, mitigating the effect of diverse masks. Then, we incorporate a multi-layer convolutional network as a discriminative reformer and learn it to convert the category-aware descriptors into identity-aware vectors, where the learning is effectively supervised by distilling relation knowledge from off-the-shelf face recognition model. In this way, the discriminative reformer together with the generative encoder serves as the pretrained backbone, providing general and discriminative representations towards masked faces. Finally, we cascade one fully-connected layer following by one softmax layer into a feature classifier and finetune it to identify the reformed identity-aware vectors. Extensive experiments on synthetic and realistic datasets demonstrate the effectiveness of our approach in recognizing masked faces",
    "checked": true,
    "id": "95087cb126f0745b58c3f423b1e76eae23f74354",
    "semantic_title": "masked face recognition with generative-to-discriminative representations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jS3CMHtYJD": {
    "title": "No Dimensional Sampling Coresets for Classification",
    "volume": "spotlight",
    "abstract": "We refine and generalize what is known about coresets for classification problems via the sensitivity sampling framework. Such coresets seek the smallest possible subsets of input data, so one can optimize a loss function on the coreset and ensure approximation guarantees with respect to the original data. Our analysis provides the first no dimensional coresets, so the size does not depend on the dimension. Moreover, our results are general, apply for distributional input and can use iid samples, so provide sample complexity bounds, and work for a variety of loss functions. A key tool we develop is a Radamacher complexity version of the main sensitivity sampling approach, which can be of independent interest",
    "checked": true,
    "id": "cdda6d099fa12e87faa6fec000452180b4152eb5",
    "semantic_title": "no dimensional sampling coresets for classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLtVGtEz5h": {
    "title": "Stereographic Spherical Sliced Wasserstein Distances",
    "volume": "spotlight",
    "abstract": "Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both speed and accuracy through a wide range of numerical studies, including gradient flows and self-supervised learning. Our code is available at https://github.com/mint-vu/s3wd",
    "checked": true,
    "id": "811c5f8f200f291687f94a03a389f6afb6a7cac9",
    "semantic_title": "stereographic spherical sliced wasserstein distances",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Dwc0RwiNI5": {
    "title": "Faster Adaptive Decentralized Learning Algorithms",
    "volume": "spotlight",
    "abstract": "Decentralized learning recently has received increasing attention in machine learning due to its advantages in implementation simplicity and system robustness, data privacy. Meanwhile, the adaptive gradient methods show superior performances in many machine learning tasks such as training neural networks. Although some works focus on studying decentralized optimization algorithms with adaptive learning rates, these adaptive decentralized algorithms still suffer from high sample complexity. To fill these gaps, we propose a class of faster adaptive decentralized algorithms (i.e., AdaMDOS and AdaMDOF) for distributed nonconvex stochastic and finite-sum optimization, respectively. Moreover, we provide a solid convergence analysis framework for our methods. In particular, we prove that our AdaMDOS obtains a near-optimal sample complexity of $\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary solution of nonconvex stochastic optimization. Meanwhile, our AdaMDOF obtains a near-optimal sample complexity of $O(\\sqrt{n}\\epsilon^{-2})$ for finding an $\\epsilon$-stationary solution of for nonconvex finite-sum optimization, where $n$ denotes the sample size. To the best of our knowledge, our AdaMDOF algorithm is the first adaptive decentralized algorithm for nonconvex finite-sum optimization. Some experimental results demonstrate efficiency of our algorithms",
    "checked": false,
    "id": "1fd1109ec922dcd683190a42dd5712b4eb6113b2",
    "semantic_title": "faster adaptive federated learning",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=wTd7dogTsB": {
    "title": "Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions",
    "volume": "spotlight",
    "abstract": "We study the asymptotic error of score-based diffusion model sampling in large-sample scenarios from a non-parametric statistics perspective. We show that a kernel-based score estimator achieves an optimal mean square error of $\\widetilde{O}\\left(n^{-1} t^{-\\frac{d+2}{2}}(t^{\\frac{d}{2}} \\vee 1)\\right)$ for the score function of $p_0*\\mathcal{N}(0,t\\boldsymbol{I}_d)$, where $n$ and $d$ represent the sample size and the dimension, $t$ is bounded above and below by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As a consequence, this yields an $\\widetilde{O}\\left(n^{-1/2} t^{-\\frac{d}{4}}\\right)$ upper bound for the total variation error of the distribution of the sample generated by the diffusion model under a mere sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric family of the $\\beta$-Sobolev space with $\\beta\\le 2$, by adopting an early stopping strategy, we obtain that the diffusion model is nearly (up to log factors) minimax optimal. This removes the crucial lower bound assumption on $p_0$ in previous proofs of the minimax optimality of the diffusion model for nonparametric families",
    "checked": true,
    "id": "0ad5f33cf7007b3a60983053b3b14e12b98fb798",
    "semantic_title": "minimax optimality of score-based diffusion models: beyond the density lower bound assumptions",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=WQbDS9RydY": {
    "title": "Memorization Through the Lens of Curvature of Loss Function Around Samples",
    "volume": "spotlight",
    "abstract": "Deep neural networks are over-parameterized and easily overfit to and memorize the datasets that they train on. In the extreme case, it has been shown that networks can memorize a randomly labeled dataset. In this paper, we propose using the curvature of the loss function around each training sample, averaged over training epochs, as a measure of memorization of a sample. We show that this curvature metric effectively captures memorization statistics, both qualitatively and quantitatively in popular image datasets. We provide quantitative validation of the proposed metric against memorization scores released by Feldman & Zhang (2020). Further, experiments on mislabeled data detection show that corrupted samples are learned with high curvature and using curvature for identifying mislabelled examples outperforms existing approaches. Qualitatively, we find that high curvature samples correspond to long-tailed, mislabeled, or conflicting instances, indicating a likelihood of memorization. Notably, this analysis helps us find, to the best of our knowledge, a novel failure mode on the CIFAR100 and ImageNet datasets: that of duplicated images with differing labels",
    "checked": true,
    "id": "ae9a30ed24edab448cdb7a2e5269deb9713d2048",
    "semantic_title": "memorization through the lens of curvature of loss function around samples",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=OdPlFWExX1": {
    "title": "Sparse and Structured Hopfield Networks",
    "volume": "spotlight",
    "abstract": "Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers. Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses. The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations. We reveal a connection between loss margins, sparsity, and exact memory retrieval. We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern. Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach",
    "checked": true,
    "id": "3144a63dc86fc58096a027779658694ee16b1ab6",
    "semantic_title": "sparse and structured hopfield networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RiQbe8RwCe": {
    "title": "Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning",
    "volume": "spotlight",
    "abstract": "The success of SGD in deep learning has been ascribed by prior works to the *implicit bias* induced by finite batch sizes (''SGD noise''). While prior works focused on *offline learning* (i.e., multiple-epoch training), we study the impact of SGD noise on *online* (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that small batch sizes do *not* confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating more cost-effective gradient steps. This suggests that SGD in the online regime can be construed as taking noisy steps along the ''golden path'' of the noiseless *gradient descent* algorithm. We study this hypothesis and provide supporting evidence in loss and function space. Our findings challenge the prevailing understanding of SGD and offer novel insights into its role in online learning",
    "checked": true,
    "id": "e288a9ea03273578bb0ee893f4cc47f9094ca6f3",
    "semantic_title": "beyond implicit bias: the insignificance of sgd noise in online learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=eY4jrFe6Qc": {
    "title": "Towards Theoretical Understanding of Learning Large-scale Dependent Data via Random Features",
    "volume": "spotlight",
    "abstract": "Random feature (RF) mapping is an attractive and powerful technique for solving large-scale nonparametric regression. Yet, the existing theoretical analysis crucially relies on the i.i.d. assumption that individuals in the data are independent and identically distributed. It is still unclear whether learning accuracy would be compromised when the i.i.d. assumption is violated. This paper aims to provide theoretical understanding of the kernel ridge regression (KRR) with RFs for large-scale dependent data. Specifically, we consider two types of data dependence structure, namely, the $\\tau$-mixing process with exponential decay coefficient, and that with polynomial decay coefficient. Theoretically, we prove that the kernel ridge estimator with RFs achieves the minimax optimality under the exponential decay scenario, but yields a sub-optimal result under the polynomial decay case. Our analysis further reveals how the decay rate of the $\\tau$-mixing coefficient impacts the learning accuracy of the kernel ridge estimator with RFs. Extensive numerical experiments on both synthetic and real examples further validate our theoretical findings and support the effectiveness of the KRR with RFs in dealing with dependent data",
    "checked": false,
    "id": "acf62b27e2f9fdc98b13d87f6a44e8586db11cae",
    "semantic_title": "battery management system design and implementation in | c39433310396b9edc4b2cf898ef36027",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VuoB86HiCL": {
    "title": "Automating the Selection of Proxy Variables of Unmeasured Confounders",
    "volume": "spotlight",
    "abstract": "Recently, interest has grown in the use of proxy variables of unobserved confounding for inferring the causal effect in the presence of unmeasured confounders from observational data. One difficulty inhibiting the practical use is finding valid proxy variables of unobserved confounding to a target causal effect of interest. These proxy variables are typically justified by background knowledge. In this paper, we investigate the estimation of causal effects among multiple treatments and a single outcome, all of which are affected by unmeasured confounders, within a linear causal model, without prior knowledge of the validity of proxy variables. To be more specific, we first extend the existing proxy variable estimator, originally addressing a single unmeasured confounder, to accommodate scenarios where multiple unmeasured confounders exist between the treatments and the outcome. Subsequently, we present two different sets of precise identifiability conditions for selecting valid proxy variables of unmeasured confounders, based on the second-order statistics and higher-order statistics of the data, respectively. Moreover, we propose two data-driven methods for the selection of proxy variables and for the unbiased estimation of causal effects. Theoretical analysis demonstrates the correctness of our proposed algorithms. Experimental results on both synthetic and real-world data show the effectiveness of the proposed approach",
    "checked": true,
    "id": "f9881b78a5626c8bf021f509b3b0a9f521a23d62",
    "semantic_title": "automating the selection of proxy variables of unmeasured confounders",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6P88DMUDvH": {
    "title": "Code as Reward: Empowering Reinforcement Learning with VLMs",
    "volume": "spotlight",
    "abstract": "Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, and can be more effective in training RL policies than the original sparse environment rewards",
    "checked": true,
    "id": "793f8572a022866caa66e44c98ff2839c1b4587a",
    "semantic_title": "code as reward: empowering reinforcement learning with vlms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LHGMXcr6zx": {
    "title": "EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data",
    "volume": "spotlight",
    "abstract": "Sample efficiency remains a crucial challenge in applying Reinforcement Learning (RL) to real-world tasks. While recent algorithms have made significant strides in improving sample efficiency, none have achieved consistently superior performance across diverse domains. In this paper, we introduce EfficientZero V2, a general framework designed for sample-efficient RL algorithms. We have expanded the performance of EfficientZero to multiple domains, encompassing both continuous and discrete actions, as well as visual and low-dimensional inputs. With a series of improvements we propose, EfficientZero V2 outperforms the current state-of-the-art (SoTA) by a significant margin in diverse tasks under the limited data setting. EfficientZero V2 exhibits a notable advancement over the prevailing general algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks across multiple benchmarks, including Atari 100k, Proprio Control, and Vision Control",
    "checked": true,
    "id": "9d7b0bd13d5d7efd31123691c5960993fe610239",
    "semantic_title": "efficientzero v2: mastering discrete and continuous control with limited data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=PDUQRBPkks": {
    "title": "Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery",
    "volume": "spotlight",
    "abstract": "In this paper, we focus on distributed estimation and support recovery for high-dimensional linear quantile regression. Quantile regression is a popular alternative tool to the least squares regression for robustness against outliers and data heterogeneity. However, the non-smoothness of the check loss function poses big challenges to both computation and theory in the distributed setting. To tackle these problems, we transform the original quantile regression into the least-squares optimization. By applying a double-smoothing approach, we extend a previous Newton-type distributed approach without the restrictive independent assumption between the error term and covariates. An efficient algorithm is developed, which enjoys high computation and communication efficiency. Theoretically, the proposed distributed estimator achieves a near-oracle convergence rate and high support recovery accuracy after a constant number of iterations. Extensive experiments on synthetic examples and a real data application further demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "d513887b07302fa8070879398c282df4743e4ac6",
    "semantic_title": "distributed high-dimensional quantile regression: estimation efficiency and support recovery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ooh8tkXKyR": {
    "title": "A Theory of Fault-Tolerant Learning",
    "volume": "spotlight",
    "abstract": "Developing machine learning models that account for potential faults encountered in real-world environments presents a fundamental challenge for mission-critical applications. In this paper, we introduce a novel theoretical framework grounded in learning theory for dealing with faults. In particular, we propose a framework called *fault-tolerant PAC learning*, aimed at identifying the most fault-tolerant models from a given hypothesis class (such as neural networks). We show that if faults occur randomly, fault-tolerant learning is equivalent to regular PAC learning. However, for *adversarial* faults, we show that the sample complexity of fault-tolerant PAC learning can grow linearly w.r.t. the number of perturbing functions induced by the faults, even for a hypothesis class with VC-dimension 1. We then provide a matching upper bound by restricting the number of perturbing functions. Finally, we show that the linear dependency on the number of perturbing functions can be substantially improved for *deletion faults* in neural networks. Our work provides a powerful formal framework and avenues for a number of future investigations on the precise characterization of fault-tolerant learning",
    "checked": false,
    "id": "5f2775ef8f8584a2749e5d6d89c83ac7656624c3",
    "semantic_title": "adaptive fault-tolerant optimal control of quav with uncertain disturbances",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=tw1PwpuAuN": {
    "title": "Faithfulness Measurable Masked Language Models",
    "volume": "spotlight",
    "abstract": "A common approach to explaining NLP models is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues, and existing solutions that address this are computationally expensive and employ proxy models. Furthermore, other metrics are very limited in scope. This work proposes an inherently faithfulness measurable model that addresses these challenges. This is achieved using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to 16 different datasets and validate it using statistical in-distribution tests. The faithfulness is then measured with 9 different importance measures. Because masking is in-distribution, importance measures that themselves use masking become consistently more faithful. Additionally, because the model makes faithfulness cheap to measure, we can optimize explanations towards maximal faithfulness; thus, our model becomes indirectly inherently explainable",
    "checked": true,
    "id": "62587d93133a91c458103fd3a6f4b584f1f9548e",
    "semantic_title": "faithfulness measurable masked language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0P3kaNluGj": {
    "title": "End-to-End Neuro-Symbolic Reinforcement Learning with Textual Explanations",
    "volume": "spotlight",
    "abstract": "Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. NS-RL entails structured state representations for tasks with visual observations, but previous methods cannot refine the structured states with rewards due to a lack of efficiency. Accessibility also remains an issue, as extensive domain knowledge is required to interpret symbolic policies. In this paper, we present a neuro-symbolic framework for jointly learning structured states and symbolic policies, whose key idea is to distill the vision foundation model into an efficient perception module and refine it during policy learning. Moreover, we design a pipeline to prompt GPT-4 to generate textual explanations for the learned policies and decisions, significantly reducing users' cognitive load to understand the symbolic policies. We verify the efficacy of our approach on nine Atari tasks and present GPT-generated explanations for policies and decisions",
    "checked": true,
    "id": "ae2192e6185beda4d33857f49b1d01385637b816",
    "semantic_title": "end-to-end neuro-symbolic reinforcement learning with textual explanations",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=qawwyKqOkj": {
    "title": "PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses",
    "volume": "spotlight",
    "abstract": "This work studies algorithms for learning from aggregate responses. We focus on the construction of aggregation sets (called *bags* in the literature) for event-level loss functions. We prove for linear regression and generalized linear models (GLMs) that the optimal bagging problem reduces to one-dimensional size-constrained $k$-means clustering. Further, we theoretically quantify the advantage of using curated bags over random bags. We then propose the $\\texttt{PriorBoost}$ algorithm, which adaptively forms bags of samples that are increasingly homogeneous with respect to (unobserved) individual responses to improve model quality. We study label differential privacy for aggregate learning, and we also provide extensive experiments showing that $\\texttt{PriorBoost}$ regularly achieves optimal model quality for event-level predictions, in stark contrast to non-adaptive algorithms",
    "checked": true,
    "id": "b0b3bc804206a6b49d0b86317e23a3d0e3dd9f39",
    "semantic_title": "priorboost: an adaptive algorithm for learning from aggregate responses",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=or8BQ4ohGb": {
    "title": "InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation",
    "volume": "spotlight",
    "abstract": "Tabular data are omnipresent in various sectors of industries. Neural networks for tabular data such as TabNet have been proposed to make predictions while leveraging the attention mechanism for interpretability. However, the inferred attention masks are often dense, making it challenging to come up with rationales about the predictive signal. To remedy this, we propose InterpreTabNet, a variant of the TabNet model that models the attention mechanism as a latent variable sampled from a Gumbel-Softmax distribution. This enables us to regularize the model to learn distinct concepts in the attention masks via a KL Divergence regularizer. It prevents overlapping feature selection by promoting sparsity which maximizes the model's efficacy and improves interpretability to determine the important features when predicting the outcome. To assist in the interpretation of feature interdependencies from our model, we employ a large language model (GPT-4) and use prompt engineering to map from the learned feature mask onto natural language text describing the learned signal. Through comprehensive experiments on real-world datasets, we demonstrate that InterpreTabNet outperforms previous methods for interpreting tabular data while attaining competitive accuracy",
    "checked": true,
    "id": "bd2db2233025f7281bf583476354fcaf2b5b8df4",
    "semantic_title": "interpretabnet: distilling predictive signals from tabular data by salient feature interpretation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yyYMAprcAR": {
    "title": "By Tying Embeddings You Are Assuming the Distributional Hypothesis",
    "volume": "spotlight",
    "abstract": "In this work, we analyze both theoretically and empirically the effect of tied input-output embeddings—a popular technique that reduces the model size while often improving training. Interestingly, we found that this technique is connected to Harris (1954)'s distributional hypothesis—often portrayed by the famous Firth (1957)'s quote \"a word is characterized by the company it keeps\". Specifically, our findings indicate that words (or, more broadly, symbols) with similar semantics tend to be encoded in similar input embeddings, while words that appear in similar contexts are encoded in similar output embeddings (thus explaining the semantic space arising in input and output embedding of foundational language models). As a consequence of these findings, the tying of the input and output embeddings is encouraged only when the distributional hypothesis holds for the underlying data. These results also provide insight into the embeddings of foundation language models (which are known to be semantically organized). Further, we complement the theoretical findings with several experiments supporting the claims",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aGBpiEcB8z": {
    "title": "BayOTIDE: Bayesian Online Multivariate Time Series Imputation with Functional Decomposition",
    "volume": "spotlight",
    "abstract": "In real-world scenarios such as traffic and energy management, we frequently encounter large volumes of time-series data characterized by missing values, noise, and irregular sampling patterns. While numerous imputation methods have been proposed, the majority tend to operate within a local horizon, which involves dividing long sequences into batches of fixed-length segments for model training. This local horizon often leads to the overlooking of global trends and periodic patterns. More importantly, most methods assume the observations are sampled at regular timestamps, and fail to handle complex irregular sampled time series in various applications. Additionally, most existing methods are learned in an offline manner. Thus, it is not suitable for applications with rapidly arriving streaming data. To address these challenges, we propose BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. Our method conceptualizes multivariate time series as the weighted combination of groups of low-rank temporal factors with different patterns. We employ a suite of Gaussian Processes (GPs),each with a unique kernel, as functional priors to model these factors. For computational efficiency, we further convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE), and developing a scalable algorithm for online inference. The proposed method can not only handle imputation over arbitrary timestamps, but also offer uncertainty quantification and interpretability for the downstream application. We evaluate our method on both synthetic and real-world datasets. We release the code at https://github.com/xuangu-fang/BayOTIDE",
    "checked": true,
    "id": "609dfa5b61f1be7c2e9ca52e4d170b1d7f8113cc",
    "semantic_title": "bayotide: bayesian online multivariate time series imputation with functional decomposition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDAAMmGsGw": {
    "title": "ACM-MILP: Adaptive Constraint Modification via Grouping and Selection for Hardness-Preserving MILP Instance Generation",
    "volume": "spotlight",
    "abstract": "Data plays a pivotal role in the development of both classic and learning-based methods for Mixed-Integer Linear Programming (MILP). However, the scarcity of data in real-world applications underscores the necessity for MILP instance generation methods. Currently, these methods primarily rely on iterating random single-constraint modifications, disregarding the underlying problem structure with constraint interrelations, thereby leading to compromised quality and solvability. In this paper, we propose ACM-MILP, a framework for MILP instance generation, to achieve adaptive constraint modification and constraint interrelation modeling. It employs an adaptive constraint selection mechanism based on probability estimation within the latent space to preserve instance characteristics. Meanwhile, it detects and groups strongly related constraints through community detection, enabling collective modifications that account for constraint dependencies. Experimental results show significant improvements in problem-solving hardness similarity under our framework. Additionally, in the downstream task, we showcase the efficacy of our generated instances for hyperparameter tuning. Source code is available: https://github.com/Thinklab-SJTU/ACM-MILP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3eHNvPHL9Z": {
    "title": "How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers",
    "volume": "spotlight",
    "abstract": "A main theoretical puzzle is why over-parameterized Neural Networks (NNs) generalize well when trained to zero loss (i.e., so they interpolate the data). Usually, the NN is trained with Stochastic Gradient Descent (SGD) or one of its variants. However, recent empirical work examined the generalization of a random NN that interpolates the data: the NN was sampled from a seemingly uniform prior over the parameters, conditioned on that the NN perfectly classifying the training set. Interestingly, such a NN sample typically generalized as well as SGD-trained NNs. We prove that such a random NN interpolator typically generalizes well if there exists an underlying narrow ``teacher NN\" that agrees with the labels. Specifically, we show that such a `flat' prior over the NN parametrization induces a rich prior over the NN functions, due to the redundancy in the NN structure. In particular, this creates a bias towards simpler functions, which require less relevant parameters to represent --- enabling learning with a sample complexity approximately proportional to the complexity of the teacher (roughly, the number of non-redundant parameters), rather than the student's",
    "checked": true,
    "id": "2290956f27d5e90245db99268ee559ffa26fcf3d",
    "semantic_title": "how uniform random weights induce non-uniform bias: typical interpolating neural networks generalize with narrow teachers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZeF75iQcAc": {
    "title": "Optimal Acceleration for Minimax and Fixed-Point Problems is Not Unique",
    "volume": "spotlight",
    "abstract": "Recently, accelerated algorithms using the anchoring mechanism for minimax optimization and fixed-point problems have been proposed, and matching complexity lower bounds establish their optimality. In this work, we present the surprising observation that the optimal acceleration mechanism in minimax optimization and fixed-point problems is not unique. Our new algorithms achieve exactly the same worst-case convergence rates as existing anchor-based methods while using materially different acceleration mechanisms. Specifically, these new algorithms are dual to the prior anchor-based accelerated methods in the sense of H-duality. This finding opens a new avenue of research on accelerated algorithms since we now have a family of methods that empirically exhibit varied characteristics while having the same optimal worst-case guarantee",
    "checked": true,
    "id": "62869f374500381f8e2a48d5745e7a49f81ff9d1",
    "semantic_title": "optimal acceleration for minimax and fixed-point problems is not unique",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y5AmNYiyCQ": {
    "title": "Nash Learning from Human Feedback",
    "volume": "spotlight",
    "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Traditionally, RLHF involves the initial step of learning a reward model from pairwise human feedback, i.e., expressed as preferences between pairs of text generations. Subsequently, the LLM's policy is fine-tuned to maximize the reward through a reinforcement learning algorithm. In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a pairwise preference model, which is conditioned on two inputs (instead of a single input in the case of a reward model) given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF). In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. We illustrate the effectiveness of our approach by presenting experimental results on a text summarization task. We believe NLHF offers a compelling avenue for fine-tuning LLMs and enhancing the alignment of LLMs with human preferences",
    "checked": true,
    "id": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
    "semantic_title": "nash learning from human feedback",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=4HCi7JGCZk": {
    "title": "Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection",
    "volume": "spotlight",
    "abstract": "This paper explores the size-invariance of evaluation metrics in Salient Object Detection (SOD), especially when multiple targets of diverse sizes co-exist in the same image. We observe that current metrics are size-sensitive, where larger objects are focused, and smaller ones tend to be ignored. We argue that the evaluation should be size-invariant because bias based on size is unjustified without additional semantic information. In pursuit of this, we propose a generic approach that evaluates each salient object separately and then combines the results, effectively alleviating the imbalance. We further develop an optimization framework tailored to this goal, achieving considerable improvements in detecting objects of different sizes. Theoretically, we provide evidence supporting the validity of our new metrics and present the generalization analysis of SOD. Extensive experiments demonstrate the effectiveness of our method",
    "checked": true,
    "id": "797b55057fb9e8b4c0f5bc912c0e808459abb7ad",
    "semantic_title": "size-invariance matters: rethinking metrics and losses for imbalanced multi-object salient object detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxFvAs7pq": {
    "title": "Closing the Gap: Achieving Global Convergence (Last Iterate) of Actor-Critic under Markovian Sampling with Neural Network Parametrization",
    "volume": "spotlight",
    "abstract": "The current state-of-the-art theoretical analysis of Actor-Critic (AC) algorithms significantly lags in addressing the practical aspects of AC implementations. This crucial gap needs bridging to bring the analysis in line with practical implementations of AC. To address this, we advocate for considering the MMCLG criteria: **M**ulti-layer neural network parametrization for actor/critic, **M**arkovian sampling, **C**ontinuous state-action spaces, the performance of the **L**ast iterate, and **G**lobal optimality. These aspects are practically significant and have been largely overlooked in existing theoretical analyses of AC algorithms. In this work, we address these gaps by providing the first comprehensive theoretical analysis of AC algorithms that encompasses all five crucial practical aspects (covers MMCLG criteria). We establish global convergence sample complexity bounds of $\\tilde{\\mathcal{O}}\\left( \\epsilon^{-3} \\right)$. We achieve this result through our novel use of the weak gradient domination property of MDP's and our unique analysis of the error in critic estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PnyYgWMMwj": {
    "title": "Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions",
    "volume": "spotlight",
    "abstract": "In recent years, deep learning-based sequence modelings, such as language models, have received much attention and success, which pushes researchers to explore the possibility of transforming non-sequential problems into a sequential form. Following this thought, deep neural networks can be represented as composite functions of a sequence of mappings, linear or nonlinear, where each composition can be viewed as a word. However, the weights of linear mappings are undetermined and hence require an infinite number of words. In this article, we investigate the finite case and constructively prove the existence of a finite vocabulary $V$=$\\phi_i: \\mathbb{R}^d \\to \\mathbb{R}^d | i=1,...,n$ with $n=O(d^2)$ for the universal approximation. That is, for any continuous mapping $f: \\mathbb{R}^d \\to \\mathbb{R}^d$, compact domain $\\Omega$ and $\\varepsilon>0$, there is a sequence of mappings $\\phi_{i_1}, ..., \\phi_{i_m} \\in V, m \\in \\mathbb{Z}^+$, such that the composition $\\phi_{i_m} \\circ ... \\circ \\phi_{i_1} $ approximates $f$ on $\\Omega$ with an error less than $\\varepsilon$. Our results demonstrate an unusual approximation power of mapping compositions and motivate a novel compositional model for regular languages",
    "checked": true,
    "id": "ba0e8f425a4328c1ce410561b368b45d6ea5ceea",
    "semantic_title": "vocabulary for universal approximation: a linguistic perspective of mapping compositions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51gXk4BISH": {
    "title": "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
    "volume": "spotlight",
    "abstract": "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality regarding $d$ and $T$ (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies",
    "checked": true,
    "id": "4ac31c8490fffd83186365bc830fa6a86d49ee27",
    "semantic_title": "pricing with contextual elasticity and heteroscedastic valuation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PYDCwWvbG7": {
    "title": "QBMK: Quantum-based Matching Kernels for Un-attributed Graphs",
    "volume": "spotlight",
    "abstract": "In this work, we develop a new Quantum-based Matching Kernel (QBMK) for un-attributed graphs, by computing the kernel-based similarity between the quantum Shannon entropies of aligned vertices through the Continuous-time Quantum Walk (CTQW). The theoretical analysis reveals that the proposed QBMK kernel not only addresses the shortcoming of neglecting the structural correspondence information between graphs arising in existing R-convolution graph kernels, but also overcomes the problem of neglecting the structural differences between pairs of aligned vertices arising in existing vertex-based matching kernels. Moreover, the proposed QBMK kernel can simultaneously capture both global and local structural characteristics through the quantum Shannon entropies. Experimental evaluations on standard graph datasets demonstrate that the proposed QBMK kernel is able to outperform state-of-the-art graph kernels and graph deep learning approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4sikyurTLX": {
    "title": "Sample-specific Masks for Visual Reprogramming-based Prompting",
    "volume": "spotlight",
    "abstract": "*Visual reprogramming* (VR) is a prompting technique that aims to re-purpose a pre-trained model (e.g., a classifier on ImageNet) to target tasks (e.g., medical data prediction) by learning a *small-scale pattern* added into input images instead of tuning considerable parameters within the model. The location of the pattern within input samples is usually determined by a pre-defined mask *shared across all samples*. In this paper, we show that the shared mask potentially limits VR's generalization and increases its approximation error due to the lack of sample-level adaptation. Motivated by this finding, we design a new framework for VR called *sample-specific multi-channel masks* (SMM). Specifically, SMM employs a lightweight ConvNet and patch-wise interpolation to generate sample-specific three-channel masks instead of a shared and pre-defined mask. Since we generate different masks for individual samples, SMM is theoretically shown to reduce approximation error for the target tasks compared with existing state-of-the-art VR methods. We also empirically demonstrate its performance gain on both ResNet and ViT. The success of SMM further highlights the broader applicability of VR in leveraging the latent knowledge of pre-trained models for various target tasks. Our code is available at https://github.com/tmlr-group/SMM",
    "checked": true,
    "id": "77ba3fc35d8272090922f0e7fd4f51c186350d51",
    "semantic_title": "sample-specific masks for visual reprogramming-based prompting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l7shXGuGBT": {
    "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation",
    "volume": "spotlight",
    "abstract": "Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms existing methods under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values. See our project page at https://shuotang123.github.io/MATRIX",
    "checked": true,
    "id": "e9640cd4bdb0fa93a94151ec00259909b5e88d6d",
    "semantic_title": "self-alignment of large language models via monopolylogue-based social scene simulation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=blGpu9aGs6": {
    "title": "Learning Decision Trees and Forests with Algorithmic Recourse",
    "volume": "spotlight",
    "abstract": "This paper proposes a new algorithm for learning accurate tree-based models while ensuring the existence of recourse actions. Algorithmic Recourse (AR) aims to provide a recourse action for altering the undesired prediction result given by a model. Typical AR methods provide a reasonable action by solving an optimization task of minimizing the required effort among executable actions. In practice, however, such actions do not always exist for models optimized only for predictive performance. To alleviate this issue, we formulate the task of learning an accurate classification tree under the constraint of ensuring the existence of reasonable actions for as many instances as possible. Then, we propose an efficient top-down greedy algorithm by leveraging the adversarial training techniques. We also show that our proposed algorithm can be applied to the random forest, which is known as a popular framework for learning tree ensembles. Experimental results demonstrated that our method successfully provided reasonable actions to more instances than the baselines without significantly degrading accuracy and computational efficiency",
    "checked": true,
    "id": "806d87528300c74a0efc6b8d39c575489c4afc20",
    "semantic_title": "learning decision trees and forests with algorithmic recourse",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Z2xWhuT6R": {
    "title": "On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning",
    "volume": "spotlight",
    "abstract": "Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4). Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible *separations* between theoretical models of multimodal and unimodal learning. In particular, Lu (ALT '24) shows a computational separation, which is relevant to *worst-case* instances of the learning task. In this paper, we give a stronger *average-case* computational separation, where for \"typical\" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy. We then question how \"natural\" the average-case separation is. Would it be encountered in practice? To this end, we prove that under basic conditions, any given computational separation between average-case unimodal and multimodal learning tasks implies a corresponding cryptographic key agreement protocol. We suggest to interpret this as evidence that very strong *computational* advantages of multimodal learning may arise *infrequently* in practice, since they exist only for the \"pathological\" case of inherently cryptographic distributions. However, this does not apply to possible (super-polynomial) *statistical* advantages",
    "checked": true,
    "id": "5ed9f16b928b9f1f38cf3aa597a855a2bdc9fa49",
    "semantic_title": "on stronger computational separations between multimodal and unimodal machine learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jnps5YwNlU": {
    "title": "Efficient Precision and Recall Metrics for Assessing Generative Models using Hubness-aware Sampling",
    "volume": "spotlight",
    "abstract": "Despite impressive results, deep generative models require massive datasets for training, and as dataset size increases, effective evaluation metrics like precision and recall (P&R) become computationally infeasible on commodity hardware. In this paper, we address this challenge by proposing efficient P&R (eP&R) metrics that give almost identical results as the original P&R but with much lower computational costs. Specifically, we identify two redundancies in the original P&R: i) redundancy in ratio computation and ii) redundancy in manifold inside/outside identification. We find both can be effectively removed via hubness-aware sampling, which extracts representative elements from synthetic/real image samples based on their hubness values, i.e., the number of times a sample becomes a k-nearest neighbor to others in the feature space. Thanks to the insensitivity of hubness-aware sampling to exact k-nearest neighbor (k-NN) results, we further improve the efficiency of our eP&R metrics by using approximate k-NN methods. Extensive experiments show that our eP&R matches the original P&R but is far more efficient in time and space. Our code is available at: https://github.com/Byronliang8/Hubness_Precision_Recall",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gSMUjrkRRk": {
    "title": "Quasi-Monte Carlo Features for Kernel Approximation",
    "volume": "spotlight",
    "abstract": "Random features (Rahimi & Recht, 2007), based on Monte Carlo (MC) method, is one of the most popular approximation techniques to accelerate kernel methods. We show for a class of kernels, including Gaussian kernels, quasi-Monte Carlo (QMC) methods can be used in place of MC to improve the approximation error from $O_P(1/\\sqrt{M})$ to $O(1/M)$ (up to logarithmic factors), for estimating both the kernel function itself and the associated integral operator, where $M$ is the number of features being used. Furthermore, we demonstrate the advantage of QMC features in the case of kernel ridge regression, where theoretically, fewer random features suffice to guarantee the same convergence rate of the excess risk. In practice, the QMC kernel approximation approach is easily implementable and shows superior performance, as supported by the empirical evidence provided in the paper",
    "checked": false,
    "id": "a1427988ec02cb2614a0c70c2b2569ccba4752b5",
    "semantic_title": "a quasi-monte carlo data structure for smooth kernel evaluations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=71ktaA3ihI": {
    "title": "Agnostic Sample Compression Schemes for Regression",
    "volume": "spotlight",
    "abstract": "We obtain the first positive results for bounded sample compression in the agnostic regression setting with the $\\ell_p$ loss, where $p\\in [1,\\infty]$. We construct a generic approximate sample compression scheme for real-valued function classes exhibiting exponential size in the fat-shattering dimension but independent of the sample size. Notably, for linear regression, an approximate compression of size linear in the dimension is constructed. Moreover, for $\\ell_1$ and $\\ell_\\infty$ losses, we can even exhibit an efficient exact sample compression scheme of size linear in the dimension. We further show that for every other $\\ell_p$ loss, $p\\in (1,\\infty)$, there does not exist an exact agnostic compression scheme of bounded size. This refines and generalizes a negative result of David, Moran, and Yehudayoff (2016) for the $\\ell_2$ loss. We close by posing general open questions: for agnostic regression with $\\ell_1$ loss, does every function class admit an exact compression scheme of polynomial size in the pseudo-dimension? For the $\\ell_2$ loss, does every function class admit an approximate compression scheme of polynomial size in the fat-shattering dimension? These questions generalize Warmuth's classic sample compression conjecture for realizable-case classification (Warmuth, 2003)",
    "checked": false,
    "id": "f83f6717272109049bd902c454d3e6fc453f93cc",
    "semantic_title": "lossy compression via sparse regression codes: an approximate message passing approach",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hoVwecMqV5": {
    "title": "Behavior Generation with Latent Actions",
    "volume": "spotlight",
    "abstract": "Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision-making. Unlike language or image generation, decision-making requires modeling actions – continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module. Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies. Importantly, we demonstrate VQ-BeT's improved ability to capture behavior modes while accelerating inference speed 5× over Diffusion Policies. Videos can be found https://sjlee.cc/vq-bet/",
    "checked": true,
    "id": "ea6fc33de0c82add215415757946f1f31c074948",
    "semantic_title": "behavior generation with latent actions",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=8f8SI9X9ox": {
    "title": "Individual Fairness in Graph Decomposition",
    "volume": "spotlight",
    "abstract": "In this paper, we consider classic randomized low diameter decomposition procedures for planar graphs that obtain connected clusters that are cohesive in that close by pairs of nodes are assigned to the same cluster with high probability. We consider the additional aspect of *individual fairness* -- pairs of nodes at comparable distances should be separated with comparable probability. We show that classic decomposition procedures do not satisfy this property. We present novel algorithms that achieve various trade-offs between this property and additional desiderata of connectivity of the clusters and optimality in number of clusters. We show that our individual fairness bounds may be difficult to improve by tying the improvement to resolving a major open question in metric embeddings. We finally show the efficacy of our algorithms on real planar networks modeling Congressional redistricting",
    "checked": true,
    "id": "61af1c4e5b1e32adc7aa60949eaa490e04815674",
    "semantic_title": "individual fairness in graph decomposition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oFDFGd9Age": {
    "title": "Position: Amazing Things Come From Having Many Good Models",
    "volume": "spotlight",
    "abstract": "The *Rashomon Effect*, coined by Leo Breiman, describes the phenomenon that there exist many equally good predictive models for the same dataset. This phenomenon happens for many real datasets and when it does, it sparks both magic and consternation, but mostly magic. In light of the Rashomon Effect, this perspective piece proposes reshaping the way we think about machine learning, particularly for tabular data problems in the nondeterministic (noisy) setting. We address how the Rashomon Effect impacts (1) the existence of simple-yet-accurate models, (2) flexibility to address user preferences, such as fairness and monotonicity, without losing performance, (3) uncertainty in predictions, fairness, and explanations, (4) reliable variable importance, (5) algorithm choice, specifically, providing advanced knowledge of which algorithms might be suitable for a given problem, and (6) public policy. We also discuss a theory of when the Rashomon Effect occurs and why. Our goal is to illustrate how the Rashomon Effect can have a massive impact on the use of machine learning for complex problems in society",
    "checked": false,
    "id": "70a40edd55ba22a0dfb1e0c385ee9ff8afff3a01",
    "semantic_title": "machines are not moral role models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=jQA5iutPzd": {
    "title": "The Perception-Robustness Tradeoff in Deterministic Image Restoration",
    "volume": "spotlight",
    "abstract": "We study the behavior of deterministic methods for solving inverse problems in imaging. These methods are commonly designed to achieve two goals: (1) attaining high perceptual quality, and (2) generating reconstructions that are consistent with the measurements. We provide a rigorous proof that the better a predictor satisfies these two requirements, the larger its Lipschitz constant must be, regardless of the nature of the degradation involved. In particular, to approach perfect perceptual quality and perfect consistency, the Lipschitz constant of the model must grow to infinity. This implies that such methods are necessarily more susceptible to adversarial attacks. We demonstrate our theory on single image super-resolution algorithms, addressing both noisy and noiseless settings. We also show how this undesired behavior can be leveraged to explore the posterior distribution, thereby allowing the deterministic model to imitate stochastic methods",
    "checked": true,
    "id": "6c603dd8f2639a6c8caefea17dc8f529ebcf2bf9",
    "semantic_title": "the perception-robustness tradeoff in deterministic image restoration",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=10hu2D3hAg": {
    "title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Models",
    "volume": "spotlight",
    "abstract": "With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. $\\textbf{P}$arameter-$\\textbf{E}$fficient $\\textbf{F}$ine-$\\textbf{T}$uning(PEFT) methods have been proposed for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named $\\textbf{S}$parse $\\textbf{I}$ncrement $\\textbf{F}$ine-$\\textbf{T}$uning(SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/",
    "checked": false,
    "id": "58be35c5d5aa4416738da26d76b3db2acaf9e7a3",
    "semantic_title": "sparse is enough in fine-tuning pre-trained large language model",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=EaJ7nqJ2Fa": {
    "title": "Position: The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning",
    "volume": "spotlight",
    "abstract": "No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models",
    "checked": false,
    "id": "0bde1c92e1786f95a1de5ddf77a34e68ec9b9414",
    "semantic_title": "the no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=eY98MVffrD": {
    "title": "Learning-Rate-Free Stochastic Optimization over Riemannian Manifolds",
    "volume": "spotlight",
    "abstract": "In recent years, interest in gradient-based optimization over Riemannian manifolds has surged. However, a significant challenge lies in the reliance on hyperparameters, especially the learning rate, which requires meticulous tuning by practitioners to ensure convergence at a suitable rate. In this work, we introduce innovative learning-rate-free algorithms for stochastic optimization over Riemannian manifolds, eliminating the need for hand-tuning and providing a more robust and user-friendly approach. We establish high probability convergence guarantees that are optimal, up to logarithmic factors, compared to the best-known optimally tuned rate in the deterministic setting. Our approach is validated through numerical experiments, demonstrating competitive performance against learning-rate-dependent algorithms",
    "checked": true,
    "id": "ce823da4c040675af4ab27910b9c1b7ccd0a5b58",
    "semantic_title": "learning-rate-free stochastic optimization over riemannian manifolds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5XQzNkAOe": {
    "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
    "volume": "spotlight",
    "abstract": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks—even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents",
    "checked": true,
    "id": "11155af5ccd1889277f4269f6bb349a7633554f4",
    "semantic_title": "travelplanner: a benchmark for real-world planning with language agents",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=PY3bKuorBI": {
    "title": "Generalization in Kernel Regression Under Realistic Assumptions",
    "volume": "spotlight",
    "abstract": "It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. When applied to common kernels, our results imply benign overfitting in high input dimensions, nearly tempered overfitting in fixed dimensions, and explicit convergence rates for regularized regression. As a by-product, we obtain time-dependent bounds for neural networks trained in the kernel regime. Our results rely on new relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the kernel implicitly leads to good generalization",
    "checked": true,
    "id": "ee6b02a348c920ec9ff09b081614cd24ca45feb4",
    "semantic_title": "generalization in kernel regression under realistic assumptions",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=6axTFAlzRV": {
    "title": "Locally Estimated Global Perturbations are Better than Local Perturbations for Federated Sharpness-aware Minimization",
    "volume": "spotlight",
    "abstract": "In federated learning (FL), the multi-step update and data heterogeneity among clients often lead to a loss landscape with sharper minima, degenerating the performance of the resulted global model. Prevalent federated approaches incorporate sharpness-aware minimization (SAM) into local training to mitigate this problem. However, the local loss landscapes may not accurately reflect the flatness of global loss landscape in heterogeneous environments; as a result, minimizing local sharpness and calculating perturbations on client data might not align the efficacy of SAM in FL with centralized training. To overcome this challenge, we propose FedLESAM, a novel algorithm that locally estimates the direction of global perturbation on client side as the difference between global models received in the previous active and current rounds. Besides the improved quality, FedLESAM also speed up federated SAM-based approaches since it only performs once backpropagation in each iteration. Theoretically, we prove a slightly tighter bound than its original FedSAM by ensuring consistent perturbation. Empirically, we conduct comprehensive experiments on four federated benchmark datasets under three partition strategies to demonstrate the superior performance and efficiency of FedLESAM",
    "checked": true,
    "id": "627fcbbc030c7dd7c89457e436741ec79c5025f7",
    "semantic_title": "locally estimated global perturbations are better than local perturbations for federated sharpness-aware minimization",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=rqyXubsBhH": {
    "title": "Designing Decision Support Systems using Counterfactual Prediction Sets",
    "volume": "spotlight",
    "abstract": "Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not require, nor assumes, an expert model. Our methodology leverages the nested structure of the prediction sets provided by any conformal predictor and a natural counterfactual monotonicity assumption to achieve an exponential improvement in regret in comparison to vanilla bandit algorithms. We conduct a large-scale human subject study ($n = 2{,}751$) to compare our methodology to several competitive baselines. The results show that, for decision support systems based on prediction sets, limiting experts' level of agency leads to greater performance than allowing experts to always exercise their own agency",
    "checked": true,
    "id": "0ed1201715dd77251421f199b8fd0b064b8ba724",
    "semantic_title": "designing decision support systems using counterfactual prediction sets",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=jZVen2JguY": {
    "title": "FiT: Flexible Vision Transformer for Diffusion Model",
    "volume": "spotlight",
    "abstract": "In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions. Repository available at https://github.com/whlzy/FiT",
    "checked": true,
    "id": "5351614e845a70c7df0582fc306336b56dd51f25",
    "semantic_title": "fit: flexible vision transformer for diffusion model",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=yb5xV8LFDq": {
    "title": "Refined Coreset Selection: Towards Minimal Coreset Size under Model Performance Constraints",
    "volume": "spotlight",
    "abstract": "Coreset selection is powerful in reducing computational costs and accelerating data processing for deep learning algorithms. It strives to identify a small subset from large-scale data, so that training only on the subset practically performs on par with full data. Practitioners regularly desire to identify the smallest possible coreset in realistic scenes while maintaining comparable model performance, to minimize costs and maximize acceleration. Motivated by this desideratum, for the first time, we pose the problem of refined coreset selection, in which the minimal coreset size under model performance constraints is explored. Moreover, to address this problem, we propose an innovative method, which maintains optimization priority order over the model performance and coreset size, and efficiently optimizes them in the coreset selection procedure. Theoretically, we provide the convergence guarantee of the proposed method. Empirically, extensive experiments confirm its superiority compared with previous strategies, often yielding better model performance with smaller coreset sizes. The implementation is available at https://github.com/xiaoboxia/LBCS",
    "checked": true,
    "id": "4f9210230095fb3467ea06855dcecf28eb54c3ef",
    "semantic_title": "refined coreset selection: towards minimal coreset size under model performance constraints",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=WzD4a5ufN8": {
    "title": "Finite Volume Features, Global Geometry Representations, and Residual Training for Deep Learning-based CFD Simulation",
    "volume": "spotlight",
    "abstract": "Computational fluid dynamics (CFD) simulation is an irreplaceable modelling step in many engineering designs, but it is often computationally expensive. Some graph neural network (GNN)-based CFD methods have been proposed. However, the current methods inherit the weakness of traditional numerical simulators, as well as ignore the cell characteristics in the mesh used in the finite volume method, a common method in practical CFD applications. Specifically, the input nodes in these GNN methods have very limited information about any object immersed in the simulation domain and its surrounding environment. Also, the cell characteristics of the mesh such as cell volume, face surface area, and face centroid are not included in the message-passing operations in the GNN methods. To address these weaknesses, this work proposes two novel geometric representations: Shortest Vector (SV) and Directional Integrated Distance (DID). Extracted from the mesh, the SV and DID provide global geometry perspective to each input node, thus removing the need to collect this information through message-passing. This work also introduces the use of Finite Volume Features (FVF) in the graph convolutions as node and edge attributes, enabling its message-passing operations to adjust to different nodes. Finally, this work is the first to demonstrate how residual training, with the availability of low-resolution data, can be adopted to improve the flow field prediction accuracy. Experimental results on two datasets with five different state-of-the-art GNN methods for CFD indicate that SV, DID, FVF and residual training can effectively reduce the predictive error of current GNN-based methods by as much as 41%. Our codes and datasets are available at https://github.com/toggled/FvFGeo",
    "checked": true,
    "id": "647f3546e31469f36803709bf191de255aa13008",
    "semantic_title": "finite volume features, global geometry representations, and residual training for deep learning-based cfd simulation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jEWpcEyuUl": {
    "title": "Tight Partial Identification of Causal Effects with Marginal Distribution of Unmeasured Confounders",
    "volume": "spotlight",
    "abstract": "Partial identification (PI) presents a significant challenge in causal inference due to the incomplete measurement of confounders. Given that obtaining auxiliary variables of confounders is not always feasible and relies on untestable assumptions, researchers are encouraged to explore the internal information of latent confounders without external assistance. However, these prevailing PI results often lack precise mathematical measurement from observational data or assume that the information pertaining to confounders falls within extreme scenarios. In our paper, we reassess the significance of the marginal confounder distribution in PI. We refrain from imposing additional restrictions on the marginal confounder distribution, such as entropy or mutual information. Instead, we establish the closed-form tight PI for any possible P(U) in the discrete case. Furthermore, we establish the if and only if criterion for discerning whether the marginal confounder information leads to non-vanilla PI regions. This reveals a fundamental negative result wherein the marginal confounder information minimally contributes to PI as the confounder's cardinality increases. Our theoretical findings are supported by experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JHRvP84SQ5": {
    "title": "Concentration Inequalities for General Functions of Heavy-Tailed Random Variables",
    "volume": "spotlight",
    "abstract": "Concentration inequalities play an essential role in the study of machine learning and high dimensional statistics. In this paper, we obtain unbounded analogues of the popular bounded difference inequality for functions of independent random variables with heavy-tailed distributions. The main results provide a general framework applicable to all heavy-tailed distributions with finite variance. To illustrate the strength of our results, we present applications to sub-exponential tails, sub-Weibull tails, and heavier polynomially decaying tails. Applied to some standard problems in statistical learning theory (vector valued concentration, Rademacher complexity, and algorithmic stability), we show that these inequalities allow an extension of existing results to heavy-tailed distributions up to finite variance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SUxarNgrUT": {
    "title": "Adaptive Proximal Gradient Methods Are Universal Without Approximation",
    "volume": "spotlight",
    "abstract": "We show that adaptive proximal gradient methods for convex problems are not restricted to traditional Lipschitzian assumptions. Our analysis reveals that a class of linesearch-free methods is still convergent under mere local Hölder gradient continuity, covering in particular continuously differentiable semi-algebraic functions. To mitigate the lack of local Lipschitz continuity, popular approaches revolve around $\\varepsilon$-oracles and/or linesearch procedures. In contrast, we exploit plain Hölder inequalities not entailing any approximation, all while retaining the linesearch-free nature of adaptive schemes. Furthermore, we prove full sequence convergence without prior knowledge of local Hölder constants nor of the order of Hölder continuity. Numerical experiments make comparisons with baseline methods on diverse tasks from machine learning covering both the locally and the globally Hölder setting",
    "checked": true,
    "id": "41f3917a3338395ac3d2d120ab8692efc3873169",
    "semantic_title": "adaptive proximal gradient methods are universal without approximation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=cXBv07GKvk": {
    "title": "Variational Learning is Effective for Large Deep Networks",
    "volume": "spotlight",
    "abstract": "We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve finetuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence that variational learning is effective. Code is available at https://github.com/team-approx-bayes/ivon",
    "checked": true,
    "id": "2e9badeda30a5e86d17b4f725ad53aa3a0b321b0",
    "semantic_title": "variational learning is effective for large deep networks",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=YdwwWRX20q": {
    "title": "Improving Interpretation Faithfulness for Vision Transformers",
    "volume": "spotlight",
    "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance for various vision tasks. One reason behind the success lies in their ability to provide plausible innate explanations for the behavior of neural architectures. However, ViTs suffer from issues with explanation faithfulness, as their focal points are fragile to adversarial attacks and can be easily changed with even slight perturbations on the input image. In this paper, we propose a rigorous approach to mitigate these issues by introducing Faithful ViTs (FViTs). Briefly speaking, an FViT should have the following two properties: (1) The top-$k$ indices of its self-attention vector should remain mostly unchanged under input perturbation, indicating stable explanations; (2) The prediction distribution should be robust to perturbations. To achieve this, we propose a new method called Denoised Diffusion Smoothing (DDS), which adopts randomized smoothing and diffusion-based denoising. We theoretically prove that processing ViTs directly with DDS can turn them into FViTs. We also show that Gaussian noise is nearly optimal for both $\\ell_2$ and $\\ell_\\infty$-norm cases. Finally, we demonstrate the effectiveness of our approach through comprehensive experiments and evaluations. Results show that FViTs are more robust against adversarial attacks while maintaining the explainability of attention, indicating higher faithfulness",
    "checked": true,
    "id": "baefc63fc1fa873776696ef13e5a938a8ae6a20a",
    "semantic_title": "improving interpretation faithfulness for vision transformers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s4h6nyjM9H": {
    "title": "High-Performance Temporal Reversible Spiking Neural Networks with $\\mathcal{O}(L)$ Training Memory and $\\mathcal{O}(1)$ Inference Cost",
    "volume": "spotlight",
    "abstract": "Multi-timestep simulation of brain-inspired Spiking Neural Networks (SNNs) boost memory requirements during training and increase inference energy cost. Current training methods cannot simultaneously solve both training and inference dilemmas. This work proposes a novel Temporal Reversible architecture for SNNs (T-RevSNN) to jointly address the training and inference challenges by altering the forward propagation of SNNs. We turn off the temporal dynamics of most spiking neurons and design multi-level temporal reversible interactions at temporal turn-on spiking neurons, resulting in a $\\mathcal{O}(L)$ training memory. Combined with the temporal reversible nature, we redesign the input encoding and network organization of SNNs to achieve $\\mathcal{O}(1)$ inference energy cost. Then, we finely adjust the internal units and residual connections of the basic SNN block to ensure the effectiveness of sparse temporal information interaction. T-RevSNN achieves excellent accuracy on ImageNet, while the memory efficiency, training time acceleration and inference energy efficiency can be significantly improved by $8.6 \\times$, $2.0 \\times$ and $1.6 \\times$, respectively. This work is expected to break the technical bottleneck of significantly increasing memory cost and training time for large-scale SNNs while maintaining both high performance and low inference energy cost",
    "checked": false,
    "id": "a656622ac14a33d8b77b16afd51e1502bda58475",
    "semantic_title": "high-performance temporal reversible spiking neural networks with o(l) training memory and o(1) inference cost",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=WSi4IiMaCx": {
    "title": "A Subquadratic Time Algorithm for Robust Sparse Mean Estimation",
    "volume": "spotlight",
    "abstract": "We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. Specifically, the algorithm observes a *corrupted* set of samples from $\\mathcal{N}(\\mu,\\mathbf{I}_d)$, where the unknown mean $\\mu \\in \\mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\\mathrm{poly}(k,\\log d, 1/\\epsilon)$ and runtime $d^2 \\mathrm{poly}(k,\\log d,1/\\epsilon)$, where $\\epsilon$ is the fraction of contamination. In particular, the fastest runtime of existing algorithms is quadratic in the dimension, which can be prohibitive in high dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in _subquadratic_ time using $\\mathrm{poly}(k,\\log d,1/\\epsilon)$ samples. Our results build on algorithmic advances in detecting weak correlations, a generalized version of the light-bulb problem by Valiant (2015)",
    "checked": false,
    "id": "9aca1f0c926c9f9fd5799e73b386e311341b2157",
    "semantic_title": "a sub-quadratic time algorithm for robust sparse mean estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VWCpm39peL": {
    "title": "Discrete Latent Perspective Learning for Segmentation and Detection",
    "volume": "spotlight",
    "abstract": "In this paper, we address the challenge of Perspective-Invariant Learning in machine learning and computer vision, which involves enabling a network to understand images from varying perspectives to achieve consistent semantic interpretation. While standard approaches rely on the labor-intensive collection of multi-view images or limited data augmentation techniques, we propose a novel framework, Discrete Latent Perspective Learning (DLPL), for latent multi-perspective fusion learning using conventional single-view images. DLPL comprises three main modules: Perspective Discrete Decomposition (PDD), Perspective Homography Transformation (PHT), and Perspective Invariant Attention (PIA), which work together to discretize visual features, transform perspectives, and fuse multi-perspective semantic information, respectively. DLPL is a universal perspective learning framework applicable to a variety of scenarios and vision tasks. Extensive experiments demonstrate that DLPL significantly enhances the network's capacity to depict images across diverse scenarios (daily photos, UAV, auto-driving) and tasks (detection, segmentation)",
    "checked": true,
    "id": "d82eab566a8e74484c7379f7adcc075af1cec310",
    "semantic_title": "discrete latent perspective learning for segmentation and detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WwLtwPHmSM": {
    "title": "Best Arm Identification for Stochastic Rising Bandits",
    "volume": "spotlight",
    "abstract": "Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected reward of the available options increases every time they are selected. This setting captures a wide range of scenarios in which the available options are learning entities whose performance improves (in expectation) over time (e.g., online best model selection). While previous works addressed the regret minimization problem, this paper focuses on the fixed-budget Best Arm Identification (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process and on the simple regret. Furthermore, we derive a lower bound on the error probability, matched by our R-SR (up to constants), and illustrate how the need for a sufficiently large budget is unavoidable in the SRB setting. Finally, we numerically validate the proposed algorithms in both synthetic and realistic environments",
    "checked": true,
    "id": "5811ac762a0849ca47171b49d085cf1f7b83c61e",
    "semantic_title": "best arm identification for stochastic rising bandits",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=yTz0u4B8ug": {
    "title": "Memoria: Resolving Fateful Forgetting Problem through Human-Inspired Memory Architecture",
    "volume": "spotlight",
    "abstract": "Making neural networks remember over the long term has been a longstanding issue. Although several external memory techniques have been introduced, most focus on retaining recent information in the short term. Regardless of its importance, information tends to be fatefully forgotten over time. We present Memoria, a memory system for artificial neural networks, drawing inspiration from humans and applying various neuroscientific and psychological theories. The experimental results prove the effectiveness of Memoria in the diverse tasks of sorting, language modeling, and classification, surpassing conventional techniques. Engram analysis reveals that Memoria exhibits the primacy, recency, and temporal contiguity effects which are characteristics of human memory",
    "checked": true,
    "id": "91f1ecd011f6a335b7f49972fbb1f058cf5a9c25",
    "semantic_title": "memoria: resolving fateful forgetting problem through human-inspired memory architecture",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MRYS3Zb4iV": {
    "title": "Integrating Global Context Contrast and Local Sensitivity for Blind Image Quality Assessment",
    "volume": "spotlight",
    "abstract": "Blind Image Quality Assessment (BIQA) mirrors subjective made by human observers. Generally, humans favor comparing relative qualities over predicting absolute qualities directly. However, current BIQA models focus on mining the \"local\" context, i.e., the relationship between information among individual images and the absolute quality of the image, ignoring the \"global\" context of the relative quality contrast among different images in the training data. In this paper, we present the Perceptual Context and Sensitivity BIQA (CSIQA), a novel contrastive learning paradigm that seamlessly integrates \"global'' and \"local'' perspectives into the BIQA. Specifically, the CSIQA comprises two primary components: 1) A Quality Context Contrastive Learning module, which is equipped with different contrastive learning strategies to effectively capture potential quality correlations in the global context of the dataset. 2) A Quality-aware Mask Attention Module, which employs the random mask to ensure the consistency with visual local sensitivity, thereby improving the model's perception of local distortions. Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jKUWlgra9b": {
    "title": "ERQ: Error Reduction for Post-Training Quantization of Vision Transformers",
    "volume": "spotlight",
    "abstract": "Post-training quantization (PTQ) for vision transformers (ViTs) has garnered significant attention due to its efficiency in compressing models. However, existing methods typically overlook the intricate interdependence between quantized weight and activation, leading to considerable quantization error. In this paper, we propose ERQ, a two-step PTQ approach meticulously crafted to sequentially reduce the quantization error arising from activation and weight quantization. ERQ first introduces Activation quantization error reduction (Aqer) that strategically formulates the minimization of activation quantization error as a Ridge Regression problem, tackling it by updating weights with full-precision. Subsequently, ERQ introduces Weight quantization error reduction (Wqer) that adopts an iterative approach to mitigate the quantization error induced by weight quantization. In each iteration, an empirically derived, efficient proxy is employed to refine the rounding directions of quantized weights, coupled with a Ridge Regression solver to curtail weight quantization error. Experimental results attest to the effectiveness of our approach. Notably, ERQ surpasses the state-of-the-art GPTQ by 22.36% in accuracy for W3A4 ViT-S",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O45u81aby2": {
    "title": "Towards Resource-friendly, Extensible and Stable Incomplete Multi-view Clustering",
    "volume": "spotlight",
    "abstract": "Incomplete multi-view clustering (IMVC) methods typically encounter three drawbacks: (1) intense time and/or space overheads; (2) intractable hyper-parameters; (3) non-zero variance results. With these concerns in mind, we give a simple yet effective IMVC scheme, termed as ToRES. Concretely, instead of self-expression affinity, we manage to construct prototype-sample affinity for incomplete data so as to decrease the memory requirements. To eliminate hyper-parameters, besides mining complementary features among views by view-wise prototypes, we also attempt to devise cross-view prototypes to capture consensus features for jointly forming high-quality clustering representation. To avoid the variance, we successfully unify representation learning and clustering operation, and directly optimize the discrete cluster indicators from incomplete data. Then, for the resulting objective function, we provide two equivalent solutions from perspectives of feasible region partitioning and objective transformation. Many results suggest that ToRES exhibits advantages against 20 SOTA algorithms, even in scenarios with a higher ratio of incomplete data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BxAvcnlS8O": {
    "title": "RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences",
    "volume": "spotlight",
    "abstract": "Preference-based Reinforcement Learning (PbRL) circumvents the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL methods excessively depend on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method utilizes a sample selection-based discriminator to dynamically filter out noise and ensure robust training. To counteract the cumulative error stemming from incorrect selection, we suggest a warm start for the reward model, which additionally bridges the performance gap during the transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the state-of-the-art PbRL method. Code is available at https://github.com/CJReinforce/RIME_ICML2024",
    "checked": true,
    "id": "87f1b39c320e1fc71584a231855523167a5588ff",
    "semantic_title": "rime: robust preference-based reinforcement learning with noisy preferences",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HaBVzgSdM7": {
    "title": "Towards Unified Multi-granularity Text Detection with Interactive Attention",
    "volume": "spotlight",
    "abstract": "Existing OCR engines or document image analysis systems typically rely on training separate models for text detection in varying scenarios and granularities, leading to significant computational complexity and resource demands. In this paper, we introduce \"Detect Any Text\" (DAT), an advanced paradigm that seamlessly unifies scene text detection, layout analysis, and document page detection into a cohesive, end-to-end model. This design enables DAT to efficiently manage text instances at different granularities, including *word*, *line*, *paragraph* and *page*. A pivotal innovation in DAT is the across-granularity interactive attention module, which significantly enhances the representation learning of text instances at varying granularities by correlating structural information across different text queries. As a result, it enables the model to achieve mutually beneficial detection performances across multiple text granularities. Additionally, a prompt-based segmentation module refines detection outcomes for texts of arbitrary curvature and complex layouts, thereby improving DAT's accuracy and expanding its real-world applicability. Experimental results demonstrate that DAT achieves state-of-the-art performances across a variety of text-related benchmarks, including multi-oriented/arbitrarily-shaped scene text detection, document layout analysis and page detection tasks",
    "checked": true,
    "id": "4b7548ad70de33ce3fe75981b04136be631bc961",
    "semantic_title": "towards unified multi-granularity text detection with interactive attention",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MZkqjV4FRT": {
    "title": "An Efficient Maximal Ancestral Graph Listing Algorithm",
    "volume": "spotlight",
    "abstract": "Maximal ancestral graph (MAG) is a prevalent graphical model to characterize causal relations in the presence of *latent variables* including latent confounders and selection variables. Given observational data, only a Markov equivalence class (MEC) of MAGs is identifiable if without some additional assumptions. Due to this fact, MAG listing, listing all the MAGs in the MEC, is usually demanded in many downstream tasks. To the best of our knowledge, there are no relevant methods for MAG listing other than brute force in the literature. In this paper, we propose the first brute-force-free MAG listing method, by determining the local structures of each vertex recursively. We provide the graphical characterization for each valid local transformation of a vertex, and present sound and complete rules to incorporate the valid local transformation in the presence of latent confounders and selection variables. Based on these components, our method can efficiently output all the MAGs in the MEC with no redundance, that is, every intermediate graph in the recursive process is necessary for the MAG listing task. The empirical analysis demonstrates the superiority of our proposed method on efficiency and effectiveness",
    "checked": false,
    "id": "1319e3f987be71ad825a4107f781403c4c739820",
    "semantic_title": "on efficient large maximal biplex discovery",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Ywl6pODXjB": {
    "title": "Transolver: A Fast Transformer Solver for PDEs on General Geometries",
    "volume": "spotlight",
    "abstract": "Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs). However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points. Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries. Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice. By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-general modeling capacity and can be efficiently computed in linear complexity. Transolver achieves consistent state-of-the-art with 22% relative gain across six standard benchmarks and also excels in large-scale industrial simulations, including car and airfoil designs. Code is available at https://github.com/thuml/Transolver",
    "checked": true,
    "id": "88f17e3a9aa3e1de7dd7dde2cb03d10d5446099f",
    "semantic_title": "transolver: a fast transformer solver for pdes on general geometries",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=2Y93PtAqCl": {
    "title": "Revisiting the Power of Prompt for Visual Tuning",
    "volume": "spotlight",
    "abstract": "Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outperforms existing methods by a remarkable margin. For instance, after MAE pre-training, our method improves accuracy by up to 10%$\\sim$30% compared to VPT, and outperforms Full fine-tuning 19 out of 24 cases while using less than 0.4% of learnable parameters. Besides, the experimental results demonstrate the proposed SPT is robust to prompt lengths and scales well with model capacity and training data size. We finally provide an insightful exploration into the amount of target data facilitating the adaptation of pre-trained models to downstream tasks. The code is available at https://github.com/WangYZ1608/Self-Prompt-Tuning",
    "checked": true,
    "id": "f53c056a38d3afeb7aedf9ca354bd1e4435969aa",
    "semantic_title": "revisiting the power of prompt for visual tuning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=XxCfToC9pJ": {
    "title": "Realistic Unsupervised CLIP Fine-tuning with Universal Entropy Optimization",
    "volume": "spotlight",
    "abstract": "The emergence of vision-language models, such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. This paper explores a realistic unsupervised fine-tuning scenario, considering the presence of out-of-distribution samples from unknown classes within the unlabeled data. In particular, we focus on simultaneously enhancing out-of-distribution detection and the recognition of instances associated with known classes. To tackle this problem, we present a simple, efficient, and effective approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompt, UEO incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Extensive experiments across 15 domains and 4 different types of prior knowledge validate the effectiveness of UEO compared to baseline methods. The code is at https://github.com/tim-learn/UEO",
    "checked": false,
    "id": "50e53f31eaf3039ad535afb0ed54e6954cf5ba3e",
    "semantic_title": "towards realistic unsupervised fine-tuning with clip",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ofzEysK2D": {
    "title": "Position: Levels of AGI for Operationalizing Progress on the Path to AGI",
    "volume": "spotlight",
    "abstract": "We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy, providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. With these principles in mind, we propose \"Levels of AGI\" based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we discuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eW0pZmziBH": {
    "title": "Novel Spectral Algorithms for the Partial Credit Model",
    "volume": "spotlight",
    "abstract": "The Partial Credit Model (PCM) of Andrich (1978) and Masters (1982) is a fundamental model within the psychometric literature with wide-ranging modern applications. It models the integer-valued response that a subject gives to an item where there is a natural notion of monotonic progress between consecutive response values, such as partial scores on a test and customer ratings of a product. In this paper, we introduce a novel, time-efficient and accurate statistical spectral algorithm for inference under the PCM model. We complement our algorithmic contribution with in-depth non-asymptotic statistical analysis, the first of its kind in the literature. We show that the spectral algorithm enjoys the optimal error guarantee under three different metrics, all under reasonable sampling assumptions. We leverage the efficiency of the spectral algorithm to propose a novel EM-based algorithm for learning mixtures of PCMs. We perform comprehensive experiments on synthetic and real-life datasets covering education testing, recommendation systems, and financial investment applications. We show that the proposed spectral algorithm is competitive with previously introduced algorithms in terms of accuracy while being orders of magnitude faster",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yFUdZfbEme": {
    "title": "Domain Generalisation via Imprecise Learning",
    "volume": "spotlight",
    "abstract": "Out-of-distribution (OOD) generalisation is challenging because it involves not only learning from empirical data, but also deciding among various notions of generalisation, e.g. optimise based on the average-case risk, worst-case risk, or interpolations thereof. While this decision should in principle be decided by the model operator like medical doctors in practice, this information might not always be available at training time. This situation leads to arbitrary commitments to specific generalisation strategies by machine learners due to these deployment uncertainties. We introduce the Imprecise Domain Generalisation framework to mitigate this, featuring an imprecise risk optimisation that allows learners to stay imprecise by optimising against a continuous spectrum of generalisation strategies during training, and a model framework that allows operators to specify their generalisation preference at deployment. Our work, supported by theoretical and empirical evidence, showcases the benefits of integrating imprecision into domain generalisation",
    "checked": true,
    "id": "a3e43297c4c78ad71dae33b1cd29a1451807c9de",
    "semantic_title": "domain generalisation via imprecise learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U97MIrs35l": {
    "title": "Auto-Encoding Morph-Tokens for Multimodal LLM",
    "volume": "spotlight",
    "abstract": "For multimodal LLMs, the synergy of visual comprehension (textual output) and generation (visual output) presents an ongoing challenge. This is due to a conflicting objective: for comprehension, an MLLM needs to abstract the visuals; for generation, it needs to preserve the visuals as much as possible. Thus, the objective is a dilemma for visual-tokens. To resolve the conflict, we propose encoding images into morph-tokens to serve a dual purpose: for comprehension, they act as visual prompts instructing MLLM to generate texts; for generation, they take on a different, non-conflicting role as complete visual-tokens for image reconstruction, where the missing visual cues are recovered by the MLLM. Extensive experiments show that morph-tokens can achieve a new SOTA for multimodal comprehension and generation simultaneously. Our project is available at https://github.com/DCDmllm/MorphTokens",
    "checked": true,
    "id": "22ffbc1864b2ed2d848c54619416a741b144e067",
    "semantic_title": "auto-encoding morph-tokens for multimodal llm",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Fw4fBE2rqW": {
    "title": "On Discrete Prompt Optimization for Diffusion Models",
    "volume": "poster",
    "abstract": "This paper introduces the first gradient-based framework for prompt optimization in text-to-image diffusion models. We formulate prompt engineering as a discrete optimization problem over the language space. Two major challenges arise in efficiently finding a solution to this problem: (1) Enormous Domain Space: Setting the domain to the entire language space poses significant difficulty to the optimization process. (2) Text Gradient: Efficiently computing the text gradient is challenging, as it requires backpropagating through the inference steps of the diffusion model and a non-differentiable embedding lookup table. Beyond the problem formulation, our main technical contributions lie in solving the above challenges. First, we design a family of dynamically generated compact subspaces comprised of only the most relevant words to user input, substantially restricting the domain space. Second, we introduce ``Shortcut Text Gradient\" --- an effective replacement for the text gradient that can be obtained with constant memory and runtime. Empirical evaluation on prompts collected from diverse sources (DiffusionDB, ChatGPT, COCO) suggests that our method can discover prompts that substantially improve (prompt enhancement) or destroy (adversarial attack) the faithfulness of images generated by the text-to-image diffusion model",
    "checked": false,
    "id": "4beb71fad1da7537f0b34638557ccf327baf282b",
    "semantic_title": "prompting hard or hardly prompting: prompt inversion for text-to-image diffusion models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=uEx2bSAJu8": {
    "title": "Multi-View Clustering by Inter-cluster Connectivity Guided Reward",
    "volume": "poster",
    "abstract": "Multi-view clustering has been widely explored for its effectiveness in harmonizing heterogeneity along with consistency in different views of data. Despite the significant progress made by recent works, the performance of most existing methods is heavily reliant on strong priori information regarding the true cluster number $\\textit{K}$, which is rarely feasible in real-world scenarios. In this paper, we propose a novel graph-based multi-view clustering algorithm to infer unknown $\\textit{K}$ through a graph consistency reward mechanism. To be specific, we evaluate the cluster indicator matrix during each iteration with respect to diverse $\\textit{K}$. We formulate the inference process of unknown $\\textit{K}$ as a parsimonious reinforcement learning paradigm, where the reward is measured by inter-cluster connectivity. As a result, our approach is capable of independently producing the final clustering result, free from the input of a predefined cluster number. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach in comparison to existing state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5vZzmCeTYu": {
    "title": "Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning",
    "volume": "poster",
    "abstract": "Recent advancements in off-policy Reinforcement Learning (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 simulation benchmarks, measuring training metrics related to overestimation, overfitting, and plasticity loss — issues that motivate the examined regularization techniques. Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior performance. Notably, a simple Soft Actor-Critic agent, appropriately regularized, reliably finds a better-performing policy within the training regime, which previously was achieved mainly through model-based approaches",
    "checked": true,
    "id": "dcf39ce4ea7c1d073dcd0dbb131a0b90a4bd2977",
    "semantic_title": "overestimation, overfitting, and plasticity in actor-critic: the bitter lesson of reinforcement learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=LDq1JPdc55": {
    "title": "Copyright Traps for Large Language Models",
    "volume": "poster",
    "abstract": "Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being actively debated. Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize significantly, we hypothesize - and later confirm - that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. We carefully design a randomized controlled experimental setup, inserting traps into original content (books) and train a 1.3B LLM from scratch. We first validate that the use of content in our target model would be undetectable using existing methods. We then show, contrary to intuition, that even medium-length trap sentences repeated a significant number of times (100) are not detectable using existing methods. However, we show that longer sequences repeated a large number of times can be reliably detected (AUC=0.75) and used as copyright traps. Beyond copyright applications, our findings contribute to the study of LLM memorization: the randomized controlled setup enables us to draw causal relationships between memorization and certain sequence properties such as repetition in model training data and perplexity",
    "checked": true,
    "id": "55a9e7e09d3ff5df147cf4ed85f0387a4d5da149",
    "semantic_title": "copyright traps for large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Fzp1DRzCIN": {
    "title": "Implicit meta-learning may lead language models to trust more reliable sources",
    "volume": "poster",
    "abstract": "We demonstrate that large language models (LLMs) may learn indicators of document usefulness and modulate their updates accordingly. We introduce random strings (\"tags\") as indicators of usefulness in a synthetic fine-tuning dataset. Fine-tuning on this dataset leads to **implicit meta-learning (IML)**: in further fine-tuning, the model updates to make more use of text that is tagged as useful. We perform a thorough empirical investigation of this phenomenon, finding (among other things) that (i) it occurs in both pretrained LLMs and those trained from scratch, as well as on a vision task, and (ii) larger models and smaller batch sizes tend to give more IML. We also use probing to examine how IML changes the way models store knowledge in their parameters. Finally, we reflect on what our results might imply about the capabilities, risks, and controllability of future AI systems",
    "checked": true,
    "id": "c5281e0e6e3672bb772e14e4645381a580c53245",
    "semantic_title": "implicit meta-learning may lead language models to trust more reliable sources",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HsseRq2FAx": {
    "title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
    "volume": "poster",
    "abstract": "Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question *whether and how an agent can ``*dream better*''* in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called **Dr. Strategy**, which is equipped with a novel **Dr**eaming **Strategy**. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way. In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks",
    "checked": true,
    "id": "b704edfc34b2ee0f8f80a73af01905d9554e7934",
    "semantic_title": "dr. strategy: model-based generalist agents with strategic dreaming",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60vC1FY0dZ": {
    "title": "When Will Gradient Regularization Be Harmful?",
    "volume": "poster",
    "abstract": "Gradient regularization (GR), which aims to penalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. However, can we trust this powerful technique? This paper reveals that GR can cause performance degeneration in adaptive optimization scenarios, particularly with learning rate warmup. Our empirical and theoretical analyses suggest this is due to GR inducing instability and divergence in gradient statistics of adaptive optimizers at the initial training stage. Inspired by the warmup heuristic, we propose three GR warmup strategies, each relaxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. With experiments on Vision Transformer family, we confirm the three GR warmup strategies can effectively circumvent these issues, thereby largely improving the model performance. Meanwhile, we note that scalable models tend to rely more on the GR warmup, where the performance can be improved by up to 3% on Cifar10 compared to baseline GR. Code is available at https://github.com/zhaoyang-0204/gnp",
    "checked": true,
    "id": "1bd68aeb3cb904fed88b673d204ae35ea6347dfa",
    "semantic_title": "when will gradient regularization be harmful?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pOJbk4Nzmi": {
    "title": "Efficient Algorithms for Empirical Group Distributionally Robust Optimization and Beyond",
    "volume": "poster",
    "abstract": "In this paper, we investigate the empirical counterpart of Group Distributionally Robust Optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups. We formulate empirical GDRO as a *two-level* finite-sum convex-concave minimax optimization problem and develop an algorithm called ALEG to benefit from its special structure. ALEG is a double-looped stochastic primal-dual algorithm that incorporates variance reduction techniques into a modified mirror prox routine. To exploit the two-level finite-sum structure, we propose a simple group sampling strategy to construct the stochastic gradient with a smaller Lipschitz constant and then perform variance reduction for all groups. Theoretical analysis shows that ALEG achieves $\\varepsilon$-accuracy within a computation complexity of $\\mathcal{O}\\left(\\frac{m\\sqrt{\\bar{n}\\ln{m}}}{\\varepsilon}\\right)$, where $\\bar n$ is the average number of samples among $m$ groups. Notably, our approach outperforms the state-of-the-art method by a factor of $\\sqrt{m}$. Based on ALEG, we further develop a two-stage optimization algorithm called ALEM to deal with the empirical Minimax Excess Risk Optimization (MERO) problem. The computation complexity of ALEM nearly matches that of ALEG, surpassing the rates of existing methods",
    "checked": false,
    "id": "eba41905be50f526cdacc9254241bd3698c0aacb",
    "semantic_title": "efficient algorithms for empirical group distributional robust optimization and beyond",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=FCmWhJQ14I": {
    "title": "Evaluation of Trajectory Distribution Predictions with Energy Score",
    "volume": "poster",
    "abstract": "Predicting the future trajectory of surrounding objects is inherently uncertain and vital in the safe and reliable planning of autonomous systems such as in self-driving cars. Although trajectory prediction models have become increasingly sophisticated in dealing with the complexities of spatiotemporal data, the evaluation methods used to assess these models have not kept pace. \"Minimum of N\" is a common family of metrics used to assess the rich outputs of such models. We critically examine the Minimum of N within the proper scoring rules framework to show that it is not strictly proper and demonstrate how that could lead to a misleading assessment of multimodal trajectory predictions. As an alternative, we propose using Energy Score-based evaluation measures, leveraging their proven propriety for a more reliable evaluation of trajectory distribution predictions",
    "checked": false,
    "id": "765f817332e764d18541bb3209a36ac06041dc95",
    "semantic_title": "biocatalysis and agricultural biotechnology",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=HpT19AKddu": {
    "title": "Causal Discovery with Fewer Conditional Independence Tests",
    "volume": "poster",
    "abstract": "Many questions in science center around the fundamental problem of understanding causal relationships. However, most constraint-based causal discovery algorithms, including the well-celebrated PC algorithm, often incur an _exponential_ number of conditional independence (CI) tests, posing limitations in various applications. Addressing this, our work focuses on characterizing what can be learned about the underlying causal graph with a reduced number of CI tests. We show that it is possible to a learn a coarser representation of the hidden causal graph with a _polynomial_ number of tests. This coarser representation, named Causal Consistent Partition Graph (CCPG), comprises of a partition of the vertices and a directed graph defined over its components. CCPG satisfies consistency of orientations and additional constraints which favor finer partitions. Furthermore, it reduces to the underlying causal graph when the causal graph is identifiable. As a consequence, our results offer the first efficient algorithm for recovering the true causal graph with a polynomial number of tests, in special cases where the causal graph is fully identifiable through observational data and potentially additional interventions",
    "checked": true,
    "id": "9ba728491b2dff8e4e544812f6794f422ebbdcdd",
    "semantic_title": "causal discovery with fewer conditional independence tests",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hZ0fWhgVch": {
    "title": "Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion",
    "volume": "poster",
    "abstract": "As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score Distillation Sampling module of Stable Diffusion is incorporated. By carefully balancing the two components during optimization, our method can produce high-quality images with near state-of-the-art performance on T2I-Compbench. The code is available at https://github.com/Pepper-lll/VLMinv",
    "checked": true,
    "id": "53332cb160c5d9d542e08523dfac43c9399b79ab",
    "semantic_title": "referee can play: an alternative approach to conditional generation via model inversion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f3TUipYU3U": {
    "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "volume": "poster",
    "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench",
    "checked": true,
    "id": "b82ccc66c14f531a444c74d2a9a9d86a86a8be99",
    "semantic_title": "harmbench: a standardized evaluation framework for automated red teaming and robust refusal",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=0tPBk24xNj": {
    "title": "Adversarial Attacks on Combinatorial Multi-Armed Bandits",
    "volume": "poster",
    "abstract": "We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, a notion to capture the vulnerability and robustness of CMAB. The attackability condition depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanning tree, cascading bandits for online ranking, and online shortest path",
    "checked": true,
    "id": "bdc8312b405eb603bc430e28f9f1dff3dfb369e2",
    "semantic_title": "adversarial attacks on combinatorial multi-armed bandits",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=LVF4P1NNwO": {
    "title": "Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers",
    "volume": "poster",
    "abstract": "In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms",
    "checked": true,
    "id": "a6d3ca7bd056a16d879fad619c2fd0874ff2536f",
    "semantic_title": "exact conversion of in-context learning to model weights in linearized-attention transformers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fM9xTkpAdu": {
    "title": "Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems",
    "volume": "poster",
    "abstract": "In-memory computing (IMC) has emerged as a promising solution to address both computation and data-movement challenges, by performing computation on data in-place directly in the memory array. IMC typically relies on analog operation, which makes analog-to-digital converters (ADCs) necessary, for converting results back to the digital domain. However, ADCs maintain computational efficiency by having limited precision, leading to substantial quantization errors in compute outputs. This work proposes RAOQ (Reshape and Adapt for Output Quantization) to overcome this issue, which comprises two classes of mechanisms including: 1) mitigating ADC quantization error by adjusting the statistics of activations and weights, through an activation-shifting approach (A-shift) and a weight reshaping technique (W-reshape); 2) adapting AI models to better tolerate ADC quantization through a bit augmentation method (BitAug), complemented by the introduction of ADC-LoRA, a low-rank approximation technique, to reduce the training overhead. RAOQ demonstrates consistently high performance across different scales and domains of neural network models for computer vision and natural language processing (NLP) tasks at various bit precisions, achieving state-of-the-art results with practical IMC implementations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gp0xZDmrA2": {
    "title": "Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel SVD and Nyström method",
    "volume": "poster",
    "abstract": "In contrast with Mercer kernel-based approaches as used e.g. in Kernel Principal Component Analysis (KPCA), it was previously shown that Singular Value Decomposition (SVD) inherently relates to asymmetric kernels and Asymmetric Kernel Singular Value Decomposition (KSVD) has been proposed. However, the existing formulation to KSVD cannot work with infinite-dimensional feature mappings, the variational objective can be unbounded, and needs further numerical evaluation and exploration towards machine learning. In this work, i) we introduce a new asymmetric learning paradigm based on coupled covariance eigenproblem (CCE) through covariance operators, allowing infinite-dimensional feature maps. The solution to CCE is ultimately obtained from the SVD of the induced asymmetric kernel matrix, providing links to KSVD. ii) Starting from the integral equations corresponding to a pair of coupled adjoint eigenfunctions, we formalize the asymmetric Nyström method through a finite sample approximation to speed up training. iii) We provide the first empirical evaluations verifying the practical utility and benefits of KSVD and compare with methods resorting to symmetrization or linear SVD across multiple tasks",
    "checked": false,
    "id": "b37f879921f85e3d03df7a7a159c9bb6d1e453bf",
    "semantic_title": "learning in feature spaces via coupled covariances: asymmetric kernel svd and nystr\\\"om method",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=OF7e0w1uon": {
    "title": "Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice via HyperAgent",
    "volume": "poster",
    "abstract": "We propose HyperAgent, a reinforcement learning (RL) algorithm based on the hypermodel framework for exploration in RL. HyperAgent allows for the efficient incremental approximation of posteriors associated with an optimal action-value function ($Q^\\star$) without the need for conjugacy and follows the greedy policies w.r.t. these approximate posterior samples. We demonstrate that HyperAgent offers robust performance in large-scale deep RL benchmarks. It can solve Deep Sea hard exploration problems with episodes that optimally scale with problem size and exhibits significant efficiency gains in the Atari suite. Implementing HyperAgent requires minimal code addition to well-established deep RL frameworks like DQN. We theoretically prove that, under tabular assumptions, HyperAgent achieves logarithmic per-step computational complexity while attaining sublinear regret, matching the best known randomized tabular RL algorithm",
    "checked": true,
    "id": "7c8ed72733835684c86420e168b2cb5515d42115",
    "semantic_title": "q-star meets scalable posterior sampling: bridging theory and practice via hyperagent",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=VUTyzH63Xa": {
    "title": "Simplicity Bias via Global Convergence of Sharpness Minimization",
    "volume": "poster",
    "abstract": "The remarkable generalization ability of neural networks is usually attributed to the implicit bias of SGD, which often yields models with lower complexity using simpler (e.g. linear) and low-rank features. Recent works have provided empirical and theoretical evidence for the bias of particular variants of SGD (such as label noise SGD) toward flatter regions of the loss landscape. Despite the folklore intuition that flat solutions are 'simple', the connection with the simplicity of the final trained model (e.g. low-rank) is not well understood. In this work, we take a step toward bridging this gap by studying the simplicity structure that arises from minimizers of the sharpness for a class of two-layer neural networks. We show that, for any high dimensional training data and certain activations, with small enough step size, label noise SGD always converges to a network that replicates a single linear feature across all neurons; thereby implying a simple rank one feature matrix. To obtain this result, our main technical contribution is to show that label noise SGD always minimizes the sharpness on the manifold of models with zero loss for two-layer networks. Along the way, we discover a novel property --- a local geodesic convexity --- of the trace of Hessian of the loss at approximate stationary points on the manifold of zero loss, which links sharpness to the geometry of the manifold. This tool may be of independent interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nU1mtFDtMX": {
    "title": "STEER: Assessing the Economic Rationality of Large Language Models",
    "volume": "poster",
    "abstract": "There is increasing interest in using LLMs as decision-making \"agents\". Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions---and more broadly, determining whether an LLM agent is reliable enough to be trusted---requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained \"elements\" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a \"rationality report card\". Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior",
    "checked": true,
    "id": "7a3d13dd79b8f3b141e8c8f0413966bb7fdaf379",
    "semantic_title": "steer: assessing the economic rationality of large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0bmXrtTDUu": {
    "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
    "volume": "poster",
    "abstract": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand ($\\sim$1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges",
    "checked": true,
    "id": "82f75d838e92196864131bad25b1abc3b5d40a6f",
    "semantic_title": "beyond chinchilla-optimal: accounting for inference in language model scaling laws",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=edHLN40DWu": {
    "title": "One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) exhibit strong generalization capabilities to novel tasks when prompted with language instructions and in-context demos. Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design. While these methods demonstrated promising results, they also restricted the searched prompt to one instruction. Such simplification significantly limits their capacity, as a single demo-free instruction might not be able to cover the entire complex problem space of the targeted task. To alleviate this issue, we adopt the Mixture-of-Expert paradigm and divide the problem space into a set of sub-regions; Each sub-region is governed by a specialized expert, equipped with both an instruction and a set of demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into experts based on their semantic similarity; (2) instruction assignment: A region-based joint search of an instruction per expert complements the demos assigned to it, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), achieves an average win rate of 81% against prior arts across several major benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUVForc3VP": {
    "title": "Double-Step Alternating Extragradient with Increasing Timescale Separation for Finding Local Minimax Points: Provable Improvements",
    "volume": "poster",
    "abstract": "In nonconvex-nonconcave minimax optimization, two-timescale gradient methods have shown their potential to find local minimax (optimal) points, provided that the timescale separation between the min and the max player is sufficiently large. However, existing two-timescale variants of gradient descent ascent and extragradient methods face two shortcomings, especially when we search for non-strict local minimax points that are prevalent in modern overparameterized setting. In specific, (1) these methods can be unstable at some non-strict local minimax points even with sufficiently large timescale separation, and even (2) computing a proper amount of timescale separation is infeasible in practice. To remedy these two issues, we propose to incorporate two simple but provably effective schemes, double-step alternating update and increasing timescale separation, into the two-timescale extragradient method, respectively. Under mild conditions, we show that the proposed methods converge to non-strict local minimax points that all existing two-timescale methods fail to converge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLTjFFiuUJ": {
    "title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration",
    "volume": "poster",
    "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established, remains limited. Inspired by recent studies that explore the presence of attention sink in the initial token, which receives disproportionately large attention scores despite their lack of semantic importance, this work delves deeper into this phenomenon. We aim to provide a more profound understanding of the existence of attention sinks within LLMs and to uncover ways to enhance the achievable accuracy of LLMs by directly optimizing the attention distributions, without the need for weight finetuning. Specifically, this work begins with comprehensive visualizations of the attention distributions in LLMs during inference across various inputs and tasks. Based on these visualizations, to the best of our knowledge, we are the first to discover that (1) attention sinks occur not only at the start of sequences but also within later tokens of the input, and (2) not all attention sinks have a positive impact on the achievable accuracy of LLMs. Building upon our findings, we propose a training-free Attention Calibration Technique (ACT) that automatically optimizes the attention distributions on the fly during inference in an input-adaptive manner. Extensive experiments validate that ACT consistently enhances the accuracy of various LLMs across different applications. Specifically, ACT achieves an average improvement of up to $7.30\\%$ in accuracy across different datasets when applied to Llama-30B",
    "checked": true,
    "id": "939cbdc260d6c2b02e72fd871ebb0f26d643ce7d",
    "semantic_title": "unveiling and harnessing hidden attention sinks: enhancing large language models without training through attention calibration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v1I4zRAjMb": {
    "title": "TENG: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets Toward Machine Precision",
    "volume": "poster",
    "abstract": "Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering. The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems. In this paper, we introduce the *Time-Evolving Natural Gradient (TENG)*, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions. Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency. TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving *machine precision* in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eaNLvrP8n1": {
    "title": "Learning Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking",
    "volume": "poster",
    "abstract": "Harnessing transformer-based models, visual tracking has made substantial strides. However, the sluggish performance of current trackers limits their practicality on devices with constrained computational capabilities, especially for real-time unmanned aerial vehicle (UAV) tracking. Addressing this challenge, we introduce AVTrack, an adaptive computation framework tailored to selectively activate transformer blocks for real-time UAV tracking in this work. Our novel Activation Module (AM) dynamically optimizes ViT architecture, selectively engaging relevant components and enhancing inference efficiency without compromising much tracking performance. Moreover, we bolster the effectiveness of ViTs, particularly in addressing challenges arising from extreme changes in viewing angles commonly encountered in UAV tracking, by learning view-invariant representations through mutual information maximization. Extensive experiments on five tracking benchmarks affirm the effectiveness and versatility of our approach, positioning it as a state-of-the-art solution in visual tracking. Code is released at: https://github.com/wuyou3474/AVTrack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wnni3cu39x": {
    "title": "Conditional Common Entropy for Instrumental Variable Testing and Partial Identification",
    "volume": "poster",
    "abstract": "Instrumental variables (IVs) are widely used for estimating causal effects. There are two main challenges when using instrumental variables. First of all, using IV without additional assumptions such as linearity, the causal effect may still not be identifiable. Second, when selecting an IV, the validity of the selected IV is typically not testable since the causal graph is not identifiable from observational data. In this paper, we propose a method for bounding the causal effect with instrumental variables under weak confounding. In addition, we present a novel criterion to falsify the IV with side information about the confounder. We demonstrate the utility of the proposed method with simulated and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z0S6fUdW68": {
    "title": "Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption",
    "volume": "poster",
    "abstract": "This study tackles the challenges of adversarial corruption in model-based reinforcement learning (RL), where the transition dynamics can be corrupted by an adversary. Existing studies on corruption-robust RL mostly focus on the setting of model-free RL, where robust least-square regression is often employed for value function estimation. However, these techniques cannot be directly applied to model-based RL. In this paper, we focus on model-based RL and take the maximum likelihood estimation (MLE) approach to learn transition model. Our work encompasses both online and offline settings. In the online setting, we introduce an algorithm called corruption-robust optimistic MLE (CR-OMLE), which leverages total-variation (TV)-based information ratios as uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of $\\tilde{\\mathcal{O}}(\\sqrt{T} + C)$, where $C$ denotes the cumulative corruption level after $T$ episodes. We also prove a lower bound to show that the additive dependence on $C$ is optimal. We extend our weighting technique to the offline setting, and propose an algorithm named corruption-robust pessimistic MLE (CR-PMLE). Under a uniform coverage condition, CR-PMLE exhibits suboptimality worsened by $\\mathcal{O}(C/n)$, nearly matching the lower bound. To the best of our knowledge, this is the first work on corruption-robust model-based RL algorithms with provable guarantees",
    "checked": true,
    "id": "63a3fa67bbf4ba6f52aa117353167e94a2fdce78",
    "semantic_title": "towards robust model-based reinforcement learning against adversarial corruption",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=fdroxYsgzQ": {
    "title": "Prompting is a Double-Edged Sword: Improving Worst-Group Robustness of Foundation Models",
    "volume": "poster",
    "abstract": "Machine learning models fail catastrophically under distribution shift, but a surprisingly effective way to empirically improve robustness to some types of shift (*e.g.*, Imagenet-A/C) is to use stronger open-vocabulary classifiers derived from foundation models. In this work, we first note that for shifts governed by spurious correlations (features spuriously correlated with the label on the training data, but not on test), the zero-shot and few-shot performance of foundation models is no better than ERM models, and remains unchanged when pretrained data/model size is scaled. Secondly, even in these situations, foundation models are quite accurate at predicting the value of the spurious feature. In a simplified setup, we theoretically analyze both these findings. Specifically, we show that during contrastive pretraining, the simplicity bias of foundation models tends to result in the learning of features that mostly rely on the spurious attribute, compared to more robust features. We leverage these observations to propose Prompting for Robustness (PfR) which first uses foundation models to zero-shot predict the spurious attribute on labeled examples, and then learns a classifier with balanced performance across different groups of labels and spurious attribute. Across 5 vision and language tasks, we show that PfR's performance nearly equals that of an oracle algorithm (group DRO) that leverages human labeled spurious attributes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=btYeH65fI3": {
    "title": "Precise Accuracy / Robustness Tradeoffs in Regression: Case of General Norms",
    "volume": "poster",
    "abstract": "In this paper, we investigate the impact of test-time adversarial attacks on linear regression models and determine the optimal level of robustness that any model can reach while maintaining a given level of standard predictive performance (accuracy). Through quantitative estimates, we uncover fundamental tradeoffs between adversarial robustness and accuracy in different regimes. We obtain a precise characterization which distinguishes between regimes where robustness is achievable without hurting standard accuracy and regimes where a tradeoff might be unavoidable. Our findings are empirically confirmed with simple experiments that represent a variety of settings. This work covers feature covariance matrices and attack norms of any nature, extending previous works in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=haUOhXo70o": {
    "title": "Hard Tasks First: Multi-Task Reinforcement Learning Through Task Scheduling",
    "volume": "poster",
    "abstract": "Multi-task reinforcement learning (RL) faces the significant challenge of varying task difficulties, often leading to negative transfer when simpler tasks overshadow the learning of more complex ones. To overcome this challenge, we propose a novel algorithm, Scheduled Multi-Task Training (SMT), that strategically prioritizes more challenging tasks, thereby enhancing overall learning efficiency. SMT introduces a dynamic task prioritization strategy, underpinned by an effective metric for assessing task difficulty. This metric ensures an efficient and targeted allocation of training resources, significantly improving learning outcomes. Additionally, SMT incorporates a reset mechanism that periodically reinitializes key network parameters to mitigate the simplicity bias, further enhancing the adaptability and robustness of the learning process across diverse tasks. The efficacy of SMT's scheduling method is validated by significantly improving performance on challenging Meta-World benchmarks",
    "checked": false,
    "id": "62d30795f7b1db0ab48476a752cab9b4914c5956",
    "semantic_title": "t3s: improving multi-task reinforcement learning with task-specific feature selector and scheduler",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ht20wtgaty": {
    "title": "Fault Tolerant ML: Efficient Meta-Aggregation and Synchronous Training",
    "volume": "poster",
    "abstract": "In this paper, we investigate the challenging framework of Byzantine-robust training in distributed machine learning (ML) systems, focusing on enhancing both efficiency and practicality. As distributed ML systems become integral for complex ML tasks, ensuring resilience against Byzantine failures—where workers may contribute incorrect updates due to malice or error—gains paramount importance. Our first contribution is the introduction of the Centered Trimmed Meta Aggregator (CTMA), an efficient meta-aggregator that upgrades baseline aggregators to optimal performance levels, while requiring low computational demands. Additionally, we propose harnessing a recently developed gradient estimation technique based on a double-momentum strategy within the Byzantine context. Our paper highlights its theoretical and practical advantages for Byzantine-robust training, especially in simplifying the tuning process and reducing the reliance on numerous hyperparameters. The effectiveness of this technique is supported by theoretical insights within the stochastic convex optimization (SCO) framework and corroborated by empirical evidence",
    "checked": true,
    "id": "7f13183746c65b7164be80b27cb0bc23afaccaf5",
    "semantic_title": "fault tolerant ml: efficient meta-aggregation and synchronous training",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xye7iNsgXn": {
    "title": "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations",
    "volume": "poster",
    "abstract": "Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundation models in recommendations",
    "checked": true,
    "id": "a1a05b4c7e8adfe15345885da40c73c12f189e39",
    "semantic_title": "actions speak louder than words: trillion-parameter sequential transducers for generative recommendations",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=XwnABAdH5y": {
    "title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning",
    "volume": "poster",
    "abstract": "Trustworthiness is an essential prerequisite for the real-world application of large language models. In this paper, we focus on the trustworthiness of language models with respect to retrieval augmentation. Despite being supported with external evidence, retrieval-augmented generation still suffers from hallucinations, one primary cause of which is the conflict between contextual and parametric knowledge. We deem that retrieval-augmented language models have the inherent capabilities of supplying response according to both contextual and parametric knowledge. Inspired by aligning language models with human preference, we take the first step towards aligning retrieval-augmented language models to a status where it responds relying merely on the external evidence and disregards the interference of parametric knowledge. Specifically, we propose a reinforcement learning based algorithm Trustworthy-Alignment, theoretically and experimentally demonstrating large language models' capability of reaching a trustworthy status without explicit supervision on how to respond. Our work highlights the potential of large language models on exploring its intrinsic abilities by its own and expands the application scenarios of alignment from fulfilling human preference to creating trustworthy agents",
    "checked": false,
    "id": "5eed774ff8607ae668db5a3755ebb0fb8a7c6ccb",
    "semantic_title": "improving the validity of automatically generated feedback via reinforcement learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=MgTzMaYHvG": {
    "title": "Instruction Tuning for Secure Code Generation",
    "volume": "poster",
    "abstract": "Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility",
    "checked": true,
    "id": "3cc4f14f4a174f524115eba6f228409db2856fdf",
    "semantic_title": "instruction tuning for secure code generation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=H8pMSJwRD5": {
    "title": "Don't be so Negative! Score-based Generative Modeling with Oracle-assisted Guidance",
    "volume": "poster",
    "abstract": "Score-based diffusion models are a powerful class of generative models, widely utilized across diverse domains. Despite significant advancements in large-scale tasks such as text-to-image generation, their application to constrained domains has received considerably less attention. This work addresses model learning in a setting where, in addition to the training dataset, there further exists side-information in the form of an oracle that can label samples as being outside the support of the true data generating distribution. Specifically we develop a new denoising diffusion probabilistic modeling methodology, Gen-neG, that leverages this additional side-information. Gen-neG builds on classifier guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle. We empirically establish the utility of Gen-neG in applications including collision avoidance in self-driving simulators and safety-guarded human motion generation",
    "checked": true,
    "id": "7da05f84e16a6003f7167ec004c413f3b1e91fa2",
    "semantic_title": "don't be so negative! score-based generative modeling with oracle-assisted guidance",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GcZjpKA37R": {
    "title": "LangCell: Language-Cell Pre-training for Cell Identity Understanding",
    "volume": "poster",
    "abstract": "Cell identity encompasses various semantic aspects of a cell, including cell type, pathway information, disease information, and more, which are essential for biologists to gain insights into its biological characteristics. Understanding cell identity from the transcriptomic data, such as annotating cell types, has become an important task in bioinformatics. As these semantic aspects are determined by human experts, it is impossible for AI models to effectively carry out cell identity understanding tasks without the supervision signals provided by single-cell and label pairs. The single-cell pre-trained language models (PLMs) currently used for this task are trained only on a single modality, transcriptomics data, lack an understanding of cell identity knowledge. As a result, they have to be fine-tuned for downstream tasks and struggle when lacking labeled data with the desired semantic labels. To address this issue, we propose an innovative solution by constructing a unified representation of single-cell data and natural language during the pre-training phase, allowing the model to directly incorporate insights related to cell identity. More specifically, we introduce **LangCell**, the first **Lang**uage-**Cell** pre-training framework. LangCell utilizes texts enriched with cell identity information to gain a profound comprehension of cross-modal knowledge. Results from experiments conducted on different benchmarks show that LangCell is the only single-cell PLM that can work effectively in zero-shot cell identity understanding scenarios, and also significantly outperforms existing models in few-shot and fine-tuning cell identity understanding scenarios",
    "checked": true,
    "id": "9ce654d654394c54b5c25b8d28af3eb81b5fd8eb",
    "semantic_title": "langcell: language-cell pre-training for cell identity understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Pte6iiXvpf": {
    "title": "Causal Representation Learning from Multiple Distributions: A General Setting",
    "volume": "poster",
    "abstract": "In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the latent causal variables (e.g., the underlying concepts or objects). For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the latent causal variables $Z_i$ and their causal relations represented by graph $\\mathcal{G}_Z$. This problem has recently been known as causal representation learning. This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes. We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions. We show that under the sparsity constraint on the recovered graph over the latent variables and suitable sufficient change conditions on the causal influences, interestingly, one can recover the moralized graph of the underlying directed acyclic graph, and the recovered latent variables and their relations are related to the underlying causal model in a specific, nontrivial way. In some cases, most latent variables can even be recovered up to component-wise transformations. Experimental results verify our theoretical claims",
    "checked": true,
    "id": "5aea80d0cad8fb2d94bf3c5f163f25034aa062df",
    "semantic_title": "causal representation learning from multiple distributions: a general setting",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=2NUGeV64y2": {
    "title": "Diffusion Models Demand Contrastive Guidance for Adversarial Purification to Advance",
    "volume": "poster",
    "abstract": "In adversarial defense, adversarial purification can be viewed as a special generation task with the purpose to remove adversarial attacks and diffusion models excel in adversarial purification for their strong generative power. With different predetermined generation requirements, various types of guidance have been proposed, but few of them focuses on adversarial purification. In this work, we propose to guide diffusion models for adversarial purification using contrastive guidance. We theoretically derive the proper noise level added in the forward process diffusion models for adversarial purification from a feature learning perspective. For the reverse process, it is implied that the role of contrastive loss guidance is to facilitate the evolution towards the signal direction. From the theoretical findings and implications, we design the forward process with the proper amount of Gaussian noise added and the reverse process with the gradient of contrastive loss as the guidance of diffusion models for adversarial purification. Empirically, extensive experiments on CIFAR-10, CIFAR-100, the German Traffic Sign Recognition Benchmark and ImageNet datasets with ResNet and WideResNet classifiers show that our method outperforms most of current adversarial training and adversarial purification methods by a large improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o5SVr80Rgg": {
    "title": "PairNet: Training with Observed Pairs to Estimate Individual Treatment Effect",
    "volume": "poster",
    "abstract": "Given a dataset of individuals each described by a covariate vector, a treatment, and an observed outcome on the treatment, the goal of the individual treatment effect (ITE) estimation task is to predict outcome changes resulting from a change in treatment. A fundamental challenge is that in the observational data, a covariate's outcome is observed only under one treatment, whereas we need to infer the difference in outcomes under two different treatments. Several existing approaches address this issue through training with inferred pseudo-outcomes, but their success relies on the quality of these pseudo-outcomes. We propose PairNet, a novel ITE estimation training strategy that minimizes losses over pairs of examples based on their factual observed outcomes. Theoretical analysis for binary treatments reveals that PairNet is a consistent estimator of ITE risk, and achieves smaller generalization error than baseline models. Empirical comparison with thirteen existing methods across eight benchmarks, covering both discrete and continuous treatments, shows that PairNet achieves significantly lower ITE error compared to the baselines. Also, it is model-agnostic and easy to implement",
    "checked": true,
    "id": "58ed48644dfadb0b0738cf185a05c368d48dff32",
    "semantic_title": "pairnet: training with observed pairs to estimate individual treatment effect",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s5PLISyNyP": {
    "title": "Meta-Learners for Partially-Identified Treatment Effects Across Multiple Environments",
    "volume": "poster",
    "abstract": "Estimating the conditional average treatment effect (CATE) from observational data is relevant for many applications such as personalized medicine. Here, we focus on the widespread setting where the observational data come from multiple environments, such as different hospitals, physicians, or countries. Furthermore, we allow for violations of standard causal assumptions, namely, overlap within the environments and unconfoundedness. To this end, we move away from point identification and focus on partial identification. Specifically, we show that current assumptions from the literature on multiple environments allow us to interpret the environment as an instrumental variable (IV). This allows us to adapt bounds from the IV literature for partial identification of CATE by leveraging treatment assignment mechanisms across environments. Then, we propose different model-agnostic learners (so-called meta-learners) to estimate the bounds that can be used in combination with arbitrary machine learning models. We further demonstrate the effectiveness of our meta-learners across various experiments using both simulated and real-world data. Finally, we discuss the applicability of our meta-learners to partial identification in instrumental variable settings, such as randomized controlled trials with non-compliance",
    "checked": true,
    "id": "28371aac7946a4ae5d549f095cdc442460b7090d",
    "semantic_title": "meta-learners for partially-identified treatment effects across multiple environments",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AwLLSlJAeJ": {
    "title": "Principled Gradient-Based MCMC for Conditional Sampling of Text",
    "volume": "poster",
    "abstract": "We consider the problem of sampling text from an energy-based model. This arises, for example, when sampling text from a neural language model subject to soft constraints. Although the target distribution is discrete, the internal computations of the energy function (given by the language model) are differentiable, so one would like to exploit gradient information within a method such as MCMC. Alas, all previous attempts to generalize gradient-based MCMC to text sampling fail to sample correctly from the target distribution. We propose a solution, along with variants, and study its theoretical properties. Through experiments on various forms of text generation, we demonstrate that our unbiased samplers are able to generate more fluent text while better adhering to the control objectives. The same methods could be used to sample from discrete energy-based models unrelated to text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7bjyambg4x": {
    "title": "Maestro: Uncovering Low-Rank Structures via Trainable Decomposition",
    "volume": "poster",
    "abstract": "Deep Neural Networks (DNNs) have been a large driver for AI breakthroughs in recent years, ranging from self-driving cars to intelligent assistants. However, these models have been getting increasingly large as they become more accurate and safe. This means that their training becomes increasingly costly and time-consuming, and typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification or quantization of the model weights and updates. While achieving high compression rates, they often incur significant computational overheads at training or lead to non-negligible accuracy penalty. Alternatively, factorization methods have been leveraged for low-rank compression of DNNs. Similarly, such techniques (e.g., SVD) frequently rely on heavy iterative decompositions of layers and are potentially sub-optimal for non-linear models, such as DNNs. We take a further step in designing efficient low-rank models and propose Maestro, a framework for trainable low-rank layers. Instead of iteratively applying a priori decompositions, the low-rank structure is baked into the training process through LoD, a low-rank ordered decomposition. Not only is this the first time importance ordering via sampling is applied on the decomposed DNN structure, but it also allows selecting ranks at a layer granularity. Our theoretical analysis demonstrates that LoD recovers the SVD decomposition of linear mapping on uniformly distributed data and PCA for linear autoencoders. Applied to DNNs, Maestro enables the extraction of lower footprint models that preserve performance. Simultaneously, it enables the graceful tradeoff between accuracy-latency for deployment to even more constrained devices, without retraining",
    "checked": true,
    "id": "5effe321c433004fe37fed5bac2f8738a29d0444",
    "semantic_title": "maestro: uncovering low-rank structures via trainable decomposition",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=h3SGdpI4Ta": {
    "title": "Infinite-Horizon Distributionally Robust Regret-Optimal Control",
    "volume": "poster",
    "abstract": "We study the infinite-horizon distributionally robust (DR) control of linear systems with quadratic costs, where disturbances have unknown, possibly time-correlated distribution within a Wasserstein-2 ambiguity set. We aim to minimize the worst-case expected regret—the excess cost of a causal policy compared to a non-causal one with access to future disturbance. Though the optimal policy lacks a finite-order state-space realization (i.e., it is non-rational), it can be characterized by a finite-dimensional parameter. Leveraging this, we develop an efficient frequency-domain algorithm to compute this optimal control policy and present a convex optimization method to construct a near-optimal state-space controller that approximates the optimal non-rational controller in the $\\mathit{H}_\\infty$-norm. This approach avoids solving a computationally expensive semi-definite program (SDP) that scales with the time horizon in the finite-horizon setting",
    "checked": true,
    "id": "89d9cbe5dffa3de1d85711d0d02d8914bfa59ab0",
    "semantic_title": "infinite-horizon distributionally robust regret-optimal control",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F3936hVwQa": {
    "title": "Conformal Validity Guarantees Exist for Any Data Distribution (and How to Find Them)",
    "volume": "poster",
    "abstract": "As artificial intelligence (AI) / machine learning (ML) gain widespread adoption, practitioners are increasingly seeking means to quantify and control the risk these systems incur. This challenge is especially salient when such systems have autonomy to collect their own data, such as in black-box optimization and active learning, where their actions induce sequential feedback-loop shifts in the data distribution. Conformal prediction is a promising approach to uncertainty and risk quantification, but prior variants' validity guarantees have assumed some form of ``quasi-exchangeability'' on the data distribution, thereby excluding many types of sequential shifts. In this paper we prove that conformal prediction can theoretically be extended to *any* joint data distribution, not just exchangeable or quasi-exchangeable ones. Although the most general case is exceedingly impractical to compute, for concrete practical applications we outline a procedure for deriving specific conformal algorithms for any data distribution, and we use this procedure to derive tractable algorithms for a series of AI/ML-agent-induced covariate shifts. We evaluate the proposed algorithms empirically on synthetic black-box optimization and active learning tasks",
    "checked": true,
    "id": "bb86b50363e9d100606a534c6a877dacbf8b0e25",
    "semantic_title": "conformal validity guarantees exist for any data distribution (and how to find them)",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kLZZWvqlEm": {
    "title": "StrWAEs to Invariant Representations",
    "volume": "poster",
    "abstract": "Autoencoders have become an indispensable tool for generative modeling and representation learning in high dimensions. Imposing structural constraints such as conditional independence in order to capture invariance of latent variables to nuisance information has been attempted through adding *ad hoc* penalties to the loss function mostly in the variational autoencoder (VAE) context, often based on heuristics. This paper demonstrates that Wasserstein autoencoders (WAEs) are highly flexible in embracing such structural constraints. Well-known extensions of VAEs for this purpose are gracefully handled within the framework of WAEs. In particular, given a conditional independence structure of the generative model (decoder), corresponding encoder structure and penalties are derived from the functional constraints that define the WAE. These structural uses of WAEs, termed StrWAEs (\"stairways\"), open up a principled way of penalizing autoencoders to impose structural constraints. Utilizing these advantages, we present handful of results on semi-supervised classification, conditional generation, and invariant representation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BrZPj9rEpN": {
    "title": "Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics",
    "volume": "poster",
    "abstract": "Developing policies that can adapt to non-stationary environments is essential for real-world reinforcement learning applications. Nevertheless, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called debiased offline representation learning for fast online adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leveraging tractable bounds of the information bottleneck principle. Our experimental evaluation across six benchmark MuJoCo tasks with variable parameters demonstrates that DORA not only achieves a more precise dynamics encoding but also significantly outperforms existing baselines in terms of performance",
    "checked": true,
    "id": "6ed0290f0497b4320e9bec15c4e14c0ba44d9a3c",
    "semantic_title": "debiased offline representation learning for fast online adaptation in non-stationary dynamics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJVjQSQ8ye": {
    "title": "Linguistic Calibration of Long-Form Generations",
    "volume": "poster",
    "abstract": "Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce long-form text with calibrated confidence statements. Through the lens of decision-making, we define linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as \"I estimate a 30% chance of...\" or \"I am certain that...\", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under significant domain shifts to scientific and biomedical questions and to an entirely held-out person biography generation task. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making",
    "checked": true,
    "id": "947687643a7d26f7ee370aa40e7ff54d29ab00ea",
    "semantic_title": "linguistic calibration of long-form generations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kXHgEYFyf3": {
    "title": "R2E: Turning any Github Repository into a Programming Agent Environment",
    "volume": "poster",
    "abstract": "While Large Language Models' (LLMs) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI coding agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R2E code is available at https://r2e.dev/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aECamk9izk": {
    "title": "Learning to Explore for Stochastic Gradient MCMC",
    "volume": "poster",
    "abstract": "Bayesian Neural Networks(BNNs) with high-dimensional parameters pose a challenge for posterior inference due to the multi-modality of the posterior distributions. Stochastic Gradient Markov Chain Monte Carlo(SGMCMC) with cyclical learning rate scheduling is a promising solution, but it requires a large number of sampling steps to explore high-dimensional multi-modal posteriors, making it computationally expensive. In this paper, we propose a meta-learning strategy to build SGMCMC which can efficiently explore the multi-modal target distributions. Our algorithm allows the learned SGMCMC to quickly explore the high-density region of the posterior landscape. Also, we show that this exploration property is transferrable to various tasks, even for the ones unseen during a meta-training stage. Using popular image classification benchmarks and a variety of downstream tasks, we demonstrate that our method significantly improves the sampling efficiency, achieving better performance than vanilla SGMCMC without incurring significant computational overhead",
    "checked": false,
    "id": "a6cb12eefca903fe935fcf104030d867b7f28663",
    "semantic_title": "a probabilistic approach to self-supervised learning using cyclical stochastic gradient mcmc",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=disVlUOH4b": {
    "title": "Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning",
    "volume": "poster",
    "abstract": "Despite the recent successes of multi-agent reinforcement learning (MARL) algorithms, efficiently adapting to co-players in mixed-motive environments remains a significant challenge. One feasible approach is to hierarchically model co-players' behavior based on inferring their characteristics. However, these methods often encounter difficulties in efficient reasoning and utilization of inferred information. To address these issues, we propose Hierarchical Opponent modeling and Planning (HOP), a novel multi-agent decision-making algorithm that enables few-shot adaptation to unseen policies in mixed-motive environments. HOP is hierarchically composed of two modules: an opponent modeling module that infers others' goals and learns corresponding goal-conditioned policies, and a planning module that employs Monte Carlo Tree Search (MCTS) to identify the best response. Our approach improves efficiency by updating beliefs about others' goals both across and within episodes and by using information from the opponent modeling module to guide planning. Experimental results demonstrate that in mixed-motive environments, HOP exhibits superior few-shot adaptation capabilities when interacting with various unseen agents, and excels in self-play scenarios. Furthermore, the emergence of social intelligence during our experiments underscores the potential of our approach in complex multi-agent environments",
    "checked": true,
    "id": "90ef2bbe472d969d21545502abbee504c701730d",
    "semantic_title": "efficient adaptation in mixed-motive environments via hierarchical opponent modeling and planning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LbcNAIgNnB": {
    "title": "How to Explore with Belief: State Entropy Maximization in POMDPs",
    "volume": "poster",
    "abstract": "Recent works have studied *state entropy maximization* in reinforcement learning, in which the agent's objective is to learn a policy inducing high entropy over states visitation (Hazan et al., 2019). They typically assume full observability of the state of the system, so that the entropy of the observations is maximized. In practice, the agent may only get *partial* observations, e.g., a robot perceiving the state of a physical space through proximity sensors and cameras. A significant mismatch between the entropy over observations and true states of the system can arise in those settings. In this paper, we address the problem of entropy maximization over the *true states* with a decision policy conditioned on partial observations *only*. The latter is a generalization of POMDPs, which is intractable in general. We develop a memory and computationally efficient *policy gradient* method to address a first-order relaxation of the objective defined on *belief* states, providing various formal characterizations of approximation gaps, the optimization landscape, and the *hallucination* problem. This paper aims to generalize state entropy maximization to more realistic domains that meet the challenges of applications",
    "checked": true,
    "id": "09c3376cee8c849d571611336d081e2460f44a6a",
    "semantic_title": "how to explore with belief: state entropy maximization in pomdps",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bfQCO9Vqhk": {
    "title": "Submodular framework for structured-sparse optimal transport",
    "volume": "poster",
    "abstract": "Unbalanced optimal transport (UOT) has recently gained much attention due to its flexible framework for handling un-normalized measures and its robustness properties. In this work, we explore learning (structured) sparse transport plans in the UOT setting, i.e., transport plans have an upper bound on the number of non-sparse entries in each column (structured sparse pattern) or in the whole plan (general sparse pattern). We propose novel sparsity-constrained UOT formulations building on the recently explored maximum mean discrepancy based UOT. We show that the proposed optimization problem is equivalent to the maximization of a weakly submodular function over a uniform matroid or a partition matroid. We develop efficient gradient-based discrete greedy algorithms and provide the corresponding theoretical guarantees. Empirically, we observe that our proposed greedy algorithms select a diverse support set and we illustrate the efficacy of the proposed approach in various applications",
    "checked": true,
    "id": "826f00d1e303f09ee0b9e033e99774436ec51865",
    "semantic_title": "submodular framework for structured-sparse optimal transport",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bPsohGR6gD": {
    "title": "Graph-Triggered Rising Bandits",
    "volume": "poster",
    "abstract": "In this paper, we propose a novel generalization of rested and restless bandits where the evolution of the arms' expected rewards is governed by a graph defined over the arms. An edge connecting a pair of arms $(i,j)$ represents the fact that a pull of arm $i$ *triggers* the evolution of arm $j$, and vice versa. Interestingly, rested and restless bandits are both special cases of our model for some suitable (degenerate) graphs. Still, the model can represent way more general and interesting scenarios. We first tackle the problem of computing the optimal policy when no specific structure is assumed on the graph, showing that it is NP-hard. Then, we focus on a specific structure forcing the graph to be composed of a set of fully connected subgraphs (i.e., cliques), and we prove that the optimal policy can be easily computed in closed form. Then, we move to the learning problem presenting regret minimization algorithms for deterministic and stochastic cases. Our regret bounds highlight the complexity of the learning problem by incorporating instance-dependent terms that encode specific properties of the underlying graph structure. Moreover, we illustrate how the knowledge of the underlying graph is not necessary for achieving the no-regret property",
    "checked": false,
    "id": "ae67f1925a14d87477f0cfea9031b06301300748",
    "semantic_title": "socioeconomic effect of the scourge of banditry in niger state, nigeria",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=59MYoLghyk": {
    "title": "Breadth-First Exploration on Adaptive Grid for Reinforcement Learning",
    "volume": "poster",
    "abstract": "Graph-based planners have gained significant attention for goal-conditioned reinforcement learning (RL), where they construct a graph consisting of confident transitions between *subgoals* as edges and run shortest path algorithms to exploit the confident edges. Meanwhile, identifying and avoiding unattainable transitions are also crucial yet overlooked by the previous graph-based planners, leading to wasting an excessive number of attempts at unattainable subgoals. To address this oversight, we propose a graph construction method that efficiently manages all the achieved and unattained subgoals on a grid graph adaptively discretizing the goal space. This enables a breadth-first exploration strategy, grounded in the local adaptive grid refinement, that prioritizes broad probing of subgoals on a coarse grid over meticulous one on a dense grid. We conducted a theoretical analysis and demonstrated the effectiveness of our approach through empirical evidence, showing that only BEAG succeeds in complex environments under the proposed fixed-goal setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pOgMluzEIH": {
    "title": "SILVER: Single-loop variance reduction and application to federated learning",
    "volume": "poster",
    "abstract": "Most variance reduction methods require multiple times of full gradient computation, which is time-consuming and hence a bottleneck in application to distributed optimization. We present a single-loop variance-reduced gradient estimator named SILVER (SIngle-Loop VariancE-Reduction) for the finite-sum non-convex optimization, which does not require multiple full gradients but nevertheless achieves the optimal gradient complexity. Notably, unlike existing methods, SILVER provably reaches second-order optimality, with exponential convergence in the Polyak-Łojasiewicz (PL) region, and achieves further speedup depending on the data heterogeneity. Owing to these advantages, SILVER serves as a new base method to design communication-efficient federated learning algorithms: we combine SILVER with local updates which gives the best communication rounds and number of communicated gradients across all range of Hessian heterogeneity, and, at the same time, guarantees second-order optimality and exponential convergence in the PL region",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FoRqdsN4IA": {
    "title": "Generative Conditional Distributions by Neural (Entropic) Optimal Transport",
    "volume": "poster",
    "abstract": "Learning conditional distributions is challenging because the desired outcome is not a single distribution but multiple distributions that correspond to multiple instances of the covariates. We introduce a novel neural entropic optimal transport method designed to effectively learn generative models of conditional distributions, particularly in scenarios characterized by limited sample sizes. Our method relies on the minimax training of two neural networks: a generative network parametrizing the inverse cumulative distribution functions of the conditional distributions and another network parametrizing the conditional Kantorovich potential. To prevent overfitting, we regularize the objective function by penalizing the Lipschitz constant of the network output. Our experiments on real-world datasets show the effectiveness of our algorithm compared to state-of-the-art conditional distribution learning techniques. Our implementation can be found at https://github.com/nguyenngocbaocmt02/GENTLE",
    "checked": true,
    "id": "60a41ea14baf17541c0ff7ed52c0e4d84652dd29",
    "semantic_title": "generative conditional distributions by neural (entropic) optimal transport",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=EWt5wsEdvc": {
    "title": "Cell2Sentence: Teaching Large Language Models the Language of Biology",
    "volume": "poster",
    "abstract": "We introduce Cell2Sentence (C2S), a novel method to directly adapt large language models to a biological context, specifically single-cell transcriptomics. By transforming gene expression data into \"cell sentences,\" C2S bridges the gap between natural language processing and biology. We demonstrate cell sentences enable the fine-tuning of language models for diverse tasks in biology, including cell generation, complex cell-type annotation, and direct data-driven text generation. Our experiments reveal that GPT-2, when fine-tuned with C2S, can generate biologically valid cells based on cell type inputs, and accurately predict cell types from cell sentences. This illustrates that language models, through C2S fine-tuning, can acquire a significant understanding of single-cell biology while maintaining robust text generation capabilities. C2S offers a flexible, accessible framework to integrate natural language processing with transcriptomics, utilizing existing models and libraries for a wide range of biological applications",
    "checked": true,
    "id": "587547493b5cf221af4b929cf390ef81e9768937",
    "semantic_title": "cell2sentence: teaching large language models the language of biology",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Wp054bnPq9": {
    "title": "Watermark Stealing in Large Language Models",
    "volume": "poster",
    "abstract": "LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying *watermark stealing* (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical *spoofing attacks*, as hypothesized in prior work, but also greatly boosts *scrubbing* attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80\\%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org",
    "checked": true,
    "id": "c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
    "semantic_title": "watermark stealing in large language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=OnOaj3g9fi": {
    "title": "A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models",
    "volume": "poster",
    "abstract": "Diffusion models have shown remarkable performance in generation problems over various domains including images, videos, text, and audio. A practical bottleneck of diffusion models is their sampling speed, due to the repeated evaluation of score estimation networks during the inference. In this work, we propose a novel framework capable of adaptively allocating compute required for the score estimation, thereby reducing the overall sampling time of diffusion models. We observe that the amount of computation required for the score estimation may vary along the time step for which the score is estimated. Based on this observation, we propose an early-exiting scheme, where we skip the subset of parameters in the score estimation network during the inference, based on a time-dependent exit schedule. Using the diffusion models for image synthesis, we show that our method could significantly improve the sampling throughput of the diffusion models without compromising image quality. Furthermore, we also demonstrate that our method seamlessly integrates with various types of solvers for faster sampling, capitalizing on their compatibility to enhance overall efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HTNgNt8CTJ": {
    "title": "Weisfeiler-Leman at the margin: When more expressivity matters",
    "volume": "poster",
    "abstract": "The Weisfeiler--Leman algorithm (1-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, the 1-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting 1-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we introduce variations of expressive 1-WL-based kernel and MPNN architectures with provable generalization properties. Our empirical study confirms the validity of our theoretical findings",
    "checked": true,
    "id": "03b4eb518db81999ad194560caa7eae57761cf9b",
    "semantic_title": "weisfeiler-leman at the margin: when more expressivity matters",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=rTBR0eqE4G": {
    "title": "Decomposing and Editing Predictions by Modeling Model Computation",
    "volume": "poster",
    "abstract": "*How does the internal computation of a machine learning model transform inputs into predictions?* To tackle this question, we introduce a framework called *component modeling* for decomposing a model prediction in terms of its components---architectural \"building blocks\" such as convolution filters or attention heads. We focus on a special case of this framework, *component attribution*, where the goal is to estimate the counterfactual impact of individual components on a given prediction. We then present COAR, a scalable algorithm for estimating component attributions, and demonstrate its effectiveness across models, datasets and modalities. Finally, we show that COAR directly enables effective model editing. Our code is available at [github.com/MadryLab/modelcomponents]([https://github.com/MadryLab/modelcomponents])",
    "checked": true,
    "id": "f0df1b98b3ab52386f290b3419c9bc3a2ed0b24c",
    "semantic_title": "decomposing and editing predictions by modeling model computation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=dMhF96PfQi": {
    "title": "Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport",
    "volume": "poster",
    "abstract": "Wasserstein gradient flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrized transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.42 on CelebA-HQ-256, which are comparable to state-of-the-art image generative models",
    "checked": true,
    "id": "4aff6ed1511dbe413ea791db92cc3f4bff4702e5",
    "semantic_title": "scalable wasserstein gradient flow for generative modeling through unbalanced optimal transport",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXaEYzrFae": {
    "title": "Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation",
    "volume": "poster",
    "abstract": "To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding methods propose to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods often incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\\times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin. We release DOMINO as open source at https://github.com/eth-sri/domino",
    "checked": true,
    "id": "b95ca121a606da32180ab8bb0c0c58bf19b1499b",
    "semantic_title": "guiding llms the right way: fast, non-invasive constrained generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=hcASxFvmZ5": {
    "title": "Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams",
    "volume": "poster",
    "abstract": "We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, peeking with expectation-based averaged capital (PEAK), builds upon the testing-by-betting framework and provides a non-asymptotic $\\alpha$-level test across any stopping time. Our contributions are two-fold: (1) we propose a novel betting scheme and provide theoretical guarantees on type-I error control, power, and asymptotic growth rate/$e$-power in the setting of a single data stream; (2) we introduce PEAK, a generalization of this betting scheme to multiple streams, that (i) avoids using wasteful union bounds via averaging, (ii) is a test of power one under mild regularity conditions on the sampling scheme of the streams, and (iii) reduces computational overhead when applying the testing-as-betting approaches for pure-exploration bandit problems. We illustrate the practical benefits of PEAK using both synthetic and real-world HeartSteps datasets. Our experiments show that PEAK provides up to an 85% reduction in the number of samples before stopping compared to existing stopping rules for pure-exploration bandit problems, and matches the performance of state-of-the-art sequential tests while improving upon computational complexity",
    "checked": true,
    "id": "af83dd5b0a95f951698c10dc7178bd457c8ef0f8",
    "semantic_title": "peeking with peak: sequential, nonparametric composite hypothesis tests for means of multiple data streams",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M3uv4qDKOL": {
    "title": "DUPLEX: Dual GAT for Complex Embedding of Directed Graphs",
    "volume": "poster",
    "abstract": "Current directed graph embedding methods build upon undirected techniques but often inadequately capture directed edge information, leading to challenges such as: (1) Suboptimal representations for nodes with low in/out-degrees, due to the insufficient neighbor interactions; (2) Limited inductive ability for representing new nodes post-training; (3) Narrow generalizability, as training is overly coupled with specific tasks. In response, we propose DUPLEX, an inductive framework for complex embeddings of directed graphs. It (1) leverages Hermitian adjacency matrix decomposition for comprehensive neighbor integration, (2) employs a dual GAT encoder for directional neighbor modeling, and (3) features two parameter-free decoders to decouple training from particular tasks. DUPLEX outperforms state-of-the-art models, especially for nodes with sparse connectivity, and demonstrates robust inductive capability and adaptability across various tasks. The code will be available upon publication",
    "checked": true,
    "id": "82b5653a5ddbdb2c4958ae147e85adf458ba43eb",
    "semantic_title": "duplex: dual gat for complex embedding of directed graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aiz79FxjaI": {
    "title": "Exploiting Human-AI Dependence for Learning to Defer",
    "volume": "poster",
    "abstract": "The learning to defer (L2D) framework allows models to defer their decisions to human experts. For L2D, the Bayes optimality is the basic requirement of theoretical guarantees for the design of consistent surrogate loss functions, which requires the minimizer (i.e., learned classifier) by the surrogate loss to be the Bayes optimality. However, we find that the original form of Bayes optimality fails to consider the dependence between the model and the expert, and such a dependence could be further exploited to design a better consistent loss for L2D. In this paper, we provide a new formulation for the Bayes optimality called dependent Bayes optimality, which reveals the dependence pattern in determining whether to defer. Based on the dependent Bayes optimality, we further present a deferral principle for L2D. Following the guidance of the deferral principle, we propose a novel consistent surrogate loss. Comprehensive experimental results on both synthetic and real-world datasets demonstrate the superiority of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ToHnqYxjB": {
    "title": "Iterative Search Attribution for Deep Neural Networks",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) have achieved state-of-the-art performance across various applications. However, ensuring the reliability and trustworthiness of DNNs requires enhanced interpretability of model inputs and outputs. As an effective means of Explainable Artificial Intelligence (XAI) research, the interpretability of existing attribution algorithms varies depending on the choice of reference point, the quality of adversarial samples, or the applicability of gradient constraints in specific tasks. To thoroughly explore the attribution integration paths, in this paper, inspired by the iterative generation of high-quality samples in the diffusion model, we propose an Iterative Search Attribution (ISA) method. To enhance attribution accuracy, ISA distinguishes the importance of samples during gradient ascent and descent, while clipping the relatively unimportant features in the model. Specifically, we introduce a scale parameter during the iterative process to ensure the features in next iteration are always more significant than those in current iteration. Comprehensive experimental results show that our method has superior interpretability in image recognition tasks compared with state-of-the-art baselines. Our code is available at: https://github.com/LMBTough/ISA",
    "checked": false,
    "id": "630b34273650d49709ef81e2ff7e89fc07ca55b4",
    "semantic_title": "scattering center extraction for isar image using deep neural network",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pwfcwEqdUz": {
    "title": "Latent Logic Tree Extraction for Event Sequence Explanation from LLMs",
    "volume": "poster",
    "abstract": "Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework",
    "checked": true,
    "id": "fd80d2ca2bb584875a4dfbf5345b8c705a9a3a26",
    "semantic_title": "latent logic tree extraction for event sequence explanation from llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cPsn9AcOYh": {
    "title": "Understanding Finetuning for Factual Knowledge Extraction",
    "volume": "poster",
    "abstract": "In this work, we study the impact of QA fine-tuning data on downstream factuality. We show that fine-tuning on lesser-known facts that are poorly stored during pretraining yields significantly worse factuality than fine-tuning on well-known facts, even when all facts are seen during pretraining. We prove this phenomenon theoretically, showing that training on lesser-known facts can lead the model to ignore subject entity names and instead output a generic plausible response even when the relevant factual knowledge is encoded in the model. On three question answering benchmarks (PopQA, Entity Questions, and MMLU) and two language models (Llama-2-7B and Mistral-7B), we find that (i) finetuning on a completely factual but lesser-known subset of the data deteriorates downstream factuality (5-10%) and (ii) finetuning on a subset of better-known examples matches or outperforms finetuning on the entire dataset. Ultimately, our results shed light on the interaction between pretrained knowledge and finetuning data and demonstrate the importance of taking into account how facts are stored in the pretrained model when fine-tuning for knowledge-intensive tasks",
    "checked": true,
    "id": "27ecd1371eb7df1b7ea46c88822dd46d7e6a7dec",
    "semantic_title": "understanding finetuning for factual knowledge extraction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1tRLxQzdep": {
    "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
    "volume": "poster",
    "abstract": "Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively expensive and computationally demanding. Recently, post-training pruning approaches introduced novel metrics, enabling the pruning of LLMs without retraining. However, these metrics require the involvement of human experts and tedious trial and error. To efficiently identify superior pruning metrics, we develop an automatic framework for searching symbolic pruning metrics using genetic programming. In particular, we devise an elaborate search space encompassing the existing pruning metrics to discover the potential symbolic pruning metric. We propose an opposing operation simplification strategy to increase the diversity of the population. In this way, Pruner-Zero allows auto-generation of symbolic pruning metrics. Based on the searched results, we explore the correlation between pruning metrics and performance after pruning and summarize some principles. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate that our Pruner-Zero obtains superior performance than SOTA post-training pruning methods. Code at: https://github.com/pprp/Pruner-Zero",
    "checked": true,
    "id": "a7919a3c6dbdcc524776a3102110d637836ad2e0",
    "semantic_title": "pruner-zero: evolving symbolic pruning metric from scratch for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qAml3FpfhG": {
    "title": "tinyBenchmarks: evaluating LLMs with fewer examples",
    "volume": "poster",
    "abstract": "The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results",
    "checked": true,
    "id": "4c0f88e80320885e8289a9af781a1101717104f2",
    "semantic_title": "tinybenchmarks: evaluating llms with fewer examples",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=2pYTCy4GUV": {
    "title": "The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling",
    "volume": "poster",
    "abstract": "With the incorporation of the UNet architecture, diffusion probabilistic models have become a dominant force in image generation tasks. One key design in UNet is the skip connections between the encoder and decoder blocks. Although skip connections have been shown to improve training stability and model performance, we point out that such shortcuts can be a limiting factor for the complexity of the transformation. As the sampling steps decrease, the generation process and the role of the UNet get closer to the push-forward transformations from Gaussian distribution to the target, posing a challenge for the network's complexity. To address this challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free tuning method on the skip connections. For instance, our method can achieve 100% FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of ODE samplers regardless of sampling steps. Surprisingly, the improvement persists when we increase the number of sampling steps and can even surpass the best result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive exploratory experiments are conducted to shed light on the surprising effectiveness of our Skip-Tuning. We observe that while Skip-Tuning increases the score-matching losses in the pixel space, the losses in the feature space are reduced, particularly at intermediate noise levels, which coincide with the most effective range accounting for image quality improvement",
    "checked": true,
    "id": "2597830307f6178eeef0158a9b5ab65fba85f3b4",
    "semantic_title": "the surprising effectiveness of skip-tuning in diffusion sampling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yn8xnK90mS": {
    "title": "Unveiling the Cycloid Trajectory of EM Iterations in Mixed Linear Regression",
    "volume": "poster",
    "abstract": "We study the trajectory of iterations and the convergence rates of the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR). The fundamental goal of MLR is to learn the regression models from unlabeled observations. The EM algorithm finds extensive applications in solving the mixture of linear regressions. Recent results have established the super-linear convergence of EM for 2MLR in the noiseless and high SNR settings under some assumptions and its global convergence rate with random initialization has been affirmed. However, the exponent of convergence has not been theoretically estimated and the geometric properties of the trajectory of EM iterations are not well-understood. In this paper, first, using Bessel functions we provide explicit closed-form expressions for the EM updates under all SNR regimes. Then, in the noiseless setting, we completely characterize the behavior of EM iterations by deriving a recurrence relation at the population level and notably show that all the iterations lie on a certain cycloid. Based on this new trajectory-based analysis, we exhibit the theoretical estimate for the exponent of super-linear convergence and further improve the statistical error bound at the finite-sample level. Our analysis provides a new framework for studying the behavior of EM for Mixed Linear Regression",
    "checked": true,
    "id": "da49a1dfa92cde2fa95983d99a24dded4551e40b",
    "semantic_title": "unveiling the cycloid trajectory of em iterations in mixed linear regression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJFQROkaj0": {
    "title": "RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive reasoning abilities and general intelligence in various domains. It inspires researchers to train end-to-end MLLMs or utilize large models to generate policies with human-selected prompts for embodied agents. However, these methods exhibit limited generalization capabilities on unseen tasks or scenarios, and overlook the multimodal environment information which is critical for robots to make decisions. In this paper, we introduce a novel **Robo**tic **M**ultimodal **P**erception-**P**lanning (**RoboMP$^2$**) framework for robotic manipulation which consists of a Goal-Conditioned Multimodal Preceptor (GCMP) and a Retrieval-Augmented Multimodal Planner (RAMP). Specially, GCMP captures environment states by employing a tailored MLLMs for embodied agents with the abilities of semantic reasoning and localization. RAMP utilizes coarse-to-fine retrieval method to find the $k$ most-relevant policies as in-context demonstrations to enhance the planner. Extensive experiments demonstrate the superiority of RoboMP$^2$ on both VIMA benchmark and real-world tasks, with around 10% improvement over the baselines",
    "checked": false,
    "id": "e922f89b83f6668cf3bd9504038edfb61825130a",
    "semantic_title": "robomp2: a robotic multimodal perception-planning framework with multimodal large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=PlVjIGaFdH": {
    "title": "Consistent Diffusion Meets Tweedie: Training Exact Ambient Diffusion Models with Noisy Data",
    "volume": "poster",
    "abstract": "Ambient diffusion is a recently proposed framework for training diffusion models using corrupted data. Both Ambient Diffusion and alternative SURE-based approaches for learning diffusion models from corrupted data resort to approximations which deteriorate performance. We present the first framework for training diffusion models that provably sample from the uncorrupted distribution given only noisy training data, solving an open problem in Ambient diffusion. Our key technical contribution is a method that uses a double application of Tweedie's formula and a consistency loss function that allows us to extend sampling at noise levels below the observed data noise. We also provide further evidence that diffusion models memorize from their training sets by identifying extremely corrupted images that are almost perfectly reconstructed, raising copyright and privacy concerns. Our method for training using corrupted samples can be used to mitigate this problem. We demonstrate this by fine-tuning Stable Diffusion XL to generate samples from a distribution using only noisy samples. Our framework reduces the amount of memorization of the fine-tuning dataset, while maintaining competitive performance",
    "checked": true,
    "id": "f7b4211d53a05737b5efc161da69c78e2e7b8850",
    "semantic_title": "consistent diffusion meets tweedie: training exact ambient diffusion models with noisy data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=1RZKuvqYCR": {
    "title": "Token-level Direct Preference Optimization",
    "volume": "poster",
    "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models. However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion. In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods, which face challenges in divergence efficiency, TDPO integrates forward KL divergence constraints for each token, improving alignment and diversity. Utilizing the Bradley-Terry model for a token-based reward system, our method enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPO's superior performance in balancing alignment with generation diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods",
    "checked": true,
    "id": "8f2254fb38cfc8f79524fd1cd2609124808f2c8c",
    "semantic_title": "token-level direct preference optimization",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=FOJE1kRcHG": {
    "title": "Mean Field Langevin Actor-Critic: Faster Convergence and Global Optimality beyond Lazy Learning",
    "volume": "poster",
    "abstract": "This work explores the feature learning capabilities of deep reinforcement learning algorithms in the pursuit of optimal policy determination. We particularly examine an over-parameterized neural actor-critic framework within the mean-field regime, where both actor and critic components undergo updates via policy gradient and temporal-difference (TD) learning, respectively. We introduce the *mean-field Langevin TD learning* (MFLTD) method, enhancing mean-field Langevin dynamics with proximal TD updates for critic policy evaluation, and assess its performance against conventional approaches through numerical analysis. Additionally, for actor policy updates, we present the *mean-field Langevin policy gradient* (MFLPG), employing policy gradient techniques augmented by Wasserstein gradient flows for parameter space exploration. Our findings demonstrate that MFLTD accurately identifies the true value function, while MFLPG ensures linear convergence of actor sequences towards the globally optimal policy, considering a Kullback-Leibler divergence regularized framework. Through both time particle and discretized analysis, we substantiate the linear convergence guarantees of our neural actor-critic algorithms, representing a notable contribution to neural reinforcement learning focusing on *global optimality* and *feature learning*, extending the existing understanding beyond the conventional scope of lazy training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N6A6t6xlKm": {
    "title": "Enabling Uncertainty Estimation in Iterative Neural Networks",
    "volume": "poster",
    "abstract": "Turning pass-through network architectures into iterative ones, which use their own output as input, is a well-known approach for boosting performance. In this paper, we argue that such architectures offer an additional benefit: The convergence rate of their successive outputs is highly correlated with the accuracy of the value to which they converge. Thus, we can use the convergence rate as a useful proxy for uncertainty. This results in an approach to uncertainty estimation that provides state-of-the-art estimates at a much lower computational cost than techniques like Ensembles, and without requiring any modifications to the original iterative model. We demonstrate its practical value by embedding it in two application domains: road detection in aerial images and the estimation of aerodynamic properties of 2D and 3D shapes",
    "checked": true,
    "id": "df20134c61135ca54c0fcd18e8885e93f39a2f1d",
    "semantic_title": "enabling uncertainty estimation in iterative neural networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XTglHJjzQI": {
    "title": "Clifford-Steerable Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of ${\\operatorname{E}}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\\mathbb{R}^{p,q}$. They specialize, for instance, to ${\\operatorname{E}}(3)$-equivariance on $\\mathbb{R}^3$ and Poincaré-equivariance on Minkowski spacetime $\\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of ${\\operatorname{O}}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks",
    "checked": true,
    "id": "60ef44cb0455ebda60233958e8db78df867f03b9",
    "semantic_title": "clifford-steerable convolutional neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=hG6gddAKnJ": {
    "title": "Keep the Momentum: Conservation Laws beyond Euclidean Gradient Flows",
    "volume": "poster",
    "abstract": "Conservation laws are well-established in the context of Euclidean gradient flow dynamics, notably for linear or ReLU neural network training. Yet, their existence and principles for non-Euclidean geometries and momentum-based dynamics remain largely unknown. In this paper, we characterize \"all\" conservation laws in this general setting. In stark contrast to the case of gradient flows, we prove that the conservation laws for momentum-based dynamics exhibit temporal dependence. Additionally, we often observe a \"conservation loss\" when transitioning from gradient flow to momentum dynamics. Specifically, for linear networks, our framework allows us to identify all momentum conservation laws, which are less numerous than in the gradient flow case except in sufficiently over-parameterized regimes. With ReLU networks, no conservation law remains. This phenomenon also manifests in non-Euclidean metrics, used e.g. for Nonnegative Matrix Factorization (NMF): all conservation laws can be determined in the gradient flow context, yet none persists in the momentum case",
    "checked": true,
    "id": "49a12fba0bc9327b8898926c76833621bbc9ca6e",
    "semantic_title": "keep the momentum: conservation laws beyond euclidean gradient flows",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CbbTF6tDhW": {
    "title": "Improving Robustness to Multiple Spurious Correlations by Multi-Objective Optimization",
    "volume": "poster",
    "abstract": "We study the problem of training an unbiased and accurate model given a dataset with multiple biases. This problem is challenging since the multiple biases cause multiple undesirable shortcuts during training, and even worse, mitigating one may exacerbate the other. We propose a novel training method to tackle this challenge. Our method first groups training data so that different groups induce different shortcuts, and then optimizes a linear combination of group-wise losses while adjusting their weights dynamically to alleviate conflicts between the groups in performance; this approach, rooted in the multi-objective optimization theory, encourages to achieve the minimax Pareto solution. We also present a new benchmark with multiple biases, dubbed MultiCelebA, for evaluating debiased training methods under realistic and challenging scenarios. Our method achieved the best on three datasets with multiple biases, and also showed superior performance on conventional single-bias datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kMBvZ40Iu9": {
    "title": "Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation",
    "volume": "poster",
    "abstract": "Due to the high computational load of modern numerical simulation, there is a demand for approaches that would reduce the size of discrete problems while keeping the accuracy reasonable. In this work, we present an original algorithm to coarsen an unstructured grid based on the concepts of differentiable physics. We achieve this by employing $k$-means clustering, autodifferentiation and stochastic minimization algorithms. We demonstrate performance of the designed algorithm on two PDEs: a linear parabolic equation which governs slightly compressible fluid flow in porous media and the wave equation. Our results show that in the considered scenarios, we reduced the number of grid points up to 10 times while preserving the modeled variable dynamics in the points of interest. The proposed approach can be applied to the simulation of an arbitrary system described by evolutionary partial differential equations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vye4OgLaTy": {
    "title": "FlashST: A Simple and Universal Prompt-Tuning Framework for Traffic Prediction",
    "volume": "poster",
    "abstract": "The objective of traffic prediction is to accurately forecast and analyze the dynamics of transportation patterns, considering both space and time. However, the presence of distribution shift poses a significant challenge in this field, as existing models struggle to generalize well when faced with test data that significantly differs from the training distribution. To tackle this issue, this paper introduces a simple and universal spatio-temporal prompt-tuning framework-FlashST, which adapts pre-trained models to the specific characteristics of diverse downstream datasets, improving generalization in diverse traffic prediction scenarios. Specifically, the FlashST framework employs a lightweight spatio-temporal prompt network for in-context learning, capturing spatio-temporal invariant knowledge and facilitating effective adaptation to diverse scenarios. Additionally, we incorporate a distribution mapping mechanism to align the data distributions of pre-training and downstream data, facilitating effective knowledge transfer in spatio-temporal forecasting. Empirical evaluations demonstrate the effectiveness of our FlashST across different spatio-temporal prediction tasks using diverse urban datasets. Code is available at [https://github.com/HKUDS/FlashST](https://github.com/HKUDS/FlashST)",
    "checked": true,
    "id": "9f3d14dbd9db00e418dbad59740957849649171c",
    "semantic_title": "flashst: a simple and universal prompt-tuning framework for traffic prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l6Hef6FVd0": {
    "title": "PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning via Hindsight Relabeling",
    "volume": "poster",
    "abstract": "In this work, we introduce PIPER: Primitive-Informed Preference-based Hierarchical reinforcement learning via Hindsight Relabeling, a novel approach that leverages preference-based learning to learn a reward model, and subsequently uses this reward model to relabel higher-level replay buffers. Since this reward is unaffected by lower primitive behavior, our relabeling-based approach is able to mitigate non-stationarity, which is common in existing hierarchical approaches, and demonstrates impressive performance across a range of challenging sparse-reward tasks. Since obtaining human feedback is typically impractical, we propose to replace the human-in-the-loop approach with our primitive-in-the-loop approach, which generates feedback using sparse rewards provided by the environment. Moreover, in order to prevent infeasible subgoal prediction and avoid degenerate solutions, we propose primitive-informed regularization that conditions higher-level policies to generate feasible subgoals for lower-level policies. We perform extensive experiments to show that PIPER mitigates non-stationarity in hierarchical reinforcement learning and achieves greater than 50$\\\\%$ success rates in challenging, sparse-reward robotic environments, where most other baselines fail to achieve any significant progress",
    "checked": true,
    "id": "5b3a3f599674d7bba97f5120bd8e5737be757333",
    "semantic_title": "piper: primitive-informed preference-based hierarchical reinforcement learning via hindsight relabeling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MrNq6rbcUi": {
    "title": "Robust Yet Efficient Conformal Prediction Sets",
    "volume": "poster",
    "abstract": "Conformal prediction (CP) can convert any model's output into prediction sets guaranteed to include the true label with any user-specified probability. However, same as the model itself, CP is vulnerable to adversarial test examples (evasion) and perturbed calibration data (poisoning). We derive provably robust sets by bounding the worst-case change in conformity scores. Our tighter bounds lead to more efficient sets. We cover both continuous and discrete (sparse) data and our guarantees work both for evasion and poisoning attacks (on both features and labels)",
    "checked": false,
    "id": "f9bcd3ec17e5d9a27bf509508ca7ddd4aea7e3ba",
    "semantic_title": "communication-efficient conformal prediction for distributed datasets",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RvwMTDYTOb": {
    "title": "Finite Smoothing Algorithm for High-Dimensional Support Vector Machines and Quantile Regression",
    "volume": "poster",
    "abstract": "This paper introduces a finite smoothing algorithm (FSA), a novel approach to tackle computational challenges in applying support vector machines (SVM) and quantile regression to high-dimensional data. The critical issue with these methods is the non-smooth nature of their loss functions, which traditionally limits the use of highly efficient coordinate descent techniques in high-dimensional settings. FSA innovatively addresses this issue by transforming these loss functions into their smooth counterparts, thereby facilitating more efficient computation. A distinctive feature of FSA is its theoretical foundation: FSA can yield exact solutions, not just approximations, despite the smoothing approach. Our simulation and benchmark tests demonstrate that FSA significantly outpaces its competitors in speed, often by orders of magnitude, while improving or at least maintaining precision. We have implemented FSA in two open-source R packages: hdsvm for high-dimensional SVM and hdqr for high-dimensional quantile regression",
    "checked": false,
    "id": "8cde92023b267ed8490ce125795a3979cdfd624f",
    "semantic_title": "program yes workshop 2022 optimal transport, statistics, machine learning and moving in between",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fNJbcxhxRj": {
    "title": "Scale-Free Image Keypoints Using Differentiable Persistent Homology",
    "volume": "poster",
    "abstract": "In computer vision, keypoint detection is a fundamental task, with applications spanning from robotics to image retrieval; however, existing learning-based methods suffer from scale dependency, and lack flexibility. This paper introduces a novel approach that leverages Morse theory and persistent homology, powerful tools rooted in algebraic topology. We propose a novel loss function based on the recent introduction of a notion of subgradient in persistent homology, paving the way towards topological learning. Our detector, MorseDet, is the first topology-based learning model for feature detection, which achieves competitive performance in keypoint repeatability and introduces a principled and theoretically robust approach to the problem",
    "checked": true,
    "id": "1910e6f1fac8d886dbf65cb9d9daf26b14f2b24e",
    "semantic_title": "scale-free image keypoints using differentiable persistent homology",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4dO5L6eCp": {
    "title": "Smooth Tchebycheff Scalarization for Multi-Objective Optimization",
    "volume": "poster",
    "abstract": "Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a lightweight and efficient smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on various real-world application problems fully demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "35ef8fff5c3d167b884a218f74c45c20938f100a",
    "semantic_title": "smooth tchebycheff scalarization for multi-objective optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=tu5fCCuua2": {
    "title": "DNCs Require More Planning Steps",
    "volume": "poster",
    "abstract": "Many recent works use machine learning models to solve various complex algorithmic problems. However, these models attempt to reach a solution without considering the problem's required computational complexity, which can be detrimental to their ability to solve it correctly. In this work we investigate the effect of computational time and memory on generalization of implicit algorithmic solvers. To do so, we focus on the Differentiable Neural Computer (DNC), a general problem solver that also lets us reason directly about its usage of time and memory. In this work, we argue that the number of planning steps the model is allowed to take, which we call \"planning budget\", is a constraint that can cause the model to generalize poorly and hurt its ability to fully utilize its external memory. We evaluate our method on Graph Shortest Path, Convex Hull, Graph MinCut and Associative Recall, and show how the planning budget can drastically change the behavior of the learned algorithm, in terms of learned time complexity, training time, stability and generalization to inputs larger than those seen during training",
    "checked": true,
    "id": "08495e9cd314b0e4518038590c5ec5cb3ee81f65",
    "semantic_title": "dncs require more planning steps",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dFEeI51O5j": {
    "title": "Self-Supervised Interpretable End-to-End Learning via Latent Functional Modularity",
    "volume": "poster",
    "abstract": "We introduce MoNet, a novel functionally modular network for self-supervised and interpretable end-to-end learning. By leveraging its functional modularity with a latent-guided contrastive loss function, MoNet efficiently learns task-specific decision-making processes in latent space without requiring task-level supervision. Moreover, our method incorporates an online, post-hoc explainability approach that enhances the interpretability of end-to-end inferences without compromising sensorimotor control performance. In real-world indoor environments, MoNet demonstrates effective visual autonomous navigation, outperforming baseline models by 7% to 28% in task specificity analysis. We further explore the interpretability of our network through post-hoc analysis of perceptual saliency maps and latent decision vectors. This provides valuable insights into the incorporation of explainable artificial intelligence into robotic learning, encompassing both perceptual and behavioral perspectives. Supplementary materials are available at https://sites.google.com/view/monet-lgc",
    "checked": true,
    "id": "fb07c15ebcc21def11683bece6370d2aaef8bbf7",
    "semantic_title": "self-supervised interpretable end-to-end learning via latent functional modularity",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=3AuoStfUIH": {
    "title": "Offline Multi-Objective Optimization",
    "volume": "poster",
    "abstract": "Offline optimization aims to maximize a black-box objective function with a static dataset and has wide applications. In addition to the objective function being black-box and expensive to evaluate, numerous complex real-world problems entail optimizing multiple conflicting objectives, i.e., multi-objective optimization (MOO). Nevertheless, offline MOO has not progressed as much as offline single-objective optimization (SOO), mainly due to the lack of benchmarks like Design-Bench for SOO. To bridge this gap, we propose a first benchmark for offline MOO, covering a range of problems from synthetic to real-world tasks. This benchmark provides tasks, datasets, and open-source examples, which can serve as a foundation for method comparisons and advancements in offline MOO. Furthermore, we analyze how the current related methods can be adapted to offline MOO from four fundamental perspectives, including data, model architecture, learning algorithm, and search algorithm. Empirical results show improvements over the best value of the training set, demonstrating the effectiveness of offline MOO methods. As no particular method stands out significantly, there is still an open challenge in further enhancing the effectiveness of offline MOO. We finally discuss future challenges for offline MOO, with the hope of shedding some light on this emerging field. Our code is available at https://github.com/lamda-bbo/offline-moo",
    "checked": true,
    "id": "16243a0b41aa8a757b97a1dbb4ad0df8b2ff9bc5",
    "semantic_title": "offline multi-objective optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yug1IEkvcb": {
    "title": "Model-Free Robust $\\phi$-Divergence Reinforcement Learning Using Both Offline and Online Data",
    "volume": "poster",
    "abstract": "The robust $\\phi$-regularized Markov Decision Process (RRMDP) framework focuses on designing control policies that are robust against parameter uncertainties due to mismatches between the simulator (nominal) model and real-world settings. This work makes *two* important contributions. First, we propose a *model-free* algorithm called *Robust $\\phi$-regularized fitted Q-iteration* for learning an $\\epsilon$-optimal robust policy that uses only the historical data collected by rolling out a behavior policy (with *robust exploratory* requirement) on the nominal model. To the best of our knowledge, we provide the *first* unified analysis for a class of $\\phi$-divergences achieving robust optimal policies in high-dimensional systems of arbitrary large state space with general function approximation. Second, we introduce the *hybrid robust $\\phi$-regularized reinforcement learning* framework to learn an optimal robust policy using both historical data and online sampling. Towards this framework, we propose a model-free algorithm called *Hybrid robust Total-variation-regularized Q-iteration*. To the best of our knowledge, we provide the *first* improved out-of-data-distribution assumption in large-scale problems of arbitrary large state space with general function approximation under the hybrid robust $\\phi$-regularized reinforcement learning framework",
    "checked": true,
    "id": "0da8029ded1f73fd259482acdab02a27f8ca1e7c",
    "semantic_title": "model-free robust $\\phi$-divergence reinforcement learning using both offline and online data",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=DYN66IJCI9": {
    "title": "Graph Distillation with Eigenbasis Matching",
    "volume": "poster",
    "abstract": "The increasing amount of graph data places requirements on the efficient training of graph neural networks (GNNs). The emerging graph distillation (GD) tackles this challenge by distilling a small synthetic graph to replace the real large graph, ensuring GNNs trained on real and synthetic graphs exhibit comparable performance. However, existing methods rely on GNN-related information as supervision, including gradients, representations, and trajectories, which have two limitations. First, GNNs can affect the spectrum (*i.e*., eigenvalues) of the real graph, causing *spectrum bias* in the synthetic graph. Second, the variety of GNN architectures leads to the creation of different synthetic graphs, requiring *traversal* to obtain optimal performance. To tackle these issues, we propose Graph Distillation with Eigenbasis Matching (GDEM), which aligns the eigenbasis and node features of real and synthetic graphs. Meanwhile, it directly replicates the spectrum of the real graph and thus prevents the influence of GNNs. Moreover, we design a discrimination constraint to balance the effectiveness and generalization of GDEM. Theoretically, the synthetic graphs distilled by GDEM are restricted spectral approximations of the real graphs. Extensive experiments demonstrate that GDEM outperforms state-of-the-art GD methods with powerful cross-architecture generalization ability and significant distillation efficiency. Our code is available at https://github.com/liuyang-tian/GDEM",
    "checked": true,
    "id": "494d744d875360461134c791049ce9408c66833a",
    "semantic_title": "graph distillation with eigenbasis matching",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=uQiFsBil3p": {
    "title": "Random matrix theory improved Fréchet mean of symmetric positive definite matrices",
    "volume": "poster",
    "abstract": "In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fréchet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory based method that estimates Fréchet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods",
    "checked": true,
    "id": "27486729914f2943a343edd1c54e8741a409fb7a",
    "semantic_title": "random matrix theory improved fréchet mean of symmetric positive definite matrices",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TuALw8xVum": {
    "title": "MS$^3$D: A RG Flow-Based Regularization for GAN Training with Limited Data",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) have made impressive advances in image generation, but they often require large-scale training data to avoid degradation caused by discriminator overfitting. To tackle this issue, we investigate the challenge of training GANs with limited data, and propose a novel regularization method based on the idea of renormalization group (RG) in physics.We observe that in the limited data setting, the gradient pattern that the generator obtains from the discriminator becomes more aggregated over time. In RG context, this aggregated pattern exhibits a high discrepancy from its coarse-grained versions, which implies a high-capacity and sensitive system, prone to overfitting and collapse. To address this problem, we introduce a **m**ulti-**s**cale **s**tructural **s**elf-**d**issimilarity (MS$^3$D) regularization, which constrains the gradient field to have a consistent pattern across different scales, thereby fostering a more redundant and robust system. We show that our method can effectively enhance the performance and stability of GANs under limited data scenarios, and even allow them to generate high-quality images with very few data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ngcZhfXCBW": {
    "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
    "volume": "poster",
    "abstract": "The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as \"Don't use emojis when drafting emails to my boss.\" However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to $\\textbf{overgeneralization}$–applying feedback in contexts where it is not relevant. We propose a new method Contextualized Critiques with Constrained Preference Optimization (C3PO) to learn from high-level verbal feedback while reducing overgeneralization compared to current work. C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset to specify when and how the feedback should (and should not) be applied. It then fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the original model for prompts where the feedback does not apply. Our experimental results indicate that our approach effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts more than current methods. For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably to in-context baselines while reducing overgeneralization by 30%",
    "checked": true,
    "id": "89bc7f4df87ef36f28d048a8a9b8a7c1ac95b909",
    "semantic_title": "rlvf: learning from verbal feedback without overgeneralization",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=QAGRPiC3FS": {
    "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
    "volume": "poster",
    "abstract": "Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evaluations demonstrate that RigorLLM not only outperforms existing baselines like OpenAI API and Perspective API in detecting harmful content but also exhibits unparalleled resilience to jailbreaking attacks. The innovative use of constrained optimization and a fusion-based guardrail approach represents a significant step forward in developing more secure and reliable LLMs, setting a new standard for content moderation frameworks in the face of evolving digital threats",
    "checked": true,
    "id": "2f4cc3f4a1c70cd5aca14c1304037491cd3aeb9b",
    "semantic_title": "rigorllm: resilient guardrails for large language models against undesired content",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=ghYrfdJfjK": {
    "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels",
    "volume": "poster",
    "abstract": "The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models. Recent theoretical results indicate the intractability of sub-quadratic softmax attention approximation under reasonable complexity assumptions. This paper addresses this challenge by first demonstrating that polynomial attention with high degree can effectively replace softmax without sacrificing model quality. Next, we develop polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees. Crucially, our approach achieves this speedup without requiring the sparsification of attention matrices. We also present a block-based algorithm to apply causal masking efficiently. Combining these techniques, we provide *PolySketchFormer*, a practical linear-time Transformer architecture for language modeling that offers provable guarantees. We validate PolySketchFormer empirically by training language models capable of handling long contexts. These experiments utilize both synthetic and real-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32k and GPT-2 style models, our model achieves 2x speedup in training compared to FlashAttention of the fastest configuration, with no observed degradation in quality across our experiments",
    "checked": true,
    "id": "d9a2f89dc65b2cb7cd03cf2d37e2f67c6f72359c",
    "semantic_title": "polysketchformer: fast transformers via sketching polynomial kernels",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64I29YeQdt": {
    "title": "Quality-Diversity with Limited Resources",
    "volume": "poster",
    "abstract": "Quality-Diversity (QD) algorithms have emerged as a powerful optimization paradigm with the aim of generating a set of high-quality and diverse solutions. To achieve such a challenging goal, QD algorithms require maintaining a large archive and a large population in each iteration, which brings two main issues, sample and resource efficiency. Most advanced QD algorithms focus on improving the sample efficiency, while the resource efficiency is overlooked to some extent. Particularly, the resource overhead during the training process has not been touched yet, hindering the wider application of QD algorithms. In this paper, we highlight this important research question, i.e., how to efficiently train QD algorithms with limited resources, and propose a novel and effective method called RefQD to address it. RefQD decomposes a neural network into representation and decision parts, and shares the representation part with all decision parts in the archive to reduce the resource overhead. It also employs a series of strategies to address the mismatch issue between the old decision parts and the newly updated representation part. Experiments on different types of tasks from small to large resource consumption demonstrate the excellent performance of RefQD: it not only uses significantly fewer resources (e.g., 16% GPU memories on QDax and 3.7% on Atari) but also achieves comparable or better performance compared to sample-efficient QD algorithms. Our code is available at [https://github.com/lamda-bbo/RefQD](https://github.com/lamda-bbo/RefQD)",
    "checked": true,
    "id": "7a0e3b62c34e412e521bca814eadfe5a96cbd1ab",
    "semantic_title": "quality-diversity with limited resources",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=cMige5MK1N": {
    "title": "Accelerating Heterogeneous Federated Learning with Closed-form Classifiers",
    "volume": "poster",
    "abstract": "Federated Learning (FL) methods often struggle in highly statistically heterogeneous settings. Indeed, non-IID data distributions cause client drift and biased local solutions, particularly pronounced in the final classification layer, negatively impacting convergence speed and accuracy. To address this issue, we introduce *Federated Recursive Ridge Regression* (Fed3R). Our method fits a Ridge Regression classifier computed in closed form leveraging pre-trained features. Fed3R is immune to statistical heterogeneity and is invariant to the sampling order of the clients. Therefore, it proves particularly effective in cross-device scenarios. Furthermore, it is fast and efficient in terms of communication and computation costs, requiring up to two orders of magnitude fewer resources than the competitors. Finally, we propose to leverage the Fed3R parameters as an initialization for a softmax classifier and subsequently fine-tune the model using any FL algorithm (Fed3R with Fine-Tuning, Fed3R+FT). Our findings also indicate that maintaining a fixed classifier aids in stabilizing the training and learning more discriminative features in cross-device settings. Official website: https://fed-3r.github.io/",
    "checked": true,
    "id": "3f64d61184638e03f7e64c6dc7118d2cc52a581a",
    "semantic_title": "accelerating heterogeneous federated learning with closed-form classifiers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gp5F6qzwGK": {
    "title": "Iterative Regularized Policy Optimization with Imperfect Demonstrations",
    "volume": "poster",
    "abstract": "Imitation learning heavily relies on the quality of provided demonstrations. In scenarios where demonstrations are imperfect and rare, a prevalent approach for refining policies is through online fine-tuning with reinforcement learning, in which a Kullback–Leibler (KL) regularization is often employed to stabilize the learning process. However, our investigation reveals that on the one hand, imperfect demonstrations can bias the online learning process, the KL regularization will further constrain the improvement of online policy exploration. To address the above issues, we propose Iterative Regularized Policy Optimization (IRPO), a framework that involves iterative offline imitation learning and online reinforcement exploration. Specifically, the policy learned online is used to serve as the demonstrator for successive learning iterations, with a demonstration boosting to consistently enhance the quality of demonstrations. Experimental validations conducted across widely used benchmarks and a novel fixed-wing UAV control task consistently demonstrate the effectiveness of IRPO in improving both the demonstration quality and the policy performance. Our code is available at https://github.com/GongXudong/IRPO",
    "checked": false,
    "id": "30354c90c447b84e5cb6977b0310bd373c577e91",
    "semantic_title": "adversarial batch inverse reinforcement learning: learn to reward from imperfect demonstration for interactive recommendation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1W712hMBi": {
    "title": "NExT: Teaching Large Language Models to Reason about Code Execution",
    "volume": "poster",
    "abstract": "A fundamental skill among human developers is the ability to understand and reason about program execution. As an example, a programmer can mentally simulate code execution in natural language to debug and repair code (aka. rubber duck debugging). However, large language models (LLMs) of code are typically trained on the surface textual form of programs, thus may lack a semantic understanding of how programs execute at run-time. To address this issue, we propose NExT, a method to teach LLMs to inspect the execution traces of programs (variable states of executed lines) and reason about their run-time behavior through chain-of-thought (CoT) rationales. Specifically, NExT uses self-training to bootstrap a synthetic training set of execution-aware rationales that lead to correct task solutions (e.g., fixed programs) without laborious manual annotation. Experiments on program repair tasks based on MBPP and HumanEval demonstrate that NExT improves the fix rate of a PaLM 2 model, by 26.1% and 10.3% absolute, respectively, with significantly improved rationale quality as verified by automated metrics and human raters. Our model can also generalize to scenarios where program traces are absent at test-time",
    "checked": true,
    "id": "49306aa1fde2a21fadc77dbc8ec7e487fac72c5b",
    "semantic_title": "next: teaching large language models to reason about code execution",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FYQIgQWH3d": {
    "title": "3D Geometric Shape Assembly via Efficient Point Cloud Matching",
    "volume": "poster",
    "abstract": "Learning to assemble geometric shapes into a larger target structure is a pivotal task in various practical applications. In this work, we tackle this problem by establishing local correspondences between point clouds of part shapes in both coarse- and fine-levels. To this end, we introduce Proxy Match Transform (PMT), an approximate high-order feature transform layer that enables reliable matching between mating surfaces of parts while incurring low costs in memory and compute. Building upon PMT, we introduce a new framework, dubbed Proxy Match TransformeR (PMTR), for the geometric assembly task. We evaluate the proposed PMTR on the large-scale 3D geometric shape assembly benchmark dataset of Breaking Bad and demonstrate its superior performance and efficiency compared to state-of-the-art methods. Project page: https://nahyuklee.github.io/pmtr",
    "checked": false,
    "id": "411a4f7cbd0c9c39e9029b4968ea85a6b4f2c120",
    "semantic_title": "an efficient 3d point cloud classification approach via persistent homology",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vGHOFeUQi8": {
    "title": "Simulation-Based Inference with Quantile Regression",
    "volume": "poster",
    "abstract": "We present Neural Quantile Estimation (NQE), a novel Simulation-Based Inference (SBI) method based on conditional quantile regression. NQE autoregressively learns individual one dimensional quantiles for each posterior dimension, conditioned on the data and previous posterior dimensions. Posterior samples are obtained by interpolating the predicted quantiles using monotonic cubic Hermite spline, with specific treatment for the tail behavior and multi-modal distributions. We introduce an alternative definition for the Bayesian credible region using the local Cumulative Density Function (CDF), offering substantially faster evaluation than the traditional Highest Posterior Density Region (HPDR). In case of limited simulation budget and/or known model misspecification, a post-processing calibration step can be integrated into NQE to ensure the unbiasedness of the posterior estimation with negligible additional computational cost. We demonstrate that NQE achieves state-of-the-art performance on a variety of benchmark problems",
    "checked": true,
    "id": "faaa1db4cb92d746651b446f4a13527f9d8b1805",
    "semantic_title": "simulation-based inference with quantile regression",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=nDps3Q8j2l": {
    "title": "Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning",
    "volume": "poster",
    "abstract": "Transformer has shown promise in reinforcement learning to model time-varying features for obtaining generalized low-level robot policies on diverse robotics datasets in embodied learning. However, it still suffers from the issues of low data efficiency and high inference latency. In this paper, we propose to investigate the task from a new perspective of the frequency domain. We first observe that the energy density in the frequency domain of a robot's trajectory is mainly concentrated in the low-frequency part. Then, we present the Fourier Controller Network (FCNet), a new network that uses Short-Time Fourier Transform (STFT) to extract and encode time-varying features through frequency domain interpolation. In order to do real-time decision-making, we further adopt FFT and Sliding DFT methods in the model architecture to achieve parallel training and efficient recurrent inference. Extensive results in both simulated (e.g., D4RL) and real-world environments (e.g., robot locomotion) demonstrate FCNet's substantial efficiency and effectiveness over existing methods such as Transformer, e.g., FCNet outperforms Transformer on multi-environmental robotics datasets of all types of sizes (from 1.9M to 120M). The project page and code can be found https://thkkk.github.io/fcnet",
    "checked": true,
    "id": "4f022613f233c45e19be774a38a6a499146c396f",
    "semantic_title": "fourier controller networks for real-time decision-making in embodied learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2cXzNDe614": {
    "title": "PDHG-Unrolled Learning-to-Optimize Method for Large-Scale Linear Programming",
    "volume": "poster",
    "abstract": "Solving large-scale linear programming (LP) problems is an important task in various areas such as communication networks, power systems, finance and logistics. Recently, two distinct approaches have emerged to expedite LP solving: (i) First-order methods (FOMs); (ii) Learning to optimize (L2O). In this work, we propose an FOM-unrolled neural network (NN) called PDHG-Net, and propose a two-stage L2O method to solve large-scale LP problems. The new architecture PDHG-Net is designed by unrolling the recently emerged PDHG method into a neural network, combined with channel-expansion techniques borrowed from graph neural networks. We prove that the proposed PDHG-Net can recover PDHG algorithm, thus can approximate optimal solutions of LP instances with a polynomial number of neurons. We propose a two-stage inference approach: first use PDHG-Net to generate an approximate solution, and then apply PDHG algorithm to further improve the solution. Experiments show that our approach can significantly accelerate LP solving, achieving up to a 3$\\times$ speedup compared to FOMs for large-scale LP problems",
    "checked": true,
    "id": "d2023275305f9936231bfc1783cdba6647858fc8",
    "semantic_title": "pdhg-unrolled learning-to-optimize method for large-scale linear programming",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5kGfm3Pa41": {
    "title": "Recurrent Distance Filtering for Graph Representation Learning",
    "volume": "poster",
    "abstract": "Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing the information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but lack graph inductive bias and have to rely on ad-hoc positional encoding. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a linear RNN to encode the sequence of hop representations. The linear RNN is parameterized in a particular diagonal form for stable long-range signal propagation and is theoretically expressive enough to encode the neighborhood hierarchy. With no need for positional encoding, we empirically show that the performance of our model is comparable to or better than that of state-of-the-art graph transformers on various benchmarks, with a significantly reduced computational cost. Our code is open-source at https://github.com/skeletondyh/GRED",
    "checked": true,
    "id": "92f38084de7ed74305ce7b9c3a3a3f845182fd82",
    "semantic_title": "recurrent distance filtering for graph representation learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9U29U3cDKq": {
    "title": "Adaptively Perturbed Mirror Descent for Learning in Games",
    "volume": "poster",
    "abstract": "This paper proposes a payoff perturbation technique for the Mirror Descent (MD) algorithm in games where the gradient of the payoff functions is monotone in the strategy profile space, potentially containing additive noise. The optimistic family of learning algorithms, exemplified by optimistic MD, successfully achieves *last-iterate* convergence in scenarios devoid of noise, leading the dynamics to a Nash equilibrium. A recent re-emerging trend underscores the promise of the perturbation approach, where payoff functions are perturbed based on the distance from an anchoring, or *slingshot*, strategy. In response, we propose *Adaptively Perturbed MD* (APMD), which adjusts the magnitude of the perturbation by repeatedly updating the slingshot strategy at a predefined interval. This innovation empowers us to find a Nash equilibrium of the underlying game with guaranteed rates. Empirical demonstrations affirm that our algorithm exhibits significantly accelerated convergence",
    "checked": true,
    "id": "9601b71af3a36548386eb8f7458d6428b3343675",
    "semantic_title": "adaptively perturbed mirror descent for learning in games",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ERo4jph0A": {
    "title": "Attack-free Evaluating and Enhancing Adversarial Robustness on Categorical Data",
    "volume": "poster",
    "abstract": "Research on adversarial robustness has predominantly focused on continuous inputs, leaving categorical inputs, especially tabular attributes, less examined. To echo this challenge, our work aims to evaluate and enhance the robustness of classification over categorical attributes against adversarial perturbations through efficient attack-free approaches. We propose a robustness evaluation metric named Integrated Gradient-Smoothed Gradient (IGSG). It is designed to evaluate the attributional sensitivity of each feature and the decision boundary of the classifier, two aspects that significantly influence adversarial risk, according to our theoretical analysis. Leveraging this metric, we develop an IGSG-based regularization to reduce adversarial risk by suppressing the sensitivity of categorical attributes. We conduct extensive empirical studies over categorical datasets of various application domains. The results affirm the efficacy of both IGSG and IGSG-based regularization. Notably, IGSG-based regularization surpasses the state-of-the-art robust training methods by a margin of approximately 0.4% to 12.2% on average in terms of adversarial accuracy, especially on high-dimension datasets. The code is available at https://github.com/YujunZhou/IGSG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5kXNMDpUVF": {
    "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization",
    "volume": "poster",
    "abstract": "An open problem in differentially private deep learning is hyperparameter optimization (HPO). DP-SGD introduces new hyperparameters and complicates existing ones, forcing researchers to painstakingly tune hyperparameters with hundreds of trials, which in turn makes it impossible to account for the privacy cost of HPO without destroying the utility. We propose an adaptive HPO method that uses cheap trials (in terms of privacy cost and runtime) to estimate optimal hyperparameters and scales them up. We obtain state-of-the-art performance on 22 benchmark tasks, across computer vision and natural language processing, across pretraining and finetuning, across architectures and a wide range of $\\varepsilon \\in [0.01,8.0]$, all while accounting for the privacy cost of HPO",
    "checked": true,
    "id": "1d589e172084a1451398f0d6c30592baa8224732",
    "semantic_title": "a new linear scaling rule for private adaptive hyperparameter optimization",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=JNN6QHhLHB": {
    "title": "Measuring Stochastic Data Complexity with Boltzmann Influence Functions",
    "volume": "poster",
    "abstract": "Estimating the uncertainty of a model's prediction on a test point is a crucial part of ensuring reliability and calibration under distribution shifts.A minimum description length approach to this problem uses the predictive normalized maximum likelihood (pNML) distribution, which considers every possible label for a data point, and decreases confidence in a prediction if other labels are also consistent with the model and training data. In this work we propose IF-COMP, a scalable and efficient approximation of the pNML distribution that linearizes the model with a temperature-scaled Boltzmann influence function. IF-COMP can be used to produce well-calibrated predictions on test points as well as measure complexity in both labelled and unlabelled settings. We experimentally validate IF-COMP on uncertainty calibration, mislabel detection, and OOD detection tasks, where it consistently matches or beats strong baseline methods",
    "checked": true,
    "id": "80e2f538f4696d618af0856c23ed9e79126295fc",
    "semantic_title": "measuring stochastic data complexity with boltzmann influence functions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=17ZwoHl65h": {
    "title": "PlanDQ: Hierarchical Plan Orchestration via D-Conductor and Q-Performer",
    "volume": "poster",
    "abstract": "Despite the recent advancements in offline RL, no unified algorithm could achieve superior performance across a broad range of tasks. Offline *value function learning*, in particular, struggles with sparse-reward, long-horizon tasks due to the difficulty of solving credit assignment and extrapolation errors that accumulates as the horizon of the task grows. On the other hand, models that can perform well in long-horizon tasks are designed specifically for goal-conditioned tasks, which commonly perform worse than value function learning methods on short-horizon, dense-reward scenarios. To bridge this gap, we propose a hierarchical planner designed for offline RL called PlanDQ. PlanDQ incorporates a diffusion-based planner at the high level, named D-Conductor, which guides the low-level policy through sub-goals. At the low level, we used a Q-learning based approach called the Q-Performer to accomplish these sub-goals. Our experimental results suggest that PlanDQ can achieve superior or competitive performance on D4RL continuous control benchmark tasks as well as AntMaze, Kitchen, and Calvin as long-horizon tasks",
    "checked": true,
    "id": "44e49dd16c7b7a368f24aeafc1d2435ea572c0f8",
    "semantic_title": "plandq: hierarchical plan orchestration via d-conductor and q-performer",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ia5XvxFUJT": {
    "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
    "volume": "poster",
    "abstract": "Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FlashLinearAttention, is faster than FlashAttention-2 as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model",
    "checked": true,
    "id": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
    "semantic_title": "gated linear attention transformers with hardware-efficient training",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=FQQ4476dT2": {
    "title": "FightLadder: A Benchmark for Competitive Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "Recent advances in reinforcement learning (RL) heavily rely on a variety of well-designed benchmarks, which provide environmental platforms and consistent criteria to evaluate existing and novel algorithms. Specifically, in multi-agent RL (MARL), a plethora of benchmarks based on cooperative games have spurred the development of algorithms that improve the scalability of cooperative multi-agent systems. However, for the competitive setting, a lightweight and open-sourced benchmark with challenging gaming dynamics and visual inputs has not yet been established. In this work, we present FightLadder, a real-time fighting game platform, to empower competitive MARL research. Along with the platform, we provide implementations of state-of-the-art MARL algorithms for competitive games, as well as a set of evaluation metrics to characterize the performance and exploitability of agents. We demonstrate the feasibility of this platform by training a general agent that consistently defeats 12 built-in characters in single-player mode, and expose the difficulty of training a non-exploitable agent without human knowledge and demonstrations in two-player mode. FightLadder provides meticulously designed environments to address critical challenges in competitive MARL research, aiming to catalyze a new era of discovery and advancement in the field. Videos and code at https://sites.google.com/view/fightladder/home",
    "checked": true,
    "id": "234eaa6bc08a2536f443e4a85dbd929150a2c17c",
    "semantic_title": "fightladder: a benchmark for competitive multi-agent reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0LoolDFw4": {
    "title": "A Language Model's Guide Through Latent Space",
    "volume": "poster",
    "abstract": "Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time. While the focus of previous work has largely been on *truthfulness*, in this paper we extend this framework to a richer set of concepts such as *appropriateness*, *humor*, *creativity* and *quality*, and explore to what degree current detection and guidance strategies work in these challenging settings. To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model. Our extensive experiments reveal that while some concepts such as *truthfulness* more easily allow for guidance with current techniques, novel concepts such as *appropriateness* or *humor* either remain difficult to elicit, need extensive tuning to work, or even experience confusion. Moreover, we find that probes with optimal detection accuracies do not necessarily make for the optimal guides, contradicting previous observations for *truthfulness*. Our work warrants a deeper investigation into the interplay between detectability, guidability, and the nature of the concept, and we hope that our rich experimental test-bed for guidance research inspires stronger follow-up approaches",
    "checked": true,
    "id": "405daa547e62fd5a0d0c69e06908324f3bc74893",
    "semantic_title": "a language model's guide through latent space",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=o8AaRKbP9K": {
    "title": "Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?",
    "volume": "poster",
    "abstract": "Transformers to do reasoning and few-shot learning, without any fine-tuning, is widely conjectured to stem from their ability to implicitly simulate a multi-step algorithms -- such as gradient descent -- with their weights in a single forward pass. Recently, there has been progress in understanding this complex phenomenon from an expressivity point of view, by demonstrating that Transformers can express such multi-step algorithms. However, our knowledge about the more fundamental aspect of its learnability, beyond single layer models, is very limited. In particular, *can training Transformers enable convergence to algorithmic solutions*? In this work we resolve this for in context linear regression with linear looped Transformers -- a multi-layer model with weight sharing that is conjectured to have an inductive bias to learn fix-point iterative algorithms. More specifically, for this setting we show that the global minimizer of the population training loss implements multi-step preconditioned gradient descent, with a preconditioner that adapts to the data distribution. Furthermore, we show a fast convergence for gradient flow on the regression loss, despite the non-convexity of the landscape, by proving a novel gradient dominance condition. To our knowledge, this is the first theoretical analysis for multi-layer Transformer in this setting. We further validate our theoretical findings through synthetic experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L0VoOdjCUb": {
    "title": "Learning with 3D rotations, a hitchhiker's guide to SO(3)",
    "volume": "poster",
    "abstract": "Many settings in machine learning require the selection of a rotation representation. However, choosing a suitable representation from the many available options is challenging. This paper acts as a survey and guide through rotation representations. We walk through their properties that harm or benefit deep learning with gradient-based optimization. By consolidating insights from rotation-based learning, we provide a comprehensive overview of learning functions with rotation representations. We provide guidance on selecting representations based on whether rotations are in the model's input or output and whether the data primarily comprises small angles",
    "checked": true,
    "id": "68f88ce04c262341894bd785da2ff9921e969a08",
    "semantic_title": "learning with 3d rotations, a hitchhiker's guide to so(3)",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GqsRKEhelH": {
    "title": "Indirectly Parameterized Concrete Autoencoders",
    "volume": "poster",
    "abstract": "Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete Autoencoders (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions' parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classification. Unlike CAE, IP-CAE effectively leverages non-linear relationships and does not require retraining the jointly optimized decoder. Furthermore, our approach is, in principle, generalizable to Gumbel-Softmax distributions beyond feature selection",
    "checked": true,
    "id": "0b3c957c02d0f86e85c9a4eb78d44e6add9d5665",
    "semantic_title": "indirectly parameterized concrete autoencoders",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7uwLvFvpis": {
    "title": "Vector Quantization Pretraining for EEG Time Series with Random Projection and Phase Alignment",
    "volume": "poster",
    "abstract": "In this paper, we propose a BERT-style self-supervised learning model, VQ-MTM (Vector Quantization Masked Time-Series Modeling), for the EEG time series data analysis. At its core, VQ-MTM comprises a theoretically grounded random-projection quantization module and a phase-aligning module guided by the Time-Phase-Shift Equivariance of Fourier Transform, the two modules can generate well-defined semantic units (akin to words in natural language) for the corrupted and periodic time series, thus offering robust and consistent learning signals for the EEG self-supervised learning. VQ-MTM also owns low model complexity and can easily adapt to large-scale datasets. We conduct experiments on five real-world datasets including two large-scale datasets to verify the efficacy of our proposed model, the experiment results show that VQ-MTM is able to consistently surpass the existing methods by large margins on both seizure detection and classification tasks. Our code is available at https://github.com/HaokunGUI/VQ_MTM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmljUnbjy6": {
    "title": "Unsupervised Parameter-free Simplicial Representation Learning with Scattering Transforms",
    "volume": "poster",
    "abstract": "Simplicial neural network models are becoming popular for processing and analyzing higher-order graph data, but they suffer from high training complexity and dependence on task-specific labels. To address these challenges, we propose simplicial scattering networks (SSNs), a parameter-free model inspired by scattering transforms designed to extract task-agnostic features from simplicial complex data without labels in a principled manner. Specifically, we propose a simplicial scattering transform based on random walk matrices for various adjacencies underlying a simplicial complex. We then use the simplicial scattering transform to construct a deep filter bank network that captures high-frequency information at multiple scales. The proposed simplicial scattering transform possesses properties such as permutation invariance, robustness to perturbations, and expressivity. We theoretically prove that including higher-order information improves the robustness of SSNs to perturbations. Empirical evaluations demonstrate that SSNs outperform existing simplicial or graph neural models in many tasks like node classification, simplicial closure, graph classification, trajectory prediction, and simplex prediction while being computationally efficient",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MWZWUyfFHC": {
    "title": "TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge",
    "volume": "poster",
    "abstract": "On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCUs), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time ($\\textit{e.g.}$ a few hours), or induce substantial accuracy loss ($\\geq$10%). In this paper, we propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that $\\textit{dynamically}$ selects the layer/channel to update based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0% in accuracy, while reducing the backward-pass memory and computation cost by up to 1,098$\\times$ and 7.68$\\times$, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5$\\times$ faster and 3.5$\\times$ more energy-efficient training over status-quo approaches, and 2.23$\\times$ smaller memory footprint than SOTA methods, while remaining within the 1 MB memory envelope of MCU-grade platforms",
    "checked": true,
    "id": "251be075c1743eba35b370ab6595f7b9a233e432",
    "semantic_title": "tinytrain: resource-aware task-adaptive sparse training of dnns at the data-scarce edge",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NeotatlYOL": {
    "title": "SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN",
    "volume": "poster",
    "abstract": "Spiking neural network (SNN) has attracted great attention due to its characteristic of high efficiency and accuracy. Currently, the ANN-to-SNN conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8 time-steps) in CNN structure on computer vision (CV) tasks. However, as Transformer-based networks have achieved prevailing precision on both CV and natural language processing (NLP), the Transformer-based SNNs are still encounting the lower accuracy w.r.t the ANN counterparts. In this work, we introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN and SNN are exactly equivalent, thus incurring no accuracy degradation. SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79% accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based SNNs. The code is available in GitHub: https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer",
    "checked": true,
    "id": "c0e12df9837da93ba4ff0786a832cb30710c1039",
    "semantic_title": "spikezip-tf: conversion is all you need for transformer-based snn",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ZkUFSwlUH": {
    "title": "Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose Diverse Skill Learning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective. We show on challenging robot simulation tasks that Di-SkilL can learn diverse and performant skills",
    "checked": true,
    "id": "dd483e4956eafba4bc82af26a29e0075a696f788",
    "semantic_title": "acquiring diverse skills using curriculum reinforcement learning with mixture of experts",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Rp8R9C0Sth": {
    "title": "AutoOS: Make Your OS More Powerful by Exploiting Large Language Models",
    "volume": "poster",
    "abstract": "With the rapid development of Artificial Intelligence of Things (AIoT), customizing and optimizing operating system (OS) kernel configurations for various AIoT application scenarios is crucial for maximizing system performance. However, existing approaches falter due to the overwhelming problem complexity (i.e., over 15,000 configuration options in the Linux kernel), together with the huge evaluation costs and error-prone options that may result in OS boot-up failure, which all make it an unresolved problem to optimize the Linux kernel automatically. In this paper, we introduce AutoOS, a novel framework exploiting Large Language Models for customizing and optimizing OS kernel configurations automatically for various AIoT application scenarios.Inspired by the inherently directory-structured kernel configuration process, we first formulate our research problem as optimizing on a dynamic tree. We then propose a novel framework integrating a state machine-based traversal algorithm as the observe-prune-propose-act-correct loop, which can effectively refine the optimization space and ensure a successful OS boot-up.Experimental results show that AutoOS can automatically customize and optimize the OS kernel configurations without human effort. More importantly, AutoOS even achieves better performance by up to 25% than vendor-provided configuration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efzkSbpyRw": {
    "title": "Conformal Predictions under Markovian Data",
    "volume": "poster",
    "abstract": "We study the split Conformal Prediction method when applied to Markovian data. We quantify the gap in terms of coverage induced by the correlations in the data (compared to exchangeable data). This gap strongly depends on the mixing properties of the underlying Markov chain, and we prove that it typically scales as $\\sqrt{t_\\mathrm{mix}\\ln(n)/n}$ (where $t_\\mathrm{mix}$ is the mixing time of the chain). We also derive upper bounds on the impact of the correlations on the size of the prediction set. Finally we present $K$-split CP, a method that consists in thinning the calibration dataset and that adapts to the mixing properties of the chain. Its coverage gap is reduced to $t_\\mathrm{mix}/(n\\ln(n))$ without really affecting the size of the prediction set. We finally test our algorithms on synthetic and real-world datasets",
    "checked": false,
    "id": "7a65e919fa4b2e0d03273935a4ff413a993764f4",
    "semantic_title": "conformal prediction under ambiguous ground truth",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=hKdJPMQvew": {
    "title": "Hyperbolic Active Learning for Semantic Segmentation under Domain Shift",
    "volume": "poster",
    "abstract": "We introduce a hyperbolic neural network approach to pixel-level active learning for semantic segmentation. Analysis of the data statistics leads to a novel interpretation of the hyperbolic radius as an indicator of data scarcity. In HALO (Hyperbolic Active Learning Optimization), for the first time, we propose the use of epistemic uncertainty as a data acquisition strategy, following the intuition of selecting data points that are the least known. The hyperbolic radius, complemented by the widely-adopted prediction entropy, effectively approximates epistemic uncertainty. We perform extensive experimental analysis based on two established synthetic-to-real benchmarks, i.e. GTAV $\\rightarrow$ Cityscapes and SYNTHIA $\\rightarrow$ Cityscapes. Additionally, we test HALO on Cityscape $\\rightarrow$ ACDC for domain adaptation under adverse weather conditions, and we benchmark both convolutional and attention-based backbones. HALO sets a new state-of-the-art in active learning for semantic segmentation under domain shift and it is the first active learning approach that surpasses the performance of supervised domain adaptation while using only a small portion of labels (i.e., 1%)",
    "checked": true,
    "id": "070f2f21290e2b43ee0dc5dedd74b3f8259896c7",
    "semantic_title": "hyperbolic active learning for semantic segmentation under domain shift",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=a9bzTv9SzO": {
    "title": "Rolling Diffusion Models",
    "volume": "poster",
    "abstract": "Diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics simulations, or climate data. These methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. This paper explores Rolling Diffusion: a new approach that uses a sliding window denoising process. It ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. Empirically, we show that when the temporal dynamics are complex, Rolling Diffusion is superior to standard diffusion. In particular, this result is demonstrated in a video prediction task using the Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment",
    "checked": true,
    "id": "8d9218a3504242959b6b894700beeda7cd26273b",
    "semantic_title": "rolling diffusion models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=OS0szhkPmF": {
    "title": "Disentangled Graph Self-supervised Learning for Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "Graph out-of-distribution (OOD) generalization, aiming to generalize graph neural networks (GNNs) under distribution shifts between training and testing environments, has attracted ever-increasing attention recently. However, existing literature heavily relies on sufficient task-dependent graph labels, which are often scarce or even unavailable, limiting their applications in real-world scenarios. In this paper, we study the self-supervised graph OOD generalization problem, i.e., learning GNNs capable of achieving relatively stable performances under distribution shifts without graph labels. However, the problem remains largely unexplored, with the critical challenge that the invariant and variant information are highly entangled in graphs. To solve this problem, we propose an OOD generalized disentangled graph contrastive learning model (OOD-GCL), which is capable of learning disentangled graph-level representations with self-supervision that can handle distribution shifts between training and testing graph data. Specifically, we first introduce a disentangled graph encoder to map each input graph into the factorized graph representation. Then we propose a tailored disentangled invariant self-supervised learning module to maximize predictive ability of the representations and make sure the representations other than from one specific channel are invariant to the environments partitioned by this latent factor for excluding the information corresponding to this latent factor for disentanglement. Finally, the disentangled graph representations are fed into a linear predictor and finetuned for the downstream tasks. We provide comprehensive theoretical analyses to show that our model can learn disentangled graph representations and achieve OOD generalization. Extensive experiments on real-world datasets demonstrate the superiority of our model against state-of-the-art baselines under distribution shifts for graph classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VyoY3Wh9Wd": {
    "title": "In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization",
    "volume": "poster",
    "abstract": "With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations. However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters. In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass. Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work. Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work",
    "checked": true,
    "id": "2ef9ee354cfedebe497b4614926b733d10a23693",
    "semantic_title": "in-context freeze-thaw bayesian optimization for hyperparameter optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=st2BTty53v": {
    "title": "Transferable Facial Privacy Protection against Blind Face Restoration via Domain-Consistent Adversarial Obfuscation",
    "volume": "poster",
    "abstract": "With the rise of social media and the proliferation of facial recognition surveillance, concerns surrounding privacy have escalated significantly. While numerous studies have concentrated on safeguarding users against unauthorized face recognition, a new and often overlooked issue has emerged due to advances in facial restoration techniques: traditional methods of facial obfuscation may no longer provide a secure shield, as they can potentially expose anonymous information to human perception. Our empirical study shows that blind face restoration (BFR) models can restore obfuscated faces with high probability by simply retraining them on obfuscated (e.g., pixelated) faces. To address it, we propose a transferable adversarial obfuscation method for privacy protection against BFR models. Specifically, we observed a common characteristic among BFR models, namely, their capability to approximate an inverse mapping of a transformation from a high-quality image domain to a low-quality image domain. Leveraging this shared model attribute, we have developed a domain-consistent adversarial method for generating obfuscated images. In essence, our method is designed to minimize overfitting to surrogate models during the perturbation generation process, thereby enhancing the generalization of adversarial obfuscated facial images. Extensive experiments on various BFR models demonstrate the effectiveness and transferability of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bq2THeNXRr": {
    "title": "Selecting Large Language Model to Fine-tune via Rectified Scaling Law",
    "volume": "poster",
    "abstract": "The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known \"power phase\" but also the previously unobserved \"pre-power phase\". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of \"pre-learned data size\" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at rectified-scaling-law.github.io",
    "checked": true,
    "id": "2b5e40c9c6c76569714b902a53838cb80ce89a26",
    "semantic_title": "selecting large language model to fine-tune via rectified scaling law",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=YNbCbcGyXE": {
    "title": "What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks",
    "volume": "poster",
    "abstract": "We study the capabilities of the transformer architecture with varying depth. Specifically, we designed a novel set of sequence learning tasks to systematically evaluate and comprehend how the depth of transformer affects its ability to perform memorization, reasoning, generalization, and contextual generalization. We show a transformer with only one attention layer can excel in memorization but falls short in other tasks. Then, we show that exhibiting reasoning and generalization ability requires the transformer to have at least two attention layers, while context generalization ability may necessitate three attention layers. Additionally, we identify a class of simple operations that a single attention layer can execute, and show that the complex tasks can be approached as the combinations of these simple operations and thus can be resolved by stacking multiple attention layers. This sheds light on studying more practical and complex tasks beyond our design. Numerical experiments corroborate our theoretical findings",
    "checked": true,
    "id": "ebbd118d7f6383f8a08eb6ea6fbfd24a19639505",
    "semantic_title": "what can transformer learn with varying depth? case studies on sequence learning tasks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CgO2cuWWLV": {
    "title": "Provable Interactive Learning with Hindsight Instruction Feedback",
    "volume": "poster",
    "abstract": "We study interactive learning in a setting where the agent has to generate a response (e.g., an action or trajectory) given a context and an instruction. In contrast, to typical approaches that train the system using reward or expert supervision on response, we study _learning with hindsight labeling_ where a teacher provides an instruction that is most suitable for the agent's generated response. This hindsight labeling of instruction is often easier to provide than providing expert supervision of the optimal response which may require expert knowledge or can be impractical to elicit. We initiate the theoretical analysis of _interactive learning with hindsight labeling_. We first provide a lower bound showing that in general, the regret of any algorithm must scale with the size of the agent's response space. Next, we study a specialized setting where the underlying instruction-response distribution can be decomposed as a low-rank matrix. We introduce an algorithm called LORIL for this setting and show that it is a no-regret algorithm with the regret scaling with $\\sqrt{T}$ and depends on the _intrinsic rank_ but does not depend on the agent's response space. We provide experiments showing the performance of LORIL in practice for 2 domains",
    "checked": true,
    "id": "af649b7481dc27f88ca1f98193e9f73110a6bffe",
    "semantic_title": "provable interactive learning with hindsight instruction feedback",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0ksNeD1SJT": {
    "title": "Scaling Exponents Across Parameterizations and Optimizers",
    "volume": "poster",
    "abstract": "Robust and effective scaling of models from small to large width typically requires the precise adjustment of many algorithmic and architectural details, such as parameterization and optimizer choices. In this work, we propose a new perspective on parameterization by investigating a key assumption in prior work about the alignment between parameters and data and derive new theoretical results under weaker assumptions and a broader set of optimizers. Our extensive empirical investigation includes *tens of thousands* of models trained with *all combinations of* three optimizers, four parameterizations, several alignment assumptions, more than a dozen learning rates, and fourteen model sizes up to 27B parameters. We find that the best learning rate scaling prescription would often have been excluded by the assumptions in prior work. Our results show that all parameterizations, not just maximal update parameterization (muP), can achieve hyperparameter transfer; moreover, our novel per-layer learning rate prescription for standard parameterization outperforms muP. Finally, we demonstrate that an overlooked aspect of parameterization, the epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow and propose *Adam-atan2*, a new numerically stable, scale-invariant version of Adam that eliminates the epsilon hyperparameter entirely",
    "checked": false,
    "id": "517910960a8a06e4d1c2b28287b3962e1c69ffb3",
    "semantic_title": "investigation of airfoil parameterizations and optimization for rotor broadband noise reduction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7tyAO5tUF8": {
    "title": "Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs",
    "volume": "poster",
    "abstract": "The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields. This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices. However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept. This phenomenon leads to instability and inefficiency when training poses are sparse. In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based MLP network known for strong bias toward low-frequency signals. The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. We demonstrate that using residual connections between them seamlessly preserves their own inherent properties. Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. We demonstrate empirically that our proposed method not only outperforms baseline models for both static and dynamic NeRFs with sparse inputs, but also achieves comparable results with fewer parameters",
    "checked": true,
    "id": "7f47c8cf6965ed60abcebd2f9b8e2cf52ab92a05",
    "semantic_title": "synergistic integration of coordinate network and tensorial feature for improving neural radiance fields from sparse inputs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t82Y3fmRtk": {
    "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
    "volume": "poster",
    "abstract": "In this paper, we propose **R**$^3$: Learning **R**easoning through **R**everse Curriculum **R**einforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. **R**$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, **R**$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, **R**$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notably, in program-based reasoning, 7B-scale models perform comparably to larger models or closed-source models with our **R**$^3$",
    "checked": true,
    "id": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9",
    "semantic_title": "training large language models for reasoning through reverse curriculum reinforcement learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=DWT9uiGjxT": {
    "title": "Localizing Task Information for Improved Model Merging and Compression",
    "volume": "poster",
    "abstract": "Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss. Previous works have linked these drops to interference in the weight space and erasure of important task-specific features. Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights. We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve >99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints. We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion. For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches. Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches. Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance",
    "checked": true,
    "id": "8b9f3344e585ebe14b3ed930ae337d4d84f50d27",
    "semantic_title": "localizing task information for improved model merging and compression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FSxTEvuFa7": {
    "title": "CarbonNovo: Joint Design of Protein Structure and Sequence Using a Unified Energy-based Model",
    "volume": "poster",
    "abstract": "De novo protein design aims to create novel protein structures and sequences unseen in nature. Recent structure-oriented design methods typically employ a two-stage strategy, where structure design and sequence design modules are trained separately, and the backbone structures and sequences are generated sequentially in inference. While diffusion-based generative models like RFdiffusion show great promise in structure design, they face inherent limitations within the two-stage framework. First, the sequence design module risks overfitting, as the accuracy of the generated structures may not align with that of the crystal structures used for training. Second, the sequence design module lacks interaction with the structure design module to further optimize the generated structures. To address these challenges, we propose CarbonNovo, a unified energy-based model for jointly generating protein structure and sequence. Specifically, we leverage a score-based generative model and Markov Random Fields for describing the energy landscape of protein structure and sequence. In CarbonNovo, the structure and sequence design module communicates at each diffusion step, encouraging the generation of more coherent structure-sequence pairs. Moreover, the unified framework allows for incorporating the protein language models as evolutionary constraints for generated proteins. The rigorous evaluation demonstrates that CarbonNovo outperforms two-stage methods across various metrics, including designability, novelty, sequence plausibility, and Rosetta Energy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M4Htd52HMH": {
    "title": "Embodied CoT Distillation From LLM To Off-the-shelf Agents",
    "volume": "poster",
    "abstract": "We address the challenge of utilizing large language models (LLMs) for complex embodied tasks, in the environment where decision-making systems operate timely on capacity-limited, off-the-shelf devices. We present DeDer, a framework for decomposing and distilling the embodied reasoning capabilities from LLMs to efficient, small language model (sLM)-based policies. In DeDer, the decision-making process of LLM-based strategies is restructured into a hierarchy with a reasoning-policy and planning-policy. The reasoning-policy is distilled from the data that is generated through the embodied in-context learning and self-verification of an LLM, so it can produce effective rationales. The planning-policy, guided by the rationales, can render optimized plans efficiently. In turn, DeDer allows for adopting sLMs for both policies, deployed on off-the-shelf devices. Furthermore, to enhance the quality of intermediate rationales, specific to embodied tasks, we devise the embodied knowledge graph, and to generate multiple rationales timely through a single inference, we also use the contrastively prompted attention model. Our experiments with the ALFRED benchmark demonstrate that DeDer surpasses leading language planning and distillation approaches, indicating the applicability and efficiency of sLM-based embodied policies derived through DeDer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sF9epWkNUG": {
    "title": "Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations",
    "volume": "poster",
    "abstract": "Transformer models are increasingly used for solving Partial Differential Equations (PDEs). Several adaptations have been proposed, all of which suffer from the typical problems of Transformers, such as quadratic memory and time complexity. Furthermore, all prevalent architectures for PDE solving lack at least one of several desirable properties of an ideal surrogate model, such as (i) generalization to PDE parameters not seen during training, (ii) spatial and temporal zero-shot super-resolution, (iii) continuous temporal extrapolation, (iv) support for 1D, 2D, and 3D PDEs, and (v) efficient inference for longer temporal rollouts. To address these limitations, we propose *Vectorized Conditional Neural Fields* (VCNeFs), which represent the solution of time-dependent PDEs as neural fields. Contrary to prior methods, however, VCNeFs compute, for a set of multiple spatio-temporal query points, their solutions in parallel and model their dependencies through attention mechanisms. Moreover, VCNeF can condition the neural field on both the initial conditions and the parameters of the PDEs. An extensive set of experiments demonstrates that VCNeFs are competitive with and often outperform existing ML-based surrogate models",
    "checked": true,
    "id": "b20c7278b9fdb16f455426220482099af102207f",
    "semantic_title": "vectorized conditional neural fields: a framework for solving time-dependent parametric partial differential equations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p1kDNFs62o": {
    "title": "Nesting Particle Filters for Experimental Design in Dynamical Systems",
    "volume": "poster",
    "abstract": "In this paper, we propose a novel approach to Bayesian experimental design for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential Monte Carlo technique to infer optimal designs, and embed it into a particle Markov chain Monte Carlo framework to perform gradient-based policy amortization. Our approach is distinct from other amortized experimental design techniques, as it does not rely on contrastive estimators. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies",
    "checked": true,
    "id": "d298b22971695dcd64c551e13cfd7e0bad4dfde1",
    "semantic_title": "nesting particle filters for experimental design in dynamical systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qGEEso256L": {
    "title": "Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks",
    "volume": "poster",
    "abstract": "A molecule's 2D representation consists of its atoms, their attributes, and the molecule's covalent bonds. A 3D (geometric) representation of a molecule is called a conformer and consists of its atom types and Cartesian coordinates. Every conformer has a potential energy, and the lower this energy, the more likely it occurs in nature. Most existing machine learning methods for molecular property prediction consider either 2D molecular graphs or 3D conformer structure representations in isolation. Inspired by recent work on using ensembles of conformers in conjunction with 2D graph representations, we propose E(3)-invariant molecular conformer aggregation networks. The method integrates a molecule's 2D representation with that of multiple of its conformers. Contrary to prior work, we propose a novel 2D–3D aggregation mechanism based on a differentiable solver for the Fused Gromov-Wasserstein Barycenter problem and the use of an efficient conformer generation method based on distance geometry. We show that the proposed aggregation mechanism is E(3) invariant and propose an efficient GPU implementation. Moreover, we demonstrate that the aggregation mechanism helps to significantly outperform state-of-the-art molecule property prediction methods on established datasets",
    "checked": true,
    "id": "76ced8a52b7b237cb30cf06f934fb349c752ff61",
    "semantic_title": "structure-aware e(3)-invariant molecular conformer aggregation networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fVg9YrSllr": {
    "title": "Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling",
    "volume": "poster",
    "abstract": "Monte Carlo methods, Variational Inference, and their combinations play a pivotal role in sampling from intractable probability distributions. However, current studies lack a unified evaluation framework, relying on disparate performance measures and limited method comparisons across diverse tasks, complicating the assessment of progress and hindering the decision-making of practitioners. In response to these challenges, our work introduces a benchmark that evaluates sampling methods using a standardized task suite and a broad range of performance criteria. Moreover, we study existing metrics for quantifying mode collapse and introduce novel metrics for this purpose. Our findings provide insights into strengths and weaknesses of existing sampling methods, serving as a valuable reference for future developments",
    "checked": true,
    "id": "ff9ab30cf541dec4ad7e135c603994ba33caca28",
    "semantic_title": "beyond elbos: a large-scale evaluation of variational methods for sampling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9yADTDHgGu": {
    "title": "When is Transfer Learning Possible?",
    "volume": "poster",
    "abstract": "We present a general framework for transfer learning that is flexible enough to capture transfer in supervised, reinforcement, and imitation learning. Our framework enables new insights into the fundamental question of *when* we can successfully transfer learned information across problems. We model the learner as interacting with a sequence of problem instances, or *environments*, each of which is generated from a common structural causal model (SCM) by choosing the SCM's parameters from restricted sets. We derive a procedure that can propagate restrictions on SCM parameters through the SCM's graph structure to other parameters that we are trying to learn. The propagated restrictions then enable more efficient learning (i.e., transfer). By analyzing the procedure, we are able to challenge widely-held beliefs about transfer learning. First, we show that having *sparse* changes across environments is neither necessary nor sufficient for transfer. Second, we show an example where the common heuristic of *freezing* a layer in a network causes poor transfer performance. We then use our procedure to select a more refined set of parameters to freeze, leading to successful transfer learning",
    "checked": false,
    "id": "675d01f13fb5fe6e056bb5d05e3b9a7586c868cb",
    "semantic_title": "transfer learning beyond bounded density ratios",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Lg8nw3ltvX": {
    "title": "Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning",
    "volume": "poster",
    "abstract": "In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: neural networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning across multiple problem settings, regardless of the amount of available replay memory",
    "checked": true,
    "id": "fb97eaa084af711276147704c10f8cc7102d623b",
    "semantic_title": "layerwise proximal replay: a proximal point method for online continual learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CHz7WshPcp": {
    "title": "Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer",
    "volume": "poster",
    "abstract": "We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to estimate the counterfactual mean of outcome under dynamic treatment policies in longitudinal problem settings. Our approach utilizes a transformer architecture with heterogeneous type embedding trained using temporal-difference learning. After obtaining an initial estimate using the transformer, following the targeted minimum loss-based likelihood estimation (TMLE) framework, we statistically corrected for the bias commonly associated with machine learning algorithms. Furthermore, our method also facilitates statistical inference by enabling the provision of 95% confidence intervals grounded in asymptotic statistical theory. Simulation results demonstrate our method's superior performance over existing approaches, particularly in complex, long time-horizon scenarios. It remains effective in small-sample, short-duration contexts, matching the performance of asymptotically efficient estimators. To demonstrate our method in practice, we applied our method to estimate counterfactual mean outcomes for standard versus intensive blood pressure management strategies in a real-world cardiovascular epidemiology cohort study",
    "checked": true,
    "id": "55f81deb5e0a514ed075994a9b58d9adcbb45ecc",
    "semantic_title": "longitudinal targeted minimum loss-based estimation with temporal-difference heterogeneous transformer",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vYYIuJDTHq": {
    "title": "Partial Optimality in the Linear Ordering Problem",
    "volume": "poster",
    "abstract": "The linear ordering problem consists in finding a linear order $<$ on a finite set $A$ so as to minimize the sum of costs associated with pairs of elements $a, b$ for which $a < b$. The problem is NP-hard and APX-hard. We introduce algorithms for solving the problem *partially* by deciding efficiently for some pairs $(a,b)$ whether $a < b$ is in an optimal solution. To do so, we construct maps from the feasible set of orders to itself and establish efficiently testable conditions on the cost function of the problem for which these maps are improving. We examine the effectiveness and efficiency of these conditions and algorithms empirically, on two data sets",
    "checked": false,
    "id": "39631f98cbe48bf8572d314d286c8a681452c3c4",
    "semantic_title": "graph search trees and the intermezzo problem",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sDjszMb2Ir": {
    "title": "LASER: Linear Compression in Wireless Distributed Optimization",
    "volume": "poster",
    "abstract": "Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce **LASER**: **L**ine**A**r Compre**S**sion in Wir**E**less Dist**R**ibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain 50-64% improvement in perplexity over our baselines for noisy channels",
    "checked": true,
    "id": "9acae5eb67d7499920023fbd9a07f1a9be48d4ea",
    "semantic_title": "laser: linear compression in wireless distributed optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MsnJl6JkZS": {
    "title": "Easing Concept Bleeding in Diffusion via Entity Localization and Anchoring",
    "volume": "poster",
    "abstract": "Recent diffusion models have manifested extraordinary capabilities in generating high-quality, diverse, and innovative images guided by textual prompts. Nevertheless, these state-of-the-art models may encounter the challenge of concept bleeding when generating images with multiple entities or attributes in the prompt, leading to the unanticipated merging or overlapping of distinct objects in the synthesized result. The current work exploits auxiliary networks to produce mask-constrained regions for entities, necessitating the training of an object detection network. In this paper, we investigate the bleeding reason and find that the cross-attention map associated with a specific entity or attribute tends to extend beyond its intended focus, encompassing the background or other unrelated objects and thereby acting as the primary source of concept bleeding. Motivated by this, we propose Entity Localization and Anchoring (ELA) to drive the entity to concentrate on the expected region accurately during inference, eliminating the necessity for training. Specifically, we initially identify the region corresponding to each entity and subsequently employ a tailored loss function to anchor entities within their designated positioning areas. Extensive experiments demonstrate its superior capability in precisely generating multiple objects as specified in the textual prompts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=87ZrVHDqmR": {
    "title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training",
    "volume": "poster",
    "abstract": "Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports. Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf multi-modal medical datasets, most existing methods have not thoroughly tapped into such extensive supervision signals. In this paper, we introduce the Med-ST framework for fine-grained spatial and temporal modeling to exploit information from multiple spatial views of chest radiographs and temporal historical records. For spatial modeling, Med-ST employs the *Mixture of View Expert (MoVE)* architecture to integrate different visual features from both frontal and lateral views. To achieve a more comprehensive alignment, Med-ST not only establishes the global alignment between whole images and texts but also introduces modality-weighted local alignment between text tokens and spatial regions of images. For temporal modeling, we propose a novel cross-modal bidirectional cycle consistency objective by forward mapping classification (FMC) and reverse mapping regression (RMR). By perceiving temporal information from simple to complex, Med-ST can learn temporal semantics. Experimental results across four distinct tasks demonstrate the effectiveness of Med-ST, especially in temporal classification tasks. Our code and model are available at https://github.com/SVT-Yang/MedST",
    "checked": true,
    "id": "2566e9dec649f241dac4eeb50f60f8d332ca24d4",
    "semantic_title": "unlocking the power of spatial and temporal information in medical multimodal pre-training",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWRI4uPG2X": {
    "title": "eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data",
    "volume": "poster",
    "abstract": "With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products – a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM exhibits excellent generalizability to out-of-domain settings, including unseen products and unseen instructions, highlighting its superiority as a generalist e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great potential in empowering versatile and effective LLMs for e-commerce. ECInstruct and eCeLLM models are publicly accessible through this link",
    "checked": true,
    "id": "315bffd94456ff2a214a5a972c2f5c7f2ddb2163",
    "semantic_title": "ecellm: generalizing large language models for e-commerce from large-scale, high-quality instruction data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1xKgDANODx": {
    "title": "Retrieval-Augmented Score Distillation for Text-to-3D Generation",
    "volume": "poster",
    "abstract": "Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed ReDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that ReDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/ReDream/",
    "checked": true,
    "id": "3675ae068acb1bd60e9c880c763150253ad1daef",
    "semantic_title": "retrieval-augmented score distillation for text-to-3d generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=elF0QoBSFV": {
    "title": "NDOT: Neuronal Dynamics-based Online Training for Spiking Neural Networks",
    "volume": "poster",
    "abstract": "Spiking Neural Networks (SNNs) are attracting great attention for their energy-efficient and fast-inference properties in neuromorphic computing. However, the efficient training of deep SNNs poses challenges in gradient calculation due to the non-differentiability of their binary spike-generating activation functions. The widely used surrogate gradient (SG) method, combined with the back-propagation through time (BPTT), has shown considerable effectiveness. Yet, BPTT's process of unfolding and back-propagating along the computation graph requires storing intermediate information at all time-steps, resulting in huge memory consumption and failing to meet online requirements. In this work, we propose Neuronal Dynamics-based Online Training (NDOT) for SNNs, which uses the neuronal dynamics-based temporal dependency/sensitivity in gradient computation. NDOT enables forward-in-time learning by decomposing the full gradient into temporal and spatial gradients. To illustrate the intuition behind NDOT, we employ the Follow-the-Regularized-Leader (FTRL) algorithm. FTRL explicitly utilizes historical information and addresses limitations in instantaneous loss. Our proposed NDOT method accurately captures temporal dependencies through neuronal dynamics, functioning similarly to FTRL's explicit utilizing historical information. Experiments on CIFAR-10, CIFAR-100, and CIFAR10-DVS demonstrate the superior performance of our NDOT method on large-scale static and neuromorphic datasets within a small number of time steps. The codes are available at https://github.com/HaiyanJiang/SNN-NDOT",
    "checked": false,
    "id": "9af44a6c1a33925e51a927bb156597f713152c4b",
    "semantic_title": "biologically-inspired training of spiking recurrent neural networks with neuromorphic hardware",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=F3Ds71Xgo1": {
    "title": "Entropy-Reinforced Planning with Large Language Models for Drug Discovery",
    "volume": "poster",
    "abstract": "The objective of drug discovery is to identify chemical compounds that possess specific pharmaceutical properties toward a binding target. Existing large language models (LLMS) can achieve high token matching scores in terms of likelihood for molecule generation. However, relying solely on LLM decoding often results in the generation of molecules that are either invalid due to a single misused token, or suboptimal due to unbalanced exploration and exploitation as a consequence of the LLM's prior experience. Here we propose ERP, Entropy-Reinforced Planning for Transformer Decoding, which employs an entropy-reinforced planning algorithm to enhance the Transformer decoding process and strike a balance between exploitation and exploration. ERP aims to achieve improvements in multiple properties compared to direct sampling from the Transformer. We evaluated ERP on the SARS-CoV-2 virus (3CLPro) and human cancer cell target protein (RTCB) benchmarks and demonstrated that, in both benchmarks, ERP consistently outperforms the current state-of-the-art algorithm by 1-5 percent, and baselines by 5-10 percent, respectively. Moreover, such improvement is robust across Transformer models trained with different objectives. Finally, to further illustrate the capabilities of ERP, we tested our algorithm on three code generation benchmarks and outperformed the current state-of-the-art approach as well. Our code is publicly available at: https://github.com/xuefeng-cs/ERP",
    "checked": true,
    "id": "580dc0b6e6e17987c01833d4cbf53c2efb6d012e",
    "semantic_title": "entropy-reinforced planning with large language models for drug discovery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vsOF7qDNhl": {
    "title": "What is the Long-Run Distribution of Stochastic Gradient Descent? A Large Deviations Analysis",
    "volume": "poster",
    "abstract": "In this paper, we examine the long-run distribution of stochastic gradient descent (SGD) in general, non-convex problems. Specifically, we seek to understand which regions of the problem's state space are more likely to be visited by SGD, and by how much. Using an approach based on the theory of large deviations and randomly perturbed dynamical systems, we show that the long-run distribution of SGD resembles the Boltzmann-Gibbs distribution of equilibrium thermodynamics with temperature equal to the method's step-size and energy levels determined by the problem's objective and the statistics of the noise. In particular, we show that, in the long run, (*a*) the problem's critical region is visited exponentially more often than any non-critical region; (*b*) the iterates of SGD are exponentially concentrated around the problem's minimum energy state (which does not always coincide with the global minimum of the objective); (*c*) all other connected components of critical points are visited with frequency that is exponentially proportional to their energy level; and, finally, (*d*) any component of local maximizers or saddle points is \"dominated\" by a component of local minimizers which is visited exponentially more often",
    "checked": true,
    "id": "d27a223c8c3c16fc675fab689ed76ae01a839cde",
    "semantic_title": "what is the long-run distribution of stochastic gradient descent? a large deviations analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yhpDKSw7yA": {
    "title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback",
    "volume": "poster",
    "abstract": "Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive. In this work, we aim to bridge this gap by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising concerns about the impact of noisy data on the learned policy. We design a novel loss function, which de-bias the effect of noise on average, making a policy trained by minimizing that loss robust to the noise. Under log-linear parameterization of the policy class and assuming good feature coverage of the SFT policy, we prove that the sub-optimality gap of the proposed robust DPO (rDPO) policy compared to the optimal policy is of the order $O(\\frac{1}{1-2\\epsilon}\\sqrt{\\frac{d}{n}})$, where $\\epsilon < 1/2$ is flip rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset. Our experiments on IMDb sentiment generation and Anthropic's helpful-harmless dataset shows that rDPO is robust to noise in preference labels compared to vanilla DPO and other heuristics proposed by practitioners",
    "checked": true,
    "id": "3730ad714aa22ebd3694974abe0bc6437a3ad4ee",
    "semantic_title": "provably robust dpo: aligning language models with noisy feedback",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=e0SKaKEEdr": {
    "title": "Positive Concave Deep Equilibrium Models",
    "volume": "poster",
    "abstract": "Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks. These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks. However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established. As a result, DEQ models are potentially unstable in practice. To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models. Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant. By imposing these constraints, we can easily ensure the existence and uniqueness of the fixed point without relying on additional complex assumptions commonly found in the DEQ literature, such as those based on monotone operator theory in convex analysis. Furthermore, the fixed point can be computed with the standard fixed point algorithm, and we provide theoretical guarantees of its geometric convergence, which, in particular, simplifies the training process. Experiments demonstrate the competitiveness of our pcDEQ models against other implicit models",
    "checked": true,
    "id": "12a50648c8748c41a2d83f2025a595a7308f5d1e",
    "semantic_title": "positive concave deep equilibrium models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LJ34pX1U5g": {
    "title": "Collaborative Heterogeneous Causal Inference Beyond Meta-analysis",
    "volume": "poster",
    "abstract": "Collaboration between different data centers is often challenged by heterogeneity across sites. To account for the heterogeneity, the state-of-the-art method is to re-weight the covariate distributions in each site to match the distribution of the target population. Nevertheless, this method still relies on the concept of traditional meta-analysis after adjusting for the distribution shift. This work proposes a collaborative inverse propensity score weighting estimator for causal inference with heterogeneous data. Instead of adjusting the distribution shift separately, we use weighted propensity score models to collaboratively adjust for the distribution shift. Our method shows significant improvements over the methods based on meta-analysis when heterogeneity increases. By incorporating outcome regression models, we prove the asymptotic normality when the covariates have dimension $d<8$. Our methods preserve privacy at individual sites by implementing federated learning protocols",
    "checked": true,
    "id": "1bf446b87f6bb6362a802bd0e0bb8a33462bc57a",
    "semantic_title": "collaborative heterogeneous causal inference beyond meta-analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pbey7LqBRl": {
    "title": "Neural SPH: Improved Neural Modeling of Lagrangian Fluid Dynamics",
    "volume": "poster",
    "abstract": "Smoothed particle hydrodynamics (SPH) is omnipresent in modern engineering and scientific disciplines. SPH is a class of Lagrangian schemes that discretize fluid dynamics via finite material points that are tracked through the evolving velocity field. Due to the particle-like nature of the simulation, graph neural networks (GNNs) have emerged as appealing and successful surrogates. However, the practical utility of such GNN-based simulators relies on their ability to faithfully model physics, providing accurate and stable predictions over long time horizons - which is a notoriously hard problem. In this work, we identify particle clustering originating from tensile instabilities as one of the primary pitfalls. Based on these insights, we enhance both training and rollout inference of state-of-the-art GNN-based simulators with varying components from standard SPH solvers, including pressure, viscous, and external force components. All Neural SPH-enhanced simulators achieve better performance than the baseline GNNs, often by orders of magnitude in terms of rollout error, allowing for significantly longer rollouts and significantly better physics modeling. Code available under https://github.com/tumaer/neuralsph",
    "checked": true,
    "id": "3f4097f2442b3e4d22e559969fa4b34922521103",
    "semantic_title": "neural sph: improved neural modeling of lagrangian fluid dynamics",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CEfr3h68KU": {
    "title": "Purifying Quantization-conditioned Backdoors via Layer-wise Activation Correction with Distribution Approximation",
    "volume": "poster",
    "abstract": "Model quantization is a compression technique that converts a full-precision model to a more compact low-precision version for better storage. Despite the great success of quantization, recent studies revealed the feasibility of malicious exploiting model quantization via implanting quantization-conditioned backdoors (QCBs). These special backdoors remain dormant in full-precision models but are exposed upon quantization. Unfortunately, existing defenses have limited effects on mitigating QCBs. In this paper, we conduct an in-depth analysis of QCBs. We reveal an intriguing characteristic of QCBs, where activation of backdoor-related neurons on even benign samples enjoy a distribution drift after quantization, although this drift is more significant on poisoned samples. Motivated by this finding, we propose to purify the backdoor-exposed quantized model by aligning its layer-wise activation with its full-precision version. To further exploit the more pronounced activation drifts on poisoned samples, we design an additional module to layer-wisely approximate poisoned activation distribution based on batch normalization statistics of the full-precision model. Extensive experiments are conducted, verifying the effectiveness of our defense. Our code is publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZwrfsrCduj": {
    "title": "AD3: Implicit Action is the Key for World Models to Distinguish the Diverse Visual Distractors",
    "volume": "poster",
    "abstract": "Model-based methods have significantly contributed to distinguishing task-irrelevant distractors for visual control. However, prior research has primarily focused on heterogeneous distractors like noisy background videos, leaving homogeneous distractors that closely resemble controllable agents largely unexplored, which poses significant challenges to existing methods. To tackle this problem, we propose Implicit Action Generator (IAG) to learn the implicit actions of visual distractors, and present a new algorithm named implicit Action-informed Diverse visual Distractors Distinguisher (AD3), that leverages the action inferred by IAG to train separated world models. Implicit actions effectively capture the behavior of background distractors, aiding in distinguishing the task-irrelevant components, and the agent can optimize the policy within the task-relevant state space. Our method achieves superior performance on various visual control tasks featuring both heterogeneous and homogeneous distractors. The indispensable role of implicit actions learned by IAG is also empirically validated",
    "checked": true,
    "id": "0ee4154dfc950aff101f36e7bd5b90f1d139e79d",
    "semantic_title": "ad3: implicit action is the key for world models to distinguish the diverse visual distractors",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OnkA4zaEU9": {
    "title": "Triadic-OCD: Asynchronous Online Change Detection with Provable Robustness, Optimality, and Convergence",
    "volume": "poster",
    "abstract": "The primary goal of online change detection (OCD) is to promptly identify changes in the data stream. OCD problem find a wide variety of applications in diverse areas, e.g., security detection in smart grids and intrusion detection in communication networks. Prior research usually assumes precise knowledge of the system parameters. Nevertheless, this presumption often proves unattainable in practical scenarios due to factors such as estimation errors, system updates, etc. This paper aims to take the first attempt to develop a triadic-OCD framework with certifiable robustness, provable optimality, and guaranteed convergence. In addition, the proposed triadic-OCD algorithm can be realized in a fully asynchronous distributed manner, easing the necessity of transmitting the data to a single server. This asynchronous mechanism could also mitigate the straggler issue that faced by traditional synchronous algorithm. Moreover, the non-asymptotic convergence property of Triadic-OCD is theoretically analyzed, and its iteration complexity to achieve an $\\epsilon$-optimal point is derived. Extensive experiments have been conducted to elucidate the effectiveness of the proposed method",
    "checked": true,
    "id": "dd92c005ae7625ab179e51183341b231282ae9b3",
    "semantic_title": "triadic-ocd: asynchronous online change detection with provable robustness, optimality, and convergence",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhqQJqe0Wq": {
    "title": "Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation",
    "volume": "poster",
    "abstract": "We introduce Score identity Distillation (SiD), an innovative data-free method that distills the generative capabilities of pretrained diffusion models into a single-step generator. SiD not only facilitates an exponentially fast reduction in Fréchet inception distance (FID) during distillation but also approaches or even exceeds the FID performance of the original teacher diffusion models. By reformulating forward diffusion processes as semi-implicit distributions, we leverage three score-related identities to create an innovative loss mechanism. This mechanism achieves rapid FID reduction by training the generator using its own synthesized images, eliminating the need for real data or reverse-diffusion-based generation, all accomplished within significantly shortened generation time. Upon evaluation across four benchmark datasets, the SiD algorithm demonstrates high iteration efficiency during distillation and surpasses competing distillation approaches, whether they are one-step or few-step, data-free, or dependent on training data, in terms of generation quality. This achievement not only redefines the benchmarks for efficiency and effectiveness in diffusion distillation but also in the broader field of diffusion-based generation. The PyTorch implementation is available at https://github.com/mingyuanzhou/SiD",
    "checked": true,
    "id": "d4f53b1c3f232f9a7828dd1ddad4d2dad123936f",
    "semantic_title": "score identity distillation: exponentially fast distillation of pretrained diffusion models for one-step generation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=geajNKab7g": {
    "title": "First-Order Manifold Data Augmentation for Regression Learning",
    "volume": "poster",
    "abstract": "Data augmentation (DA) methods tailored to specific domains generate synthetic samples by applying transformations that are appropriate for the characteristics of the underlying data domain, such as rotations on images and time warping on time series data. In contrast, *domain-independent* approaches, e.g. *mixup*, are applicable to various data modalities, and as such they are general and versatile. While regularizing classification tasks via DA is a well-explored research topic, the effect of DA on regression problems received less attention. To bridge this gap, we study the problem of domain-independent augmentation for regression, and we introduce *FOMA*: a new data-driven domain-independent data augmentation method. Essentially, our approach samples new examples from the tangent planes of the train distribution. Augmenting data in this way aligns with the network tendency towards capturing the dominant features of its input signals. We evaluate *FOMA* on in-distribution generalization and out-of-distribution robustness benchmarks, and we show that it improves the generalization of several neural architectures. We also find that strong baselines based on *mixup* are less effective in comparison to our approach. Our code is publicly available at https://github.com/azencot-group/FOMA",
    "checked": true,
    "id": "6869cca20e125ffe74c5e280b2d895235f7c9095",
    "semantic_title": "first-order manifold data augmentation for regression learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JVhUR8q27o": {
    "title": "Towards AutoAI: Optimizing a Machine Learning System with Black-box and Differentiable Components",
    "volume": "poster",
    "abstract": "*Machine learning* (ML) models in the real world typically do not exist in isolation. They are usually part of a complex system (e.g., healthcare systems, self-driving cars) containing multiple ML and *black-box* components. The problem of optimizing such systems, which we refer to as *automated AI* (AutoAI), requires us to *jointly* train all ML components together and presents a significant challenge because the number of system parameters is extremely high and the system has no analytical form. To circumvent this, we introduce a novel algorithm called A-BAD-BO which uses each ML component's local loss as an auxiliary indicator for system performance. A-BAD-BO uses *Bayesian optimization* (BO) to optimize the local loss configuration of a system in a smaller dimensional space and exploits the differentiable structure of ML components to recover optimal system parameters from the optimized configuration. We show A-BAD-BO converges to optimal system parameters by showing that it is *asymptotically no regret*. We use A-BAD-BO to optimize several synthetic and real-world complex systems, including a prompt engineering pipeline for *large language models* containing millions of system parameters. Our results demonstrate that A-BAD-BO yields better system optimality than gradient-driven baselines and is more sample-efficient than pure BO algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YsQI04KaN": {
    "title": "Antibody Design Using a Score-based Diffusion Model Guided by Evolutionary, Physical and Geometric Constraints",
    "volume": "poster",
    "abstract": "Antibodies are central proteins in adaptive immune responses, responsible for protecting against viruses and other pathogens. Rational antibody design has proven effective in the diagnosis and treatment of various diseases like cancers and virus infections. While recent diffusion-based generative models show promise in designing antigen-specific antibodies, the primary challenge lies in the scarcity of labeled antibody-antigen complex data and binding affinity data. We present AbX, a new score-based diffusion generative model guided by evolutionary, physical, and geometric constraints for antibody design. These constraints serve to narrow the search space and provide priors for plausible antibody sequences and structures. Specifically, we leverage a pre-trained protein language model as priors for evolutionary plausible antibodies and introduce additional training objectives for geometric and physical constraints like van der Waals forces. Furthermore, as far as we know, AbX is the first score-based diffusion model with continuous timesteps for antibody design, jointly modeling the discrete sequence space and the $\\mathrm{SE}(3)$ structure space. Evaluated on two independent testing sets, we show that AbX outperforms other published methods, achieving higher accuracy in sequence and structure generation and enhanced antibody-antigen binding affinity. Ablation studies highlight the clear contributions of the introduced constraints to antibody design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LO4xhXmFal": {
    "title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data",
    "volume": "poster",
    "abstract": "*How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed?* We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content is included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP outperforms the prior best method by 8.6% in detection accuracy (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give approximately 0% accuracy. The code and datasets are available at https://github.com/LeiLiLab/DE-COP",
    "checked": true,
    "id": "a932b662645ab4a348c44c73bb81876cb415ae95",
    "semantic_title": "de-cop: detecting copyrighted content in language models training data",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=RPMTNGMq0O": {
    "title": "Dealing With Unbounded Gradients in Stochastic Saddle-point Optimization",
    "volume": "poster",
    "abstract": "We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span",
    "checked": true,
    "id": "f8d58f71b2a04db6cc4a531dbc4db00359afe436",
    "semantic_title": "dealing with unbounded gradients in stochastic saddle-point optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=nvHlHfjJPe": {
    "title": "$H$-Consistency Guarantees for Regression",
    "volume": "poster",
    "abstract": "We present a detailed study of $H$-consistency bounds for regression. We first present new theorems that generalize the tools previously given to establish $H$-consistency bounds. This generalization proves essential for analyzing $H$-consistency bounds specific to regression. Next, we prove a series of novel $H$-consistency bounds for surrogate loss functions of the squared loss, under the assumption of a symmetric distribution and a bounded hypothesis set. This includes positive results for the Huber loss, all $\\ell_p$ losses, $p \\geq 1$, the squared $\\epsilon$-insensitive loss, as well as a negative result for the $\\epsilon$-insensitive loss used in Support Vector Regression (SVR). We further leverage our analysis of $H$-consistency for regression and derive principled surrogate losses for adversarial regression (Section 5). This readily establishes novel algorithms for adversarial regression, for which we report favorable experimental results in Section 6",
    "checked": false,
    "id": "fa9c8a28e42c3a55a632ffd7145fa65ca2a4cbdf",
    "semantic_title": "h-consistency guarantees for regression",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=9zlZuAAb08": {
    "title": "Quality Diversity through Human Feedback: Towards Open-Ended Diversity-Driven Optimization",
    "volume": "poster",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown potential in qualitative tasks where easily defined performance measures are lacking. However, there are drawbacks when RLHF is commonly used to optimize for average human preferences, especially in generative tasks that demand diverse model responses. Meanwhile, Quality Diversity (QD) algorithms excel at identifying diverse and high-quality solutions but often rely on manually crafted diversity metrics. This paper introduces Quality Diversity through Human Feedback (QDHF), a novel approach that progressively infers diversity metrics from human judgments of similarity among solutions, thereby enhancing the applicability and effectiveness of QD algorithms in complex and open-ended domains. Empirical studies show that QDHF significantly outperforms state-of-the-art methods in automatic diversity discovery and matches the efficacy of QD with manually crafted diversity metrics on standard benchmarks in robotics and reinforcement learning. Notably, in open-ended generative tasks, QDHF substantially enhances the diversity of text-to-image generation from a diffusion model and is more favorably received in user studies. We conclude by analyzing QDHF's scalability, robustness, and quality of derived diversity metrics, emphasizing its strength in open-ended optimization tasks. Code and tutorials are available at https://liding.info/qdhf",
    "checked": false,
    "id": "c47785aa96672fdb1d2da3515a5c9a0cf8997e69",
    "semantic_title": "quality diversity through human feedback",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=rMV86cAOh6": {
    "title": "Stochastic Conditional Diffusion Models for Robust Semantic Image Synthesis",
    "volume": "poster",
    "abstract": "Semantic image synthesis (SIS) is a task to generate realistic images corresponding to semantic maps (labels). However, in real-world applications, SIS often encounters noisy user inputs. To address this, we propose Stochastic Conditional Diffusion Model (SCDM), which is a robust conditional diffusion model that features novel forward and generation processes tailored for SIS with noisy labels. It enhances robustness by stochastically perturbing the semantic label maps through Label Diffusion, which diffuses the labels with discrete diffusion. Through the diffusion of labels, the noisy and clean semantic maps become similar as the timestep increases, eventually becoming identical at $t=T$. This facilitates the generation of an image close to a clean image, enabling robust generation. Furthermore, we propose a class-wise noise schedule to differentially diffuse the labels depending on the class. We demonstrate that the proposed method generates high-quality samples through extensive experiments and analyses on benchmark datasets, including a novel experimental setup simulating human errors during real-world applications. Code is available at https://github.com/mlvlab/SCDM",
    "checked": true,
    "id": "2dca8ba2656838e2a10ba4c7d6cb491d4caa1341",
    "semantic_title": "stochastic conditional diffusion models for robust semantic image synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ohH3sbUue2": {
    "title": "Optimal bounds for $\\ell_p$ sensitivity sampling via $\\ell_2$ augmentation",
    "volume": "poster",
    "abstract": "Data subsampling is one of the most natural methods to approximate a massively large data set by a small representative proxy. In particular, sensitivity sampling received a lot of attention, which samples points proportional to an individual importance measure called sensitivity. This framework reduces in very general settings the size of data to roughly the VC dimension $d$ times the total sensitivity $\\mathfrak S$ while providing strong $(1\\pm\\varepsilon)$ guarantees on the quality of approximation. The recent work of Woodruff & Yasuda (2023c) improved substantially over the general $\\tilde O(\\varepsilon^{-2}\\mathfrak Sd)$ bound for the important problem of $\\ell_p$ subspace embeddings to $\\tilde O(\\varepsilon^{-2}\\mathfrak S^{2/p})$ for $p\\in[1,2]$. Their result was subsumed by an earlier $\\tilde O(\\varepsilon^{-2}\\mathfrak Sd^{1-p/2})$ bound which was implicitly given in the work of Chen & Derezinski (2021). We show that their result is tight when sampling according to plain $\\ell_p$ sensitivities. We observe that by augmenting the $\\ell_p$ sensitivities by $\\ell_2$ sensitivities, we obtain better bounds improving over the aforementioned results to optimal linear $\\tilde O(\\varepsilon^{-2}(\\mathfrak S+d)) = \\tilde O(\\varepsilon^{-2}d)$ sampling complexity for all $p \\in [1,2]$. In particular, this resolves an open question of Woodruff & Yasuda (2023c) in the affirmative for $p \\in [1,2]$ and brings sensitivity subsampling into the regime that was previously only known to be possible using Lewis weights (Cohen & Peng, 2015). As an application of our main result, we also obtain an $\\tilde O(\\varepsilon^{-2}\\mu d)$ sensitivity sampling bound for logistic regression, where $\\mu$ is a natural complexity measure for this problem. This improves over the previous $\\tilde O(\\varepsilon^{-2}\\mu^2 d)$ bound of Mai et al. (2021) which was based on Lewis weights subsampling",
    "checked": true,
    "id": "28225d03b2bb5641a875f2c3277a6a1e5ae534dc",
    "semantic_title": "optimal bounds for $\\ell_p$ sensitivity sampling via $\\ell_2$ augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HrzQZXzrN2": {
    "title": "Predictive Performance Comparison of Decision Policies Under Confounding",
    "volume": "poster",
    "abstract": "Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy",
    "checked": true,
    "id": "05db3d0f88615ac5edb5499f7ea5bdebe2c149bc",
    "semantic_title": "predictive performance comparison of decision policies under confounding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SlRcJvf1yd": {
    "title": "Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates",
    "volume": "poster",
    "abstract": "We study the problem of efficiently computing the derivative of the fixed-point of a parametric nondifferentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. We build upon the work by Bolte et al. (2022), who prove linear convergence of nonsmooth ITD under a piecewise Lipschitz smooth assumption. In the deterministic case, we provide a linear rate for AID and an improved linear rate for ITD which closely match the ones for the smooth setting. We further introduce NSID, a new stochastic method to compute the implicit derivative when the contraction map is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of NSID, encompassing the best available rates in the smooth setting. We also present illustrative experiments confirming our analysis",
    "checked": true,
    "id": "1b732690b1310cd7755955852ea60b86b06c98c7",
    "semantic_title": "nonsmooth implicit differentiation: deterministic and stochastic convergence rates",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9DMMvMTDur": {
    "title": "EvIL: Evolution Strategies for Generalisable Imitation Learning",
    "volume": "poster",
    "abstract": "Often times in imitation learning (IL), the environment we collect expert demonstrations in and the environment we want to deploy our learned policy in aren't exactly the same (e.g. demonstrations collected in simulation but deployment in the real world). Compared to policy-centric approaches to IL like behavioural cloning, reward-centric approaches like *inverse reinforcement learning* (IRL) often better replicate expert behaviour in new environments. This transfer is usually performed by optimising the recovered reward under the dynamics of the target environment. However, *(a)* we find that modern deep IL algorithms frequently recover rewards which induce policies far weaker than the expert, *even in the same environment the demonstrations were collected in*. Furthermore, *(b)* these rewards are often quite poorly shaped, necessitating extensive environment interaction to optimise effectively. We provide simple and scalable fixes to both of these concerns. For *(a)*, we find that *reward model ensembles* combined with a slightly different training objective significantly improves re-training and transfer performance. For *(b)*, we propose a novel *evolution-strategies* based method (EvIL) to optimise for a reward-shaping term that speeds up re-training in the target environment, closing a gap left open by the classical theory of IRL. On a suite of continuous control tasks, we are able to re-train policies in target (and source) environments more interaction-efficiently than prior work",
    "checked": true,
    "id": "0b211723fedff3feddf54a2e2be2d935aeac8257",
    "semantic_title": "evil: evolution strategies for generalisable imitation learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=uku9r6RROl": {
    "title": "DRED: Zero-Shot Transfer in Reinforcement Learning via Data-Regularised Environment Design",
    "volume": "poster",
    "abstract": "Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when these environments share characteristics with the ones they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which assume control over level generation. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce *data-regularised environment design* (DRED). DRED generates levels using a generative model trained to approximate the ground truth distribution of an initial set of level parameters. Through its grounding, DRED achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods",
    "checked": true,
    "id": "44115f2c0cdd038d7c5956a3e99e910ab020d448",
    "semantic_title": "dred: zero-shot transfer in reinforcement learning via data-regularised environment design",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6Zl9rv6PDx": {
    "title": "Causal Action Influence Aware Counterfactual Data Augmentation",
    "volume": "poster",
    "abstract": "Offline data are both valuable and practical resources for teaching robots complex behaviors. Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize beyond the training distribution. However, the complexity of real-world scenarios typically requires huge amounts of data to prevent neural network policies from picking up on spurious correlations and learning non-causal relationships. We propose CAIAC, a data augmentation method that can create feasible synthetic transitions from a fixed dataset without having access to online environment interactions. By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping $\\textit{action}$-unaffected parts of the state-space between independent trajectories in the dataset. We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift",
    "checked": true,
    "id": "ea9542d9cec6dca54664b4a58787c61ed35f2362",
    "semantic_title": "causal action influence aware counterfactual data augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aP0H8A1ywk": {
    "title": "How Smooth Is Attention?",
    "volume": "poster",
    "abstract": "Self-attention and masked self-attention are at the heart of Transformers' outstanding success. Still, our mathematical understanding of attention, in particular of its Lipschitz properties — which are key when it comes to analyzing robustness and expressive power — is incomplete. We provide a detailed study of the Lipschitz constant of self-attention in several practical scenarios, discussing the impact of the sequence length $n$ and layer normalization on the local Lipschitz constant of both unmasked and masked self-attention. In particular, we show that for inputs of length $n$ in any compact set, the Lipschitz constant of self-attention is bounded by $\\sqrt{n}$ up to a constant factor and that this bound is tight for reasonable sequence lengths. When the sequence length $n$ is too large for the previous bound to be tight, which we refer to as the mean-field regime, we provide an upper bound and a matching lower bound which are independent of $n$. Our mean-field framework for masked self-attention is novel and of independent interest. Our experiments on pretrained and randomly initialized BERT and GPT-2 support our theoretical findings",
    "checked": true,
    "id": "2cb91c1a9488c3bdb05eee7668ac4a77c7e18a3b",
    "semantic_title": "how smooth is attention?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=4Vqr8SRfyX": {
    "title": "Case-Based or Rule-Based: How Do Transformers Do the Math?",
    "volume": "poster",
    "abstract": "Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic *rules* of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar *cases* seen in the training corpus for help. We define these two different reasoning mechanisms as \"*rule-based reasoning*\" and \"*case-based reasoning*\". Since rule-based reasoning is essential for acquiring systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to use rules explicitly helps them learn rule-based reasoning and generalize better in length. Code is available at https://github.com/GraphPKU/Case_or_Rule",
    "checked": true,
    "id": "fc45b0c7249c9e48de2cd35fc3d9984490229392",
    "semantic_title": "case-based or rule-based: how do transformers do the math?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=biE1uHyG0l": {
    "title": "Fundamental Limits of Distributed Covariance Matrix Estimation Under Communication Constraints",
    "volume": "poster",
    "abstract": "Estimating high-dimensional covariance matrices is crucial in various domains. This work considers a scenario where two collaborating agents access disjoint dimensions of $m$ samples from a high--dimensional random vector, and they can only communicate a limited number of bits to a central server, which wants to accurately approximate the covariance matrix. We analyze the fundamental trade--off between communication cost, number of samples, and estimation accuracy. We prove a lower bound on the error achievable by any estimator, highlighting the impact of dimensions, number of samples, and communication budget. Furthermore, we present an algorithm that achieves this lower bound up to a logarithmic factor, demonstrating its near-optimality in practical settings",
    "checked": false,
    "id": "fcc0a5d4f295bf4675c07837ed0e1e97752c6737",
    "semantic_title": "isac meets swipt: multi-functional wireless systems integrating sensing, communication, and powering",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=FYvpxyS43U": {
    "title": "RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation",
    "volume": "poster",
    "abstract": "We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis that jointly trains $\\textit{low-rank}$ and *highly-sparse* components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms LoRA, pure sparse fine-tuning, and alternative hybrid methods at the same parameter budget, and can even recover the performance of FFT on some tasks. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memory- and computationally-efficient training, and show that it is also compatible with low-precision base weights, resulting in the first joint representation combining quantization, low-rank and sparse approximations. Our code is available at https://github.com/IST-DASLab/RoSA",
    "checked": true,
    "id": "a9f5e62bd132e43dd300fefac71093ef5c7c8596",
    "semantic_title": "rosa: accurate parameter-efficient fine-tuning via robust adaptation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzCY0fRver": {
    "title": "Estimating Canopy Height at Scale",
    "volume": "poster",
    "abstract": "We propose a framework for global-scale canopy height estimation based on satellite data. Our model leverages advanced data preprocessing techniques, resorts to a novel loss function designed to counter geolocation inaccuracies inherent in the ground-truth height measurements, and employs data from the Shuttle Radar Topography Mission to effectively filter out erroneous labels in mountainous regions, enhancing the reliability of our predictions in those areas. A comparison between predictions and ground-truth labels yields an MAE/RMSE of 2.43 / 4.73 (meters) overall and 4.45 / 6.72 (meters) for trees taller than five meters, which depicts a substantial improvement compared to existing global-scale products. The resulting height map as well as the underlying framework will facilitate and enhance ecological analyses at a global scale, including, but not limited to, large-scale forest and biomass monitoring",
    "checked": true,
    "id": "a32d1a23e10762abee2eb1c1b1e972b25407f198",
    "semantic_title": "estimating canopy height at scale",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0sH9HGMwq": {
    "title": "Prediction Accuracy of Learning in Games : Follow-the-Regularized-Leader meets Heisenberg",
    "volume": "poster",
    "abstract": "We investigate the accuracy of prediction in deterministic learning dynamics of zero-sum games with random initializations, specifically focusing on observer uncertainty and its relationship to the evolution of covariances. Zero-sum games are a prominent field of interest in machine learning due to their various applications. Concurrently, the accuracy of prediction in dynamical systems from mechanics has long been a classic subject of investigation since the discovery of the Heisenberg Uncertainty Principle. This principle employs covariance and standard deviation of particle states to measure prediction accuracy. In this study, we bring these two approaches together to analyze the Follow-the-Regularized-Leader (FTRL) algorithm in two-player zero-sum games. We provide growth rates of covariance information for continuous-time FTRL, as well as its two canonical discretization methods (Euler and Symplectic). A Heisenberg-type inequality is established for FTRL. Our analysis and experiments also show that employing Symplectic discretization enhances the accuracy of prediction in learning dynamics",
    "checked": true,
    "id": "b71b055d5aeef1eef9b00c144b364421e6afd674",
    "semantic_title": "prediction accuracy of learning in games : follow-the-regularized-leader meets heisenberg",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nxzXTLByXO": {
    "title": "BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback",
    "volume": "poster",
    "abstract": "Distribution matching methods for language model alignment such as Generation with Distributional Control (GDC) and Distributional Policy Gradient (DPG) have not received the same level of attention in reinforcement learning from human feedback (RLHF) as contrastive methods such as Sequence Likelihood Calibration (SLiC), Direct Preference Optimization (DPO) and its variants. We identify high variance of the gradient estimate as the primary reason for the lack of success of these methods and propose a self-normalized baseline to reduce the variance. We further generalize the target distribution in DPG, GDC and DPO by using Bayes' rule to define the reward-conditioned posterior. The resulting approach, referred to as BRAIn - Bayesian Reward-conditioned Amortized Inference acts as a bridge between distribution matching methods and DPO and significantly outperforms prior art in summarization and Antropic HH tasks",
    "checked": true,
    "id": "8ed08720d3d0d15608089f6242634ecd30618217",
    "semantic_title": "brain: bayesian reward-conditioned amortized inference for natural language generation from feedback",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=zEqeNEuiJr": {
    "title": "SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding",
    "volume": "poster",
    "abstract": "Distributed learning is an effective approach to accelerate model training by using parallel computing power of multiple workers. However, substantial communication delays arise between workers and a parameter server due to the massive costs associated with communicating gradients. SignSGD with majority voting (signSGD-MV) is a simple yet effective optimizer that reduces communication costs through sign quantization, but its convergence rate significantly decreases when adversarial workers arbitrarily manipulate datasets or local gradient updates. In this paper, we consider a distributed learning problem where the workforce comprises a mixture of honest and adversarial workers. In this setting, we show that the convergence rate can remain invariant as long as the number of honest workers providing trustworthy local updates to the parameter server exceeds the number of adversarial workers. The key idea behind this counter-intuitive result is our novel aggregation method, signSGD with federated defense (signSGD-FD). Unlike traditional approaches, signSGD-FD utilizes the gradient information sent by adversarial workers with appropriate weights, obtained through gradient sign decoding. Experimental results demonstrate that signSGD-FD achieves superior convergence rates compared to traditional algorithms in various adversarial attack scenarios",
    "checked": true,
    "id": "89d6b9c49cc14d007e679a1796874d0be981fae4",
    "semantic_title": "signsgd with federated defense: harnessing adversarial attacks through gradient sign decoding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NwYsuFuelg": {
    "title": "Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers",
    "volume": "poster",
    "abstract": "Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques focus on the *static* setting, wherein the identity of Byzantine workers remains unchanged throughout the learning process. This assumption fails to capture real-world *dynamic* Byzantine behaviors, which may include intermittent malfunctions or targeted, time-limited attacks. Addressing this limitation, we propose DynaBRO -- a new method capable of withstanding any sub-linear number of identity changes across rounds. Specifically, when the number of such changes is $\\mathcal{O}(\\sqrt{T})$ (where $T$ is the total number of training rounds), DynaBRO nearly matches the state-of-the-art asymptotic convergence rate of the static setting. Our method utilizes a multi-level Monte Carlo (MLMC) gradient estimation technique applied at the server to robustly aggregated worker updates. By additionally leveraging an adaptive learning rate, we circumvent the need for prior knowledge of the fraction of Byzantine workers",
    "checked": true,
    "id": "3144176620cf1f6483c64b163bcf0faf9faa1f34",
    "semantic_title": "dynamic byzantine-robust learning: adapting to switching byzantine workers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jaJxpKkBcL": {
    "title": "Unmasking Vulnerabilities: Cardinality Sketches under Adaptive Inputs",
    "volume": "poster",
    "abstract": "Cardinality sketches are popular data structures that enhance the efficiency of working with large data sets. The sketches are randomized representations of sets that are only of logarithmic size but can support set merges and approximate cardinality (i.e., distinct count) queries. When queries are not adaptive, that is, they do not depend on preceding query responses, the design provides strong guarantees of correctly answering a number of queries exponential in the sketch size $k$. In this work, we investigate the performance of cardinality sketches in adaptive settings and unveil inherent vulnerabilities. We design an attack against the ``standard'' estimators that constructs an adversarial input by post-processing responses to a set of simple non-adaptive queries of size linear in the sketch size $k$. Empirically, our attack used only $4k$ queries with the widely used HyperLogLog (HLL++) Flajolet et al., 2007; Heule et al., 2013) sketch. The simple attack technique suggests it can be effective with post-processed natural workloads. Finally and importantly, we demonstrate that the vulnerability is inherent as any estimator applied to known sketch structures can be attacked using a number of queries that is quadratic in $k$, matching a generic upper bound",
    "checked": true,
    "id": "abd19ce1ab86a64293873cc02d778f9ea12833b2",
    "semantic_title": "unmasking vulnerabilities: cardinality sketches under adaptive inputs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YMjzz2g81": {
    "title": "SPABA: A Single-Loop and Probabilistic Stochastic Bilevel Algorithm Achieving Optimal Sample Complexity",
    "volume": "poster",
    "abstract": "While stochastic bilevel optimization methods have been extensively studied for addressing large-scale nested optimization problems in machine learning, it remains an open question whether the optimal complexity bounds for solving bilevel optimization are the same as those in single-level optimization. Our main result resolves this question: SPABA, an adaptation of the PAGE method for nonconvex optimization in (Li et al., 2021) to the bilevel setting, can achieve optimal sample complexity in both the finite-sum and expectation settings. We show the optimality of SPABA by proving that there is no gap in complexity analysis between stochastic bilevel and single-level optimization when implementing PAGE. Notably, as indicated by the results of (Dagréou et al., 2022), there might exist a gap in complexity analysis when implementing other stochastic gradient estimators, like SGD and SAGA. In addition to SPABA, we propose several other single-loop stochastic bilevel algorithms, that either match or improve the state-of-the-art sample complexity results, leveraging our convergence rate and complexity analysis. Numerical experiments demonstrate the superior practical performance of the proposed methods",
    "checked": true,
    "id": "4ad8d690df3d8ca1cec950f46764df8138590122",
    "semantic_title": "spaba: a single-loop and probabilistic stochastic bilevel algorithm achieving optimal sample complexity",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CuiRGtVI55": {
    "title": "Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control",
    "volume": "poster",
    "abstract": "Vision Transformers (ViT), when paired with large-scale pretraining, have shown remarkable performance across various computer vision tasks, primarily due to their weak inductive bias. However, while such weak inductive bias aids in pretraining scalability, this may hinder the effective adaptation of ViTs for visuo-motor control tasks as a result of the absence of control-centric inductive biases. Such absent inductive biases include spatial locality and translation equivariance bias which convolutions naturally offer. To this end, we introduce Convolution Injector (CoIn), an add-on module that injects convolutions which are rich in locality and equivariance biases into a pretrained ViT for effective adaptation in visuo-motor control. We evaluate CoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12 varied control tasks within three separate domains (Adroit, MetaWorld, DMC), and demonstrate that CoIn consistently enhances control task performance across all experimented environments and models, validating the effectiveness of providing pretrained ViTs with control-centric biases",
    "checked": true,
    "id": "1381b776a6a99fd9c13141d4116d44c40e919e64",
    "semantic_title": "adapting pretrained vits with convolution injector for visuo-motor control",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KCVCFsPkrm": {
    "title": "Shifted Interpolation for Differential Privacy",
    "volume": "poster",
    "abstract": "Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the \"privacy amplification by iteration\" phenomenon in the unifying framework of $f$-differential privacy---which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\\varepsilon,\\delta)$-DP and Rényi DP. Our key technical insight is the construction of *shifted interpolated processes* that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first *exact* privacy analysis in the foundational setting of strongly convex optimization. Our techniques extend to many settings: convex/strongly convex, constrained/unconstrained, full/cyclic/stochastic batches, and all combinations thereof. As an immediate corollary, we recover the $f$-DP characterization of the exponential mechanism for strongly convex optimization in Gopi et al. (2022), and moreover extend this result to more general settings",
    "checked": true,
    "id": "6ba8972d91654cd11fdc919103f46aa462ce100c",
    "semantic_title": "shifted interpolation for differential privacy",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=8tzjEMF0Vq": {
    "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
    "volume": "poster",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, the single reward model overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. Next, we propose to learn a mixture of reward models via an expectation-maximization algorithm and solve a MaxMin alignment objective inspired by the Egalitarian principle in social choice theory to better honor diverse human preferences. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language (with Tulu2-7B)) and show the efficacy of the proposed approach in the presence of diversity among human preferences. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general",
    "checked": false,
    "id": "dacc3a8d45968616f220628dc0db8d5d78c1a389",
    "semantic_title": "maxmin-rlhf: towards equitable alignment of large language models with diverse human preferences",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=VDgfJnOEMV": {
    "title": "On the Convergence of Projected Bures-Wasserstein Gradient Descent under Euclidean Strong Convexity",
    "volume": "poster",
    "abstract": "The Bures-Wasserstein (BW) gradient descent method has gained considerable attention in various domains, including Gaussian barycenter, matrix recovery and variational inference problems, due to its alignment with the Wasserstein geometry of normal distributions. Despite its popularity, existing convergence analysis are often contingent upon specific loss functions, and the exploration of constrained settings within this framework remains limited. In this work, we make an attempt to bridge this gap by providing a general convergence rate guarantee for BW gradient descent when the Euclidean strong convexity of the loss and the constraints is assumed. In an effort to advance practical implementations, we also derive a closed-form solution for the projection onto BW distance-constrained sets, which enables the fast implementation of projected BW gradient descent for problems that arise in the constrained barycenter and distributionally robust optimization literature. Experimental results demonstrate significant improvements in computational efficiency and convergence speed, underscoring the efficacy of our method in practical scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZXsNkm3bxu": {
    "title": "CaPS: Collaborative and Private Synthetic Data Generation from Distributed Sources",
    "volume": "poster",
    "abstract": "Data is the lifeblood of the modern world, forming a fundamental part of AI, decision-making, and research advances. With increase in interest in data, governments have taken important steps towards a regulated data world, drastically impacting data sharing and data usability and resulting in massive amounts of data confined within the walls of organizations. While synthetic data generation (SDG) is an appealing solution to break down these walls and enable data sharing, the main drawback of existing solutions is the assumption of a trusted aggregator for generative model training. Given that many data holders may not want to, or be legally allowed to, entrust a central entity with their raw data, we propose a framework for collaborative and private generation of synthetic tabular data from distributed data holders. Our solution is general, applicable to any marginal-based SDG, and provides input privacy by replacing the trusted aggregator with secure multi-party computation (MPC) protocols and output privacy via differential privacy (DP). We demonstrate the applicability and scalability of our approach for the state-of-the-art select-measure-generate SDG algorithms MWEM+PGM and AIM",
    "checked": true,
    "id": "f08ef9da30f5529f9ab42131402a31a851227b73",
    "semantic_title": "caps: collaborative and private synthetic data generation from distributed sources",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gtYdvSGMYV": {
    "title": "LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "In cooperative multi-agent reinforcement learning (MARL), agents collaborate to achieve common goals, such as defeating enemies and scoring a goal. However, learning goal-reaching paths toward such a semantic goal takes a considerable amount of time in complex tasks and the trained model often fails to find such paths. To address this, we present LAtent Goal-guided Multi-Agent reinforcement learning (LAGMA), which generates a goal-reaching trajectory in latent space and provides a latent goal-guided incentive to transitions toward this reference trajectory. LAGMA consists of three major components: (a) quantized latent space constructed via a modified VQ-VAE for efficient sample utilization, (b) goal-reaching trajectory generation via extended VQ codebook, and (c) latent goal-guided intrinsic reward generation to encourage transitions towards the sampled goal-reaching path. The proposed method is evaluated by StarCraft II with both dense and sparse reward settings and Google Research Football. Empirical results show further performance improvement over state-of-the-art baselines",
    "checked": true,
    "id": "af0851a60ac3b5f003e3fefad23a856ed5f91f99",
    "semantic_title": "lagma: latent goal-guided multi-agent reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NCjlFw1Ab0": {
    "title": "Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Traveling waves are a fundamental phenomenon in the brain, playing a crucial role in short-term information storage. In this study, we leverage the concept of traveling wave dynamics within a neural lattice to formulate a theoretical model of neural working memory in Recurrent Neural Networks (RNNs), study its properties, and its real world implications in AI. The proposed model diverges from traditional approaches, which assume information storage in static, register-like locations updated by interference. Instead, the model stores data as waves that is updated by the wave's boundary conditions. We rigorously examine the model's capabilities in representing and learning state histories, which are vital for learning history-dependent dynamical systems. The findings reveal that the model reliably stores external information and enhances the learning process by addressing the diminishing gradient problem of RNNs. To understand the model's real-world applicability, we explore two cases: linear boundary condition and non-linear, self-attention-driven boundary condition. The experiments reveal that the linear scenario is effectively *learned* by RNNs through backpropagation when modeling history-dependent dynamical systems. Conversely, the non-linear scenario parallels an attention-only transformer. Collectively, our findings suggest the broader relevance of traveling waves in AI and its potential in advancing neural network architectures",
    "checked": true,
    "id": "e253d32de875c4bcde644255c77adf476dcdf91f",
    "semantic_title": "hidden traveling waves bind working memory variables in recurrent neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=BNAvYSCrLD": {
    "title": "In-Context Learning Agents Are Asymmetric Belief Updaters",
    "volume": "poster",
    "abstract": "We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition",
    "checked": true,
    "id": "d408a2931262e0b21fe607900bff0c3822d0a634",
    "semantic_title": "in-context learning agents are asymmetric belief updaters",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=xMJT4XW468": {
    "title": "Critical feature learning in deep neural networks",
    "volume": "poster",
    "abstract": "A key property of neural networks driving their success is their ability to learn features from data. Understanding feature learning from a theoretical viewpoint is an emerging field with many open questions. In this work we capture finite-width effects with a systematic theory of network kernels in deep non-linear neural networks. We show that the Bayesian prior of the network can be written in closed form as a superposition of Gaussian processes, whose kernels are distributed with a variance that depends inversely on the network width $N$. A large deviation approach, which is exact in the proportional limit for the number of data points $P=\\alpha N\\to\\infty$, yields a pair of forward-backward equations for the maximum a posteriori kernels in all layers at once. We study their solutions perturbatively, to demonstrate how the backward propagation across layers aligns kernels with the target. An alternative field-theoretic formulation shows that kernel adaptation of the Bayesian posterior at finite-width results from fluctuations in the prior: larger fluctuations correspond to a more flexible network prior and thus enable stronger adaptation to data. We thus find a bridge between the classical edge-of-chaos NNGP theory and feature learning, exposing an intricate interplay between criticality, response functions, and feature scale",
    "checked": true,
    "id": "229cc0f14c36e9bb22f95f906320bf9eed5d92cf",
    "semantic_title": "critical feature learning in deep neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=hJaWoU3Emh": {
    "title": "Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains",
    "volume": "poster",
    "abstract": "Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments on synthetic and real datasets, we demonstrate that the proposed method substantially improves over existing alternatives in prediction accuracy and robustness on both regression and classification tasks. We also assess its effectiveness on a user city prediction dataset from Meta",
    "checked": true,
    "id": "72acf3a27cd5c3f8c7a4d1b45041811005fb003a",
    "semantic_title": "multiply robust estimation for local distribution shifts with multiple domains",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cj5HbaX14p": {
    "title": "BOtied: Multi-objective Bayesian optimization with tied multivariate ranks",
    "volume": "poster",
    "abstract": "Many scientific and industrial applications require the joint optimization of multiple, potentially competing objectives. Multi-objective Bayesian optimization (MOBO) is a sample-efficient framework for identifying Pareto-optimal solutions. At the heart of MOBO is the acquisition function, which determines the next candidate to evaluate by navigating the best compromises among the objectives. Acquisition functions that rely on integrating over the objective space scale poorly to a large number of objectives. In this paper, we show a natural connection between the non-dominated solutions and the highest multivariate rank, which coincides with the extreme level line of the joint cumulative distribution function (CDF). Motivated by this link, we propose the CDF indicator, a Pareto-compliant metric for evaluating the quality of approximate Pareto sets, that can complement the popular hypervolume indicator. We then introduce an acquisition function based on the CDF indicator, called BOtied. BOtied can be implemented efficiently with copulas, a statistical tool for modeling complex, high-dimensional distributions. Our experiments on a variety of synthetic and real-world experiments demonstrate that BOtied outperforms state-of-the-art MOBO algorithms while being computationally efficient for many objectives",
    "checked": true,
    "id": "1cecd4233c5031d6938c42569462fdae1a10398e",
    "semantic_title": "botied: multi-objective bayesian optimization with tied multivariate ranks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=L1eJ3NKPCd": {
    "title": "Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks",
    "volume": "poster",
    "abstract": "Transformers trained on huge text corpora exhibit a remarkable set of capabilities, e.g., performing simple logical operations. Given the inherent compositional nature of language, one can expect the model to learn to compose these capabilities, potentially yielding a combinatorial explosion of what operations it can perform on an input. Motivated by the above, we aim to assess in this paper \"how capable can a transformer become?\". Specifically, we train autoregressive Transformer models on a data-generating process that involves compositions of a set of well-defined monolithic capabilities. Through a series of extensive and systematic experiments on this data-generating process, we show that: (1) autoregressive Transformers can learn compositional structures from small amounts of training data and generalize to exponentially or even combinatorially many functions; (2) composing functions by generating intermediate outputs is more effective at generalizing to unseen compositions, compared to generating no intermediate outputs; (3) biases in the order of the compositions in the training data, results in Transformers that fail to compose some combinations of functions; and (4) the attention layers seem to select the capability to apply while the feed-forward layers execute the capability",
    "checked": true,
    "id": "c2f9b21da0ce660c09fe8e740d020d9a752e748b",
    "semantic_title": "compositional capabilities of autoregressive transformers: a study on synthetic, interpretable tasks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=inEuvSg0y1": {
    "title": "Mol-AE: Auto-Encoder Based Molecular Representation Learning With 3D Cloze Test Objective",
    "volume": "poster",
    "abstract": "3D molecular representation learning has gained tremendous interest and achieved promising performance in various downstream tasks. A series of recent approaches follow a prevalent framework: an encoder-only model coupled with a coordinate denoising objective. However, through a series of analytical experiments, we prove that the encoder-only model with coordinate denoising objective exhibits inconsistency between pre-training and downstream objectives, as well as issues with disrupted atomic identifiers. To address these two issues, we propose Mol-AE for molecular representation learning, an auto-encoder model using positional encoding as atomic identifiers. We also propose a new training objective named 3D Cloze Test to make the model learn better atom spatial relationships from real molecular substructures. Empirical results demonstrate that Mol-AE achieves a large margin performance gain compared to the current state-of-the-art 3D molecular modeling approach",
    "checked": true,
    "id": "1752894059292c54204f824ee1105f16ebdd640d",
    "semantic_title": "mol-ae: auto-encoder based molecular representation learning with 3d cloze test objective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SLqdDWwibH": {
    "title": "Few-Shot Unsupervised Implicit Neural Shape Representation Learning with Spatial Adversaries",
    "volume": "poster",
    "abstract": "Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from sparse 3D point clouds in the absence of ground truth supervision remains a very challenging task. While recent methods rely on smoothness priors to regularize the learning, our method introduces a regularization term that leverages adversarial samples around the shape to improve the learned SDFs. Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve SDF learning with respect to baselines and the state-of-the-art using synthetic and real data",
    "checked": false,
    "id": "dd31ab689bbae4dc4c4e5f9d71fbed4e509a8eac",
    "semantic_title": "visual imitation learning of task-oriented object grasping and rearrangement",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UZZaWUR0n4": {
    "title": "Active Ranking and Matchmaking, with Perfect Matchings",
    "volume": "poster",
    "abstract": "We address the challenge of actively ranking a set of items/players with varying values/strengths. The comparison outcomes are random, with a greater noise the closer the values. A crucial requirement is that, at each iteration of the algorithm, all items must be compared once, i.e., an iteration is a perfect matching. Furthermore, we presume that comparing two players with closely matched strengths incurs no cost and, in contrast, a unit cost is associated with comparing players whose strength difference is more substantial. Our secondary objective is to determine an optimal matching between players based on this cost function: we propose and analyze an algorithm that draws on concepts from both AKS sorting networks and bandit theory. Our algorithm achieves both objectives with high probability, and the total cost is optimal (up to logarithmic terms)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDofzHLuX4": {
    "title": "Estimating Distributional Treatment Effects in Randomized Experiments: Machine Learning for Variance Reduction",
    "volume": "poster",
    "abstract": "We propose a novel regression adjustment method designed for estimating distributional treatment effect parameters in randomized experiments. Randomized experiments have been extensively used to estimate treatment effects in various scientific fields. However, to gain deeper insights, it is essential to estimate distributional treatment effects rather than relying solely on average effects. Our approach incorporates pre-treatment covariates into a distributional regression framework, utilizing machine learning techniques to improve the precision of distributional treatment effect estimators. The proposed approach can be readily implemented with off-the-shelf machine learning methods and remains valid as long as the nuisance components are reasonably well estimated. Also, we establish the asymptotic properties of the proposed estimator and present a uniformly valid inference method. Through simulation results and real data analysis, we demonstrate the effectiveness of integrating machine learning techniques in reducing the variance of distributional treatment effect estimators in finite samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wUgTnf918v": {
    "title": "An Interpretable Evaluation of Entropy-based Novelty of Generative Models",
    "volume": "poster",
    "abstract": "The massive developments of generative model frameworks require principled methods for the evaluation of a model's novelty compared to a reference dataset. While the literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a reference model has not been adequately explored in the machine learning community. In this work, we focus on the novelty assessment for multi-modal distributions and attempt to address the following differential clustering task: Given samples of a generative model $P_\\mathcal{G}$ and a reference model $P_\\mathrm{ref}$, how can we discover the sample types expressed by $P_\\mathcal{G}$ more frequently than in $P_\\mathrm{ref}$? We introduce a spectral approach to the differential clustering task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty of $P_\\mathcal{G}$ with respect to $P_\\mathrm{ref}$. We analyze the KEN score for mixture distributions with well-separable components and develop a kernel-based method to compute the KEN score from empirical data. We support the KEN framework by presenting numerical results on synthetic and real image datasets, indicating the framework's effectiveness in detecting novel modes and comparing generative models. The paper's code is available at: github.com/buyeah1109/KEN",
    "checked": true,
    "id": "d2a158cab21f49745a929210e1bf07830884d578",
    "semantic_title": "an interpretable evaluation of entropy-based novelty of generative models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=L8nSGvoyvb": {
    "title": "Relaxed Quantile Regression: Prediction Intervals for Asymmetric Noise",
    "volume": "poster",
    "abstract": "Constructing valid prediction intervals rather than point estimates is a well-established approach for uncertainty quantification in the regression setting. Models equipped with this capacity output an interval of values in which the ground truth target will fall with some prespecified probability. This is an essential requirement in many real-world applications where simple point predictions' inability to convey the magnitude and frequency of errors renders them insufficient for high-stakes decisions. Quantile regression is a leading approach for obtaining such intervals via the empirical estimation of quantiles in the (non-parametric) distribution of outputs. This method is simple, computationally inexpensive, interpretable, assumption-free, and effective. However, it does require that the specific quantiles being learned are chosen a priori. This results in (a) intervals that are arbitrarily symmetric around the median which is sub-optimal for realistic skewed distributions, or (b) learning an excessive number of intervals. In this work, we propose Relaxed Quantile Regression (RQR), a direct alternative to quantile regression based interval construction that removes this arbitrary constraint whilst maintaining its strengths. We demonstrate that this added flexibility results in intervals with an improvement in desirable qualities (e.g. mean width) whilst retaining the essential coverage guarantees of quantile regression",
    "checked": true,
    "id": "fc3c66b3a41c8720b71ff75b7a58d00fd013e50b",
    "semantic_title": "relaxed quantile regression: prediction intervals for asymmetric noise",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3McL91pE6x": {
    "title": "How Flawed Is ECE? An Analysis via Logit Smoothing",
    "volume": "poster",
    "abstract": "Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction. By far the most common method in the literature for measuring calibration is the expected calibration error (ECE). Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors. In this work, we ask: how fundamental are these issues, and what are their impacts on existing results? Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces. We then use the nature of these discontinuities to motivate a novel *continuous, easily estimated* miscalibration metric, which we term *Logit-Smoothed ECE (LS-ECE)*. By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical pathologies of ECE may be avoidable in practice",
    "checked": true,
    "id": "684f6205f83f4cbf8985d2b0e6dbfd4391567f3d",
    "semantic_title": "how flawed is ece? an analysis via logit smoothing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzqmqZS0nj": {
    "title": "Can Machines Learn the True Probabilities?",
    "volume": "poster",
    "abstract": "When there exists uncertainty, AI machines are designed to make decisions so as to reach the best expected outcomes. Expectations are based on true facts about the objective environment the machines interact with, and those facts can be encoded into AI models in the form of true objective probability functions. Accordingly, AI models involve probabilistic machine learning in which the probabilities should be objectively interpreted. We prove under some basic assumptions when machines can learn the true objective probabilities, if any, and when machines cannot learn them",
    "checked": false,
    "id": "98ad27e2f4060e9bd7aed02b6facf9557edcba08",
    "semantic_title": "when do support vector machines learn fast ?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=WSpPC1Jm0p": {
    "title": "Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions",
    "volume": "poster",
    "abstract": "The increasing complexity of foundational models underscores the necessity for explainability, particularly for fine-tuning, the most widely used training method for adapting models to downstream tasks. Instance attribution, one type of explanation, attributes the model prediction to each training example by an instance score. However, the robustness of instance scores, specifically towards dataset resampling, has been overlooked. To bridge this gap, we propose a notion of robustness on the sign of the instance score. We theoretically and empirically demonstrate that the popular leave-one-out-based methods lack robustness, while the Shapley value behaves significantly better, but at a higher computational cost. Accordingly, we introduce an efficient fine-tuning-free approximation of the Shapley value (FreeShap) for instance attribution based on the neural tangent kernel. We empirically demonstrate that FreeShap outperforms other methods for instance attribution and other data-centric applications such as data removal, data selection, and wrong label detection, and further generalize our scale to large language models (LLMs). Our code is available at https://github.com/JTWang2000/FreeShap",
    "checked": true,
    "id": "ab6b11235338ca66328be4d34e9ac3ffdbb2d5a1",
    "semantic_title": "helpful or harmful data? fine-tuning-free shapley attribution for explaining language model predictions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LGz7GaUSEB": {
    "title": "A Hierarchical Adaptive Multi-Task Reinforcement Learning Framework for Multiplier Circuit Design",
    "volume": "poster",
    "abstract": "Multiplier design---which aims to explore a large combinatorial design space to simultaneously optimize multiple conflicting objectives---is a fundamental problem in the integrated circuits industry. Although traditional approaches tackle the multi-objective multiplier optimization problem by manually designed heuristics, reinforcement learning (RL) offers a promising approach to discover high-speed and area-efficient multipliers. However, the existing RL-based methods struggle to find Pareto-optimal circuit designs for all possible preferences, i.e., weights over objectives, in a sample-efficient manner. To address this challenge, we propose a novel hierarchical adaptive (HAVE) multi-task reinforcement learning framework. The hierarchical framework consists of a meta-agent to generate diverse multiplier preferences, and an adaptive multi-task agent to collaboratively optimize multipliers conditioned on the dynamic preferences given by the meta-agent. To the best of our knowledge, HAVE is the first to well approximate Pareto-optimal circuit designs for the entire preference space with high sample efficiency. Experiments on multipliers across a wide range of input widths demonstrate that HAVE significantly Pareto-dominates state-of-the-art approaches, achieving up to 28% larger hypervolume. Moreover, experiments demonstrate that multipliers designed by HAVE can well generalize to large-scale computation-intensive circuits",
    "checked": true,
    "id": "ad272e77928dfce82ffb8739ee03afd9e0355668",
    "semantic_title": "a hierarchical adaptive multi-task reinforcement learning framework for multiplier circuit design",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IxZ4xaHSYG": {
    "title": "Effects of Exponential Gaussian Distribution on (Double Sampling) Randomized Smoothing",
    "volume": "poster",
    "abstract": "Randomized Smoothing (RS) is currently a scalable certified defense method providing robustness certification against adversarial examples. Although significant progress has been achieved in providing defenses against $\\ell_p$ adversaries, the interaction between the smoothing distribution and the robustness certification still remains vague. In this work, we comprehensively study the effect of two families of distributions, named Exponential Standard Gaussian (ESG) and Exponential General Gaussian (EGG) distributions, on Randomized Smoothing and Double Sampling Randomized Smoothing (DSRS). We derive an analytic formula for ESG's certified radius, which converges to the origin formula of RS as the dimension $d$ increases. Additionally, we prove that EGG can provide tighter constant factors than DSRS in providing $\\Omega(\\sqrt{d})$ lower bounds of $\\ell_2$ certified radius, and thus further addresses the curse of dimensionality in RS. Our experiments on real-world datasets confirm our theoretical analysis of the ESG distributions, that they provide almost the same certification under different exponents $\\eta$ for both RS and DSRS. In addition, EGG brings a significant improvement to the DSRS certification, but the mechanism can be different when the classifier properties are different. Compared to the primitive DSRS, the increase in certified accuracy provided by EGG is prominent, up to 6.4% on ImageNet",
    "checked": true,
    "id": "49652d893dcda4fd986bfc5501e4a61b0427f9a2",
    "semantic_title": "effects of exponential gaussian distribution on (double sampling) randomized smoothing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JndWnomyIc": {
    "title": "FRAPPÉ: A Group Fairness Framework for Post-Processing Everything",
    "volume": "poster",
    "abstract": "Despite achieving promising fairness-error trade-offs, in-processing mitigation techniques for group fairness cannot be employed in numerous practical applications with limited computation resources or no access to the training pipeline of the prediction model. In these situations, post-processing is a viable alternative. However, current methods are tailored to specific problem settings and fairness definitions and hence, are not as broadly applicable as in-processing. In this work, we propose a framework that turns any regularized in-processing method into a post-processing approach. This procedure prescribes a way to obtain post-processing techniques for a much broader range of problem settings than the prior post-processing literature. We show theoretically and through extensive experiments that our framework preserves the good fairness-error trade-offs achieved with in-processing and can improve over the effectiveness of prior post-processing methods. Finally, we demonstrate several advantages of a modular mitigation strategy that disentangles the training of the prediction model from the fairness mitigation, including better performance on tasks with partial group labels",
    "checked": false,
    "id": "98925332209004fd8d9a732e3e39bddb139a85ac",
    "semantic_title": "frappe: a group fairness framework for post-processing everything",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=piecKJ2DlB": {
    "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
    "volume": "poster",
    "abstract": "The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents---it can successfully complete 51.1% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out to be not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML structure and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement. All code, data, and evaluation tools are available at https://github.com/OSU-NLP-Group/SeeAct",
    "checked": true,
    "id": "c844694387a89a477e7a8bbf918171cdc3b85672",
    "semantic_title": "gpt-4v(ision) is a generalist web agent, if grounded",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=IejxxE9DO2": {
    "title": "A Neural-Guided Dynamic Symbolic Network for Exploring Mathematical Expressions from Data",
    "volume": "poster",
    "abstract": "Symbolic regression (SR) is a powerful technique for discovering the underlying mathematical expressions from observed data. Inspired by the success of deep learning, recent deep generative SR methods have shown promising results. However, these methods face difficulties in processing high-dimensional problems and learning constants due to the large search space, and they don't scale well to unseen problems. In this work, we propose DySymNet, a novel neural-guided **Dy**namic **Sym**bolic **Net**work for SR. Instead of searching for expressions within a large search space, we explore symbolic networks with various structures, guided by reinforcement learning, and optimize them to identify expressions that better-fitting the data. Based on extensive numerical experiments on low-dimensional public standard benchmarks and the well-known SRBench with more variables, DySymNet shows clear superiority over several representative baseline models. Open source code is available at https://github.com/AILWQ/DySymNet",
    "checked": true,
    "id": "96558b4ff44919fc1b09d51bdcb9f469f0c768ad",
    "semantic_title": "a neural-guided dynamic symbolic network for exploring mathematical expressions from data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=yh6Y7ppf46": {
    "title": "Accelerating Legacy Numerical Solvers by Non-intrusive Gradient-based Meta-solving",
    "volume": "poster",
    "abstract": "Scientific computing is an essential tool for scientific discovery and engineering design, and its computational cost is always a main concern in practice. To accelerate scientific computing, it is a promising approach to use machine learning (especially meta-learning) techniques for selecting hyperparameters of traditional numerical methods. There have been numerous proposals to this direction, but many of them require automatic-differentiable numerical methods. However, in reality, many practical applications still depend on well-established but non-automatic-differentiable legacy codes, which prevents practitioners from applying the state-of-the-art research to their own problems. To resolve this problem, we propose a non-intrusive methodology with a novel gradient estimation technique to combine machine learning and legacy numerical codes without any modification. We theoretically and numerically show the advantage of the proposed method over other baselines and present applications of accelerating established non-automatic-differentiable numerical solvers implemented in PETSc, a widely used open-source numerical software library",
    "checked": true,
    "id": "5caa0a22e7f5ef4b3d8788e92a18746ef2f38449",
    "semantic_title": "accelerating legacy numerical solvers by non-intrusive gradient-based meta-solving",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lwTshcWlmB": {
    "title": "Degeneration-free Policy Optimization: RL Fine-Tuning for Language Models without Degeneration",
    "volume": "poster",
    "abstract": "As the pre-training objectives (e.g., next token prediction) of language models (LMs) are inherently not aligned with task scores, optimizing LMs to achieve higher downstream task scores is essential. One of the promising approaches is to fine-tune LMs through reinforcement learning (RL). However, conventional RL methods based on PPO and a penalty of KL divergence are vulnerable to text degeneration where LMs do not generate natural texts anymore after RL fine-tuning. To address this problem, we provide Degeneration-free Policy Optimization (DfPO) that can fine-tune LMs to generate texts that achieve improved downstream task scores, while preserving the ability to generate natural texts. To achieve this, we introduce KL-masking which masks out the actions that potentially cause deviation from the reference policy when its likelihood is increased or decreased. Then, we devise truncated advantage functions for separately performing likelihood maximization and minimization to improve the task performance. In the experiments, we provide the results of DfPO and baseline algorithms on various generative NLP tasks including text continuation, text detoxification, and commonsense generation. Our experiments demonstrate that DfPO successfully improves the downstream task scores while preserving the ability to generate natural texts, without requiring additional hyperparameter search",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yOe5lqDPvM": {
    "title": "RODEO: Robust Outlier Detection via Exposing Adaptive Out-of-Distribution Samples",
    "volume": "poster",
    "abstract": "In recent years, there have been significant improvements in various forms of image outlier detection. However, outlier detection performance under adversarial settings lags far behind that in standard settings. This is due to the lack of effective exposure to adversarial scenarios during training, especially on unseen outliers, leading detection models failing to learn robust features. To bridge this gap, we introduce RODEO, a data-centric approach that generates effective outliers for robust outlier detection. More specifically, we show that incorporating outlier exposure (OE) and adversarial training could be an effective strategy for this purpose, as long as the exposed training outliers meet certain characteristics, including diversity, and both conceptual differentiability and analogy to the inlier samples. We leverage a text-to-image model to achieve this goal. We demonstrate both quantitatively and qualitatively that our adaptive OE method effectively generates ''diverse'' and ''near-distribution'' outliers, leveraging information from both text and image domains. Moreover, our experimental results show that utilizing our synthesized outliers significantly enhances the performance of the outlier detector, particularly in adversarial settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QZd3rvlP76": {
    "title": "Polynomial-based Self-Attention for Table Representation Learning",
    "volume": "poster",
    "abstract": "Structured data, which constitutes a significant portion of existing data types, has been a long-standing research topic in the field of machine learning. Various representation learning methods for tabular data have been proposed, ranging from encoder-decoder structures to Transformers. Among these, Transformer-based methods have achieved state-of-the-art performance not only in tabular data but also in various other fields, including computer vision and natural language processing. However, recent studies have revealed that self-attention, a key component of Transformers, can lead to an oversmoothing issue. We show that Transformers for tabular data also face this problem. To tackle the problem, we suggest a novel self-attention layer for tabular data, leveraging matrix polynomials. This proposed layer serves as a replacement for the original self-attention layer, contributing to the improvement of model scalability. In our experiments with three representative table learning models equipped with our proposed layer, we illustrate that the layer effectively mitigates the oversmoothing problem and enhances the representation performance of the existing methods, outperforming the state-of-the-art table representation methods",
    "checked": true,
    "id": "f86ea57bb32217d15671d1bee975ea4c7ca67f17",
    "semantic_title": "polynomial-based self-attention for table representation learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JXGusc7E2": {
    "title": "See More Details: Efficient Image Super-Resolution by Experts Mining",
    "volume": "poster",
    "abstract": "Reconstructing high-resolution (HR) images from low-resolution (LR) inputs poses a significant challenge in image super-resolution (SR). While recent approaches have demonstrated the efficacy of intricate operations customized for various objectives, the straightforward stacking of these disparate operations can result in a substantial computational burden, hampering their practical utility. In response, we introduce SeemoRe, an efficient SR model employing expert mining. Our approach strategically incorporates experts at different levels, adopting a collaborative methodology. At the macro scale, our experts address rank-wise and spatial-wise informative features, providing a holistic understanding. Subsequently, the model delves into the subtleties of rank choice by leveraging a mixture of low-rank experts. By tapping into experts specialized in distinct key factors crucial for accurate SR, our model excels in uncovering intricate intra-feature details. This collaborative approach is reminiscent of the concept of ``see more\", allowing our model to achieve an optimal performance with minimal computational costs in efficient settings",
    "checked": true,
    "id": "6a8460c412106da8250d117dddf2c5aa47b75f6d",
    "semantic_title": "see more details: efficient image super-resolution by experts mining",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mUT1biz09t": {
    "title": "Privacy-Preserving Instructions for Aligning Large Language Models",
    "volume": "poster",
    "abstract": "Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna",
    "checked": true,
    "id": "f648d5bff46b180c633c812c71aa1cfafd7576dc",
    "semantic_title": "privacy-preserving instructions for aligning large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=tQPkzTdaaN": {
    "title": "PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have shown success in many natural language processing tasks. Despite rigorous safety alignment processes, supposedly safety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to jailbreaks, leading to security risks and abuse of the models. One option to mitigate such risks is to augment the LLM with a dedicated \"safeguard\", which checks the LLM's inputs or outputs for undesired behaviour. A promising approach is to use the LLM itself as the safeguard. Nonetheless, baseline methods, such as prompting the LLM to self-classify toxic content, demonstrate limited efficacy. We hypothesise that this is due to domain shift: the alignment training imparts a self-censoring behaviour to the model (\"Sorry I can't do that\"), while the self-classify approach shifts it to a classification format (\"Is this prompt malicious\"). In this work, we propose PARDEN, which avoids this domain shift by simply asking the model to repeat its own outputs. PARDEN neither requires finetuning nor white box access to the model. We empirically verify the effectiveness of our method and show that PARDEN significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2. We find that PARDEN is particularly powerful in the relevant regime of high True Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for Llama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in the FPR from 24.8% to 2.0% on the harmful behaviours dataset. Code and data are available at https://github.com/Ed-Zh/PARDEN",
    "checked": true,
    "id": "f7ff29cf1a4e954ce43a060df61a995ec58c2442",
    "semantic_title": "parden, can you repeat that? defending against jailbreaks via repetition",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ycXo4tQIpN": {
    "title": "Learning Shadow Variable Representation for Treatment Effect Estimation under Collider Bias",
    "volume": "poster",
    "abstract": "One of the significant challenges in treatment effect estimation is collider bias, a specific form of sample selection bias induced by the common causes of both the treatment and outcome. Identifying treatment effects under collider bias requires well-defined shadow variables in observational data, which are assumed to be related to the outcome and independent of the sample selection mechanism, conditional on the other observed variables. However, finding a valid shadow variable is not an easy task in real-world scenarios and requires domain-specific knowledge from experts. Therefore, in this paper, we propose a novel method that can automatically learn shadow-variable representations from observational data without prior knowledge. To ensure the learned representations satisfy the assumptions of the shadow variable, we introduce a tester to perform hypothesis testing in the representation learning process. We iteratively generate representations and test whether they satisfy the shadow-variable assumptions until they pass the test. With the help of the learned shadow-variable representations, we propose a novel treatment effect estimator to address collider bias. Experiments show that the proposed methods outperform existing treatment effect estimation methods under collider bias and prove their potential application value",
    "checked": false,
    "id": "5cd83a86584065c82837cd9e73fc844fb2b2e18e",
    "semantic_title": "causal estimation for text data with (apparent) overlap violations",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=8VEGkphQaK": {
    "title": "Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
    "volume": "poster",
    "abstract": "Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. To unravel the underlying mechanisms of stepwise inference we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. We find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy trade-off in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon",
    "checked": true,
    "id": "5c4c0b687ae98e3292a1cdf3772b105213dad7af",
    "semantic_title": "towards an understanding of stepwise inference in transformers: a synthetic graph navigation model",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xSkIxKdO08": {
    "title": "CF-OPT: Counterfactual Explanations for Structured Prediction",
    "volume": "poster",
    "abstract": "Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications. Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver. Our goal is to improve the transparency of such methods by providing counterfactual explanations. We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations. We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context. These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures. Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature",
    "checked": true,
    "id": "e0e350dd181bfb4928706ff4033ca45d40e3d15b",
    "semantic_title": "cf-opt: counterfactual explanations for structured prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WofwaWjIf7": {
    "title": "Cross-domain Open-world Discovery",
    "volume": "poster",
    "abstract": "In many real-world applications, test data may commonly exhibit categorical shifts, characterized by the emergence of novel classes, as well as distribution shifts arising from feature distributions different from the ones the model was trained on. However, existing methods either discover novel classes in the open-world setting or assume domain shifts without the ability to discover novel classes. In this work, we consider a cross-domain open-world discovery setting, where the goal is to assign samples to seen classes and discover unseen classes under a domain shift. To address this challenging problem, we present CROW, a prototype-based approach that introduces a cluster-then-match strategy enabled by a well-structured representation space of foundation models. In this way, CROW discovers novel classes by robustly matching clusters with previously seen classes, followed by fine-tuning the representation space using an objective designed for cross-domain open-world discovery. Extensive experimental results on image classification benchmark datasets demonstrate that CROW outperforms alternative baselines, achieving an 8% average performance improvement across 75 experimental settings",
    "checked": true,
    "id": "07e85641dc05f82437a3da921ebd3cfc36344212",
    "semantic_title": "cross-domain open-world discovery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rSfzchjIYu": {
    "title": "Effective Federated Graph Matching",
    "volume": "poster",
    "abstract": "Graph matching in the setting of federated learning is still an open problem. This paper proposes an unsupervised federated graph matching algorithm, UFGM, for inferring matched node pairs on different graphs across clients while maintaining privacy requirement, by leveraging graphlet theory and trust region optimization. First, the nodes' graphlet features are captured to generate pseudo matched node pairs on different graphs across clients as pseudo training data for tackling the dilemma of unsupervised graph matching in federated setting and leveraging the strength of supervised graph matching. An approximate graphlet enumeration method is proposed to sample a small number of graphlets and capture nodes' graphlet features. Theoretical analysis is conducted to demonstrate that the approximate method is able to maintain the quality of graphlet estimation while reducing its expensive cost. Second, we propose a separate trust region algorithm for pseudo supervised federated graph matching while maintaining the privacy constraints. In order to avoid expensive cost of the second-order Hessian computation in the trust region algorithm, we propose two weak quasi-Newton conditions to construct a positive definite scalar matrix as the Hessian approximation with only first-order gradients. We theoretically derive the error introduced by the separate trust region due to the Hessian approximation and conduct the convergence analysis of the approximation method",
    "checked": false,
    "id": "cd743889c9c9c0c202ee07adcb946b765db4cf19",
    "semantic_title": "multi-robot active mapping via neural bipartite graph matching",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=q0lxAs5GGO": {
    "title": "Disentanglement Learning via Topology",
    "volume": "poster",
    "abstract": "We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding a multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art methods are based on VAE and encourage the joint distribution of latent variables to be factorized. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement learning. Our experiments have shown that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score, and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality. Our method works in an unsupervised manner, permitting us to apply it to problems without labeled factors of variation. The TopDis loss works even when factors of variation are correlated. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN",
    "checked": true,
    "id": "d7ff83cb989e5bdf7b9de766e235c37394614189",
    "semantic_title": "disentanglement learning via topology",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7rTbqkKvA6": {
    "title": "Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks",
    "volume": "poster",
    "abstract": "Metallic Glasses (MGs) are widely used materials that are stronger than steel while being shapeable as plastic. While understanding the structure-property relationship of MGs remains a challenge in materials science, studying their energy barriers (EBs) as an intermediary step shows promise. In this work, we utilize Graph Neural Networks (GNNs) to model MGs and study EBs. We contribute a new dataset for EB prediction and a novel Symmetrized GNN (SymGNN) model that is E(3)-invariant in expectation. SymGNN handles invariance by aggregating over orthogonal transformations of the graph structure. When applied to EB prediction, SymGNN are more accurate than molecular dynamics (MD) local-sampling methods and other machine-learning models. Compared to precise MD simulations, SymGNN reduces the inference time on new MGs from roughly **41 days** to **less than one second**. We apply explanation algorithms to reveal the relationship between structures and EBs. The structures that we identify through explanations match the medium-range order (MRO) hypothesis and possess unique topological properties. Our work enables effective prediction and interpretation of MG EBs, bolstering material science research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jP8mf34iCW": {
    "title": "Training Greedy Policy for Proposal Batch Selection in Expensive Multi-Objective Combinatorial Optimization",
    "volume": "poster",
    "abstract": "Active learning is increasingly adopted for expensive multi-objective combinatorial optimization problems, but it involves a challenging subset selection problem, optimizing the batch acquisition score that quantifies the goodness of a batch for evaluation. Due to the excessively large search space of the subset selection problem, prior methods optimize the batch acquisition on the latent space, which has discrepancies with the actual space, or optimize individual acquisition scores without considering the dependencies among candidates in a batch instead of directly optimizing the batch acquisition. To manage the vast search space, a simple and effective approach is the greedy method, which decomposes the problem into smaller subproblems, yet it has difficulty in parallelization since each subproblem depends on the outcome from the previous ones. To this end, we introduce a novel greedy-style subset selection algorithm that optimizes batch acquisition directly on the combinatorial space by sequential greedy sampling from the greedy policy, specifically trained to address all greedy subproblems concurrently. Notably, our experiments on the red fluorescent proteins design task show that our proposed method achieves the baseline performance in 1.69x fewer queries, demonstrating its efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vw4Yar2fmW": {
    "title": "Self-Consistency Training for Density-Functional-Theory Hamiltonian Prediction",
    "volume": "poster",
    "abstract": "Predicting the mean-field Hamiltonian matrix in density functional theory is a fundamental formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose self-consistency training, an exact training method that does not require labeled data. It distinguishes the task from predicting other molecular properties by the following benefits: (1) it enables the model to be trained on a large amount of unlabeled data, hence addresses the data scarcity challenge and enhances generalization; (2) it is more efficient than running DFT to generate labels for supervised training, since it amortizes DFT calculation over a set of queries. We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios, and the better efficiency over DFT labeling. These benefits push forward the applicability of Hamiltonian prediction to an ever-larger scale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GktjBAGgo4": {
    "title": "Reducing Balancing Error for Causal Inference via Optimal Transport",
    "volume": "poster",
    "abstract": "Most studies on causal inference tackle the issue of confounding bias by reducing the distribution shift between the control and treated groups. However, it remains an open question to adopt an appropriate metric for distribution shift in practice. In this paper, we define a generic balancing error on reweighted samples to characterize the confounding bias, and study the connection between the balancing error and the Wasserstein discrepancy derived from the theory of optimal transport. We not only regard the Wasserstein discrepancy as the metric of distribution shift, but also explore the association between the balancing error and the underlying cost function involved in the Wasserstein discrepancy. Motivated by this, we propose to reduce the balancing error under the framework of optimal transport with learnable marginal distributions and the cost function, which is implemented by jointly learning weights and representations associated with factual outcomes. The experiments on both synthetic and real-world datasets demonstrate the effectiveness of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mphq2jMFLZ": {
    "title": "Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective",
    "volume": "poster",
    "abstract": "In this paper, we study the feature learning ability of two-layer neural networks in the mean-field regime through the lens of kernel methods. To focus on the dynamics of the kernel induced by the first layer, we utilize a two-timescale limit, where the second layer moves much faster than the first layer. In this limit, the learning problem is reduced to the minimization problem over the intrinsic kernel. Then, we show the global convergence of the mean-field Langevin dynamics and derive time and particle discretization error. We also demonstrate that two-layer neural networks can learn a union of multiple reproducing kernel Hilbert spaces more efficiently than any kernel methods, and neural networks aquire data-dependent kernel which aligns with the target function. In addition, we develop a label noise procedure, which converges to the global optimum and show that the degrees of freedom appears as an implicit reguralization",
    "checked": true,
    "id": "10172d5172d5e2c8255b5f6311d63912c08a937d",
    "semantic_title": "mean-field analysis on two-layer neural networks from a kernel perspective",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=znz261CQK7": {
    "title": "Bias of Stochastic Gradient Descent or the Architecture: Disentangling the Effects of Overparameterization of Neural Networks",
    "volume": "poster",
    "abstract": "Neural networks typically generalize well when fitting the data perfectly, even though they are heavily overparameterized. Many factors have been pointed out as the reason for this phenomenon, including an implicit bias of stochastic gradient descent (SGD) and a possible simplicity bias arising from the neural network architecture. The goal of this paper is to disentangle the factors that influence generalization stemming from optimization and architectural choices by studying *random* and *SGD-optimized* networks that achieve zero training error. We experimentally show, in the low sample regime, that overparameterization in terms of increasing width is beneficial for generalization, and this benefit is due to the bias of SGD and not due to an architectural bias. In contrast, for increasing depth, overparameterization is detrimental for generalization, but random and SGD-optimized networks behave similarly, so this can be attributed to an architectural bias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pvg1OdUtDQ": {
    "title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models",
    "volume": "poster",
    "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form optimal solution with reasonable modeling capacity. Despite the success, several challenges arise when apply NADO to a wide range of scenarios. Vanilla NADO suffers from gradient vanishing for low-probability control signals and is highly reliant on a regularization to satisfy the stochastic version of Bellman equation. In addition, the vanilla implementation of NADO introduces a few additional transformer layers, suffering from a limited capacity especially compared to other finetune-based model adaptation methods like LoRA. In this paper, we propose a improved version of the NADO algorithm, namely DiNADO (norm-**Di**sentangled **N**eur**A**lly-**D**ecomposed **O**racles), which improves the performance of the NADO algorithm through disentangling the step-wise global norm over the approximated oracle $R$-value for all potential next-tokens, allowing DiNADO to be combined with finetuning methods like LoRA. We discuss in depth how DiNADO achieves better capacity, stability and flexibility with both empirical and theoretical results. Experiments on formality control in machine translation and the lexically constrained generation task CommonGen demonstrates the significance of the improvements",
    "checked": true,
    "id": "ef4bd399a66f6e2b4a0b6bfe2aa2063eaef10375",
    "semantic_title": "dinado: norm-disentangled neurally-decomposed oracles for controlling language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bOhzU7NpTB": {
    "title": "Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference",
    "volume": "poster",
    "abstract": "Sound and complete algorithms have been proposed to compute identifiable causal queries using the causal structure and data. However, most of these algorithms assume accurate estimation of the data distribution, which is impractical for high-dimensional variables such as images. On the other hand, modern deep generative architectures can be trained to sample from high-dimensional distributions. However, training these networks are typically very costly. Thus, it is desirable to leverage pre-trained models to answer causal queries using such high-dimensional data. To address this, we propose modular training of deep causal generative models that not only makes learning more efficient, but also allows us to utilize large, pre-trained conditional generative models. To the best of our knowledge, our algorithm, Modular-DCM is the first algorithm that, given the causal structure, uses adversarial training to learn the network weights, and can make use of pre-trained models to provably sample from any identifiable causal query in the presence of latent confounders. With extensive experiments on the Colored-MNIST dataset, we demonstrate that our algorithm outperforms the baselines. We also show our algorithm's convergence on the COVIDx dataset and its utility with a causal invariant prediction problem on CelebA-HQ",
    "checked": true,
    "id": "53f7ceee9fae76e7d3b446f8d64751adee832a24",
    "semantic_title": "modular learning of deep causal generative models for high-dimensional causal inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FVvf69a5rx": {
    "title": "MOMENT: A Family of Open Time-series Foundation Models",
    "volume": "poster",
    "abstract": "We introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on [Huggingface](https://huggingface.co/AutonLab)",
    "checked": true,
    "id": "aabfdbd9db5ce9b1d598eae44e0d6250e7f0fc00",
    "semantic_title": "moment: a family of open time-series foundation models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=ETNx4SekbY": {
    "title": "Observable Propagation: Uncovering Feature Vectors in Transformers",
    "volume": "poster",
    "abstract": "A key goal of current mechanistic interpretability research in NLP is to find *linear features* (also called \"feature vectors\") for transformers: directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data -- both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called \"observable propagation\" (in short: ObProp), for finding linear features used by transformer language models in computing a given task -- *using almost no data*. Our paradigm centers on the concept of \"observables\", linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors, including a similarity metric between feature vectors called the *coupling coefficient* which estimates the degree to which one feature's output correlates with another's. We use ObProp to perform extensive qualitative investigations into several tasks, including gendered occupational bias, political party prediction, and programming language detection. Our results suggest that ObProp surpasses traditional approaches for finding feature vectors in the low-data regime, and that ObProp can be used to better understand the mechanisms responsible for bias in large language models",
    "checked": true,
    "id": "cfc076ea80f8a900ddb32b9b6808069eef6e5bb1",
    "semantic_title": "observable propagation: uncovering feature vectors in transformers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=upO8FUwf92": {
    "title": "Towards Compositionality in Concept Learning",
    "volume": "poster",
    "abstract": "Concept-based interpretability methods offer a lens into the internals of foundation models by decomposing their embeddings into high-level concepts. These concept representations are most useful when they are *compositional*, meaning that the individual concepts compose to explain the full sample. We show that existing unsupervised concept extraction methods find concepts which are not compositional. To automatically discover compositional concept representations, we identify two salient properties of such representations, and propose Compositional Concept Extraction (CCE) for finding concepts which obey these properties. We evaluate CCE on five different datasets over image and text data. Our evaluation shows that CCE finds more compositional concept representations than baselines and yields better accuracy on four downstream classification tasks",
    "checked": true,
    "id": "b70bd92cc3aececef5efdb55302cd9c2dc46f7fd",
    "semantic_title": "towards compositionality in concept learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pEWAcejiU2": {
    "title": "Better & Faster Large Language Models via Multi-token Prediction",
    "volume": "poster",
    "abstract": "Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following $n$ tokens using $n$ independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12% more problems on Human Eval and 17% more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to $3\\times$ faster at inference, even with large batch sizes",
    "checked": true,
    "id": "b9c289f141c54ab465a885fb7be7936d98e8c853",
    "semantic_title": "better & faster large language models via multi-token prediction",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=qwQVV5R8Y7": {
    "title": "$S^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
    "volume": "poster",
    "abstract": "Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embedding space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maximizing the cosine similarity in the joint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as prompts to provide strong indicators (context) for time series that exhibit different temporal dynamics. With thorough empirical studies on multiple benchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve superior forecasting performance over state-of-the-art baselines. Furthermore, our ablation studies and visualizations verify the necessity of prompt learning informed by semantic space",
    "checked": false,
    "id": "acc8fba34f2cb8fe35f19ce1c9136bc583f56e5c",
    "semantic_title": "s2ip-llm: semantic space informed prompt learning with llm for time series forecasting",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=2T00oYk54P": {
    "title": "Explaining Graph Neural Networks via Structure-aware Interaction Index",
    "volume": "poster",
    "abstract": "The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods",
    "checked": true,
    "id": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
    "semantic_title": "explaining graph neural networks via structure-aware interaction index",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vq7ITv8a49": {
    "title": "Kernel Debiased Plug-in Estimation: Simultaneous, Automated Debiasing without Influence Functions for Many Target Parameters",
    "volume": "poster",
    "abstract": "When estimating target parameters in nonparametric models with nuisance parameters, substituting the unknown nuisances with nonparametric estimators can introduce \"plug-in bias.\" Traditional methods addressing this suboptimal bias-variance trade-off rely on the influence function (IF) of the target parameter. When estimating multiple target parameters, these methods require debiasing the nuisance parameter multiple times using the corresponding IFs, which poses analytical and computational challenges. In this work, we leverage the targeted maximum likelihood estimation (TMLE) framework to propose a novel method named kernel debiased plug-in estimation (KDPE). KDPE refines an initial estimate through regularized likelihood maximization steps, employing a nonparametric model based on reproducing kernel Hilbert spaces. We show that KDPE: (i) simultaneously debiases all pathwise differentiable target parameters that satisfy our regularity conditions, (ii) does not require the IF for implementation, and (iii) remains computationally tractable. We numerically illustrate the use of KDPE and validate our theoretical results",
    "checked": true,
    "id": "6fbfe27d17c7cd5e73c2be666099454e59aacc20",
    "semantic_title": "kernel debiased plug-in estimation: simultaneous, automated debiasing without influence functions for many target parameters",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t8WDBcegae": {
    "title": "The Computational Complexity of Finding Second-Order Stationary Points",
    "volume": "poster",
    "abstract": "Non-convex minimization problems are universally considered hard, and even guaranteeing that a computed solution is locally minimizing is known to be NP-hard. In this general context, our paper focuses on the problem of finding stationary points that satisfy an approximate second-order optimality condition, which serves to exclude strict saddles and other non-minimizing stationary points. Our main result is that the problem of finding approximate second-order stationary points (SOSPs) is PLS-complete, i.e., of the same complexity as the problem of finding first-order stationary points (FOSPs), thus resolving an open question in the field. In particular, our results imply that, under the widely believed complexity conjecture that PLS $\\neq$ FNP, finding approximate SOSPs in unconstrained domains is *easier* than in constrained domains, which is known to be NP-hard. This comes in stark contrast with earlier results which implied that, unless PLS = CLS, finding approximate FOSPs in unconstrained domains is *harder* than in constrained domains",
    "checked": false,
    "id": "e078bcd3764ea9a621e038ff3e96c5a0e72317d5",
    "semantic_title": "reducing communication in federated learning with a novel single-loop variance reduction method",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0mklK4h0rX": {
    "title": "Exact Soft Analytical Side-Channel Attacks using Tractable Circuits",
    "volume": "poster",
    "abstract": "Detecting weaknesses in cryptographic algorithms is of utmost importance for designing secure information systems. The state-of-the-art *soft analytical side-channel attack* (SASCA) uses physical leakage information to make probabilistic predictions about intermediate computations and combines these \"guesses\" with the known algorithmic logic to compute the posterior distribution over the key. This attack is commonly performed via loopy belief propagation, which, however, lacks guarantees in terms of convergence and inference quality. In this paper, we develop a fast and exact inference method for SASCA, denoted as ExSASCA, by leveraging knowledge compilation and tractable probabilistic circuits. When attacking the *Advanced Encryption Standard* (AES), the most widely used encryption algorithm to date, ExSASCA outperforms SASCA by more than 31% top-1 success rate absolute. By leveraging sparse belief messages, this performance is achieved with little more computational cost than SASCA, and about 3 orders of magnitude less than exact inference via exhaustive enumeration. Even with dense belief messages, ExSASCA still uses 6 times less computations than exhaustive inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=If4xW9vF7U": {
    "title": "Training-Free Long-Context Scaling of Large Language Models",
    "volume": "poster",
    "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose a training-free approach named Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of up to 100k tokens. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of models built through continual training. All code and data used in this work are released at https://github.com/HKUNLP/ChunkLlama",
    "checked": true,
    "id": "cf7ab5df804575bad88a9fcf0fbf7707bf500944",
    "semantic_title": "training-free long-context scaling of large language models",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=SAbL40d8A4": {
    "title": "Winner-takes-all learners are geometry-aware conditional density estimators",
    "volume": "poster",
    "abstract": "Winner-takes-all training is a simple learning paradigm, which handles ambiguous tasks by predicting a set of plausible hypotheses. Recently, a connection was established between Winner-takes-all training and centroidal Voronoi tessellations, showing that, once trained, hypotheses should quantize optimally the shape of the conditional distribution to predict. However, the best use of these hypotheses for uncertainty quantification is still an open question. In this work, we show how to leverage the appealing geometric properties of the Winner-takes-all learners for conditional density estimation, without modifying its original training scheme. We theoretically establish the advantages of our novel estimator both in terms of quantization and density estimation, and we demonstrate its competitiveness on synthetic and real-world datasets, including audio data",
    "checked": true,
    "id": "7e03e69e0e3f26ad2ed82dde7aaa234cc6c50925",
    "semantic_title": "winner-takes-all learners are geometry-aware conditional density estimators",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LZeixIvQcB": {
    "title": "TabLog: Test-Time Adaptation for Tabular Data Using Logic Rules",
    "volume": "poster",
    "abstract": "We consider the problem of test-time adaptation of predictive models trained on tabular data. Effective solution of this problem requires adaptation of predictive models trained on the source domain to a target domain, using only unlabeled target domain data, without access to source domain data. Existing test-time adaptation methods for tabular data have difficulty coping with the heterogeneous features and their complex dependencies inherent in tabular data. To overcome these limitations, we consider test-time adaptation in the setting wherein the logical structure of the rules is assumed to remain invariant despite distribution shift between source and target domains whereas the numerical parameters associated with the rules and the weights assigned to them can vary to accommodate distribution shift. TabLog discretizes numerical features, models dependencies between heterogeneous features, introduces a novel contrastive loss for coping with distribution shift, and presents an end-to-end framework for efficient training and test-time adaptation by taking advantage of a logical neural network representation of a rule ensemble. We present results of experiments using several benchmark data sets that demonstrate TabLog is competitive with or improves upon the state-of-the-art methods for test-time adaptation of predictive models trained on tabular data. Our code is available at https://github.com/WeijieyingRen/TabLog",
    "checked": true,
    "id": "7264f5e5599e8b04b5b1c683adaadbcce6b76f8a",
    "semantic_title": "tablog: test-time adaptation for tabular data using logic rules",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=l0OGoZPZuC": {
    "title": "Semantically-correlated memories in a dense associative model",
    "volume": "poster",
    "abstract": "I introduce a novel associative memory model named *Correlated Dense Associative Memory* (CDAM), which integrates both auto- and hetero-association in a unified framework for continuous-valued memory patterns. Employing an arbitrary graph structure to semantically link memory patterns, CDAM is theoretically and numerically analysed, revealing four distinct dynamical modes: auto-association, narrow hetero-association, wide hetero-association, and neutral quiescence. Drawing inspiration from inhibitory modulation studies, I employ anti-Hebbian learning rules to control the range of hetero-association, extract multi-scale representations of community structures in graphs, and stabilise the recall of temporal sequences. Experimental demonstrations showcase CDAM's efficacy in handling real-world data, replicating a classical neuroscience experiment, performing image retrieval, and simulating arbitrary finite automata",
    "checked": true,
    "id": "b44e41fcaaead1b50f20fda0c78d395b51002f7f",
    "semantic_title": "semantically-correlated memories in a dense associative model",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=f6QenZyyeP": {
    "title": "How Deep Do We Need: Accelerating Training and Inference of Neural ODEs via Control Perspective",
    "volume": "poster",
    "abstract": "Neural Ordinary Differential Equations (ODEs) have shown promise in learning continuous dynamics. However, their slow training and inference speed hinder wider applications. In this paper, we propose to optimize Neural ODEs from a spatial and temporal perspective, drawing inspiration from control theory. We aim to find a reasonable depth of the network, accelerating both training and inference while maintaining network performance. Two approaches are proposed. One reformulates training as a minimum-time optimal control problem directly in a single stage to search for the terminal time and network weights. The second approach uses pre-training coupled with a Lyapunov method in an initial stage, and then at a secondary stage introduces a safe terminal time updating mechanism in the forward direction. Experimental results demonstrate the effectiveness of speeding up Neural ODEs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TN3fi7dwPo": {
    "title": "Tandem Transformers for Inference Efficient LLMs",
    "volume": "poster",
    "abstract": "The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative (Leviathan et al., 2023) and parallel (Stern et al., 2018) decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations. We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the Tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model. This ensures that the tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy",
    "checked": true,
    "id": "f5c83f5156904bdf92ff2f169871e320394f7c0a",
    "semantic_title": "tandem transformers for inference efficient llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=1V50J0emll": {
    "title": "Physics and Lie symmetry informed Gaussian processes",
    "volume": "poster",
    "abstract": "Physics-informed machine learning (PIML) has established itself as a new scientific paradigm which enables the seamless integration of observational data with partial differential equation (PDE) based physics models. A powerful tool for the analysis, reduction and solution of PDEs is the Lie symmetry method. Nevertheless, only recently has the integration of such symmetries into PIML frameworks begun to be explored. The present work adds to this growing literature by introducing an approach for incorporating a Lie symmetry into a physics-informed Gaussian process (GP) model. The symmetry is introduced as a constraint on the GP; either in a soft manner via virtual observations of an induced PDE called the invariant surface condition, or explicitly through the design of the kernel. Experimental results demonstrate that the use of symmetry constraints improves the performance of the GP for both forward and inverse problems, and that our approach offers competitive performance with neural networks in the low-data environment",
    "checked": false,
    "id": "c3588c4ada39c8db4738289c4cbc36025ce952f8",
    "semantic_title": "symmetry-regularized neural ordinary differential equations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xgrey8uQhr": {
    "title": "Graph Structure Extrapolation for Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "Out-of-distribution (OOD) generalization deals with the prevalent learning scenario where test distribution shifts from training distribution. With rising application demands and inherent complexity, graph OOD problems call for specialized solutions. While data-centric methods exhibit performance enhancements on many generic machine learning tasks, there is a notable absence of data augmentation methods tailored for graph OOD generalization. In this work, we propose to achieve graph OOD generalization with the novel design of non-Euclidean-space linear extrapolation. The proposed augmentation strategy extrapolates structure spaces to generate OOD graph data. Our design tailors OOD samples for specific shifts without corrupting underlying causal mechanisms. Theoretical analysis and empirical results evidence the effectiveness of our method in solving target shifts, showing substantial and constant improvements across various graph OOD tasks",
    "checked": false,
    "id": "33538d46152a3f1c7bd220a75ad78075a1570b83",
    "semantic_title": "graph structure and feature extrapolation for out-of-distribution generalization",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=CrUmgUaAQp": {
    "title": "Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs",
    "volume": "poster",
    "abstract": "Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter settings and difficult to optimize. We build on these results to offer insights into improving debating strategies, such as adjusting agent agreement levels, which can significantly enhance performance and even surpass all other non-debate protocols we evaluated. We provide an open-source repository to the community with several state-of-the-art protocols together with evaluation scripts to benchmark across popular research datasets",
    "checked": true,
    "id": "fce6367bb0a97efe1baded2ff311947e696caab2",
    "semantic_title": "should we be going mad? a look at multi-agent debate strategies for llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=09Robz3Ppy": {
    "title": "Optimal Transport for Structure Learning Under Missing Data",
    "volume": "poster",
    "abstract": "Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or, preferably, causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirically shown to be sub-optimal. To address this problem, we propose a score-based algorithm for learning causal structures from missing data based on optimal transport. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on expectation maximization. We formulate structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the observed data distribution. Our framework is shown to recover the true causal graphs more effectively than competing methods in most simulations and real-data settings. Empirical evidence also shows the superior scalability of our approach, along with the flexibility to incorporate any off-the-shelf causal discovery methods for complete data",
    "checked": true,
    "id": "76dbbb4fbb429a78fb9e7c1d198227c45a7c6c99",
    "semantic_title": "optimal transport for structure learning under missing data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=c18noxRh3X": {
    "title": "A3S: A General Active Clustering Method with Pairwise Constraints",
    "volume": "poster",
    "abstract": "Active clustering aims to boost the clustering performance by integrating human-annotated pairwise constraints through strategic querying. Conventional approaches with semi-supervised clustering schemes encounter high query costs when applied to large datasets with numerous classes. To address these limitations, we propose a novel Adaptive Active Aggregation and Splitting (A3S) framework, falling within the cluster-adjustment scheme in active clustering. A3S features strategic active clustering adjustment on the initial cluster result, which is obtained by an adaptive clustering algorithm. In particular, our cluster adjustment is inspired by the quantitative analysis of Normalized mutual information gain under the information theory framework and can provably improve the clustering quality. The proposed A3S framework significantly elevates the performance and scalability of active clustering. In extensive experiments across diverse real-world datasets, A3S achieves desired results with significantly fewer human queries compared with existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HbdeEGVfEN": {
    "title": "Deep Regression Representation Learning with Topology",
    "volume": "poster",
    "abstract": "Most works studying representation learning focus only on classification and neglect regression. Yet, the learning objectives and, therefore, the representation topologies of the two tasks are fundamentally different: classification targets class separation, leading to disconnected representations, whereas regression requires ordinality with respect to the target, leading to continuous representations. We thus wonder how the effectiveness of a regression representation is influenced by its topology, with evaluation based on the Information Bottleneck (IB) principle. The IB principle is an important framework that provides principles for learning effective representations. We establish two connections between it and the topology of regression representations. The first connection reveals that a lower intrinsic dimension of the feature space implies a reduced complexity of the representation $Z$. This complexity can be quantified as the conditional entropy of $Z$ on the target $Y$, and serves as an upper bound on the generalization error. The second connection suggests a feature space that is topologically similar to the target space will better align with the IB principle. Based on these two connections, we introduce PH-Reg, a regularizer specific to regression that matches the intrinsic dimension and topology of the feature space with the target space. Experiments on synthetic and real-world regression tasks demonstrate the benefits of PH-Reg. Code: https://github.com/needylove/PH-Reg",
    "checked": true,
    "id": "bada96f9376c3e555d75f0e0b57c00b71784833a",
    "semantic_title": "deep regression representation learning with topology",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UW5nO9NGjt": {
    "title": "Rethinking Momentum Knowledge Distillation in Online Continual Learning",
    "volume": "poster",
    "abstract": "Online Continual Learning (OCL) addresses the problem of training neural networks on a continuous data stream where multiple classification tasks emerge in sequence. In contrast to offline Continual Learning, data can be seen only once in OCL, which is a very severe constraint. In this context, replay-based strategies have achieved impressive results and most state-of-the-art approaches heavily depend on them. While Knowledge Distillation (KD) has been extensively used in offline Continual Learning, it remains under-exploited in OCL, despite its high potential. In this paper, we analyze the challenges in applying KD to OCL and give empirical justifications. We introduce a direct yet effective methodology for applying Momentum Knowledge Distillation (MKD) to many flagship OCL methods and demonstrate its capabilities to enhance existing approaches. In addition to improving existing state-of-the-art accuracy by more than $10\\%$ points on ImageNet100, we shed light on MKD internal mechanics and impacts during training in OCL. We argue that similar to replay, MKD should be considered a central component of OCL. The code is available at https://github.com/Nicolas1203/mkd_ocl",
    "checked": true,
    "id": "4f36298b0bcda6da0c1e98f6dcb918af1b2f0645",
    "semantic_title": "rethinking momentum knowledge distillation in online continual learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D5IRvFF1lN": {
    "title": "Learning-Efficient Yet Generalizable Collaborative Filtering for Item Recommendation",
    "volume": "poster",
    "abstract": "The weighted squared loss is a common component in several Collaborative Filtering (CF) algorithms for item recommendation, including the representative implicit Alternating Least Squares (iALS). Despite its widespread use, this loss function lacks a clear connection to ranking objectives such as Discounted Cumulative Gain (DCG), posing a fundamental challenge in explaining the exceptional ranking performance observed in these algorithms. In this work, we make a breakthrough by establishing a connection between squared loss and ranking metrics through a Taylor expansion of the DCG-consistent surrogate loss—softmax loss. We also discover a new surrogate squared loss function, namely Ranking-Generalizable Squared (RG$^2$) loss, and conduct thorough theoretical analyses on the DCG-consistency of the proposed loss function. Later, we present an example of utilizing the RG$^2$ loss with Matrix Factorization (MF), coupled with a generalization upper bound and an ALS optimization algorithm that leverages closed-form solutions over all items. Experimental results over three public datasets demonstrate the effectiveness of the RG$^2$ loss, exhibiting ranking performance on par with, or even surpassing, the softmax loss while achieving faster convergence",
    "checked": false,
    "id": "19d276ff6389bedd3d369673adcfa9a169cff934",
    "semantic_title": "adversarial collaborative filtering for free",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=pTFud6SetK": {
    "title": "SelMatch: Effectively Scaling Up Dataset Distillation via Selection-Based Initialization and Partial Updates by Trajectory Matching",
    "volume": "poster",
    "abstract": "Dataset distillation aims to synthesize a small number of images per class (IPC) from a large dataset to approximate full dataset training with minimal performance loss. While effective in very small IPC ranges, many distillation methods become less effective, even underperforming random sample selection, as IPC increases. Our examination of state-of-the-art trajectory-matching based distillation methods across various IPC scales reveals that these methods struggle to incorporate the complex, rare features of harder samples into the synthetic dataset even with the increased IPC, resulting in a persistent coverage gap between easy and hard test samples. Motivated by such observations, we introduce SelMatch, a novel distillation method that effectively scales with IPC. SelMatch uses selection-based initialization and partial updates through trajectory matching to manage the synthetic dataset's desired difficulty level tailored to IPC scales. When tested on CIFAR-10/100 and TinyImageNet, SelMatch consistently outperforms leading selection-only and distillation-only methods across subset ratios from 5% to 30%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vdr87ZUfnl": {
    "title": "Reinforcement Learning and Regret Bounds for Admission Control",
    "volume": "poster",
    "abstract": "The expected regret of any reinforcement learning algorithm is lower bounded by $\\Omega\\left(\\sqrt{DXAT}\\right)$ for undiscounted returns, where $D$ is the diameter of the Markov decision process, $X$ the size of the state space, $A$ the size of the action space and $T$ the number of time steps. However, this lower bound is general. A smaller regret can be obtained by taking into account some specific knowledge of the problem structure. In this article, we consider an admission control problem to an $M/M/c/S$ queue with $m$ job classes and class-dependent rewards and holding costs. Queuing systems often have a diameter that is exponential in the buffer size $S$, making the previous lower bound prohibitive for any practical use. We propose an algorithm inspired by UCRL2, and use the structure of the problem to upper bound the expected total regret by $O(S\\log T + \\sqrt{mT \\log T})$ in the finite server case. In the infinite server case, we prove that the dependence of the regret on $S$ disappears",
    "checked": true,
    "id": "13b89c5975c4ab5afd58b67b388d01e11c71c0ca",
    "semantic_title": "reinforcement learning and regret bounds for admission control",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cBWVJh5Fvf": {
    "title": "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids complex program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks including HumanEval and MBPP. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our code and model are publicly available at https://github.com/gonglinyuan/ast_t5",
    "checked": true,
    "id": "fbd1b4f09b19bd23c16b54525347c6643bb06322",
    "semantic_title": "ast-t5: structure-aware pretraining for code generation and understanding",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=0tuwdgBiSN": {
    "title": "Complexity Matters: Feature Learning in the Presence of Spurious Correlations",
    "volume": "poster",
    "abstract": "Existing research often posits spurious features as **easier** to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored. Moreover, studies mainly focus on end performance rather than the learning dynamics of feature learning. In this paper, we propose a theoretical framework and an associated synthetic dataset grounded in boolean function analysis. This setup allows for fine-grained control over the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlations. Our findings uncover several interesting phenomena: (1) stronger spurious correlations or simpler spurious features slow down the learning rate of the core features, (2) two distinct subnetworks are formed to learn core and spurious features separately, (3) learning phases of spurious and core features are not always separable, (4) spurious features are not forgotten even after core features are fully learned. We demonstrate that our findings justify the success of retraining the last layer to remove spurious correlation and also identifies limitations of popular debiasing algorithms that exploit early learning of spurious features. We support our empirical findings with theoretical analyses for the case of learning XOR features with a one-hidden-layer ReLU network",
    "checked": false,
    "id": "3d28faf650805a7eecb0435658de6940b8288559",
    "semantic_title": "complexity matters: dynamics of feature learning in the presence of spurious correlations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mxjB0LIgpT": {
    "title": "Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?",
    "volume": "poster",
    "abstract": "Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-order loss minimization, and the relative (rather than absolute) nature of epistemic uncertainty measures",
    "checked": true,
    "id": "a3d1eec0988634cf6486165e36cf74ead3048a1e",
    "semantic_title": "is epistemic uncertainty faithfully represented by evidential deep learning methods?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=DwwI9L67B5": {
    "title": "State-Free Inference of State-Space Models: The *Transfer Function* Approach",
    "volume": "poster",
    "abstract": "We approach designing a state-space model for deep learning applications through its dual representation, the *transfer function*, and uncover a highly efficient sequence parallel inference algorithm that is *state-free*: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size. We achieve this using properties of the proposed frequency domain transfer function parametrization, which enables direct computation of its corresponding convolutional kernel's spectrum via a single Fast Fourier Transform. Our experimental results across multiple sequence lengths and state sizes illustrates, on average, a 35% training speed improvement over S4 layers -- parametrized in time-domain -- on the Long Range Arena benchmark, while delivering state-of-the-art downstream performances over other attention-free approaches. Moreover, we report improved perplexity in language modeling over a long convolutional Hyena baseline, by simply introducing our transfer function parametrization. Our code is available at https://github.com/ruke1ire/RTF",
    "checked": false,
    "id": "101914972192c4fa0c7eab6ffc72f13d3ae1dde0",
    "semantic_title": "state-free inference of state-space models: the transfer function approach",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wK9RvVmi7u": {
    "title": "Understanding Heterophily for Graph Neural Networks",
    "volume": "poster",
    "abstract": "Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of heterophily for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Our theoretical investigation comprehensively analyze the impact of heterophily from three critical aspects. Firstly, for the impact of different heterophily patterns, we show that the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\\sqrt{\\mathbb{E}\\left[\\operatorname{deg}\\right]}$, where $\\mathbb{E}\\left[\\operatorname{deg}\\right]$ is the averaged node degree. Secondly, we show that the neighborhood inconsistency has a detrimental impact on separability, which is similar to degrading $\\mathbb{E}\\left[\\operatorname{deg}\\right]$ by a specific factor. Finally, for the impact of stacking multiple layers, we show that the separability gains are determined by the normalized distance of the $l$-powered neighborhood distributions, indicating that nodes still possess separability in various regimes, even when over-smoothing occurs. Extensive experiments on both synthetic and real-world data verify the effectiveness of our theory",
    "checked": true,
    "id": "037d5596abc836d06cc8a3fcaba51bd782fafc76",
    "semantic_title": "understanding heterophily for graph neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=olbTrkWo1D": {
    "title": "Mitigating Catastrophic Forgetting in Online Continual Learning by Modeling Previous Task Interrelations via Pareto Optimization",
    "volume": "poster",
    "abstract": "Catastrophic forgetting remains a core challenge in continual learning (CL), where the models struggle to retain previous knowledge when learning new tasks. While existing replay-based CL methods have been proposed to tackle this challenge by utilizing a memory buffer to store data from previous tasks, they generally overlook the interdependence between previously learned tasks and fail to encapsulate the optimally integrated knowledge in previous tasks, leading to sub-optimal performance of the previous tasks. Against this issue, we first reformulate replay-based CL methods as a unified hierarchical gradient aggregation framework. We then incorporate the Pareto optimization to capture the interrelationship among previously learned tasks and design a Pareto-Optimized CL algorithm (POCL), which effectively enhances the overall performance of past tasks while ensuring the performance of the current task. Comprehensive empirical results demonstrate that the proposed POCL outperforms current state-of-the-art CL methods across multiple datasets and different settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XT6iF8FDZx": {
    "title": "Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States",
    "volume": "poster",
    "abstract": "In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Experiments corroborate our theory, and demonstrate its conclusions on problems beyond LQR, where systems are non-linear and controllers are neural networks. We hypothesize that real-world optimal control may be greatly improved by developing methods for informed selection of initial states to train on",
    "checked": true,
    "id": "602a7bae963affe7a028c0fd1d876de2efbc66a3",
    "semantic_title": "implicit bias of policy gradient in linear quadratic control: extrapolation to unseen initial states",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vxPmrxKe0J": {
    "title": "On the Hardness of Probabilistic Neurosymbolic Learning",
    "volume": "poster",
    "abstract": "The limitations of purely neural learning have sparked an interest in probabilistic neurosymbolic models, which combine neural networks with probabilistic logical reasoning. As these neurosymbolic models are trained with gradient descent, we study the complexity of differentiating probabilistic reasoning. We prove that although approximating these gradients is intractable in general, it becomes tractable during training. Furthermore, we introduce *WeightME*, an unbiased gradient estimator based on model sampling. Under mild assumptions, WeightME approximates the gradient with probabilistic guarantees using a logarithmic number of calls to a SAT solver. Lastly, we evaluate the necessity of these guarantees on the gradient. Our experiments indicate that the existing biased approximations indeed struggle to optimize even when exact solving is still feasible",
    "checked": true,
    "id": "fc3eb15b83fd7070d8f7ceca07d35ab4b21bcd40",
    "semantic_title": "on the hardness of probabilistic neurosymbolic learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y8NevOhrnW": {
    "title": "Neural Collapse in Multi-label Learning with Pick-all-label Loss",
    "volume": "poster",
    "abstract": "We study deep neural networks for the multi-label classification (MLab) task through the lens of neural collapse (NC). Previous works have been restricted to the multi-class classification setting and discovered a prevalent NC phenomenon comprising of the following properties for the last-layer features: (i) the variability of features within every class collapses to zero, (ii) the set of feature means form an equi-angular tight frame (ETF), and (iii) the last layer classifiers collapse to the feature mean upon some scaling. We generalize the study to multi-label learning, and prove for the first time that a generalized NC phenomenon holds with the \"pick-all-label'' formulation, which we term as MLab NC. While the ETF geometry remains consistent for features with a single label, multi-label scenarios introduce a unique combinatorial aspect we term the \"tag-wise average\" property, where the means of features with multiple labels are the scaled averages of means for single-label instances. Theoretically, under proper assumptions on the features, we establish that the only global optimizer of the pick-all-label cross-entropy loss satisfy the multi-label NC. In practice, we demonstrate that our findings can lead to better test performance with more efficient training techniques for MLab learning",
    "checked": true,
    "id": "7b4222baa75436f41735df0d7e212ebf8859653d",
    "semantic_title": "neural collapse in multi-label learning with pick-all-label loss",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=OJTKlubFk1": {
    "title": "Error Feedback Can Accurately Compress Preconditioners",
    "volume": "poster",
    "abstract": "Leveraging second-order information about the loss at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to small-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via a novel and efficient error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression before it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks show that this approach can compress full-matrix preconditioners to up to 99% sparsity without accuracy loss, effectively removing the memory overhead of fullmatrix preconditioners such as GGT and M-FAC",
    "checked": true,
    "id": "e01189d18f5ed5ac235c38aab82977e630c1546d",
    "semantic_title": "error feedback can accurately compress preconditioners",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=9kArQnKLDp": {
    "title": "CARTE: Pretraining and Transfer for Tabular Learning",
    "volume": "poster",
    "abstract": "Pretrained deep-learning models are the go-to solution for images or text. However, for tabular data the standard is still to train tree-based models. Indeed, transfer learning on tables hits the challenge of *data integration*: finding correspondences, correspondences in the entries (*entity matching*) where different words may denote the same entity, correspondences across columns (*schema matching*), which may come in different orders, names... We propose a neural architecture that does not need such correspondences. As a result, we can pretrain it on background data that has not been matched. The architecture --CARTE for Context Aware Representation of Table Entries-- uses a graph representation of tabular (or relational) data to process tables with different columns, string embedding of entries and columns names to model an open vocabulary, and a graph-attentional network to contextualize entries with column names and neighboring entries. An extensive benchmark shows that CARTE facilitates learning, outperforming a solid set of baselines including the best tree-based models. CARTE also enables joint learning across tables with unmatched columns, enhancing a small table with bigger ones. CARTE opens the door to large pretrained models for tabular data",
    "checked": true,
    "id": "659fe890e963c574c083f1b60754a071d945b5b2",
    "semantic_title": "carte: pretraining and transfer for tabular learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bZNH0SU37Y": {
    "title": "On the Recoverability of Causal Relations from Temporally Aggregated I.I.D. Data",
    "volume": "poster",
    "abstract": "We consider the effect of temporal aggregation on instantaneous (non-temporal) causal discovery in general setting. This is motivated by the observation that the true causal time lag is often considerably shorter than the observational interval. This discrepancy leads to high aggregation, causing time-delay causality to vanish and instantaneous dependence to manifest. Although we expect such instantaneous dependence has consistency with the true causal relation in certain sense to make the discovery results meaningful, it remains unclear what type of consistency we need and when will such consistency be satisfied. We proposed functional consistency and conditional independence consistency in formal way correspond functional causal model-based methods and conditional independence-based methods respectively and provide the conditions under which these consistencies will hold. We show theoretically and experimentally that causal discovery results may be seriously distorted by aggregation especially in complete nonlinear case and we also find causal relationship still recoverable from aggregated data if we have partial linearity or appropriate prior. Our findings suggest community should take a cautious and meticulous approach when interpreting causal discovery results from such data and show why and when aggregation will distort the performance of causal discovery methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S4LqI6CcJ3": {
    "title": "Be Your Own Neighborhood: Detecting Adversarial Examples by the Neighborhood Relations Built on Self-Supervised Learning",
    "volume": "poster",
    "abstract": "Deep Neural Networks (DNNs) are vulnerable to Adversarial Examples (AEs), hindering their use in safety-critical systems. In this paper, we present **BEYOND**, an innovative AE detection framework designed for reliable predictions. BEYOND identifies AEs by distinguishing the AE's abnormal relation with its augmented versions, i.e. neighbors, from two prospects: representation similarity and label consistency. An off-the-shelf Self-Supervised Learning (SSL) model is used to extract the representation and predict the label for its highly informative representation capacity compared to supervised learning models. We found clean samples maintain a high degree of representation similarity and label consistency relative to their neighbors, in contrast to AEs which exhibit significant discrepancies. We explain this observation and show that leveraging this discrepancy BEYOND can accurately detect AEs. Additionally, we develop a rigorous justification for the effectiveness of BEYOND. Furthermore, as a plug-and-play model, BEYOND can easily cooperate with the Adversarial Trained Classifier (ATC), achieving state-of-the-art (SOTA) robustness accuracy. Experimental results show that BEYOND outperforms baselines by a large margin, especially under adaptive attacks. Empowered by the robust relationship built on SSL, we found that BEYOND outperforms baselines in terms of both detection ability and speed. Project page: https://huggingface.co/spaces/allenhzy/Be-Your-Own-Neighborhood",
    "checked": false,
    "id": "e957d77852fc16ad12c1f4f838250339bcca1f52",
    "semantic_title": "be your own neighborhood: detecting adversarial example by the neighborhood relations built on self-supervised learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=OQ97v7uRGc": {
    "title": "On The Complexity of First-Order Methods in Stochastic Bilevel Optimization",
    "volume": "poster",
    "abstract": "We consider the problem of finding stationary points in Bilevel optimization when the lower-level problem is unconstrained and strongly convex. The problem has been extensively studied in recent years; the main technical challenge is to keep track of lower-level solutions $y^*(x)$ in response to the changes in the upper-level variables $x$. Subsequently, all existing approaches tie their analyses to a genie algorithm that knows lower-level solutions and, therefore, need not query any points far from them. We consider a dual question to such approaches: suppose we have an oracle, which we call $y^*$-aware, that returns an $O(\\epsilon)$-estimate of the lower-level solution, in addition to first-order gradient estimators *locally unbiased* within the $\\Theta(\\epsilon)$-ball around $y^*(x)$. We study the complexity of finding stationary points with such an $y^*$-aware oracle: we propose a simple first-order method that converges to an $\\epsilon$ stationary point using $O(\\epsilon^{-6}), O(\\epsilon^{-4})$ access to first-order $y^*$-aware oracles. Our upper bounds also apply to standard unbiased first-order oracles, improving the best-known complexity of first-order methods by $O(\\epsilon)$ with minimal assumptions. We then provide the matching $\\Omega(\\epsilon^{-6})$, $\\Omega(\\epsilon^{-4})$ lower bounds without and with an additional smoothness assumption, respectively. Our results imply that any approach that simulates an algorithm with an $y^*$-aware oracle must suffer the same lower bounds",
    "checked": true,
    "id": "e25ff4ec0cdb9a08bd8a01bf90b3a107665dfdb3",
    "semantic_title": "on the complexity of first-order methods in stochastic bilevel optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=WCwxFM7n5S": {
    "title": "Agnostic Interactive Imitation Learning: New Theory and Practical Algorithms",
    "volume": "poster",
    "abstract": "We study interactive imitation learning, where a learner interactively queries a demonstrating expert for action annotations, aiming to learn a policy that has performance competitive with the expert, using as few annotations as possible. We focus on the general agnostic setting where the expert demonstration policy may not be contained in the policy class used by the learner. We propose a new oracle-efficient algorithm MFTPL-P (abbreviation for Mixed Follow the Perturbed Leader with Poisson perturbations) with provable finite-sample guarantees, under the assumption that the learner is given access to samples from some ``explorative'' distribution over states. Our guarantees hold for any policy class, which is considerably broader than prior state of the art. We further propose Bootstrap-DAgger, a more practical variant that does not require additional sample access",
    "checked": false,
    "id": "7484ccffabe3ad27225200e274becd0f514ff7c1",
    "semantic_title": "dependency as modality, parsing as permutation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9CCoVyFuEp": {
    "title": "Loss Shaping Constraints for Long-Term Time Series Forecasting",
    "volume": "poster",
    "abstract": "Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show that despite its non-convexity, the resulting problem has a bounded duality gap. We propose a practical primal-dual algorithm to tackle it, and demonstrate that the proposed approach exhibits competitive average performance in time series forecasting benchmarks, while shaping the distribution of errors across the predicted window",
    "checked": true,
    "id": "8dbc20029d404d08d81cf5ad84466e623931bab9",
    "semantic_title": "loss shaping constraints for long-term time series forecasting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hunSEjeCPE": {
    "title": "Energy-Guided Diffusion Sampling for Offline-to-Online Reinforcement Learning",
    "volume": "poster",
    "abstract": "Combining offline and online reinforcement learning (RL) techniques is indeed crucial for achieving efficient and safe learning where data acquisition is expensive. Existing methods replay offline data directly in the online phase, resulting in a significant challenge of data distribution shift and subsequently causing inefficiency in online fine-tuning. To address this issue, we introduce an innovative approach, **E**nergy-guided **DI**ffusion **S**ampling (EDIS), which utilizes a diffusion model to extract prior knowledge from the offline dataset and employs energy functions to distill this knowledge for enhanced data generation in the online phase. The theoretical analysis demonstrates that EDIS exhibits reduced suboptimality compared to solely utilizing online data or directly reusing offline data. EDIS is a plug-in approach and can be combined with existing methods in offline-to-online RL setting. By implementing EDIS to off-the-shelf methods Cal-QL and IQL, we observe a notable 20% average improvement in empirical performance on MuJoCo, AntMaze, and Adroit environments. Code is available at https://github.com/liuxhym/EDIS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ytz2naZoDB": {
    "title": "Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process",
    "volume": "poster",
    "abstract": "Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effectively and efficiently train SDEs. We evaluate our algorithm on the task of structure-based drug design and optimize the binding affinity of generated ligand molecules. Our method achieves the best Vina score (-9.07) on the CrossDocked2020 dataset",
    "checked": true,
    "id": "9c8ae3055eaa52f89164552a7d4ba45a7e0deb7c",
    "semantic_title": "stabilizing policy gradients for stochastic differential equations via consistency with perturbation process",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aZnZOqUOHq": {
    "title": "Predicting Lagrangian Multipliers for Mixed Integer Linear Programs",
    "volume": "poster",
    "abstract": "Lagrangian Relaxation stands among the most efficient approaches for solving Mixed Integer Linear Programs (MILPs) with difficult constraints. Given any duals for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on the optimal value of the MILP, and Lagrangian methods seek the LMs giving the best such bound. But these methods generally rely on iterative algorithms resembling gradient descent to maximize the concave piecewise linear dual function: the computational burden grows quickly with the number of relaxed constraints. We introduce a deep learning approach that bypasses the descent, effectively amortizing per instance optimization. A probabilistic encoder based on a graph neural network computes, given a MILP instance and its Continuous Relaxation (CR) solution, high-dimensional representations of relaxed constraints, which are turned into LMs by a decoder. We train the encoder and the decoder jointly by directly optimizing the bound obtained from the predicted multipliers. Our method is applicable to any problem with a compact MILP formulation, and to any Lagrangian Relaxation providing a tighter bound than CR. Experiments on two widely known problems, Multi-Commodity Network Design and Generalized Assignment, show that our approach closes up to 85% of the gap between the continuous relaxation and the best Lagrangian bound, and provides a high-quality warm-start for descent-based Lagrangian methods",
    "checked": false,
    "id": "342ba6071938303b9dfd91c463e5a7b5aef0aeea",
    "semantic_title": "predicting accurate lagrangian multipliers for mixed integer linear programs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iJWeK2snMH": {
    "title": "Sampling-based Multi-dimensional Recalibration",
    "volume": "poster",
    "abstract": "Calibration of probabilistic forecasts in the regression setting has been widely studied in the single dimensional case, where the output variables are assumed to be univariate. In many problem settings, however, the output variables are multi-dimensional, and in the presence of dependence across the output dimensions, measuring calibration and performing recalibration for each dimension separately can be both misleading and detrimental. In this work, we focus on representing predictive uncertainties via samples, and propose a recalibration method which accounts for the joint distribution across output dimensions to produce calibrated samples. Based on the concept of highest density regions (HDR), we define the notion of HDR calibration, and show that our recalibration method produces samples which are HDR calibrated. We demonstrate the performance of our method and the quality of the recalibrated samples on a suite of benchmark datasets in multi-dimensional regression, a real-world dataset in modeling plasma dynamics during nuclear fusion reactions, and on a decision-making application in forecasting demand",
    "checked": false,
    "id": "89c5b4ec40fedadf544aff9ee6a13373d510bb96",
    "semantic_title": "optimal sampling designs for multi-dimensional streaming time series with application to power grid sensor data",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=hgHQvrvwH9": {
    "title": "Privacy Profiles for Private Selection",
    "volume": "poster",
    "abstract": "Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are fundamental primitives of differentially private (DP) data analysis with wide applications to private query release, voting, and hyperparameter tuning. Recent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made significant progress in both generalizing private selection mechanisms and tightening their privacy analysis using modern numerical privacy accounting tools, e.g., Rényi DP. But Rényi DP is known to be lossy when $(\\epsilon,\\delta)$-DP is ultimately needed, and there is a trend to close the gap by directly handling privacy profiles, i.e., $\\delta$ as a function of $\\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work out an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax and PrivateTuning using the privacy profiles of the base algorithms they corral. Numerically, our approach improves over the RDP-based accounting in all regimes of interest and leads to substantial benefits in end-to-end private learning experiments. Our analysis also suggests new distributions, e.g., binomial distribution for randomizing the number of rounds that leads to more substantial improvements in certain regimes",
    "checked": true,
    "id": "800e0fade67f77eb97b403686bb57c01f7fe9650",
    "semantic_title": "privacy profiles for private selection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rHylzxK3HU": {
    "title": "Restoring balance: principled under/oversampling of data for optimal classification",
    "volume": "poster",
    "abstract": "Class imbalance in real-world data poses a common bottleneck for machine learning tasks, since achieving good generalization on under-represented examples is often challenging. Mitigation strategies, such as under or oversampling the data depending on their abundances, are routinely proposed and tested empirically, but how they should adapt to the data statistics remains poorly understood. In this work, we determine exact analytical expressions of the generalization curves in the high-dimensional regime for linear classifiers (Support Vector Machines). We also provide a sharp prediction of the effects of under/oversampling strategies depending on class imbalance, first and second moments of the data, and the metrics of performance considered. We show that mixed strategies involving under and oversampling of data lead to performance improvement. Through numerical experiments, we show the relevance of our theoretical predictions on real datasets, on deeper architectures and with sampling strategies based on unsupervised probabilistic models",
    "checked": true,
    "id": "f873b65be770f42ee0874df84454f95044efc19b",
    "semantic_title": "restoring balance: principled under/oversampling of data for optimal classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qkhbyDqlNI": {
    "title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems",
    "volume": "poster",
    "abstract": "In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world. To this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively. Under this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting. Under proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning. Additionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret. As a remedy, we introduce an $\\epsilon$-greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small. Finally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors",
    "checked": true,
    "id": "cf5fb966719eec6e75ad5f75f4ecf8d7e8723bec",
    "semantic_title": "from words to actions: unveiling the theoretical underpinnings of llm-driven autonomous systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dWxb80a0TW": {
    "title": "Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning",
    "volume": "poster",
    "abstract": "Many processes in biology and drug discovery involve various 3D interactions between molecules, such as protein and protein, protein and small molecule, etc. Given that different molecules are usually represented in different granularity, existing methods usually encode each type of molecules independently with different models, leaving it defective to learn the various underlying interaction physics. In this paper, we first propose to universally represent an arbitrary 3D complex as a geometric graph of sets, shedding light on encoding all types of molecules with one model. We then propose a Generalist Equivariant Transformer (GET) to effectively capture both domain-specific hierarchies and domain-agnostic interaction physics. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where each module is E(3) equivariant and specialized for handling sets of variable sizes. Notably, in contrast to conventional pooling-based hierarchical models, our GET is able to retain fine-grained information of all levels. Extensive experiments on the interactions between proteins, small molecules and RNA/DNAs verify the effectiveness and generalization capability of our proposed method across different domains",
    "checked": true,
    "id": "3187b8fe34086d71135787ad12afe6eb8993ce1f",
    "semantic_title": "generalist equivariant transformer towards 3d molecular interaction learning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=srejp9uOx7": {
    "title": "ReLUs Are Sufficient for Learning Implicit Neural Representations",
    "volume": "poster",
    "abstract": "Motivated by the growing theoretical understanding of neural networks that employ the Rectified Linear Unit (ReLU) as their activation function, we revisit the use of ReLU activation functions for learning implicit neural representations (INRs). Inspired by second order B-spline wavelets, we incorporate a set of simple constraints to the ReLU neurons in each layer of a deep neural network (DNN) to remedy the spectral bias. This in turn enables its use for various INR tasks. Empirically, we demonstrate that, contrary to popular belief, one *can learn* state-of-the-art INRs based on a DNN composed of only ReLU neurons. Next, by leveraging recent theoretical works which characterize the kinds of functions ReLU neural networks learn, we provide a way to quantify the regularity of the learned function. This offers a principled approach to selecting the hyperparameters in INR architectures. We substantiate our claims through experiments in signal representation, super resolution, and computed tomography, demonstrating the versatility and effectiveness of our method. The code for all experiments can be found at https://github.com/joeshenouda/relu-inrs",
    "checked": true,
    "id": "f74c009a670fca3474b0d8369cc8a82ac5e98071",
    "semantic_title": "relus are sufficient for learning implicit neural representations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t8mt4YrPsq": {
    "title": "Larimar: Large Language Models with Episodic Memory Control",
    "volume": "poster",
    "abstract": "Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed---yielding speed-ups of 8-10x depending on the base LLM ---as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at https://github.com/IBM/larimar",
    "checked": true,
    "id": "5372f4364fe5b7991c81ea19ee944c717afe6e0a",
    "semantic_title": "larimar: large language models with episodic memory control",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y6y2HauOpR": {
    "title": "Roping in Uncertainty: Robustness and Regularization in Markov Games",
    "volume": "poster",
    "abstract": "We study robust Markov games (RMG) with $s$-rectangular uncertainty. We show a general equivalence between computing a robust Nash equilibrium (RNE) of a $s$-rectangular RMG and computing a Nash equilibrium (NE) of an appropriately constructed regularized MG. The equivalence result yields a planning algorithm for solving $s$-rectangular RMGs, as well as provable robustness guarantees for policies computed using regularized methods. However, we show that even for just reward-uncertain two-player zero-sum matrix games, computing an RNE is PPAD-hard. Consequently, we derive a special uncertainty structure called efficient player-decomposability and show that RNE for two-player zero-sum RMG in this class can be provably solved in polynomial time. This class includes commonly used uncertainty sets such as $L_1$ and $L_\\infty$ ball uncertainty sets",
    "checked": true,
    "id": "39c0ed8b0dde35e1dc2dde7f468cb88d291ba212",
    "semantic_title": "roping in uncertainty: robustness and regularization in markov games",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HDrXBr26UI": {
    "title": "Neuro-Symbolic Temporal Point Processes",
    "volume": "poster",
    "abstract": "Our goal is to $\\textit{efficiently}$ discover a compact set of temporal logic rules to explain irregular events of interest. We introduce a neural-symbolic rule induction framework within the temporal point process model. The negative log-likelihood is the loss that guides the learning, where the explanatory logic rules and their weights are learned end-to-end in a $\\textit{differentiable}$ way. Specifically, predicates and logic rules are represented as $\\textit{vector embeddings}$, where the predicate embeddings are fixed and the rule embeddings are trained via gradient descent to obtain the most appropriate compositional representations of the predicate embeddings. To make the rule learning process more efficient and flexible, we adopt a $\\textit{sequential covering algorithm}$, which progressively adds rules to the model and removes the event sequences that have been explained until all event sequences have been covered. All the found rules will be fed back to the models for a final rule embedding and weight refinement. Our approach showcases notable efficiency and accuracy across synthetic and real datasets, surpassing state-of-the-art baselines by a wide margin in terms of efficiency",
    "checked": true,
    "id": "b8000ce4385df5546433f5f67531285330be2aaa",
    "semantic_title": "neuro-symbolic temporal point processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H86WzfH5N1": {
    "title": "On the Trajectory Regularity of ODE-based Diffusion Sampling",
    "volume": "poster",
    "abstract": "Diffusion-based generative models use stochastic differential equations (SDEs) and their equivalent ordinary differential equations (ODEs) to establish a smooth connection between a complex data distribution and a tractable prior distribution. In this paper, we identify several intriguing trajectory properties in the ODE-based sampling process of diffusion models. We characterize an implicit denoising trajectory and discuss its vital role in forming the coupled sampling trajectory with a strong shape regularity, regardless of the generated content. We also describe a dynamic programming-based scheme to make the time schedule in sampling better fit the underlying trajectory structure. This simple strategy requires minimal modification to any given ODE-based numerical solvers and incurs negligible computational cost, while delivering superior performance in image generation, especially in $5\\sim 10$ function evaluations",
    "checked": true,
    "id": "48856ff99053bb9769c900c93aea2272c764cb30",
    "semantic_title": "on the trajectory regularity of ode-based diffusion sampling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9xUpLGAOy9": {
    "title": "Chain-of-Thought Predictive Control",
    "volume": "poster",
    "abstract": "We study generalizable policy learning from demonstrations for complex low-level control (e.g., contact-rich object manipulations). We propose a novel hierarchical imitation learning method that utilizes sub-optimal demos. Firstly, we propose an observation space-agnostic approach that efficiently discovers the multi-step subskill decomposition of the demos in an unsupervised manner. By grouping temporarily close and functionally similar actions into subskill-level demo segments, the observations at the segment boundaries constitute a chain of planning steps for the task, which we refer to as the chain-of-thought (CoT). Next, we propose a Transformer-based design that effectively learns to predict the CoT as the subskill-level guidance. We couple action and subskill predictions via learnable prompt tokens and a hybrid masking strategy, which enable dynamically updated guidance at test time and improve feature representation of the trajectory for generalizable policy learning. Our method, Chain-of-Thought Predictive Control (CoTPC), consistently surpasses existing strong baselines on various challenging low-level manipulation tasks with sub-optimal demos. See project page at https://sites.google.com/view/cotpc",
    "checked": true,
    "id": "e1bd151a3f670fd0f77580702fe7a85dc78a41cb",
    "semantic_title": "chain-of-thought predictive control",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=EYvEVbfoDp": {
    "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
    "volume": "poster",
    "abstract": "While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate HALC's effectiveness in reducing OH, outperforming state-of-the-arts across four benchmarks. Code is released at https://github.com/BillChan226/HALC",
    "checked": true,
    "id": "7751f6cdec0f4473c1733eec91699744a7d5176f",
    "semantic_title": "halc: object hallucination reduction via adaptive focal-contrast decoding",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=xB6YJZOKyT": {
    "title": "RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "In this paper, we propose an off-policy deep reinforcement learning (DRL) method utilizing the average reward criterion. While most existing DRL methods employ the discounted reward criterion, this can potentially lead to a discrepancy between the training objective and performance metrics in continuing tasks, making the average reward criterion a recommended alternative. We introduce RVI-SAC, an extension of the state-of-the-art off-policy DRL method, Soft Actor-Critic (SAC), to the average reward criterion. Our proposal consists of (1) Critic updates based on RVI Q-learning, (2) Actor updates introduced by the average reward soft policy improvement theorem, and (3) automatic adjustment of Reset Cost enabling the average reward reinforcement learning to be applied to tasks with termination. We apply our method to the Gymnasium's Mujoco tasks, a subset of locomotion tasks, and demonstrate that RVI-SAC shows competitive performance compared to existing methods",
    "checked": false,
    "id": "fe1c528f2074e4d976836d9e9c7e7fc1be6b0064",
    "semantic_title": "v2n service scaling with deep reinforcement learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=69RewQwWA9": {
    "title": "An Iterative Min-Min Optimization Method for Sparse Bayesian Learning",
    "volume": "poster",
    "abstract": "As a well-known machine learning algorithm, sparse Bayesian learning (SBL) can find sparse representations in linearly probabilistic models by imposing a sparsity-promoting prior on model coefficients. However, classical SBL algorithms lack the essential theoretical guarantees of global convergence. To address this issue, we propose an iterative Min-Min optimization method to solve the marginal likelihood function (MLF) of SBL based on the concave-convex procedure. The method can optimize the hyperparameters related to both the prior and noise level analytically at each iteration by re-expressing MLF using auxiliary functions. Particularly, we demonstrate that the method globally converges to a local minimum or saddle point of MLF. With rigorous theoretical guarantees, the proposed novel SBL algorithm outperforms classical ones in finding sparse representations on simulation and real-world examples, ranging from sparse signal recovery to system identification and kernel regression",
    "checked": false,
    "id": "3cf8226870872c87e3a3addfa444fd17de9cc8e6",
    "semantic_title": "extensible max-min collaborative retention for online mini-batch learning hash retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RZHRnnGcEx": {
    "title": "Let Go of Your Labels with Unsupervised Transfer",
    "volume": "poster",
    "abstract": "Foundation vision-language models have enabled remarkable zero-shot transferability of the pre-trained representations to a wide range of downstream tasks. However, to solve a new task, zero-shot transfer still necessitates human guidance to define visual categories that appear in the data. Here, we show that fully unsupervised transfer emerges when searching for the labeling of a dataset that induces maximal margin classifiers in representation spaces of different foundation models. We present TURTLE, a fully unsupervised method that effectively employs this guiding principle to uncover the underlying labeling of a downstream dataset without any supervision and task-specific representation learning. We evaluate TURTLE on a diverse benchmark suite of 26 datasets and show that it achieves new state-of-the-art unsupervised performance. Furthermore, TURTLE, although being fully unsupervised, outperforms zero-shot transfer baselines on a wide range of datasets. In particular, TURTLE matches the average performance of CLIP zero-shot on 26 datasets by employing the same representation space, spanning a wide range of architectures and model sizes. By guiding the search for the underlying labeling using the representation spaces of two foundation models, TURTLE surpasses zero-shot transfer and unsupervised prompt tuning baselines, demonstrating the surprising power and effectiveness of unsupervised transfer",
    "checked": true,
    "id": "e0c5f90d42d6809f6d8777b86da1ee037fedce59",
    "semantic_title": "let go of your labels with unsupervised transfer",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YiblhkVl2w": {
    "title": "Stability and Multigroup Fairness in Ranking with Uncertain Predictions",
    "volume": "poster",
    "abstract": "Rankings are ubiquitous across many applications, from search engines to hiring committees. In practice, many rankings are derived from the output of predictors. However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings. Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups. Not only is stability an important requirement for its own sake, but --- as we show --- it composes harmoniously with individual fairness in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al. (2021) are stable. Our main result is that UA rankings also achieve group fairness through successful composition with multiaccurate or multicalibrated predictors. Our work demonstrates that UA rankings naturally interpolate between group and individual level fairness guarantees, while simultaneously satisfying stability guarantees important whenever machine-learned predictions are used",
    "checked": true,
    "id": "f295354edaf8c3a9b166947c4f4ef23b7323a248",
    "semantic_title": "stability and multigroup fairness in ranking with uncertain predictions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Nxz3CDtGXp": {
    "title": "An Empirical Examination of Balancing Strategy for Counterfactual Estimation on Time Series",
    "volume": "poster",
    "abstract": "Counterfactual estimation from observations represents a critical endeavor in numerous application fields, such as healthcare and finance, with the primary challenge being the mitigation of treatment bias. The balancing strategy aimed at reducing covariate disparities between different treatment groups serves as a universal solution. However, when it comes to the time series data, the effectiveness of balancing strategies remains an open question, with a thorough analysis of the robustness and applicability of balancing strategies still lacking. This paper revisits counterfactual estimation in the temporal setting and provides a brief overview of recent advancements in balancing strategies. More importantly, we conduct a critical empirical examination for the effectiveness of the balancing strategies within the realm of temporal counterfactual estimation in various settings on multiple datasets. Our findings could be of significant interest to researchers and practitioners and call for a reexamination of the balancing strategy in time series settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VZ5A0LPbnc": {
    "title": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks",
    "volume": "poster",
    "abstract": "Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. In addition, we can control a model's behavior by finding codes that activate on a desired behavior, then activating those same codes during generation. We first validate codebook features on a finite state machine dataset with far more hidden states than neurons. In this setting, our approach overcomes the superposition problem by assigning states to distinct codes, and we find that we can make the neural network behave as if it is in a different state by activating the code for that state. We then train Transformer language models with up to 410M parameters on two natural language datasets. We identify codes in these models representing diverse, disentangled concepts (ranging from negative emotions to months of the year) and find that we can guide the model to generate different topics and pronoun genders by activating these codes during inference. Overall, codebook features appear to be a promising unit of analysis and control for neural networks and interpretability. Our codebase and models are open-sourced at [this URL](https://github.com/taufeeque9/codebook-features)",
    "checked": true,
    "id": "9f20b0aa426c18512e820d1c1311378f71114d8a",
    "semantic_title": "codebook features: sparse and discrete interpretability for neural networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=NZgbwzaOIx": {
    "title": "Revisit the Essence of Distilling Knowledge through Calibration",
    "volume": "poster",
    "abstract": "Knowledge Distillation (KD) has evolved into a practical technology for transferring knowledge from a well-performing model (teacher) to a weak model (student). A counter-intuitive phenomenon known as capacity mismatch has been identified, wherein KD performance may not be good when a better teacher instructs the student. Various preliminary methods have been proposed to alleviate capacity mismatch, but a unifying explanation for its cause remains lacking. In this paper, we propose *a unifying analytical framework to pinpoint the core of capacity mismatch based on calibration*. Through extensive analytical experiments, we observe a positive correlation between the calibration of the teacher model and the KD performance with original KD methods. As this correlation arises due to the sensitivity of metrics (e.g., KL divergence) to calibration, we recommend employing measurements insensitive to calibration such as ranking-based loss. Our experiments demonstrate that ranking-based loss can effectively replace KL divergence, aiding large models with poor calibration to teach better",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b9VfvegTEO": {
    "title": "Fine-grained Classes and How to Find Them",
    "volume": "poster",
    "abstract": "In many practical applications, coarse-grained labels are readily available compared to fine-grained labels that reflect subtle differences between classes. However, existing methods cannot leverage coarse labels to infer fine-grained labels in an unsupervised manner. To bridge this gap, we propose FALCON, a method that discovers fine-grained classes from coarsely labeled data without any supervision at the fine-grained level. FALCON simultaneously infers unknown fine-grained classes and underlying relationships between coarse and fine-grained classes. Moreover, FALCON is a modular method that can effectively learn from multiple datasets labeled with different strategies. We evaluate FALCON on eight image classification tasks and a single-cell classification task. FALCON outperforms baselines by a large margin, achieving 22% improvement over the best baseline on the tieredImageNet dataset with over 600 fine-grained classes",
    "checked": true,
    "id": "03ff17a9a989a78846b2899ed796926ca5352f76",
    "semantic_title": "fine-grained classes and how to find them",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mk8oRhox2l": {
    "title": "GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding",
    "volume": "poster",
    "abstract": "Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models",
    "checked": true,
    "id": "382d3b37f53fd6118ab979cb56f7f3d13eb0951d",
    "semantic_title": "glide with a cape: a low-hassle method to accelerate speculative decoding",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=dqpg8jdA2w": {
    "title": "Offline Transition Modeling via Contrastive Energy Learning",
    "volume": "poster",
    "abstract": "Learning a high-quality transition model is of great importance for sequential decision-making tasks, especially in offline settings. Nevertheless, the complex behaviors of transition dynamics in real-world environments pose challenges for the standard forward models because of their inductive bias towards smooth regressors, conflicting with the inherent nature of transitions such as discontinuity or large curvature. In this work, we propose to model the transition probability implicitly through a scalar-value energy function, which enables not only flexible distribution prediction but also capturing complex transition behaviors. The Energy-based Transition Models (ETM) are shown to accurately fit the discontinuous transition functions and better generalize to out-of-distribution transition data. Furthermore, we demonstrate that energy-based transition models improve the evaluation accuracy and significantly outperform other off-policy evaluation methods in DOPE benchmark. Finally, we show that energy-based transition models also benefit reinforcement learning and outperform prior offline RL algorithms in D4RL Gym-Mujoco tasks",
    "checked": false,
    "id": "10b28bd8e9a7c54d063a1647dd8f38dae48cea22",
    "semantic_title": "making linear mdps practical via contrastive representation learning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=DlR8fWgJRl": {
    "title": "Model-based Reinforcement Learning for Confounded POMDPs",
    "volume": "poster",
    "abstract": "We propose a model-based offline reinforcement learning (RL) algorithm for confounded partially observable Markov decision processes (POMDPs) under general function approximations and show it is provably efficient under some technical conditions such as the partial coverage imposed on the offline data distribution. Specifically, we first establish a novel model-based identification result for learning the effect of any action on the reward and future transitions in the confounded POMDP. Using this identification result, we then design a nonparametric two-stage estimation procedure to construct an estimator for off-policy evaluation (OPE), which permits general function approximations. Finally, we learn the optimal policy by performing a conservative policy optimization within the confidence regions based on the proposed estimation procedure for OPE. Under some mild conditions, we establish a finite-sample upper bound on the suboptimality of the learned policy in finding the optimal one, which depends on the sample size and the length of horizons polynomially",
    "checked": false,
    "id": "809a31eac1641002a626e1ce03c4be0f5e8d7079",
    "semantic_title": "on-robot bayesian reinforcement learning for pomdps",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=O4nXWHPl6g": {
    "title": "Enhancing Class-Imbalanced Learning with Pre-Trained Guidance through Class-Conditional Knowledge Distillation",
    "volume": "poster",
    "abstract": "In class-imbalanced learning, the scarcity of information about minority classes presents challenges in obtaining generalizable features for these classes. Leveraging large-scale pre-trained models with powerful generalization capabilities as teacher models can help fill this information gap. Traditional knowledge distillation transfers the label distribution $p(\\boldsymbol{y}|\\boldsymbol{x})$ predicted by the teacher model to the student model. However, this method falls short on imbalanced data as it fails to capture the class-conditional probability distribution $p(\\boldsymbol{x}|\\boldsymbol{y})$ from the teacher model, which is crucial for enhancing generalization. To overcome this, we propose Class-Conditional Knowledge Distillation (CCKD), a novel approach that enables learning of the teacher model's class-conditional probability distribution during the distillation process. Additionally, we introduce Augmented CCKD (ACCKD), which involves distillation on a constructed class-balanced dataset (formed through data mixing) and feature imitation on the entire dataset to further facilitate the learning of features. Experimental results on various imbalanced datasets demonstrate an average accuracy improvement of 7.4% using our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DbyHDYslM7": {
    "title": "BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization",
    "volume": "poster",
    "abstract": "Nowadays, Large Language Models (LLMs) mostly possess billions of parameters, bringing significant challenges to hardware platforms. Although quantization is an efficient approach to reduce computation and memory overhead for inference optimization, we stress the challenge that mainstream low-bit quantization approaches still suffer from either various data distribution outliers or a lack of hardware efficiency. We also find that low-bit data format has further potential expressiveness to cover the atypical language data distribution. In this paper, we propose a novel numerical representation, Bi-Exponent Block Floating Point (BiE), and a new quantization flow. BiE quantization shows accuracy superiority and hardware friendliness on various models and benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ie3vXkMvRY": {
    "title": "On the Unexpected Effectiveness of Reinforcement Learning for Sequential Recommendation",
    "volume": "poster",
    "abstract": "In recent years, Reinforcement Learning (RL) has shown great promise in session-based recommendation. Sequential models that use RL have reached state-of-the-art performance for the Next-item Prediction (NIP) task. This result is intriguing, as the NIP task only evaluates how well the system can correctly recommend the next item to the user, while the goal of RL is to find a policy that optimizes rewards in the long term -- sometimes at the expense of suboptimal short-term performance. Then, how can RL improve the system's performance on short-term metrics? This article investigates this question by exploring proxy learning objectives, which we identify as goals RL models might be following, and thus could explain the performance boost. We found that RL -- when used as an auxiliary loss -- promotes the learning of embeddings that capture information about the user's previously interacted items. Subsequently, we replaced the RL objective with a straightforward auxiliary loss designed to predict the number of items the user interacted with. This substitution results in performance gains comparable to RL. These findings pave the way to improve performance and understanding of RL methods for recommender systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IW45Dr1Kxi": {
    "title": "Quantum Positional Encodings for Graph Neural Networks",
    "volume": "poster",
    "abstract": "In this work, we propose novel families of positional encodings tailored to graph neural networks obtained with quantum computers. These encodings leverage the long-range correlations inherent in quantum systems that arise from mapping the topology of a graph onto interactions between qubits in a quantum computer. Our inspiration stems from the recent advancements in quantum processing units, which offer computational capabilities beyond the reach of classical hardware. We prove that some of these quantum features are theoretically more expressive for certain graphs than the commonly used relative random walk probabilities. Empirically, we show that the performance of state-of-the-art models can be improved on standard benchmarks and large-scale datasets by computing tractable versions of quantum features. Our findings highlight the potential of leveraging quantum computing capabilities to enhance the performance of transformers in handling graph data",
    "checked": true,
    "id": "acf083d4e040dd2799ea749e97386c742eee310a",
    "semantic_title": "quantum positional encodings for graph neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ia0Z8d1DbY": {
    "title": "Diffuse, Sample, Project: Plug-And-Play Controllable Graph Generation",
    "volume": "poster",
    "abstract": "Diffusion models lend transformative capabilities to the graph generation task, yet controlling the properties of the generated graphs remains challenging. Recent approaches augment support for controlling soft, differentiable properties but they fail to handle user-specified hard constraints that are non-differentiable. This often results in vague control, unsuitable for applications like drug discovery that demand satisfaction of precise constraints, e.g., the maximum number of bonds. To address this, we formalize the problem of controlled graph generation and introduce PRODIGY (PROjected DIffusion for controlled Graph Generation), an innovative plug-and-play approach enabling the generation of graphs with precise control, from any pre-trained diffusion model. PRODIGY employs a novel operator to project the samples at each diffusion step onto the specified constrained space. For a large class of practical constraints and a variety of graphs, our extensive experiments demonstrate that PRODIGY empowers state-of-the-art continuous and discrete diffusion models to produce graphs meeting specific, hard constraints. Our approach achieves up to 100% constraint satisfaction for non-attributed and molecular graphs, under a variety of constraints, marking a significant step forward in precise, interpretable graph generation. Code is provided on the project webpage: [https://prodigy-diffusion.github.io/](https://prodigy-diffusion.github.io/)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDz9leJ9iK": {
    "title": "Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities",
    "volume": "poster",
    "abstract": "The rise of foundation models holds immense promise for advancing AI, but this progress may amplify existing risks and inequalities, leaving marginalized communities behind. In this position paper, we discuss that disparities towards marginalized communities – performance, representation, privacy, robustness, interpretability and safety – are not isolated concerns but rather interconnected elements of a cascading disparity phenomenon. We contrast foundation models with traditional models and highlight the potential for exacerbated disparity against marginalized communities. Moreover, we emphasize the unique threat of cascading impacts in foundation models, where interconnected disparities can trigger long-lasting negative consequences, specifically to the people on the margin. We define marginalized communities within the machine learning context and explore the multifaceted nature of disparities. We analyze the sources of these disparities, tracing them from data creation, training and deployment procedures to highlight the complex technical and socio-technical landscape. To mitigate the pressing crisis, we conclude with a set of calls to action to mitigate disparity at its source",
    "checked": true,
    "id": "7a79d484cb01f988bc70a07d2f81e7407e07405c",
    "semantic_title": "position: cracking the code of cascading disparity towards marginalized communities",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zFHaB7KESM": {
    "title": "Guarantees for Nonlinear Representation Learning: Non-identical Covariates, Dependent Data, Fewer Samples",
    "volume": "poster",
    "abstract": "A driving force behind the diverse applicability of modern machine learning is the ability to extract meaningful features across many sources. However, many practical domains involve data that are non-identically distributed across sources, and possibly statistically dependent within its source, violating vital assumptions in existing theoretical studies of representation learning. Toward addressing these issues, we establish statistical guarantees for learning general *nonlinear* representations from multiple data sources that admit different input distributions and possibly dependent data. Specifically, we study the sample-complexity of learning $T+1$ functions $f_\\star^{(t)} \\circ g_\\star$ from a function class $\\mathcal{F} \\times \\mathcal{G}$, where $f_\\star^{(t)}$ are task specific linear functions and $g_\\star$ is a shared non-linear representation. An approximate representation $\\hat g$ is estimated using $N$ samples from each of $T$ source tasks, and a fine-tuning function $\\hat f^{(0)}$ is fit using $N'$ samples from a target task passed through $\\hat g$. Our results show that the excess risk of the estimate $\\hat f^{(0)} \\circ \\hat g$ on the target task decays as $\\tilde{\\mathcal{O}}\\Big(\\frac{\\mathrm{C}(\\mathcal{G})}{N T} + \\frac{\\text{dim}(\\mathcal{F})}{N'}\\Big)$, where $\\mathrm{C}(\\mathcal{G})$ denotes the complexity of $\\mathcal{G}$. Notably, our rates match that of the iid setting, while requiring fewer samples per task than prior analysis and admitting *no dependence on the mixing time*. We support our analysis with numerical experiments performing imitation learning over non-linear dynamical systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ULleq1Dtaw": {
    "title": "Towards General Algorithm Discovery for Combinatorial Optimization: Learning Symbolic Branching Policy from Bipartite Graph",
    "volume": "poster",
    "abstract": "Machine learning (ML) approaches have been successfully applied to accelerating exact combinatorial optimization (CO) solvers. However, many of them fail to explain what patterns they have learned that accelerate the CO algorithms due to the black-box nature of ML models like neural networks, and thus they prevent researchers from further understanding the tasks they are interested in. To tackle this problem, we propose the *first* graph-based algorithm discovery framework---namely, graph symbolic discovery for exact combinatorial optimization solver (GS4CO)---that learns interpretable branching policies directly from the *general* bipartite graph representation of CO problems. Specifically, we design a unified representation for symbolic policies with graph inputs, and then we employ a Transformer with multiple tree-structural encodings to generate symbolic trees end-to-end, which effectively reduces the cumulative error from iteratively distilling graph neural networks. Experiments show that GS4CO learned interpretable and lightweight policies outperform all the baselines on CPU machines, including both the human-designed and the learning-based. GS4CO shows an encouraging step towards general algorithm discovery on modern CO solvers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4pFgOzKF76": {
    "title": "Online Learning with Bounded Recall",
    "volume": "poster",
    "abstract": "We study the problem of full-information online learning in the ``bounded recall'' setting popular in the study of repeated games. An online learning algorithm $\\mathcal{A}$ is $M$-*bounded-recall* if its output at time $t$ can be written as a function of the $M$ previous rewards (and not e.g. any other internal state of $\\mathcal{A}$). We first demonstrate that a natural approach to constructing bounded-recall algorithms from mean-based no-regret learning algorithms (e.g., running Hedge over the last $M$ rounds) fails, and that any such algorithm incurs constant regret per round. We then construct a stationary bounded-recall algorithm that achieves a per-round regret of $\\Theta(1/\\sqrt{M})$, which we complement with a tight lower bound. Finally, we show that unlike the perfect recall setting, any low regret bound bounded-recall algorithm must be aware of the ordering of the past $M$ losses -- any bounded-recall algorithm which plays a symmetric function of the past $M$ losses must incur constant regret per round",
    "checked": true,
    "id": "02708263ac20d2ea59fbc9efeef0bc28a4016a0f",
    "semantic_title": "online learning with bounded recall",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=io1XSRtcO8": {
    "title": "The Expressive Power of Path-Based Graph Neural Networks",
    "volume": "poster",
    "abstract": "We systematically investigate the expressive power of path-based graph neural networks. While it has been shown that path-based graph neural networks can achieve strong empirical results, an investigation into their expressive power is lacking. Therefore, we propose PATH-WL, a general class of color refinement algorithms based on paths and shortest path distance information. We show that PATH-WL is incomparable to a wide range of expressive graph neural networks, can count cycles, and achieves strong empirical results on the notoriously difficult family of strongly regular graphs. Our theoretical results indicate that PATH-WL forms a new hierarchy of highly expressive graph neural networks",
    "checked": false,
    "id": "724aff7b0589471aab7747c883194b75b09ad4f6",
    "semantic_title": "distance information improves heterogeneous graph neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X8uQ1TslUc": {
    "title": "OT-CLIP: Understanding and Generalizing CLIP via Optimal Transport",
    "volume": "poster",
    "abstract": "We propose to understand Contrastive Language-Image Pretraining model (CLIP) from the Optimal Transport (OT) perspective. Specifically, we show that training of CLIP is an embodiment of inverse OT and the adopted two InfoNCE losses in CLIP correspond to a special case of bilevel optimization of modified entropic OT. We then generalize the original CLIP loss to an OT-based loss family using variants of Regularized OT (e.g. Fused Gromov OT, unbalanced OT, etc.), and demonstrate their superior performance on public datasets for both image and text downstream tasks. We also rethink the inference stage of CLIP by using the tool of OT, and propose to adopt the fused Gromov OT for (zero-shot) classification, in which the prediction is based on the graph representation whereby images and texts are nodes for graph matching. By our new technique, we show how to generalize zero-shot classification to other more flexible zero-shot tasks with competitive performance: long-tailed classification and selective classification. The former assumes the known prior distribution of labels, while in the latter case, only a subset of samples are asked to predict, yet with high prediction confidence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ffS0aYP6mk": {
    "title": "Lessons from Generalization Error Analysis of Federated Learning: You May Communicate Less Often!",
    "volume": "poster",
    "abstract": "We investigate the generalization error of statistical learning models in a Federated Learning (FL) setting. Specifically, we study the evolution of the generalization error with the number of communication rounds $R$ between $K$ clients and a parameter server (PS), i.e. the effect on the generalization error of how often the clients' local models are aggregated at PS. In our setup, the more the clients communicate with PS the less data they use for local training in each round, such that the amount of training data per client is identical for distinct values of $R$. We establish PAC-Bayes and rate-distortion theoretic bounds on the generalization error that account explicitly for the effect of the number of rounds $R$, in addition to the number of participating devices $K$ and individual datasets size $n$. The bounds, which apply to a large class of loss functions and learning algorithms, appear to be the first of their kind for the FL setting. Furthermore, we apply our bounds to FL-type Support Vector Machines (FSVM); and derive (more) explicit bounds in this case. In particular, we show that the generalization bound of FSVM increases with $R$, suggesting that more frequent communication with PS diminishes the generalization power. This implies that the population risk decreases less fast with $R$ than does the empirical risk. Moreover, our bound suggests that the generalization error of FSVM decreases faster than that of centralized learning by a factor of $\\mathcal{O}(\\sqrt{\\log(K)/K})$. Finally, we provide experimental results obtained using neural networks (ResNet-56) which show evidence that not only may our observations for FSVM hold more generally but also that the population risk may even start to increase beyond some value of $R$",
    "checked": true,
    "id": "153856056a35d404970126a85b3aee290061aa70",
    "semantic_title": "lessons from generalization error analysis of federated learning: you may communicate less often!",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=vMUnnS4OWC": {
    "title": "Particle Denoising Diffusion Sampler",
    "volume": "poster",
    "abstract": "Denoising diffusion models have become ubiquitous for generative modeling. The core idea is to transport the data distribution to a Gaussian by using a diffusion. Approximate samples from the data distribution are then obtained by estimating the time-reversal of this diffusion using score matching ideas. We follow here a similar strategy to sample from unnormalized probability densities and compute their normalizing constants. However, the time-reversed diffusion is here simulated by using an original iterative particle scheme relying on a novel score matching loss. Contrary to standard denoising diffusion models, the resulting Particle Denoising Diffusion Sampler (PDDS) provides asymptotically consistent estimates under mild assumptions. We demonstrate PDDS on multimodal and high dimensional sampling tasks",
    "checked": true,
    "id": "187308ff437755641bcc0bd14b08a7c365d88253",
    "semantic_title": "particle denoising diffusion sampler",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=KI3JKFKciG": {
    "title": "DFD: Distillng the Feature Disparity Differently for Detectors",
    "volume": "poster",
    "abstract": "Knowledge distillation is a widely adopted model compression technique that has been successfully applied to object detection. In feature distillation, it is common practice for the student model to imitate the feature responses of the teacher model, with the underlying objective of improving its own abilities by reducing the disparity with the teacher. However, it is crucial to recognize that the disparities between the student and teacher are inconsistent, highlighting their varying abilities. In this paper, we explore the inconsistency in the disparity between teacher and student feature maps and analyze their impact on the efficiency of the distillation. We find that regions with varying degrees of difference should be treated separately, with different distillation constraints applied accordingly. We introduce our distillation method called Disparity Feature Distillation(DFD). The core idea behind DFD is to apply different treatments to regions with varying learning difficulties, simultaneously incorporating leniency and strictness. It enables the student to better assimilate the teacher's knowledge. Through extensive experiments, we demonstrate the effectiveness of our proposed DFD in achieving significant improvements. For instance, when applied to detectors based on ResNet50 such as RetinaNet, FasterRCNN, and RepPoints, our method enhances their mAP from 37.4%, 38.4%, 38.6% to 41.7%, 42.4%, 42.7%, respectively. Our approach also demonstrates substantial improvements on YOLO and ViT-based models. The code is available at https://github.com/luckin99/DFD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VxI0gInNlh": {
    "title": "Symmetric Matrix Completion with ReLU Sampling",
    "volume": "poster",
    "abstract": "We study the problem of symmetric positive semi-definite low-rank matrix completion (MC) with deterministic entry-dependent sampling. In particular, we consider rectified linear unit (ReLU) sampling, where only positive entries are observed, as well as a generalization to threshold-based sampling. We first empirically demonstrate that the landscape of this MC problem is not globally benign: Gradient descent (GD) with random initialization will generally converge to stationary points that are not globally optimal. Nevertheless, we prove that when the matrix factor with a small rank satisfies mild assumptions, the nonconvex objective function is geodesically strongly convex on the quotient manifold in a neighborhood of a planted low-rank matrix. Moreover, we show that our assumptions are satisfied by a matrix factor with i.i.d. Gaussian entries. Finally, we develop a tailor-designed initialization for GD to solve our studied formulation, which empirically always achieves convergence to the global minima. We also conduct extensive experiments and compare MC methods, investigating convergence and completion performance with respect to initialization, noise level, dimension, and rank",
    "checked": true,
    "id": "f23c225208b89fa4696e75194aa8e3a8ec399e2e",
    "semantic_title": "symmetric matrix completion with relu sampling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Yu5FWdzde": {
    "title": "Prompt Sketching for Large Language Models",
    "volume": "poster",
    "abstract": "Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders as part of https://github.com/eth-sri/lmql",
    "checked": true,
    "id": "a3aa43522071dd895619650d26db01cfffbf6fd5",
    "semantic_title": "prompt sketching for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=W4pB7VbzZI": {
    "title": "FlowMM: Generating Materials with Riemannian Flow Matching",
    "volume": "poster",
    "abstract": "Crystalline materials are a fundamental component in next-generation technologies, yet modeling their distribution presents unique computational challenges. Of the plausible arrangements of atoms in a periodic lattice only a vanishingly small percentage are thermodynamically stable, which is a key indicator of the materials that can be experimentally realized. Two fundamental tasks in this area are to (a) predict the stable crystal structure of a known composition of elements and (b) propose novel compositions along with their stable structures. We present FlowMM, a pair of generative models that achieve state-of-the-art performance on both tasks while being more efficient and more flexible than competing methods. We extend Riemannian Flow Matching to suit the symmetries inherent to crystals: translation, rotation, permutation, and periodic boundary conditions. Our framework enables the freedom to choose the flow base distributions, drastically simplifying the problem of learning crystal structures compared with diffusion models. In addition to standard benchmarks, we validate FlowMM's generated structures with quantum chemistry calculations, demonstrating that it is $\\sim$3x more efficient, in terms of integration steps, at finding stable materials compared to previous open methods",
    "checked": true,
    "id": "fea73f1702471c4d009a110aa7e9b3c95da7c3d9",
    "semantic_title": "flowmm: generating materials with riemannian flow matching",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJGwSx0RUV": {
    "title": "Reinforcement Learning within Tree Search for Fast Macro Placement",
    "volume": "poster",
    "abstract": "Macro placement is a crucial step in modern chip design, and reinforcement learning (RL) has recently emerged as a promising technique for improving the placement quality. However, existing RL-based techniques are hindered by their low sample efficiency, requiring numerous online rollouts or substantial offline expert data to achieve bootstrap, which are often impractical in industrial scenarios. To address this challenge, we propose a novel sample-efficient framework, namely **EfficientPlace**, for fast macro placement. EfficientPlace integrates a global tree search algorithm to strategically direct the optimization process, as well as a RL agent for local policy learning to advance the tree search. Experiments on commonly used benchmarks demonstrate that EfficientPlace achieves remarkable placement quality within a short timeframe, outperforming recent state-of-the-art approaches",
    "checked": false,
    "id": "859ce3a91fae1610d1ccd0d5c3e952cb68d5a881",
    "semantic_title": "challenges in floorplanning and macro placement for modern socs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VfWrXJtLSL": {
    "title": "Improved Bounds for Pure Private Agnostic Learning: Item-Level and User-Level Privacy",
    "volume": "poster",
    "abstract": "Machine Learning has made remarkable progress in a wide range of fields. In many scenarios, learning is performed on datasets involving sensitive information, in which privacy protection is essential for learning algorithms. In this work, we study pure private learning in the agnostic model -- a framework reflecting the learning process in practice. We examine the number of users required under item-level (where each user contributes one example) and user-level (where each user contributes multiple examples) privacy and derive several improved upper bounds. For item-level privacy, our algorithm achieves a near optimal bound for general concept classes. We extend this to the user-level setting, rendering a tighter upper bound than the one proved by Ghazi et al. (2023). Lastly, we consider the problem of learning thresholds under user-level privacy and present an algorithm with a nearly tight user complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U4Yvwu1RQY": {
    "title": "Exponential Spectral Pursuit: An Effective Initialization Method for Sparse Phase Retrieval",
    "volume": "poster",
    "abstract": "Sparse phase retrieval aims to reconstruct an $n$-dimensional $k$-sparse signal from its phaseless measurements. For most of the existing reconstruction algorithms, their sampling complexity is known to be dominated by the initialization stage. In this paper, in order to improve the sampling complexity for initialization, we propose a novel method termed exponential spectral pursuit (ESP). Theoretically, our method offers a tighter bound of sampling complexity compared to the state-of-the-art ones, such as the truncated power method. Moreover, it empirically outperforms the existing initialization methods for sparse phase retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xPMW9JURD": {
    "title": "Lyapunov-stable Neural Control for State and Output Feedback: A Novel Formulation",
    "volume": "poster",
    "abstract": "Learning-based neural-network (NN) control policies have shown impressive empirical performance in a wide range of tasks in robotics and control. However, formal (Lyapunov) stability guarantees over the region-of-attraction (ROA) for NN controllers with nonlinear dynamical systems are challenging to obtain, and most existing approaches rely on expensive solvers for sums-of-squares (SOS), mixed-integer programming (MIP), or satisfiability modulo theories (SMT). In this paper, we demonstrate a new framework for learning NN controllers together with Lyapunov certificates using fast empirical falsification and strategic regularizations. We propose a novel formulation that defines a larger verifiable region-of-attraction (ROA) than shown in the literature, and refines the conventional restrictive constraints on Lyapunov derivatives to focus only on certifiable ROAs. The Lyapunov condition is rigorously verified post-hoc using branch-and-bound with scalable linear bound propagation-based NN verification techniques. The approach is efficient and flexible, and the full training and verification procedure is accelerated on GPUs without relying on expensive solvers for SOS, MIP, nor SMT. The flexibility and efficiency of our framework allow us to demonstrate Lyapunov-stable output feedback control with synthesized NN-based controllers and NN-based observers with formal stability guarantees, for the first time in literature",
    "checked": false,
    "id": "7317c4df0654d9d4cd8b5b273a1caff1c308017a",
    "semantic_title": "lyapunov-stable neural control for state and output feedback: a novel formulation for efficient synthesis and verification",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=pAPykbqUHf": {
    "title": "Theory of Consistency Diffusion Models: Distribution Estimation Meets Fast Sampling",
    "volume": "poster",
    "abstract": "Diffusion models have revolutionized various application domains, including computer vision and audio generation. Despite the state-of-the-art performance, diffusion models are known for their slow sample generation due to the extensive number of steps involved. In response, consistency models have been developed to merge multiple steps in the sampling process, thereby significantly boosting the speed of sample generation without compromising quality. This paper contributes towards the first statistical theory for consistency models, formulating their training as a distribution discrepancy minimization problem. Our analysis yields statistical estimation rates based on the Wasserstein distance for consistency models, matching those of vanilla diffusion models. Additionally, our results encompass the training of consistency models through both distillation and isolation methods, demystifying their underlying advantage",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KycvgOCBBR": {
    "title": "Improving Group Robustness on Spurious Correlation Requires Preciser Group Inference",
    "volume": "poster",
    "abstract": "Standard empirical risk minimization (ERM) models may prioritize learning spurious correlations between spurious features and true labels, leading to poor accuracy on groups where these correlations do not hold. Mitigating this issue often requires expensive spurious attribute (group) labels or relies on trained ERM models to infer group labels when group information is unavailable. However, the significant performance gap in worst-group accuracy between using pseudo group labels and using oracle group labels inspires us to consider further improving group robustness through preciser group inference. Therefore, we propose GIC, a novel method that accurately infers group labels, resulting in improved worst-group performance. GIC trains a spurious attribute classifier based on two key properties of spurious correlations: (1) high correlation between spurious attributes and true labels, and (2) variability in this correlation between datasets with different group distributions. Empirical studies on multiple datasets demonstrate the effectiveness of GIC in inferring group labels, and combining GIC with various downstream invariant learning methods improves worst-group accuracy, showcasing its powerful flexibility. Additionally, through analyzing the misclassifications in GIC, we identify an interesting phenomenon called semantic consistency, which may contribute to better decoupling the association between spurious attributes and labels, thereby mitigating spurious correlation. The code for GIC is available at [https://github.com/yujinhanml/GIC9](https://github.com/yujinhanml/GIC)",
    "checked": true,
    "id": "c3f81f72de99d31323bd69cc9261c5cfc91a0290",
    "semantic_title": "improving group robustness on spurious correlation requires preciser group inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yrFUJzcTsk": {
    "title": "Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning",
    "volume": "poster",
    "abstract": "Second-order information is valuable for many applications but challenging to compute. Several works focus on computing or approximating Hessian diagonals, but even this simplification introduces significant additional costs compared to computing a gradient. In the absence of efficient exact computation schemes for Hessian diagonals, we revisit an early approximation scheme proposed by Becker and LeCun (1989, BL89), which has a cost similar to gradients and appears to have been overlooked by the community. We introduce HesScale, an improvement over BL89, which adds negligible extra computation. On small networks, we find that this improvement is of higher quality than all alternatives, even those with theoretical guarantees, such as unbiasedness, while being much cheaper to compute. We use this insight in reinforcement learning problems where small networks are used and demonstrate HesScale in second-order optimization and scaling the step-size parameter. In our experiments, HesScale optimizes faster than existing methods and improves stability through step-size scaling. These findings are promising for scaling second-order methods in larger models in the future",
    "checked": true,
    "id": "36c8ca6fce0c299ca128faaaa8c4386cd254a434",
    "semantic_title": "revisiting scalable hessian diagonal approximations for applications in reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7R3pzxTSlg": {
    "title": "Structured Chemistry Reasoning with Large Language Models",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning *structure* that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs' chemical reasoning capability. Testing across four chemistry areas---quantum chemistry, mechanics, physical chemistry, and kinetics---StructChem substantially enhances GPT-4's performance, with up to 30% peak improvement. Our analysis also underscores the unique difficulties of precise grounded reasoning in science with LLMs, highlighting a need for more research in this area",
    "checked": true,
    "id": "e56aa728aaa32c087c8f7bc56a7eb225675dd8ae",
    "semantic_title": "structured chemistry reasoning with large language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=EsWJ5wd2ir": {
    "title": "Diffusion Rejection Sampling",
    "volume": "poster",
    "abstract": "Recent advances in powerful pre-trained diffusion models encourage the development of methods to improve the sampling performance under well-trained diffusion models. This paper introduces Diffusion Rejection Sampling (DiffRS), which uses a rejection sampling scheme that aligns the sampling transition kernels with the true ones at each timestep. The proposed method can be viewed as a mechanism that evaluates the quality of samples at each intermediate timestep and refines them with varying effort depending on the sample. Theoretical analysis shows that DiffRS can achieve a tighter bound on sampling error compared to pre-trained models. Empirical results demonstrate the state-of-the-art performance of DiffRS on the benchmark datasets and the effectiveness of DiffRS for fast diffusion samplers and large-scale text-to-image diffusion models. Our code is available at https://github.com/aailabkaist/DiffRS",
    "checked": true,
    "id": "ff3e26e489b275feec436f502ea4138a5ba91dea",
    "semantic_title": "diffusion rejection sampling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VAKkoJjVpn": {
    "title": "A Neural-Preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions",
    "volume": "poster",
    "abstract": "We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. Typical Poisson discretizations yield large, ill-conditioned linear systems. Iterative solvers can be effective for these problems, but only when equipped with powerful preconditioners. Unfortunately, effective preconditioners like multigrid require costly setup phases that must be re-executed every time domain shapes or boundary conditions change, forming a severe bottleneck for problems with evolving boundaries. In contrast, we present a neural preconditioner trained to efficiently approximate the inverse of the discrete Laplacian in the presence of such changes. Our approach generalizes to domain shapes, boundary conditions, and grid sizes outside the training set. The key to our preconditioner's success is a novel, lightweight neural network architecture featuring spatially varying convolution kernels and supporting fast inference. We demonstrate that our solver outperforms state-of-the-art methods like algebraic multigrid as well as recently proposed neural preconditioners on challenging test cases arising from incompressible fluid simulations",
    "checked": true,
    "id": "ef05f6ae61ab4f7a22af0464e00d7de2c07f24a2",
    "semantic_title": "a neural-preconditioned poisson solver for mixed dirichlet and neumann boundary conditions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=scSB9RynSd": {
    "title": "Scaling Laws for the Value of Individual Data Points in Machine Learning",
    "volume": "poster",
    "abstract": "Recent works have shown that machine learning models improve at a predictable rate with the amount of training data, leading to scaling laws that describe the relationship between error and dataset size. These scaling laws can help determine a model's training dataset, but they take an aggregate view of the data by only considering the dataset's size. We consider a new perspective by investigating scaling behavior for the value of individual data points: we find that a data point's contribution to model's performance shrinks predictably with the size of the dataset in a log-linear manner. Interestingly, there is significant variability in the scaling exponent among different data points, indicating that certain points are more valuable in small datasets and other points are relatively more useful as a part of large datasets. We provide learning theory support for our scaling laws and we observe empirically that it holds across several model classes. We further propose a maximum likelihood estimator and an amortized estimator to efficiently learn the individualized scaling behaviors from a small number of noisy observations per data point. Using our efficient estimators, we provide insights into factors that influence the scaling behavior of different data points. Finally we demonstrate applications of the individualized scaling laws to data valuation and data subset selection",
    "checked": true,
    "id": "da363589a39d5932ac625365c40654e0045fd88b",
    "semantic_title": "scaling laws for the value of individual data points in machine learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fOBas5H4Xc": {
    "title": "Learning Low-dimensional Latent Dynamics from High-dimensional Observations: Non-asymptotics and Lower Bounds",
    "volume": "poster",
    "abstract": "In this paper, we focus on learning a linear time-invariant (LTI) model with low-dimensional latent variables but high-dimensional observations. We provide an algorithm that recovers the high-dimensional features, i.e. column space of the observer, embeds the data into low dimensions and learns the low-dimensional model parameters. Our algorithm enjoys a sample complexity guarantee of order $\\tilde{\\mathcal{O}}(n/\\epsilon^2)$, where $n$ is the observation dimension. We further establish a fundamental lower bound indicating this complexity bound is optimal up to logarithmic factors and dimension-independent constants. We show that this inevitable linear factor of $n$ is due to the learning error of the observer's column space in the presence of high-dimensional noises. Extending our results, we consider a meta-learning problem inspired by various real-world applications, where the observer column space can be collectively learned from datasets of multiple LTI systems. An end-to-end algorithm is then proposed, facilitating learning LTI systems from a meta-dataset which breaks the sample complexity lower bound in certain scenarios",
    "checked": true,
    "id": "5a624f76c5d25645f744f4c9fd61310534c0fc6f",
    "semantic_title": "learning low-dimensional latent dynamics from high-dimensional observations: non-asymptotics and lower bounds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VyfEv6EjKR": {
    "title": "Graph Neural Networks with a Distribution of Parametrized Graphs",
    "volume": "poster",
    "abstract": "Traditionally, graph neural networks have been trained using a single observed graph. However, the observed graph represents only one possible realization. In many applications, the graph may encounter uncertainties, such as having erroneous or missing edges, as well as edge weights that provide little informative value. To address these challenges and capture additional information previously absent in the observed graph, we introduce latent variables to parameterize and generate multiple graphs. The parameters follow an unknown distribution to be estimated. We propose a formulation in terms of maximum likelihood estimation of the network parameters. Therefore, it is possible to devise an algorithm based on Expectation-Maximization (EM). Specifically, we iteratively determine the distribution of the graphs using a Markov Chain Monte Carlo (MCMC) method, incorporating the principles of PAC-Bayesian theory. Numerical experiments demonstrate improvements in performance against baseline models on node classification for both heterogeneous and homogeneous graphs",
    "checked": true,
    "id": "40ee6711e3c44764748fea1ed288b3d90ab47e2d",
    "semantic_title": "graph neural networks with a distribution of parametrized graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kIh7GJmRfD": {
    "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories",
    "volume": "poster",
    "abstract": "Training autonomous agents with sparse rewards is a long-standing problem in online reinforcement learning (RL), due to low data efficiency. Prior work overcomes this challenge by extracting useful knowledge from offline data, often accomplished through the learning of action distribution from offline data and utilizing the learned distribution to facilitate online RL. However, since the offline data are given and fixed, the extracted knowledge is inherently limited, making it difficult to generalize to new tasks. We propose a novel approach that leverages offline data to learn a generative diffusion model, coined as Adaptive Trajectory Diffuser (ATraDiff). This model generates synthetic trajectories, serving as a form of data augmentation and consequently enhancing the performance of online RL methods. The key strength of our diffuser lies in its adaptability, allowing it to effectively handle varying trajectory lengths and mitigate distribution shifts between online and offline data. Because of its simplicity, ATraDiff seamlessly integrates with a wide spectrum of RL methods. Empirical evaluation shows that ATraDiff consistently achieves state-of-the-art performance across a variety of environments, with particularly pronounced improvements in complicated settings. Our code and demo video are available at https://atradiff.github.io",
    "checked": true,
    "id": "ed7d213de959004feab13b1f713a6116a9dfa320",
    "semantic_title": "atradiff: accelerating online reinforcement learning with imaginary trajectories",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A9MiJdetnZ": {
    "title": "A Statistical Framework for Data-dependent Retrieval-Augmented Models",
    "volume": "poster",
    "abstract": "Modern ML systems increasingly augment input instances with additional relevant information to enhance final prediction. Despite growing interest in such retrieval-augmented models, their fundamental properties and training are not well understood. We propose a statistical framework to study such models with two components: 1) a retriever to identify the relevant information out of a large corpus via a data-dependent metric; and 2) a predictor that consumes the input instances along with the retrieved information to make the final predictions. We present a principled method for end-to-end training of both components and draw connections with various training approaches in the literature. Furthermore, we establish excess risk bounds for retrieval-augmented models while delineating the contributions of both retriever and predictor towards the model performance.We validate the utility of our proposed training methods along with the key takeaways from our statistical analysis on open domain question answering task where retrieval augmentation is important",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xEB2oF3vvb": {
    "title": "Position: Application-Driven Innovation in Machine Learning",
    "volume": "poster",
    "abstract": "In this position paper, we argue that application-driven research has been systemically under-valued in the machine learning community. As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved",
    "checked": false,
    "id": "8d58209234136f9c1d785170921b258d3582f9ef",
    "semantic_title": "application-driven innovation in machine learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=eqIGoEoI10": {
    "title": "Asymptotically Optimal and Computationally Efficient Average Treatment Effect Estimation in A/B testing",
    "volume": "poster",
    "abstract": "Motivated by practical applications in clinical trials and online platforms, we study A/B testing with the aim of estimating a confidence interval (CI) for the average treatment effect (ATE) using the minimum expected sample size. This CI should have a width at most $\\epsilon$ while ensuring that the probability of the CI not containing the true ATE is at most $\\delta$. To answer this, we first establish a lower bound on the expected sample size needed for any adaptive policy which constructs a CI of ATE with desired properties. Specifically, we prove that the lower bound is based on the solution to a max-min non-convex optimization problem for small $\\delta$. Tailoring the ``plug-in'' approach for the ATE problem, we construct an adaptive policy that is asymptotically optimal, i.e., matches the lower bound on the expected sample size for small $\\delta$. Interestingly, we find that, for small $\\epsilon$ and $\\delta$, the asymptotically optimal fraction of treatment assignment for A and B is proportional to the standard deviation of the outcome distributions of treatments A and B, respectively. However, as the proposed approach can be computationally intensive, we propose an alternative adaptive policy. This new policy, informed by insights from our lower bound analysis, is computationally efficient while remaining asymptotically optimal for small values of $\\epsilon$ and $\\delta$. Numerical comparisons demonstrate that both policies perform similarly across practical values of $\\epsilon$ and $\\delta$, offering efficient solutions for A/B testing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xzX7kf486K": {
    "title": "Neural Diffusion Models",
    "volume": "poster",
    "abstract": "Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs through experiments on many image generation benchmarks, including MNIST, CIFAR-10, downsampled versions of ImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms of likelihood, achieving state-of-the-art results on ImageNet and CelebA-HQ, and produces high-quality samples",
    "checked": true,
    "id": "0dab896e3659e1a61724ecaa552071949e6cc73f",
    "semantic_title": "neural diffusion models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=XsDWw1Mn2p": {
    "title": "How Learning by Reconstruction Produces Uninformative Features For Perception",
    "volume": "poster",
    "abstract": "Input space reconstruction is an attractive representation learning paradigm. Despite interpretability benefit of reconstruction and generation, we identify a misalignment between learning to reconstruct, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90% of the pixel variance can be solved with 45% test accuracy. Using the bottom subspace instead, accounting for only 20% of the pixel variance, reaches 55% test accuracy. Learning by reconstruction is also wasteful as the features for perception are learned last, pushing the need for long training schedules. We finally prove that learning by denoising can alleviate that misalignment for some noise strategies, e.g., masking. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide a solution to detect if a noise strategy is never beneficial regardless of the perception task, e.g., additive Gaussian noise",
    "checked": false,
    "id": "e735aa0c9e4742ed4de14d99c10f3e7b40762112",
    "semantic_title": "learning by reconstruction produces uninformative features for perception",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=WWo9G5zyh0": {
    "title": "GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model",
    "volume": "poster",
    "abstract": "This work tackles the problem of geo-localization with a new paradigm using a large vision-language model (LVLM) augmented with human inference knowledge. A primary challenge here is the scarcity of data for training the LVLM - existing street-view datasets often contain numerous low-quality images lacking visual clues, and lack any reasoning inference. To address the data-quality issue, we devise a CLIP-based network to quantify the degree of street-view images being locatable, leading to the creation of a new dataset comprising highly locatable street views. To enhance reasoning inference, we integrate external knowledge obtained from real geo-localization games, tapping into valuable human inference capabilities. The data are utilized to train GeoReasoner, which undergoes fine-tuning through dedicated reasoning and location-tuning stages. Qualitative and quantitative evaluations illustrate that GeoReasoner outperforms counterpart LVLMs by more than 25% at country-level and 38% at city-level geo-localization tasks, and surpasses StreetCLIP performance while requiring fewer training resources. The data and code are available at https://github.com/lingli1996/GeoReasoner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5mCaITRTmO": {
    "title": "Extreme Compression of Large Language Models via Additive Quantization",
    "volume": "poster",
    "abstract": "The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of ``extreme'' LLM compression---defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter---from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic *Additive Quantization (AQ)* approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint",
    "checked": true,
    "id": "2209dd35db8098b6c80caeda705f75339f141e22",
    "semantic_title": "extreme compression of large language models via additive quantization",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=mk3A5IUdn8": {
    "title": "Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling",
    "volume": "poster",
    "abstract": "Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance. Code to reproduce our experiments is available here: https://github.com/kuleshov-group/caduceus",
    "checked": true,
    "id": "6c1578d9eff8f9d25ddf0398a77ffcc888a4593b",
    "semantic_title": "caduceus: bi-directional equivariant long-range dna sequence modeling",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=re6es2atbl": {
    "title": "A New Theoretical Perspective on Data Heterogeneity in Federated Optimization",
    "volume": "poster",
    "abstract": "In federated learning (FL), data heterogeneity is the main reason that existing theoretical analyses are pessimistic about the convergence rate. In particular, for many FL algorithms, the convergence rate grows dramatically when the number of local updates becomes large, especially when the product of the gradient divergence and local Lipschitz constant is large. However, empirical studies can show that more local updates can improve the convergence rate even when these two parameters are large, which is inconsistent with the theoretical findings. This paper aims to bridge this gap between theoretical understanding and practical performance by providing a theoretical analysis from a new perspective on data heterogeneity. In particular, we propose a new and weaker assumption compared to the local Lipschitz gradient assumption, named the heterogeneity-driven pseudo-Lipschitz assumption. We show that this and the gradient divergence assumptions can jointly characterize the effect of data heterogeneity. By deriving a convergence upper bound for FedAvg and its extensions, we show that, compared to the existing works, local Lipschitz constant is replaced by the much smaller heterogeneity-driven pseudo-Lipschitz constant and the corresponding convergence upper bound can be significantly reduced for the same number of local updates, although its order stays the same. In addition, when the local objective function is quadratic, more insights on the impact of data heterogeneity can be obtained using the heterogeneity-driven pseudo-Lipschitz constant. For example, we can identify a region where FedAvg can outperform mini-batch SGD even when the gradient divergence can be arbitrarily large. Our findings are validated using experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uQ2FUoFjnF": {
    "title": "An LLM Compiler for Parallel Function Calling",
    "volume": "poster",
    "abstract": "The reasoning capabilities of the recent LLMs enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed LLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multiple function calls. Drawing inspiration from the principles of classical compilers, LLMCompiler enables parallel function calling with three components: (i) a Function Calling Planner, formulating execution plans for function calling; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically generates an optimized orchestration for the function calls and can be used with both open-source and closed-source models. We have benchmarked LLMCompiler on a range of tasks with different patterns of function calling. We observe consistent latency speedup of up to $3.7 \\times$, cost savings of up to $6.7 \\times$, and accuracy improvement of up to $\\sim 9 \\%$ compared to ReAct.Our code is available at https://github.com/SqueezeAILab/LLMCompiler",
    "checked": true,
    "id": "36f71673d9337b432babc51da77ef38b2070b5ed",
    "semantic_title": "an llm compiler for parallel function calling",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=1jHiq640y1": {
    "title": "Unifying Bayesian Flow Networks and Diffusion Models through Stochastic Differential Equations",
    "volume": "poster",
    "abstract": "Bayesian flow networks (BFNs) iteratively refine the parameters, instead of the samples in diffusion models (DMs), of distributions at various noise levels through Bayesian inference. Owing to its differentiable nature, BFNs are promising in modeling both continuous and discrete data, while simultaneously maintaining fast sampling capabilities. This paper aims to understand and enhance BFNs by connecting them with DMs through stochastic differential equations (SDEs). We identify the linear SDEs corresponding to the noise-addition processes in BFNs, demonstrate that BFN's regression losses are aligned with denoise score matching, and validate the sampler in BFN as a first-order solver for the respective reverse-time SDE. Based on these findings and existing recipes of fast sampling in DMs, we propose specialized solvers for BFNs that markedly surpass the original BFN sampler in terms of sample quality with a limited number of function evaluations (e.g., 10) on both image and text datasets. Notably, our best sampler achieves an increase in speed of $5\\sim20$ times for free",
    "checked": true,
    "id": "a85c64c4f002c2b53c5d2e8c5c1afd68ba286f0e",
    "semantic_title": "unifying bayesian flow networks and diffusion models through stochastic differential equations",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=LGDYsBslWi": {
    "title": "On Statistical Learning Theory for Distributional Inputs",
    "volume": "poster",
    "abstract": "Kernel-based statistical learning on distributional inputs appears in many relevant applications, from medical diagnostics to causal inference, and poses intriguing theoretical questions. While this learning scenario received considerable attention from the machine learning community recently, many gaps in the theory remain. In particular, most works consider only the distributional regression setting, and focus on the regularized least-squares algorithm for this problem. In this work, we start to fill these gaps. We prove two oracle inequalities for kernel machines in general distributional learning scenarios, as well as a generalization result based on algorithmic stability. Our main results are formulated in great generality, utilizing general Hilbertian embeddings, which makes them applicable to a wide array of approaches to distributional learning. Additionally, we specialize our results to the cases of kernel mean embeddings and of the recently introduced Hilbertian embeddings based on sliced Wasserstein distances, providing concrete instances of the general setup. Our results considerably enlarge the scope of theoretically grounded distributional learning, and provide many interesting avenues for future work",
    "checked": false,
    "id": "5c5ccf94d0f064cc5ca134385f7d60741d16c73f",
    "semantic_title": "statistical learning from a regression perspective",
    "citation_count": 287,
    "authors": []
  },
  "https://openreview.net/forum?id=O1hmwi51pp": {
    "title": "Automated Loss function Search for Class-imbalanced Node Classification",
    "volume": "poster",
    "abstract": "Class-imbalanced node classification tasks are prevalent in real-world scenarios. Due to the uneven distribution of nodes across different classes, learning high-quality node representations remains a challenging endeavor. The engineering of loss functions has shown promising potential in addressing this issue. It involves the meticulous design of loss functions, utilizing information about the quantities of nodes in different categories and the network's topology to learn unbiased node representations. However, the design of these loss functions heavily relies on human expert knowledge and exhibits limited adaptability to specific target tasks. In this paper, we introduce a high-performance, flexible, and generalizable automated loss function search framework to tackle this challenge. Across 15 combinations of graph neural networks and datasets, our framework achieves a significant improvement in performance compared to state-of-the-art methods. Additionally, we observe that homophily in graph-structured data significantly contributes to the transferability of the proposed framework",
    "checked": true,
    "id": "061bcc04e6155c3ef1c2e6c563770cf056a63104",
    "semantic_title": "automated loss function search for class-imbalanced node classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dfR6FU53qk": {
    "title": "Consistent Long-Term Forecasting of Ergodic Dynamical Systems",
    "volume": "poster",
    "abstract": "We study the problem of forecasting the evolution of a function of the state (observable) of a discrete ergodic dynamical system over multiple time steps. The elegant theory of Koopman and transfer operators can be used to evolve any such function forward in time. However, their estimators are usually unreliable in long-term forecasting. We show how classical techniques of eigenvalue deflation from operator theory and feature centering from statistics can be exploited to enhance standard estimators. We develop a novel technique to derive high probability bounds on powers of empirical estimators. Our approach, rooted in the stability _theory of non-normal operators_, allows us to establish uniform in time bounds for the forecasting error, which hold even on _infinite time horizons_. We further show that our approach can be seamlessly employed to forecast future state distributions from an initial one, with provably uniform error bounds. Numerical experiments illustrate the advantages of our approach in practice",
    "checked": true,
    "id": "6bd0baa7c1b4d851b8654eade805ce3184dedd58",
    "semantic_title": "consistent long-term forecasting of ergodic dynamical systems",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BTkaKA74mS": {
    "title": "Promises and Pitfalls of Generative Masked Language Modeling: Theoretical Framework and Practical Guidelines",
    "volume": "poster",
    "abstract": "Autoregressive language models are the currently dominant paradigm for text generation, however they have some fundamental limitations that cannot be remedied by scale---for example inherently sequential and unidirectional generation. While alternate classes of models have been explored, we have limited mathematical understanding of their fundamental power and limitations. In this paper we focus on Generative Masked Language Models (GMLMs), a non-autoregressive paradigm in which we train a model to fit conditional probabilities of the data distribution via masking, which are subsequently used as inputs to a Markov Chain to draw samples from the model. These models empirically strike a promising speed-quality trade-off as each step can be typically parallelized by decoding the entire sequence in parallel. We develop a mathematical framework for analyzing and improving such models which sheds light on questions of sample complexity and inference speed and quality. Empirically, we adapt the T5 model for iteratively-refined parallel decoding, achieving 2-3x speedup in machine translation with minimal sacrifice in quality compared with autoregressive models. We run careful ablation experiments to give recommendations on key design choices, and make fine-grained observations on the common error modes in connection with our theory. Our mathematical analyses and empirical observations characterize both potentials and limitations of this approach, and can be applied to future works on improving understanding and performance of GMLMs. We released codes for our experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ss3h1ixJAU": {
    "title": "Absolute Policy Optimization: Enhancing Lower Probability Bound of Performance with High Confidence",
    "volume": "poster",
    "abstract": "In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function, optimizing which leads to guaranteed monotonic improvement in the lower probability bound of performance with high confidence. Building upon this groundbreaking theoretical advancement, we further introduce a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO as well as its efficient variation Proximal Absolute Policy Optimization (PAPO) significantly outperforms state-of-the-art policy gradient algorithms, resulting in substantial improvements in worst-case performance, as well as expected performance",
    "checked": false,
    "id": "ff437eb6108d35b1b70ebb98f5085a5c0154086b",
    "semantic_title": "absolute policy optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CTgEV6qgUy": {
    "title": "Active Preference Learning for Large Language Models",
    "volume": "poster",
    "abstract": "As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data",
    "checked": true,
    "id": "128e6eb7b0c1caf02dc1f0c7246954f6bd1b84dd",
    "semantic_title": "active preference learning for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=18rzx2PXKm": {
    "title": "Finite-Time Convergence and Sample Complexity of Actor-Critic Multi-Objective Reinforcement Learning",
    "volume": "poster",
    "abstract": "Reinforcement learning with multiple, potentially conflicting objectives is pervasive in real-world applications, while this problem remains theoretically under-explored. This paper tackles the multi-objective reinforcement learning (MORL) problem and introduces an innovative actor-critic algorithm named MOAC which finds a policy by iteratively making trade-offs among conflicting reward signals. Notably, we provide the first analysis of finite-time Pareto-stationary convergence and corresponding sample complexity in both discounted and average reward settings. Our approach has two salient features: (a) MOAC mitigates the cumulative estimation bias resulting from finding an optimal common gradient descent direction out of stochastic samples. This enables provable convergence rate and sample complexity guarantees independent of the number of objectives; (b) With proper momentum coefficient, MOAC initializes the weights of individual policy gradients using samples from the environment, instead of manual initialization. This enhances the practicality and robustness of our algorithm. Finally, experiments conducted on a real-world dataset validate the effectiveness of our proposed method",
    "checked": true,
    "id": "f679ff94bcd4bb0e9a8277844cc552d16ae0a64d",
    "semantic_title": "finite-time convergence and sample complexity of actor-critic multi-objective reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ULKvSqmSgA": {
    "title": "Convergence of Online Learning Algorithm for a Mixture of Multiple Linear Regressions",
    "volume": "poster",
    "abstract": "This paper considers the parameter learning and data clustering problem for MLR with multiple sub-models and arbitrary mixing weights. To deal with the data streaming case, we propose an online learning algorithm to estimate the unknown parameters. By utilizing Ljung's ODE method, we establish the almost sure convergence results of this MLR problem without the traditional i.i.d. assumption on the input data for the first time. Based on the convergence property and using the classical stochastic Lyapunov function method, we also obtain the convergence rate analysis of the proposed algorithm for the first time. In addition, the data clustering can asymptotically achieve the same performance as the case with known parameters. Future work will consider how to relax the asymptotically stationary and ergodic assumption on the input data, and how to design algorithms with global convergence performance for the MLR problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5906M4Wnd": {
    "title": "Automated Statistical Model Discovery with Language Models",
    "volume": "poster",
    "abstract": "Statistical model discovery is a challenging search over a vast space of models subject to domain-specific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery",
    "checked": true,
    "id": "892dba6e3240433b30c8ac776c3b6dad61ebdfa9",
    "semantic_title": "automated statistical model discovery with language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=yL6hljtjW4": {
    "title": "Towards Efficient Spiking Transformer: a Token Sparsification Framework for Training and Inference Acceleration",
    "volume": "poster",
    "abstract": "Nowadays Spiking Transformers have exhibited remarkable performance close to Artificial Neural Networks (ANNs), while enjoying the inherent energy-efficiency of Spiking Neural Networks (SNNs). However, training Spiking Transformers on GPUs is considerably more time-consuming compared to the ANN counterparts, despite the energy-efficient inference through neuromorphic computation. In this paper, we investigate the token sparsification technique for efficient training of Spiking Transformer and find conventional methods suffer from noticeable performance degradation. We analyze the issue and propose our Sparsification with Timestep-wise Anchor Token and dual Alignments (STATA). Timestep-wise Anchor Token enables precise identification of important tokens across timesteps based on standardized criteria. Additionally, dual Alignments incorporate both Intra and Inter Alignment of the attention maps, fostering the learning of inferior attention. Extensive experiments show the effectiveness of STATA thoroughly, which demonstrates up to $\\sim$1.53$\\times$ training speedup and $\\sim$48% energy reduction with comparable performance on various datasets and architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CY0lFwD4qx": {
    "title": "Reservoir Computing for Short High-Dimensional Time Series: an Application to SARS-CoV-2 Hospitalization Forecast",
    "volume": "poster",
    "abstract": "In this work, we aimed at forecasting the number of SARS-CoV-2 hospitalized patients at 14 days to help anticipate the bed requirements of a large scale hospital using public data and electronic health records data. Previous attempts led to mitigated performance in this high-dimension setting; we introduce a novel approach to time series forecasting by providing an alternative to conventional methods to deal with high number of potential features of interest (409 predictors). We integrate Reservoir Computing (RC) with feature selection using a genetic algorithm (GA) to gather optimal non-linear combinations of inputs to improve prediction in sample-efficient context. We illustrate that the RC-GA combination exhibits excellent performance in forecasting SARS-CoV-2 hospitalizations. This approach outperformed the use of RC alone and other conventional methods: LSTM, Transformers, Elastic-Net, XGBoost. Notably, this work marks the pioneering use of RC (along with GA) in the realm of short and high-dimensional time series, positioning it as a competitive and innovative approach in comparison to standard methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BIbjwcrg0V": {
    "title": "Scaling Tractable Probabilistic Circuits: A Systems Perspective",
    "volume": "poster",
    "abstract": "Probabilistic Circuits (PCs) are a general framework for tractable deep generative models, which support exact and efficient probabilistic inference on their learned distributions. Recent modeling and training advancements have enabled their application to complex real-world tasks. However, the time and memory inefficiency of existing PC implementations hinders further scaling up. This paper proposes PyJuice, a general GPU implementation design for PCs that improves prior art in several regards. Specifically, PyJuice is 1-2 orders of magnitude faster than existing systems (including very recent ones) at training large-scale PCs. Moreover, PyJuice consumes 2-5x less GPU memory, which enables us to train larger models. At the core of our system is a compilation process that converts a PC into a compact representation amenable to efficient block-based parallelization, which significantly reduces IO and makes it possible to leverage Tensor Cores available in modern GPUs. Empirically, PyJuice can be used to improve state-of-the-art PCs trained on image (e.g., ImageNet32) and language (e.g., WikiText, CommonGen) datasets. We further establish a new set of baselines on natural image and language datasets by benchmarking existing PC structures but with much larger sizes and more training epochs, with the hope of incentivizing future research. Code is available at https://github.com/Tractables/pyjuice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8k5JrGip6": {
    "title": "Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation",
    "volume": "poster",
    "abstract": "Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the \"distraction phenomenon\", where irrelevant context in the prompt degrades output quality. To address these drawbacks, we propose a novel RAG prompting methodology, *superposition prompting*, which can be directly applied to pre-trained transformer-based LLMs *without the need for fine-tuning*. At a high level, superposition prompting allows the LLM to process input documents in parallel *prompt paths*, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on. For example, our approach facilitates a $93\\times$ reduction in compute time while *improving* accuracy by $43\\%$ on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG",
    "checked": true,
    "id": "9c45b4af25e192733d42a8d384e41002786d0d32",
    "semantic_title": "superposition prompting: improving and accelerating retrieval-augmented generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=cU20finY8V": {
    "title": "Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics",
    "volume": "poster",
    "abstract": "Despite attaining high empirical generalization, the sharpness of models trained with sharpness-aware minimization (SAM) do not always correlate with generalization error. Instead of viewing SAM as minimizing sharpness to improve generalization, our paper considers a new perspective based on SAM's training dynamics. We propose that perturbations in SAM perform *perturbed forgetting*, where they discard undesirable model biases to exhibit learning signals that generalize better. We relate our notion of forgetting to the information bottleneck principle, use it to explain observations like the better generalization of smaller perturbation batches, and show that perturbed forgetting can exhibit a stronger correlation with generalization than flatness. While standard SAM targets model biases exposed by the steepest ascent directions, we propose a new perturbation that targets biases exposed through the model's outputs. Our output bias forgetting perturbations outperform standard SAM, GSAM, and ASAM on ImageNet, robustness benchmarks, and transfer to CIFAR-10,100, while sometimes converging to sharper regions. Our results suggest that the benefits of SAM can be explained by alternative mechanistic principles that do not require flatness of the loss surface",
    "checked": true,
    "id": "9cbada191135fa6004e9c9cd4d69d37d001d114e",
    "semantic_title": "forget sharpness: perturbed forgetting of model biases within sam dynamics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lwWV4Zl3h1": {
    "title": "Adaptive Conformal Inference by Betting",
    "volume": "poster",
    "abstract": "Conformal prediction is a valuable tool for quantifying predictive uncertainty of machine learning models. However, its applicability relies on the assumption of data exchangeability, a condition which is often not met in real-world scenarios. In this paper, we consider the problem of adaptive conformal inference without any assumptions about the data generating process. Existing approaches for adaptive conformal inference are based on optimizing the pinball loss using variants of online gradient descent. A notable shortcoming of such approaches is in their explicit dependence on and sensitivity to the choice of the learning rates. In this paper, we propose a different approach for adaptive conformal inference that leverages parameter-free online convex optimization techniques. We prove that our method controls long-term miscoverage frequency at a nominal level and demonstrate its convincing empirical performance without any need of performing cumbersome parameter tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DwTgy1hXXo": {
    "title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
    "volume": "poster",
    "abstract": "Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Mattew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs. Code is available at: https://github.com/microsoft/promptbench",
    "checked": false,
    "id": "0334987f094121c094d5043ab38f14ebf5852c05",
    "semantic_title": "dyval 2: dynamic evaluation of large language models by meta probing agents",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=UQYXZdca92": {
    "title": "Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes",
    "volume": "poster",
    "abstract": "We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be learned efficiently by square loss regression over the time-series data. We show that the drift and the diffusion coefficients of this SDE can be adjusted after training, and that a specific choice that minimizes the impact of the estimation error gives a Föllmer process. We highlight the utility of our approach on several complex, high-dimensional forecasting problems, including stochastically forced Navier-Stokes and video prediction on the KTH and CLEVRER datasets. The code is available at https://github.com/interpolants/forecasting",
    "checked": true,
    "id": "22296ea460bd88c1b331ac95f046715b71e6a0ca",
    "semantic_title": "probabilistic forecasting with stochastic interpolants and föllmer processes",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2dlmcTXfcY": {
    "title": "Kernel-Based Evaluation of Conditional Biological Sequence Models",
    "volume": "poster",
    "abstract": "We propose a set of kernel-based tools to evaluate the designs and tune the hyperparameters of conditional sequence models, with a focus on problems in computational biology. The backbone of our tools is a new measure of discrepancy between the true conditional distribution and the model's estimate, called the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided that the model can be sampled from, the ACMMD can be estimated unbiasedly from data to quantify absolute model fit, integrated within hypothesis tests, and used to evaluate model reliability. We demonstrate the utility of our approach by analyzing a popular protein design model, ProteinMPNN. We are able to reject the hypothesis that ProteinMPNN fits its data for various protein families, and tune the model's temperature hyperparameter to achieve a better fit",
    "checked": false,
    "id": "664548ce5ae632ddc3919524e46e2b53a1933bec",
    "semantic_title": "a kernelized stein discrepancy for biological sequences",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uog14iBFLA": {
    "title": "Fast and Sample Efficient Multi-Task Representation Learning in Stochastic Contextual Bandits",
    "volume": "poster",
    "abstract": "We study how representation learning can improve the learning efficiency of contextual bandit problems. We study the setting where we play T linear contextual bandits with dimension simultaneously, and these T bandit tasks collectively share a common linear representation with a dimensionality of r ≪ d. We present a new algorithm based on alternating projected gradient descent (GD) and minimization estimator to recover a low-rank feature matrix. We obtain constructive provable guarantees for our estimator that provide a lower bound on the required sample complexity and an upper bound on the iteration complexity (total number of iterations needed to achieve a certain error level). Using the proposed estimator, we present a multi-task learning algorithm for linear contextual bandits and prove the regret bound of our algorithm. We presented experiments and compared the performance of our algorithm against benchmark algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJkGOARXns": {
    "title": "In-context Learning on Function Classes Unveiled for Transformers",
    "volume": "poster",
    "abstract": "Transformer-based neural sequence models exhibit a remarkable ability to perform in-context learning. Given some training examples, a pre-trained model can make accurate predictions on an unseen input. This paper studies why transformers can learn different types of function classes in-context. We first show by construction that there exists a family of transformers (with different activation functions) that implement approximate gradient descent on the parameters of neural networks, and we provide an upper bound for the number of heads, hidden dimensions, and layers of the transformer. We also show that a transformer can learn linear functions, the indicator function of a unit ball, and smooth functions in-context by learning neural networks that approximate them. The above instances mainly focus on a transformer pre-trained on single tasks. We also prove that when pre-trained on two tasks: linear regression and classification, a transformer can make accurate predictions on both tasks simultaneously. Our results move beyond linearity in terms of in-context learning instances and provide a comprehensive understanding of why transformers can learn many types of function classes through the bridge of neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WIbntm28cM": {
    "title": "Linear Explanations for Individual Neurons",
    "volume": "poster",
    "abstract": "In recent years many methods have been developed to understand the internal workings of neural networks, often by describing the function of individual neurons in the model. However, these methods typically only focus on explaining the very highest activations of a neuron. In this paper we show this is not sufficient, and that the highest activation range is only responsible for a very small percentage of the neuron's causal effect. In addition, inputs causing lower activations are often very different and can't be reliably predicted by only looking at high activations. We propose that neurons should instead be understood as a linear combination of concepts, and develop an efficient method for producing these linear explanations. In addition, we show how to automatically evaluate description quality using simulation, i.e. predicting neuron activations on unseen inputs in vision setting",
    "checked": true,
    "id": "be1005d48d9bc003491531df5b9b0b40c6fe06b7",
    "semantic_title": "linear explanations for individual neurons",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=99UFZV2VpU": {
    "title": "How Does Goal Relabeling Improve Sample Efficiency?",
    "volume": "poster",
    "abstract": "Hindsight experience replay and goal relabeling are successful in reinforcement learning (RL) since they enable agents to learn from failures. Despite their successes, we lack a theoretical understanding, such as (i) why hindsight experience replay improves sample efficiency and (ii) how to design a relabeling method that achieves sample efficiency. To this end, we construct an example to show the information-theoretical improvement in sample efficiency achieved by goal relabeling. Our example reveals that goal relabeling can enhance sample efficiency and exploit the rich information in observations through better hypothesis elimination. Based on these insights, we develop an RL algorithm called GOALIVE. To analyze the sample complexity of GOALIVE, we introduce a complexity measure, the goal-conditioned Bellman-Eluder (GOAL-BE) dimension, which characterizes the sample complexity of goal-conditioned RL problems. Compared to the Bellman-Eluder dimension, the goal-conditioned version offers an exponential improvement in the best case. To the best of our knowledge, our work provides the first characterization of the theoretical improvement in sample efficiency achieved by goal relabeling",
    "checked": false,
    "id": "82c89641e2455bcf14370c79fa1899f74ae37a17",
    "semantic_title": "learning and reusing primitive behaviours to improve hindsight experience replay sample efficiency",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itDhUBY2xf": {
    "title": "Learning from Integral Losses in Physics Informed Neural Networks",
    "volume": "poster",
    "abstract": "This work proposes a solution for the problem of training physics-informed networks under partial integro-differential equations. These equations require an infinite or a large number of neural evaluations to construct a single residual for training. As a result, accurate evaluation may be impractical, and we show that naive approximations at replacing these integrals with unbiased estimates lead to biased loss functions and solutions. To overcome this bias, we investigate three types of potential solutions: the deterministic sampling approaches, the double-sampling trick, and the delayed target method. We consider three classes of PDEs for benchmarking; one defining Poisson problems with singular charges and weak solutions of up to 10 dimensions, another involving weak solutions on electro-magnetic fields and a Maxwell equation, and a third one defining a Smoluchowski coagulation problem. Our numerical results confirm the existence of the aforementioned bias in practice and also show that our proposed delayed target approach can lead to accurate solutions with comparable quality to ones estimated with a large sample size integral. Our implementation is open-source and available at https://github.com/ehsansaleh/btspinn",
    "checked": true,
    "id": "bad659e31236c37e94a40336f957a13b1ad690c2",
    "semantic_title": "learning from integral losses in physics informed neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=DzLna0cFL1": {
    "title": "Position: Towards Unified Alignment Between Agents, Humans, and Environment",
    "volume": "poster",
    "abstract": "The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of **U**nified **A**lignment for **A**gents (**UA**$^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of **UA**$^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles demonstrating intentions, personalized reranking reflecting complex environmental dynamics, and runtime cost statistics as self-constraints. We then follow the principles of **UA**$^2$ to propose an initial design of our agent and benchmark its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of **UA**$^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C1iNBLIClt": {
    "title": "Causal Effect Identification in LiNGAM Models with Latent Confounders",
    "volume": "poster",
    "abstract": "We study the generic identifiability of causal effects in linear non-Gaussian acyclic models (LiNGAM) with latent variables. We consider the problem in two main settings: When the causal graph is known a priori, and when it is unknown. In both settings, we provide a complete graphical characterization of the identifiable direct or total causal effects among observed variables. Moreover, we propose efficient algorithms to certify the graphical conditions. Finally, we propose an adaptation of the reconstruction independent component analysis (RICA) algorithm that estimates the causal effects from the observational data given the causal graph. Experimental results show the effectiveness of the proposed method in estimating the causal effects",
    "checked": true,
    "id": "4ee9f6702756746bd7ff6a01fd6886d6d37bb985",
    "semantic_title": "causal effect identification in lingam models with latent confounders",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lsHZNNoC7r": {
    "title": "DistiLLM: Towards Streamlined Distillation for Large Language Models",
    "volume": "poster",
    "abstract": "Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\\times$ speedup compared to recent KD methods",
    "checked": true,
    "id": "49b7baceecd32f81a08aa8e84e2fe71c2b879ee6",
    "semantic_title": "distillm: towards streamlined distillation for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CQI3f1U9X1": {
    "title": "Quantum Algorithms and Lower Bounds for Finite-Sum Optimization",
    "volume": "poster",
    "abstract": "Finite-sum optimization has wide applications in machine learning, covering important problems such as support vector machines, regression, etc. In this paper, we initiate the study of solving finite-sum optimization problems by quantum computing. Specifically, let $f_1,\\ldots,f_n:\\mathbb{R}^d\\to\\mathbb{R}$ be $\\ell$-smooth convex functions and $\\psi:\\mathbb{R}^d\\to\\mathbb{R}$ be a $\\mu$-strongly convex proximal function. The goal is to find an $\\epsilon$-optimal point for $F(\\mathbf{x})=\\frac{1}{n}\\sum_{i=1}^n f_i(\\mathbf{x})+\\psi(\\mathbf{x})$. We give a quantum algorithm with complexity $\\tilde{O}\\big(n+\\sqrt{d}+\\sqrt{\\ell/\\mu}\\big(n^{1/3}d^{1/3}+n^{-2/3}d^{5/6}\\big)\\big)$, improving the classical tight bound $\\tilde{\\Theta}\\big(n+\\sqrt{n\\ell/\\mu}\\big)$. We also prove a quantum lower bound $\\tilde{\\Omega}(n+n^{3/4}(\\ell/\\mu)^{1/4})$ when $d$ is large enough. Both our quantum upper and lower bounds can extend to the cases where $\\psi$ is not necessarily strongly convex, or each $f_i$ is Lipschitz but not necessarily smooth. In addition, when $F$ is nonconvex, our quantum algorithm can find an $\\epsilon$-critial point using $\\tilde{O}(n+\\ell(d^{1/3}n^{1/3}+\\sqrt{d})/\\epsilon^2)$ queries",
    "checked": true,
    "id": "e8e53ec6ecf3c3c23c2fc9d27c85951b20ba4718",
    "semantic_title": "quantum algorithms and lower bounds for finite-sum optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jw2f9v59g0": {
    "title": "Data-free Distillation of Diffusion Models with Bootstrapping",
    "volume": "poster",
    "abstract": "Diffusion models have demonstrated great potential for generating diverse images. However, their performance often suffers from slow generation due to iterative denoising. Knowledge distillation has been recently proposed as a remedy which can reduce the number of inference steps to one or a few, without significant quality degradation. However, existing distillation methods either require significant amounts of offline computation for generating synthetic training data from the teacher model, or need to perform expensive online learning with the help of real data. In this work, we present a novel technique called BOOT, that overcomes these limitations with an efficient data-free distillation algorithm. The core idea is to learn a time-conditioned model that predicts the output of a pre-trained diffusion model teacher given any time-step. Such a model can be efficiently trained based on bootstrapping from two consecutive sampled steps. Furthermore, our method can be easily adapted to large-scale text-to-image diffusion models, which are challenging for previous methods given the fact that the training sets are often large and difficult to access. We demonstrate the effectiveness of our approach on several benchmark datasets in the DDIM setting, achieving comparable generation quality while being orders of magnitude faster than the diffusion teacher. The text-to-image results show that the proposed approach is able to handle highly complex distributions, shedding light on more efficient generative modeling",
    "checked": false,
    "id": "a8e6a8480543efdfd5189a19c6c54a40d2cc6efe",
    "semantic_title": "boot: data-free distillation of denoising diffusion models with bootstrapping",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=laIOUtstMs": {
    "title": "Meta-Reinforcement Learning Robust to Distributional Shift Via Performing Lifelong In-Context Learning",
    "volume": "poster",
    "abstract": "A key challenge in Meta-Reinforcement Learning (meta-RL) is the task distribution shift, since the generalization ability of most current meta-RL methods is limited to tasks sampled from the training distribution. In this paper, we propose Posterior Sampling Bayesian Lifelong In-Context Reinforcement Learning (PSBL), which is robust to task distribution shift. PSBL meta-trains a variant of transformer to directly perform amortized inference about the Predictive Posterior Distribution (PPD) of the optimal policy. Once trained, the network can infer the PPD online with frozen parameters. The agent then samples actions from the approximate PPD to perform online exploration, which progressively reduces uncertainty and enhances performance in the interaction with the environment. This property is known as in-context learning. Experimental results demonstrate that PSBL significantly outperforms standard Meta RL methods both in tasks with sparse rewards and dense rewards when the test task distribution is strictly shifted from the training distribution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3KMMPxrAk5": {
    "title": "Local Causal Structure Learning in the Presence of Latent Variables",
    "volume": "poster",
    "abstract": "Discovering causal relationships from observational data, particularly in the presence of latent variables, poses a challenging problem. While current local structure learning methods have proven effective and efficient when the focus lies solely on the local relationships of a target variable, they operate under the assumption of causal sufficiency. This assumption implies that all the common causes of the measured variables are observed, leaving no room for latent variables. Such a premise can be easily violated in various real-world applications, resulting in inaccurate structures that may adversely impact downstream tasks. In light of this, our paper delves into the primary investigation of locally identifying potential parents and children of a target from observational data that may include latent variables. Specifically, we harness the causal information from m-separation and V-structures to derive theoretical consistency results, effectively bridging the gap between global and local structure learning. Together with the newly developed stop rules, we present a principled method for determining whether a variable is a direct cause or effect of a target. Further, we theoretically demonstrate the correctness of our approach under the standard causal Markov and faithfulness conditions, with infinite samples. Experimental results on both synthetic and real-world data validate the effectiveness and efficiency of our approach",
    "checked": true,
    "id": "a7456468020edaae9df1a29fbc72bc782b6f5fe8",
    "semantic_title": "local causal structure learning in the presence of latent variables",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KpeGdDzucX": {
    "title": "Parallelized Spatiotemporal Slot Binding for Videos",
    "volume": "poster",
    "abstract": "While modern best practices advocate for scalable architectures that support long-range interactions, object-centric models are yet to fully embrace these architectures. In particular, existing object-centric models for handling sequential inputs, due to their reliance on RNN-based implementation, show poor stability and capacity and are slow to train on long sequences. We introduce Parallelizable Spatiotemporal Binder or PSB, the first temporally-parallelizable slot learning architecture for sequential inputs. Unlike conventional RNN-based approaches, PSB produces object-centric representations, known as slots, for all time-steps in parallel. This is achieved by refining the initial slots across all time-steps through a fixed number of layers equipped with causal attention. By capitalizing on the parallelism induced by our architecture, the proposed model exhibits a significant boost in efficiency. In experiments, we test PSB extensively as an encoder within an auto-encoding framework paired with a wide variety of decoder options. Compared to the state-of-the-art, our architecture demonstrates stable training on longer sequences, achieves parallelization that results in a 60% increase in training speed, and yields performance that is on par with or better on unsupervised 2D and 3D object-centric scene decomposition and understanding",
    "checked": false,
    "id": "12f361a506b0842aa8a83a5787650a2a4aa32f05",
    "semantic_title": "parallelized spatiotemporal binding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jKnW7r7de1": {
    "title": "BetterV: Controlled Verilog Generation with Discriminative Guidance",
    "volume": "poster",
    "abstract": "Due to the growing complexity of modern Integrated Circuits (ICs), there is a need for automated circuit design methods. Recent years have seen increasing research in hardware design language generation to facilitate the design process. In this work, we propose a Verilog generation framework, BetterV, which fine-tunes large language models (LLMs) on processed domain-specific datasets and incorporates generative discriminators for guidance on particular design demands. Verilog modules are collected, filtered, and processed from the internet to form a clean and abundant dataset. Instruct-tuning methods are specially designed to fine-tune the LLMs to understand knowledge about Verilog. Furthermore, data are augmented to enrich the training set and are also used to train a generative discriminator on particular downstream tasks, providing guidance for the LLMs to optimize Verilog implementation. BetterV has the ability to generate syntactically and functionally correct Verilog, outperforming GPT-4 on the VerilogEval benchmark. With the help of task-specific generative discriminators, BetterV achieves remarkable improvements on various electronic design automation (EDA) downstream tasks, including netlist node reduction for synthesis and verification runtime reduction with Boolean Satisfiability (SAT) solving",
    "checked": true,
    "id": "bfeba7fc78b16da9c23c91d280e56f13e936e04d",
    "semantic_title": "betterv: controlled verilog generation with discriminative guidance",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=FVmqX0sYz9": {
    "title": "Auditing Private Prediction",
    "volume": "poster",
    "abstract": "Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of an algorithm, while empirical auditing establishes a practical lower bound. Auditing techniques exist for DP training algorithms. However machine learning can also be made private at inference. We propose the first framework for auditing private prediction where we instantiate adversaries with varying poisoning and query capabilities. This enables us to study the privacy leakage of four private prediction algorithms: PATE (Papernot et al., 2016), CaPC (Choquette-Choo et al., 2020), PromptPATE (Duan et al., 2023), and Private-kNN (Zhu et al., 2020). To conduct our audit, we introduce novel techniques to empirically evaluate privacy leakage in terms of Renyi DP. Our experiments show that (i) the privacy analysis of private prediction can be improved, (ii) algorithms which are easier to poison lead to much higher privacy leakage, and (iii) the privacy leakage is significantly lower for adversaries without query control than those with full control",
    "checked": true,
    "id": "2b5662dae7f2563ea94a04010f4d02910840fa55",
    "semantic_title": "auditing private prediction",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=j56JAd29uH": {
    "title": "FADAS: Towards Federated Adaptive Asynchronous Optimization",
    "volume": "poster",
    "abstract": "Federated learning (FL) has emerged as a widely adopted training paradigm for privacy-preserving machine learning. While the SGD-based FL algorithms have demonstrated considerable success in the past, there is a growing trend towards adopting adaptive federated optimization methods, particularly for the training of large-scale models. However, the conventional synchronous aggregation design poses a significant challenge to the practical deployment of those adaptive federated optimization methods, particularly in the presence of straggler clients. To fill this research gap, this paper introduces federated adaptive asynchronous optimization, named FADAS, a novel method that incorporates asynchronous updates into adaptive federated optimization with provable guarantees. To further enhance the efficiency and resilience of our proposed method in scenarios with significant asynchronous delays, we also extend FADAS with a delay-adaptive learning adjustment strategy. We rigorously establish the convergence rate of the proposed algorithms and empirical results demonstrate the superior performance of FADAS over other asynchronous FL baselines",
    "checked": false,
    "id": "0086fbd72a940abda75bf8328b178e2ccaccf8a7",
    "semantic_title": "federated online and bandit convex optimization",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=zMue490KMr": {
    "title": "Deep Networks Always Grok and Here is Why",
    "volume": "poster",
    "abstract": "Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on the local complexity of a DNN's input-output mapping. Our local complexity measures the density of so-called ``linear regions'' (aka, spline partition regions) that tile the DNN input space and serves as a utile progress measure for training. We provide the first evidence that, for classification problems, the linear regions undergo a phase transition during training whereafter they migrate away from the training samples (making the DNN mapping smoother there) and towards the decision boundary (making the DNN mapping less smooth there). Grokking occurs post phase transition as a robust partition of the input space thanks to the linearization of the DNN mapping around the training points. Web: https://bit.ly/grok-adversarial",
    "checked": true,
    "id": "69d15a3ec038a9396181d2f813ec6da2018e4d40",
    "semantic_title": "deep networks always grok and here is why",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mkbSXxovP5": {
    "title": "Double Stochasticity Gazes Faster: Snap-Shot Decentralized Stochastic Gradient Tracking Methods",
    "volume": "poster",
    "abstract": "In decentralized optimization, $m$ agents form a network and only communicate with their neighbors, which gives advantages in data ownership, privacy, and scalability. At the same time, decentralized stochastic gradient descent ($\\texttt{SGD}$) methods, as popular decentralized algorithms for training large-scale machine learning models, have shown their superiority over centralized counterparts. Distributed stochastic gradient tracking $\\texttt{DSGT}$ has been recognized as the popular and state-of-the-art decentralized $\\texttt{SGD}$ method due to its proper theoretical guarantees. However, the theoretical analysis of $\\texttt{DSGT}$ shows that its iteration complexity is $\\tilde{\\mathcal{O}} \\left(\\frac{\\bar{\\sigma}^2}{m\\mu \\varepsilon} + \\frac{\\sqrt{L}\\bar{\\sigma}}{\\mu(1 - \\lambda_2(W))^{1/2} C_W \\sqrt{\\varepsilon} }\\right)$, where the doubly stochastic matrix $W$ represents the network topology and $ C_W $ is a parameter that depends on $W$. Thus, it indicates that the convergence property of $\\texttt{DSGT}$ is heavily affected by the topology of the communication network. To overcome the weakness of $\\texttt{DSGT}$, we resort to the snap-shot gradient tracking skill and propose two novel algorithms, snap-shot $\\texttt{DSGT}$ ($\\texttt{SS-DSGT}$) and accelerated snap-shot $\\texttt{DSGT}$ ($\\texttt{ASS-DSGT}$). We further justify that $\\texttt{SS-DSGT}$ exhibits a lower iteration complexity compared to $\\texttt{DSGT}$ in the general communication network topology. Additionally, $\\texttt{ASS-DSGT}$ matches $\\texttt{DSGT}$'s iteration complexity $\\mathcal{O}\\left( \\frac{\\bar{\\sigma}^2}{m\\mu \\varepsilon} + \\frac{\\sqrt{L}\\bar{\\sigma}}{\\mu (1 - \\lambda_2(W))^{1/2}\\sqrt{\\varepsilon}} \\right)$ under the same conditions as $\\texttt{DSGT}$. Numerical experiments validate $\\texttt{SS-DSGT}$'s superior performance performance in the general communication network topology and exhibit better practical performance of $\\texttt{ASS-DSGT}$ on the specified $W$ compared to $\\texttt{DSGT}$",
    "checked": false,
    "id": "aee13248080b44c7286137f4f35b0e1d63938454",
    "semantic_title": "snap-shot decentralized stochastic gradient tracking methods",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8uzBOVmh8H": {
    "title": "CLLMs: Consistency Large Language Models",
    "volume": "poster",
    "abstract": "Jacobi decoding shows promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into more parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point in a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks",
    "checked": true,
    "id": "a1849a77644ff411a03833b5aa7a65ff57158c50",
    "semantic_title": "cllms: consistency large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Ug1m4P4AKf": {
    "title": "ELF: Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis",
    "volume": "poster",
    "abstract": "In this work, we propose a novel method for modeling numerous speakers, which enables expressing the overall characteristics of speakers in detail like a trained multi-speaker model without additional training on the target speaker's dataset. Although various works with similar purposes have been actively studied, their performance has not yet reached that of trained multi-speaker models due to their fundamental limitations. To overcome previous limitations, we propose effective methods for feature learning and representing target speakers' speech characteristics by discretizing the features and conditioning them to a speech synthesis model. Our method obtained a significantly higher similarity mean opinion score (SMOS) in subjective similarity evaluation than seen speakers of a high-performance multi-speaker model, even with unseen speakers. The proposed method also outperforms a zero-shot method by significant margins. Furthermore, our method shows remarkable performance in generating new artificial speakers. In addition, we demonstrate that the encoded latent features are sufficiently informative to reconstruct an original speaker's speech completely. It implies that our method can be used as a general methodology to encode and reconstruct speakers' characteristics in various tasks",
    "checked": false,
    "id": "76dc4d5e222dee8322c9155e145cad58f1f21018",
    "semantic_title": "encoding speaker-specific latent speech feature for speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2W3KUAaZgO": {
    "title": "Implicit Representations via Operator Learning",
    "volume": "poster",
    "abstract": "The idea of representing a signal as the weights of a neural network, called *Implicit Neural Representations* (INRs), has led to exciting implications for compression, view synthesis and 3D volumetric data understanding. One problem in this setting pertains to the use of INRs for downstream processing tasks. Despite some conceptual results, this remains challenging because the INR for a given image/signal often exists in isolation. What does the neighborhood around a given INR correspond to? Based on this question, we offer an operator theoretic reformulation of the INR model, which we call Operator INR (or O-INR). At a high level, instead of mapping positional encodings to a signal, O-INR maps one function space to another function space. A practical form of this general casting is obtained by appealing to Integral Transforms. The resultant model does not need multi-layer perceptrons (MLPs), used in most existing INR models -- we show that convolutions are sufficient and offer benefits including numerically stable behavior. We show that O-INR can easily handle most problem settings in the literature, and offers a similar performance profile as baselines. These benefits come with minimal, if any, compromise. Our code is available at https://github.com/vsingh-group/oinr",
    "checked": false,
    "id": "bed338ecd879183940683f4cfccdd81b068623f7",
    "semantic_title": "implicit causal representation learning via switchable mechanisms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tSjyKR8WIf": {
    "title": "Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping",
    "volume": "poster",
    "abstract": "Humans are able to segment images effortlessly without supervision using perceptual grouping. Here, we propose a counter-intuitive computational approach to solving unsupervised perceptual grouping and segmentation: that they arise *because* of neural noise, rather than in spite of it. We (1) mathematically demonstrate that under realistic assumptions, neural noise can be used to separate objects from each other; (2) that adding noise in a DNN enables the network to segment images even though it was never trained on any segmentation labels; and (3) that segmenting objects using noise results in segmentation performance that aligns with the perceptual grouping phenomena observed in humans, and is sample-efficient. We introduce the Good Gestalt (GG) datasets --- six datasets designed to specifically test perceptual grouping, and show that our DNN models reproduce many important phenomena in human perception, such as illusory contours, closure, continuity, proximity, and occlusion. Finally, we (4) show that our model improves performance on our GG datasets compared to other tested unsupervised models by $24.9$%. Together, our results suggest a novel unsupervised segmentation method requiring few assumptions, a new explanation for the formation of perceptual grouping, and a novel potential benefit of neural noise",
    "checked": true,
    "id": "d46e941db93239887d363c2ea616cb6a5100f28b",
    "semantic_title": "latent noise segmentation: how neural noise leads to the emergence of segmentation and grouping",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dmHHVcHFdM": {
    "title": "Causality Based Front-door Defense Against Backdoor Attack on Language Models",
    "volume": "poster",
    "abstract": "We have developed a new framework based on the theory of causal inference to protect language models against backdoor attacks. Backdoor attackers can poison language models with different types of triggers, such as words, sentences, grammar, and style, enabling them to selectively modify the decision-making of the victim model. However, existing defense approaches are only effective when the backdoor attack form meets specific assumptions, making it difficult to counter diverse backdoor attacks. We propose a new defense framework **F**ront-door **A**djustment for **B**ackdoor **E**limination (FABE) based on causal reasoning that does not rely on assumptions about the form of triggers. This method effectively differentiates between spurious and legitimate associations by creating a 'front door' that maps out the actual causal relationships. The term 'front door' refers to a text that retains the semantic equivalence of the initial input, which is generated by an additional, fine-tuned language model, denoted as the defense model. Our defense experiments against various attack methods at the token, sentence, and syntactic levels reduced the attack success rate from 93.63% to 15.12%, improving the defense effect by 2.91 times compared to the best baseline result of 66.61%, achieving state-of-the-art results. Through ablation study analysis, we analyzed the effect of each module in FABE, demonstrating the importance of complying with the front-door criterion and front-door adjustment formula, which also explains why previous methods failed. Our code to reproduce the experiments is available at: https://github.com/lyr17/Frontdoor-Adjustment-Backdoor-Elimination",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uaExqhJ2Ag": {
    "title": "Fast White-Box Adversarial Streaming Without a Random Oracle",
    "volume": "poster",
    "abstract": "Recently, the question of adversarially robust streaming, where the stream is allowed to depend on the randomness of the streaming algorithm, has gained a lot of attention. In this work, we consider a strong white-box adversarial model (Ajtai et al. PODS 2022), in which the adversary has access to all past random coins and the parameters used by the streaming algorithm. We focus on the sparse recovery problem and extend our result to other tasks such as distinct element estimation and low-rank approximation of matrices and tensors. The main drawback of previous work is that it requires a *random oracle*, which is especially problematic in the streaming model since the amount of randomness is counted in the space complexity of a streaming algorithm. Also, the previous work suffers from large update time. We construct a near-optimal solution for the sparse recovery problem in white-box adversarial streams, based on the subexponentially secure Learning with Errors assumption. Importantly, our solution does not require a random oracle and has a polylogarithmic per item processing time. We also give results in a related white-box adversarially robust distributed model. Our constructions are based on homomorphic encryption schemes satisfying very mild structural properties that are currently satisfied by most known schemes",
    "checked": true,
    "id": "90bd70210e51300c9b76b4359a517b262eb13121",
    "semantic_title": "fast white-box adversarial streaming without a random oracle",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7xzhKEPfBo": {
    "title": "Risk Estimation in a Markov Cost Process: Lower and Upper Bounds",
    "volume": "poster",
    "abstract": "We tackle the problem of estimating risk measures of the infinite-horizon discounted cost of a Markov cost process. The risk measures we study include variance, Value-at-Risk (VaR), and Conditional Value-at-Risk (CVaR). First, we show that estimating any of these risk measures with $\\epsilon$-accuracy, either in expected or high-probability sense, requires at least $\\Omega(1/\\epsilon^2)$ samples. Then, using a truncation scheme, we derive an upper bound for the CVaR and variance estimation. This bound matches our lower bound up to logarithmic factors. Finally, we discuss an extension of our estimation scheme that covers more general risk measures satisfying a certain continuity criterion, such as spectral risk measures and utility-based shortfall risk. To the best of our knowledge, our work is the first to provide lower and upper bounds for estimating any risk measure beyond the mean within a Markovian setting. Our lower bounds also extend to the infinite-horizon discounted costs' mean. Even in that case, our lower bound of $\\Omega(1/\\epsilon^2) $ improves upon the existing $\\Omega(1/\\epsilon)$ bound (Metelli et al. 2023",
    "checked": true,
    "id": "79dff42bfacfb6cc33bc451e489eb75edb68ca0b",
    "semantic_title": "risk estimation in a markov cost process: lower and upper bounds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1bJLl4fY6i": {
    "title": "Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras",
    "volume": "poster",
    "abstract": "This paper proposes an equivariant neural network that takes data in any finite-dimensional semi-simple Lie algebra as input. The corresponding group acts on the Lie algebra as adjoint operations, making our proposed network adjoint-equivariant. Our framework generalizes the Vector Neurons, a simple $\\mathrm{SO}(3)$-equivariant network, from 3-D Euclidean space to Lie algebra spaces, building upon the invariance property of the Killing form. Furthermore, we propose novel Lie bracket layers and geometric channel mixing layers that extend the modeling capacity. Experiments are conducted for the $\\mathfrak{so}(3)$, $\\mathfrak{sl}(3)$, and $\\mathfrak{sp}(4)$ Lie algebras on various tasks, including fitting equivariant and invariant functions, learning system dynamics, point cloud registration, and homography-based shape classification. Our proposed equivariant network shows wide applicability and competitive performance in various domains",
    "checked": true,
    "id": "3a3c047956251e2d54f4f98b452436288171b6b7",
    "semantic_title": "lie neurons: adjoint-equivariant neural networks for semisimple lie algebras",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fv9GLw0LkO": {
    "title": "Accelerating PDE Data Generation via Differential Operator Action in Solution Space",
    "volume": "poster",
    "abstract": "Recent advancements in data-driven approaches, such as Neural Operator (NO), have demonstrated their effectiveness in reducing the solving time of Partial Differential Equations (PDEs). However, one major challenge faced by these approaches is the requirement for a large amount of high-precision training data, which needs significant computational costs during the generation process. To address this challenge, we propose a novel PDE dataset generation algorithm, namely **Diff**erential **O**perator **A**ction in **S**olution space (**DiffOAS**), which speeds up the data generation process and enhances the precision of the generated data simultaneously. Specifically, DiffOAS obtains a few basic PDE solutions and then combines them to get solutions. It applies differential operators on these solutions, a process we call 'operator action', to efficiently generate precise PDE data points. Theoretical analysis shows that the time complexity of DiffOAS method is one order lower than the existing generation method. Experimental results show that DiffOAS accelerates the generation of large-scale datasets with 10,000 instances by 300 times. Even with just 5% of the generation time, NO trained on the data generated by DiffOAS exhibits comparable performance to that using the existing generation method, which highlights the efficiency of DiffOAS",
    "checked": true,
    "id": "e1b5d3ab19370ca2a1eb74300fc2655d2d9ecbc7",
    "semantic_title": "accelerating pde data generation via differential operator action in solution space",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cbZTnjqIib": {
    "title": "Understanding the Impact of Introducing Constraints at Inference Time on Generalization Error",
    "volume": "poster",
    "abstract": "Since machine learning technologies are being used in various practical situations, models with merely low prediction errors might not be satisfactory; prediction errors occurring with a low probability might yield dangerous results in some applications. Therefore, there are attempts to achieve an ML model whose input-output pairs are guaranteed to satisfy given constraints. Among such attempts, many previous works chose the approach of modifying the outputs of an ML model at the inference time to satisfy the constraints. Such a strategy is handy because we can control its output without expensive training or fine-tuning. However, it is unclear whether using constraints only in the inference time degrades a model's predictive performance. This paper analyses how the generalization error bounds change when we only put constraints in the inference time. Our main finding is that a class of loss functions preserves the relative generalization error, i.e., the difference in generalization error compared with the best model will not increase by imposing constraints at the inference time on multi-class classification. Some popular loss functions preserve the relative error, including the softmax cross-entropy loss. On the other hand, we also show that some loss functions do not preserve relative error when we use constraints. Our results suggest the importance of choosing a suitable loss function when we only use constraints in the inference time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=emtXYlBrNF": {
    "title": "AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers",
    "volume": "poster",
    "abstract": "Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a single backward pass. Through extensive evaluations against existing methods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an LRP library at https://github.com/rachtibat/LRP-eXplains-Transformers",
    "checked": true,
    "id": "a2539713a31ee08cc8116c1355da9f96896df145",
    "semantic_title": "attnlrp: attention-aware layer-wise relevance propagation for transformers",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=RtnGLJNtEG": {
    "title": "Compositional Curvature Bounds for Deep Neural Networks",
    "volume": "poster",
    "abstract": "A key challenge that threatens the widespread use of neural networks in safety-critical applications is their vulnerability to adversarial attacks. In this paper, we study the second-order behavior of continuously differentiable deep neural networks, focusing on robustness against adversarial perturbations. First, we provide a theoretical analysis of robustness and attack certificates for deep classifiers by leveraging local gradients and upper bounds on the second derivative (curvature constant). Next, we introduce a novel algorithm to analytically compute provable upper bounds on the second derivative of neural networks. This algorithm leverages the compositional structure of the model to propagate the curvature bound layer-by-layer, giving rise to a scalable and modular approach. The proposed bound can serve as a differentiable regularizer to control the curvature of neural networks during training, thereby enhancing robustness. Finally, we demonstrate the efficacy of our method on classification tasks using the MNIST and CIFAR-10 datasets",
    "checked": true,
    "id": "426a34b46903ce269229493f7825919f7e2c8f4e",
    "semantic_title": "compositional curvature bounds for deep neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jP1zeEqHli": {
    "title": "Learning the Target Network in Function Space",
    "volume": "poster",
    "abstract": "We focus on the task of learning the value function in the reinforcement learning (RL) setting. This task is often solved by updating a pair of online and target networks while ensuring that the parameters of these two networks are equivalent. We propose Lookahead-Replicate (LR), a new value-function approximation algorithm that is agnostic to this parameter-space equivalence. Instead, the LR algorithm is designed to maintain an equivalence between the two networks in the function space. This value-based equivalence is obtained by employing a new target-network update. We show that LR leads to a convergent behavior in learning the value function. We also present empirical results demonstrating that LR-based target-network updates significantly improve deep RL on the Atari benchmark",
    "checked": true,
    "id": "cf0df4e6ac66ea3e76562c13375a92b1139e68b1",
    "semantic_title": "learning the target network in function space",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OI1YP53WKI": {
    "title": "ReDiffuser: Reliable Decision-Making Using a Diffuser with Confidence Estimation",
    "volume": "poster",
    "abstract": "The diffusion model has demonstrated impressive performance in offline reinforcement learning. However, non-deterministic sampling in diffusion models can lead to unstable performance. Furthermore, the lack of confidence measurements makes it difficult to evaluate the reliability and trustworthiness of the sampled decisions. To address these issues, we present ReDiffuser, which utilizes confidence estimation to ensure reliable decision-making. We achieve this by learning a confidence function based on Random Network Distillation. The confidence function measures the reliability of sampled decisions and contributes to quantitative recognition of reliable decisions. Additionally, we integrate the confidence function into task-specific sampling procedures to realize adaptive-horizon planning and value-embedded planning. Experiments show that the proposed ReDiffuser achieves state-of-the-art performance on standard offline RL datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HCDMiaT0Pf": {
    "title": "Adversarially Robust Hypothesis Transfer Learning",
    "volume": "poster",
    "abstract": "In this work, we explore Hypothesis Transfer Learning (HTL) under adversarial attacks. In this setting, a learner has access to a training dataset of size $n$ from an underlying distribution $\\mathcal{D}$ and a set of auxiliary hypotheses. These auxiliary hypotheses, which can be viewed as prior information originating either from expert knowledge or as pre-trained foundation models, are employed as an initialization for the learning process. Our goal is to develop an adversarially robust model for $\\mathcal{D}$. We begin by examining an adversarial variant of the regularized empirical risk minimization learning rule that we term A-RERM. Assuming a non-negative smooth loss function with a strongly convex regularizer, we establish a bound on the robust generalization error of the hypothesis returned by A-RERM in terms of the robust empirical loss and the quality of the initialization. If the initialization is good, i.e., there exists a weighted combination of auxiliary hypotheses with a small robust population loss, the bound exhibits a fast rate of $\\mathcal{O}(1/n)$. Otherwise, we get the standard rate of $\\mathcal{O}(1/\\sqrt{n})$. Additionally, we provide a bound on the robust excess risk which is similar in nature, albeit with a slightly worse rate. We also consider solving the problem using a practical variant, namely proximal stochastic adversarial training, and present a bound that depends on the initialization. This bound has the same dependence on the sample size as the ARERM bound, except for an additional term that depends on the size of the adversarial perturbation",
    "checked": false,
    "id": "190090de9c1dbcf48eaf744975409e2d7357ef46",
    "semantic_title": "distributionally robust transfer learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4BIOZSz7zU": {
    "title": "Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making",
    "volume": "poster",
    "abstract": "Fair decision making has largely been studied with respect to a single decision. Here we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points *within* the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We explore the interplay between non-Markovian fairness and memory and how memory can support construction of fair policies. Finally, we introduce the FairQCM algorithm, which can automatically augment its training data to improve sample efficiency in the synthesis of fair policies via reinforcement learning",
    "checked": true,
    "id": "47e49c575eee9607b873c2f6e80816ec4616f521",
    "semantic_title": "remembering to be fair: non-markovian fairness in sequential decision making",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2zI2scD2Iz": {
    "title": "Hybrid Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using *hybrid RL* -- training on a mixture of online and expert data -- to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formally, we derive a reduction from inverse RL to *expert-competitive RL* (rather than globally optimal RL) that allows us to dramatically reduce interaction during the inner policy search loop while maintaining the benefits of the IRL approach. This allows us to derive both model-free and model-based hybrid inverse RL algorithms with strong policy performance guarantees. Empirically, we find that our approaches are significantly more sample efficient than standard inverse RL and several other baselines on a suite of continuous control tasks",
    "checked": true,
    "id": "598fdcc2550c63104c94728491af6b6f008e2314",
    "semantic_title": "hybrid inverse reinforcement learning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=3tJDnEszco": {
    "title": "CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback",
    "volume": "poster",
    "abstract": "The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and reaction energy barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automatically guide the exploration without human input, providing competitive performance against expert-enumerated chemical descriptor-based implementations. By integrating language-guided reasoning with computational chemistry feedback, our work pioneers AI-accelerated, trustworthy catalyst discovery",
    "checked": true,
    "id": "7c8a6552fe0e3b33456afdac79614e7dc04f0b4d",
    "semantic_title": "chemreasoner: heuristic search over a large language model's knowledge space using quantum-chemical feedback",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=9ANyvRtFGa": {
    "title": "HexGen: Generative Inference of Large Language Model over Heterogeneous Environment",
    "volume": "poster",
    "abstract": "Serving generative inference of the large language model is a crucial component of contemporary AI applications. In this paper, our focus lies in deploying such services in a heterogeneous and cross-datacenter setting to mitigate the substantial inference costs typically associated with a single centralized datacenter. Towards this end, we propose HexGen, a flexible distributed inference engine that uniquely supports the asymmetric partition of generative inference computations over both tensor model parallelism and pipeline parallelism, which allows for effective deployment across diverse GPUs interconnected by a fully heterogeneous network. We further propose a sophisticated scheduling algorithm grounded in constrained optimization that can adaptively assign asymmetric inference computation across the GPUs to fulfill inference requests while maintaining acceptable latency levels. We conduct an extensive empirical study to evaluate the efficiency of HexGen by serving the state-of-the-art Llama-2 (70B) model. The experimental results suggest that HexGen can choose to achieve up to $2.3\\times$ lower latency deadlines or tolerate up to $4\\times$ more traffic request rates compared with the homogeneous baseline given the same budget",
    "checked": true,
    "id": "4d91f837962d8cb586e501f08f610b2ac3448508",
    "semantic_title": "hexgen: generative inference of large language model over heterogeneous environment",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aC1LSa4nXs": {
    "title": "Protein Conformation Generation via Force-Guided SE(3) Diffusion Models",
    "volume": "poster",
    "abstract": "The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes. Traditional physics-based computational methods, such as molecular dynamics (MD) simulations, suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems. Recently, deep generative modeling techniques, especially diffusion models, have been employed to generate novel protein conformations. However, existing score-based diffusion methods cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution. In this paper, to overcome these limitations, we propose a force-guided $\\mathrm{SE}(3)$ diffusion model, ConfDiff, for protein conformation generation. By incorporating a force-guided network with a mixture of data-based score models, ConfDiff can generate protein conformations with rich diversity while preserving high fidelity. Experiments on a variety of protein conformation prediction tasks, including 12 fast-folding proteins and the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method surpasses the state-of-the-art method",
    "checked": true,
    "id": "2516bb58657965236cab56e71a98b9fa7ffc886d",
    "semantic_title": "protein conformation generation via force-guided se(3) diffusion models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=1lDAGDe0UR": {
    "title": "CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables",
    "volume": "poster",
    "abstract": "For Multivariate Time Series Forecasting (MTSF), recent deep learning applications show that univariate models frequently outperform multivariate ones. To address the deficiency in multivariate models, we introduce a method to Construct Auxiliary Time Series (CATS) that functions like a 2D temporal-contextual attention mechanism, which generates Auxiliary Time Series (ATS) from Original Time Series (OTS) to effectively represent and incorporate inter-series relationships for forecasting. Key principles of ATS—continuity, sparsity, and variability—are identified and implemented through different modules. Even with a basic 2-layer MLP as the core predictor, CATS achieves state-of-the-art, significantly reducing complexity and parameters compared to previous multivariate models, marking it as an efficient and transferable MTSF solution",
    "checked": true,
    "id": "09e098f5bc0a187bea27437a78a0e9ae83cf6a3d",
    "semantic_title": "cats: enhancing multivariate time series forecasting by constructing auxiliary time series as exogenous variables",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ld255Mbx9F": {
    "title": "On the Diminishing Returns of Width for Continual Learning",
    "volume": "poster",
    "abstract": "While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from *catastrophic forgetting* when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN), demonstrating that the diminishing returns of increasing widths to reduce forgetting. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory",
    "checked": true,
    "id": "3eb2c5e27b541038ff26fbd1267aaccc5e9c8d45",
    "semantic_title": "on the diminishing returns of width for continual learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0NdU4y9dWC": {
    "title": "Enhancing Size Generalization in Graph Neural Networks through Disentangled Representation Learning",
    "volume": "poster",
    "abstract": "Although most graph neural networks (GNNs) can operate on graphs of any size, their classification performance often declines on graphs larger than those encountered during training. Existing methods insufficiently address the removal of size information from graph representations, resulting in sub-optimal performance and reliance on backbone models. In response, we propose DISGEN, a novel and model-agnostic framework designed to disentangle size factors from graph representations. DISGEN employs size- and task-invariant augmentations and introduces a decoupling loss that minimizes shared information in hidden representations, with theoretical guarantees for its effectiveness. Our empirical results show that DISGEN outperforms the state-of-the-art models by up to 6% on real-world datasets, underscoring its effectiveness in enhancing the size generalizability of GNNs. Our codes are available at: https://github.com/GraphmindDartmouth/DISGEN",
    "checked": true,
    "id": "625920dff26468f86c9bb3b13e31538496bf222e",
    "semantic_title": "enhancing size generalization in graph neural networks through disentangled representation learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1v1oFF3aw0": {
    "title": "Not all distributional shifts are equal: Fine-grained robust conformal inference",
    "volume": "poster",
    "abstract": "We introduce a fine-grained framework for uncertainty quantification of predictive models under distributional shifts. This framework distinguishes the shift in covariate distributions from that in the conditional relationship between the outcome ($Y$) and the covariates ($X$). We propose to reweight the training samples to adjust for an identifiable shift in covariate distribution while protecting against the worst-case conditional distribution shift bounded in an $f$-divergence ball. Based on ideas from conformal inference and distributionally robust learning, we present an algorithm that outputs (approximately) valid and efficient prediction intervals in the presence of distributional shifts. As a use case, we apply the framework to sensitivity analysis of individual treatment effects with hidden confounding. The proposed methods are evaluated in simulations and four real data applications, demonstrating superior robustness and efficiency compared with existing benchmarks",
    "checked": true,
    "id": "c419b6952b204798b5a36d04e4a82b75aabeb581",
    "semantic_title": "not all distributional shifts are equal: fine-grained robust conformal inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aXD94eATtT": {
    "title": "Improving Open-Ended Text Generation via Adaptive Decoding",
    "volume": "poster",
    "abstract": "Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that dynamically empowers language models to ascertain a sensible candidate set during generation. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence. Experimental results reveal that our method balances diversity and coherence well. The human evaluation shows that our method can generate human-preferred text. Additionally, our method can potentially improve the reasoning ability of language models",
    "checked": true,
    "id": "0a96634f5b5d80ad0d4eea7d10b1fd4535ca5a9d",
    "semantic_title": "improving open-ended text generation via adaptive decoding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BiENLaUwlK": {
    "title": "Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation",
    "volume": "poster",
    "abstract": "A key aspect of Safe Reinforcement Learning (Safe RL) involves estimating the constraint condition for the next policy, which is crucial for guiding the optimization of safe policy updates. However, the existing *Advantage-based Estimation* (ABE) method relies on the infinite-horizon discounted advantage function. This dependence leads to catastrophic errors in finite-horizon scenarios with non-discounted constraints, resulting in safety-violation updates. In response, we propose the first estimation method for finite-horizon non-discounted constraints in deep Safe RL, termed *Gradient-based Estimation* (GBE), which relies on the analytic gradient derived along trajectories. Our theoretical and empirical analyses demonstrate that GBE can effectively estimate constraint changes over a finite horizon. Constructing a surrogate optimization problem with GBE, we developed a novel Safe RL algorithm called *Constrained Gradient-based Policy Optimization* (CGPO). CGPO identifies feasible optimal policies by iteratively resolving sub-problems within trust regions. Our empirical results reveal that CGPO, unlike baseline algorithms, successfully estimates the constraint functions of subsequent policies, thereby ensuring the efficiency and feasibility of each update",
    "checked": false,
    "id": "037d5ef2da31b6e5f4d7e18a49ebd4c679b4b121",
    "semantic_title": "safe reinforcement learning under temporal logic with reward design and quantum action selection",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=ViZcgDQjyG": {
    "title": "Position: Will we run out of data? Limits of LLM scaling based on human-generated data",
    "volume": "poster",
    "abstract": "We investigate the potential constraints on LLM scaling posed by the availability of public human-generated text data. We forecast the growing demand for training data based on current trends and estimate the total stock of public human text data. Our findings indicate that if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained. We explore how progress in language modeling can continue when human-generated text datasets cannot be scaled any further. We argue that synthetic data generation, transfer learning from data-rich domains, and data efficiency improvements might support further progress",
    "checked": false,
    "id": "6cf65eb8aa66116e14a97bb8f71552359ff814ba",
    "semantic_title": "will we run out of data? limits of llm scaling based on human-generated data",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=7DbIyQlfaO": {
    "title": "Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension",
    "volume": "poster",
    "abstract": "We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness of our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs",
    "checked": true,
    "id": "bea410bb93f6a5a51463db8842ecaac009b89650",
    "semantic_title": "characterizing truthfulness in large language model generations with local intrinsic dimension",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=EDEISRmi6X": {
    "title": "Bipartite Matching in Massive Graphs: A Tight Analysis of EDCS",
    "volume": "poster",
    "abstract": "Maximum matching is one of the most fundamental combinatorial optimization problems with applications in various contexts such as balanced clustering, data mining, resource allocation, and online advertisement. In many of these applications, the input graph is massive. The sheer size of these inputs makes it impossible to store the whole graph in the memory of a single machine and process it there. Graph sparsification has been an extremely powerful tool to alleviate this problem. In this paper, we study a highly successful and versatile sparsifier for the matching problem: the *edge-degree constrained subgraph (EDCS)* introduced first by Bernstein & Stein 2015 The EDCS has a parameter $\\beta \\geq 2$ which controls the density of the sparsifier. It has been shown through various proofs in the literature that by picking a subgraph with $O(n\\beta)$ edges, the EDCS includes a matching of size at least $2/3-O(1/\\beta)$ times the maximum matching size. As such, by increasing $\\beta$ the approximation ratio of EDCS gets closer and closer to $2/3$. In this paper, we propose a new approach for analyzing the approximation ratio of EDCS. Our analysis is *tight* for any value of $\\beta$. Namely, we pinpoint the precise approximation ratio of EDCS for any sparsity parameter $\\beta$. Our analysis reveals that one does not necessarily need to increase $\\beta$ to improve approximation, as suggested by previous analysis. In particular, the best choice turns out to be $\\beta = 6$, which achieves an approximation ratio of $.677$! This is arguably surprising as it is even better than $2/3 \\sim .666$, the bound that was widely believed to be the limit for EDCS",
    "checked": true,
    "id": "cbd50dd9eb4708e7433baf9572a22fec9a07b27e",
    "semantic_title": "bipartite matching in massive graphs: a tight analysis of edcs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5pg9YJBaiG": {
    "title": "Creative Text-to-Audio Generation via Synthesizer Programming",
    "volume": "poster",
    "abstract": "Neural audio synthesis methods now allow specifying ideas in natural language. However, these methods produce results that cannot be easily tweaked, as they are based on large latent spaces and up to billions of uninterpretable parameters. We propose a text-to-audio generation method that leverages a virtual modular sound synthesizer with only 78 parameters. Synthesizers have long been used by skilled sound designers for media like music and film due to their flexibility and intuitive controls. Our method, CTAG, iteratively updates a synthesizer's parameters to produce high-quality audio renderings of text prompts that can be easily inspected and tweaked. Sounds produced this way are also more abstract, capturing essential conceptual features over fine-grained acoustic details, akin to how simple sketches can vividly convey visual concepts. Our results show how CTAG produces sounds that are distinctive, perceived as artistic, and yet similarly identifiable to recent neural audio synthesis models, positioning it as a valuable and complementary tool",
    "checked": true,
    "id": "b23f3a01ebd9b06da95f74d5f3b846fdc1fb0c61",
    "semantic_title": "creative text-to-audio generation via synthesizer programming",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qKL25sGjxL": {
    "title": "GiLOT: Interpreting Generative Language Models via Optimal Transport",
    "volume": "poster",
    "abstract": "While large language models (LLMs) surge with the rise of generative AI, algorithms to explain LLMs highly desire. Existing feature attribution methods adequate for discriminative language models like BERT often fail to deliver faithful explanations for LLMs, primarily due to two issues: (1) For every specific prediction, the LLM outputs a probability distribution over the vocabulary–a large number of tokens with unequal semantic distance; (2) As an autoregressive language model, the LLM handles input tokens while generating a sequence of probability distributions of various tokens. To address above two challenges, this work proposes GiLOT that leverages Optimal Transport to measure the distributional change of all possible generated sequences upon the absence of every input token, while taking into account the tokens' similarity, so as to faithfully estimate feature attribution for LLMs. We have carried out extensive experiments on top of Llama families and their fine-tuned derivatives across various scales to validate the effectiveness of GiLOT for estimating the input attributions. The results show that GiLOT outperforms existing solutions on a number of faithfulness metrics under fair comparison settings. Source code is publicly available at https://github.com/holyseven/GiLOT",
    "checked": false,
    "id": "770731115023c18c1082675db75cc44089eedc6c",
    "semantic_title": "pointot: interpretable geometry-inspired point cloud generative model via optimal transport",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=C4OpREezgj": {
    "title": "AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training",
    "volume": "poster",
    "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the multi-step reasoning capabilities of LLMs by using tree-search algorithms. These methods rely on prompting a pre-trained model to serve as a value function and focus on problems with low search depth. As a result, these methods cannot benefit from in-domain training and only rely on pretraining process — they will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning. To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding. TS-LLM distinguishes itself in two key ways. (1) Leveraging a learned value function and AlphaZero-like algorithms, our approach can be generally adaptable to a wide range of tasks, language models of any size, and tasks of varying search depths. (2) Our approach can guide LLMs during both inference and training, iteratively improving the LLMs. Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64",
    "checked": true,
    "id": "e8df1cf6742b50a15500b8dd3dde3942e9c91418",
    "semantic_title": "alphazero-like tree-search can guide large language model decoding and training",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=XXioxiADDC": {
    "title": "CW Complex Hypothesis for Image Data",
    "volume": "poster",
    "abstract": "We examine both the manifold hypothesis (Bengio et al., 2013) and the union of manifold hypothesis (Brown et al., 2023), and argue that, in contrast to these hypotheses, the local intrinsic dimension varies from point to point even in the same connected component. We propose an alternative CW complex hypothesis that image data is distributed in ``manifolds with skeletons\". We support the hypothesis through visualization of distributions of image data of random geometric objects, as well as by introducing and testing a criterion on natural image datasets. One motivation of our work is to explain why diffusion models have difficulty generating accurate higher dimensional details such as human hands. Under the CW complex hypothesis and with both theoretical and empirical evidences, we provide an interpretation that the mixture of higher and lower dimensional components in data obstructs diffusion models from efficient learning",
    "checked": false,
    "id": "86297e9cfdb22405188c20b76c7cd4427a343820",
    "semantic_title": "the amyloid cascade hypothesis: an updated critical review",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=aoAPOOtN9E": {
    "title": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have been routinely used to solve various tasks using step-by-step reasoning. However, the structure of intermediate reasoning steps, or *thoughts*, is rigid and unidirectional, such as chains, trees, or acyclic-directed graphs. Consequently, the resulting inflexible and forward-only reasoning may not address challenging tasks and fail when the LLM frequently gives false responses, i.e., hallucinations. This paper proposes a new reasoning framework, called *Thought Rollback* (TR), allowing LLMs to adaptively build thought structure while maintaining effective reasoning toward problem-solving under hallucinations. The core mechanism of TR is *rolling back thoughts*, which allows LLMs to perform error analysis on thoughts, and thus roll back to any previously mistaken thought for revision. Subsequently, by including such trial-and-error in the prompt to guide the LLM, each rollback leads to one more reliable reasoning path. Therefore, starting with a simple prompt without human annotations, LLM with TR adaptively and gradually explores thoughts for a correct solution. Comprehensive experiments on mathematical problems and multi-task reasoning demonstrate the state-of-the-art performance of TR in terms of problem-solving rate and interaction cost. For instance, the solving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH dataset. The source code is available under the folder *examples/ThoughtRollback* of https://github.com/iQua/llmpebase",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAbZExIIgG": {
    "title": "Decomposable Submodular Maximization in Federated Setting",
    "volume": "poster",
    "abstract": "Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a *federated optimization* setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular *continuous greedy* algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is performed only on a subsampled set. Further, the aggregation is performed only intermittently between stretches of parallel local steps, which reduces communication cost significantly. We show that our federated algorithm is guaranteed to provide a good approximate solution, even in the presence of above cost-cutting measures. Finally, we show how the federated setting can be incorporated in solving fundamental discrete submodular optimization problems such as Maximum Coverage and Facility Location",
    "checked": true,
    "id": "9c45098d516bef4345bcc766c860c8720cb5fe23",
    "semantic_title": "decomposable submodular maximization in federated setting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8296yUBoXr": {
    "title": "DIDI: Diffusion-Guided Diversity for Offline Behavioral Generation",
    "volume": "poster",
    "abstract": "In this paper, we propose a novel approach called DIffusion-guided DIversity (DIDI) for offline behavioral generation. The goal of DIDI is to learn a diverse set of skills from a mixture of label-free offline data. We achieve this by leveraging diffusion probabilistic models as priors to guide the learning process and regularize the policy. By optimizing a joint objective that incorporates diversity and diffusion-guided regularization, we encourage the emergence of diverse behaviors while maintaining the similarity to the offline data. Experimental results in four decision-making domains (Push, Kitchen, Humanoid, and D4RL tasks) show that DIDI is effective in discovering diverse and discriminative skills. We also introduce skill stitching and skill interpolation, which highlight the generalist nature of the learned skill space. Further, by incorporating an extrinsic reward function, DIDI enables reward-guided behavior generation, facilitating the learning of diverse and optimal behaviors from sub-optimal data",
    "checked": true,
    "id": "fef9ebfca6253665f3de9247b30baff3f6524224",
    "semantic_title": "didi: diffusion-guided diversity for offline behavioral generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MV2b44zDd3": {
    "title": "Consistent Adversarially Robust Linear Classification: Non-Parametric Setting",
    "volume": "poster",
    "abstract": "For binary classification in $d$ dimensions, it is known that with a sample size of $n$, an excess adversarial risk of $O(d/n)$ is achievable under strong parametric assumptions about the underlying data distribution (e.g., assuming a Gaussian mixture model). In the case of well-separated distributions, this rate can be further refined to $O(1/n)$. Our work studies the non-parametric setting, where very little is known. With only mild regularity conditions on the conditional distribution of the features, we examine adversarial attacks with respect to arbitrary norms and introduce a straightforward yet effective estimator with provable consistency w.r.t adversarial risk. Our estimator is given by minimizing a series of smoothed versions of the robust 0/1 loss, with a smoothing bandwidth that adapts to both $n$ and $d$. Furthermore, we demonstrate that our estimator can achieve the minimax excess adversarial risk of $\\widetilde O(\\sqrt{d/n})$ for linear classifiers, at the cost of solving possibly rougher optimization problems",
    "checked": false,
    "id": "b427dbff2e4a544430cf8b3e125f201bac0b28f2",
    "semantic_title": "learning to be adversarially robust and differentially private",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=8m4V6Fx6ma": {
    "title": "Optimal Exact Recovery in Semi-Supervised Learning: A Study of Spectral Methods and Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "We delve into the challenge of semi-supervised node classification on the Contextual Stochastic Block Model (CSBM) dataset. Here, nodes from the two-cluster Stochastic Block Model (SBM) are coupled with feature vectors, which are derived from a Gaussian Mixture Model (GMM) that corresponds to their respective node labels. With only a subset of the CSBM node labels accessible for training, our primary objective becomes the accurate classification of the remaining nodes. Venturing into the transductive learning landscape, we, for the first time, pinpoint the information-theoretical threshold for the exact recovery of all test nodes in CSBM. Concurrently, we design an optimal spectral estimator inspired by Principal Component Analysis (PCA) with the training labels and essential data from both the adjacency matrix and feature vectors. We also evaluate the efficacy of graph ridge regression and Graph Convolutional Networks (GCN) on this synthetic dataset. Our findings underscore that graph ridge regression and GCN possess the ability to achieve the information threshold of exact recovery in a manner akin to the optimal estimator when using the optimal weighted self-loops. This highlights the potential role of feature learning in augmenting the proficiency of GCN, especially in the realm of semi-supervised learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=39UqOkTjFn": {
    "title": "When Do Skills Help Reinforcement Learning? A Theoretical Analysis of Temporal Abstractions",
    "volume": "poster",
    "abstract": "Skills are temporal abstractions that are intended to improve reinforcement learning (RL) performance through hierarchical RL. Despite our intuition about the properties of an environment that make skills useful, a precise characterization has been absent. We provide the first such characterization, focusing on the utility of deterministic skills in deterministic sparse-reward environments with finite action spaces. We show theoretically and empirically that RL performance gain from skills is worse in environments where solutions to states are less compressible. Additional theoretical results suggest that skills benefit exploration more than they benefit learning from existing experience, and that using unexpressive skills such as macroactions may worsen RL performance. We hope our findings can guide research on automatic skill discovery and help RL practitioners better decide when and how to use skills",
    "checked": true,
    "id": "e6d22bc95ad8afe6d17634784a6a3f2b42bd5ae6",
    "semantic_title": "when do skills help reinforcement learning? a theoretical analysis of temporal abstractions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xSizvCoI79": {
    "title": "Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning",
    "volume": "poster",
    "abstract": "Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models with S2N translation can process 183 -- 711 times more subgraph samples than state-of-the-art models at a better or similar performance level",
    "checked": true,
    "id": "2dcaffe196afe65912db5034ab5c678f1fa56b23",
    "semantic_title": "translating subgraphs to nodes makes simple gnns strong and efficient for subgraph representation learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=OdsZS0E0AO": {
    "title": "Compact Optimality Verification for Optimization Proxies",
    "volume": "poster",
    "abstract": "Recent years have witnessed increasing interest in optimization proxies, i.e., machine learning models that approximate the input-output mapping of parametric optimization problems and return near-optimal feasible solutions. Following recent work by (Nellikkath & Chatzivasileiadis, 2021), this paper reconsiders the optimality verification problem for optimization proxies, i.e., the determination of the worst-case optimality gap over the instance distribution. The paper proposes a compact formulation for optimality verification and a gradient-based primal heuristic that brings significant computational benefits to the original formulation. The compact formulation is also more general and applies to non-convex optimization problems. The benefits of the compact formulation are demonstrated on large-scale DC Optimal Power Flow and knapsack problems",
    "checked": true,
    "id": "b2ad769c6bf1431d0209f1f74660a2d0f055b817",
    "semantic_title": "compact optimality verification for optimization proxies",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W4mLp5KuKl": {
    "title": "Generalization Analysis for Multi-Label Learning",
    "volume": "poster",
    "abstract": "Despite great advances in algorithms for multi-label learning, research on the theoretical analysis of generalization is still in the early stage. Some recent theoretical results has investigated the generalization performance of multi-label learning under several evaluation metrics, however, how to reduce the dependency on the number of labels, explicitly introduce label correlations, and quantitatively analyze the impact of various inductive biases in the generalization analysis of multi-label learning is still a crucial and open problem. In an attempt to make up for the gap in the generalization theory of multi-label learning, we develop several novel vector-contraction inequalities, which exploit the Lipschitz continuity of loss functions, and derive generalization bounds with a weaker dependency on the number of labels than the state of the art in the case of decoupling the relationship among different components, which serves as theoretical guarantees for the generalization of multi-label learning. In addition, we derive the generalization bound for Macro-Averaged AUC and analyze its relationship with class-imbalance. The mild bounds without strong assumptions explain the good generalization ability of multi-label learning with first-order label correlations and high-order label correlations induced by norm regularizers",
    "checked": false,
    "id": "3309aef7d57717f59c2c210fa39fb5a030e8417f",
    "semantic_title": "towards understanding generalization of macro-auc in multi-label learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=myCgfQZzbc": {
    "title": "BeigeMaps: Behavioral Eigenmaps for Reinforcement Learning from Images",
    "volume": "poster",
    "abstract": "Training reinforcement learning (RL) agents directly from high-dimensional image observations continues to be a challenging problem. Recent line of work on behavioral distances proposes to learn representations that encode behavioral similarities quantified by the bisimulation metric. By learning an isometric mapping to a lower dimensional space that preserves this metric, such methods attempt to learn representations that group together functionally similar states. However, such an isometric mapping may not exist, making the learning objective ill-defined. We propose an alternative objective that allows distortions in long-range distances, while preserving *local* metric structure -- inducing representations that highlight natural clusters in the state space. This leads to new representations, which we term Behavioral Eigenmaps (BeigeMaps), corresponding to the eigenfunctions of similarity kernels induced by behavioral distances. We empirically demonstrate that when added as a drop-in modification, BeigeMaps improve the policy performance of prior behavioral distance based RL algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zdTOOgutk": {
    "title": "Unsupervised Episode Generation for Graph Meta-learning",
    "volume": "poster",
    "abstract": "We propose Unsupervised Episode Generation method called **Neighbors as Queries (NaQ)** to solve the Few-Shot Node-Classification (FSNC) task by *unsupervised Graph Meta-learning*. Doing so enables full utilization of the information of all nodes in a graph, which is not possible in current supervised meta-learning methods for FSNC due to the label-scarcity problem. In addition, unlike unsupervised Graph Contrastive Learning (GCL) methods that overlook the downstream task to be solved at the training phase resulting in vulnerability to class imbalance of a graph, we adopt the episodic learning framework that allows the model to be aware of the downstream task format, i.e., FSNC. The proposed NaQ is a simple but effective *unsupervised* episode generation method that randomly samples nodes from a graph to make a support set, followed by similarity-based sampling of nodes to make the corresponding query set. Since NaQ is *model-agnostic*, any existing supervised graph meta-learning methods can be trained in an unsupervised manner, while not sacrificing much of their performance or sometimes even improving them. Extensive experimental results demonstrate the effectiveness of our proposed unsupervised episode generation method for graph meta-learning towards the FSNC task. Our code is available at: https://github.com/JhngJng/NaQ-PyTorch",
    "checked": true,
    "id": "a7430dae143c96af570e4b80e74d9fbe003cf98b",
    "semantic_title": "unsupervised episode generation for graph meta-learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eo88noTbb5": {
    "title": "Agnostic Learning of Mixed Linear Regressions with EM and AM Algorithms",
    "volume": "poster",
    "abstract": "Mixed linear regression is a well-studied problem in parametric statistics and machine learning. Given a set of samples, tuples of covariates and labels, the task of mixed linear regression is to find a small list of linear relationships that best fit the samples. Usually it is assumed that the label is generated stochastically by randomly selecting one of two or more linear functions, applying this chosen function to the covariates, and potentially introducing noise to the result. In that situation, the objective is to estimate the ground-truth linear functions up to some parameter error. The popular expectation maximization (EM) and alternating minimization (AM) algorithms have been previously analyzed for this. In this paper, we consider the more general problem of agnostic learning of mixed linear regression from samples, without such generative models. In particular, we show that the AM and EM algorithms, under standard conditions of separability and good initialization, lead to agnostic learning in mixed linear regression by converging to the population loss minimizers, for suitably defined loss functions. In some sense, this shows the strength of AM and EM algorithms that converges to ``optimal solutions'' even in the absence of realizable generative models",
    "checked": true,
    "id": "abb6d0271ab87a2215218422f07841014e568d4b",
    "semantic_title": "agnostic learning of mixed linear regressions with em and am algorithms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rEZ24oJhbn": {
    "title": "UPOCR: Towards Unified Pixel-Level OCR Interface",
    "volume": "poster",
    "abstract": "Existing optical character recognition (OCR) methods rely on task-specific designs with divergent paradigms, architectures, and training strategies, which significantly increases the complexity of research and maintenance and hinders the fast deployment in applications. To this end, we propose UPOCR, a simple-yet-effective generalist model for Unified Pixel-level OCR interface. Specifically, the UPOCR unifies the paradigm of diverse OCR tasks as image-to-image transformation and the architecture as a vision Transformer (ViT)-based encoder-decoder with learnable task prompts. The prompts push the general feature representations extracted by the encoder towards task-specific spaces, endowing the decoder with task awareness. Moreover, the model training is uniformly aimed at minimizing the discrepancy between the predicted and ground-truth images regardless of the inhomogeneity among tasks. Experiments are conducted on three pixel-level OCR tasks including text removal, text segmentation, and tampered text detection. Without bells and whistles, the experimental results showcase that the proposed method can simultaneously achieve state-of-the-art performance on three tasks with a unified single model, which provides valuable strategies and insights for future research on generalist OCR models. Code is available at https://github.com/shannanyinxiang/UPOCR",
    "checked": true,
    "id": "adee8299464cbd308db9d6f2daf73c7dbf83977a",
    "semantic_title": "upocr: towards unified pixel-level ocr interface",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=qg6AlnpEQH": {
    "title": "On the Emergence of Cross-Task Linearity in Pretraining-Finetuning Paradigm",
    "volume": "poster",
    "abstract": "The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL). Specifically, we show that if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are often approximately equal to the linear interpolation of features in two finetuned models at each layer. We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks approximately function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, particularly by translating operations from the parameter space to the feature space. Furthermore, we delve deeper into the root cause for the emergence of CTL, highlighting the role of pretraining",
    "checked": false,
    "id": "c9f2588a0e60f09ac1e57538edaf42a6a94465e5",
    "semantic_title": "on the emergence of cross-task linearity in the pretraining-finetuning paradigm",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kt4fwiuKqf": {
    "title": "Path-Guided Particle-based Sampling",
    "volume": "poster",
    "abstract": "Particle-based Bayesian inference methods by sampling from a partition-free target (posterior) distribution, e.g., Stein variational gradient descent (SVGD), have attracted significant attention. We propose a path-guided particle-based sampling (PGPS) method based on a novel Log-weighted Shrinkage (LwS) density path linking an initial distribution to the target distribution. We propose to utilize a Neural network to learn a vector field motivated by the Fokker-Planck equation of the designed density path. Particles, initiated from the initial distribution, evolve according to the ordinary differential equation defined by the vector field. The distribution of these particles is guided along a density path from the initial distribution to the target distribution. The proposed LwS density path allows for an efficient search of modes of the target distribution while canonical methods fail. We theoretically analyze the Wasserstein distance of the distribution of the PGPS-generated samples and the target distribution due to approximation and discretization errors. Practically, the proposed PGPS-LwS method demonstrates higher Bayesian inference accuracy and better calibration ability in experiments conducted on both synthetic and real-world Bayesian learning tasks, compared to baselines, such as SVGD and Langevin dynamics, etc",
    "checked": false,
    "id": "e74359c8181bb1810f390958d2fb8d05d83b09f7",
    "semantic_title": "manifold path guiding for importance sampling specular chains",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HOMXUneCTR": {
    "title": "Towards Understanding Inductive Bias in Transformers: A View From Infinity",
    "volume": "poster",
    "abstract": "We study inductive bias in Transformers in the infinitely over-parameterized Gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space. We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. We present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. We show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length. Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry",
    "checked": true,
    "id": "9efa97c717f2e1775d419b22bf8b34663281a224",
    "semantic_title": "towards understanding inductive bias in transformers: a view from infinity",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=NEv8YqBROO": {
    "title": "LoRA+: Efficient Low Rank Adaptation of Large Models",
    "volume": "poster",
    "abstract": "In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in (Hu et al., 2021) leads to suboptimal finetuning of models with large width. This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate in ADAM. Using scaling arguments for large width networks, we demonstrate that the same learning rate does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen fixed ratio. We call this proposed algorithm LoRA+. In our extensive experiments, LoRA+ improves finetuning speed (up to ∼ 2X SpeedUp) and performance (1% − 2% improvements), at the same computational cost as LoRA. The code is available at https://github.com/nikhil-ghosh-berkeley/loraplus",
    "checked": true,
    "id": "241eefc1bb11e693e0fef6977a65a0a822fb8f5e",
    "semantic_title": "lora+: efficient low rank adaptation of large models",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=YvAyOYeGlo": {
    "title": "AND: Audio Network Dissection for Interpreting Deep Acoustic Models",
    "volume": "poster",
    "abstract": "Neuron-level interpretations aim to explain network behaviors and properties by investigating neurons responsive to specific perceptual or structural input patterns. Although there is emerging work in the vision and language domains, none is explored for acoustic models. To bridge the gap, we introduce *AND*, the first **A**udio **N**etwork **D**issection framework that automatically establishes natural language explanations of acoustic neurons based on highly responsive audio. *AND* features the use of LLMs to summarize mutual acoustic features and identities among audio. Extensive experiments are conducted to verify *AND*'s precise and informative descriptions. In addition, we highlight two acoustic model behaviors with analysis by *AND*. First, models discriminate audio with a combination of basic acoustic features rather than high-level abstract concepts. Second, training strategies affect neuron behaviors. Supervised training guides neurons to gradually narrow their attention, while self-supervised learning encourages neurons to be polysemantic for exploring high-level features. Finally, we demonstrate a potential use of *AND* in audio model unlearning by conducting concept-specific pruning based on the descriptions",
    "checked": true,
    "id": "b690fd4bfd2c1891868a8279e9479d109485a74e",
    "semantic_title": "and: audio network dissection for interpreting deep acoustic models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MlzUD5CKvZ": {
    "title": "Improving Prototypical Visual Explanations with Reward Reweighing, Reselection, and Retraining",
    "volume": "poster",
    "abstract": "In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the Prototypical Part Network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this architecture is able to produce visually interpretable classifications, it often learns to classify based on parts of the image that are not semantically meaningful. To address this problem, we propose the Reward Reweighing, Reselecting, and Retraining (R3) post-processing framework, which performs three additional corrective updates to a pretrained ProtoPNet in an offline and efficient manner. The first two steps involve learning a reward model based on collected human feedback and then aligning the prototypes with human preferences. The final step is retraining, which realigns the base features and the classifier layer of the original model with the updated prototypes. We find that our R3 framework consistently improves both the interpretability and the predictive accuracy of ProtoPNet and its variants",
    "checked": true,
    "id": "6a0866621b89f715e50a8babff7fd46550f3c3e5",
    "semantic_title": "improving prototypical visual explanations with reward reweighing, reselection, and retraining",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E41gvBG4s6": {
    "title": "Recovering Labels from Local Updates in Federated Learning",
    "volume": "poster",
    "abstract": "Gradient inversion (GI) attacks present a threat to the privacy of clients in federated learning (FL) by aiming to enable reconstruction of the clients' data from communicated model updates. A number of such techniques attempts to accelerate data recovery by first reconstructing labels of the samples used in local training. However, existing label extraction methods make strong assumptions that typically do not hold in realistic FL settings. In this paper we present a novel label recovery scheme, Recovering Labels from Local Updates (RLU), which provides near-perfect accuracy when attacking untrained (most vulnerable) models. More significantly, RLU achieves high performance even in realistic real-world settings where the clients in an FL system run multiple local epochs, train on heterogeneous data, and deploy various optimizers to minimize different objective functions. Specifically, RLU estimates labels by solving a least-square problem that emerges from the analysis of the correlation between labels of the data points used in a training round and the resulting update of the output layer. The experimental results on several datasets, architectures, and data heterogeneity scenarios demonstrate that the proposed method consistently outperforms existing baselines, and helps improve quality of the reconstructed images in GI attacks in terms of both PSNR and LPIPS",
    "checked": true,
    "id": "8379adbbffb632c3794172c0259581ceaa02557a",
    "semantic_title": "recovering labels from local updates in federated learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GDp7Gyd9nf": {
    "title": "Mechanistic Design and Scaling of Hybrid Architectures",
    "volume": "poster",
    "abstract": "The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology",
    "checked": true,
    "id": "05c1dc502ed51162580ccd320d5668d2fec94a7a",
    "semantic_title": "mechanistic design and scaling of hybrid architectures",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=MjGCD8wk1k": {
    "title": "LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits",
    "volume": "poster",
    "abstract": "In the realm of electronic and electrical engineering, automation of analog circuit is increasingly vital given the complexity and customized requirements of modern applications. However, existing methods only develop search-based algorithms that require many simulation iterations to design a custom circuit topology, which is usually a time-consuming process. To this end, we introduce LaMAGIC, a pioneering language model-based topology generation model that leverages supervised finetuning for automated analog circuit design. LaMAGIC can efficiently generate an optimized circuit design from the custom specification in a single pass. Our approach involves a meticulous development and analysis of various input and output formulations for circuit. These formulations can ensure canonical representations of circuits and align with the autoregressive nature of LMs to effectively addressing the challenges of representing analog circuits as graphs. The experimental results show that LaMAGIC achieves a success rate of up to 96% under a strict tolerance of 0.01. We also examine the scalability and adaptability of LaMAGIC, specifically testing its performance on more complex circuits. Our findings reveal the enhanced effectiveness of our adjacency matrix-based circuit formulation with floating-point input, suggesting its suitability for handling intricate circuit designs. This research not only demonstrates the potential of language models in graph generation, but also builds a foundational framework for future explorations in automated analog circuit design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mz1lcJPymz": {
    "title": "Online Learning in Betting Markets: Profit versus Prediction",
    "volume": "poster",
    "abstract": "We examine two types of binary betting markets, whose primary goal is for profit (such as sports gambling) or to gain information (such as prediction markets). We articulate the interplay between belief and price-setting to analyse both types of markets, and show that the goals of maximising bookmaker profit and eliciting information are fundamentally incompatible. A key insight is that profit hinges on the deviation between (the distribution of) bettor and true beliefs, and that heavier tails in bettor belief distribution implies higher profit. Our algorithmic contribution is to introduce online learning methods for price-setting. Traditionally bookmakers update their prices rather infrequently, we present two algorithms that guide price updates upon seeing each bet, assuming very little of bettor belief distributions. The online pricing algorithm achieves stochastic regret of $\\mathcal{O}(\\sqrt{T})$ against the worst local maximum, or $\\mathcal{O}(\\sqrt{T \\log T})$ with high probability against the global maximum under fair odds. More broadly, the inherent tradeoff between profit and information-seeking in binary betting may inspire new understandings of large-scale multi-agent behaviour",
    "checked": true,
    "id": "1ba971a3cf90b6f7f87e04d6e5af9a0ab365d3cb",
    "semantic_title": "online learning in betting markets: profit versus prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EEinDTdKr1": {
    "title": "Fine-grained Local Sensitivity Analysis of Standard Dot-Product Self-Attention",
    "volume": "poster",
    "abstract": "Self-attention has been widely used in various machine learning models, such as vision transformers. The standard dot-product self-attention is arguably the most popular structure, and there is a growing interest in understanding the mathematical properties of such attention mechanisms. This paper presents a fine-grained local sensitivity analysis of the standard dot-product self-attention, leading to new non-vacuous certified robustness results for vision transformers. Despite the well-known fact that dot-product self-attention is not (globally) Lipschitz, we develop new theoretical analysis of Local Fine-grained Attention Sensitivity (LoFAST) quantifying the effect of input feature perturbations on the attention output. Our analysis reveals that the local sensitivity of dot-product self-attention to $\\ell_2$ perturbations can actually be controlled by several key quantities associated with the attention weight matrices and the unperturbed input. We empirically validate our theoretical findings by computing non-vacuous certified $\\ell_2$-robustness for vision transformers on CIFAR-10 and SVHN datasets. The code for LoFAST is available at https://github.com/AaronHavens/LoFAST",
    "checked": false,
    "id": "d1a15445ce84fe12d6d4c279288cd56d8b191cd8",
    "semantic_title": "ndsu explore showcase of undergraduate research and creative activity research of gamma the society",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CpI37NA7MO": {
    "title": "Beyond Point Prediction: Score Matching-based Pseudolikelihood Estimation of Neural Marked Spatio-Temporal Point Process",
    "volume": "poster",
    "abstract": "Spatio-temporal point processes (STPPs) are potent mathematical tools for modeling and predicting events with both temporal and spatial features. Despite their versatility, most existing methods for learning STPPs either assume a restricted form of the spatio-temporal distribution, or suffer from inaccurate approximations of the intractable integral in the likelihood training objective. These issues typically arise from the normalization term of the probability density function. Moreover, existing works only provide point prediction for events without quantifying their uncertainty, such as confidence intervals for the event's arrival time and confidence regions for the event's location, which is crucial given the considerable randomness of the data. To tackle these challenges, we introduce SMASH: a Score MAtching-based pSeudolikeliHood estimator for learning marked STPPs. Specifically, our framework adopts a normalization-free objective by estimating the pseudolikelihood of marked STPPs through score-matching and predicts confidence intervals/regions for event time and location by generating samples through a score-based sampling algorithm. The superior performance of our proposed framework is demonstrated through extensive experiments on both point and confidence interval/region prediction of events",
    "checked": false,
    "id": "5970c3214db59ee25af3a83e5b873f138ab62630",
    "semantic_title": "score matching-based pseudolikelihood estimation of neural marked spatio-temporal point process with uncertainty quantification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jklD0TV5Hw": {
    "title": "Seesaw: Compensating for Nonlinear Reduction with Linear Computations for Private Inference",
    "volume": "poster",
    "abstract": "With increasingly serious data privacy concerns and strict regulations, privacy-preserving machine learning (PPML) has emerged to securely execute machine learning tasks without violating privacy. Unfortunately, the computational cost to securely execute nonlinear computations in PPML remains significant, calling for new model architecture designs with fewer nonlinear operations. We propose Seesaw, a novel neural architecture search method tailored for PPML. Seesaw exploits a previously unexplored opportunity to leverage more linear computations and nonlinear result reuse, in order to compensate for the accuracy loss due to nonlinear reduction. It incorporates specifically designed pruning and search strategies, not only to efficiently handle the much larger design space of both linear and nonlinear operators, but also to achieve a better balance between the model accuracy and the online/offline execution latencies. Compared to the state-of-the-art design for image classification on ImageNet, Seesaw achieves 1.68$\\times$ lower online latency and 1.55$\\times$ lower total online + offline latency at 71% iso-accuracy, or 3.65% higher accuracy at iso-latency of 190 seconds, while using much simpler and faster search and training methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vuMD71R20q": {
    "title": "Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective",
    "volume": "poster",
    "abstract": "Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of non-diagonal adaptive methods through the concept of preconditioner invariance. In contrast to root-based methods like Shampoo, the root-free counterparts do not require numerically unstable matrix root decompositions and inversions, thus work well in half precision. Our findings provide new insights into the development of adaptive methods and raise important questions regarding the currently overlooked role of adaptivity for their success",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VaZVZQSgTP": {
    "title": "A Single-Loop Robust Policy Gradient Method for Robust Markov Decision Processes",
    "volume": "poster",
    "abstract": "Robust Markov Decision Processes (RMDPs) have recently been recognized as a valuable and promising approach to discovering a policy with creditable performance, particularly in the presence of a dynamic environment and estimation errors in the transition matrix due to limited data. Despite extensive exploration of dynamic programming algorithms for solving RMDPs, there has been a notable upswing in interest in developing efficient algorithms using the policy gradient method. In this paper, we propose the first single-loop robust policy gradient (SRPG) method with the global optimality guarantee for solving RMDPs through its minimax formulation. Moreover, we complement the convergence analysis of the nonconvex-nonconcave min-max optimization problem with the objective function's gradient dominance property, which is not explored in the prior literature. Numerical experiments validate the efficacy of SRPG, demonstrating its faster and more robust convergence behavior compared to its nested-loop counterpart",
    "checked": true,
    "id": "360c565a5040558f62fda59897aef791050bed12",
    "semantic_title": "a single-loop robust policy gradient method for robust markov decision processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=knZ4NYzGUd": {
    "title": "Bayesian Knowledge Distillation: A Bayesian Perspective of Distillation with Uncertainty Quantification",
    "volume": "poster",
    "abstract": "Knowledge distillation (KD) has been widely used for model compression and deployment acceleration. Nonetheless, the statistical insight of the remarkable performance of KD remains elusive, and methods for evaluating the uncertainty of the distilled model/student model are lacking. To address these issues, we establish a close connection between KD and a Bayesian model. In particular, we develop an innovative method named Bayesian Knowledge Distillation (BKD) to provide a transparent interpretation of the working mechanism of KD, and a suite of Bayesian inference tools for the uncertainty quantification of the student model. In BKD, the regularization imposed by the teacher model in KD is formulated as a teacher-informed prior for the student model's parameters. Consequently, we establish the equivalence between minimizing the KD loss and estimating the posterior mode in BKD. Efficient Bayesian inference algorithms are developed based on the stochastic gradient Langevin Monte Carlo and examined with extensive experiments on uncertainty ranking and credible intervals construction for predicted class probabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RLA4JTckXe": {
    "title": "Mitigating Oversmoothing Through Reverse Process of GNNs for Heterophilic Graphs",
    "volume": "poster",
    "abstract": "Graph Neural Network (GNN) resembles the diffusion process, leading to the over-smoothing of learned representations when stacking many layers. Hence, the reverse process of message passing can produce the distinguishable node representations by inverting the forward message propagation. The distinguishable representations can help us to better classify neighboring nodes with different labels, such as in heterophilic graphs. In this work, we apply the design principle of the reverse process to the three variants of the GNNs. Through the experiments on heterophilic graph data, where adjacent nodes need to have different representations for successful classification, we show that the reverse process significantly improves the prediction performance in many cases. Additional analysis reveals that the reverse mechanism can mitigate the over-smoothing over hundreds of layers. Our code is available at https://github.com/ml-postech/reverse-gnn",
    "checked": true,
    "id": "41b3f7861191b93a7f3648c20b46f4703d3fae9a",
    "semantic_title": "mitigating oversmoothing through reverse process of gnns for heterophilic graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vFk9fqXLst": {
    "title": "Interpreting Equivariant Representations",
    "volume": "poster",
    "abstract": "Latent representations are extensively used for tasks like visualization, interpolation, or feature extraction in deep learning models. This paper demonstrates the importance of considering the inductive bias imposed by an equivariant model when using latent representations as neglecting these biases can lead to decreased performance in downstream tasks. We propose principles for choosing invariant projections of latent representations and show their effectiveness in two examples: A permutation equivariant variational auto-encoder for molecular graph generation, where an invariant projection can be designed to maintain information without loss, and for a rotation-equivariant representation in image classification, where random invariant projections proves to retain a high degree of information. In both cases, the analysis of invariant latent representations proves superior to their equivariant counterparts. Finally, we illustrate that the phenomena documented here for equivariant neural networks have counterparts in standard neural networks where invariance is encouraged via augmentation",
    "checked": true,
    "id": "fa423012fc36e88854e80054484842a788f3d2b5",
    "semantic_title": "interpreting equivariant representations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQzmDFlsHX": {
    "title": "Unsupervised Concept Discovery Mitigates Spurious Correlations",
    "volume": "poster",
    "abstract": "Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring subgroups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the benchmark datasets for sub-population shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation. Code is available at https://github.com/rarefin/CoBalT",
    "checked": true,
    "id": "2a870dec6c17331b2c6eadd6a4687b035ecdf796",
    "semantic_title": "unsupervised concept discovery mitigates spurious correlations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xcDRx8vzCa": {
    "title": "CHAI: Clustered Head Attention for Efficient LLM Inference",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-head attention is one of the key components of LLMs, which can for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered HeadAttention ( CHAI ). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73× without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets",
    "checked": true,
    "id": "7b46d6e2e00a4b4693dbadbbee3a2586bbae3c2f",
    "semantic_title": "chai: clustered head attention for efficient llm inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kmugaw9Kfq": {
    "title": "On the Expressive Power of Spectral Invariant Graph Neural Networks",
    "volume": "poster",
    "abstract": "Incorporating spectral information to enhance Graph Neural Networks (GNNs) has shown promising results but raises a fundamental challenge due to the inherent ambiguity of eigenvectors. Various architectures have been proposed to address this ambiguity, referred to as spectral invariant architectures. Notable examples include GNNs and Graph Transformers that use spectral distances, spectral projection matrices, or other invariant spectral features. However, the potential expressive power of these spectral invariant architectures remains largely unclear. The goal of this work is to gain a deep theoretical understanding of the expressive power obtainable when using spectral features. We first introduce a novel message-passing framework for designing spectral invariant GNNs, called Eigenspace Projection GNN (EPNN). Our comprehensive analysis shows that EPNN essentially unifies all prior spectral invariant architectures, in that they are either strictly less expressive or equivalent to EPNN. A fine-grained expressiveness hierarchy among different architectures is also established. On the other hand, we present a surprising result that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL. Finally, we demonstrate that these spectral features offer no additional advantage when combined with more expressive GNNs",
    "checked": true,
    "id": "f94662cb70835acf14384fd6c8d7eafca0ba34d7",
    "semantic_title": "on the expressive power of spectral invariant graph neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kXde6Qa6Uy": {
    "title": "Parameter Estimation in DAGs from Incomplete Data via Optimal Transport",
    "volume": "poster",
    "abstract": "Estimating the parameters of a probabilistic directed graphical model from incomplete data is a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a general framework that operates on any directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the versatility and robustness of our approach. Across experiments, we show that not only can our method effectively recover the ground-truth parameters but it also performs comparably or better than competing baselines on downstream applications",
    "checked": true,
    "id": "0ed3b14e1d399709f68302ed483844489f8151b5",
    "semantic_title": "parameter estimation in dags from incomplete data via optimal transport",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=15MpDbv3IQ": {
    "title": "Tuning-free Estimation and Inference of Cumulative Distribution Function under Local Differential Privacy",
    "volume": "poster",
    "abstract": "We introduce a novel algorithm for estimating Cumulative Distribution Function (CDF) values under Local Differential Privacy (LDP) by exploiting an unexpected connection between LDP and the current status problem, a classical survival data problem in statistics. This connection leads to the development of tools for constrained isotonic estimation based on binary queries. Through mathematical proofs and extensive numerical testing, we demonstrate that our method achieves uniform and $L_2$ error bounds when estimating the entire CDF curve. By employing increasingly dense grids, the error bound can be improved, exhibiting an asymptotic normal distribution of the proposed estimator. Theoretically, we show that the error bound smoothly changes as the number of grids increases relative to the sample size $n$. Computationally, we demonstrate that our constrained isotonic estimator can be efficiently computed deterministically, eliminating the need for hyperparameters or random optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DbMm8pmoAP": {
    "title": "Evolving Subnetwork Training for Large Language Models",
    "volume": "poster",
    "abstract": "Large language models have ushered in a new era of artificial intelligence research. However, their substantial training costs hinder further development and widespread adoption. In this paper, inspired by the redundancy in the parameters of large language models, we propose a novel training paradigm: Evolving Subnetwork Training (EST). EST samples subnetworks from the layers of the large language model and from commonly used modules within each layer, Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP). By gradually increasing the size of the subnetworks during the training process, EST can save the cost of training. We apply EST to train GPT2 model and TinyLlama model, resulting in 26.7% FLOPs saving for GPT2 and 25.0% for TinyLlama without an increase in loss on the pre-training dataset. Moreover, EST leads to performance improvements in downstream tasks, indicating that it benefits generalization. Additionally, we provide intuitive theoretical studies based on training dynamics and Dropout theory to ensure the feasibility of EST",
    "checked": true,
    "id": "0ec87f61e3e8d31c92d5960c9950813653f7e83d",
    "semantic_title": "evolving subnetwork training for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=foPMkomvk1": {
    "title": "Pursuing Overall Welfare in Federated Learning through Sequential Decision Making",
    "volume": "poster",
    "abstract": "In traditional federated learning, a single global model cannot perform equally well for all clients. Therefore, the need to achieve the *client-level fairness* in federated system has been emphasized, which can be realized by modifying the static aggregation scheme for updating the global model to an adaptive one, in response to the local signals of the participating clients. Our work reveals that existing fairness-aware aggregation strategies can be unified into an online convex optimization framework, in other words, a central server's *sequential decision making* process. To enhance the decision making capability, we propose simple and intuitive improvements for suboptimal designs within existing methods, presenting $\\texttt{AAggFF}$. Considering practical requirements, we further subdivide our method tailored for the *cross-device* and the *cross-silo* settings, respectively. Theoretical analyses guarantee sublinear regret upper bounds for both settings: $\\mathcal{O}(\\sqrt{T \\log{K}})$ for the cross-device setting, and $\\mathcal{O}(K \\log{T})$ for the cross-silo setting, with $K$ clients and $T$ federation rounds. Extensive experiments demonstrate that the federated system equipped with $\\texttt{AAggFF}$ achieves better degree of client-level fairness than existing methods in both practical settings",
    "checked": true,
    "id": "82adc7efc6212a8789409e23374b3881de87988c",
    "semantic_title": "pursuing overall welfare in federated learning through sequential decision making",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKYfaYQlML": {
    "title": "Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models",
    "volume": "poster",
    "abstract": "Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, \"How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?\", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x, 4x and 15x reduction in pretraining compute cost when compared to task-agnostic VFM distillation, ImageNet pretraining and DINO pretraining, respectively, while outperforming them. We also show that the dataset used for transferring knowledge has a significant effect on the final target task performance, and introduce a retrieval-augmented knowledge transfer strategy that uses web-scale image retrieval to curate effective transfer sets",
    "checked": true,
    "id": "6dd6101b2d7709f2a05f17510d98a01424672e1e",
    "semantic_title": "knowledge transfer from vision foundation models for efficient training of small task-specific models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OXzkw7vFIO": {
    "title": "Counterfactual Image Editing",
    "volume": "poster",
    "abstract": "Counterfactual image editing is a challenging task within generative AI. The current literature on the topic focuses primarily on changing individual features while being silent about the causal relationships between features, which are present in the real world. In this paper, we first formalize this task through causal language, modeling the causal relationships between latent generative factors and images through a special type of causal model called *augmented structural causal models (ASCMs)*. Second, we show two fundamental impossibility results: (1) counterfactual editing is impossible from i.i.d. image samples and their corresponding labels alone; (2) also, even when the causal relationships between latent generative factors and images are available, no guarantees regarding the output of the generative model can be provided. Third, we propose a relaxation over this hard problem aiming to approximate the non-identifiable target counterfactual distributions while still preserving features the users care about and that are causally consistent with the true generative model, which we call **ctf-consistent estimators**. Finally, we develop an efficient algorithm to generate counterfactual image samples leveraging neural causal models",
    "checked": true,
    "id": "cb6be40199a18c6d57bf6cad42df33c5711df00e",
    "semantic_title": "counterfactual image editing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yDXnXJE1RK": {
    "title": "Variational Partial Group Convolutions for Input-Aware Partial Equivariance of Rotations and Color-Shifts",
    "volume": "poster",
    "abstract": "Group Equivariant CNNs (G-CNNs) have shown promising efficacy in various tasks, owing to their ability to capture hierarchical features in an equivariant manner. However, their equivariance is fixed to the symmetry of the whole group, limiting adaptability to diverse partial symmetries in real-world datasets, such as limited rotation symmetry of handwritten digit images and limited color-shift symmetry of flower images. Recent efforts address this limitation, one example being Partial G-CNN which restricts the output group space of convolution layers to break full equivariance. However, such an approach still fails to adjust equivariance levels across data. In this paper, we propose a novel approach, Variational Partial G-CNN (VP G-CNN), to capture varying levels of partial equivariance specific to each data instance. VP G-CNN redesigns the distribution of the output group elements to be conditioned on input data, leveraging variational inference to avoid overfitting. This enables the model to adjust its equivariance levels according to the needs of individual data points. Additionally, we address training instability inherent in discrete group equivariance models by redesigning the reparametrizable distribution. We demonstrate the effectiveness of VP G-CNN on both toy and real-world datasets, including MNIST67-180, CIFAR10, ColorMNIST, and Flowers102. Our results show robust performance, even in uncertainty metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bq1JEgioLr": {
    "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
    "volume": "poster",
    "abstract": "Most existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery",
    "checked": true,
    "id": "4993258852711c4e3d0011325ac3db680eae84f4",
    "semantic_title": "scibench: evaluating college-level scientific problem-solving abilities of large language models",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=iPFuWc1TV2": {
    "title": "Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers",
    "volume": "poster",
    "abstract": "Graph transformers typically lack third-order interactions, limiting their geometric understanding which is crucial for tasks like molecular geometry prediction. We propose the Triplet Graph Transformer (TGT) that enables direct communication between pairs within a 3-tuple of nodes via novel triplet attention and aggregation mechanisms. TGT is applied to molecular property prediction by first predicting interatomic distances from 2D graphs and then using these distances for downstream tasks. A novel three-stage training procedure and stochastic inference further improve training efficiency and model performance. Our model achieves new state-of-the-art (SOTA) results on open challenge benchmarks PCQM4Mv2 and OC20 IS2RE. We also obtain SOTA results on QM9, MOLPCBA, and LIT-PCBA molecular property prediction benchmarks via transfer learning. We also demonstrate the generality of TGT with SOTA results on the traveling salesman problem (TSP)",
    "checked": true,
    "id": "d85316b6eb3bff2f47f3d0382de7aee3929a55c9",
    "semantic_title": "triplet interaction improves graph transformers: accurate molecular graph learning with triplet graph transformers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2hidpjUPvV": {
    "title": "Stochastic Bandits with ReLU Neural Networks",
    "volume": "poster",
    "abstract": "We study the stochastic bandit problem with ReLU neural network structure. We show that a $\\tilde{O}(\\sqrt{T})$ regret guarantee is achievable by considering bandits with one-layer ReLU neural networks; to the best of our knowledge, our work is the first to achieve such a guarantee. In this specific setting, we propose an OFU-ReLU algorithm that can achieve this upper bound. The algorithm first explores randomly until it reaches a *linear* regime, and then implements a UCB-type linear bandit algorithm to balance exploration and exploitation. Our key insight is that we can exploit the piecewise linear structure of ReLU activations and convert the problem into a linear bandit in a transformed feature space, once we learn the parameters of ReLU relatively accurately during the exploration stage. To remove dependence on model parameters, we design an OFU-ReLU+ algorithm based on a batching strategy, which can provide the same theoretical guarantee",
    "checked": true,
    "id": "afa279cfb6c2335b1c5bf67acdc2ea90c1b4d2a0",
    "semantic_title": "stochastic bandits with relu neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IzqpUC34Jg": {
    "title": "Provable Privacy with Non-Private Pre-Processing",
    "volume": "poster",
    "abstract": "When analyzing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication, standard scaling and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines",
    "checked": true,
    "id": "09c1219ce152774c2f372dd5d8e8d17870537825",
    "semantic_title": "provable privacy with non-private pre-processing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KfXXPCcobh": {
    "title": "Exploring the Benefit of Activation Sparsity in Pre-training",
    "volume": "poster",
    "abstract": "Pre-trained Transformers inherently possess the characteristic of sparse activation, where only a small fraction of the neurons are activated for each token. While sparse activation has been explored through post-training methods, its potential in pre-training remains untapped. In this work, we first study how activation properties change during pre-training. Our examination reveals that Transformers exhibit sparse activation throughout the majority of the pre-training process while the activation correlation keeps evolving as training progresses. Leveraging this observation, we propose Switchable Sparse-Dense Learning (SSD). SSD adaptively switches between the Mixtures-of-Experts (MoE) based sparse training and the conventional dense training during the pre-training process, leveraging the efficiency of sparse training and avoiding the static activation correlation of sparse training. Compared to dense training, SSD achieves comparable performance with identical model size and reduces pre-training costs. Moreover, the models trained with SSD can be directly used as MoE models for sparse inference and achieve the same performance as dense models with up to $2\\times$ faster inference speed. Codes are available at https://github.com/thunlp/moefication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=61A1bsVjRg": {
    "title": "Tilt and Average : Geometric Adjustment of the Last Layer for Recalibration",
    "volume": "poster",
    "abstract": "After the revelation that neural networks tend to produce overconfident predictions, the problem of calibration, which aims to align confidence with accuracy to enhance the reliability of predictions, has gained significant importance. Several solutions based on calibration maps have been proposed to address the problem of recalibrating a trained classifier using additional datasets. In this paper, we offer an algorithm that transforms the weights of the last layer of the classifier, distinct from the calibration-map-based approach. We concentrate on the geometry of the final linear layer, specifically its angular aspect, and adjust the weights of the corresponding layer. We name the method Tilt and Average, and validate the calibration effect empirically and theoretically. Through this, we demonstrate that our approach, in addition to the existing calibration-map-based techniques, can yield improved calibration performance",
    "checked": true,
    "id": "5a6eb0b7d4597f453db55638edf986533cd3e0e5",
    "semantic_title": "tilt and average : geometric adjustment of the last layer for recalibration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8KeD4mEh3j": {
    "title": "Data-Efficient Molecular Generation with Hierarchical Textual Inversion",
    "volume": "poster",
    "abstract": "Developing an effective molecular generation framework even with a limited number of molecules is often important for its practical deployment, e.g., drug discovery, since acquiring task-related molecular data requires expensive and time-consuming experimental costs. To tackle this issue, we introduce Hierarchical Textual Inversion for Molecular Generation (HI-Mol), a novel data-efficient molecular generation method. HI-Mol is inspired by the importance of hierarchical information, e.g., both coarse- and fine-grained features, in understanding the molecule distribution. We propose to use multi-level embeddings to reflect such hierarchical features based on the adoption of the recent textual inversion technique in the visual domain, which achieves data-efficient image generation. Compared to the conventional textual inversion method in the image domain using a single-level token embedding, our multi-level token embeddings allow the model to effectively learn the underlying low-shot molecule distribution. We then generate molecules based on the interpolation of the multi-level token embeddings. Extensive experiments demonstrate the superiority of HI-Mol with notable data-efficiency. For instance, on QM9, HI-Mol outperforms the prior state-of-the-art method with 50x less training data. We also show the effectiveness of molecules generated by HI-Mol in low-shot molecular property prediction. Code is available at https://github.com/Seojin-Kim/HI-Mol",
    "checked": true,
    "id": "4ffd4b3b22c471c628bba8e3b69e11a392976190",
    "semantic_title": "data-efficient molecular generation with hierarchical textual inversion",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=WPt9HRmMrG": {
    "title": "Active Label Correction for Semantic Segmentation with Foundation Models",
    "volume": "poster",
    "abstract": "Training and validating models for semantic segmentation require datasets with pixel-wise annotations, which are notoriously labor-intensive. Although useful priors such as foundation models or crowdsourced datasets are available, they are error-prone. We hence propose an effective framework of active label correction (ALC) based on a design of correction query to rectify pseudo labels of pixels, which in turn is more annotator-friendly than the standard one inquiring to classify a pixel directly according to our theoretical analysis and user study. Specifically, leveraging foundation models providing useful zero-shot predictions on pseudo labels and superpixels, our method comprises two key techniques: (i) an annotator-friendly design of correction query with the pseudo labels, and (ii) an acquisition function looking ahead label expansions based on the superpixels. Experimental results on PASCAL, Cityscapes, and Kvasir-SEG datasets demonstrate the effectiveness of our ALC framework, outperforming prior methods for active semantic segmentation and label correction. Notably, utilizing our method, we obtained a revised dataset of PASCAL by rectifying errors in 2.6 million pixels in PASCAL dataset",
    "checked": true,
    "id": "c07de46ee1f89a2b1d842df95c38d1d6147523f2",
    "semantic_title": "active label correction for semantic segmentation with foundation models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n2kq2EOHFE": {
    "title": "Learning the Uncertainty Sets of Linear Control Systems via Set Membership: A Non-asymptotic Analysis",
    "volume": "poster",
    "abstract": "This paper studies uncertainty set estimation for unknown linear systems. Uncertainty sets are crucial for the quality of robust control since they directly influence the conservativeness of the control design. Departing from the confidence region analysis of least squares estimation, this paper focuses on set membership estimation (SME). Though good numerical performances have attracted applications of SME in the control literature, the non-asymptotic convergence rate of SME for linear systems remains an open question. This paper provides the first convergence rate bounds for SME and discusses variations of SME under relaxed assumptions. We also provide numerical results demonstrating SME's practical promise",
    "checked": false,
    "id": "b35b6c34785dfb2ae95ff3f6f9037e982e363099",
    "semantic_title": "learning the uncertainty sets for control dynamics via set membership: a non-asymptotic analysis",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=4iBJyJeBX5": {
    "title": "Generalized Smooth Variational Inequalities: Methods with Adaptive Stepsizes",
    "volume": "poster",
    "abstract": "Variational Inequality (VI) problems have attracted great interest in the machine learning (ML) community due to their application in adversarial and multi-agent training. Despite its relevance in ML, the oft-used strong-monotonicity and Lipschitz continuity assumptions on VI problems are restrictive and do not hold in many machine learning problems. To address this, we relax smoothness and monotonicity assumptions and study structured non-monotone generalized smoothness. The key idea of our results is in adaptive stepsizes. We prove the first-known convergence results for solving generalized smooth VIs for the three popular methods, namely, projection, Korpelevich, and Popov methods. Our convergence rate results for generalized smooth VIs match or improve existing results on smooth VIs. We present numerical experiments that support our theoretical guarantees and highlight the efficiency of proposed adaptive stepsizes",
    "checked": false,
    "id": "ae794f921fa64737f7c6a57b5d8c432e3e3f7091",
    "semantic_title": "adaptive methods or variational inequalities with relatively smooth and reletively strongly monotone operators",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VsvfSMI5bs": {
    "title": "BAGEL: Bootstrapping Agents by Guiding Exploration with Language",
    "volume": "poster",
    "abstract": "Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories to synthetic demonstrations via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory. By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We adapt the base LM agent at test time with in-context learning by retrieving relevant BAGEL demonstrations based on the instruction, and find improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x reduction in execution failures",
    "checked": true,
    "id": "fd50db0489828d0da2557a6da8d931d523479957",
    "semantic_title": "bagel: bootstrapping agents by guiding exploration with language",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=vITl6CqIkk": {
    "title": "Semantic-Aware Human Object Interaction Image Generation",
    "volume": "poster",
    "abstract": "Recent text-to-image generative models have demonstrated remarkable abilities in generating realistic images. Despite their great success, these models struggle to generate high-fidelity images with prompts oriented toward human-object interaction (HOI). The difficulty in HOI generation arises from two aspects. Firstly, the complexity and diversity of human poses challenge plausible human generation. Furthermore, untrustworthy generation of interaction boundary regions may lead to deficiency in HOI semantics. To tackle the problems, we propose a Semantic-Aware HOI generation framework SA-HOI . It utilizes human pose quality and interaction boundary region information as guidance for denoising process, thereby encouraging refinement in these regions to produce more reasonable HOI images. Based on it, we establish an iterative inversion and image refinement pipeline to continually enhance generation quality. Further, we introduce a comprehensive benchmark for HOI generation, which comprises a dataset involving diverse and fine-grained HOI categories, along with multiple custom-tailored evaluation metrics for HOI generation. Experiments demonstrate that our method significantly improves generation quality under both HOI-specific and conventional image evaluation metrics. The code is available at https://github.com/XZPKU/SA-HOI.git",
    "checked": false,
    "id": "71b98977567c02e7d28e5ef13f361692327bdfcb",
    "semantic_title": "hand-object interaction image generation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=y8YovS0lOg": {
    "title": "On the Implicit Bias of Adam",
    "volume": "poster",
    "abstract": "In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different \"norm\" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, conversely, impede its reduction (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization",
    "checked": true,
    "id": "7b444736da839085f423609e22284b9882f85c5b",
    "semantic_title": "on the implicit bias of adam",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=bcN7KSB2YS": {
    "title": "State-Constrained Zero-Sum Differential Games with One-Sided Information",
    "volume": "poster",
    "abstract": "We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2). The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to violate the state constraints if possible, or to maximize the payoff otherwise. One example of the game is a man-to-man matchup in football. Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players. Our theoretical contribution is an extension of this result to games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing behavioral strategies. Different from existing works that are concerned about the scalability of no-regret learning in games with discrete dynamics, our study reveals the underlying structure of strategies for belief manipulation resulting from information asymmetry and state constraints. This structure will be necessary for scalable learning on games with continuous actions and long time windows. We use a simplified football game to demonstrate the utility of this work, where we reveal player positions and belief states in which the attacker should (or should not) play specific random deceptive moves to take advantage of information asymmetry, and compute how the defender should respond",
    "checked": true,
    "id": "2caf056de1ca914c5282a8b7a7c0f6c50f58d61f",
    "semantic_title": "state-constrained zero-sum differential games with one-sided information",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HOG80Yk4Gw": {
    "title": "Relational DNN Verification With Cross Executional Bound Refinement",
    "volume": "poster",
    "abstract": "We focus on verifying relational properties defined over deep neural networks (DNNs) such as robustness against universal adversarial perturbations (UAP), certified worst-case hamming distance for binary string classifications, etc. Precise verification of these properties requires reasoning about multiple executions of the same DNN. However, most of the existing works in DNN verification only handle properties defined over single executions and as a result, are imprecise for relational properties. Though few recent works for relational DNN verification, capture linear dependencies between the inputs of multiple executions, they do not leverage dependencies between the outputs of hidden layers producing imprecise results. We develop a scalable relational verifier RACoon that utilizes cross-execution dependencies at all layers of the DNN gaining substantial precision over SOTA baselines on a wide range of datasets, networks, and relational properties",
    "checked": true,
    "id": "7a463fb6447e3b231efd7bf8fb88bd8a705e1d25",
    "semantic_title": "relational dnn verification with cross executional bound refinement",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rtyqBfcg8j": {
    "title": "Sampling in Unit Time with Kernel Fisher-Rao Flow",
    "volume": "poster",
    "abstract": "We introduce a new mean-field ODE and corresponding interacting particle systems (IPS) for sampling from an unnormalized target density. The IPS are gradient-free, available in closed form, and only require the ability to sample from a reference density and compute the (unnormalized) target-to-reference density ratio. The mean-field ODE is obtained by solving a Poisson equation for a velocity field that transports samples along the geometric mixture of the two densities, $\\pi_0^{1-t} \\pi_1^t$, which is the path of a particular Fisher-Rao gradient flow. We employ a RKHS ansatz for the velocity field, which makes the Poisson equation tractable and enables discretization of the resulting mean-field ODE over finite samples. The mean-field ODE can be additionally be derived from a discrete-time perspective as the limit of successive linearizations of the Monge-Ampère equations within a framework known as sample-driven optimal transport. We introduce a stochastic variant of our approach and demonstrate empirically that our IPS can produce high-quality samples from varied target distributions, outperforming comparable gradient-free particle systems and competitive with gradient-based alternatives",
    "checked": true,
    "id": "a6a94bfda4cc6faf0a531bb9c4f649d41f1050ab",
    "semantic_title": "sampling in unit time with kernel fisher-rao flow",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=sZla6SnooP": {
    "title": "Physics-Informed Neural Network Policy Iteration: Algorithms, Convergence, and Verification",
    "volume": "poster",
    "abstract": "Solving nonlinear optimal control problems is a challenging task, particularly for high-dimensional problems. We propose algorithms for model-based policy iterations to solve nonlinear optimal control problems with convergence guarantees. The main component of our approach is an iterative procedure that utilizes neural approximations to solve linear partial differential equations (PDEs), ensuring convergence. We present two variants of the algorithms. The first variant formulates the optimization problem as a linear least square problem, drawing inspiration from extreme learning machine (ELM) for solving PDEs. This variant efficiently handles low-dimensional problems with high accuracy. The second variant is based on a physics-informed neural network (PINN) for solving PDEs and has the potential to address high-dimensional problems. We demonstrate that both algorithms outperform traditional approaches, such as Galerkin methods, by a significant margin. We provide a theoretical analysis of both algorithms in terms of convergence of neural approximations towards the true optimal solutions in a general setting. Furthermore, we employ formal verification techniques to demonstrate the verifiable stability of the resulting controllers",
    "checked": true,
    "id": "a344e8bcc44996d228fd591ad6f9267c0ca89558",
    "semantic_title": "physics-informed neural network policy iteration: algorithms, convergence, and verification",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=yUxdk32TU6": {
    "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability",
    "volume": "poster",
    "abstract": "Jailbreaks on large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent (suffix) attack with continuation constraint, but also allow us to address new controllable attack settings such as revising a user query adversarially with paraphrasing constraint, and inserting stealthy attacks in context with position constraint. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5, and GPT-4) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack",
    "checked": true,
    "id": "b7ef6182f617ef3e7cc9682f562f794115a4c62c",
    "semantic_title": "cold-attack: jailbreaking llms with stealthiness and controllability",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=UZlMXUGI6e": {
    "title": "Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach",
    "volume": "poster",
    "abstract": "Forecasting of Irregular Multivariate Time Series (IMTS) is critical for numerous areas, such as healthcare, biomechanics, climate science, and astronomy. Despite existing research addressing irregularities in time series through ordinary differential equations, the challenge of modeling correlations between asynchronous IMTS remains underexplored. To bridge this gap, this study proposes Transformable Patching Graph Neural Networks (t-PatchGNN), which transforms each univariate irregular time series into a series of transformable patches encompassing a varying number of observations with uniform temporal resolution. It seamlessly facilitates local semantics capture and inter-time series correlation modeling while avoiding sequence length explosion in aligned IMTS. Building on the aligned patching outcomes, we then present time-adaptive graph neural networks to model dynamic intertime series correlation based on a series of learned time-varying adaptive graphs. We demonstrate the remarkable superiority of t-PatchGNN on a comprehensive IMTS forecasting benchmark we build, which contains four real-world scientific datasets covering healthcare, biomechanics and climate science, and seventeen competitive baselines adapted from relevant research fields",
    "checked": false,
    "id": "b2d47289154822bf53ce7ff0dbc5c1fd0efb8a80",
    "semantic_title": "modeling oceanic variables with graph-guided networks for irregularly sampled multivariate time series",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dJTChKgv3a": {
    "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering",
    "volume": "poster",
    "abstract": "Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs",
    "checked": true,
    "id": "267ef391ebaa4fc3c8ec62cb92107e8efe82a5db",
    "semantic_title": "in-context vectors: making in context learning more effective and controllable through latent space steering",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=XMlUlY7ONf": {
    "title": "From Neurons to Neutrons: A Case Study in Interpretability",
    "volume": "poster",
    "abstract": "Mechanistic Interpretability (MI) proposes a path toward fully understanding how neural networks make their predictions. Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters. Does this mean neuron-level interpretability techniques have limited applicability? Here, we argue that high-dimensional neural networks can learn *useful* low-dimensional representations of the data they were trained on, going beyond simply making good predictions: Such representations can be understood with the MI lens and provide insights that are surprisingly faithful to human-derived domain knowledge. This indicates that such approaches to interpretability can be useful for deriving a new understanding of a problem from models trained to solve it. As a case study, we extract nuclear physics concepts by studying models trained to reproduce nuclear data",
    "checked": true,
    "id": "f1483d479bf5c62a4688ab2397e0794f18288232",
    "semantic_title": "from neurons to neutrons: a case study in interpretability",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YSoMmNWZZx": {
    "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
    "volume": "poster",
    "abstract": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains — including classic control, as well as manipulation of rigid, articulated, and deformable objects — without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/",
    "checked": true,
    "id": "550006bea81e4ccb67743dd1b82a70b86b48d93a",
    "semantic_title": "rl-vlm-f: reinforcement learning from vision language foundation model feedback",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=hcQfTsVnBo": {
    "title": "Grokking Group Multiplication with Cosets",
    "volume": "poster",
    "abstract": "The complex and unpredictable nature of deep neural networks prevents their safe use in many high-stakes applications. There have been many techniques developed to interpret deep neural networks, but all have substantial limitations. Algorithmic tasks have proven to be a fruitful test ground for interpreting a neural network end-to-end. Building on previous work, we completely reverse engineer fully connected one-hidden layer networks that have ``grokked'' the arithmetic of the permutation groups $S_5$ and $S_6$. The models discover the true subgroup structure of the full group and converge on neural circuits that decompose the group arithmetic using the permutation group's subgroups. We relate how we reverse engineered the model's mechanisms and confirmed our theory was a faithful description of the circuit's functionality. We also draw attention to current challenges in conducting interpretability research by comparing our work to Chughtai et al. (2023) which alleges to find a different algorithm for this same problem",
    "checked": true,
    "id": "5bd63e1c69fe729a2805ea5d69d27a96dd814d92",
    "semantic_title": "grokking group multiplication with cosets",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=4axAQHwBOE": {
    "title": "Certifiably Byzantine-Robust Federated Conformal Prediction",
    "volume": "poster",
    "abstract": "Conformal prediction has shown impressive capacity in constructing statistically rigorous prediction sets for machine learning models with exchangeable data samples. The siloed datasets, coupled with the escalating privacy concerns related to local data sharing, have inspired recent innovations extending conformal prediction into federated environments with distributed data samples. However, this framework for distributed uncertainty quantification is susceptible to Byzantine failures. A minor subset of malicious clients can significantly compromise the practicality of coverage guarantees. To address this vulnerability, we introduce a novel framework Rob-FCP, which executes robust federated conformal prediction, effectively countering malicious clients capable of reporting arbitrary statistics with the conformal calibration process. We theoretically provide the conformal coverage bound of Rob-FCP in the Byzantine setting and show that the coverage of Rob-FCP is asymptotically close to the desired coverage level. We also propose a malicious client number estimator to tackle a more challenging setting where the number of malicious clients is unknown to the defender and theoretically shows its effectiveness. We empirically demonstrate the robustness of Rob-FCP against diverse proportions of malicious clients under a variety of Byzantine attacks on five standard benchmark and real-world healthcare datasets",
    "checked": true,
    "id": "7e6ea5b28f82f1dedfaaa2eba48a81d9d68b300f",
    "semantic_title": "certifiably byzantine-robust federated conformal prediction",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kHjOmAUfVe": {
    "title": "RoboDreamer: Learning Compositional World Models for Robot Imagination",
    "volume": "poster",
    "abstract": "Text-to-video models have demonstrated substantial potential in robotic decision-making, enabling the imagination of realistic plans of future actions as well as accurate environment simulation. However, one major issue in such models is generalization -- models are limited to synthesizing videos subject to language instructions similar to those seen at training time. This is heavily limiting in decision-making, where we seek a powerful world model to synthesize plans of unseen combinations of objects and actions in order to solve previously unseen tasks in new environments. To resolve this issue, we introduce RoboDreamer, an innovative approach for learning a compositional world model by factorizing the video generation. We leverage the natural compositionality of language to parse instructions into a set of lower-level primitives, which we condition a set of models on to generate videos. We illustrate how this factorization naturally enables compositional generalization, by allowing us to formulate a new natural language instruction as a combination of previously seen components. We further show how such a factorization enables us to add additional multimodal goals, allowing us to specify a video we wish to generate given both natural language instructions and a goal image. Our approach can successfully synthesize video plans on unseen goals in the RT-X, enables successful robot execution in simulation, and substantially outperforms monolithic baseline approaches to video generation",
    "checked": true,
    "id": "458b47731153b8462f2d3265c3fdfa15d38a6f55",
    "semantic_title": "robodreamer: learning compositional world models for robot imagination",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6FXtu8clyp": {
    "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
    "volume": "poster",
    "abstract": "Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance – a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization, and challenge sets that probe properties such as hallucination; evaluations that provide fine-grained insight VLM capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and training from base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible training code, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open VLMs",
    "checked": true,
    "id": "f86f71cfd2e9682a56d7334736a7b8a0b1c70b45",
    "semantic_title": "prismatic vlms: investigating the design space of visually-conditioned language models",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=uhHDhVKFMW": {
    "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
    "volume": "poster",
    "abstract": "Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS",
    "checked": true,
    "id": "ef1b02dc1b82f9955fc4760fcefd92c0fff9f227",
    "semantic_title": "get more with less: synthesizing recurrence with kv cache compression for efficient llm inference",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=A0N39kgRZq": {
    "title": "Learning Multiple Secrets in Mastermind",
    "volume": "poster",
    "abstract": "In the Generalized Mastermind problem, there is an unknown subset $H$ of the hypercube 0,1$^d$ containing $n$ points. The goal is to learn $H$ by making a few queries to an oracle which given a point $q$ in 0,1$^d$, returns the point in $H$ nearest to $q$. We give a two-round adaptive algorithm for this problem that learns $H$ while making at most $\\exp(\\widetilde{O}(\\sqrt{d \\log n}))$. Furthermore, we show that any $r$-round adaptive randomized algorithm that learns $H$ with constant probability must make $\\exp(\\Omega(d^{3^{-(r-1)}}))$ queries even when the input has poly$(d)$ points; thus, any poly$(d)$ query algorithm must necessarily use $\\Omega(\\log \\log d)$ rounds of adaptivity. We give optimal query complexity bounds for the variant of the problem where queries are allowed to be from 0,1,2$^d$. We also study a continuous variant of the problem in which $H$ is a subset of unit vectors in $\\mathbb{R}^d$ and one can query unit vectors in $\\mathbb{R}^d$. For this setting, we give a $O(n^{\\lfloor d/2 \\rfloor})$ query deterministic algorithm to learn the hidden set of points",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SRzb3QDjdV": {
    "title": "PIDformer: Transformer Meets Control Theory",
    "volume": "poster",
    "abstract": "In this work, we address two main shortcomings of transformer architectures: input corruption and rank collapse in their output representation. We unveil self-attention as an autonomous state-space model that inherently promotes smoothness in its solutions, leading to lower-rank outputs and diminished representation capacity. Moreover, the steady-state solution of the model is sensitive to input perturbations. We incorporate a Proportional-Integral-Derivative (PID) closed-loop feedback control system with a reference point into the model to improve robustness and representation capacity. This integration aims to preserve high-frequency details while bolstering model stability, rendering it more noise-resilient. The resulting controlled state-space model is theoretically proven robust and adept at addressing the rank collapse. Motivated by this control framework, we derive a novel class of transformers, PID-controlled Transformer (PIDformer), aimed at improving robustness and mitigating the rank-collapse issue inherent in softmax transformers. We empirically evaluate the model for advantages and robustness against baseline transformers across various practical tasks, including object classification, image segmentation, and language modeling",
    "checked": true,
    "id": "bdc2a4127e9c5fcda100e652ebb8452769e8fc7a",
    "semantic_title": "pidformer: transformer meets control theory",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=TUKOklS3gg": {
    "title": "Inverse-Variance Weighting for Estimation of Heterogeneous Treatment Effects",
    "volume": "poster",
    "abstract": "Many methods for estimating conditional average treatment effects (CATEs) can be expressed as weighted pseudo-outcome regressions (PORs). Previous comparisons of POR techniques have paid careful attention to the choice of pseudo-outcome transformation. However, we argue that the dominant driver of performance is actually the choice of weights. For example, we point out that R-Learning implicitly performs a POR with inverse-variance weights (IVWs). In the CATE setting, IVWs mitigate the instability associated with inverse-propensity weights, and lead to convenient simplifications of bias terms. We demonstrate the superior performance of IVWs in simulations, and derive convergence rates for IVWs that are, to our knowledge, the fastest yet shown without assuming knowledge of the covariate distribution",
    "checked": false,
    "id": "006a306e7a49c57462061b8e9716a6d43aa2eab2",
    "semantic_title": "the connection between r-learning and inverse-variance weighting for estimation of heterogeneous treatment effects",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hbsKxUEreL": {
    "title": "Online Adaptive Anomaly Thresholding with Confidence Sequences",
    "volume": "poster",
    "abstract": "Selecting appropriate thresholds for anomaly detection in online, unsupervised settings is a challenging task, especially in the presence of data distribution shifts. Addressing these challenges is critical in many practical large scale systems, such as infrastructure monitoring and network intrusion detection. This paper proposes an algorithm that connects online thresholding with constructing confidence sequences achieving (1) adaptive online threshold selection robust to distribution shifts, (2) statistical guarantees on false positive and false negative rates without any distributional assumptions, and (3) improved performance when given relevant offline data to warm-start the online algorithm, while having bounded degradation if the offline data is irrelevant. We complement our theoretical results by empirical evidence that our method outperforms commonly used baselines across synthetic and real world datasets",
    "checked": false,
    "id": "23b892babfe1eeea75f9370f90ce01cc47d0ebc9",
    "semantic_title": "an adaptive filtering method for cooperative localization in leader–follower auvs",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=KgfGxXbjjE": {
    "title": "CKGConv: General Graph Convolution with Continuous Kernels",
    "volume": "poster",
    "abstract": "The existing definitions of graph convolution, either from spatial or spectral perspectives, are inflexible and not unified. Defining a general convolution operator in the graph domain is challenging due to the lack of canonical coordinates, the presence of irregular structures, and the properties of graph symmetries. In this work, we propose a novel and general graph convolution framework by parameterizing the kernels as continuous functions of pseudo-coordinates derived via graph positional encoding. We name this Continuous Kernel Graph Convolution (CKGConv). Theoretically, we demonstrate that CKGConv is flexible and expressive. CKGConv encompasses many existing graph convolutions, and exhibits a stronger expressiveness, as powerful as graph transformers in terms of distinguishing non-isomorphic graphs. Empirically, we show that CKGConv-based Networks outperform existing graph convolutional networks and perform comparably to the best graph transformers across a variety of graph datasets. The code and models are publicly available at https://github.com/networkslab/CKGConv",
    "checked": true,
    "id": "dc7ac4cd64a55db1e91ad8cf0ddbc55c9f178b57",
    "semantic_title": "ckgconv: general graph convolution with continuous kernels",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=po4NsL9KvX": {
    "title": "An Intrinsic Vector Heat Network",
    "volume": "poster",
    "abstract": "Vector fields are widely used to represent and model flows for many science and engineering applications. This paper introduces a novel neural network architecture for learning tangent vector fields that are intrinsically defined on manifold surfaces embedded in 3D. Previous approaches to learning vector fields on surfaces treat vectors as multi-dimensional scalar fields, using traditional scalar-valued architectures to process channels individually, thus fail to preserve fundamental intrinsic properties of the vector field. The core idea of this work is to introduce a trainable vector heat diffusion module to spatially propagate vector-valued feature data across the surface, which we incorporate into our proposed architecture that consists of vector-valued neurons. Our architecture is invariant to rigid motion of the input, isometric deformation, and choice of local tangent bases, and is robust to discretizations of the surface. We evaluate our Vector Heat Network on triangle meshes, and empirically validate its invariant properties. We also demonstrate the effectiveness of our method on the useful industrial application of quadrilateral mesh generation",
    "checked": true,
    "id": "9a9baa006d5fcbe47eb73dccc1c513060430b872",
    "semantic_title": "an intrinsic vector heat network",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wDDGQabYPQ": {
    "title": "InferCept: Efficient Intercept Support for Augmented Large Language Model Inference",
    "volume": "poster",
    "abstract": "Large language models are increasingly integrated with external environments, tools, and agents like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat each external interaction as the end of LLM generation and form a new request when the interaction finishes, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents **InferCept, the first LLM inference framework targeting augmented LLMs** and supporting the efficient interception of LLM generation. InferCept minimizes the GPU resource waste caused by LLM interceptions and dedicates saved memory for serving more requests.InferCept improves the overall serving throughput by **1.6x-2x** and completes 2x more requests per second compared to the state-of-the-art LLM inference systems",
    "checked": true,
    "id": "daed41785efecb393898af61b27934ffa5421230",
    "semantic_title": "infercept: efficient intercept support for augmented large language model inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0wso32h0jc": {
    "title": "Neural-Kernel Conditional Mean Embeddings",
    "volume": "poster",
    "abstract": "Kernel conditional mean embeddings (CMEs) offer a powerful framework for representing conditional distributions, but they often face scalability and expressiveness challenges. In this work, we propose a new method that effectively combines the strengths of deep learning with CMEs in order to address these challenges. Specifically, our approach leverages the end-to-end neural network (NN) optimization framework using a kernel-based objective. This design circumvents the computationally expensive Gram matrix inversion required by current CME methods. To further enhance performance, we provide efficient strategies to optimize the remaining kernel hyperparameters. In conditional density estimation tasks, our NN-CME hybrid achieves competitive performance and often surpasses existing deep learning-based methods. Lastly, we showcase its remarkable versatility by seamlessly integrating it into reinforcement learning (RL) contexts. Building on Q-learning, our approach naturally leads to a new variant of distributional RL methods, which demonstrates consistent effectiveness across different environments",
    "checked": true,
    "id": "cb0d0b8bfcc2363d44e11556812170419d65f7de",
    "semantic_title": "neural-kernel conditional mean embeddings",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=a8ZpjLJuKk": {
    "title": "Critical windows: non-asymptotic theory for feature emergence in diffusion models",
    "volume": "poster",
    "abstract": "We develop theory to understand an intriguing property of diffusion models for image generation that we term *critical windows*. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Meng et al., 2022; Choi et al., 2022; Raya & Ambrogioni, 2023; Georgiev et al., 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned Gaussian mixtures. Finally, we use our bounds to give a rigorous interpretation of diffusion models as hierarchical samplers that progressively \"decide\" output features over a discrete sequence of times. We validate our bounds with experiments on synthetic data and show that critical windows may serve as a useful tool for diagnosing fairness and privacy violations in real-world diffusion models",
    "checked": true,
    "id": "2185853ce4844742cefdd52fde21c7fd0b9e6355",
    "semantic_title": "critical windows: non-asymptotic theory for feature emergence in diffusion models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=SkI6u81AkI": {
    "title": "Efficient and Effective Time-Series Forecasting with Spiking Neural Networks",
    "volume": "poster",
    "abstract": "Spiking neural networks (SNNs), inspired by the spiking behavior of biological neurons, provide a unique pathway for capturing the intricacies of temporal data. However, applying SNNs to time-series forecasting is challenging due to difficulties in effective temporal alignment, complexities in encoding processes, and the absence of standardized guidelines for model selection. In this paper, we propose a framework for SNNs in time-series forecasting tasks, leveraging the efficiency of spiking neurons in processing temporal information. Through a series of experiments, we demonstrate that our proposed SNN-based approaches achieve comparable or superior results to traditional time-series forecasting methods on diverse benchmarks with much less energy consumption. Furthermore, we conduct detailed analysis experiments to assess the SNN's capacity to capture temporal dependencies within time-series data, offering valuable insights into its nuanced strengths and effectiveness in modeling the intricate dynamics of temporal data. Our study contributes to the expanding field of SNNs and offers a promising alternative for time-series forecasting tasks, presenting a pathway for the development of more biologically inspired and temporally aware forecasting models. Our code is available at https://github.com/microsoft/SeqSNN",
    "checked": true,
    "id": "6c806691d8758f6c7924f4541849a2f0995d1d02",
    "semantic_title": "efficient and effective time-series forecasting with spiking neural networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kIHIA6Lr0B": {
    "title": "Pluvial Flood Emulation with Hydraulics-informed Message Passing",
    "volume": "poster",
    "abstract": "Machine Learning (ML) has emerged as a promising alternative to numerical methods for physics-based simulation due to its flexibility and efficiency. Flood modeling is a key case study for ML-based simulation due to its relevance as a tool for supporting preventive and emergency measures to mitigate flood risks. However, the complexity of the topography or domain (ground elevation) and the sparsity of the time-evolving precipitations (external forcing) can be challenging for most existing ML approaches for simulating flooding processes in space and time. Another critical challenge is incorporating physics domain knowledge (hydraulics) into these data-driven models. This paper addresses these challenges by introducing a hydraulics-informed graph neural network for flood simulation. Given a (geographical) region and precipitation data, our model predicts water depths in an auto-regressive fashion. We propose a message-passing framework inspired by the conservation of momentum and mass expressed in the shallow-water equations, which describe the physical process of a flooding event. Empirical results on a dataset covering 9 regions and 7 historical precipitation events demonstrate that our model outperforms the best baseline, and can capture the propagation of water flow more effectively, especially at the very early stage of the flooding event when the amount of water in the domain is scarce. Differently from some of the most recent methods for ML-based simulation, which tend to work well only when the domain is a smooth surface (e.g., flat terrain), we show that our solution achieves accurate results for real ground elevation data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=igRAPavrrS": {
    "title": "Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation",
    "volume": "poster",
    "abstract": "We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss. Under modest assumptions on the input, we characterize the distribution of the iterate at each time step. Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data. This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023). Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set. We validate our theorems through experiments on synthetic data",
    "checked": true,
    "id": "3e80d492e909d1bed8fd02270f9addc86ab29f48",
    "semantic_title": "private gradient descent for linear regression: tighter error bounds and instance-specific uncertainty estimation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=jsmaWEdx9g": {
    "title": "Efficient Algorithms for Sum-Of-Minimum Optimization",
    "volume": "poster",
    "abstract": "In this work, we propose a novel optimization model termed ``sum-of-minimum\" optimization. This model seeks to minimize the sum or average of $N$ objective functions over $k$ parameters, where each objective takes the minimum value of a predefined sub-function with respect to the $k$ parameters. This universal framework encompasses numerous clustering applications in machine learning and related fields. We develop efficient algorithms for solving sum-of-minimum optimization problems, inspired by a randomized initialization algorithm for the classic $k$-means (Arthur & Vassilvitskii, 2007) and Lloyd's algorithm (Lloyd, 1982). We establish a new tight bound for the generalized initialization algorithm and prove a gradient-descent-like convergence rate for generalized Lloyd's algorithm. The efficiency of our algorithms is numerically examined on multiple tasks, including generalized principal component analysis, mixed linear regression, and small-scale neural network training. Our approach compares favorably to previous ones based on simpler-but-less-precise optimization reformulations",
    "checked": true,
    "id": "5a6f518936907b0d1f8012d912a22e0b6fb5eee9",
    "semantic_title": "efficient algorithms for sum-of-minimum optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=X8Ha2NiQcy": {
    "title": "Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency",
    "volume": "poster",
    "abstract": "Recent research has focused on weight sparsity in deep neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often compromises accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., the sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training (DST) with Sparse-IFT models effectively navigate this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting any training hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To the best of our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a set of simple-to-use sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT",
    "checked": true,
    "id": "c62321199fb07ce317921ff8832e9da1cee778af",
    "semantic_title": "sparse-ift: sparse iso-flop transformations for maximizing training efficiency",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=rJti61Uere": {
    "title": "Learning to Compile Programs to Neural Networks",
    "volume": "poster",
    "abstract": "A *neural surrogate* is a neural network that mimics the behavior of a program. Neural surrogates of programs have been used to automatically tune program inputs, adapt programs to new settings, and accelerate computations. Neural surrogates have traditionally been developed by training on input-output examples for a single program. Language models present another approach wherein a model is trained on a single, large dataset then directly consumes program text, to act as a neural surrogate of the program. Having the language model as both the neural surrogate generator and the neural surrogate, however, poses a tradeoff of limited accuracy or excessive resource consumption. We present *neural surrogate compilation*, a technique for producing neural surrogates directly from program text without coupling neural surrogate generation and execution. We implement neural surrogate compilers using hypernetworks trained on a dataset of C programs and find they produce neural surrogates that are $1.91$-$9.50\\times$ as data-efficient and train in $4.31$-$7.28\\times$ fewer epochs than neural surrogates trained from scratch",
    "checked": false,
    "id": "e978f2f723aaa7772da7c03153cc89c847d18109",
    "semantic_title": "explanatory learning: beyond empiricism in neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=XtDJaSe8jE": {
    "title": "Transferring Knowledge From Large Foundation Models to Small Downstream Models",
    "volume": "poster",
    "abstract": "How do we transfer the relevant knowledge from ever larger foundation models into small, task-specific downstream models that can run at much lower costs? Standard transfer learning using pre-trained weights as the initialization transfers limited information and commits us to often massive pre-trained architectures. This procedure also precludes combining multiple pre-trained models that learn complementary information. To address these shortcomings, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the smaller downstream model. Rather than indiscriminately compressing all pre-trained features, AFT adaptively transfers pre-trained features that are most useful for performing the downstream task, using a simple regularization that adds minimal overhead. Across multiple vision, language, and multi-modal datasets, AFT achieves significantly better downstream performance compared to alternatives with a similar computational cost. Furthermore, AFT reliably translates improvement in pre-trained models into improvement in downstream performance, even if the downstream model is over $50\\times$ smaller, and can effectively transfer complementary information learned by multiple pre-trained models",
    "checked": true,
    "id": "1e381cb0d3061f64a271d047706ec42505c96795",
    "semantic_title": "transferring knowledge from large foundation models to small downstream models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uRz9GZN17X": {
    "title": "Bidirectional Reciprocative Information Communication for Few-Shot Semantic Segmentation",
    "volume": "poster",
    "abstract": "Existing few-shot semantic segmentation methods typically rely on a one-way flow of category information from support to query, ignoring the impact of intra-class diversity. To address this, drawing inspiration from cybernetics, we introduce a Query Feedback Branch (QFB) to propagate query information back to support, generating a query-related support prototype that is more aligned with the query. Subsequently, a Query Amplifier Branch (QAB) is employed to amplify target objects in the query using the acquired support prototype. To further improve the model, we propose a Query Rectification Module (QRM), which utilizes the prediction disparity in the query before and after support activation to identify challenging positive and negative samples from ambiguous regions for query self-rectification. Furthermore, we integrate the QFB, QAB, and QRM into a feedback and rectification layer and incorporate it into an iterative pipeline. This configuration enables the progressive enhancement of bidirectional reciprocative flow of category information between query and support, effectively providing query-adaptive support information and addressing the intra-class diversity problem. Extensive experiments conducted on both PASCAL-5i and COCO-20i datasets validate the effectiveness of our approach. The code is available at https://github.com/LIUYUANWEI98/IFRNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KzACYw0MTV": {
    "title": "QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
    "volume": "poster",
    "abstract": "As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at https://github.com/mit-han-lab/quest",
    "checked": true,
    "id": "1c7db9fb18246787fbe3de6e0eaa370ae749e795",
    "semantic_title": "quest: query-aware sparsity for efficient long-context llm inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OiI12sNbgD": {
    "title": "Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "Recently, various pre-training methods have been introduced in vision-based Reinforcement Learning (RL). However, their generalization ability remains unclear due to evaluations being limited to in-distribution environments and non-unified experimental setups. To address this, we introduce the Atari Pre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10 million transitions from 50 Atari games and evaluates it across diverse environment distributions. Our experiments show that pre-training objectives focused on learning task-agnostic features (e.g., identifying objects and understanding temporal dynamics) enhance generalization across different environments. In contrast, objectives focused on learning task-specific knowledge (e.g., identifying agents and fitting reward functions) improve performance in environments similar to the pre-training dataset but not in varied ones. We publicize our codes, datasets, and model checkpoints at https://github.com/dojeon-ai/Atari-PB",
    "checked": true,
    "id": "7f1b7dff5330f57eb9587fd0c7f0699acb12d5a5",
    "semantic_title": "investigating pre-training objectives for generalization in vision-based reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ucl3B05EsX": {
    "title": "Integrated Hardware Architecture and Device Placement Search",
    "volume": "poster",
    "abstract": "Distributed execution of deep learning training involves a dynamic interplay between hardware accelerator architecture and device placement strategy. This is the first work to explore the co-optimization of determining the optimal architecture and device placement strategy through novel algorithms, improving the balance of computational resources, memory usage, and data distribution. Our architecture search leverages tensor and vector units, determining their quantity and dimensionality, and on-chip and off-chip memory configurations. It also determines the microbatch size and decides whether to recompute or stash activations, balancing the memory footprint of training and storage size. For each explored architecture configuration, we use an Integer Linear Program (ILP) to find the optimal schedule for executing operators on the accelerator. The ILP results then integrate with a dynamic programming solution to identify the most effective device placement strategy, combining data, pipeline, and tensor model parallelism across multiple accelerators. Our approach achieves higher throughput on large language models compared to the state-of-the-art TPUv4 and the Spotlight accelerator search framework. The entire source code of PHAZE is available at https://github.com/msr-fiddle/phaze",
    "checked": false,
    "id": "c18ca9e487c9551b86b2ff54bfa2c48880b583f3",
    "semantic_title": "designing efficient dnns via hardware-aware neural architecture search and beyond",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=UCKFhc9SFC": {
    "title": "Provably Efficient Long-Horizon Exploration in Monte Carlo Tree Search through State Occupancy Regularization",
    "volume": "poster",
    "abstract": "Monte Carlo tree search (MCTS) has been successful in a variety of domains, but faces challenges with long-horizon exploration when compared to sampling-based motion planning algorithms like Rapidly-Exploring Random Trees. To address these limitations of MCTS, we derive a tree search algorithm based on policy optimization with state-occupancy measure regularization, which we call *Volume-MCTS*. We show that count-based exploration and sampling-based motion planning can be derived as approximate solutions to this state-occupancy measure regularized objective. We test our method on several robot navigation problems, and find that Volume-MCTS outperforms AlphaZero and displays significantly better long-horizon exploration properties",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rADFNrIss3": {
    "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
    "volume": "poster",
    "abstract": "Large language models (LLMs) are instruction followers but the performance varies under different instructions. It is challenging to create the best instruction, especially for black-box LLMs on which backpropagation is forbidden. Instead of directly optimizing the discrete instruction, we optimize a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM. In each optimization step of the proposed method InstructZero, a soft prompt is converted into an instruction by the open-source LLM, which is then submitted to the black-box LLM for zero-shot evaluation, whose result is sent to Bayesian optimization to produce new soft prompts improving the zero-shot performance. We evaluate InstructZero on different combinations of open-source LLMs and APIs including Vicuna and ChatGPT. InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks",
    "checked": true,
    "id": "d6fb1c21a46fb8b0f3f4383fd467b21e5b58c55f",
    "semantic_title": "instructzero: efficient instruction optimization for black-box large language models",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=uun4fzaiat": {
    "title": "Understanding the Training Speedup from Sampling with Approximate Losses",
    "volume": "poster",
    "abstract": "It is well known that selecting samples with large losses/gradients can significantly reduce the number of training steps. However, the selection overhead is often too high to yield any meaningful gains in terms of overall training time. In this work, we focus on the greedy approach of selecting samples with large *approximate losses* instead of exact losses in order to reduce the selection overhead. For smooth convex losses, we show that such a greedy strategy can converge to a constant factor of the minimum value of the average loss in fewer iterations than the standard approach of random selection. We also theoretically quantify the effect of the approximation level. We then develop SIFT which uses early exiting to obtain approximate losses with an intermediate layer's representations for sample selection. We evaluate SIFT on the task of training a 110M parameter 12 layer BERT base model, and show significant gains (in terms of training hours and number of backpropagation steps) without any optimized implementation over vanilla training. For e.g., to reach 64% validation accuracy, SIFT with exit at the first layer takes $\\sim$ 43 hours compared to $\\sim$ 57 hours of vanilla training",
    "checked": true,
    "id": "e96e8db4eaed629090dff1e04d24ba45a57ebf5d",
    "semantic_title": "understanding the training speedup from sampling with approximate losses",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QLOvxGwbIM": {
    "title": "Bayesian Power Steering: An Effective Approach for Domain Adaptation of Diffusion Models",
    "volume": "poster",
    "abstract": "We propose a Bayesian framework for fine-tuning large diffusion models with a novel network structure called Bayesian Power Steering (BPS). We clarify the meaning behind adaptation from a large probability space to a small probability space and explore the task of fine-tuning pre-trained models using learnable modules from a Bayesian perspective. BPS extracts task-specific knowledge from a pre-trained model's learned prior distribution. It efficiently leverages large diffusion models, differentially intervening different hidden features with a head-heavy and foot-light configuration. Experiments highlight the superiority of BPS over contemporary methods across a range of tasks even with limited amount of data. Notably, BPS attains an FID score of 10.49 under the sketch condition on the COCO17 dataset",
    "checked": true,
    "id": "d74b8bbcef89c39d55694067d089765b8f6efd30",
    "semantic_title": "bayesian power steering: an effective approach for domain adaptation of diffusion models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DqC9XiI71U": {
    "title": "Adaptive Group Personalization for Federated Mutual Transfer Learning",
    "volume": "poster",
    "abstract": "Mutual transfer learning aims to improve prediction with knowledge from related domains. Recently, federated learning is applied in this field to address the communication and privacy concerns. However, previous clustered federated learning (CFL) solutions lack theoretical guarantee of learnability recovery and require time-consuming hyper-parameter tuning, while centralized mutual transfer learning methods lack adaptability to concept drifts. In this paper, we propose the Adaptive Group Personalization method (**AdaGrP**) to overcome these challenges. We adaptively decide the recovery threshold with a nonparametric method, *adaptive threshold correction*, for tuning-free solution with relaxed condition. Theoretical results guarantee the perfect learnability recovery with the corrected threshold. Empirical results show AdaGrP achieves 16.9% average improvement in learnability structure recovery compared with state-of-the-art CFL baselines",
    "checked": false,
    "id": "82e8904da002bdb4f5b4903b477ade5aaac2db14",
    "semantic_title": "uncertainty quantification in federated learning for heterogeneous health data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0ZFWfeVsaD": {
    "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
    "volume": "poster",
    "abstract": "Given the increasing number of parameter-efficient adapters of large language models (LLMs), how can we reuse them to improve LLM performance on new tasks? We study how to best build a *library* of adapters given multi-task data and devise techniques for both *zero-shot* and *supervised* task generalization through *routing* in such library. We benchmark existing approaches to build this library and introduce model-based clustering, $\\texttt{MBC}$, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. In order to reuse the library, we present a novel zero-shot routing mechanism, $\\texttt{Arrow}$, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. Thus, we make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training",
    "checked": true,
    "id": "6839e8ef0205ad4732e9f743977eb5bfc296ec2c",
    "semantic_title": "towards modular llms by building and reusing a library of loras",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=v8MgLJ7kbL": {
    "title": "Model Assessment and Selection under Temporal Distribution Shift",
    "volume": "poster",
    "abstract": "We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and empirical experiments underscore the adaptivity of our proposed methods to the non-stationarity in data",
    "checked": true,
    "id": "d3485b562d0225041f3e37f0fd1bdf6e1531d8c3",
    "semantic_title": "model assessment and selection under temporal distribution shift",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=x1G7ieRgRd": {
    "title": "Improved Communication-Privacy Trade-offs in $L_2$ Mean Estimation under Streaming Differential Privacy",
    "volume": "poster",
    "abstract": "We study $L_2$ mean estimation under central differential privacy and communication constraints, and address two key challenges: firstly, existing mean estimation schemes that simultaneously handle both constraints are usually optimized for $L_\\infty$ geometry and rely on random rotation or Kashin's representation to adapt to $L_2$ geometry, resulting in suboptimal leading constants in mean square errors (MSEs); secondly, schemes achieving order-optimal communication-privacy trade-offs do not extend seamlessly to streaming differential privacy (DP) settings (e.g., tree aggregation or matrix factorization), rendering them incompatible with DP-FTRL type optimizers. In this work, we tackle these issues by introducing a novel privacy accounting method for the sparsified Gaussian mechanism that incorporates the randomness inherent in sparsification into the DP noise. Unlike previous approaches, our accounting algorithm directly operates in $L_2$ geometry, yielding MSEs that fast converge to those of the uncompressed Gaussian mechanism. Additionally, we extend the sparsification scheme to the matrix factorization framework under streaming DP and provide a precise accountant tailored for DP-FTRL type optimizers. Empirically, our method demonstrates at least a 100x improvement of compression for DP-SGD across various FL tasks",
    "checked": false,
    "id": "80f33e2da153d56ba496f1075912a80b8df84f2c",
    "semantic_title": "improved communication-privacy trade-offs in l2 mean estimation under streaming differential privacy",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kbd9A4lVoX": {
    "title": "Causally Motivated Personalized Federated Invariant Learning with Shortcut-Averse Information-Theoretic Regularization",
    "volume": "poster",
    "abstract": "Exploiting invariant relations and mitigating spurious correlation (a.k.a., shortcut) between representation and target across varied data distributions can tackle the challenging out-of-distribution (OOD) generalization problem. In personalized federated learning (PFL), heterogeneous data distribution across local clients offers the inherent prerequisites to extract the invariant features that maintain invariant relation with target. Nevertheless, personalized features are closely entangled with spurious features in PFL since they exhibit similar variability across different clients, which makes preserving personalization knowledge and eliminating shortcuts two conflicting objectives in PFL. To address the above challenge, we analyse the heterogeneous data generation on local clients through the lens of structured causal model and propose a crucial causal signature which can distinguish personalized features from spurious features with global invariant features as the anchor. Then the causal signature is quantified as an information-theoretic constraint that facilitates the shortcut-averse personalized invariant learning on each client. Theoretical analysis demonstrates our method, FedPIN, can yield a tighter bound on generalization error than the prevalent PFL approaches when train-test distribution shift exists on clients. Moreover, we provide a theoretical guarantee on the convergence rate of FedPIN in this paper. The results of extensive experiments show that our method can achieve superior OOD generalization performance compared with the state-of-the-art competitors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5SpjhZNXtt": {
    "title": "Position: Data-driven Discovery with Large Generative Models",
    "volume": "poster",
    "abstract": "With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery—a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DataVoyager, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata—a feat previously unattainable—while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging. We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility",
    "checked": false,
    "id": "8b7fee2d868132955d0eed1ef33d49b49c5f456e",
    "semantic_title": "data-driven discovery with large generative models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=blzDxD6bKt": {
    "title": "Nonlinear Filtering with Brenier Optimal Transport Maps",
    "volume": "poster",
    "abstract": "This paper is concerned with the problem of nonlinear filtering, i.e., computing the conditional distribution of the state of a stochastic dynamical system given a history of noisy partial observations. Conventional sequential importance resampling (SIR) particle filters suffer from fundamental limitations, in scenarios involving degenerate likelihoods or high-dimensional states, due to the weight degeneracy issue. In this paper, we explore an alternative method, which is based on estimating the Brenier optimal transport (OT) map from the current prior distribution of the state to the posterior distribution at the next time step. Unlike SIR particle filters, the OT formulation does not require the analytical form of the likelihood. Moreover, it allows us to harness the approximation power of neural networks to model complex and multi-modal distributions and employ stochastic optimization algorithms to enhance scalability. Extensive numerical experiments are presented that compare the OT method to the SIR particle filter and the ensemble Kalman filter, evaluating the performance in terms of sample efficiency, high-dimensional scalability, and the ability to capture complex and multi-modal distributions",
    "checked": true,
    "id": "1ffd2ff11b5b65a83c2224c56731d3a9c01d196c",
    "semantic_title": "nonlinear filtering with brenier optimal transport maps",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=SQIDlJd3hN": {
    "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
    "volume": "poster",
    "abstract": "We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates simulation environments by populating pertinent assets with proper spatial configurations. Afterwards, the agent decomposes the proposed task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments",
    "checked": true,
    "id": "c62711f6b5d8620ba36bc2c378ec6ab53f6e197c",
    "semantic_title": "robogen: towards unleashing infinite data for automated robot learning via generative simulation",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=9HdQr68Zyl": {
    "title": "Open-Domain Text Evaluation via Contrastive Distribution Methods",
    "volume": "poster",
    "abstract": "Recent advancements in open-domain text generation, driven by the power of large pre-trained language models (LLMs), have demonstrated remarkable performance. However, assessing these models' generation quality remains a challenge. In this paper, we introduce a novel method for evaluating open-domain text generation called Contrastive Distribution Methods (CDM). Leveraging the connection between increasing model parameters and enhanced LLM performance, CDM creates a mapping from the _contrast_ of two probabilistic distributions -- one known to be superior to the other -- to quality measures. We investigate CDM for open-domain text generation evaluation under two paradigms: 1) _Generative_ CDM, which harnesses the contrast of two language models' distributions to generate synthetic examples for training discriminator-based metrics; 2) _Discriminative_ CDM, which directly uses distribution disparities between two language models for evaluation. Our experiments on coherence evaluation for multi-turn dialogue and commonsense evaluation for controllable generation demonstrate CDM's superior correlate with human judgment than existing automatic evaluation metrics, highlighting the strong performance and generalizability of our approach",
    "checked": true,
    "id": "0a89829b68a10ee441357b64cb521a6379e953b2",
    "semantic_title": "open-domain text evaluation via contrastive distribution methods",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=t3SEfoTaYQ": {
    "title": "Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation",
    "volume": "poster",
    "abstract": "Adaptive brain stimulation can treat neurological conditions such as Parkinson's disease and post-stroke motor deficits by influencing abnormal neural activity. Because of patient heterogeneity, each patient requires a unique stimulation policy to achieve optimal neural responses. Model-free reinforcement learning (MFRL) holds promise in learning effective policies for a variety of similar control tasks, but is limited in domains like brain stimulation by a need for numerous costly environment interactions. In this work we introduce Coprocessor Actor Critic, a novel, model-based reinforcement learning (MBRL) approach for learning neural coprocessor policies for brain stimulation. Our key insight is that coprocessor policy learning is a combination of learning how to act optimally in the world and learning how to induce optimal actions in the world through stimulation of an injured brain. We show that our approach overcomes the limitations of traditional MFRL methods in terms of sample efficiency and task success and outperforms baseline MBRL approaches in a neurologically realistic model of an injured brain",
    "checked": true,
    "id": "528190b726e03b77b617bd327bc70a630509a1c9",
    "semantic_title": "coprocessor actor critic: a model-based reinforcement learning approach for adaptive brain stimulation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G1igwiBBUj": {
    "title": "GFlowNet Training by Policy Gradients",
    "volume": "poster",
    "abstract": "Generative Flow Networks (GFlowNets) have been shown effective to generate combinatorial objects with desired properties. We here propose a new GFlowNet training framework, with policy-dependent rewards, that bridges keeping flow balance of GFlowNets to optimizing the expected accumulated reward in traditional Reinforcement-Learning (RL). This enables the derivation of new policy-based GFlowNet training methods, in contrast to existing ones resembling value-based RL. It is known that the design of backward policies in GFlowNet training affects efficiency. We further develop a coupled training strategy that jointly solves GFlowNet forward policy training and backward policy design. Performance analysis is provided with a theoretical guarantee of our policy-based GFlowNet training. Experiments on both simulated and real-world datasets verify that our policy-based strategies provide advanced RL perspectives for robust gradient estimation to improve GFlowNet performance. Our code is available at: [github.com/niupuhua1234/GFN-PG](https://github.com/niupuhua1234/GFN-PG)",
    "checked": false,
    "id": "d04a26939d9783d75fa307fec3b358a3264e10f0",
    "semantic_title": "gflownets and variational inference",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=lon750Kf7n": {
    "title": "Density-Softmax: Efficient Test-time Model for Uncertainty Estimation and Robustness under Distribution Shifts",
    "volume": "poster",
    "abstract": "Sampling-based methods, e.g., Deep Ensembles and Bayesian Neural Nets have become promising approaches to improve the quality of uncertainty estimation and robust generalization. However, they suffer from a large model size and high latency at test time, which limits the scalability needed for low-resource devices and real-time applications. To resolve these computational issues, we propose Density-Softmax, a sampling-free deterministic framework via combining a density function built on a Lipschitz-constrained feature extractor with the softmax layer. Theoretically, we show that our model is the solution of minimax uncertainty risk and is distance-aware on feature space, thus reducing the over-confidence of the standard softmax under distribution shifts. Empirically, our method enjoys competitive results with state-of-the-art techniques in terms of uncertainty and robustness, while having a lower number of model parameters and a lower latency at test time",
    "checked": true,
    "id": "69640b4f08dbac192d1e2643a007fb48b88069d5",
    "semantic_title": "density-softmax: efficient test-time model for uncertainty estimation and robustness under distribution shifts",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XobPpcN4yZ": {
    "title": "Value-Evolutionary-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "Combining Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for policy search has been proven to improve RL performance. However, previous works largely overlook value-based RL in favor of merging EAs with policy-based RL. This paper introduces Value-Evolutionary-Based Reinforcement Learning (VEB-RL) that focuses on the integration of EAs with value-based RL. The framework maintains a population of value functions instead of policies and leverages negative Temporal Difference error as the fitness metric for evolution. The metric is more sample-efficient for population evaluation than cumulative rewards and is closely associated with the accuracy of the value function approximation. Additionally, VEB-RL enables elites of the population to interact with the environment to offer high-quality samples for RL optimization, whereas the RL value function participates in the population's evolution in each generation. Experiments on MinAtar and Atari demonstrate the superiority of VEB-RL in significantly improving DQN, Rainbow, and SPR. Our code is available on https://github.com/yeshenpy/VEB-RL",
    "checked": false,
    "id": "477b3dd382fc0a50b80732c2b1df7eea19517a3f",
    "semantic_title": "parameter control framework for multiobjective evolutionary computation based on deep reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qqPL0DkcrI": {
    "title": "Learning High-Frequency Functions Made Easy with Sinusoidal Positional Encoding",
    "volume": "poster",
    "abstract": "Fourier features based positional encoding (PE) is commonly used in machine learning tasks that involve learning high-frequency features from low-dimensional inputs, such as 3D view synthesis and time series regression with neural tangent kernels. Despite their effectiveness, existing PEs require manual, empirical adjustment of crucial hyperparameters, specifically the Fourier features, tailored to each unique task. Further, PEs face challenges in efficiently learning high-frequency functions, particularly in tasks with limited data. In this paper, we introduce sinusoidal PE (SPE), designed to efficiently learn adaptive frequency features closely aligned with the true underlying function. Our experiments demonstrate that SPE, without hyperparameter tuning, consistently achieves enhanced fidelity and faster training across various tasks, including 3D view synthesis, Text-to-Speech generation, and 1D regression. SPE is implemented as a direct replacement for existing PEs. Its plug-and-play nature lets numerous tasks easily adopt and benefit from SPE",
    "checked": false,
    "id": "251dad991994f36c5e32a14335a30599ffe5c9f7",
    "semantic_title": "coordinate quantized neural implicit representations for multi-view reconstruction",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=8PTx4CpNoT": {
    "title": "Emergent Representations of Program Semantics in Language Models Trained on Programs",
    "volume": "poster",
    "abstract": "We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the *unobserved, intermediate* grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to *interpret* programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We anticipate that this technique may be generally applicable to a broad range of *semantic* probing experiments. In summary, this paper does not propose any new techniques for training LMs of code, but develops an experimental framework for and provides insights into the acquisition and representation of formal semantics in statistical models of code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RbnojVv4HK": {
    "title": "Ameliorate Spurious Correlations in Dataset Condensation",
    "volume": "poster",
    "abstract": "Dataset Condensation has emerged as a technique for compressing large datasets into smaller synthetic counterparts, facilitating downstream training tasks. In this paper, we study the impact of bias inside the original dataset on the performance of dataset condensation. With a comprehensive empirical evaluation on canonical datasets with color, corruption and background biases, we found that color and background biases in the original dataset will be amplified through the condensation process, resulting in a notable decline in the performance of models trained on the condensed dataset, while corruption bias is suppressed through the condensation process. To reduce bias amplification in dataset condensation, we introduce a simple yet highly effective approach based on a sample reweighting scheme utilizing kernel density estimation. Empirical results on multiple real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Notably, on CMNIST with 5% bias-conflict ratio and IPC 50, our method achieves 91.5% test accuracy compared to 23.8% from vanilla DM, boosting the performance by 67.7%, whereas applying state-of-the-art debiasing method on the same dataset only achieves 53.7% accuracy. Our findings highlight the importance of addressing biases in dataset condensation and provide a promising avenue to address bias amplification in the process",
    "checked": true,
    "id": "0149bf6c34cd06b9bc89022212099a91611ebf31",
    "semantic_title": "ameliorate spurious correlations in dataset condensation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NKirMgDsut": {
    "title": "Random Scaling and Momentum for Non-smooth Non-convex Optimization",
    "volume": "poster",
    "abstract": "Training neural networks requires optimizing a loss function that may be highly irregular, and in particular neither convex nor smooth. Popular training algorithms are based on stochastic gradient descent with momentum (SGDM), for which classical analysis applies only if the loss is either convex or smooth. We show that a very small modification to SGDM closes this gap: simply scale the update at each time point by an exponentially distributed random scalar. The resulting algorithm achieves optimal convergence guarantees. Intriguingly, this result is not derived by a specific analysis of SGDM: instead, it falls naturally out of a more general framework for converting online convex optimization algorithms to non-convex optimization algorithms",
    "checked": true,
    "id": "ede828aae339089a2a37c08fefff4ef8fcb6357b",
    "semantic_title": "random scaling and momentum for non-smooth non-convex optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FMEhnS0948": {
    "title": "Ranking-based Client Imitation Selection for Efficient Federated Learning",
    "volume": "poster",
    "abstract": "Federated Learning (FL) enables multiple devices to collaboratively train a shared model while ensuring data privacy. The selection of participating devices in each training round critically affects both the model performance and training efficiency, especially given the vast heterogeneity in training capabilities and data distribution across devices. To deal with these challenges, we introduce a novel device selection solution called FedRank, which is based on an end-to-end, ranking-based model that is pre-trained by imitation learning against state-of-the-art analytical approaches. It not only considers data and system heterogeneity at runtime but also adaptively and efficiently chooses the most suitable clients for model training. Specifically, FedRank views client selection in FL as a ranking problem and employs a pairwise training strategy for the smart selection process. Additionally, an imitation learning-based approach is designed to counteract the cold-start issues often seen in state-of-the-art learning-based approaches. Experimental results reveal that FedRank boosts model accuracy by 5.2% to 56.9%, accelerates the training convergence up to $2.01 \\times$ and saves the energy consumption up to 40.1%",
    "checked": false,
    "id": "eb62e1ce5e263dc508a235f98cbbaac0b60e7b23",
    "semantic_title": "ranking-based client selection with imitation learning for efficient federated learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=KwgAThfxEd": {
    "title": "Improving Computational Complexity in Statistical Models with Local Curvature Information",
    "volume": "poster",
    "abstract": "It is known that when the statistical models are singular, i.e., the Fisher information matrix at the true parameter is degenerate, the fixed step-size gradient descent algorithm takes polynomial number of steps in terms of the sample size $n$ to converge to a final statistical radius around the true parameter, which can be unsatisfactory for the practical application. To further improve that computational complexity, we consider utilizing the local curvature information for parameter estimation. Even though there is a rich literature in using the local curvature information for optimization, the statistical rate of these methods in statistical models, to the best of our knowledge, has not been studied rigorously. The major challenge of this problem is due to the non-convex nature of sample loss function. To shed light on these problems, we specifically study the normalized gradient descent (NormGD) algorithm, a variant of gradient descent algorithm whose step size is scaled by the maximum eigenvalue of the Hessian matrix of the empirical loss function, and deal with the aforementioned issue with a population-to-sample analysis. When the population loss function is homogeneous, the NormGD iterates reach a final statistical radius around the true parameter after a logarithmic number of iterations in terms of $n$. Therefore, for fixed dimension $d$, the NormGD algorithm achieves the optimal computational complexity $\\mathcal{O}(n)$ to reach the final statistical radius, which is cheaper than the complexity $\\mathcal{O}(n^{\\tau})$ of the fixed step-size gradient descent algorithm for some $\\tau > 1$",
    "checked": false,
    "id": "67621c1f52a480e13c5bec481f0744dc7214297f",
    "semantic_title": "random g -circulant matrices. spectrum of random convolution operators on large ﬁnite groups",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S6a6gHvMWx": {
    "title": "Position: Social Environment Design Should be Further Developed for AI-based Policy-Making",
    "volume": "poster",
    "abstract": "Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing **Social Environment Design**, a general framework for the use of AI in automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policymaking. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making",
    "checked": false,
    "id": "4430e08f7fb16e787d7891174d78a2ea70a3eb94",
    "semantic_title": "variables that affect parental goals for visiting a children's museum",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zcIV8OQFVF": {
    "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
    "volume": "poster",
    "abstract": "In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators and achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads to predict the preference, one trained to correlate with length and the other trained to decorrelate with length and therefore focusing more on the actual content. We then discard the length head in RL to ignore the spurious length reward. Experiments demonstrate that our approach eliminates the reward correlation with length, and improves the obtained policy by a significant margin",
    "checked": true,
    "id": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
    "semantic_title": "odin: disentangled reward mitigates hacking in rlhf",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=HsliOqZkc0": {
    "title": "The Emergence of Reproducibility and Consistency in Diffusion Models",
    "volume": "poster",
    "abstract": "In this work, we investigate an intriguing and prevalent phenomenon of diffusion models which we term as \"consistent model reproducibility'': given the same starting noise input and a deterministic sampler, different diffusion models often yield remarkably similar outputs. We confirm this phenomenon through comprehensive experiments, implying that different diffusion models consistently reach the same data distribution and score function regardless of diffusion model frameworks, model architectures, or training procedures. More strikingly, our further investigation implies that diffusion models are learning *distinct distributions* influenced by the training data size. This is evident in two distinct training regimes: (I) \"memorization regime,'' where the diffusion model overfits to the training data distribution, and (ii) \"generalization regime,'' where the model learns the underlying data distribution. Our study also finds that this valuable property generalizes to many variants of diffusion models, including those for conditional generation and solving inverse problems. Lastly, we discuss how our findings connect to existing research and highlight the practical implications of our discoveries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0THUA66D8Z": {
    "title": "On the Calibration of Human Pose Estimation",
    "volume": "poster",
    "abstract": "2D human pose estimation predicts keypoint locations and the corresponding confidence. Calibration-wise, the confidence should be aligned with the pose accuracy. Yet existing pose estimation methods tend to estimate confidence with heuristics such as the maximum value of heatmaps. This work shows, through theoretical analysis and empirical verification, a calibration gap in current pose estimation frameworks. Our derivations directly lead to closed-form adjustments in the confidence based on additionally inferred instance size and visibility. Given the black-box nature of deep neural networks, however, it is not possible to close the gap with only closed-form adjustments. We go one step further and propose a Calibrated ConfidenceNet (CCNet) to explicitly learn network-specific adjustments with a confidence prediction branch. The proposed CCNet, as a lightweight post-hoc addition, improves the calibration of standard off-the-shelf pose estimation frameworks",
    "checked": true,
    "id": "bbd748e923d7ee400299d16faf743e3971fde0a6",
    "semantic_title": "on the calibration of human pose estimation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=iGMTxygzcJ": {
    "title": "Gibbs Sampling of Continuous Potentials on a Quantum Computer",
    "volume": "poster",
    "abstract": "Gibbs sampling from continuous real-valued functions is a challenging problem of interest in machine learning. Here we leverage quantum Fourier transforms to build a quantum algorithm for this task when the function is periodic. We use the quantum algorithms for solving linear ordinary differential equations to solve the Fokker–Planck equation and prepare a quantum state encoding the Gibbs distribution. We show that the efficiency of interpolation and differentiation of these functions on a quantum computer depends on the rate of decay of the Fourier coefficients of the Fourier transform of the function. We view this property as a concentration of measure in the Fourier domain, and also provide functional analytic conditions for it. Our algorithm makes zeroeth order queries to a quantum oracle of the function and achieves polynomial quantum speedups in mean estimation in the Gibbs measure for generic non-convex periodic functions. At high temperatures the algorithm also allows for exponentially improved precision in sampling from Morse functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6OkvBGqW62": {
    "title": "Hyperbolic Geometric Latent Diffusion Model for Graph Generation",
    "volume": "poster",
    "abstract": "Diffusion models have made significant contributions to computer vision, sparking a growing interest in the community recently regarding the application of it to graph generation. The existing discrete graph diffusion models exhibit heightened computational complexity and diminished training efficiency. A preferable and natural way is to directly diffuse the graph within the latent space. However, due to the non-Euclidean structure of graphs is not isotropic in the latent space, the existing latent diffusion models effectively make it difficult to capture and preserve the topological information of graphs. To address the above challenges, we propose a novel geometrically latent diffusion framework HypDiff. Specifically, we first establish a geometrically latent space with interpretability measures based on hyperbolic geometry, to define anisotropic latent diffusion processes for graphs. Then, we propose a geometrically latent diffusion process that is constrained by both radial and angular geometric properties, thereby ensuring the preservation of the original topological properties in the generative graphs. Extensive experimental results demonstrate the superior effectiveness of HypDiff for graph generation with various topologies",
    "checked": true,
    "id": "b1b6e6b34600df7ac53c10f034ca6794ce749ff8",
    "semantic_title": "hyperbolic geometric latent diffusion model for graph generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjoUXwuZdy": {
    "title": "VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context",
    "volume": "poster",
    "abstract": "Large Multimodal Models (LMMs) have achieved impressive success in visual reasoning, particularly in visual mathematics. However, problem-solving capabilities in graph theory remain less explored for LMMs, despite being a crucial aspect of mathematical reasoning that requires an accurate understanding of graphical structures and multi-step reasoning on visual graphs. To step forward in this direction, we are the first to design a benchmark named **VisionGraph**, used to explore the capabilities of advanced LMMs in solving multimodal graph theory problems. It encompasses eight complex graph problem tasks, from connectivity to shortest path problems. Subsequently, we present a Description-Program-Reasoning (DPR) chain to enhance the logical accuracy of reasoning processes through graphical structure description generation and algorithm-aware multi-step reasoning. Our extensive study shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph reasoning; 2) All LMMs exhibit inferior perception accuracy for graphical structures, whether in zero/few-shot settings or with supervised fine-tuning (SFT), which further affects problem-solving performance; 3) DPR significantly improves the multi-step graph reasoning capabilities of LMMs and the GPT-4V (DPR) agent achieves SOTA performance",
    "checked": true,
    "id": "eb352664a873392f6b87ba42e171788726c1236a",
    "semantic_title": "visiongraph: leveraging large multimodal models for graph theory problems in visual context",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=tp6ruPIfIV": {
    "title": "Diffusion Posterior Sampling is Computationally Intractable",
    "volume": "poster",
    "abstract": "Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \\mid x)$ and a measurement $y$, and would like to sample from $p(x \\mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time. In this paper we show that posterior sampling is *computationally intractable*: under the most basic assumption in cryptography---that one-way functions exist---there are instances for which *every* algorithm takes superpolynomial time, even though *unconditional* sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert",
    "checked": true,
    "id": "911765f85f9ba6516fac8078ad2182abad604b89",
    "semantic_title": "diffusion posterior sampling is computationally intractable",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=uYISs2tpwP": {
    "title": "Language Models with Conformal Factuality Guarantees",
    "volume": "poster",
    "abstract": "Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. Our insight is that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can provide 80-90% correctness guarantees while retaining the majority of the LM's original output",
    "checked": true,
    "id": "2495700b4303512784fbdbfccc58c6c4f7771ac2",
    "semantic_title": "language models with conformal factuality guarantees",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=BRfqYrikdo": {
    "title": "WorkArena: How Capable are Web Agents at Solving Common Knowledge Work Tasks?",
    "volume": "poster",
    "abstract": "We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 33 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field",
    "checked": true,
    "id": "fa96417f8766568ba570088513940bbf14e3b356",
    "semantic_title": "workarena: how capable are web agents at solving common knowledge work tasks?",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=QGAeWRRe6e": {
    "title": "Optimizing Watermarks for Large Language Models",
    "volume": "poster",
    "abstract": "With the rise of large language models (LLMs) and concerns about potential misuse, watermarks for generative LLMs have recently attracted much attention. An important aspect of such watermarks is the trade-off between their identifiability and their impact on the quality of the generated text. This paper introduces a systematic approach to this trade-off in terms of a multi-objective optimization problem. For a large class of robust, efficient watermarks, the associated Pareto optimal solutions are identified and shown to outperform existing robust, efficient watermarks",
    "checked": true,
    "id": "11fb5bc775a0e6a096ae5891bf472347b3354e23",
    "semantic_title": "optimizing watermarks for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0urN0PnNDj": {
    "title": "PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation",
    "volume": "poster",
    "abstract": "In preference-based Reinforcement Learning (RL), obtaining a large number of preference labels are both time-consuming and costly. Furthermore, the queried human preferences cannot be utilized for the new tasks. In this paper, we propose Zero-shot Cross-task Preference Alignment and Robust Reward Learning (PEARL), which learns policies from cross-task preference transfer without any human labels of the target task. Our contributions include two novel components that facilitate the transfer and learning process. The first is Cross-task Preference Alignment (CPA), which transfers the preferences between tasks via optimal transport. The key idea of CPA is to use Gromov-Wasserstein distance to align the trajectories between tasks, and the solved optimal transport matrix serves as the correspondence between trajectories. The target task preferences are computed as the weighted sum of source task preference labels with the correspondence as weights. Moreover, to ensure robust learning from these transferred labels, we introduce Robust Reward Learning (RRL), which considers both reward mean and uncertainty by modeling rewards as Gaussian distributions. Empirical results on robotic manipulation tasks from Meta-World and Robomimic demonstrate that our method is capable of transferring preference labels across tasks accurately and then learns well-behaved policies. Notably, our approach significantly exceeds existing methods when there are few human preferences. The code and videos of our method are available at: https://sites.google.com/view/pearl-preference",
    "checked": true,
    "id": "3211678bf2f01c5ea28bc67399b956b6273478cb",
    "semantic_title": "pearl: zero-shot cross-task preference alignment and robust reward learning for robotic manipulation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ATvN9JnqZ8": {
    "title": "Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates",
    "volume": "poster",
    "abstract": "Enzymes are genetically encoded biocatalysts capable of accelerating chemical reactions. How can we automatically design functional enzymes? In this paper, we propose EnzyGen, an approach to learn a unified model to design enzymes across all functional families. Our key idea is to generate an enzyme's amino acid sequence and their three-dimensional (3D) coordinates based on functionally important sites and substrates corresponding to a desired catalytic function. These sites are automatically mined from enzyme databases. EnzyGen consists of a novel interleaving network of attention and neighborhood equivariant layers, which captures both long-range correlation in an entire protein sequence and local influence from nearest amino acids in 3D space. To learn the generative model, we devise a joint training objective, including a sequence generation loss, a position prediction loss and an enzyme-substrate interaction loss. We further construct EnzyBench, a dataset with 3157 enzyme families, covering all available enzymes within the protein data bank (PDB). Experimental results show that our EnzyGen consistently achieves the best performance across all 323 testing families, surpassing the best baseline by 10.79% in terms of substrate binding affinity. These findings demonstrate EnzyGen's superior capability in designing well-folded and effective enzymes binding to specific substrates with high affinities. Our code, model and dataset are provided at https://github.com/LeiLiLab/EnzyGen",
    "checked": true,
    "id": "333f4070be5b712cf394d2ef411c837b05a4557b",
    "semantic_title": "generative enzyme design guided by functionally important sites and small-molecule substrates",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tOO6PD3kYP": {
    "title": "Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency",
    "volume": "poster",
    "abstract": "We consider Bayesian optimization using Gaussian Process models, also referred to as kernel-based bandit optimization. We study the methodology of exploring the domain using random samples drawn from a distribution. We show that this random exploration approach achieves the optimal error rates. Our analysis is based on novel concentration bounds in an infinite dimensional Hilbert space established in this work, which may be of independent interest. We further develop an algorithm based on random exploration with domain shrinking and establish its order-optimal regret guarantees under both noise-free and noisy settings. In the noise-free setting, our analysis closes the existing gap in regret performance under a mild assumption on the underlying function and thereby *partially resolves a COLT open problem*. The proposed algorithm also enjoys a computational advantage over prevailing methods due to the random exploration that obviates the expensive optimization of a non-convex acquisition function for choosing the query points at each iteration",
    "checked": true,
    "id": "20c6f78b9c1d3a427d966d5a748f9237eb6572e8",
    "semantic_title": "random exploration in bayesian optimization: order-optimal regret and computational efficiency",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Dk0RBrqiyk": {
    "title": "Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery",
    "volume": "poster",
    "abstract": "We study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\\in [m]\\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\\varepsilon$-optimal policy with probability at least $1-\\delta$ typically scales as $\\frac{m+n}{\\varepsilon^2}\\log(1/\\delta)$. Our regret minimization algorithm enjoys minimax guarantees typically scaling as $r^{5/4}(m+n)^{3/4}\\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they first leverage spectral methods to estimate the left and right singular subspaces of the low-rank reward matrix. We show that these estimates enjoy tight error guarantees in the two-to-infinity norm. This in turn allows us to reformulate our problems as a misspecified linear bandit problem with dimension roughly $r(m+n)$ and misspecification controlled by the subspace recovery error, as well as to design the second phase of our algorithms efficiently",
    "checked": true,
    "id": "c3119e7e151e427255652dbba329e07348f64c24",
    "semantic_title": "low-rank bandits via tight two-to-infinity singular subspace recovery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0j28mmQ023": {
    "title": "Domain-wise Data Acquisition to Improve Performance under Distribution Shift",
    "volume": "poster",
    "abstract": "Despite notable progress in enhancing the capability of machine learning against distribution shifts, training data quality remains a bottleneck for cross-distribution generalization. Recently, from a data-centric perspective, there have been considerable efforts to improve model performance through refining the preparation of training data. Inspired by realistic scenarios, this paper addresses a practical requirement of acquiring training samples from various domains on a limited budget to facilitate model generalization to target test domain with distribution shift. Our empirical evidence indicates that the advance in data acquisition can significantly benefit the model performance on shifted data. Additionally, by leveraging unlabeled test domain data, we introduce a Domain-wise Active Acquisition framework. This framework iteratively optimizes the data acquisition strategy as training samples are accumulated, theoretically ensuring the effective approximation of test distribution. Extensive real-world experiments demonstrate our proposal's advantages in machine learning applications. The code is available at https://github.com/dongbaili/DAA",
    "checked": false,
    "id": "513fc9d2500a0e532ddf17ed7f8ed4d1cfb727af",
    "semantic_title": "generative models improve fairness of medical classifiers under distribution shifts",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=EVMzCKLpdD": {
    "title": "A Geometric Explanation of the Likelihood OOD Detection Paradox",
    "volume": "poster",
    "abstract": "Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a *pre-trained* DGM. Our method can be applied to normalizing flows and score-based diffusion models, and obtains results which match or surpass state-of-the-art OOD detection benchmarks using the same DGM backbones. Our code is available at our [GitHub repository](https://github.com/layer6ai-labs/dgm_ood_detection)",
    "checked": true,
    "id": "f119a2370955d0087b9db32555671f05d6557e1a",
    "semantic_title": "a geometric explanation of the likelihood ood detection paradox",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=9iRGs3wBTy": {
    "title": "Online Linear Regression in Dynamic Environments via Discounting",
    "volume": "poster",
    "abstract": "We develop algorithms for online linear regression which achieve optimal static and dynamic regret guarantees *even in the complete absence of prior knowledge*. We present a novel analysis showing that a discounted variant of the Vovk-Azoury-Warmuth forecaster achieves dynamic regret of the form $R_{T}(\\vec{u})\\le O\\Big(d\\log(T)\\vee \\sqrt{dP_{T}^{\\gamma}(\\vec{u})T}\\Big)$, where $P_{T}^{\\gamma}(\\vec{u})$ is a measure of variability of the comparator sequence, and show that the discount factor achieving this result can be learned on-the-fly. We show that this result is optimal by providing a matching lower bound. We also extend our results to *strongly-adaptive* guarantees which hold over every sub-interval $[a,b]\\subseteq[1,T]$ simultaneously",
    "checked": true,
    "id": "f6fecdb9f4fc8798ff47c983653cc3e3f21b7320",
    "semantic_title": "online linear regression in dynamic environments via discounting",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=24zMewdzyJ": {
    "title": "Risk-Sensitive Policy Optimization via Predictive CVaR Policy Gradient",
    "volume": "poster",
    "abstract": "This paper addresses a policy optimization task with the conditional value-at-risk (CVaR) objective. We introduce the *predictive CVaR policy gradient*, a novel approach that seamlessly integrates risk-neutral policy gradient algorithms with minimal modifications. Our method incorporates a reweighting strategy in gradient calculation -- individual cost terms are reweighted in proportion to their *predicted* contribution to the objective. These weights can be easily estimated through a separate learning procedure. We provide theoretical and empirical analyses, demonstrating the validity and effectiveness of our proposed method",
    "checked": false,
    "id": "2a1b41221def527e17eb1ca04f4f32442fa09ba7",
    "semantic_title": "a risk-sensitive approach to policy optimization",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=44qxX6Ty6F": {
    "title": "Position: Scarce Resource Allocations That Rely On Machine Learning Should Be Randomized",
    "volume": "poster",
    "abstract": "Contrary to traditional deterministic notions of algorithmic fairness, this paper argues that fairly allocating scarce resources using machine learning often requires randomness. We address why, when, and how to randomize by offering a set of stochastic procedures that more adequately account for all of the claims individuals have to allocations of social goods or opportunities and effectively balances their interests",
    "checked": false,
    "id": "fedc26fbe6cea0ca91c241c12419b52c3cdc94e4",
    "semantic_title": "scarce resource allocations that rely on machine learning should be randomized",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=sHtIStlg0v": {
    "title": "Large Language Models are Geographically Biased",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
    "checked": true,
    "id": "ce0219aab283c07369187348d72f8c5fe9898e9b",
    "semantic_title": "large language models are geographically biased",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=v0VUsQI5yw": {
    "title": "Smoothness Adaptive Hypothesis Transfer Learning",
    "volume": "poster",
    "abstract": "Many existing two-phase kernel-based hypothesis transfer learning algorithms employ the same kernel regularization across phases and rely on the known smoothness of functions to obtain optimality. Therefore, they fail to adapt to the varying and unknown smoothness between the target/source and their offset. This paper introduces Smoothness Adaptive Transfer Learning (SATL), a two-phase kernel ridge regression (KRR)-based algorithm to address these limitations. We first demonstrate that employing a misspecified fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax optimality when the true function resides in Sobolev spaces. Leveraging this result, SATL enables the estimators to provably and universally adapt to the varying and unknown Sobolev smoothness of the source and offset functions. We derive the minimax lower bound of the learning problem in excess risk and show that SATL achieves a matching upper bound up to logarithmic factors. The optimal statistical rate reveals the factors influencing the transfer dynamics and efficacy, including the source sample size and the relative strength between domains. The theoretical findings and the effectiveness of SATL are confirmed by several experiments",
    "checked": true,
    "id": "f6e2b257cdd7c9d3eccd52cc20f7b517b5e7e4c4",
    "semantic_title": "smoothness adaptive hypothesis transfer learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Z19JQ6WFtJ": {
    "title": "Learning Reward for Robot Skills Using Large Language Models via Self-Alignment",
    "volume": "poster",
    "abstract": "Learning reward functions remains the bottleneck to equip a robot with a broad repertoire of skills. Large Language Models (LLM) contain valuable task-related knowledge that can potentially aid in the learning of reward functions. However, the proposed reward function can be imprecise, thus ineffective which requires to be further grounded with environment information. We proposed a method to learn rewards more efficiently in the absence of humans. Our approach consists of two components: We first use the LLM to propose features and parameterization of the reward, then update the parameters through an iterative self-alignment process. In particular, the process minimizes the ranking inconsistency between the LLM and the learnt reward functions based on the execution feedback. The method was validated on 9 tasks across 2 simulation environments. It demonstrates a consistent improvement in training efficacy and efficiency, meanwhile consuming significantly fewer GPT tokens compared to the alternative mutation-based method",
    "checked": true,
    "id": "83331243fcaeab4d5c02145d62f3d42df2c2d651",
    "semantic_title": "learning reward for robot skills using large language models via self-alignment",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=2rPoTgEmjV": {
    "title": "Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive and Masked Pretraining",
    "volume": "poster",
    "abstract": "In recent years, the rise of generative self-supervised learning (SSL) paradigms has exhibited impressive performance across visual, language, and multi-modal domains. While the varied designs of generative SSL objectives lead to distinct properties in downstream tasks, a theoretical understanding of these differences remains largely unexplored. In this paper, we establish the first theoretical comparisons between two leading generative SSL paradigms: autoregressive SSL and masked SSL. Through establishing theoretical frameworks, we elucidate the strengths and limitations of autoregressive and masked SSL within the primary evaluation tasks of classification and content generation. Our findings demonstrate that in classification tasks, the flexibility of targeted tokens in masked SSL fosters more inter-sample connections compared to the fixed position of target tokens in autoregressive SSL, which yields superior clustering performance. In content generation tasks, the misalignment between the flexible lengths of test samples and the fixed length of unmasked texts in masked SSL (vs. flexible lengths of conditional texts in autoregressive SSL) hinders its generation performance. To leverage each other's strengths and mitigate weaknesses, we propose diversity-enhanced autoregressive and variable-length masked objectives, which substantially improve the classification performance of autoregressive SSL and the generation performance of masked SSL. Code is available at https://github.com/PKU-ML/LookAheadLookAround",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bID9PiBFpT": {
    "title": "Policy Evaluation for Variance in Average Reward Reinforcement Learning",
    "volume": "poster",
    "abstract": "We consider an average reward reinforcement learning (RL) problem and work with asymptotic variance as a risk measure to model safety-critical applications. We design a temporal-difference (TD) type algorithm tailored for policy evaluation in this context. Our algorithm is based on linear stochastic approximation of an equivalent formulation of the asymptotic variance in terms of the solution of the Poisson equation. We consider both the tabular and linear function approximation settings, and establish $\\tilde {O}(1/k)$ finite time convergence rate, where $k$ is the number of steps of the algorithm. Our work paves the way for developing actor-critic style algorithms for variance-constrained RL. To the best of our knowledge, our result provides the first sequential estimator for asymptotic variance of a Markov chain with provable finite sample guarantees, which is of independent interest",
    "checked": false,
    "id": "595a000ef8e76c064f1d4055e03c895796565477",
    "semantic_title": "digital twin of a driver-in-the-loop race car simulation with contextual reinforcement learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=JNHK11bAGl": {
    "title": "Feasibility Consistent Representation Learning for Safe Reinforcement Learning",
    "volume": "poster",
    "abstract": "In the field of safe reinforcement learning (RL), finding a balance between satisfying safety constraints and optimizing reward performance presents a significant challenge. A key obstacle in this endeavor is the estimation of safety constraints, which is typically more difficult than estimating a reward metric due to the sparse nature of the constraint signals. To address this issue, we introduce a novel framework named Feasibility Consistent Safe Reinforcement Learning (FCSRL). This framework combines representation learning with feasibility-oriented objectives to identify and extract safety-related information from the raw state for safe RL. Leveraging self-supervised learning techniques and a more learnable safety metric, our approach enhances the policy learning and constraint estimation. Empirical evaluations across a range of vector-state and image-based tasks demonstrate that our method is capable of learning a better safety-aware embedding and achieving superior performance than previous representation learning baselines",
    "checked": true,
    "id": "f23bd924fd676d2f45d26f71ccd9498b6f53dce8",
    "semantic_title": "feasibility consistent representation learning for safe reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6VZOONPn8S": {
    "title": "Disparate Impact on Group Accuracy of Linearization for Private Inference",
    "volume": "poster",
    "abstract": "Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the finetuning step for linearized models can serve as an effective mitigation strategy",
    "checked": true,
    "id": "18a226b794e89d3cf8d93734724b42275c9c4c0d",
    "semantic_title": "disparate impact on group accuracy of linearization for private inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DiyE6OOGBa": {
    "title": "GenCO: Generating Diverse Designs with Combinatorial Constraints",
    "volume": "poster",
    "abstract": "Deep generative models like GAN and VAE have shown impressive results in generating unconstrained objects like images. However, many design settings arising in industrial design, material science, computer graphics and more require that the generated objects satisfy hard combinatorial constraints or meet objectives in addition to modeling a data distribution. To address this, we propose GenCO, a generative framework that guarantees constraint satisfaction throughout training by leveraging differentiable combinatorial solvers to enforce feasibility. GenCO imposes the generative loss on provably feasible solutions rather than intermediate soft solutions, meaning that the deep generative network can focus on ensuring the generated objects match the data distribution without having to also capture feasibility. This shift enables practitioners to enforce hard constraints on the generated outputs during end-to-end training, enabling assessments of their feasibility and introducing additional combinatorial loss components to deep generative training. We demonstrate the effectiveness of our approach on a variety of generative combinatorial tasks, including game level generation, map creation for path planning, and photonic device design, consistently demonstrating its capability to yield diverse, high-quality solutions that verifiably adhere to user-specified combinatorial properties",
    "checked": true,
    "id": "ccd0fee6d65e545d57afa550447d4b0e79459da8",
    "semantic_title": "genco: generating diverse designs with combinatorial constraints",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ScRhEuj480": {
    "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
    "volume": "poster",
    "abstract": "In dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric _corrector network_ that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring \"hard negatives.\" We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2FHWFG5ahw": {
    "title": "Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich Differentiable Simulation",
    "volume": "poster",
    "abstract": "Model-Free Reinforcement Learning (MFRL), leveraging the policy gradient theorem, has demonstrated considerable success in continuous control tasks. However, these approaches are plagued by high gradient variance due to zeroth-order gradient estimation, resulting in suboptimal policies. Conversely, First-Order Model-Based Reinforcement Learning (FO-MBRL) methods employing differentiable simulation provide gradients with reduced variance but are susceptible to sampling error in scenarios involving stiff dynamics, such as physical contact. This paper investigates the source of this error and introduces Adaptive Horizon Actor-Critic (AHAC), an FO-MBRL algorithm that reduces gradient error by adapting the model-based horizon to avoid stiff dynamics. Empirical findings reveal that AHAC outperforms MFRL baselines, attaining 40% more reward across a set of locomotion tasks and efficiently scaling to high-dimensional control environments with improved wall-clock-time efficiency. [adaptive-horizon-actor-critic.github.io](https://adaptive-horizon-actor-critic.github.io/)",
    "checked": true,
    "id": "5c97fb626bfa0ebc924eab30e588e3f4f2447b18",
    "semantic_title": "adaptive horizon actor-critic for policy learning in contact-rich differentiable simulation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TejqrQBvll": {
    "title": "Generalization Bounds for Causal Regression: Insights, Guarantees and Sensitivity Analysis",
    "volume": "poster",
    "abstract": "Many algorithms have been recently proposed for causal machine learning. Yet, there is little to no theory on their quality, especially considering finite samples. In this work, we propose a theory based on generalization bounds that provides such guarantees. By introducing a novel change-of-measure inequality, we are able to tightly bound the model loss in terms of the deviation of the treatment propensities over the population, which we show can be empirically limited. Our theory is fully rigorous and holds even in the face of hidden confounding and violations of positivity. We demonstrate our bounds on semi-synthetic and real data, showcasing their remarkable tightness and practical utility",
    "checked": true,
    "id": "626b0f3d18a8ac1de403e02542f118347c2e4504",
    "semantic_title": "generalization bounds for causal regression: insights, guarantees and sensitivity analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DyvhD8J3Wl": {
    "title": "Benign Overfitting in Adversarial Training of Neural Networks",
    "volume": "poster",
    "abstract": "Benign overfitting is the phenomenon wherein none of the predictors in the hypothesis class can achieve perfect accuracy (i.e., non-realizable or noisy setting), but a model that interpolates the training data still achieves good generalization. A series of recent works aim to understand this phenomenon for regression and classification tasks using linear predictors as well as two-layer neural networks. In this paper, we study such a benign overfitting phenomenon in an adversarial setting. We show that under a distributional assumption, interpolating neural networks found using adversarial training generalize well despite inference-time attacks. Specifically, we provide convergence and generalization guarantees for adversarial training of two-layer networks (with smooth as well as non-smooth activation functions) showing that under moderate $\\ell_2$ norm perturbation budget, the trained model has near-zero robust training loss and near-optimal robust generalization error. We support our theoretical findings with an empirical study on synthetic and real-world data",
    "checked": false,
    "id": "7312f2a342299cc46f5380ab02a8aba78daadea0",
    "semantic_title": "the surprising harmfulness of benign overfitting for adversarial robustness",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kpDd2HCBka": {
    "title": "Efficient Policy Evaluation with Offline Data Informed Behavior Policy Design",
    "volume": "poster",
    "abstract": "Most reinforcement learning practitioners evaluate their policies with online Monte Carlo estimators for either hyperparameter tuning or testing different algorithmic design choices, where the policy is repeatedly executed in the environment to get the average outcome. Such massive interactions with the environment are prohibitive in many scenarios. In this paper, we propose novel methods that improve the data efficiency of online Monte Carlo estimators while maintaining their unbiasedness. We first propose a tailored closed-form behavior policy that provably reduces the variance of an online Monte Carlo estimator. We then design efficient algorithms to learn this closed-form behavior policy from previously collected offline data. Theoretical analysis is provided to characterize how the behavior policy learning error affects the amount of reduced variance. Compared with previous works, our method achieves better empirical performance in a broader set of environments, with fewer requirements for offline data",
    "checked": true,
    "id": "55e7ddbff1ac24e3dbc3fb36bb2163dbc6179044",
    "semantic_title": "efficient policy evaluation with offline data informed behavior policy design",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oiY7yhyi6W": {
    "title": "Off-policy Evaluation Beyond Overlap: Sharp Partial Identification Under Smoothness",
    "volume": "poster",
    "abstract": "Off-policy evaluation, and the complementary problem of policy learning, use historical data collected under a logging policy to estimate and/or optimize the value of a target policy. Methods for these tasks typically assume overlap between the target and logging policy, enabling solutions based on importance weighting and/or imputation. Absent such an overlap assumption, existing work either relies on a well-specified model or optimizes needlessly conservative bounds. In this work, we develop methods for no-overlap policy evaluation without a well-specified model, relying instead on non-parametric assumptions on the expected outcome, with a particular focus on Lipschitz smoothness. Under such assumptions we are able to provide sharp bounds on the off-policy value, along with optimal estimators of those bounds. For Lipschitz smoothness, we construct a pair of linear programs that upper and lower bound the contribution of the no-overlap region to the off-policy value. We show that these programs have a concise closed form solution, and that their solutions converge under the Lipschitz assumption to the sharp partial identification bounds at a minimax optimal rate, up to log factors. We demonstrate the effectiveness our methods on two semi-synthetic examples, and obtain informative and valid bounds that are tighter than those possible without smoothness assumptions",
    "checked": false,
    "id": "d186f3787e1679211eee8b9b8b37f9228c063764",
    "semantic_title": "off-policy evaluation beyond overlap: partial identification through smoothness",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=iYYA5zDoCm": {
    "title": "Locally Interdependent Multi-Agent MDP: Theoretical Framework for Decentralized Agents with Dynamic Dependencies",
    "volume": "poster",
    "abstract": "Many multi-agent systems in practice are decentralized and have dynamically varying dependencies. There has been a lack of attempts in the literature to analyze these systems theoretically. In this paper, we propose and theoretically analyze a decentralized model with dynamically varying dependencies called the Locally Interdependent Multi-Agent MDP. This model can represent problems in many disparate domains such as cooperative navigation, obstacle avoidance, and formation control. Despite the intractability that general partially observable multi-agent systems suffer from, we propose three closed-form policies that are theoretically near-optimal in this setting and can be scalable to compute and store. Consequentially, we reveal a fundamental property of Locally Interdependent Multi-Agent MDP's that the partially observable decentralized solution is exponentially close to the fully observable solution with respect to the visibility radius. We then discuss extensions of our closed-form policies to further improve tractability. We conclude by providing simulations to investigate some long horizon behaviors of our closed-form policies",
    "checked": true,
    "id": "da8093d677760ec4837f68a413ac7a76c7ca2d5b",
    "semantic_title": "locally interdependent multi-agent mdp: theoretical framework for decentralized agents with dynamic dependencies",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z7MzVDFWDV": {
    "title": "Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach",
    "volume": "poster",
    "abstract": "We consider the task of identifying and estimating a parameter of interest in settings where data is missing not at random (MNAR). In general, such parameters are not identified without strong assumptions on the missing data model. In this paper, we take an alternative approach and introduce a method inspired by data fusion, where information in the MNAR dataset is augmented by information in an auxiliary dataset subject to missingness at random (MAR). We show that even if the parameter of interest cannot be identified given either dataset alone, it can be identified given pooled data, under two complementary sets of assumptions. We derive inverse probability weighted (IPW) estimators for identified parameters under both sets of assumptions, and evaluate the performance of our estimation strategies via simulation studies, and a data application",
    "checked": true,
    "id": "c65bc9c4a514ff48d056da10602ef605f8dd84f6",
    "semantic_title": "identification and estimation for nonignorable missing data: a data fusion approach",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L1W9ZWPq9E": {
    "title": "Debiased Distribution Compression",
    "volume": "poster",
    "abstract": "Modern compression methods can summarize a target distribution $\\mathbb{P}$ more succinctly than i.i.d. sampling but require access to a low-bias input sequence like a Markov chain converging quickly to $\\mathbb{P}$. We introduce a new suite of compression methods suitable for compression with biased input sequences. Given $n$ points targeting the wrong distribution and quadratic time, Stein kernel thinning (SKT) returns $\\sqrt{n}$ equal-weighted points with $\\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\\mathbb{P}$. For larger-scale compression tasks, low-rank SKT achieves the same feat in sub-quadratic time using an adaptive low-rank debiasing procedure that may be of independent interest. For downstream tasks that support simplex or constant-preserving weights, Stein recombination and Stein Cholesky achieve even greater parsimony, matching the guarantees of SKT with as few as $\\text{poly-log}(n)$ weighted points. Underlying these advances are new guarantees for the quality of simplex-weighted coresets, the spectral decay of kernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In our experiments, our techniques provide succinct and accurate posterior summaries while overcoming biases due to burn-in, approximate Markov chain Monte Carlo, and tempering",
    "checked": true,
    "id": "f8c93997241ed1aaeed1f602e86b4bab7362dbc1",
    "semantic_title": "debiased distribution compression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=reB9FFAaKw": {
    "title": "SaVeR: Optimal Data Collection Strategy for Safe Policy Evaluation in Tabular MDP",
    "volume": "poster",
    "abstract": "In this paper, we study safe data collection for the purpose of policy evaluation in tabular Markov decision processes (MDPs). In policy evaluation, we are given a target policy and asked to estimate the expected cumulative reward it will obtain. Policy evaluation requires data and we are interested in the question of what *behavior* policy should collect the data for the most accurate evaluation of the target policy. While prior work has considered behavior policy selection, in this paper, we additionally consider a safety constraint on the behavior policy. Namely, we assume there exists a known default policy that incurs a particular expected cost when run and we enforce that the cumulative cost of all behavior policies ran is better than a constant factor of the cost that would be incurred had we always run the default policy. We first show that there exists a class of intractable MDPs where no safe oracle algorithm with knowledge about problem parameters can efficiently collect data and satisfy the safety constraints. We then define the tractability condition for an MDP such that a safe oracle algorithm can efficiently collect data and using that we prove the first lower bound for this setting. We then introduce an algorithm SaVeR for this problem that approximates the safe oracle algorithm and bound the finite-sample mean squared error of the algorithm while ensuring it satisfies the safety constraint. Finally, we show in simulations that SaVeR produces low MSE policy evaluation while satisfying the safety constraint",
    "checked": true,
    "id": "efeba0d15befb4c32f64c6996047b678f74734af",
    "semantic_title": "saver: optimal data collection strategy for safe policy evaluation in tabular mdp",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vn92qYjL1F": {
    "title": "Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection",
    "volume": "poster",
    "abstract": "Both transduction and rejection have emerged as important techniques for defending against adversarial perturbations. A recent work by Goldwasser et. al showed that rejection combined with transduction can give *provable* guarantees (for certain problems) that cannot be achieved otherwise. Nevertheless, under recent strong adversarial attacks (GMSA), Goldwasser et al.'s work was shown to have low performance in a practical deep-learning setting. In this paper, we take a step towards realizing the promise of transduction+rejection in more realistic scenarios. Our key observation is that a novel application of a reduction technique by Tramèr, which was until now only used to demonstrate the vulnerability of certain defenses, can be used to actually construct effective defenses. Theoretically, we show that a careful application of this technique in the transductive setting can give significantly improved sample-complexity for robust generalization. Our theory guides us to design a new transductive algorithm for learning a selective model; extensive experiments using state of the art attacks (AutoAttack, GMSA) show that our approach provides significantly better robust accuracy (81.6% on CIFAR-10 and 57.9% on CIFAR-100 under $l_\\infty$ with budget 8/255) than existing techniques. The implementation is available at https://github.com/nilspalumbo/transduction-rejection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GxOFM3f5Vm": {
    "title": "EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence",
    "volume": "poster",
    "abstract": "A key challenge in contrastive learning is to generate negative samples from a large sample set to contrast with positive samples, for learning better encoding of the data. These negative samples often follow a softmax distribution which are dynamically updated during the training process. However, sampling from this distribution is non-trivial due to the high computational costs in computing the partition function. In this paper, we propose an $\\underline{\\text{E}}$fficient $\\underline{\\text{M}}$arkov $\\underline{\\text{C}}$hain Monte Carlo negative sampling method for $\\underline{\\text{C}}$ontrastive learning (EMC$^2$). We follow the global contrastive learning loss as introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive Metropolis-Hastings subroutine to generate hardness-aware negative samples in an online fashion during the optimization. We prove that EMC$^2$ finds an $\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in $T$ iterations. Compared to prior works, EMC$^2$ is the first algorithm that exhibits global convergence (to stationarity) regardless of the choice of batch size while exhibiting low computation and memory cost. Numerical experiments validate that EMC$^2$ is effective with small batch training and achieves comparable or better performance than baseline algorithms. We report the results for pre-training image encoders on STL-10 and Imagenet-100",
    "checked": false,
    "id": "8a3f0624a0615b1b7af58de8efb3955427a356a1",
    "semantic_title": "emc2: efficient mcmc negative sampling for contrastive learning with global convergence",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MikandLqtW": {
    "title": "Knowledge-aware Reinforced Language Models for Protein Directed Evolution",
    "volume": "poster",
    "abstract": "Directed evolution, a cornerstone of protein optimization, is to harness natural mutational processes to enhance protein functionality. Existing Machine Learning-assisted Directed Evolution (MLDE) methodologies typically rely on data-driven strategies and often overlook the profound domain knowledge in biochemical fields. In this paper, we introduce a novel Knowledge-aware Reinforced Language Model (KnowRLM) for MLDE. An Amino Acid Knowledge Graph (AAKG) is constructed to represent the intricate biochemical relationships among amino acids. We further propose a Protein Language Model (PLM)-based policy network that iteratively samples mutants through preferential random walks on the AAKG using a dynamic sliding window mechanism. The novel mutants are actively sampled to fine-tune a fitness predictor as the reward model, providing feedback to the knowledge-aware policy. Finally, we optimize the whole system in an active learning approach that mimics biological settings in practice.KnowRLM stands out for its ability to utilize contextual amino acid information from knowledge graphs, thus attaining advantages from both statistical patterns of protein sequences and biochemical properties of amino acids.Extensive experiments demonstrate the superior performance of KnowRLM in more efficiently identifying high-fitness mutants compared to existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Zgjrowepn": {
    "title": "Fair Federated Learning via the Proportional Veto Core",
    "volume": "poster",
    "abstract": "Previous work on fairness in federated learning introduced the notion of *core stability*, which provides utility-based fairness guarantees to any subset of participating agents. However, these guarantees require strong assumptions on agent utilities that render them impractical. To address this shortcoming, we measure the quality of output models in terms of their ordinal *rank* instead of their cardinal utility, and use this insight to adapt the classical notion of *proportional veto core (PVC)* from social choice theory to the federated learning setting. We prove that models that are *PVC-stable* exist in very general learning paradigms, even allowing non-convex model sets, as well as non-convex and non-concave loss functions. We also design Rank-Core-Fed, a distributed federated learning algorithm, to train a PVC-stable model. Finally, we demonstrate that Rank-Core-Fed outperforms baselines in terms of fairness on different datasets",
    "checked": false,
    "id": "e9f83e80b33896ff455b45d7498ec23451de5339",
    "semantic_title": "fairness in federated learning via core-stability",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=MQirNNU2pC": {
    "title": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks",
    "volume": "poster",
    "abstract": "This study investigates how weight decay affects the update behavior of individual neurons in deep neural networks through a combination of applied analysis and experimentation. Weight decay can cause the expected magnitude and angular updates of a neuron's weight vector to converge to a steady state we call rotational equilibrium. These states can be highly homogeneous, effectively balancing the average rotation---a proxy for the effective learning rate---across different layers and neurons. Our work analyzes these dynamics across optimizers like Adam, Lion, and SGD with momentum, offering a new simple perspective on training that elucidates the efficacy of widely used but poorly understood methods in deep learning. We demonstrate how balanced rotation plays a key role in the effectiveness of normalization like Weight Standardization, as well as that of AdamW over Adam with L2-regularization. Finally, we show that explicitly controlling the rotation provides the benefits of weight decay while substantially reducing the need for learning rate warmup",
    "checked": true,
    "id": "8df063d355e17d2f958d9d4a83068f16447156b7",
    "semantic_title": "rotational equilibrium: how weight decay balances learning across neural networks",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Kqa5JakTjB": {
    "title": "Federated Continual Learning via Prompt-based Dual Knowledge Transfer",
    "volume": "poster",
    "abstract": "In Federated Continual Learning (FCL), the challenge lies in effectively facilitating knowledge transfer and enhancing the performance across various tasks on different clients. Current FCL methods predominantly focus on avoiding interference between tasks, thereby overlooking the potential for positive knowledge transfer across tasks learned by different clients at separate time intervals. To address this issue, we introduce a **P**rompt-based kn**ow**le**d**ge transf**er** FCL algorithm, called **Powder**, designed to effectively foster the transfer of knowledge encapsulated in prompts between various sequentially learned tasks and clients. Furthermore, we have devised a unique approach for prompt generation and aggregation, intending to alleviate privacy protection concerns and communication overhead, while still promoting knowledge transfer. Comprehensive experimental results demonstrate the superiority of our method in terms of reduction in communication costs, and enhancement of knowledge transfer. Code is available at https://github.com/piaohongming/Powder",
    "checked": false,
    "id": "8c2a8b876d7d2ed3df7b520d4310287303fafc1e",
    "semantic_title": "continual offline reinforcement learning via diffusion-based dual generative replay",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SERrqPDvoY": {
    "title": "ESNet: Evolution and Succession Network for High-Resolution Salient Object Detection",
    "volume": "poster",
    "abstract": "Preserving details and avoiding high computational costs are the two main challenges for the High-Resolution Salient Object Detection (HRSOD) task. In this paper, we propose a two-stage HRSOD model from the perspective of evolution and succession, including an evolution stage with Low-resolution Location Model (LrLM) and a succession stage with High-resolution Refinement Model (HrRM). The evolution stage achieves detail-preserving salient objects localization on the low-resolution image through the evolution mechanisms on supervision and feature; the succession stage utilizes the shallow high-resolution features to complement and enhance the features inherited from the first stage in a lightweight manner and generate the final high-resolution saliency prediction. Besides, a new metric named Boundary-Detail-aware Mean Absolute Error (${MAE}_{{BD}}$) is designed to evaluate the ability to detect details in high-resolution scenes. Extensive experiments on five datasets demonstrate that our network achieves superior performance at real-time speed (49 FPS) compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CQH63IbI5o": {
    "title": "Interacting Diffusion Processes for Event Sequence Forecasting",
    "volume": "poster",
    "abstract": "Neural Temporal Point Processes (TPPs) have emerged as the primary framework for predicting sequences of events that occur at irregular time intervals, but their sequential nature can hamper performance for long-horizon forecasts. To address this, we introduce a novel approach that incorporates a diffusion generative model. The model facilitates sequence-to-sequence prediction, allowing multi-step predictions based on historical event sequences. In contrast to previous approaches, our model directly learns the joint probability distribution of types and inter-arrival times for multiple events. The model is composed of two diffusion processes, one for the time intervals and one for the event types. These processes interact through their respective denoising functions, which can take as input intermediate representations from both processes, allowing the model to learn complex interactions. We demonstrate that our proposal outperforms state-of-the-art baselines for long-horizon forecasting of TPPs",
    "checked": true,
    "id": "d8dffd9cb0b72c20a997e0e2b6edec6af80ca50d",
    "semantic_title": "interacting diffusion processes for event sequence forecasting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nuW5iBAJS": {
    "title": "Unveiling the Potential of AI for Nanomaterial Morphology Prediction",
    "volume": "poster",
    "abstract": "Creation of nanomaterials with specific morphology remains a complex experimental process, even though there is a growing demand for these materials in various industry sectors. This study explores the potential of AI to predict the morphology of nanoparticles within the data availability constraints. For that, we first generated a new multi-modal dataset that is double the size of analogous studies. Then, we systematically evaluated performance of classical machine learning and large language models in prediction of nanomaterial shapes and sizes. Finally, we prototyped a text-to-image system, discussed the obtained empirical results, as well as the limitations and promises of existing approaches",
    "checked": true,
    "id": "137da047009dbab4ee10dac6e5b1e8c970489a4c",
    "semantic_title": "unveiling the potential of ai for nanomaterial morphology prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FovMAzXUpj": {
    "title": "ReGAL: Refactoring Programs to Discover Generalizable Abstractions",
    "volume": "poster",
    "abstract": "While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e., restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On five datasets – LOGO graphics generation, Date reasoning, TextCraft (a Minecraft-based text-game) MATH, and TabMWP – both open-source and proprietary LLMs improve in accuracy when predicting programs with REGAL functions. For CodeLlama-13B, REGAL results in absolute accuracy increases of 11.5% on LOGO, 26.1% on date understanding, and 8.1% on TextCraft, out-performing GPT-3.5 in two of three domains. Our analysis reveals REGAL's abstractions encapsulate frequently-used subroutines as well as environment dynamics",
    "checked": true,
    "id": "94701d9c6cfc1aecc174ff62ccda939f790c1710",
    "semantic_title": "regal: refactoring programs to discover generalizable abstractions",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=GentO2E4ID": {
    "title": "A Doubly Recursive Stochastic Compositional Gradient Descent Method for Federated Multi-Level Compositional Optimization",
    "volume": "poster",
    "abstract": "Federated compositional optimization has been actively studied in the past few years. However, existing methods mainly focus on the two-level compositional optimization problem, which cannot be directly applied to the multi-level counterparts. Moreover, the convergence rate of existing federated two-level compositional optimization learning algorithms fails to achieve linear speedup with respect to the number of workers under heterogeneous settings. After identifying the reason for this failure, we developed a novel federated stochastic multi-level compositional optimization algorithm by introducing a novel Jacobian-vector product estimator. This innovation mitigates both the heterogeneity issue and the communication efficiency issue simultaneously. We then theoretically proved that our algorithm can achieve the level-independent and linear speedup convergence rate for nonconvex problems. To our knowledge, this is the first time that a federated learning algorithm can achieve such a favorable convergence rate for multi-level compositional problems. Moreover, experimental results confirm the efficacy of our algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SKPhvzxO1g": {
    "title": "Benchmarking Deletion Metrics with the Principled Explanations",
    "volume": "poster",
    "abstract": "Insertion/deletion metrics and their variants have been extensively applied to evaluate attribution-based explanation methods. Such metrics measure the significance of features by observing changes in model predictions as features are incrementally inserted or deleted. Given the direct connection between the attribution values and model predictions that insertion/deletion metrics enable, they are commonly used as the decisive metrics for novel attribution methods. Such influential metrics for explanation methods should be handled with great scrutiny. However, contemporary research on insertion/deletion metrics falls short of a comprehensive analysis. To address this, we propose the TRAjectory importanCE (TRACE) framework, which achieves the best scores of the insertion/deletion metric. Our contribution includes two aspects: 1) TRACE stands as the principled explanation for explaining the influence of feature deletion on model predictions. We demonstrate that TRACE is guaranteed to achieve almost optimal results both theoretically and empirically. 2) Using TRACE, we benchmark insertion/deletion metrics across all possible settings and study critical problems such as the out-of-distribution (OOD) issue, and provide practical guidance on applying these metrics in practice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wz4lgc8dsN": {
    "title": "Online Cascade Learning for Efficient Inference over Streams",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose *online cascade learning*, the first approach to address this challenge. The objective here is to learn a ``cascade'' of models, starting with lower-capacity models (such as logistic regression) and ending with a powerful LLM, along with a *deferral policy* that determines the model to be used on a given input. We formulate the task of learning cascades online as an imitation-learning problem, where smaller models are updated over time imitating the collected LLM demonstrations, and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90% with strong robustness against input distribution shifts, underscoring its efficacy and adaptability in stream processing. Our source code is available at https://github.com/flitternie/online_cascade_learning",
    "checked": true,
    "id": "d3093494024474e8858bbf723d303b6913d9e3c4",
    "semantic_title": "online cascade learning for efficient inference over streams",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9BrydUVcoe": {
    "title": "QuIP$\\#$: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks",
    "volume": "poster",
    "abstract": "Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves QuIP's (Chee et al., 2023) incoherence processing by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference. Our code can be found at https://github.com/Cornell-RelaxML/quip-sharp",
    "checked": false,
    "id": "8fbf2eb4587b5c271979c3f96eee1b109496143e",
    "semantic_title": "quip#: even better llm quantization with hadamard incoherence and lattice codebooks",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=LbEB39lZqp": {
    "title": "USTAD: Unified Single-model Training Achieving Diverse Scores for Information Retrieval",
    "volume": "poster",
    "abstract": "Modern information retrieval (IR) systems consists of multiple stages like retrieval and ranking, with Transformer-based models achieving state-of-the-art performance at each stage. In this paper, we challenge the tradition of using separate models for different stages and ask if a single Transformer encoder can provide relevance score needed in each stage. We present USTAD – a new unified approach to train a single network that can provide powerful ranking scores as a cross-encoder (CE) model as well as factorized embeddings for large-scale retrieval as a dual-encoder (DE) model. Empirically, we find a single USTAD model to be competitive to separate ranking CE and retrieval DE models. Furthermore, USTAD combines well with a novel embedding matching-based distillation, significantly improving CE to DE distillation. It further motivates novel asymmetric architectures for student models to ensure a better embedding alignment between the student and the teacher while ensuring small online inference cost. On standard benchmarks like MSMARCO, we demonstrate that USTAD with our proposed distillation method leads to asymmetric students with only 1/10th trainable parameter but retaining 95-97% of the teacher performance",
    "checked": false,
    "id": "fb829391ad0ee0bc3216352ffc0ecb0c0894a028",
    "semantic_title": "robust visual place recognition for severe appearance changes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r0qcGcFL4U": {
    "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization",
    "volume": "poster",
    "abstract": "Recently, there has been a widespread proliferation of \"expert\" language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose $\\textbf{P}$ost-$\\textbf{H}$oc $\\textbf{A}$daptive $\\textbf{T}$okenwise $\\textbf{G}$ating $\\textbf{O}$ver an $\\textbf{O}$cean of $\\textbf{S}$pecialized $\\textbf{E}$xperts (**PHATGOOSE**), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is *post-hoc* - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range of specialized model collections and zero-shot generalization benchmarks, we find that PHATGOOSE outperforms past methods for post-hoc routing and, in some cases, outperforms explicit multitask training (which requires simultaneous data access). To better understand the routing strategy learned by PHATGOOSE, we perform qualitative experiments to validate that PHATGOOSE's performance stems from its ability to make adaptive per-token and per-module expert choices",
    "checked": true,
    "id": "036d3ac1c22ce7058ae74d82b445290c8849561d",
    "semantic_title": "learning to route among specialized experts for zero-shot generalization",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=cUMOVfOIve": {
    "title": "SIN: Selective and Interpretable Normalization for Long-Term Time Series Forecasting",
    "volume": "poster",
    "abstract": "In real-world applications, time series data frequently exhibit non-stationarity, with statistics changing over time. This variability undermines the forecasting accuracy of deep learning models that are trained on historical data but deployed for future prediction. A common approach to mitigate this issue involves normalizing the data to counteract statistical drift, followed by denormalization on the prediction. However, existing methods often employ heuristic normalization techniques that do not fully account for the unique characteristics of the series. Our paper addresses the critical question in this context: which statistics should be removed and restored? We argue that the statistics selected for normalization should exhibit both local invariance and global variability to ensure their correctness and helpfulness. To this end, we propose the Selective and Interpretable Normalization methodology, dubbed SIN. This approach maximizes the covariance between a given look-back window and its subsequent future values, thereby identifying key statistics for normalization and simultaneously learning the corresponding normalization transformations. The interpretable framework can be used to explain the success and limitations of some popular normalization methods. By integrating SIN, we demonstrate improvements in the performance of several prevalent forecasting models, thereby validating the utility of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WjvEvYTy3w": {
    "title": "Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control",
    "volume": "poster",
    "abstract": "Learning a universal policy across different robot morphologies can significantly improve learning efficiency and enable zero-shot generalization to unseen morphologies. However, learning a highly performant universal policy requires sophisticated architectures like transformers (TF) that have larger memory and computational cost than simpler multi-layer perceptrons (MLP). To achieve both good performance like TF and high efficiency like MLP at inference time, we propose HyperDistill, which consists of: (1) A morphology-conditioned hypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy distillation approach that is essential for successful training. We show that on UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill performs as well as a universal TF teacher policy on both training and unseen test robots, but reduces model size by 6-14 times, and computational cost by 67-160 times in different environments. Our analysis attributes the efficiency advantage of HyperDistill at inference time to knowledge decoupling, i.e., the ability to decouple inter-task and intra-task knowledge, a general principle that could also be applied to improve inference efficiency in other domains. The code is publicly available at https://github.com/MasterXiong/Universal-Morphology-Control",
    "checked": true,
    "id": "b7df9c5f7627eaf49ee58022c29416d1e43fb8dd",
    "semantic_title": "distilling morphology-conditioned hypernetworks for efficient universal morphology control",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kn2xp8UOvQ": {
    "title": "Monotone, Bi-Lipschitz, and Polyak-Łojasiewicz Networks",
    "volume": "poster",
    "abstract": "This paper presents a new *bi-Lipschitz* invertible neural network, the BiLipNet, which has the ability to smoothly control both its *Lipschitzness* (output sensitivity to input perturbations) and *inverse Lipschitzness* (input distinguishability from different outputs). The second main contribution is a new scalar-output network, the PLNet, which is a composition of a BiLipNet and a quadratic potential. We show that PLNet satisfies the Polyak-Łojasiewicz condition and can be applied to learn non-convex surrogate losses with a unique and efficiently-computable global minimum. The central technical element in these networks is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build the BiLipNet. The certification of these properties is based on incremental quadratic constraints, resulting in much tighter bounds than can be achieved with spectral normalization. Moreover, we formulate the calculation of the inverse of a BiLipNet -- and hence the minimum of a PLNet -- as a series of three-operator splitting problems, for which fast algorithms can be applied",
    "checked": true,
    "id": "799257d189bee130a3452d93812bcd007148c12e",
    "semantic_title": "monotone, bi-lipschitz, and polyak-łojasiewicz networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dSrdnhLS2h": {
    "title": "Adaptive Observation Cost Control for Variational Quantum Eigensolvers",
    "volume": "poster",
    "abstract": "The objective to be minimized in the variational quantum eigensolver (VQE) has a restricted form, which allows a specialized sequential minimal optimization (SMO) that requires only a few observations in each iteration. However, the SMO iteration is still costly due to the observation noise---one *observation* at a point typically requires averaging over hundreds to thousands of repeated quantum *measurement shots* for achieving a reasonable noise level. In this paper, we propose an adaptive cost control method, named *subspace in confident region* (SubsCoRe), for SMO. SubsCoRe uses the Gaussian process (GP) surrogate, and requires it to have low uncertainty over the subspace being updated, so that optimization in each iteration is performed with guaranteed accuracy. Adaptive cost control is performed by setting the required accuracy according to the progress of the optimization, and identifying the minimum number of measurement shots, as well as their distribution, satisfying the SubsCoRe requirement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dtVlc9ybTm": {
    "title": "Feedback Efficient Online Fine-Tuning of Diffusion Models",
    "volume": "poster",
    "abstract": "Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to finetune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that efficiently explores on the manifold of feasible samples. We present a theoretical analysis providing a regret guarantee, as well as empirical validation across three domains: images, biological sequences, and molecules",
    "checked": true,
    "id": "6a225bb2454f66d1b106bbae7e5129d6569aa260",
    "semantic_title": "feedback efficient online fine-tuning of diffusion models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=XmLNDlQuzO": {
    "title": "Generative Marginalization Models",
    "volume": "poster",
    "abstract": "We introduce *marginalization models* (MAMs), a new family of generative models for high-dimensional discrete data. They offer scalable and flexible generative modeling by explicitly modeling all induced marginal distributions. Marginalization models enable fast approximation of arbitrary marginal probabilities with a single forward pass of the neural network, which overcomes a major limitation of arbitrary marginal inference models, such as any-order autoregressive models. MAMs also address the scalability bottleneck encountered in training any-order generative models for high-dimensional problems under the context of *energy-based training*, where the goal is to match the learned distribution to a given desired probability (specified by an unnormalized log-probability function such as energy or reward function). We propose scalable methods for learning the marginals, grounded in the concept of \"*marginalization self-consistency*\". We demonstrate the effectiveness of the proposed model on a variety of discrete data distributions, including images, text, physical systems, and molecules, for *maximum likelihood* and *energy-based training* settings. MAMs achieve orders of magnitude speedup in evaluating the marginal probabilities on both settings. For energy-based training tasks, MAMs enable any-order generative modeling of high-dimensional problems beyond the scale of previous methods. Code is available at github.com/PrincetonLIPS/MaM",
    "checked": true,
    "id": "2b8027cf2e82c266c82d024b3a0cc4705337d1e6",
    "semantic_title": "generative marginalization models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YWuSLBkfOw": {
    "title": "To Cool or not to Cool? Temperature Network Meets Large Foundation Models via DRO",
    "volume": "poster",
    "abstract": "The temperature parameter plays a profound role during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models. Particularly, it adjusts the logits in the softmax function in LLMs, which is crucial for next token generation, and it scales the similarities in the contrastive loss for training CLIP models. A significant question remains: `` Is it viable to learn a neural network to predict a personalized temperature of any input data for enhancing LFMs?\" In this paper, we present a principled framework for learning a small yet generalizable temperature prediction network (TempNet) to improve LFMs. Our solution is composed of a novel learning framework with robust losses underpinned by constrained distributionally robust optimization (DRO), and a properly designed TempNet with theoretical inspiration. TempNet can be trained together with a large foundation model from scratch or learned separately given a pretrained foundation model. It is not only useful for predicting personalized temperature to promote the training of LFMs but also generalizable and transferable to new tasks. Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models",
    "checked": true,
    "id": "d2ed4aab8093d857e3c5f00266e3865e5bdd5c09",
    "semantic_title": "to cool or not to cool? temperature network meets large foundation models via dro",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6PTiCmGcNx": {
    "title": "Contamination-Resilient Anomaly Detection via Adversarial Learning on Partially-Observed Normal and Anomalous Data",
    "volume": "poster",
    "abstract": "Many existing anomaly detection methods assume the availability of a large-scale normal dataset. But for many applications, limited by resources, removing all anomalous samples from a large un-labeled dataset is unrealistic, resulting in contaminated datasets. To detect anomalies accurately under such scenarios, from the probabilistic perspective, the key question becomes how to learn the normal-data distribution from a contaminated dataset. To this end, we propose to collect two additional small datasets that are comprised of partially-observed normal and anomaly samples, and then use them to help learn the distribution under an adversarial learning scheme. We prove that under some mild conditions, the proposed method is able to learn the correct normal-data distribution. Then, we consider the overfitting issue caused by the small size of the two additional datasets, and a correctness-guaranteed flipping mechanism is further developed to alleviate it. Theoretical results under incomplete observed anomaly types are also presented. Extensive experimental results demonstrate that our method outperforms representative baselines when detecting anomalies under contaminated datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=scMAQ3mFAA": {
    "title": "Bayesian Optimization of Function Networks with Partial Evaluations",
    "volume": "poster",
    "abstract": "Bayesian optimization is a powerful framework for optimizing functions that are expensive or time-consuming to evaluate. Recent work has considered Bayesian optimization of function networks (BOFN), where the objective function is given by a network of functions, each taking as input the output of previous nodes in the network as well as additional parameters. Leveraging this network structure has been shown to yield significant performance improvements. Existing BOFN algorithms for general-purpose networks evaluate the full network at each iteration. However, many real-world applications allow for evaluating nodes individually. To exploit this, we propose a novel knowledge gradient acquisition function that chooses which node and corresponding inputs to evaluate in a cost-aware manner, thereby reducing query costs by evaluating only on a part of the network at each step. We provide an efficient approach to optimizing our acquisition function and show that it outperforms existing BOFN methods and other benchmarks across several synthetic and real-world problems. Our acquisition function is the first to enable cost-aware optimization of a broad class of function networks",
    "checked": true,
    "id": "0b14fe3d669111919332d1fdf5ff9752a90c0b36",
    "semantic_title": "bayesian optimization of function networks with partial evaluations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GJzqRKOdRi": {
    "title": "From Inverse Optimization to Feasibility to ERM",
    "volume": "poster",
    "abstract": "Inverse optimization involves inferring unknown parameters of an optimization problem from known solutions and is widely used in fields such as transportation, power systems, and healthcare. We study the *contextual inverse optimization setting* that utilizes additional contextual information to better predict the unknown problem parameters. We focus on contextual inverse linear programming (CILP) addressing the challenges posed by the non-differentiable nature of LPs. For a linear prediction model, we reduce CILP to a convex feasibility problem allowing the use of standard algorithms such as alternating projections. The resulting algorithm for CILP is equipped with theoretical convergence guarantees without additional assumptions such as degeneracy or interpolation. Next, we reduce CILP to empirical risk minimization (ERM) on a smooth, convex loss that satisfies the Polyak-Lojasiewicz condition. This reduction enables the use of scalable first-order optimization methods to solve large non-convex problems while maintaining theoretical guarantees in the convex setting. Subsequently, we use the reduction to ERM to quantify the generalization performance of the proposed algorithm on previously unseen instances. Finally, we experimentally validate our approach on synthetic and real-world problems and demonstrate improved performance compared to existing methods",
    "checked": true,
    "id": "5b622f9993504c8829beedffd2ac8966dbfe7710",
    "semantic_title": "from inverse optimization to feasibility to erm",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMsMQJraEj": {
    "title": "Impact of Decentralized Learning on Player Utilities in Stackelberg Games",
    "volume": "poster",
    "abstract": "When deployed in the world, a learning agent such as a recommender system or a chatbot often repeatedly interacts with another learning agent (such as a user) over time. In many such two-agent systems, each agent learns separately and the rewards of the two agents are not perfectly aligned. To better understand such cases, we examine the learning dynamics of the two-agent system and the implications for each agent's objective. We model these systems as Stackelberg games with decentralized learning and show that standard regret benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case linear regret for at least one player. To better capture these systems, we construct a relaxed regret benchmark that is tolerant to small learning errors by agents. We show that standard learning algorithms fail to provide sublinear regret, and we develop algorithms to achieve near-optimal $\\mathcal{O}(T^{2/3})$ regret for both players with respect to these benchmarks. We further design relaxed environments under which faster learning ($\\mathcal{O}(\\sqrt{T})$) is possible. Altogether, our results take a step towards assessing how two-agent interactions in sequential and decentralized learning environments affect the utility of both agents",
    "checked": true,
    "id": "90a438e6ab671152b8427c5ec137b38d0da73033",
    "semantic_title": "impact of decentralized learning on player utilities in stackelberg games",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VF177x7Syw": {
    "title": "Slow and Steady Wins the Race: Maintaining Plasticity with Hare and Tortoise Networks",
    "volume": "poster",
    "abstract": "This study investigates the loss of generalization ability in neural networks, revisiting warm-starting experiments from Ash & Adams. Our empirical analysis reveals that common methods designed to enhance plasticity by maintaining trainability provide limited benefits to generalization. While reinitializing the network can be effective, it also risks losing valuable prior knowledge. To this end, we introduce the Hare & Tortoise, inspired by the brain's complementary learning system. Hare & Tortoise consists of two components: the Hare network, which rapidly adapts to new information analogously to the hippocampus, and the Tortoise network, which gradually integrates knowledge akin to the neocortex. By periodically reinitializing the Hare network to the Tortoise's weights, our method preserves plasticity while retaining general knowledge. Hare & Tortoise can effectively maintain the network's ability to generalize, which improves advanced reinforcement learning algorithms on the Atari-100k benchmark. The code is available at https://github.com/dojeon-ai/hare-tortoise",
    "checked": true,
    "id": "59251fe65315612d8f6a927134f35bc4f7101079",
    "semantic_title": "slow and steady wins the race: maintaining plasticity with hare and tortoise networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=K6xxnKN2gm": {
    "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
    "volume": "poster",
    "abstract": "Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3$ % at the parameter level and $2.5$ % at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs",
    "checked": true,
    "id": "aa6a03f3368cbb4a413f7e11650fb8a6a2b71de1",
    "semantic_title": "assessing the brittleness of safety alignment via pruning and low-rank modifications",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=3JhmHCVPa8": {
    "title": "Learning to Reach Goals via Diffusion",
    "volume": "poster",
    "abstract": "We present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of denoising diffusion models. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy to reverse these deviations, analogous to the score function. This approach, which we call Merlin, can reach specified goals from arbitrary initial states without learning a separate value function. In contrast to recent works utilizing diffusion models in offline RL, Merlin stands out as the first method to perform diffusion in the state space, requiring only one \"denoising\" iteration per environment step. We experimentally validate our approach in various offline goal-reaching tasks, demonstrating substantial performance enhancements compared to state-of-the-art methods while improving computational efficiency over other diffusion-based RL methods by an order of magnitude. Our results suggest that this perspective on diffusion for RL is a simple and scalable approach for sequential decision making",
    "checked": true,
    "id": "a0de92d6dc8e1dd457e2b30d2680a49d90b1dcb5",
    "semantic_title": "learning to reach goals via diffusion",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=3MW8GKNyzI": {
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQfA0etfB7": {
    "title": "High-Dimensional Geometric Streaming for Nearly Low Rank Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kZKopcDp2q": {
    "title": "Hyperbolic Optimizer as a Dynamical System",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gKPkipJ3gm": {
    "title": "Causal-IQA: Towards the Generalization of Image Quality Assessment Based on Causal Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6VQXLUy4sQ": {
    "title": "Sample as you Infer: Predictive Coding with Langevin Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=htq0FbPOsY": {
    "title": "On the Tractability of SHAP Explanations under Markovian Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7bg10Jj3bG": {
    "title": "Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eDjvSFOkXw": {
    "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BO0jookxk8": {
    "title": "On Least Square Estimation in Softmax Gating Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5sgkNtexs2": {
    "title": "Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Pcl5qOOfL": {
    "title": "Leveraging VLM-Based Pipelines to Annotate 3D Objects",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ffLblkoCw8": {
    "title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qjqlhWDcId": {
    "title": "Transformers Provably Learn Sparse Token Selection While Fully-Connected Nets Cannot",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AEHXvoOxV9": {
    "title": "On the Consistency of Kernel Methods with Dependent Observations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0NphYCmgua": {
    "title": "Self-Rewarding Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AVEc9LvSlO": {
    "title": "Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yoTCwNqQS6": {
    "title": "Rethinking Guidance Information to Utilize Unlabeled Samples: A Label Encoding Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wjq2bS7fTK": {
    "title": "FedREDefense: Defending against Model Poisoning Attacks for Federated Learning using Model Update Reconstruction Error",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z3PUNzdmGs": {
    "title": "Dynamic Metric Embedding into lp Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ntak1BGBd": {
    "title": "ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JcxlFe2fGC": {
    "title": "Trainable Transformer in Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7C4EQqtb02": {
    "title": "Graphon Mean Field Games with a Representative Player: Analysis and Learning Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yHs3jIPgaF": {
    "title": "Performative Prediction with Bandit Feedback: Learning through Reparameterization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zOZ0yKhm6": {
    "title": "Probabilistic Modeling of Interpersonal Coordination Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iqAyWVLUEO": {
    "title": "Statistical Properties of Robust Satisficing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4B42sxNq3": {
    "title": "Recurrent Early Exits for Federated Learning with Heterogeneous Clients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jn2iTJas6h": {
    "title": "A decoder-only foundation model for time-series forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WC14xZIaC2": {
    "title": "Learning High-Order Relationships of Brain Regions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPhwhueqjR": {
    "title": "A Universal Transfer Theorem for Convex Optimization Algorithms Using Inexact First-order Oracles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7KtFQnF368": {
    "title": "Convergence and Complexity Guarantee for Inexact First-order Riemannian Optimization Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l8GrPpsZfy": {
    "title": "Understanding Stochastic Natural Gradient Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0nMzOmkBHC": {
    "title": "FedSC: Provable Federated Self-supervised Learning with Spectral Contrastive Objective over Non-i.i.d. Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pxMIjB9QK": {
    "title": "Biharmonic Distance of Graphs and its Higher-Order Variants: Theoretical Properties with Applications to Centrality and Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUeoOBid3x": {
    "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JIWtKcR78C": {
    "title": "Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=opieUcKjPa": {
    "title": "A New Robust Partial p-Wasserstein-Based Metric for Comparing Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jNM4imlHZv": {
    "title": "How Transformers Learn Causal Structure with Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J4HJUF70qm": {
    "title": "Clustered Federated Learning via Gradient-based Partitioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1khG2xf1yt": {
    "title": "On PI Controllers for Updating Lagrange Multipliers in Constrained Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0e8SEDSpNT": {
    "title": "KernelWarehouse: Rethinking the Design of Dynamic Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H3bATm4mKn": {
    "title": "Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wWdkNkUY8k": {
    "title": "Improving Equivariant Graph Neural Networks on Large Geometric Graphs via Virtual Nodes Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SMUXPVKUBg": {
    "title": "Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kjww7ZN47M": {
    "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PAPY0cAB3C": {
    "title": "In-Context Principle Learning from Mistakes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lwm6TiUP4X": {
    "title": "Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ymgcTqrZLT": {
    "title": "Estimating Barycenters of Distributions with Neural Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=93gjGDwqim": {
    "title": "ReconBoost: Boosting Can Achieve Modality Reconcilement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gbD9MAc9p0": {
    "title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wkCUmO7oi2": {
    "title": "Joint Composite Latent Space Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wj5wm3Os5v": {
    "title": "Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I4HTPws9P6": {
    "title": "How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qLZ32oS7j2": {
    "title": "Learning from Streaming Data when Users Choose",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RXxTuxPopa": {
    "title": "Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BrCrnaCYDc": {
    "title": "Dynamic Anisotropic Smoothing for Noisy Derivative-Free Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LIQYhV45D4": {
    "title": "Federated Representation Learning in the Under-Parameterized Regime",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2hWd4CVhXz": {
    "title": "New Sample Complexity Bounds for Sample Average Approximation in Heavy-Tailed Stochastic Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f47ZK6gy3I": {
    "title": "Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itYGbe0Cs1": {
    "title": "AI Alignment with Changing and Influenceable Reward Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ZwEifshyo": {
    "title": "Explorations of Self-Repair in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1dtYo5ywXZ": {
    "title": "Understanding MLP-Mixer as a wide and sparse MLP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xQiYCmDrjp": {
    "title": "Learning Temporal Distances: Contrastive Successor Features Can Provide a Metric Structure for Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GcW9pg4P9x": {
    "title": "Constrained Reinforcement Learning Under Model Mismatch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y2wRKE0Qor": {
    "title": "Structured Inverse-Free Natural Gradient Descent: Memory-Efficient & Numerically-Stable KFAC",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=acTLXagzqd": {
    "title": "Graph Geometry-Preserving Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zAHgkiCQg": {
    "title": "Premise Order Matters in Reasoning with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q14AbM4kdv": {
    "title": "An Effective Dynamic Gradient Calibration Method for Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vQmVmMN5ft": {
    "title": "Achieving Lossless Gradient Sparsification via Mapping to Alternative Space in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Rroj9GIOQ": {
    "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g43yUNWX4V": {
    "title": "Momentum for the Win: Collaborative Federated Reinforcement Learning across Heterogeneous Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TwZ2sY6eJj": {
    "title": "How to Trace Latent Generative Model Generated Images without Artificial Watermark?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FWlNA3et6X": {
    "title": "To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vhMq3eAB34": {
    "title": "DiffDA: a Diffusion model for weather-scale Data Assimilation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A7CtiozznN": {
    "title": "Graph2Tac: Online Representation Learning of Formal Math Concepts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hjwx3H6Vci": {
    "title": "Distribution Alignment Optimization through Neural Collapse for Long-tailed Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8iWDWQKxJ1": {
    "title": "Optimal Coresets for Low-Dimensional Geometric Median",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j6rG1ETRyu": {
    "title": "Discovering Multiple Solutions from a Single Task in Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jvVWPtJYbc": {
    "title": "Minimizing $f$-Divergences by Interpolating Velocity Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sO5qtpvsUZ": {
    "title": "Optimal Eye Surgeon: Finding image priors through sparse generators at initialization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Sl0lPF6ka": {
    "title": "A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gQpBnRHwxM": {
    "title": "Position: A Roadmap to Pluralistic Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jhWSzTO0Jl": {
    "title": "Post-hoc Part-Prototype Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=txRZBD8tBV": {
    "title": "Asymmetry in Low-Rank Adapters of Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GVmvBNxB73": {
    "title": "Retrieval Across Any Domains via Large-scale Pre-trained Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64fdhmogiD": {
    "title": "Learning to Stabilize Online Reinforcement Learning in Unbounded State Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EWJn6hfZ4J": {
    "title": "Light and Optimal Schrödinger Bridge Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IUijgjJgWO": {
    "title": "Fool Your (Vision and) Language Model with Embarrassingly Simple Permutations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZUXvpIrz5l": {
    "title": "Safe Exploration in Dose Finding Clinical Trials with Heterogeneous Participants",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZQcqXCuoxD": {
    "title": "Cooperative Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2XkRIijUKw": {
    "title": "Online conformal prediction with decaying step sizes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AZ1tWCa9j3": {
    "title": "Robustly Learning Single-Index Models via Alignment Sharpness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A54CXWn9VB": {
    "title": "A Statistical Theory of Regularization-Based Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w8BnKGFIYN": {
    "title": "Learning to Play Atari in a World of Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=40foON48am": {
    "title": "Generalization Analysis of Deep Non-linear Matrix Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=55HfvJ6lDB": {
    "title": "Learning Optimal Projection for Forecast Reconciliation of Hierarchical Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uYIFQOtb58": {
    "title": "Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xnQ1qoly7Q": {
    "title": "RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HQtTg1try7": {
    "title": "Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QKnWXX3aVm": {
    "title": "Prediction-powered Generalization of Causal Inferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GYGkt2M8ee": {
    "title": "Can Implicit Bias Imply Adversarial Robustness?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MIRQ3L8vtn": {
    "title": "Differentially private exact recovery for stochastic block models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WJ5fJhwvCl": {
    "title": "Breaking the Barrier: Enhanced Utility and Robustness in Smoothed DRL Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=glfcwSsks8": {
    "title": "Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nACGn4US1R": {
    "title": "Position: Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CpgKRKBUTl": {
    "title": "Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SvvvB5t5EW": {
    "title": "Bridging Mini-Batch and Asymptotic Analysis in Contrastive Learning: From InfoNCE to Kernel-Based Losses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OTmcsyEO5G": {
    "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XlgeQ47Ra9": {
    "title": "MAGNOLIA: Matching Algorithms via GNNs for Online Value-to-go Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uz4Qr40Y3C": {
    "title": "Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7zvl9mNQG2": {
    "title": "Compositional Image Decomposition with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49vHLSxjzy": {
    "title": "A Probabilistic Approach to Learning the Degree of Equivariance in Steerable CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7gEcbhMqKU": {
    "title": "Faster Sampling via Stochastic Gradient Proximal Sampler",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O6tenHWTUU": {
    "title": "Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zo9zXdVhW2": {
    "title": "Probabilistic Constrained Reinforcement Learning with Formal Interpretability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q5q59s2WJy": {
    "title": "Byzantine Resilient and Fast Federated Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xVXnXk9I3I": {
    "title": "A Dense Reward View on Aligning Text-to-Image Diffusion with Preference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F3x6uYILgL": {
    "title": "An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hqNz4LDuhn": {
    "title": "Nearest Neighbour Score Estimators for Diffusion Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iHSgfGob9j": {
    "title": "CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfQH4JJ4We": {
    "title": "Fast Algorithms for Hypergraph PageRank with Applications to Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EK7fuAMNoI": {
    "title": "Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YNvGFaOG1p": {
    "title": "Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EZLsxOgcDg": {
    "title": "Reducing sequential change detection to sequential estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HTMFUKAm8B": {
    "title": "A Field Guide for Pacing Budget and ROS Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PjVqEErDgK": {
    "title": "Prospector Heads: Generalized Feature Attribution for Large Models & Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wCMNbdshcY": {
    "title": "Fast Adversarial Attacks on Language Models In One GPU Minute",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LabSWooau0": {
    "title": "Enabling Few-Shot Learning with PID Control: A Layer Adaptive Optimizer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7rfZ6bMZq4": {
    "title": "DOGE: Domain Reweighting with Generalization Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WRIn2HmtBS": {
    "title": "Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ApRKrKZJSk": {
    "title": "Weisfeiler Leman for Euclidean Equivariant Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mJhXlsZzzE": {
    "title": "What Improves the Generalization of Graph Transformers? A Theoretical Dive into the Self-attention and Positional Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IUBhvyJ9Sr": {
    "title": "Hieros: Hierarchical Imagination on Structured State Space Sequence World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AbGbGZFYOD": {
    "title": "DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KYrAZSbEv6": {
    "title": "Inferring Dynamic Networks from Marginals with Iterative Proportional Fitting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k10805cgak": {
    "title": "Learning to Remove Cuts in Integer Linear Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZvJ2lQQKjz": {
    "title": "Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sNjxqSnXFO": {
    "title": "Stochastic Quantum Sampling for Non-Logconcave Distributions and Estimating Partition Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=47ahBl70xb": {
    "title": "Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n9pru4bJU9": {
    "title": "Scaling Down Deep Learning with MNIST-1D",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OgG0I5toZZ": {
    "title": "Pragmatic Feature Preferences: Learning Reward-Relevant Preferences from Human Input",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DwniHlwcOB": {
    "title": "Incorporating Information into Shapley Values: Reweighting via a Maximum Entropy Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Paw0BkPaTN": {
    "title": "Robust Universal Adversarial Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fsVBsxjRER": {
    "title": "On Mechanistic Knowledge Localization in Text-to-Image Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ud4GSrqUKI": {
    "title": "Distinguishing the Knowable from the Unknowable with Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AYWBRwsZ8z": {
    "title": "Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1AAlMSo7Js": {
    "title": "Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=swTG6xju8O": {
    "title": "IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c3ls5AVOw7": {
    "title": "Position: Insights from Survey Methodology can Improve Training Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zRrzSLwNHQ": {
    "title": "Homomorphism Counts for Graph Neural Networks: All About That Basis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EIcxV7T0Sy": {
    "title": "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tRESfzWFtf": {
    "title": "Barrier Algorithms for Constrained Non-Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rXnBvu5D7i": {
    "title": "Piecewise Constant and Linear Regression Trees: An Optimal Dynamic Programming Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y9qzwNlKVU": {
    "title": "Random Latent Exploration for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nsjfoziR5j": {
    "title": "Can a Few Decide for Many? The Metric Distortion of Sortition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XQz7ytgETQ": {
    "title": "Network Tight Community Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=61JD8wp4Id": {
    "title": "Neural Tangent Kernels Motivate Cross-Covariance Graphs in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0vozy8vstt": {
    "title": "Efficient Contextual Bandits with Uninformed Feedback Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xW79geE0RA": {
    "title": "Model-based Reinforcement Learning for Parameterized Action Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QLtxj3erlJ": {
    "title": "Optimization without Retraction on the Random Generalized Stiefel Manifold",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQkK6YH0Th": {
    "title": "Nash Incentive-compatible Online Mechanism Learning via Weakly Differentially Private Online Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J16WEPdqhJ": {
    "title": "Accelerated Policy Gradient for s-rectangular Robust MDPs with Large State Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QPy7zLfvof": {
    "title": "Interpretable Deep Clustering for Tabular Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZ6lrZzB02": {
    "title": "Causal Inference from Competing Treatments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5WEIVj98Ju": {
    "title": "IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YT1dtdLvSN": {
    "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CLJZI5kDhX": {
    "title": "An Independence-promoting Loss for Music Generation with Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nYX7I6PsL7": {
    "title": "HAMLET: Graph Transformer Neural Operator for Partial Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMwFvxr6CV": {
    "title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCEtumPYDt": {
    "title": "Uncertainty for Active Learning on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S0DPCE7tt4": {
    "title": "Why Do Animals Need Shaping? A Theory of Task Composition and Curriculum Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dkdilv4XD4": {
    "title": "Balanced Resonate-and-Fire Neurons",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=va3r3hSA6n": {
    "title": "Comparing Graph Transformers via Positional Encodings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kTaX87Zn6M": {
    "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FPlaQyAGHu": {
    "title": "How Language Model Hallucinations Can Snowball",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdZfEMuoj2": {
    "title": "Towards Neural Architecture Search through Hierarchical Generative Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ExHTFXEhc9": {
    "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ui8ewXg1hV": {
    "title": "Two Tales of Single-Phase Contrastive Hebbian Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ao9UUaScAU": {
    "title": "Why do Variational Autoencoders Really Promote Disentanglement?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ixdfvnO0uy": {
    "title": "Differentiability and Optimization of Multiparameter Persistent Homology",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YB1O99gK7b": {
    "title": "On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6TCeizkLJV": {
    "title": "Confidence Aware Inverse Constrained Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Pq6uI1MTE": {
    "title": "Differentiable Combinatorial Scheduling at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jM9A3Kz6Ki": {
    "title": "Averaging $n$-step Returns Reduces Variance in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VyGo1S5A6d": {
    "title": "Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EQXZqBXeW9": {
    "title": "Federated Neuro-Symbolic Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b1YQ5WKY3w": {
    "title": "Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e5admkWKgV": {
    "title": "Position: A Call for Embodied AI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hLuNVjRnY3": {
    "title": "Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cXBPPfNUZJ": {
    "title": "Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iup9NElHji": {
    "title": "Visual Transformer with Differentiable Channel Selection: An Information Bottleneck Inspired Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lWy2lCTyJa": {
    "title": "Revisiting Inexact Fixed-Point Iterations for Min-Max Problems: Stochasticity and Structured Nonconvexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTVSyqD6XX": {
    "title": "Private and Federated Stochastic Convex Optimization: Efficient Strategies for Centralized Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KjazcKPMME": {
    "title": "Understanding the Effects of Iterative Prompting on Truthfulness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KJhLpzqNri": {
    "title": "Embarrassingly Parallel GFlowNets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cY9g0bwiZx": {
    "title": "The Max-Min Formulation of Multi-Objective Reinforcement Learning: From Theory to a Model-Free Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9laB7ytoMp": {
    "title": "Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AxmefV2NEf": {
    "title": "TimeMIL: Advancing Multivariate Time Series Classification via a Time-aware Multiple Instance Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=otuTw4Mghk": {
    "title": "On the Origins of Linear Representations in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kRxCDDFNpp": {
    "title": "Fewer Truncations Improve Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iJlPJsTw2B": {
    "title": "Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QZgo9JZpLq": {
    "title": "The Illusion of State in State-Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wea7nsJdMc": {
    "title": "Incremental Topological Ordering and Cycle Detection with Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dBMLtuKH01": {
    "title": "Position: The Causal Revolution Needs Scientific Pragmatism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IwqE4QqBew": {
    "title": "Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E3V5MMwFgd": {
    "title": "Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Et8Pk97u4u": {
    "title": "Practical Hamiltonian Monte Carlo on Riemannian Manifolds via Relativity Theory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3MIuPRJYwf": {
    "title": "Expand-and-Cluster: Parameter Recovery of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xPmSNLle1w": {
    "title": "A New Branch-and-Bound Pruning Framework for $\\ell_0$-Regularized Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OS5dqxmmtl": {
    "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XvmooikuHE": {
    "title": "Probability Distribution of Hypervolume Improvement in Bi-objective Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XiemSZpvh0": {
    "title": "Neuro-Visualizer: A Novel Auto-Encoder-Based Loss Landscape Visualization Method With an Application in Knowledge-Guided Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xTYIAD2NND": {
    "title": "Out-of-Domain Generalization in Dynamical Systems Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u6PeRHEsjL": {
    "title": "Position: Evolving AI Collectives Enhance Human Diversity and Enable Self-Regulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfJgeBNCFI": {
    "title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yo9Jyt3XCY": {
    "title": "Position: AI/ML Influencers Have a Place in the Academic Process",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XuQPA4D396": {
    "title": "Autoencoding Conditional Neural Processes for Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7joG3i2pUR": {
    "title": "Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NlM4gp8hyO": {
    "title": "Sequence Compression Speeds Up Credit Assignment in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3MfvxH3Gia": {
    "title": "Multimodal Prototyping for cancer survival prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Tq4L3Go9f": {
    "title": "Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=poEPRuNvM3": {
    "title": "Fair Off-Policy Learning from Observational Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=321GwKMtxO": {
    "title": "REMEDI: Corrective Transformations for Improved Neural Entropy Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lmiurzioja": {
    "title": "Learning Modality Knowledge Alignment for Cross-Modality Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PG5fV50maR": {
    "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ibwxzYCep9": {
    "title": "DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9C4Kwm56G": {
    "title": "Rapid Learning without Catastrophic Forgetting in the Morris Water Maze",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=awo5H10K6v": {
    "title": "Language-guided Skill Learning with Temporal Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SE20BFqj6J": {
    "title": "D-Flow: Differentiating through Flows for Controlled Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3104y8djk": {
    "title": "CogBench: a large language model walks into a psychology lab",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sz9mAYuqlE": {
    "title": "Optimally Improving Cooperative Learning in a Social Setting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=amRSBdZlw9": {
    "title": "Position: Why Tabular Foundation Models Should Be a Research Priority",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kOczKjmYum": {
    "title": "MusicFlow: Cascaded Flow Matching for Text Guided Music Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AqGCEHK9dZ": {
    "title": "Profile Reconstruction from Private Sketches",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nbOY1OmtRc": {
    "title": "A Dynamical Model of Neural Scaling Laws",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4uTJfGYA2t": {
    "title": "An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o2ND9v0CeK": {
    "title": "Interpreting and Improving Diffusion Models from an Optimization Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pfnBLXgFVS": {
    "title": "A General Online Algorithm for Optimizing Complex Performance Metrics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rI6lxIX0uX": {
    "title": "Visual Representation Learning with Stochastic Frame Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KJL2b6BthC": {
    "title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWZKvF0g7G": {
    "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UTSCK582Yo": {
    "title": "Graph Positional and Structural Encoder",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9iGdh0wAgB": {
    "title": "Sliding Down the Stairs: How Correlated Latent Variables Accelerate Learning with Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wdTiuvd0fR": {
    "title": "Feature Reuse and Scaling: Understanding Transfer Learning with Protein Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tASXcrMekp": {
    "title": "MADA: Meta-Adaptive Optimizers Through Hyper-Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LJcIIhqGDN": {
    "title": "Successor Features for Efficient Multi-Subject Controlled Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lVQ4FUZ6dp": {
    "title": "Generalization to New Sequential Decision Making Tasks with In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rVWsTjMW1m": {
    "title": "Coactive Learning for Large Language Models using Implicit User Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ENNGAY5uKC": {
    "title": "SuDA: Support-based Domain Adaptation for Sim2Real Hinge Joint Tracking with Flexible Sensors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=japBn31gXC": {
    "title": "Simple Ingredients for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pspyQm4ko0": {
    "title": "The Balanced-Pairwise-Affinities Feature Transform",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E8FpcUyPuS": {
    "title": "Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=psup68MBvt": {
    "title": "DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AIXUuLCuMe": {
    "title": "Position: Stop Making Unscientific AGI Performance Claims",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jNab9mXEyj": {
    "title": "Partially Stochastic Infinitely Deep Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eCCaHZKdl4": {
    "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=66k81s33p3": {
    "title": "Towards Efficient Exact Optimization of Language Model Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wnhp34K5jR": {
    "title": "Universal Gradient Methods for Stochastic Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a1GvTbadqA": {
    "title": "$\\mathtt{VITS}$ : Variational Inference Thompson Sampling for contextual bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CduFAALvGe": {
    "title": "Learning Iterative Reasoning through Energy Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s4EYBJ30WY": {
    "title": "Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nP7Q1PnuLK": {
    "title": "Thermometer: Towards Universal Calibration for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0FWPKHMCSc": {
    "title": "Large Scale Dataset Distillation with Domain Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BiWIERWBFX": {
    "title": "Efficient World Models with Context-Aware Tokenization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5g6y7JlMw": {
    "title": "Random features models: a way to study the success of naive imputation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gVg8V9isul": {
    "title": "Long Range Propagation on Continuous-Time Dynamic Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=01M0N8VgfB": {
    "title": "Improved Modelling of Federated Datasets using Mixtures-of-Dirichlet-Multinomials",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eGZH3HCuGm": {
    "title": "Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=seo9V9QRZp": {
    "title": "In value-based deep reinforcement learning, a pruned network is a good network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kkqIEp2bRa": {
    "title": "Differentially Private Domain Adaptation with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mXUDDL4r1Q": {
    "title": "Reinforcement Learning from Reachability Specifications: PAC Guarantees with Expected Conditional Distance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ADnUzsmsLW": {
    "title": "Do Large Code Models Understand Programming Concepts? Counterfactual Analysis for Code Predicates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WFyolnFZOR": {
    "title": "Language Models as Science Tutors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aA2326y3hf": {
    "title": "Simulation of Graph Algorithms with Looped Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EruV94XRDs": {
    "title": "MusicRL: Aligning Music Generation to Human Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkCRgoGtt6": {
    "title": "Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6AwZauZPV": {
    "title": "Probabilistic Subgoal Representations for Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MKGrRVODWR": {
    "title": "A Sparsity Principle for Partially Observable Causal Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lrFwPeDdEQ": {
    "title": "Federated Combinatorial Multi-Agent Multi-Armed Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VHO4nE7v41": {
    "title": "Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0AZAjkXhit": {
    "title": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dV9QGostQk": {
    "title": "A Unified View of FANOVA: A Comprehensive Bayesian Framework for Component Selection and Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T0zR4mdSce": {
    "title": "PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q5Bg858Hef": {
    "title": "Disguised Copyright Infringement of Latent Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fwxnHViGNj": {
    "title": "Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmarks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uDoy7AGvEC": {
    "title": "LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o6N1Bqay0k": {
    "title": "How Spurious Features are Memorized: Precise Analysis for Random and NTK Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PudBRuNa8r": {
    "title": "Building Socially-Equitable Public Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oCI9gHocws": {
    "title": "KISA: A Unified Keyframe Identifier and Skill Annotator for Long-Horizon Robotics Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GwA4go0Mw4": {
    "title": "Representation Surgery: Theory and Practice of Affine Steering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hLGxDYo0eF": {
    "title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hRBdOHVn7y": {
    "title": "Chasing Convex Functions with Long-term Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZ0JnRxi0x": {
    "title": "An Explicit Frame Construction for Normalizing 3D Point Clouds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xC7SYAZygF": {
    "title": "Simultaneous identification of models and parameters of scientific simulators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nd47Za5jk5": {
    "title": "Graph-based Time Series Clustering for End-to-End Hierarchical Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JBaPBPrn93": {
    "title": "Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QH4mXDEULp": {
    "title": "Diffusive Gibbs Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdSe1qnuia": {
    "title": "Score-Based Causal Discovery of Latent Variable Causal Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mz55Ox0Igz": {
    "title": "Bayesian Regret Minimization in Offline Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ndVXXmxSC5": {
    "title": "Bringing Motion Taxonomies to Continuous Domains via GPLVM on Hyperbolic manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G4b32bKnBy": {
    "title": "Minimum Norm Interpolation Meets The Local Theory of Banach Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MSMKQuZhD5": {
    "title": "Amortized Variational Deep Kernel Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KaAQu5rNU1": {
    "title": "MolCRAFT: Structure-Based Drug Design in Continuous Parameter Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KSNl7VgeVr": {
    "title": "Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D8zn1DnTuj": {
    "title": "Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0xmfExPqFf": {
    "title": "Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AoYhtJ4A90": {
    "title": "FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f49AkFT5jf": {
    "title": "Data Poisoning Attacks against Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bNgAdyv7ZP": {
    "title": "Robust Data-driven Prescriptiveness Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k7G4N1x7f9": {
    "title": "Improving SAM Requires Rethinking its Optimization Formulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xLikRS9OhW": {
    "title": "Do Efficient Transformers Really Save Computation?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUj9b2CezT": {
    "title": "A Generative Approach for Treatment Effect Estimation under Collider Bias: From an Out-of-Distribution Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AEqim4X0NV": {
    "title": "Understanding Diffusion Models by Feynman's Path Integral",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1DyruVvVaQ": {
    "title": "Decoupling Learning and Decision-Making: Breaking the $\\mathcal{O}(\\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0jpbpFia8m": {
    "title": "SqueezeLLM: Dense-and-Sparse Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XGq30hC5MW": {
    "title": "Risk-Sensitive Reward-Free Reinforcement Learning with CVaR",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MGkeWJxQVl": {
    "title": "Reason for Future, Act for Now: A Principled Architecture for Autonomous LLM Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s9RKqT7jVM": {
    "title": "Understanding and Diagnosing Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ZM8MXGFRA": {
    "title": "Auto-Linear Phenomenon in Subsurface Imaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWNPx6t0sF": {
    "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4KQ0VwqPg8": {
    "title": "To the Max: Reinventing Reward in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zfmwAaB9Nw": {
    "title": "Individualized Privacy Accounting via Subsampling with Applications in Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y5L8W0KRUX": {
    "title": "Evolution-Inspired Loss Functions for Protein Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dW29JZj0G5": {
    "title": "Denoising Autoregressive Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KVvku47shW": {
    "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JApt4Ty89Y": {
    "title": "Quantum Algorithm for Online Exp-concave Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXVn5IFsrs": {
    "title": "CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dwWef5w2cR": {
    "title": "Robust Inverse Graphics via Probabilistic Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SiEfsCecd": {
    "title": "Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3umNqxjFad": {
    "title": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tDRYrAkOB7": {
    "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6KtXzUUEp4": {
    "title": "Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5S8ukkEQr2": {
    "title": "Provably Efficient Partially Observable Risk-sensitive Reinforcement Learning with Hindsight Observation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IoUOhnCmlX": {
    "title": "Bagged Deep Image Prior for Recovering Images in the Presence of Speckle Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=66KmnMhGU5": {
    "title": "Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MmZJ3kJXjX": {
    "title": "What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oOlooUu2Sb": {
    "title": "How to Leverage Diverse Demonstrations in Offline Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W7Vqx1Jvc2": {
    "title": "Position: Quo Vadis, Unsupervised Time Series Anomaly Detection?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6UGSDDPkJw": {
    "title": "Position: Do Not Explain Vision Models Without Context",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eG42XBhV9a": {
    "title": "OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CbIRQgAYE4": {
    "title": "Bayesian Program Learning by Decompiling Amortized Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uubBZKM99Y": {
    "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D2MNVeVh5J": {
    "title": "Outlier-robust Kalman Filtering through Generalised Bayes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HPQaMmABgK": {
    "title": "Stochastic Q-learning for Large Discrete Action Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zdNTiTs5gU": {
    "title": "TIC-TAC: A Framework For Improved Covariance Estimation In Deep Heteroscedastic Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fywWm06IGn": {
    "title": "Feature Importance Disparities for Data Bias Investigations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5PqzKxmfag": {
    "title": "Tilt your Head: Activating the Hidden Spatial-Invariance of Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iaV2fU6Dif": {
    "title": "Modeling Caption Diversity in Contrastive Vision-Language Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Z9CRr5srL": {
    "title": "In-Context Language Learning: Architectures and Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y4VgJfbjfl": {
    "title": "CuTS: Customizable Tabular Synthetic Data Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zB6VQzDmK8": {
    "title": "Overcoming the Optimizer's Curse: Obtaining Realistic Prescriptions from Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LkJ6qOMv77": {
    "title": "Collage: Light-Weight Low-Precision Strategy for LLM Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1NdN7eXyb4": {
    "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2cEhQ4vtTf": {
    "title": "Editing Partially Observable Networks via Graph Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWNUTRgBso": {
    "title": "Slicing Mutual Information Generalization Bounds for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pA2Q5Wfspp": {
    "title": "RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cwIhvoTzuK": {
    "title": "Mean Estimation in the Add-Remove Model of Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GGnYDXZC1B": {
    "title": "No-Regret Reinforcement Learning in Smooth MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GbFluKMmtE": {
    "title": "Can Mamba Learn How To Learn? A Comparative Study on In-Context Learning Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QLcBzRI3V3": {
    "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=72oT4mPLUb": {
    "title": "From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e76GrGhIgf": {
    "title": "Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MEZydkOr3l": {
    "title": "Two Fists, One Heart: Multi-Objective Optimization Based Strategy Fusion for Long-tailed Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ug2uoAZ9c2": {
    "title": "Towards Scalable and Versatile Weight Space Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D32aTei4p5": {
    "title": "A Fine-grained Analysis of Fitted Q-evaluation: Beyond Parametric Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XyhgssAo5b": {
    "title": "Robust Learning-Augmented Dictionaries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lGZUvfP2ZF": {
    "title": "Coarse-To-Fine Tensor Trains for Compact Visual Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3o7G6tIo4X": {
    "title": "Improved Generalization of Weight Space Networks via Augmentations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qc5umSsUi8": {
    "title": "Scalable Safe Policy Improvement for Factored Multi-Agent MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LVgT0ShxN5": {
    "title": "tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J6prHJsIlf": {
    "title": "Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AQYabSOfci": {
    "title": "Analysis for Abductive Learning and Neural-Symbolic Reasoning Shortcuts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6WYk5R86Wl": {
    "title": "Learning in Deep Factor Graphs with Gaussian Belief Propagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tgsSKziIEa": {
    "title": "Keypoint-based Progressive Chain-of-Thought Distillation for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wnkC5T11Z9": {
    "title": "Attention Meets Post-hoc Interpretability: A Mathematical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OjBW993g79": {
    "title": "MGit: A Model Versioning and Management System",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cy3JBZKCw1": {
    "title": "Collapse-Aware Triplet Decoupling for Adversarially Robust Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hnqlgwcRxb": {
    "title": "Theoretical Guarantees for Variational Inference with Fixed-Variance Mixture of Gaussians",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M1ADedSnlJ": {
    "title": "Theoretical insights for diffusion guidance: A case study for Gaussian mixture models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JYcbgiSh0L": {
    "title": "Stochastic Optimization with Arbitrary Recurrent Data Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FCtO757Onl": {
    "title": "Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pktvuR7b5v": {
    "title": "Efficient Denoising Diffusion via Probabilistic Masking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L4ERlHrJRT": {
    "title": "Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X7UnDevHOM": {
    "title": "DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6BYD121JFO": {
    "title": "Neural NeRF Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RlibRvH4B4": {
    "title": "Open Ad Hoc Teamwork with Cooperative Game Theory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ks8qSwkkuZ": {
    "title": "Feasible Reachable Policy Iteration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rvaN2P1rvC": {
    "title": "Differentiable Annealed Importance Sampling Minimizes The Jensen-Shannon Divergence Between Initial and Target Distribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4FJJfYjUQR": {
    "title": "Aligning Transformers with Weisfeiler-Leman",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IpSKpOY2EH": {
    "title": "Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fowZNENcVJ": {
    "title": "Using AI Uncertainty Quantification to Improve Human Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9cG1oRnqNd": {
    "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in low-data regimes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YBXwr7wF7i": {
    "title": "Subhomogeneous Deep Equilibrium Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RfsagmV1AG": {
    "title": "On the Second-Order Convergence of Biased Policy Gradient Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0GC0NG6Orr": {
    "title": "Generalized Sobolev Transport for Probability Measures on a Graph",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QE6iC9s6vU": {
    "title": "The Merit of River Network Topology for Neural Flood Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EZH4CsKV6O": {
    "title": "Position: Video as the New Language for Real-World Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QJkG8Mln72": {
    "title": "DPZero: Private Fine-Tuning of Language Models without Backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JQlEUfzhuA": {
    "title": "Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k1J2GbamLi": {
    "title": "Exploiting Negative Samples: A Catalyst for Cohort Discovery in Healthcare Analytics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFSppFiVYG": {
    "title": "Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RI4GA8amUI": {
    "title": "Asymptotics of Learning with Deep Structured (Random) Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=36rWa8zVkh": {
    "title": "A Nearly Optimal Single Loop Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eC1OOpOGZW": {
    "title": "Saliency strikes back: How filtering out high frequencies improves white-box explanations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ExWEazod5": {
    "title": "Vision Transformers as Probabilistic Expansion from Learngene",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dYDPcx78tm": {
    "title": "On The Statistical Complexity of Offline Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C4jkx6AgWc": {
    "title": "Learning Latent Dynamic Robust Representations for World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t6dBpwkbea": {
    "title": "TimeX++: Learning Time-Series Explanations with Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mBc8Pestd5": {
    "title": "Reinformer: Max-Return Sequence Modeling for Offline RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ElNxZ40tBJ": {
    "title": "Differentially Private Worst-group Risk Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4byOXWrJay": {
    "title": "Preventing Model Collapse in Gaussian Process Latent Variable Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FhWH9TQSMh": {
    "title": "Intersectional Unfairness Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xwOENWCo46": {
    "title": "Learning Graph Representation via Graph Entropy Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kGXUL6qGso": {
    "title": "Acquisition Conditioned Oracle for Nongreedy Active Feature Acquisition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M407RM0z6h": {
    "title": "Overcoming Saturation in Density Ratio Estimation by Iterated Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Olix9pk6nV": {
    "title": "A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ohG9bVMs5j": {
    "title": "Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m5nB7ucXHT": {
    "title": "When Representations Align: Universality in Representation Learning Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JAfIDm7NED": {
    "title": "Mimicking Better by Matching the Approximate Action Distribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iQTElQbAqo": {
    "title": "Beyond the Calibration Point: Mechanism Comparison in Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jxvqvZLBuU": {
    "title": "RNAFlow: RNA Structure & Sequence Design via Inverse Folding-Based Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyY7ScNpGL": {
    "title": "Rethinking Transformers in Solving POMDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=anM1M5aoM8": {
    "title": "EvoluNet: Advancing Dynamic Non-IID Transfer Learning on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vBJZ93tvoE": {
    "title": "Modelling Microbial Communities with Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AD5QC1BTJL": {
    "title": "Parsimonious Learning-Augmented Approximations for Dense Instances of $\\mathcal{NP}$-hard Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3B6vmW2L80": {
    "title": "Single-Trajectory Distributionally Robust Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NeEbsvnaWE": {
    "title": "Privately Learning Smooth Distributions on the Hypercube by Projections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTmQmaNkGn": {
    "title": "Human-like Category Learning by Injecting Ecological Priors from Large Language Models into Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0ntTjTfHb": {
    "title": "Trust the Model Where It Trusts Itself - Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vCN5lwcWWE": {
    "title": "Lookbehind-SAM: k steps back, 1 step forward",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3FKEtlX4aM": {
    "title": "Neuroexplicit Diffusion Models for Inpainting of Optical Flow Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5M4Qa9AqY7": {
    "title": "Scalable and Flexible Causal Discovery with an Efficient Test for Adjacency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoNexFx8qz": {
    "title": "Position: Compositional Generative Modeling: A Single Model is Not All You Need",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cbacx90Wkt": {
    "title": "OAK: Enriching Document Representations using Auxiliary Knowledge for Extreme Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mK6FB9xQ7v": {
    "title": "Studying K-FAC Heuristics by Viewing Adam through a Second-Order Lens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RfQT6vJt8b": {
    "title": "How Far Can Fairness Constraints Help Recover From Biased Data?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gVjMwLDFoQ": {
    "title": "Iterated Denoising Energy Matching for Sampling from Boltzmann Densities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F2Tegvyqlo": {
    "title": "Decoupling Feature Extraction and Classification Layers for Calibrated Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GBxflz0qdX": {
    "title": "Differentiable Weightless Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a7MW5kFFOf": {
    "title": "A New Computationally Efficient Algorithm to solve Feature Selection for Functional Data Classification in High-dimensional Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jbPc3pW6sC": {
    "title": "Online Variational Sequential Monte Carlo",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4qsduFJDEB": {
    "title": "Mean-field Underdamped Langevin Dynamics and its Spacetime Discretization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CvRu2inbGV": {
    "title": "Standardized Interpretable Fairness Measures for Continuous Risk Scores",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JpzIGzru5F": {
    "title": "A Fixed-Point Approach for Causal Generative Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHjm3sXPFy": {
    "title": "Near-Linear Time Approximation Algorithms for k-means with Outliers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GfNyqrwECJ": {
    "title": "Incorporating probabilistic domain knowledge into deep multiple instance learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5kVgd2MwMY": {
    "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qY63FnLuJ1": {
    "title": "Pre-Training Protein Bi-level Representation Through Span Mask Strategy On 3D Protein Chains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPDTXQwUPy": {
    "title": "ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=75Hes6Zse4": {
    "title": "EvoRainbow: Combining Improvements in Evolutionary Reinforcement Learning for Policy Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vSerUPYFtB": {
    "title": "One for All: A Universal Generator for Concept Unlearnability via Multi-Modal Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JJSj8UXqd4": {
    "title": "Box Facets and Cut Facets of Lifted Multicut Polytopes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xwxUbBHC1q": {
    "title": "Convergence Guarantees for the DeepWalk Embedding on Block Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PMkV6oKw3": {
    "title": "Nonparametric Teaching of Implicit Neural Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g89jAdrnAF": {
    "title": "Learning to Predict Mutational Effects of Protein-Protein Interactions by Microenvironment-aware Hierarchical Prompt Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W9GaJUVLCT": {
    "title": "Time Series Diffusion in the Frequency Domain",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9oAXix8da9": {
    "title": "Pi-DUAL: Using privileged information to distinguish clean from noisy labels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=elCOPIm4Xw": {
    "title": "Contrastive Learning for Clinical Outcome Prediction with Partial Data Sources",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ScIHQoTUjT": {
    "title": "Assessing Large Language Models on Climate Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mslTE1qgLa": {
    "title": "Major-Minor Mean Field Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IaV6AgrTUp": {
    "title": "Implicit Representations for Constrained Image Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QPsEPI9bvp": {
    "title": "Delving into the Convergence of Generalized Smooth Minimax Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YRWdiaupCr": {
    "title": "Two-Stage Shadow Inclusion Estimation: An IV Approach for Causal Inference under Latent Confounding and Collider Bias",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YnFuUX08CE": {
    "title": "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtOXZCTgBa": {
    "title": "SeMOPO: Learning High-quality Model and Policy from Low-quality Offline Visual Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DBlkjCDg2i": {
    "title": "StyDeSty: Min-Max Stylization and Destylization for Single Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3e8poX3kb": {
    "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S9DV6ZP4eE": {
    "title": "Adaptive-Gradient Policy Optimization: Enhancing Policy Learning in Non-Smooth Differentiable Simulations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eyxVRMrZ4m": {
    "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=65XKBGH5PO": {
    "title": "MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QgvBcOsF4B": {
    "title": "Enhancing Storage and Computational Efficiency in Federated Multimodal Learning for Large-Scale Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOorDpKHiJ": {
    "title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pftXzp6Yn3": {
    "title": "Translation Equivariant Transformer Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DM0r4qatjT": {
    "title": "Optimal Batched Linear Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TfWKkSAziC": {
    "title": "LPGD: A General Framework for Backpropagation through Embedded Optimization Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IYI61L7SPk": {
    "title": "A General Framework for Sequential Decision-Making under Adaptivity Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JV5WpLQgv": {
    "title": "PointMC: Multi-instance Point Cloud Registration based on Maximal Cliques",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1wzdf6NjHd": {
    "title": "Aligned Objective for Soft-Pseudo-Label Generation in Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U841CrDUx9": {
    "title": "Configurable Mirror Descent: Towards a Unification of Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k1JXxbpIY6": {
    "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CbIZatwz9z": {
    "title": "Online Isolation Forest",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J0ty1o7nCj": {
    "title": "Neural operators meet conjugate gradients: The FCG-NO method for efficient PDE solving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Chy4rSqy4Y": {
    "title": "IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image- and Video-Quality Metrics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RFhkcqRmTD": {
    "title": "$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OYL91MHfuU": {
    "title": "Controllable Prompt Tuning For Balancing Group Distributional Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZRMQX6aTUS": {
    "title": "On Convergence of Incremental Gradient for Non-convex Smooth Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M2cwkGleRL": {
    "title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mv8y13wfDm": {
    "title": "Risk Aware Benchmarking of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aLSA3JH08h": {
    "title": "Boosting Offline Optimizers with Surrogate Sensitivity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a0XiA6v256": {
    "title": "Diffusion Models Encode the Intrinsic Dimension of Data Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqA8ZHO0j8": {
    "title": "Fast, Scalable, Warm-Start Semidefinite Programming with Spectral Bundling and Sketching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gUFufRkzjV": {
    "title": "VNN: Verification-Friendly Neural Networks with Hard Robustness Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dskLpg8WFb": {
    "title": "Community-Invariant Graph Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XSsoggg8pz": {
    "title": "Fair Classification with Partial Feedback: An Exploration-Based Data Collection Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ltb2XaIr9p": {
    "title": "Convergence and Trade-Offs in Riemannian Gradient Descent and Riemannian Proximal Point",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gPBMkJG7bt": {
    "title": "Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8WSNl2XA9r": {
    "title": "Rethinking Specificity in SBDD: Leveraging Delta Score and Energy-Guided Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DN7uk4gQ7C": {
    "title": "Enhancing Sufficient Dimension Reduction via Hellinger Correlation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rU8o0QQCy0": {
    "title": "Position: Is machine learning good or bad for the natural sciences?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Qf1uHTahP": {
    "title": "Policy Learning for Balancing Short-Term and Long-Term Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duyl8sy8qV": {
    "title": "Slot Abstractors: Toward Scalable Abstract Visual Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0z4bCNmkG": {
    "title": "ByMI: Byzantine Machine Identification with False Discovery Rate Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5j7Lq2ASiU": {
    "title": "Distributed Bilevel Optimization with Communication Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ulUrcOZ64": {
    "title": "Switched Flow Matching: Eliminating Singularities via Switching ODEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ELFZWG9C7l": {
    "title": "Topological Neural Networks go Persistent, Equivariant, and Continuous",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qv5szC1zp7": {
    "title": "Online Learning in CMDPs: Handling Stochastic and Adversarial Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1sesUtOIH5": {
    "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xxL7CEWuxz": {
    "title": "Exploring Intrinsic Dimension for Vision-Language Model Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zc3bAEI5lp": {
    "title": "Differentially Private Sum-Product Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2P6GVfSrfZ": {
    "title": "Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JVORowD4MD": {
    "title": "Estimating the Permanent by Nesting Importance Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R0SoZvqXyQ": {
    "title": "MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zS8zUuAU8T": {
    "title": "DSD-DA: Distillation-based Source Debiasing for Domain Adaptive Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CpcaL75UgY": {
    "title": "Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pz4B2kHVKo": {
    "title": "Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qFILbkTQWw": {
    "title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CV1N7hhpA": {
    "title": "Stationarity without mean reversion in improper Gaussian processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HPLzSCOecY": {
    "title": "Learning with Adaptive Resource Allocation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=coP4kPdhKr": {
    "title": "Dynamic Spectral Clustering with Provable Approximation Guarantee",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l4ZjeDDnu9": {
    "title": "Turnstile $\\ell_p$ leverage score sampling with applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OnidGtOhg3": {
    "title": "Diffusion Model-Augmented Behavioral Cloning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8RwhTPACAO": {
    "title": "On the Asymptotic Distribution of the Minimum Empirical Risk",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b89JtZj9gm": {
    "title": "Not Just Pretty Pictures: Toward Interventional Data Augmentation Using Text-to-Image Generators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=svm53KQAtN": {
    "title": "Sparser, Better, Deeper, Stronger: Improving Static Sparse Training with Exact Orthogonal Initialization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nTgzmXvuEA": {
    "title": "Predictive Coding beyond Correlations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a3XFF0PGLU": {
    "title": "Reward Shaping for Reinforcement Learning with An Assistant Reward Agent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R8nbccD7kv": {
    "title": "ODIM: Outlier Detection via Likelihood of Under-Fitted Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0SrNCSklZx": {
    "title": "SLOG: An Inductive Spectral Graph Neural Network Beyond Polynomial Filter",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qTX1vxzs8b": {
    "title": "Extracting Training Data From Document-Based VQA Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ed4KgHoKNe": {
    "title": "Learning from Memory: Non-Parametric Memory Augmented Self-Supervised Learning of Visual Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JvMLkGF2Ms": {
    "title": "Position: Building Guardrails for Large Language Models Requires Systematic Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ySQaphUYH": {
    "title": "WISER: Weak Supervision and Supervised Representation Learning to Improve Drug Response Prediction in Cancer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n3smZl8itR": {
    "title": "Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IpPnmhjw30": {
    "title": "Learning to Continually Learn with the Bayesian Principle",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D9EfAkQCzh": {
    "title": "Adversarially Robust Deep Multi-View Clustering: A Novel Attack and Defense Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gQz30hTkRE": {
    "title": "Applying language models to algebraic topology: generating simplicial cycles using multi-labeling in Wu's formula",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=buW1Bi6XFw": {
    "title": "Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eVGpdivOnQ": {
    "title": "Graph-enhanced Large Language Models in Asynchronous Plan Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pLtuwhoQh7": {
    "title": "Mechanistic Neural Networks for Scientific Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tPB5VXo87": {
    "title": "Full-Atom Peptide Design based on Multi-modal Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j2pLfsBm4J": {
    "title": "Distributional Bellman Operators over Mean Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQIN9ZyMLz": {
    "title": "Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pBTLGM9uWx": {
    "title": "RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4mU6LNMaIu": {
    "title": "GroupCover: A Secure, Efficient and Scalable Inference Framework for On-device Model Protection based on TEEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Stn8hXkpe6": {
    "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kZArjKc64o": {
    "title": "Intersecting-Boundary-Sensitive Fingerprinting for Tampering Detection of DNN Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ddjRdm3wUW": {
    "title": "Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhHMx51ir6": {
    "title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTXv8KDD1P": {
    "title": "Performance Bounds for Active Binary Testing with Information Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=afnyJfQddk": {
    "title": "Gaussian Processes on Cellular Complexes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AFfXlKFHXJ": {
    "title": "A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hzpt1Gws9g": {
    "title": "Finding NEM-U: Explaining unsupervised representation learning through neural network generated explanation masks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OYw6sS8QmL": {
    "title": "Bayesian Exploration Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nxIRQ8GNa": {
    "title": "Improving fine-grained understanding in image-text pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aksdU1KOpT": {
    "title": "Multi-layer Rehearsal Feature Augmentation for Class-Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ecO7WOIlMD": {
    "title": "MF-CLR: Multi-Frequency Contrastive Learning Representation for Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CtyLla0DU8": {
    "title": "Unlock the Cognitive Generalization of Deep Reinforcement Learning via Granular Ball Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kZbTkpnafR": {
    "title": "How do Transformers Perform In-Context Autoregressive Learning ?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7yixJXmzb8": {
    "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uA3FRvO2DJ": {
    "title": "On the Universality of Volume-Preserving and Coupling-Based Normalizing Flows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xeh8171Fce": {
    "title": "On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YCzbfs2few": {
    "title": "IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LRnXPxDksA": {
    "title": "Refining Minimax Regret for Unsupervised Environment Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VRv8KjJNuj": {
    "title": "Equivariant Diffusion for Crystal Structure Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dT6ZbSxh33": {
    "title": "Human vs. Generative AI in Content Creation Competition: Symbiosis or Conflict?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GiHo83ozsF": {
    "title": "Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lsavZkUjFZ": {
    "title": "CauDiTS: Causal Disentangled Domain Adaptation of Multivariate Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=weixEb6Wjd": {
    "title": "On The Fairness Impacts of Hardware Selection in Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bdKaQmrM81": {
    "title": "Riemannian coordinate descent algorithms on matrix manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J3xYTh6xtL": {
    "title": "Learning Label Shift Correction for Test-Agnostic Long-Tailed Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2RQqg2Y7Y6": {
    "title": "Human Alignment of Large Language Models through Online Preference Optimisation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j4HtfTqr0f": {
    "title": "MILP-FBGen: LP/MILP Instance Generation with Feasibility/Boundedness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJx1K4lAAX": {
    "title": "Multi-View Stochastic Block Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SWrwurHAeq": {
    "title": "SiT: Symmetry-invariant Transformers for Generalisation in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7wgXuNOF0V": {
    "title": "Boximator: Generating Rich and Controllable Motions for Video Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGR3vpX63X": {
    "title": "TSLANet: Rethinking Transformers for Time Series Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UqoG0YRfQx": {
    "title": "Bring Your Own (Non-Robust) Algorithm to Solve Robust MDPs by Estimating The Worst Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ePDnv4xESI": {
    "title": "Few-shot Adaptation to Distribution Shifts By Mixing Source and Target Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C0sGIO2MZN": {
    "title": "Toward Availability Attacks in 3D Point Clouds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OndZHBUA1G": {
    "title": "A Study of First-Order Methods with a Deterministic Relative-Error Gradient Oracle",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SPBxFwIdMk": {
    "title": "Mapping the Multiverse of Latent Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BtbijvkWLC": {
    "title": "A connection between Tempering and Entropic Mirror Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z8sYc334fU": {
    "title": "What is Dataset Distillation Learning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AFAX28TdO4": {
    "title": "DFlow: A Generative Model Combining Denoising AutoEncoder and Normalizing Flow for High Fidelity Waveform Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycLHJuLYuD": {
    "title": "Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N3ZrpSCJcJ": {
    "title": "Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2JYOxcGlRe": {
    "title": "Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=limyQ1Kk0k": {
    "title": "Spike Distance Function as a Learning Objective for Spike Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cHJAUdam3i": {
    "title": "Discovering Mixtures of Structural Causal Models from Time Series Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xl82CcbYaT": {
    "title": "An Analysis of Linear Time Series Forecasting Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OERwuPzHdh": {
    "title": "DNA-SE: Towards Deep Neural-Nets Assisted Semiparametric Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XGGcnKelda": {
    "title": "Enforcing Constraints in RNA Secondary Structure Predictions: A Post-Processing Framework Based on the Assignment Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E4qjDAdVte": {
    "title": "From Fourier to Neural ODEs: Flow Matching for Modeling Complex Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ztn8FCR1td": {
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJ9BoXAfFa": {
    "title": "Executable Code Actions Elicit Better LLM Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aj18fUB6Th": {
    "title": "Two-timescale Derivative Free Optimization for Performative Prediction with Markovian Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Eew3yUQQtE": {
    "title": "On the Identifiability of Switching Dynamical Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CmXkdlO6JJ": {
    "title": "Implicit Bias of AdamW: $\\ell_\\infty$-Norm Constrained Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDUaH9xHVV": {
    "title": "Model-Based Minimum Bayes Risk Decoding for Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IArWwIim8M": {
    "title": "Activation-Descent Regularization for Input Optimization of ReLU Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NS8z5FinYl": {
    "title": "A Bayesian Approach to Online Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XhH1OKLANY": {
    "title": "LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OenMwDPqWn": {
    "title": "Structure Your Data: Towards Semantic Graph Counterfactuals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gB3E8IwQZy": {
    "title": "Can Gaussian Sketching Converge Faster on a Preconditioned Landscape?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9BWRs6XF8P": {
    "title": "A Contextual Combinatorial Bandit Approach to Negotiation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SL6V527p1F": {
    "title": "Causal Representation Learning Made Identifiable by Grouping of Observational Variables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SDCx6rQV2l": {
    "title": "Confidence-aware Contrastive Learning for Selective Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PMASooqgoq": {
    "title": "Sobolev Space Regularised Pre Density Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=35ahHydjXo": {
    "title": "Learning a Diffusion Model Policy from Rewards via Q-Score Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nbpwNmXTTw": {
    "title": "Conformalized Adaptive Forecasting of Heterogeneous Trajectories",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pAyX8q1IIn": {
    "title": "Stochastic Weakly Convex Optimization beyond Lipschitz Continuity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EhU0xBSP4l": {
    "title": "Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Cp042s1Nc": {
    "title": "Rethinking Generative Large Language Model Evaluation for Semantic Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7XZKzQtooN": {
    "title": "Online Matrix Completion: A Collaborative Approach with Hott Items",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pPNMhdYMaz": {
    "title": "Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Htw0bSgjXE": {
    "title": "CurBench: Curriculum Learning Benchmark",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kAFevjEYsz": {
    "title": "OODRobustBench: a Benchmark and Large-Scale Analysis of Adversarial Robustness under Distribution Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dMOhgHNYAf": {
    "title": "Compositional Text-to-Image Generation with Dense Blob Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PTGJOUlQ68": {
    "title": "Private Vector Mean Estimation in the Shuffle Model: Optimal Rates Require Many Messages",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4PB1RMsUy4": {
    "title": "SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lrPrkWXqzd": {
    "title": "E$^2$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mbBehLOAqR": {
    "title": "Distributionally Robust Data Valuation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dZsEOFUDew": {
    "title": "Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2NfpFwJfKu": {
    "title": "UniCorn: A Unified Contrastive Learning Approach for Multi-view Molecular Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sHswzNWUW2": {
    "title": "Language-Driven Cross-Modal Classifier for Zero-Shot Multi-Label Image Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRDfBIhrJq": {
    "title": "Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K9NTPRvVRI": {
    "title": "Neighboring Perturbations of Knowledge Editing on Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8onaVSFTEj": {
    "title": "HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0iXp5P77ho": {
    "title": "Tripod: Three Complementary Inductive Biases for Disentangled Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPbcUBcTAk": {
    "title": "Conformal Prediction with Learned Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QFMcXz6e4Y": {
    "title": "Lightweight Image Super-Resolution via Flexible Meta Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SvBLKoBL4q": {
    "title": "MLI Formula: A Nearly Scale-Invariant Solution with Noise Perturbation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2B2U5kkGUA": {
    "title": "On the Duality Between Sharpness-Aware Minimization and Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eQaOb4r6YC": {
    "title": "Fast Decision Boundary based Out-of-Distribution Detector",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=isUSVgS7W1": {
    "title": "EvGGS: A Collaborative Learning Framework for Event-based Generalizable Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a8QpoEJCRI": {
    "title": "SurfPro: Functional Protein Design Based on Continuous Surface",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IuvpVcGUOB": {
    "title": "Correlation-Induced Label Prior for Semi-Supervised Multi-Label Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ErkzxOlOLy": {
    "title": "Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gTBjkJvadC": {
    "title": "Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JGL39NaARS": {
    "title": "MFTN: A Multi-scale Feature Transfer Network Based on IMatchFormer for Hyperspectral Image Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zji9DLksTz": {
    "title": "Flexible Residual Binarization for Image Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tDMlQkJRhZ": {
    "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vsy21Xodrt": {
    "title": "Efficient Contrastive Learning for Fast and Accurate Inference on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YlJy1FcM9E": {
    "title": "Supervised Matrix Factorization: Local Landscape Analysis and Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IC9UZ8lm25": {
    "title": "MultiMax: Sparse and Multi-Modal Attention Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aw6L8sB2Ts": {
    "title": "Towards Theoretical Understandings of Self-Consuming Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vG7YpsJT74": {
    "title": "Gradient Compressed Sensing: A Query-Efficient Gradient Estimator for High-Dimensional Zeroth-Order Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xPypr0kufs": {
    "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MMMHufVc2v": {
    "title": "The Non-linear $F$-Design and Applications to Interactive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jU6iPouOZ6": {
    "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6DBvBcW770": {
    "title": "Sub-token ViT Embedding via Stochastic Resonance Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JPNBFWQ9H2": {
    "title": "Bifurcated Attention for Single-Context Large-Batch Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LGhtl9ktop": {
    "title": "Switchable Decision: Dynamic Neural Generation Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p0MGN0LSnx": {
    "title": "Bridging Model Heterogeneity in Federated Learning via Uncertainty-based Asymmetrical Reciprocity Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=odCl49tWA6": {
    "title": "Uniformly Stable Algorithms for Adversarial Training and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WIaZFk02fI": {
    "title": "An Empirical Study of Realized GNN Expressiveness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zw7TcnTmHj": {
    "title": "Minimum-Norm Interpolation Under Covariate Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=znKAWRZSF9": {
    "title": "S3GCL: Spectral, Swift, Spatial Graph Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RYmmgedVjR": {
    "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0XDO74NlOd": {
    "title": "On the Role of Edge Dependency in Graph Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qIOSNyPPwB": {
    "title": "Graph Neural Network Explanations are Fragile",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5cm2jGct2W": {
    "title": "On the Maximal Local Disparity of Fairness-Aware Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ho1l6RZNB": {
    "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FMa4c5NhOe": {
    "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XoencoHWy7": {
    "title": "Non-confusing Generation of Customized Concepts in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FNKnLhLuhY": {
    "title": "Towards General Neural Surrogate Solvers with Specialized Neural Accelerators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3XG69ZmfsB": {
    "title": "FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mDw42ZanmE": {
    "title": "A Multimodal Automated Interpretability Agent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EfUrTeuUfy": {
    "title": "Junk DNA Hypothesis: Pruning Small Pre-Trained Weights $\\textit{Irreversibly}$ and $\\textit{Monotonically}$ Impairs ``Difficult\" Downstream Tasks in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dcwUGaK9sQ": {
    "title": "On Which Nodes Does GCN Fail? Enhancing GCN From the Node Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yHRxnhKyEJ": {
    "title": "Provable Benefits of Local Steps in Heterogeneous Federated Learning for Neural Networks: A Feature Learning Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dmfvHU1LNF": {
    "title": "ACPO: A Policy Optimization Algorithm for Average MDPs with Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xe7n2ZqpBP": {
    "title": "Position: Benchmarking is Limited in Reinforcement Learning Research",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R1auM3tLPE": {
    "title": "Efficient Online Set-valued Classification with Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=051jaf8MQy": {
    "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nue7KgVZ6e": {
    "title": "Multigroup Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sbl2keQEML": {
    "title": "Representation Surgery for Multi-Task Model Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BajM6YzKvm": {
    "title": "Two-sided Competing Matching Recommendation Markets With Quota and Complementary Preferences Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WPfYVdJHPk": {
    "title": "Position: Exploring the Robustness of Pipeline-Parallelism-Based Decentralized Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=teteOa9nJ9": {
    "title": "Batch Singular Value Polarization and Weighted Semantic Augmentation for Universal Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aK1FyEP2Sn": {
    "title": "Copula-Nested Spectral Kernel Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TtSFg4s3F0": {
    "title": "Rethinking DP-SGD in Discrete Domain: Exploring Logistic Distribution in the Realm of signSGD",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l7vQQi0I2d": {
    "title": "On the Feasibility of Single-Pass Full-Capacity Learning in Linear Threshold Neurons with Binary Input Vectors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mv9beA1wDF": {
    "title": "Learning Surrogates for Offline Black-Box Optimization via Gradient Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OHFxcU9jwW": {
    "title": "Diffusion-based Missing-view Generation With the Application on Incomplete Multi-view Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NUAbSFqyqb": {
    "title": "Diffusion Language Models Are Versatile Protein Learners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YEQM0asWCH": {
    "title": "Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zj7YuTE4t8": {
    "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LH6R06NxdB": {
    "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8t8zBaGFar": {
    "title": "Approximate Nearest Neighbor Search with Window Filters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=283cGgWfM2": {
    "title": "ESM All-Atom: Multi-Scale Protein Language Model for Unified Molecular Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ax90jQPbgF": {
    "title": "Learning Constraints from Offline Demonstrations via Superior Distribution Correction Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zCmMkWK4Ly": {
    "title": "Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WYi3WKZjYe": {
    "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rmEgJ7bhuZ": {
    "title": "Listening to the noise: Blind Denoising with Gibbs Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NeO2hoSexj": {
    "title": "Augmenting Decision with Hypothesis in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TqcZfMZjgM": {
    "title": "PinNet: Pinpoint Instructive Information for Retrieval Augmented Code-to-Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iOEReiiTit": {
    "title": "Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mUVydzrkgz": {
    "title": "Adaptive Accompaniment with ReaLchords",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJmGl01S4l": {
    "title": "Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XPP6K57bop": {
    "title": "Stability Evaluation through Distributional Perturbation Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=stMhi1Sn2G": {
    "title": "Accelerated Speculative Sampling Based on Tree Monte Carlo",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gxOQEMRbRa": {
    "title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkN6wrYXe5": {
    "title": "A Federated Stochastic Multi-level Compositional Minimax Algorithm for Deep AUC Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rIc9adYbH2": {
    "title": "GNNs Also Deserve Editing, and They Need It More Than Once",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WUi1AqhKn5": {
    "title": "One Size Fits All for Semantic Shifts: Adaptive Prompt Tuning for Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KB6slOUQP9": {
    "title": "Accelerating Convergence of Score-Based Diffusion Models, Provably",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2LH6Va7L2": {
    "title": "How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=muBJPCIqZT": {
    "title": "Soft Prompt Recovers Compressed LLMs, Transferably",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PpPZ6W7rxy": {
    "title": "Efficient Exploration for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nMWxLnSBGW": {
    "title": "SHINE: Shielding Backdoors in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wilej5VnqL": {
    "title": "InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Ub6nLqdMo": {
    "title": "A Universal Class of Sharpness-Aware Minimization Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J1NIXxiDbu": {
    "title": "PANDA: Expanded Width-Aware Message Passing Beyond Rewiring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ecvuJWE1YY": {
    "title": "Deletion-Anticipative Data Selection with a Limited Budget",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dHXKCyaIkp": {
    "title": "Deep Functional Factor Models: Forecasting High-Dimensional Functional Time Series via Bayesian Nonparametric Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DYd4vyyhUu": {
    "title": "Optimal Kernel Choice for Score Function-based Causal Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MKzgqtRtGY": {
    "title": "ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wG2SgnH6Zv": {
    "title": "A Unified Framework for Learning with Nonlinear Model Classes from Arbitrary Linear Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j5wf1NNhFs": {
    "title": "Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PjiRSyUt7e": {
    "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OrVl8R13Wy": {
    "title": "Sparse Cocktail: Every Sparse Pattern Every Sparse Ratio All At Once",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gu3nacA9AH": {
    "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZctlF8RlV4": {
    "title": "OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XKxuTZRCXq": {
    "title": "Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDw4FxMubj": {
    "title": "Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUmXcewb83": {
    "title": "Foundations of Testing for Finite-Sample Causal Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BIMSHniyCP": {
    "title": "Position: Relational Deep Learning - Graph Representation Learning on Relational Databases",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bb8pOvWIe4": {
    "title": "Causal Inference out of Control: Estimating Performativity without Treatment Randomization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9GbAea74O6": {
    "title": "REST: Efficient and Accelerated EEG Seizure Analysis through Residual State Updates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LM7j0zrUZB": {
    "title": "Test-Time Regret Minimization in Meta Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l4H7Hv7LhJ": {
    "title": "Multi-group Learning for Hierarchical Groups",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Fs1LvjYQW": {
    "title": "MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p7gpooFIr3": {
    "title": "Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nCZYRBK1J4": {
    "title": "Learning Coverage Paths in Unknown Environments with Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wwItuHdus6": {
    "title": "A Computational Framework for Solving Wasserstein Lagrangian Flows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DRGgT7SyC7": {
    "title": "Sparsest Models Elude Pruning: An Exposé of Pruning's Current Capabilities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1oU4FKpVx5": {
    "title": "A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sKjcrAC4eZ": {
    "title": "Sample Complexity Bounds for Estimating Probability Divergences under Invariances",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DJXt63RLO1": {
    "title": "Deconstructing the Goldilocks Zone of Neural Network Initialization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64MQCia06B": {
    "title": "Deep Stochastic Mechanics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6djDWVTUEq": {
    "title": "Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DCmahCZJYb": {
    "title": "Constrained Exploration via Reflected Replica Exchange Stochastic Gradient Langevin Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9L7BZiTtJR": {
    "title": "Tilting the Odds at the Lottery: the Interplay of Overparameterisation and Curricula in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cAWbm9KRZO": {
    "title": "Transforming and Combining Rewards for Aligning Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MSFxOMM0gK": {
    "title": "A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case Graph Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xF656w37Mj": {
    "title": "Online Algorithms with Uncertainty-Quantified Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ad5I6No9G1": {
    "title": "Why Do You Grok? A Theoretical Analysis on Grokking Modular Addition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CTEMHDSwIj": {
    "title": "Understanding Unimodal Bias in Multimodal Deep Linear Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lt8Lk7IQ5b": {
    "title": "COPAL: Continual Pruning in Large Language Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nB6ERIud2y": {
    "title": "Combining Experimental and Historical Data for Policy Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lfp5Dk1xb6": {
    "title": "Improving Token-Based World Models with Parallel Observation Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4iy0q0carb": {
    "title": "Equivariant Frames and the Impossibility of Continuous Canonicalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0mYAK6Yhhm": {
    "title": "CLIPZyme: Reaction-Conditioned Virtual Screening of Enzymes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rGCvMARXkG": {
    "title": "PASOA- PArticle baSed Bayesian Optimal Adaptive design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W97gFmrKe6": {
    "title": "Spectral Phase Transition and Optimal PCA in Block-Structured Spiked Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZVmMV3AHjC": {
    "title": "Improving Sample Efficiency of Model-Free Algorithms for Zero-Sum Markov Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r9XICONppE": {
    "title": "Reweighted Solutions for Weighted Low Rank Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sjv5RcqfuH": {
    "title": "GATE: How to Keep Out Intrusive Neighbors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xl2yU3dsHK": {
    "title": "Improved Differentially Private and Lazy Online Convex Optimization: Lower Regret without Smoothness Requirements",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uzb45nolTb": {
    "title": "No Free Prune: Information-Theoretic Barriers to Pruning at Initialization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=61RlaY9EIn": {
    "title": "MaSS: Multi-attribute Selective Suppression for Utility-preserving Data Transformation from an Information-theoretic Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pD9BTIDUoX": {
    "title": "Revealing Vision-Language Integration in the Brain with Multimodal Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ElVHUWyL3n": {
    "title": "Dual Operating Modes of In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PSzyBN7LIA": {
    "title": "An Online Optimization Perspective on First-Order and Zero-Order Decentralized Nonsmooth Nonconvex Stochastic Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LhAuVPWq6q": {
    "title": "Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=szRHR9XGrY": {
    "title": "Advancing Dynamic Sparse Training by Exploring Optimization Opportunities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DRBgNQ2N7U": {
    "title": "Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70jplnkLMe": {
    "title": "PPFLOW: Target-Aware Peptide Design with Torsional Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6pHP51F55x": {
    "title": "GeoAB: Towards Realistic Antibody Design and Reliable Affinity Maturation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uCdcXRuHnC": {
    "title": "An amortized approach to non-linear mixed-effects modeling based on neural posterior estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yoqdlynCRs": {
    "title": "Scaling Laws for Fine-Grained Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oBYv73nOoA": {
    "title": "SPADE: Sparsity-Guided Debugging for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MOrvoYrlOg": {
    "title": "Position: Optimization in SciML Should Employ the Function Space Geometry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EKye56rLuv": {
    "title": "FairProof : Confidential and Certifiable Fairness for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZNuYKtoOZ": {
    "title": "Continuous Treatment Effects with Surrogate Outcomes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=43HZG9zwaj": {
    "title": "Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a1Olc2QhPv": {
    "title": "PAC-Bayesian Error Bound, via Rényi Divergence, for a Class of Linear Time-Invariant State-Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dh8k41g775": {
    "title": "LQER: Low-Rank Quantization Error Reconstruction for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b0lxGL2n3d": {
    "title": "ILILT: Implicit Learning of Inverse Lithography Technologies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTD3WoQyFR": {
    "title": "Learning to Explore in POMDPs with Informational Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3VnSgdget6": {
    "title": "Provably Better Explanations with Optimized Aggregation of Feature Attributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H5FDHzrWe2": {
    "title": "Stealthy Imitation: Reward-guided Environment-free Policy Stealing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0bGsVoumFL": {
    "title": "The Entropy Enigma: Success and Failure of Entropy Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4dxR7awO5n": {
    "title": "Unveiling Privacy, Memorization, and Input Curvature Links",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=46vXhZn7lN": {
    "title": "Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CtgJUQxmEo": {
    "title": "Floating Anchor Diffusion Model for Multi-motif Scaffolding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JtkruFHcRK": {
    "title": "Uncertainty Estimation by Density Aware Evidential Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=njwv9BsGHF": {
    "title": "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BmPWtzL7Eq": {
    "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8nd1yBRCDl": {
    "title": "EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nSGnx8lNJ6": {
    "title": "Enhancing Value Function Estimation through First-Order State-Action Dynamics in Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y8KsHT1kTV": {
    "title": "Emergence of In-Context Reinforcement Learning from Noise Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GhPFmTJNfj": {
    "title": "Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PKdege0U6Z": {
    "title": "Graph Mixup on Approximate Gromov–Wasserstein Geodesics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pp3v2ch5Sd": {
    "title": "In-Context Reinforcement Learning for Variable Action Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JNeeRjKbuH": {
    "title": "Differentially Private Post-Processing for Fair Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YuGnRORkJm": {
    "title": "Sample Average Approximation for Conditional Stochastic Optimization with Dependent Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yUPBkPKzHw": {
    "title": "Adapting Static Fairness to Sequential Decision-Making: Bias Mitigation Strategies towards Equal Long-term Benefit Rate",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duRRoGeoQT": {
    "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8GYclcxQXB": {
    "title": "Bridging discrete and continuous state spaces: Exploring the Ehrenfest process in time-continuous diffusion models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cFDaYtZR4u": {
    "title": "Towards Causal Foundation Model: on Duality between Optimal Balancing and Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JKPhWzp7Oi": {
    "title": "Improved Stability and Generalization Guarantees of the Decentralized SGD Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cVp8blEw2i": {
    "title": "FESSNC: Fast Exponentially Stable and Safe Neural Controller",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=URtUYfC3GA": {
    "title": "WAVES: Benchmarking the Robustness of Image Watermarks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JOKOsJHSao": {
    "title": "Information-Directed Pessimism for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AaTYLZQPyC": {
    "title": "PcLast: Discovering Plannable Continuous Latent States",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZGEICuuUJo": {
    "title": "The Fundamental Limits of Least-Privilege Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tTtSnpH4fc": {
    "title": "Finite Time Logarithmic Regret Bounds for Self-Tuning Regulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nvfZgdHtHc": {
    "title": "Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse Training Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4BWCecFEcQ": {
    "title": "PerceptAnon: Exploring the Human Perception of Image Anonymization Beyond Pseudonymization for GDPR",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zrQIc9mQQN": {
    "title": "Rethinking Independent Cross-Entropy Loss For Graph-Structured Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LLdeUPOUXk": {
    "title": "Decentralized Convex Finite-Sum Optimization with Better Dependence on Condition Numbers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gSrruVd4": {
    "title": "On the Independence Assumption in Neurosymbolic Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lb8G2dZjcB": {
    "title": "Sign Rank Limitations for Inner Product Graph Decoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YvPNwLedpQ": {
    "title": "Non-stationary Online Convex Optimization with Arbitrary Delays",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0rV7VIrcjX": {
    "title": "Graph External Attention Enhanced Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oSOZ31ISBV": {
    "title": "On the sample complexity of conditional independence testing with Von Mises estimator with application to causal discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v6eaD7Wekw": {
    "title": "Adaptive Robust Learning using Latent Bernoulli Variables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lpHjmPvxW1": {
    "title": "TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hdpv6mall8": {
    "title": "Position: Machine Learning-powered Assessments of the EU Digital Services Act Aid Quantify Policy Impacts on Online Harms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v6tAdeCXKH": {
    "title": "Balancing Similarity and Complementarity for Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zII3Olw7cr": {
    "title": "Transitional Uncertainty with Layered Intermediate Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pmsPKIBAu6": {
    "title": "More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mjh7AOWozN": {
    "title": "Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bqgtkBDkNs": {
    "title": "Challenges and Considerations in the Evaluation of Bayesian Causal Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PzjDsfYwLC": {
    "title": "Diagnosing the Compositional Knowledge of Vision Language Models from a Game-Theoretic View",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=spOpHW1No2": {
    "title": "On a Combinatorial Problem Arising in Machine Teaching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1N7pjXKkx8": {
    "title": "PID: Prompt-Independent Data Protection Against Latent Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HLHQxMydFk": {
    "title": "Bayesian Design Principles for Offline-to-Online Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f8G2KSCSdp": {
    "title": "Amend to Alignment: Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60vx5AfM3C": {
    "title": "No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j5csKrtyAe": {
    "title": "Position: Fundamental Limitations of LLM Censorship Necessitate New Approaches",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0M2tNui8jX": {
    "title": "Global Reinforcement Learning : Beyond Linear and Convex Rewards via Submodular Semi-gradient Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d5tJWH5yCi": {
    "title": "Fully-Dynamic Approximate Decision Trees With Worst-Case Update Time Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AocOA4h3bu": {
    "title": "Sequential Disentanglement by Extracting Static Information From A Single Sequence Element",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SRmZw7nEGW": {
    "title": "UniAudio: Towards Universal Audio Generation with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M4ejBhNNrn": {
    "title": "No Double Descent in Principal Component Regression: A High-Dimensional Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uyhjKoaIQa": {
    "title": "Delaunay Graph: Addressing Over-Squashing and Over-Smoothing Using Delaunay Triangulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eP3vsbB5wW": {
    "title": "Ensemble Pruning for Out-of-distribution Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=23tMOWscus": {
    "title": "Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1zFkjbTgwC": {
    "title": "Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSQAf5YlvN": {
    "title": "Online Learning and Information Exponents: The Importance of Batch size & Time/Complexity Tradeoffs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Izv7gBnap3": {
    "title": "Byzantine-Robust Federated Learning: Impact of Client Subsampling and Local Updates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fPwWfoyxL1": {
    "title": "Riemannian Accelerated Zeroth-order Algorithm: Improved Robustness and Lower Query Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bBzlapzeR1": {
    "title": "High-Dimensional Kernel Methods under Covariate Shift: Data-Dependent Implicit Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JCG0KTPVYy": {
    "title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TaAqeo7lUh": {
    "title": "Data Engineering for Scaling Language Models to 128K Context",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6FtAXU4ean": {
    "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AzUCfhJ9Bs": {
    "title": "On the Weight Dynamics of Deep Normalized Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FV3kY9FBW6": {
    "title": "Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=da7MMwICjC": {
    "title": "Reparameterized Importance Sampling for Robust Variational Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryDa4mS18V": {
    "title": "SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yShA4VPYZB": {
    "title": "${\\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qOMQ0UGLYl": {
    "title": "DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J9YKDvqr65": {
    "title": "Improving Sharpness-Aware Minimization by Lookahead",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kf9CqdI8Rb": {
    "title": "Learning Linear Block Error Correction Codes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zxxSJAVQPc": {
    "title": "A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w5oUo0LhO1": {
    "title": "Kernel Semi-Implicit Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2bUFIsg2f5": {
    "title": "Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gDQuupz8mm": {
    "title": "Small-loss Adaptive Regret for Online Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1HDrfUahXv": {
    "title": "Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g1Gf0hoPSz": {
    "title": "Latent variable model for high-dimensional point process with structured missingness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tdomF3PW6A": {
    "title": "Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KVa4i4RR1O": {
    "title": "convSeq: Fast and Scalable Method for Detecting Patterns in Spike Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CjVWen8aJL": {
    "title": "Accelerating Parallel Sampling of Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NBAc36V00H": {
    "title": "Residual Quantization with Implicit Neural Codebooks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8h0x12p3zq": {
    "title": "Generalization Error of Graph Neural Networks in the Mean-field Regime",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykZYLBcA9g": {
    "title": "Learning with Complementary Labels Revisited: The Selected-Completely-at-Random Setting Is More Practical",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d2vONO90Rw": {
    "title": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language Models with Pinpoint Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MDAg5Q7IsI": {
    "title": "Predicting Dose-Response Curves with Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6wVlH96oMX": {
    "title": "Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4XxsheIbtn": {
    "title": "Detecting Any instruction-to-answer interaction relationship:Universal Instruction-to-Answer Navigator for Med-VQA",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZFYBnLljtT": {
    "title": "Getting the most out of your tokenizer for pre-training and domain adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51iwkioZpn": {
    "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykgZk6vFrh": {
    "title": "Incentivized Learning in Principal-Agent Bandit Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0pSTzCnEmi": {
    "title": "Improving Neural Additive Models with Bayesian Principles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d5jXW2H4gg": {
    "title": "KernelSHAP-IQ: Weighted Least Square Optimization for Shapley Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WUQ4YzIQt2": {
    "title": "Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lIYtJtpJR0": {
    "title": "Robust Stable Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3nlBesNxcm": {
    "title": "Regression Learning with Limited Observations of Multivariate Outcomes and Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=klKk9ETAyU": {
    "title": "High-Probability Bound for Non-Smooth Non-Convex Stochastic Optimization with Heavy Tails",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nLRKnO74RB": {
    "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZyOz9VEg4": {
    "title": "Optimal Recurrent Network Topologies for Dynamical Systems Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wlOaG9g0uq": {
    "title": "The Good, The Bad, and Why: Unveiling Emotions in Generative AI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FG5hjRBtpm": {
    "title": "Jacobian Regularizer-based Neural Granger Causality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nYsh5GFIqX": {
    "title": "video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ez3Lckpe4l": {
    "title": "The Role of Learning Algorithms in Collective Action",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWng0GXeE4": {
    "title": "Mind the Boundary: Coreset Selection via Reconstructing the Decision Boundary",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BoPj12CnAn": {
    "title": "Weighted distance nearest neighbor condensing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EhPpZV6KLk": {
    "title": "Surprisingly Strong Performance Prediction with Neural Graph Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D7wi9LIE6i": {
    "title": "Improved Dimensionality Dependence for Zeroth-Order Optimisation over Cross-Polytopes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vXUqOCsbj8": {
    "title": "On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8AeuhCgRRv": {
    "title": "An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AlJkqMnyjL": {
    "title": "Consistent Submodular Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uo3LNg5SLY": {
    "title": "Explain Temporal Black-Box Models via Functional Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWUU0LwwMp": {
    "title": "Position: TrustLLM: Trustworthiness in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFvoL7BOny": {
    "title": "Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ZJLNLZIpk": {
    "title": "Towards Interpretable Deep Local Learning with Successive Gradient Reconciliation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O4cHTxW9BS": {
    "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XecUTmB9yD": {
    "title": "FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ar174skI9u": {
    "title": "DPN: Decoupling Partition and Navigation for Neural Solvers of Min-max Vehicle Routing Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bZ4fzw1iz7": {
    "title": "Split-and-Denoise: Protect large language model inference with local differential privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zgiT3uxvCF": {
    "title": "Conditionally-Conjugate Gaussian Process Factor Analysis for Spike Count Data via Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yj8h567Ia7": {
    "title": "Amortizing Pragmatic Program Synthesis with Rankings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UdXDUDxq11": {
    "title": "Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PHUAG63Efe": {
    "title": "AegisFL: Efficient and Flexible Privacy-Preserving Byzantine-Robust Cross-silo Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jXn1qIcjyG": {
    "title": "Conditional Language Learning with Context",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hRX1o7FBhT": {
    "title": "Progressive Inference: Explaining Decoder-Only Sequence Classification Models Using Intermediate Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jPaEOH56JB": {
    "title": "Learning Divergence Fields for Shift-Robust Graph Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q0vILV7zAw": {
    "title": "Sparse-to-dense Multimodal Image Registration via Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NbOlmrB59Z": {
    "title": "SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xS2YKQlBIZ": {
    "title": "Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uLonuOfrwp": {
    "title": "Statistical Test for Attention Maps in Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=icijMMWwdG": {
    "title": "Best of Both Worlds Guarantees for Smoothed Online Quadratic Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zatLnLvbs8": {
    "title": "Contrastive Predict-and-Search for Mixed Integer Linear Programs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PAbkWU0KDG": {
    "title": "Limited Preference Aided Imitation Learning from Imperfect Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VOcsmIBiXE": {
    "title": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6TM62kpI5c": {
    "title": "Rethinking the Flat Minima Searching in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kzz0kn546b": {
    "title": "Provably Neural Active Learning Succeeds via Prioritizing Perplexing Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTYuORAMaP": {
    "title": "Efficient Stochastic Approximation of Minimax Excess Risk Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CV9PiQGt0i": {
    "title": "Improving Generalization in Offline Reinforcement Learning via Adversarial Data Splitting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ap1MmUqO6": {
    "title": "Partial Multi-View Multi-Label Classification via Semantic Invariance Learning and Prototype Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQwSbv0BR4": {
    "title": "Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A9fLbXLRTK": {
    "title": "Learning Associative Memories with Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pY2UpspnBB": {
    "title": "Open-Vocabulary Calibration for Fine-tuned CLIP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7emOSb5UfX": {
    "title": "Adaptive Text Watermark for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h8aTi32tul": {
    "title": "SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DrE7jVF4VW": {
    "title": "Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kc4dZYJlJG": {
    "title": "FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L057s2Rq8O": {
    "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DsVzHj7jcA": {
    "title": "Stability and Generalization for Stochastic Recursive Momentum-based Algorithms for (Strongly-)Convex One to $K$-Level Stochastic Optimizations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzFTrzo0Cp": {
    "title": "MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B48Pzc4oKi": {
    "title": "LLaGA: Large Language and Graph Assistant",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qoOt02l2WC": {
    "title": "Manifold Integrated Gradients: Riemannian Geometry for Feature Attribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KLmWRMg6nL": {
    "title": "Fair Resource Allocation in Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UWjqrMmFp": {
    "title": "Coresets for Multiple $\\ell_p$ Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQWVUbqQtQ": {
    "title": "Position: The Reasonable Person Standard for AI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DCNCwaMJjI": {
    "title": "TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=snhurpZt63": {
    "title": "Diving into Underwater: Segment Anything Model Guided Underwater Salient Instance Segmentation and A Large-scale Dataset",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R4Ng8zYaiz": {
    "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E4ItiEU8Iu": {
    "title": "Run-Time Task Composition with Safety Semantics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8dX4YnosqG": {
    "title": "On the Error-Propagation of Inexact Hotelling's Deflation for Principal Component Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Irkcamqg4d": {
    "title": "Binary Decomposition: A Problem Transformation Perspective for Open-Set Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dGDFZM018a": {
    "title": "Sign is Not a Remedy: Multiset-to-Multiset Message Passing for Learning on Heterophilic Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LyJ85kgHFe": {
    "title": "$\\texttt{MoE-RBench}$: Towards Building Reliable Language Models with Sparse Mixture-of-Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Al5GlVytqi": {
    "title": "Knowledge Graphs Can be Learned with Just Intersection Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kao5hRX9YA": {
    "title": "BAT: Learning to Reason about Spatial Sounds with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d5LURMSfTx": {
    "title": "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FKkkdyRdsD": {
    "title": "Faster Maximum Inner Product Search in High Dimensions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aRZjRj41WQ": {
    "title": "Self-attention Networks Localize When QK-eigenspectrum Concentrates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pAzDdYzEva": {
    "title": "QORA: Zero-Shot Transfer via Interpretable Object-Relational Model Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NQ6KDfSDFK": {
    "title": "Exploring Training on Heterogeneous Data with Mixture of Low-rank Adapters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IWi6iLZeRG": {
    "title": "Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6n99bIxb3r": {
    "title": "Tackling Prevalent Conditions in Unsupervised Combinatorial Optimization: Cardinality, Minimum, Covering, and More",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WJn1BAx9aj": {
    "title": "Robust Graph Matching when Nodes are Corrupt",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LIPGadocTe": {
    "title": "Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wuQ2DRPAuy": {
    "title": "Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UZstTlLq1E": {
    "title": "Initial Guessing Bias: How Untrained Networks Favor Some Classes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kKWjZoaRLv": {
    "title": "Learning Latent Structures in Network Games via Data-Dependent Gated-Prior Graph Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjgRKbdYR7": {
    "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QXqXGDapkQ": {
    "title": "SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity, ECG and Respiratory Signals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1n3aC5rvdE": {
    "title": "Variational Linearized Laplace Approximation for Bayesian Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kVgpa1rfLO": {
    "title": "Towards the Theory of Unsupervised Federated Learning: Non-asymptotic Analysis of Federated EM Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HUJK9dFOW6": {
    "title": "Differentiable Distributionally Robust Optimization Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Msjovr9hUe": {
    "title": "Local Feature Selection without Label or Feature Leakage for Interpretable Machine Learning Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JJZBZW28Gn": {
    "title": "Stable Differentiable Causal Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UKHfmzLR7P": {
    "title": "Collaborative Learning with Different Labeling Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hh8pUBfxXh": {
    "title": "MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7sgqXa4aNM": {
    "title": "A General Framework for Learning from Weak Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TWu1fzFJm0": {
    "title": "A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EYOo48YGhy": {
    "title": "Exploring the Enigma of Neural Dynamics Through A Scattering-Transform Mixer Landscape for Riemannian Manifold",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o9uOuIwhZK": {
    "title": "Learning Latent Space Hierarchical EBM Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E6Nm3x7acv": {
    "title": "Contextual Feature Selection with Conditional Stochastic Gates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4PuM6iGPPi": {
    "title": "Deep Fusion: Efficient Network Training via Pre-trained Initializations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6rA0kAHT1": {
    "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZ7UKgK4c1": {
    "title": "Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iE2lMjeXRR": {
    "title": "Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e5tA3Apbmy": {
    "title": "Adaptively Learning to Select-Rank in Online Platforms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89kZWloYQx": {
    "title": "Understanding Forgetting in Continual Learning with Linear Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6h6ovHcC9G": {
    "title": "Faster Streaming and Scalable Algorithms for Finding Directed Dense Subgraphs in Large Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mXLcbRBA8v": {
    "title": "Beyond the ROC Curve: Classification Trees Using Cost-Optimal Curves, with Application to Imbalanced Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TTZXl9WYFF": {
    "title": "Multi-Agent Reinforcement Learning with Hierarchical Coordination for Emergency Responder Stationing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oaACFfNbXl": {
    "title": "The Relative Value of Prediction in Algorithmic Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HvwOtYzHBX": {
    "title": "LLark: A Multimodal Instruction-Following Language Model for Music",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UjDp4Wkq2V": {
    "title": "On dimensionality of feature vectors in MPNNs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Su0qe33cWA": {
    "title": "Wasserstein Wormhole: Scalable Optimal Transport Distance with Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uydQ2W41KO": {
    "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSAEhcdB9N": {
    "title": "Private Heterogeneous Federated Learning Without a Trusted Server Revisited: Error-Optimal and Communication-Efficient Algorithms for Convex Losses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CiZN2OATRp": {
    "title": "Statistical Inference Under Constrained Selection Bias",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FpbKoIPHxb": {
    "title": "Combinatorial Approximations for Cluster Deletion: Simpler, Faster, and Better",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JA6ThxAmth": {
    "title": "Understanding Inter-Concept Relationships in Concept-Based Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cw6Xl0g8a5": {
    "title": "Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0b7txvPYlr": {
    "title": "Early Time Classification with Accumulated Accuracy Gap Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hXQOO6VsxH": {
    "title": "Near-Optimal Regret in Linear MDPs with Aggregate Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mcg6jppkwb": {
    "title": "Position: Tensor Networks are a Valuable Asset for Green AI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FHkavpr5Ze": {
    "title": "Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JFIKpzumn": {
    "title": "Multi-Sender Persuasion: A Computational Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=axwrD8F1yq": {
    "title": "Category-Aware Active Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UIxOkdBmxh": {
    "title": "A Persuasive Approach to Combating Misinformation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mw8kNVfdMs": {
    "title": "Attribute Based Interpretable Evaluation Metrics for Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mQ6ZKTSQl": {
    "title": "Prompting a Pretrained Transformer Can Be a Universal Approximator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LlqphyBdeT": {
    "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zkjGpZrIX3": {
    "title": "Trustworthy Actionable Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IgwtflILyj": {
    "title": "Careful with that Scalpel: Improving Gradient Surgery with an EMA",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=76zq8Wkl6Z": {
    "title": "The Pitfalls of Next-Token Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CDnv4vg02f": {
    "title": "Accelerating Iterative Retrieval-augmented Language Model Serving with Speculation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzWFmMySpn": {
    "title": "Robust Multi-Task Learning with Excess Risks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wBr5ozDEKp": {
    "title": "Position: Future Directions in the Theory of Graph Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pmncWWkGMz": {
    "title": "Agent-Specific Effects: A Causal Effect Propagation Analysis in Multi-Agent MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gWEwIlZrbQ": {
    "title": "Accelerating Federated Learning with Quick Distributed Mean Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IckJCzsGVS": {
    "title": "Proteus: Exploring Protein Structure Generation for Enhanced Designability and Efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXsUCgn9Ks": {
    "title": "Fundamental Limitations of Alignment in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3FBO41d4T2": {
    "title": "Logistic Variational Bayes Revisited",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=axl3FAkpik": {
    "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QwgSOwynxD": {
    "title": "A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A9hJvQHEEP": {
    "title": "Quantum Theory and Application of Contextual Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAoiUlz4Bf": {
    "title": "Verifying message-passing neural networks via topology-based bounds tightening",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zkcya47Sq5": {
    "title": "Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K3fEkECWgu": {
    "title": "Structure-based drug design by denoising voxel grids",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qQjUgItPq4": {
    "title": "Controlling Behavioral Diversity in Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RnbobOgbn0": {
    "title": "Projection-Free Online Convex Optimization with Time-Varying Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ZDFn7BDaH": {
    "title": "ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rZD9hV0Bc4": {
    "title": "Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-Loop and Hessian-Free Solution Strategy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iC8l9DI1ZX": {
    "title": "On the Effectiveness of Supervision in Asymmetric Non-Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WajJf47TUi": {
    "title": "Graph Neural PDE Solvers with Conservation and Similarity-Equivariance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYltxxam2t": {
    "title": "A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VW7Jk8KhNC": {
    "title": "Non-parametric Online Change Point Detection on Riemannian Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ckuC9C2FZ": {
    "title": "NeuralIndicator: Implicit Surface Reconstruction from Neural Indicator Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZrM67ZZ5vj": {
    "title": "Bridging Environments and Language with Rendering Functions and Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jWHU4b7Yk6": {
    "title": "SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PykISfqvet": {
    "title": "Density Ratio Estimation with Doubly Strong Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xgoilgLPGD": {
    "title": "Langevin Policy for Safe Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qmUbSAgz08": {
    "title": "Multi-Source Conformal Inference Under Distribution Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JV84NVo1em": {
    "title": "Safe and Robust Subgame Exploitation in Imperfect Information Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cs0Xy6WETl": {
    "title": "Reflective Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cmy38XZlJu": {
    "title": "Hierarchical Integral Probability Metrics: A distance on random probability measures with low sample complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c1AKcA6ry1": {
    "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0NacraIYrA": {
    "title": "Autonomous Sparse Mean-CVaR Portfolio Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=30waYPIZUA": {
    "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pmcusTywXO": {
    "title": "Graph Out-of-Distribution Detection Goes Neighborhood Shaping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DYMj03Gbri": {
    "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=byAXJTk0LH": {
    "title": "Plug-and-Play image restoration with Stochastic deNOising REgularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ksph9pkEDc": {
    "title": "Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3pYoZfcoo": {
    "title": "Conformal Prediction for Deep Classifier via Label Ranking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKcwle8XC9": {
    "title": "In-Context Unlearning: Language Models as Few-Shot Unlearners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mhI5nc5QwX": {
    "title": "LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rReWhol66R": {
    "title": "Contrastive Representation for Data Filtering in Cross-Domain Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w8ei1o9U5y": {
    "title": "Sample-Efficient Multiagent Reinforcement Learning with Reset Replay",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvHWlYTLWe": {
    "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GFfWzAReAc": {
    "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=meItvvCO7X": {
    "title": "Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xLyc5TkFl": {
    "title": "The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YBetKvUlF7": {
    "title": "Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Features Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rk4kmL8aOY": {
    "title": "Reducing Item Discrepancy via Differentially Private Robust Embedding Alignment for Privacy-Preserving Cross Domain Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y4wxCICbD0": {
    "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sra298VMFM": {
    "title": "An Information Theoretic Approach to Interaction-Grounded Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CecY6XiUfu": {
    "title": "Reference Neural Operators: Learning the Smooth Dependence of Solutions of PDEs on Geometric Deformations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HwVZbPbMjw": {
    "title": "Planning, Fast and Slow: Online Reinforcement Learning with Action-Free Offline Data via Multiscale Planners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xZO7SmM12y": {
    "title": "Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VoMPNYTZud": {
    "title": "Towards Realistic Model Selection for Semi-supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=59oXyDTLJv": {
    "title": "Discovering Symmetry Breaking in Physical Systems with Relaxed Group Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0hbeZQm1Se": {
    "title": "DAG-Based Column Generation for Adversarial Team Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5PQhu8flSO": {
    "title": "Detecting and Identifying Selection Structure in Sequential Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XwVkqvyziD": {
    "title": "Generalization Analysis of Stochastic Weight Averaging with General Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e3Dpq3WdMv": {
    "title": "Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JymXv7mkrQ": {
    "title": "Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YxmcEfcgp3": {
    "title": "Neural Tangent Kernels for Axis-Aligned Tree Ensembles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=36jWuAmGRC": {
    "title": "Projection-Free Variance Reduction Methods for Stochastic Constrained Multi-Level Compositional Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jzHmElqpPe": {
    "title": "Position: Foundation Agents as the Paradigm Shift for Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KsUddQl39v": {
    "title": "Improving Accuracy-robustness Trade-off via Pixel Reweighted Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WCVC5wGZyz": {
    "title": "GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WVORGH73Cg": {
    "title": "Criterion Collapse and Loss Distribution Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jr0W36wOBx": {
    "title": "Conformalized Survival Distributions: A Generic Post-Process to Increase Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9PQnc6EWdL": {
    "title": "Accelerating Convergence in Bayesian Few-Shot Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Tzdpjc59k": {
    "title": "Borda Regret Minimization for Generalized Linear Dueling Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JU3xHh1vWw": {
    "title": "Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2zLt2Odckx": {
    "title": "Beyond the Federation: Topology-aware Federated Learning for Generalization to Unseen Clients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cmD5E6ami4": {
    "title": "Symmetric Replay Training: Enhancing Sample Efficiency in Deep Reinforcement Learning for Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iLfk2CwEHA": {
    "title": "DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M5ne8enLcr": {
    "title": "Hypergraph-enhanced Dual Semi-supervised Graph Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RsIMGYzBcv": {
    "title": "Online Resource Allocation with Non-Stationary Customers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6EF0bxcZvT": {
    "title": "Monotone Individual Fairness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZoTIdyExx6": {
    "title": "Discounted Adaptive Online Learning: Towards Better Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IyeXM58vIC": {
    "title": "Total Variation Floodgate for Variable Importance Inference in Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w1d9DOGymR": {
    "title": "Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJUhgvM2u8": {
    "title": "Graph Neural Stochastic Diffusion for Estimating Uncertainty in Node Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iroZNDxFJZ": {
    "title": "Position: What Can Large Language Models Tell Us about Time Series Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l9ga3iQuHt": {
    "title": "Feel-Good Thompson Sampling for Contextual Dueling Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XWkRyIjYDp": {
    "title": "Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qIiPM5CbRY": {
    "title": "On Interpolating Experts and Multi-Armed Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6kMMgmeM2U": {
    "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VZsxhPpu9T": {
    "title": "Rényi Pufferfish Privacy: General Additive Noise Mechanisms and Privacy Amplification by Iteration via Shift Reduction Lemmas",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTiNFCNxM1": {
    "title": "From Biased Selective Labels to Pseudo-Labels: An Expectation-Maximization Framework for Learning from Biased Decisions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xb3IXEBYuw": {
    "title": "Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=shzEkKPrsn": {
    "title": "Online Learning under Budget and ROI Constraints via Weak Adaptivity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c8qWiNiqRY": {
    "title": "A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=idyUNsoZ75": {
    "title": "Evaluating Model Bias Requires Characterizing its Mistakes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xqqccG7gf1": {
    "title": "Membership Inference Attacks on Diffusion Models via Quantile Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HO0g6cHVZx": {
    "title": "EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in Linear Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DA2AiCiCaM": {
    "title": "Mollification Effects of Policy Gradient Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uoved2xD81": {
    "title": "Do Transformer World Models Give Better Policy Gradients?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GUEsK9xJny": {
    "title": "Learning to Scale Logits for Temperature-Conditional GFlowNets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ahEm3l2P6w": {
    "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYeioWoF9u": {
    "title": "Language Models as Semantic Indexers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zwUEk9WpsR": {
    "title": "Understanding Server-Assisted Federated Learning in the Presence of Incomplete Client Participation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5353dJE9Ek": {
    "title": "PAGER: Accurate Failure Characterization in Deep Regression Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kLiDMGJKx1": {
    "title": "Outlier-Efficient Hopfield Layers for Large Transformer-Based Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mNzkumTSVL": {
    "title": "Overcoming Data and Model heterogeneities in Decentralized Federated Learning via Synthetic Anchors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0tYrMtQyPT": {
    "title": "Log Neural Controlled Differential Equations: The Lie Brackets Make A Difference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9QRcp2ubDt": {
    "title": "Centralized Selection with Preferences in the Presence of Biases",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WXg6MJo1FH": {
    "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1WWpIEFdlk": {
    "title": "Correcting Diffusion-Based Perceptual Image Compression with Privileged End-to-End Decoder",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6KLNiRdWH6": {
    "title": "Geometry-Aware Instrumental Variable Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tHBLwSYnLf": {
    "title": "Zero-Shot Reinforcement Learning via Function Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tc3Nmcpmnx": {
    "title": "Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOFjRnJ9mX": {
    "title": "A Space Group Symmetry Informed Network for O(3) Equivariant Crystal Tensor Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OBs0AjXE3F": {
    "title": "KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ClWdplZ12B": {
    "title": "Towards Global Optimality for Practical Average Reward Reinforcement Learning without Mixing Time Oracles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tpYHbEl7P1": {
    "title": "How to Escape Sharp Minima with Random Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OQ7TlOphGX": {
    "title": "Enhancing Trajectory Prediction through Self-Supervised Waypoint Distortion Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yqj3DzIC79": {
    "title": "On the Generalization of Equivariant Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hs9GcILuZN": {
    "title": "Single-Model Attribution of Generative Models Through Final-Layer Inversion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=twm7qPVX1F": {
    "title": "Bivariate Causal Discovery using Bayesian Model Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bic3Vmy2DG": {
    "title": "Proactive Detection of Voice Cloning with Localized Watermarking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wdezvnc9EG": {
    "title": "Perfect Alignment May be Poisonous to Graph Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAXp5dMYv7": {
    "title": "Parallel Affine Transformation Tuning of Markov Chain Monte Carlo",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZAW37OZ6ig": {
    "title": "NExT-Chat: An LMM for Chat, Detection and Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=INb8xV1xmf": {
    "title": "Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GTnn6bNE3j": {
    "title": "Neurodegenerative Brain Network Classification via Adaptive Diffusion with Temporal Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0IDaPnY5d5": {
    "title": "Boosting Reinforcement Learning with Strongly Delayed Feedback Through Auxiliary Short Delays",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=puSMYmHmJW": {
    "title": "Relational Learning in Pre-Trained Models: A Theory from Hypergraph Recovery Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CBcNl5Eo32": {
    "title": "Fast Peer Adaptation with Context-aware Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KNedb3bQ4h": {
    "title": "Efficient Non-stationary Online Learning by Wavelets with Applications to Online Distribution Shift Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JObct1zyTb": {
    "title": "Improving Neural Logic Machines via Failure Reflection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aQl4xiwVBc": {
    "title": "SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gEbl6XNLK6": {
    "title": "Learning to Intervene on Concept Bottlenecks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uqWfZ23O9g": {
    "title": "Amortized Equation Discovery in Hybrid Dynamical Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PDO2Oc1cS1": {
    "title": "High-Order Contrastive Learning with Fine-grained Comparative Levels for Sparse Ordinal Tensor Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MwQ53xAIPs": {
    "title": "Matroid Semi-Bandits in Sublinear Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fuX4hyLPmO": {
    "title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xFCA2yWVs4": {
    "title": "Ai-sampler: Adversarial Learning of Markov kernels with involutive maps",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vjkq5fwsj3": {
    "title": "Graph Automorphism Group Equivariant Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bgP8Rxv2eB": {
    "title": "Unbiased Multi-Label Learning from Crowdsourced Annotations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SPygKwms0X": {
    "title": "A Provable Decision Rule for Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8mKXMnhnFW": {
    "title": "Sharpness-Aware Data Generation for Zero-shot Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EncFNR3hxM": {
    "title": "KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y2WorV5ag6": {
    "title": "Towards a Self-contained Data-driven Global Weather Forecasting Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6yQ5mIYxjj": {
    "title": "Algorithmic Stability Unleashed: Generalization Bounds with Unbounded Losses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dn4B53IcCW": {
    "title": "How Graph Neural Networks Learn: Lessons from Training Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oWYzIodyC4": {
    "title": "BWS: Best Window Selection Based on Sample Scores for Data Pruning across Broad Ranges",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QXEx16jWdN": {
    "title": "Improving Adversarial Energy-Based Model via Diffusion Process",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EEO4Iktfjp": {
    "title": "Modeling Language Tokens as Functionals of Semantic Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YDeZU8Lt5": {
    "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vl9GB3fbht": {
    "title": "Neural Operators with Localized Integral and Differential Kernels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MoTUdh9ZCc": {
    "title": "DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EZcFK8HupF": {
    "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ada9Z68nvb": {
    "title": "Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PLAGGbssT8": {
    "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pDoAjdrMf0": {
    "title": "SF-DQN: Provable Knowledge Transfer using Successor Feature for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d2f2sCXQuI": {
    "title": "GRATH: Gradual Self-Truthifying for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dQveBV9lZl": {
    "title": "Solving Poisson Equations using Neural Walk-on-Spheres",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jmmji1EU3g": {
    "title": "In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ebt5BfRHcW": {
    "title": "Harnessing Hierarchical Label Distribution Variations in Test Agnostic Long-tail Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ISG3l8nXrI": {
    "title": "Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMgpE58PMj": {
    "title": "AdsorbDiff: Adsorbate Placement via Conditional Denoising Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bBkQ51PmjC": {
    "title": "Learning Solution-Aware Transformers for Efficiently Solving Quadratic Assignment Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XoSF46Pc2e": {
    "title": "How to Make the Gradients Small Privately: Improved Rates for Differentially Private Non-Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nn5OPHom8t": {
    "title": "EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S2XgbBCJy0": {
    "title": "Equilibrium of Data Markets with Externality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AqBz54aFyj": {
    "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8STOjGCkfH": {
    "title": "HyperFields: Towards Zero-Shot Generation of NeRFs from Text",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k2axqNsVVO": {
    "title": "Self-Driven Entropy Aggregation for Byzantine-Robust Heterogeneous Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BvBdYSIkpb": {
    "title": "Uncertainty-Aware Reward-Free Exploration with General Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=If6Q9OYfoJ": {
    "title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aVqqoFAavs": {
    "title": "Beyond Regular Grids: Fourier-Based Neural Operators on Arbitrary Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qESG5HaaoJ": {
    "title": "Operator SVD with Neural Networks via Nested Low-Rank Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=syXFAVqx85": {
    "title": "Dirichlet Flow Matching with Applications to DNA Sequence Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YlcSyCz21c": {
    "title": "Enhancing Cross-Modal Fine-Tuning with Gradually Intermediate Modality Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CJbhtpcyGL": {
    "title": "Position: On the Possibilities of AI-Generated Text Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bV9yT24t9B": {
    "title": "Self-Infilling Code Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gq1ajaKhBC": {
    "title": "Rich-Observation Reinforcement Learning with Continuous Latent Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VtqyurB4Af": {
    "title": "Guidance with Spherical Gaussian Constraint for Conditional Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WjNzXeiOSL": {
    "title": "From Generalization Analysis to Optimization Designs for State Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0HUInAsdoo": {
    "title": "OxyGenerator: Reconstructing Global Ocean Deoxygenation Over a Century with Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uLpyWQPyF9": {
    "title": "Scaling Beyond the GPU Memory Limit for Large Mixture-of-Experts Model Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dyfsPNuYCk": {
    "title": "Imitation Learning from Purified Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sf5KYznS2G": {
    "title": "Reflected Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5QWKec0eDF": {
    "title": "Diversified Batch Selection for Training Acceleration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=piujJIF3zs": {
    "title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HfxFasUfbN": {
    "title": "AMPA: Adaptive Mixed Precision Allocation for Low-Bit Integer Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONOtpXLqqw": {
    "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JsPvL6ExK8": {
    "title": "Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lqeVCc9zYq": {
    "title": "SMaRt: Improving GANs with Score Matching Regularity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RKlmOBFwAh": {
    "title": "Et Tu Certifications: Robustness Certificates Yield Better Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nOyj26YdIQ": {
    "title": "Viewing Transformers Through the Lens of Long Convolutions Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9HPoJ6ulgV": {
    "title": "Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlM30j9i80": {
    "title": "Learning 1-Bit Tiny Object Detector with Discriminative Feature Refinement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eVlx8DaG9h": {
    "title": "StrokeNUWA—Tokenizing Strokes for Vector Graphic Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJ6tVNt6Y4": {
    "title": "Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u9qmjV2khT": {
    "title": "A Global Geometric Analysis of Maximal Coding Rate Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RLENZ8pNnn": {
    "title": "Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DhxZVq1ZOo": {
    "title": "Collective Certified Robustness against Graph Injection Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ah1BlQcLv4": {
    "title": "Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HmKMpJXH67": {
    "title": "Stationary Latent Weight Inference for Unreliable Observations from Online Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fz9PaJNViP": {
    "title": "MOKD: Cross-domain Finetuning for Few-shot Classification via Maximizing Optimized Kernel Dependence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VrwIrAa1Lc": {
    "title": "Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mGsF8Q0fGZ": {
    "title": "On Hypothesis Transfer Learning of Functional Linear Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jh7FDDwDBf": {
    "title": "Plug-in Performative Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bmeUeCUMHA": {
    "title": "Sequential Kernel Goodness-of-fit Testing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATRnM8PyQX": {
    "title": "COALA: A Practical and Vision-Centric Federated Learning Platform",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OVn8FpeBpG": {
    "title": "Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c9HddKGiYk": {
    "title": "Bayesian Adaptation of Network Depth and Width for Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pPnkpvBeZN": {
    "title": "Class-Imbalanced Graph Learning without Class Rebalancing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RuH78kOcDi": {
    "title": "Locally Differentially Private Decentralized Stochastic Bilevel Optimization with Guaranteed Convergence Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hr8OXXMb7a": {
    "title": "Stochastic positional embeddings improve masked image modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBGBzV4It3": {
    "title": "Align Your Steps: Optimizing Sampling Schedules in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NQn2tYLv5I": {
    "title": "An Information-Theoretic Analysis of In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ICvWruTEDH": {
    "title": "Graph Adversarial Diffusion Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jtjurj7oIJ": {
    "title": "Position: Scaling Simulation is Neither Necessary Nor Sufficient for In-the-Wild Robot Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9BGi9PEhNn": {
    "title": "ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nOjZfpLyh1": {
    "title": "Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9jXS07TIBH": {
    "title": "Regularizing with Pseudo-Negatives for Continual Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XTrMY9sHKF": {
    "title": "Harmonic Self-Conditioned Flow Matching for joint Multi-Ligand Docking and Binding Site Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HusShERjlc": {
    "title": "Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8l1KYguM4w": {
    "title": "Make-A-Shape: a Ten-Million-scale 3D Shape Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sOyJSNUrzQ": {
    "title": "PAC-Bayesian Generalization Bounds for Knowledge Graph Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7RHFdkAkVY": {
    "title": "AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBPnmk6EeO": {
    "title": "Equivariant Deep Weight Space Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zw52bJCZXc": {
    "title": "Sarah Frank-Wolfe: Methods for Constrained Optimization with Best Rates and Practical Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIMRKeeVsr": {
    "title": "Understanding Retrieval-Augmented Task Adaptation for Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DKKg5EFAFr": {
    "title": "Evaluating Quantized Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ash2ksk1r": {
    "title": "Statistically Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xlWcdtCyOC": {
    "title": "InstructSpeech: Following Speech Editing Instructions via Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rx9GMufByc": {
    "title": "Multi-Patch Prediction: Adapting Language Models for Time Series Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cVkqItmYLQ": {
    "title": "A sampling theory perspective on activations for implicit neural representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XyxuhLtFA2": {
    "title": "Sliced Wasserstein with Random-Path Projecting Directions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5zXTwX92qv": {
    "title": "BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPQHXwVNvl": {
    "title": "Online Speculative Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bM2s12t4hR": {
    "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lGvIV4Bgsz": {
    "title": "Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ccSSKTz9LX": {
    "title": "Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c6rVlTKpb5": {
    "title": "Hybrid Reinforcement Learning from Offline Observation Alone",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zL9q2JD1dC": {
    "title": "GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DgLFkAPwuZ": {
    "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dztd61efGy": {
    "title": "Discovering Bias in Latent Space: An Unsupervised Debiasing Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6dKUu2EkZy": {
    "title": "Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vFATIZXlCm": {
    "title": "UGrid: An Efficient-And-Rigorous Neural Multigrid Solver for Linear PDEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fSnMqHZ8xr": {
    "title": "Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hz8cFsdz7P": {
    "title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K5h6VAsJaV": {
    "title": "Improving Gradient-Guided Nested Sampling for Posterior Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OkChMnjF6s": {
    "title": "Verification of Machine Unlearning is Fragile",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhKsE7YAJk": {
    "title": "Naive Bayes Classifiers over Missing Data: Decision and Poisoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qklMNNub0H": {
    "title": "Implicit Regularization in Feedback Alignment Learning Mechanisms for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=47jMS97wJX": {
    "title": "Eluder-based Regret for Stochastic Contextual MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wn4QwCrDvH": {
    "title": "Variational Inference with Coverage Guarantees in Simulation-Based Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=teHPKqjX8q": {
    "title": "MD tree: a model-diagnostic tree grown on loss landscape",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4ZrppmS42b": {
    "title": "LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s7RDnNUJy6": {
    "title": "WARM: On the Benefits of Weight Averaged Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qOl2WWOqFg": {
    "title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i56plqPpEa": {
    "title": "Auto-Regressive Next-Token Predictors are Universal Learners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmUorldOWN": {
    "title": "Rethinking Adversarial Robustness in the Context of the Right to be Forgotten",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BNH8spaR3l": {
    "title": "Probabilistic Time Series Modeling with Decomposable Denoising Diffusion Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F3RdeyiR5H": {
    "title": "Trust Regions for Explanations via Black-Box Probabilistic Certification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kRv0WPJd00": {
    "title": "Variational Schrödinger Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U1uKihiG39": {
    "title": "Causal Bandits: The Pareto Optimal Frontier of Adaptivity, a Reduction to Linear Bandits, and Limitations around Unknown Marginals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtMqsSkIHX": {
    "title": "Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ajnQyZgK": {
    "title": "Learning Universal Predictors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JgCpZS17T": {
    "title": "Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xGlVkBSDdt": {
    "title": "Dynamic Survival Analysis with Controlled Latent States",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=07f24ya6eX": {
    "title": "Regularized Q-learning through Robust Averaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iKkFruh4d5": {
    "title": "The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28SEr5iFyT": {
    "title": "Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CR6Sl80cn8": {
    "title": "Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=akyElNlUVA": {
    "title": "FedLMT: Tackling System Heterogeneity of Federated Learning via Low-Rank Model Training with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b1iurBHDck": {
    "title": "Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Grrydzui3A": {
    "title": "Efficient Mixture Learning in Black-Box Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lgq1E92h1U": {
    "title": "Stacking Deep Set Networks and Pooling by Quantiles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bwZlD7mYoa": {
    "title": "An Unsupervised Approach for Periodic Source Detection in Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=18f6iPn0zq": {
    "title": "On the Nonlinearity of Layer Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eMQyb1tvvc": {
    "title": "Towards efficient deep spiking neural networks construction with spiking activity based pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xFk0w9zoV3": {
    "title": "EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mU7FfQT6VE": {
    "title": "PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v2o9rRJcEv": {
    "title": "Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xbQqhojHTg": {
    "title": "Positive and Unlabeled Learning with Controlled Probability Boundary Fence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=91QmrfztSP": {
    "title": "Knowledge Distillation with Auxiliary Variable",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3uPSQmjXzd": {
    "title": "Cross-Domain Policy Adaptation by Capturing Representation Mismatch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aeXRBnLoPP": {
    "title": "Accelerated Policy Gradient: On the Convergence Rates of the Nesterov Momentum for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hFEgae0od4": {
    "title": "Discovering Features with Synergistic Interactions in Multiple Views",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kZBCFQe1Ej": {
    "title": "More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OnEaBGU3LO": {
    "title": "FRAG: Frequency Adapting Group for Diffusion Video Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YuNFJSEkTi": {
    "title": "CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AtVtt9xsO1": {
    "title": "Trustless Audits without Revealing Data or Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D4B7kkB89m": {
    "title": "Generalized Neural Collapse for a Large Number of Classes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DKOHE4n8jk": {
    "title": "Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dEH0u8w0b": {
    "title": "Do Topological Characteristics Help in Knowledge Distillation?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t4908PyZxs": {
    "title": "Compositional Few-Shot Class-Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=04Fx1u2BUD": {
    "title": "SSL4Q: Semi-Supervised Learning of Quantum Data with Application to Quantum State Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o1gS6MNAw8": {
    "title": "Using Left and Right Brains Together: Towards Vision and Language Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hy88Jp0kQT": {
    "title": "Understanding the Learning Dynamics of Alignment with Human Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Asakozn3Z": {
    "title": "HarmoDT: Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6NQ77Vj3DT": {
    "title": "Convex and Bilevel Optimization for Neural-Symbolic Inference and Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4CO45y7Mlv": {
    "title": "Conformal Prediction Sets Improve Human Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KHymcy2xxF": {
    "title": "Leverage Class-Specific Accuracy to Guide Data Generation for Improving Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6D0nyemiWk": {
    "title": "On Positivity Condition for Causal Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bVIcZb7Qa0": {
    "title": "Controlled Decoding from Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5uwBzcn885": {
    "title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J5Yg7HMy39": {
    "title": "Learning Mixtures of Gaussian Processes through Random Projection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=181hXof7ho": {
    "title": "NeWRF: A Deep Learning Framework for Wireless Radiation Field Reconstruction and Channel Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FzyMdAm2fZ": {
    "title": "Delving into Differentially Private Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ooikIHLHCs": {
    "title": "Position: Explain to Question not to Justify",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pa3GyTe3kf": {
    "title": "A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S80a4hJtuE": {
    "title": "A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Linear MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s4Hy0L4mml": {
    "title": "MS-TIP: Imputation Aware Pedestrian Trajectory Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7OPHCeXcSS": {
    "title": "Double Momentum Method for Lower-Level Constrained Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5t4V7Q6lmz": {
    "title": "Efficient Error Certification for Physics-Informed Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s0Jvdolv2I": {
    "title": "Don't trust your eyes: on the (un)reliability of feature visualizations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VnI9200eeL": {
    "title": "Auctionformer: A Unified Deep Learning Algorithm for Solving Equilibrium Strategies in Auction Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ufgVvFmUom": {
    "title": "Sparse Dimensionality Reduction Revisited",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sqv2xP8rfb": {
    "title": "Ambiguity-Aware Abductive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WLGWMDtj8L": {
    "title": "Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ySQI9LE4w": {
    "title": "Non-convex Stochastic Composite Optimization with Polyak Momentum",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JD03zxWZzs": {
    "title": "Federated Optimization with Doubly Regularized Drift Correction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ALc7DmOTI2": {
    "title": "Interplay of ROC and Precision-Recall AUCs: Theoretical Limits and Practical Implications in Binary Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ZxnPZGmPU": {
    "title": "Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fqeANcjBMT": {
    "title": "Differentially Private Bias-Term Fine-tuning of Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HssOwuZiaB": {
    "title": "Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oj18qGN1gC": {
    "title": "Straight-Through Meets Sparse Recovery: the Support Exploration Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4boDu42RtE": {
    "title": "Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cxiqxDnrCx": {
    "title": "Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yXlQL9goY8": {
    "title": "Towards Generalization beyond Pointwise Learning: A Unified Information-theoretic Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mWV8NeU79e": {
    "title": "Spider: A Unified Framework for Context-dependent Concept Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCTmppB165": {
    "title": "CaM: Cache Merging for Memory-efficient LLMs Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FvLd8Gr7xq": {
    "title": "Vague Prototype-Oriented Diffusion Model for Multi-Class Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YoUb2vW9WP": {
    "title": "Measures of diversity and space-filling designs for categorical data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoqxSnEUi1": {
    "title": "Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jrE7geZekq": {
    "title": "PGODE: Towards High-quality System Dynamics Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nMN5hNZMQK": {
    "title": "StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PEpbUobfJv": {
    "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lc1HlMo77m": {
    "title": "Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAbfF37H6t": {
    "title": "A fast algorithm to simulate nonlinear resistive networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T0lFfO8HaK": {
    "title": "Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jZEY5SxbL4": {
    "title": "Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WT4X3QYopC": {
    "title": "Gradient-based Visual Explanation for Transformer-based CLIP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eDtty9ZCvt": {
    "title": "Harnessing Neural Unit Dynamics for Effective and Scalable Class-Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fgBWtOw66T": {
    "title": "SFC: Achieve Accurate Fast Convolution under Low-precision Arithmetic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XkHJo8iXGQ": {
    "title": "A Closer Look at the Limitations of Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xFDJBzPhci": {
    "title": "CRoFT: Robust Fine-Tuning with Concurrent Optimization for OOD Generalization and Open-Set OOD Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KfN76nAcOO": {
    "title": "Hierarchical Novelty Detection via Fine-Grained Evidence Allocation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N1BPyf7wC2": {
    "title": "Parameter-Dependent Competitive Analysis for Online Capacitated Coverage Maximization through Boostings and Attenuations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CmOmaxkt8p": {
    "title": "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ea2MgKn3sV": {
    "title": "Position: Leverage Foundational Models for Black-Box Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x0vLj1S6Wg": {
    "title": "Randomized Confidence Bounds for Stochastic Partial Monitoring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OMKNBzf6HJ": {
    "title": "Liouville Flow Importance Sampler",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u26c52rxZC": {
    "title": "Improving Antibody Humanness Prediction using Patent Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1nT6uc3HdY": {
    "title": "Proactive DP: A Multiple Target Optimization Framework for DP-SGD",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ffpg52swvg": {
    "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u9oSQtujCF": {
    "title": "Empowering Graph Invariance Learning with Deep Spurious Infomax",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6yHkQpSwZ": {
    "title": "Graph As Point Set",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7mFSaP6IiN": {
    "title": "When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ugxGpOEkox": {
    "title": "On Prompt-Driven Safeguarding for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nw7yOe8nBi": {
    "title": "Differentially Private Representation Learning via Image Captioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HGSIpeNNfM": {
    "title": "Receptive Fields As Experts in Convolutional Neural Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DprrMz24tk": {
    "title": "Position: Why We Must Rethink Empirical Research in Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QTt2xJI8vk": {
    "title": "Reward-Free Kernel-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=apxON2uH4N": {
    "title": "Privacy-Preserving Embedding via Look-up Table Evaluation with Fully Homomorphic Encryption",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=chDpBp2P6b": {
    "title": "Beyond Individual Input for Deep Anomaly Detection on Tabular Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G8zDeKOp0R": {
    "title": "SCoRe: Submodular Combinatorial Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uh5XN9d2J4": {
    "title": "Outlier-aware Slicing for Post-Training Quantization in Vision Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JJpOssn0uP": {
    "title": "Prodigy: An Expeditiously Adaptive Parameter-Free Learner",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KOTutrSR2y": {
    "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1pj0Sk8GfP": {
    "title": "Going beyond Compositions, DDPMs Can Produce Zero-Shot Interpolations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ngjmcfowtc": {
    "title": "Momentum Particle Maximum Likelihood",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tABvuya05B": {
    "title": "Task-aware Orthogonal Sparse Network for Exploring Shared Knowledge in Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7iH9RgMrzX": {
    "title": "Adaptive Stabilization Based on Machine Learning for Column Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IGdpKP0N6w": {
    "title": "Neural Networks Learn Statistics of Increasing Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KmCoS6WkgG": {
    "title": "Data-efficient Large Vision Models through Sequential Autoregression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aaeJpJw5Ur": {
    "title": "Socialized Learning: Making Each Other Better Through Multi-Agent Collaboration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkWxjpUV0S": {
    "title": "Learning Causal Dynamics Models in Object-Oriented Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=61WtHsVKWF": {
    "title": "Online bipartite matching with imperfect advice",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YbHCqn4qF4": {
    "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkYOxLLv2x": {
    "title": "Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7zEoinErzQ": {
    "title": "Layerwise Change of Knowledge in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3abgRKnK1W": {
    "title": "DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LhNsSaAKub": {
    "title": "Foundation Policies with Hilbert Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=udFZhUgtkI": {
    "title": "Learning Scale-Aware Spatio-temporal Implicit Representation for Event-based Motion Deblurring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zos5wsaB5r": {
    "title": "Fast-Slow Test-Time Adaptation for Online Vision-and-Language Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hQpUhySEJi": {
    "title": "Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mY93trX2Qz": {
    "title": "Low-Rank Similarity Mining for Multimodal Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uGoi3nY62g": {
    "title": "BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJMZbdiQnf": {
    "title": "LLM-Empowered State Representation for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PPoQz8K4GZ": {
    "title": "Prompt-based Visual Alignment for Zero-shot Policy Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J4LTDgwAZq": {
    "title": "Efficient Value Iteration for s-rectangular Robust Markov Decision Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lgh8bhWpVC": {
    "title": "Disentangled 3D Scene Generation with Layout Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nl3RG5XWAt": {
    "title": "Position: Topological Deep Learning is the New Frontier for Relational Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PrmxFWI1Fr": {
    "title": "Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qstt2OguvM": {
    "title": "Latent Space Symmetry Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hrwIndai8e": {
    "title": "Prompt-tuning Latent Diffusion Models for Inverse Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xcyKKACmSd": {
    "title": "S3O: A Dual-Phase Approach for Reconstructing Dynamic Shape and Skeleton of Articulated Objects from Single Monocular Video",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UGpGkLzwpP": {
    "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jVXJdGQ4eD": {
    "title": "MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAEUO7847g": {
    "title": "Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W8hBNk1FhQ": {
    "title": "Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qHt8FzPvU9": {
    "title": "The Effect of Weight Precision on the Neuron Count in Deep ReLU Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PApqOVbHYF": {
    "title": "Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJWlUMW6YP": {
    "title": "Interpretability Illusions in the Generalization of Simplified Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LYpGLrC4oq": {
    "title": "Predictive Dynamic Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=87CYNyCGOo": {
    "title": "Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CSIfCpXhCF": {
    "title": "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8xKGZsnV2a": {
    "title": "AquaLoRA: Toward White-box Protection for Customized Stable Diffusion Models via Watermark LoRA",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4oD0tRrUOX": {
    "title": "$\\bf{\\Phi}_\\textrm{Flow}$: Differentiable Simulations for PyTorch, TensorFlow and Jax",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUOHKSsurt": {
    "title": "Parameter-Efficient Fine-Tuning with Discrete Fourier Transform",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zN9tvZfns": {
    "title": "Privacy-Preserving Data Release Leveraging Optimal Transport and Particle Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m8t1yzfBsJ": {
    "title": "Smooth Min-Max Monotonic Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WUdq1WFUPr": {
    "title": "Cascade-CLIP: Cascaded Vision-Language Embeddings Alignment for Zero-Shot Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=szvKJgmubh": {
    "title": "DataFreeShield: Defending Adversarial Attacks without Training Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AZWqXfM6z9": {
    "title": "Revisiting Character-level Adversarial Attacks for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CG44RLeXt1": {
    "title": "Self-cognitive Denoising in the Presence of Multiple Noisy Label Sources",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hsHIxrnrMx": {
    "title": "RMIB: Representation Matching Information Bottleneck for Matching Text Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V4qV08Vk6S": {
    "title": "An Embodied Generalist Agent in 3D World",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UE79AkNg60": {
    "title": "Is Kernel Prediction More Powerful than Gating in Convolutional Neural Networks?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u00dmbI8Db": {
    "title": "Multi-Factor Adaptive Vision Selection for Egocentric Video Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TRrXkVdhwi": {
    "title": "Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q3Bz1TVTq4": {
    "title": "Classification Under Strategic Self-Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RtCmp5F9lN": {
    "title": "PAPM: A Physics-aware Proxy Model for Process Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BRIcZiK5Fr": {
    "title": "Mitigating Label Noise on Graphs via Topological Sample Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NCT3w7VKjo": {
    "title": "Unraveling the Impact of Heterophilic Structures on Graph Positive-Unlabeled Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ShkKSDrfG6": {
    "title": "OTMatch: Improving Semi-Supervised Learning with Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NgaYcefBnZ": {
    "title": "Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vxDjeeBnTu": {
    "title": "Information Flow in Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xtwCf7iAs2": {
    "title": "Memory Efficient Neural Processes via Constant Memory Attention Block",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wleAlsklEh": {
    "title": "Matrix Information Theory for Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V3ya8RlbrW": {
    "title": "Provable Contrastive Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=50vc4HBuKU": {
    "title": "Quantum Implicit Neural Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c2CKmP9l5X": {
    "title": "Deep Neural Room Acoustics Primitive",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z373OXJXWU": {
    "title": "Demystifying SGD with Doubly Stochastic Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0miAQ1qHiw": {
    "title": "Provably Scalable Black-Box Variational Inference with Structured Variational Families",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5hfvLBgnNE": {
    "title": "Unveiling the Dynamics of Information Interplay in Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojtddicekd": {
    "title": "Q-value Regularized Transformer for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MurkwIl0h3": {
    "title": "Balancing Feature Similarity and Label Variability for Optimal Size-Aware One-shot Subset Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CquFGSIU6w": {
    "title": "Meta Evidential Transformer for Few-Shot Open-Set Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n2eppIzHlL": {
    "title": "Multiply-Robust Causal Change Attribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y50K6DSrWo": {
    "title": "Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ljhrv1Wmbr": {
    "title": "Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w1HdBXSJXn": {
    "title": "PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=us6zMORsMe": {
    "title": "Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Wauue8pWd": {
    "title": "Multicalibration for Confidence Scoring in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rORsGuE2hV": {
    "title": "Highway Value Iteration Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJLcXGB2uA": {
    "title": "Non-clairvoyant Scheduling with Partial Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iLSgF7jMtI": {
    "title": "CogDPM: Diffusion Probabilistic Models via Cognitive Predictive Coding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UcOze9EXEc": {
    "title": "Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C7Z8EhZ6bl": {
    "title": "Factored-Reward Bandits with Intermediate Observations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aR3uxWlZhX": {
    "title": "UP2ME: Univariate Pre-training to Multivariate Fine-tuning as a General-purpose Framework for Multivariate Time Series Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DChQpB4AJy": {
    "title": "Imitation Learning in Discounted Linear MDPs without exploration assumptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=usUPvQH3XK": {
    "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FM61SQzF3N": {
    "title": "IIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual Speech Separation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sjJZHPV9Id": {
    "title": "Revisiting Context Aggregation for Image Matting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WDgV1BJEW0": {
    "title": "Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=njpTpkvUbO": {
    "title": "Efficient Exploration in Average-Reward Constrained Reinforcement Learning: Achieving Near-Optimal Regret With Posterior Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=THPjMr2r0S": {
    "title": "Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=asJTE8EBjg": {
    "title": "Language Models Represent Beliefs of Self and Others",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2zxPwCkAZ": {
    "title": "FedBAT: Communication-Efficient Federated Learning via Learnable Binarization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k2dVVIWWho": {
    "title": "Differentially Private Decentralized Learning with Random Walks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YYwERRXsJW": {
    "title": "Harmonizing Generalization and Personalization in Federated Prompt Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=szxtVHOh0C": {
    "title": "Surface-VQMAE: Vector-quantized Masked Auto-encoders on Molecular Surfaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gzis9n5r7e": {
    "title": "Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w9nxTXuaCc": {
    "title": "Position: $C^*$-Algebraic Machine Learning $-$ Moving in a New Direction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NV0q2jdwo0": {
    "title": "Enhancing Vision Transformer: Amplifying Non-Linearity in Feedforward Network Module",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ofXRBPtol3": {
    "title": "Generative Active Learning for Long-tailed Instance Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hg7C5YYifi": {
    "title": "Disentangled Continual Graph Neural Architecture Search with Invariant Modular Supernet",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLZzFTMWSt": {
    "title": "CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykRY34kL3j": {
    "title": "Bootstrap AutoEncoders With Contrastive Paradigm for Self-supervised Gaze Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KU9mn6deDR": {
    "title": "UPAM: Unified Prompt Attack in Text-to-Image Generation Models Against Both Textual Filters and Visual Checkers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=girxGkdECL": {
    "title": "Can AI Assistants Know What They Don't Know?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PqWuSuWvX": {
    "title": "Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60HydCpCMZ": {
    "title": "Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZTFxktg23": {
    "title": "Graph Generation with Diffusion Mixture",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQ1dwuheR0": {
    "title": "Prompt-guided Precise Audio Editing with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xuX2rDSSco": {
    "title": "Drug Discovery with Dynamic Goal-aware Fragments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cit0hg4sEz": {
    "title": "Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ihv6pWuILN": {
    "title": "Causal Customer Churn Analysis with Low-rank Tensor Block Hazard Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fSNHK7mu3j": {
    "title": "Graph Neural Networks Use Graphs When They Shouldn't",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i0nVanexij": {
    "title": "Self-Correcting Self-Consuming Loops for Generative Model Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wLoESsgZIq": {
    "title": "What's the score? Automated Denoising Score Matching for Nonlinear Diffusions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ttaTyweIr1": {
    "title": "Learning to Infer Generative Template Programs for Visual Concepts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u4VR3WBH7a": {
    "title": "STELLA: Continual Audio-Video Pre-training with SpatioTemporal Localized Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2PVjIQdq7N": {
    "title": "Efficient PAC Learnability of Dynamical Systems Over Multilayer Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpSlt67vxQ": {
    "title": "Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pkUl39b0in": {
    "title": "Robust Inverse Constrained Reinforcement Learning under Model Misspecification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zsz9Pdfvtg": {
    "title": "GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zs3qW8Njov": {
    "title": "Smoothing Proximal Gradient Methods for Nonsmooth Sparsity Constrained Optimization: Optimality Conditions and Global Convergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NUlyqMyhO9": {
    "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r9rzU9QzPe": {
    "title": "BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8uJyOwOsd": {
    "title": "Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1mf1ISuyS3": {
    "title": "Towards Certified Unlearning for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZFRrOiZruJ": {
    "title": "A Unified Adaptive Testing System Enabled by Hierarchical Structure Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m8lCi7rG4u": {
    "title": "Layer-Aware Analysis of Catastrophic Overfitting: Revealing the Pseudo-Robust Shortcut Dependency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EFtNP211X3": {
    "title": "Defense against Model Extraction Attack by Bayesian Active Watermarking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h2uBuQvpp8": {
    "title": "Adaptive Sampling of k-Space in Magnetic Resonance for Rapid Pathology Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qbIKUfastZ": {
    "title": "Provably Efficient Reinforcement Learning for Adversarial Restless Multi-Armed Bandits with Unknown Transitions and Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qwKSTLbati": {
    "title": "Multi-Agent Reinforcement Learning Meets Leaf Sequencing in Radiotherapy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EIGbXbxcUQ": {
    "title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fqPH6ejwGi": {
    "title": "Encodings for Prediction-based Neural Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C64clssMVU": {
    "title": "Scalable Online Exploration via Coverability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=knhbhDLdry": {
    "title": "When and How Does In-Distribution Label Help Out-of-Distribution Detection?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9GLvXGkUE2": {
    "title": "In-context Convergence of Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HOoVTsPPn7": {
    "title": "Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4HF3N6CZR": {
    "title": "ReLU Network with Width $d+\\mathcal{O}(1)$ Can Achieve Optimal Approximation Rate",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z7zHsNFXHc": {
    "title": "Characterizing ResNet's Universal Approximation Capability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O3CFN1VIwt": {
    "title": "Exploring Correlations of Self-Supervised Tasks for Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dqdctVbSfs": {
    "title": "An Improved Finite-time Analysis of Temporal Difference Learning with Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7AF0AMI4AE": {
    "title": "Symmetry Induces Structure and Constraint of Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xaSpuvNYwS": {
    "title": "Robust Classification via a Single Diffusion Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fiugPLSXjK": {
    "title": "EDISON: Enhanced Dictionary-Induced Tensorized Incomplete Multi-View Clustering with Gaussian Error Rank Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7E4c2gyP0R": {
    "title": "DiffFPR: Diffusion Prior for Oversampled Fourier Phase Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qRtM5EqE9l": {
    "title": "BLO-SAM: Bi-level Optimization Based Finetuning of the Segment Anything Model for Overfitting-Preventing Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dccRCYmL5x": {
    "title": "Equivariant Graph Neural Operator for Modeling 3D Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lm04PyXoEl": {
    "title": "Detecting Influence Structures in Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qb68Rs0p9f": {
    "title": "Potential Based Diffusion Motion Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2FKzbEE24s": {
    "title": "A Differentiable Partially Observable Generalized Linear Model with Forward-Backward Message Passing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TK7xkOsXDu": {
    "title": "Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mbx2pLK5Eq": {
    "title": "A2Q+: Improving Accumulator-Aware Weight Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2K87GFLYWz": {
    "title": "Breaking through the learning plateaus of in-context learning in Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J5VB1h3Aed": {
    "title": "Revisiting the Role of Language Priors in Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MWTicAxmRP": {
    "title": "Optimistic Multi-Agent Policy Gradient",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mCzyRdDak5": {
    "title": "Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dhrNfAJAH6": {
    "title": "ELTA: An Enhancer against Long-Tail for Aesthetics-oriented Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3FeYlKIPr3": {
    "title": "Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y5Zi59N265": {
    "title": "GeoMFormer: A General Architecture for Geometric Molecular Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=126SR50BEL": {
    "title": "A Dual-module Framework for Counterfactual Estimation over Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C4nalr0DoE": {
    "title": "Parameter-Efficient Fine-Tuning with Controls",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OURP5Z58jt": {
    "title": "One-Shot Strategic Classification Under Unknown Costs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ufCptn28vG": {
    "title": "Isometric Representation Learning for Disentangled Latent Space of Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPVwOAr1aW": {
    "title": "On the Embedding Collapse when Scaling up Recommendation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VHtIDVaOKC": {
    "title": "Mobile Attention: Mobile-Friendly Linear-Attention for Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I44Em5D5xy": {
    "title": "Swallowing the Bitter Pill: Simplified Scalable Conformer Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mggc3oYHy4": {
    "title": "Privacy Attacks in Decentralized Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4DAl3IsvlU": {
    "title": "From Geometry to Causality- Ricci Curvature and the Reliability of Causal Inference on Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H9fNj8ivTy": {
    "title": "Position: Towards Implicit Prompt For Text-To-Image Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iRcmqXZjeK": {
    "title": "Learning Decision Policies with Instrumental Variables through Double Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j35VcooKG8": {
    "title": "On Multi-Armed Bandit with Impatient Arms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b9uHveqszc": {
    "title": "Analyzing $D^\\alpha$ seeding for $k$-means",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPiEIhQpos": {
    "title": "Sampling is as easy as keeping the consistency: convergence guarantee for Consistency Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5lgbVR6BP": {
    "title": "Scalable Multiple Kernel Clustering: Learning Clustering Structure from Expectation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ad9msn1SKC": {
    "title": "Counterfactual Metarules for Local and Global Recourse",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xIRKB5nRJl": {
    "title": "Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ILo43JIzg": {
    "title": "Kepler codebook",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dplgaRn4Ae": {
    "title": "Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQ3SEBH1gF": {
    "title": "GaussianPro: 3D Gaussian Splatting with Progressive Propagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wrTzLoqbCg": {
    "title": "TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rIrpzmqRBk": {
    "title": "Exploration and Anti-Exploration with Distributional Random Network Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bULHOW1RXM": {
    "title": "Differentiable Model Scaling using Differentiable Topk",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdqiT0McON": {
    "title": "Generalization Bound and New Algorithm for Clean-Label Backdoor Attack",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=opkluZm9gX": {
    "title": "Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fRG45xL1WT": {
    "title": "Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7JKVPNEBkU": {
    "title": "Position: Standardization of Behavioral Use Clauses is Necessary for the Adoption of Responsible Licensing of AI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOunbuapcv": {
    "title": "VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eejhD9FCP3": {
    "title": "Interaction-based Retrieval-augmented Diffusion Models for Protein-specific 3D Molecule Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMGUDsPopK": {
    "title": "A Rate-Distortion View of Uncertainty Quantification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g9mYBdooPA": {
    "title": "Policy-conditioned Environment Models are More Generalizable",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DJdVzxemdA": {
    "title": "Deep Demonstration Tracing: Learning Generalizable Imitator Policy for Runtime Imitation from a Single Demonstration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j6QZy90B93": {
    "title": "Hybrid Neural Representations for Spherical Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u8TZ9gm4im": {
    "title": "Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4RqG4K5UwL": {
    "title": "Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nm6jYZsBum": {
    "title": "Improving Context Understanding in Multimodal Large Language Models via Multimodal Composition Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lghifYrSU": {
    "title": "High-dimensional Linear Bandits with Knapsacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=761UxjOTHB": {
    "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LZkhKZvhHs": {
    "title": "Adaptive Feature Selection for No-Reference Image Quality Assessment by Mitigating Semantic Noise Sensitivity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUc29ydmLX": {
    "title": "Towards a Better Theoretical Understanding of Independent Subnetwork Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AOJCCFTlfJ": {
    "title": "Constrained Ensemble Exploration for Unsupervised Skill Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QMy2RLnxGN": {
    "title": "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models (Exemplified as A Video Agent)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=phGHQOKmaU": {
    "title": "DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LwOfVWgEzS": {
    "title": "Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TvoG41N1Y3": {
    "title": "Gaussian Plane-Wave Neural Operator for Electron Density Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kfpe7Dg23G": {
    "title": "Sign Gradient Descent-based Neuronal Dynamics: ANN-to-SNN Conversion Beyond ReLU Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WsM4TVsZpJ": {
    "title": "Rethinking Decision Transformer via Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzXNCQGzqT": {
    "title": "Ditto: Quantization-aware Secure Inference of Transformers upon MPC",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHIEOZtDDF": {
    "title": "Rethinking Optimization and Architecture for Tiny Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8iUgr2nuwo": {
    "title": "Wukong: Towards a Scaling Law for Large-Scale Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JUa5XNXuoT": {
    "title": "Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LpAzlcGzJ6": {
    "title": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=merZTLSdC9": {
    "title": "On Online Experimentation without Device Identifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bylZbZOsGA": {
    "title": "Autoformalizing Euclidean Geometry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pFWmHUdJE5": {
    "title": "O$n$ Learning Deep O($n$)-Equivariant Hyperspheres",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OfT8MgIqHT": {
    "title": "Vanilla Bayesian Optimization Performs Great in High Dimensions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pQyoBWA146": {
    "title": "Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F3G2udCF3Q": {
    "title": "How Interpretable Are Interpretable Graph Neural Networks?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nLgtHHBgl3": {
    "title": "Completing Visual Objects via Bridging Generation and Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x0yIaw2fgk": {
    "title": "HarmonyDream: Task Harmonization Inside World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XnsI1HKAKC": {
    "title": "Pseudo-Calibration: Improving Predictive Uncertainty Estimation in Unsupervised Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0vZ5ENrJQ": {
    "title": "Learning Pseudo-Contractive Denoisers for Inverse Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=maVIKlGqr7": {
    "title": "HumanTOMATO: Text-aligned Whole-body Motion Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qMG3OK7Xcg": {
    "title": "Cluster-Aware Similarity Diffusion for Instance Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pSnhA7Em1P": {
    "title": "Subgoal-based Demonstration Learning for Formal Theorem Proving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1YbS3qkdk": {
    "title": "Causal Discovery via Conditional Independence Testing with Proxy Variables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4ye2I5OelI": {
    "title": "Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eN1T7I7OpZ": {
    "title": "Advancing DRL Agents in Commercial Fighting Games: Training, Integration, and Agent-Human Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ft5jK9uPgC": {
    "title": "On Universally Optimal Algorithms for A/B Testing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0LBNdbmQCM": {
    "title": "Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LALSZ88Xpx": {
    "title": "Language Generation with Strictly Proper Scoring Rules",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MUXTt9Yr4T": {
    "title": "Unifying Image Processing as Visual Prompting Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=07fSWltF6M": {
    "title": "ProtoGate: Prototype-based Neural Networks with Global-to-local Feature Selection for Tabular Biomedical Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NvBJOcmti6": {
    "title": "Spectral Preconditioning for Gradient Methods on Graded Non-convex Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bYRYb7DMNo": {
    "title": "Timer: Generative Pre-trained Transformers Are Large Time Series Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QgMqvxvWpX": {
    "title": "Exploring the Complexity of Deep Neural Networks through Functional Equivalence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WtvI3QijEF": {
    "title": "Exploring the LLM Journey from Cognition to Expression with Linear Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s0UDX7Kswl": {
    "title": "DiffAug: Enhance Unsupervised Contrastive Learning with Domain-Knowledge-Free Diffusion-based Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=luqH1eL4PN": {
    "title": "Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1IZLOPxtfK": {
    "title": "INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Zr7T6UrBS": {
    "title": "Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sCGRhnuMUJ": {
    "title": "Compressing Large Language Models by Joint Sparsification and Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tTq3qMkJ8w": {
    "title": "Scene Graph Generation Strategy with Co-occurrence Knowledge and Learnable Term Frequency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZxDqSBgFSM": {
    "title": "Federated Self-Explaining GNNs with Anti-shortcut Augmentations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M3qRRkOuTN": {
    "title": "Sequential Asynchronous Action Coordination in Multi-Agent Systems: A Stackelberg Decision Transformer Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mrd4e8ZJjm": {
    "title": "Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qoxuPshrZb": {
    "title": "An Empirical Study Into What Matters for Calibrating Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jvh8HM9YEJ": {
    "title": "MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eRThYD9BGD": {
    "title": "Calibration Bottleneck: Over-compressed Representations are Less Calibratable",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VSwrXRqD9o": {
    "title": "Latent Optimal Paths by Gumbel Propagation for Variational Bayesian Dynamic Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=scFlbJQdm1": {
    "title": "Projecting Molecules into Synthesizable Chemical Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wELbEYgnmo": {
    "title": "Position: A Call to Action for a Human-Centered AutoML Paradigm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6OSLjErBhh": {
    "title": "Total Variation Distance Meets Probabilistic Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ZTuy5CrL7": {
    "title": "TVE: Learning Meta-attribution for Transferable Vision Explainer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fq0NaiU8Ex": {
    "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P7qwBmzwwZ": {
    "title": "Invariant Risk Minimization Is A Total Variation Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p9SMltcfsu": {
    "title": "Generalizing Orthogonalization for Models with Non-Linearities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBR8Gwe1E2": {
    "title": "DetKDS: Knowledge Distillation Search for Object Detectors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ry4RAzdOWl": {
    "title": "EvTexture: Event-driven Texture Enhancement for Video Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WOa96EG26M": {
    "title": "Why Larger Language Models Do In-context Learning Differently?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gL5djEYLx2": {
    "title": "New Bounds on the Cohesion of Complete-link and Other Linkage Methods for Agglomerative Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pAdI75JG3G": {
    "title": "Combinatorial Multivariant Multi-Armed Bandits with Applications to Episodic Reinforcement Learning and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PHjkVjR78A": {
    "title": "Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LvuuYqU0BW": {
    "title": "Learning Causal Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lcX5GbDIi8": {
    "title": "DMTG: One-Shot Differentiable Multi-Task Grouping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KpUdNe9lsr": {
    "title": "HGAP: Boosting Permutation Invariant and Permutation Equivariant in Multi-Agent Reinforcement Learning via Graph Attention Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B4rViOCoNf": {
    "title": "CCM: Real-Time Controllable Visual Content Creation Using Text-to-Image Consistency Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4G5Dcjcm1s": {
    "title": "VinT-6D: A Large-Scale Object-in-hand Dataset from Vision, Touch and Proprioception",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gE7qZurGH3": {
    "title": "Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jEoIkNkqyc": {
    "title": "Cross-view Masked Diffusion Transformers for Person Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sFN49CfklF": {
    "title": "Tell, Don't Show: Language Guidance Eases Transfer Across Domains in Images and Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lsQnneYa8p": {
    "title": "MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=chhIZGqlUG": {
    "title": "Taylor Videos for Action Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NFEJQn7vX0": {
    "title": "Optimal Differentially Private Model Training with Public Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVwzR1myUp": {
    "title": "ContPhy: Continuum Physical Concept Learning and Reasoning from Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b2D9PBNNQ2": {
    "title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xlr6AUDuJz": {
    "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TTYVG17wfc": {
    "title": "OSN: Infinite Representations of Dynamic 3D Scenes from Monocular Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHJFfDFbm6": {
    "title": "HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xbkWiEuR1": {
    "title": "Offline Training of Language Model Agents with Functions as Learnable Weights",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XBNhJQU84y": {
    "title": "Enhancing Implicit Shape Generators Using Topological Regularizations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oBP8vXFJNQ": {
    "title": "VideoPrism: A Foundational Visual Encoder for Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c92KDfEZTg": {
    "title": "Scalable Pre-training of Large Autoregressive Image Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=igRjCCAz2a": {
    "title": "Unified Generation, Reconstruction, and Representation: Generalized Diffusion with Adaptive Latent Encoding-Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p0lKWzdikQ": {
    "title": "MEMORYLLM: Towards Self-Updatable Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUm9iuvwIQ": {
    "title": "Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJCUrzhbiH": {
    "title": "diff History for Neural Language Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CXZqGJonmt": {
    "title": "CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v9tIJW1fzt": {
    "title": "Energy-Efficient Gaussian Processes Using Low-Precision Arithmetic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WvvkbWD1vL": {
    "title": "MoMo: Momentum Models for Adaptive Learning Rates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7PXSc5fURu": {
    "title": "Switching the Loss Reduces the Cost in Batch Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F1drhMjN7s": {
    "title": "Libra: Building Decoupled Vision System on Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kAIkYOE5pV": {
    "title": "Autaptic Synaptic Circuit Enhances Spatio-temporal Predictive Learning of Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PSQ5Z920M8": {
    "title": "Think Before You Act: Decision Transformers with Working Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QvABoVGdRp": {
    "title": "Enhancing Adversarial Robustness in SNNs with Sparse Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Kg9p8URlj": {
    "title": "Non-Vacuous Generalization Bounds for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8NfHmzo0Op": {
    "title": "Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uiqbnV4msl": {
    "title": "Stability-Informed Initialization of Neural Ordinary Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JOrLz5d7OW": {
    "title": "Prototypical Transformer As Unified Motion Learners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oDUJmNCV8D": {
    "title": "Image Restoration Through Generalized Ornstein-Uhlenbeck Bridge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0f4u3Wg9zT": {
    "title": "Exploring the Low-Pass Filtering Behavior in Image Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eOtjMYdGLt": {
    "title": "Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eqY64Z1rsT": {
    "title": "Image Fusion via Vision-Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WfJuiIiFzB": {
    "title": "Decouple then Classify: A Dynamic Multi-view Labeling Strategy with Shared and Specific Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5JrlywYHRi": {
    "title": "The Privacy Power of Correlated Noise in Decentralized Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AYbXN9poJl": {
    "title": "X-Oscar: A Progressive Framework for High-quality Text-guided 3D Animatable Avatar Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wlBtHP8KqS": {
    "title": "Better Locally Private Sparse Estimation Given Multiple Samples Per User",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k5ncz7TIPX": {
    "title": "Directly Denoising Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Va7mhTVy5s": {
    "title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=irBHPlknxP": {
    "title": "Residual-Conditioned Optimal Transport: Towards Structure-Preserving Unpaired and Paired Image Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sv4u9PtvT5": {
    "title": "Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e3geukCBw6": {
    "title": "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CD2xl1L5es": {
    "title": "Pedestrian Attribute Recognition as Label-balanced Multi-label Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wDDprThYeT": {
    "title": "xT: Nested Tokenization for Larger Context in Large Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q6fXuPLpao": {
    "title": "MLIP: Efficient Multi-Perspective Language-Image Pretraining with Exhaustive Data Utilization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}