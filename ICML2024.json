{
  "https://proceedings.mlr.press/v235/abad-rocamora24a.html": {
    "title": "Revisiting Character-level Adversarial Attacks for Language Models",
    "volume": "main",
    "abstract": "Adversarial attacks in Natural Language Processing apply perturbations in the character or token levels. Token-level attacks, gaining prominence for their use of gradient-based methods, are susceptible to altering sentence semantics, leading to invalid adversarial examples. While character-level attacks easily maintain semantics, they have received less attention as they cannot easily adopt popular gradient-based methods, and are thought to be easy to defend. Challenging these beliefs, we introduce Charmer, an efficient query-based adversarial attack capable of achieving high attack success rate (ASR) while generating highly similar adversarial examples. Our method successfully targets both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2, Charmer improves the ASR in $4.84$% points and the USE similarity in $8$% points with respect to the previous art. Our implementation is available in https://github.com/LIONS-EPFL/Charmer",
    "checked": false,
    "id": "8006de7c0f4578403db5f333282a7f8a8690b37b",
    "semantic_title": "revisiting character-level adversarial attacks",
    "citation_count": 1,
    "authors": [
      "Elias Abad Rocamora",
      "Yongtao Wu",
      "Fanghui Liu",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://proceedings.mlr.press/v235/abe24a.html": {
    "title": "Adaptively Perturbed Mirror Descent for Learning in Games",
    "volume": "main",
    "abstract": "This paper proposes a payoff perturbation technique for the Mirror Descent (MD) algorithm in games where the gradient of the payoff functions is monotone in the strategy profile space, potentially containing additive noise. The optimistic family of learning algorithms, exemplified by optimistic MD, successfully achieves last-iterate convergence in scenarios devoid of noise, leading the dynamics to a Nash equilibrium. A recent re-emerging trend underscores the promise of the perturbation approach, where payoff functions are perturbed based on the distance from an anchoring, or slingshot, strategy. In response, we propose Adaptively Perturbed MD (APMD), which adjusts the magnitude of the perturbation by repeatedly updating the slingshot strategy at a predefined interval. This innovation empowers us to find a Nash equilibrium of the underlying game with guaranteed rates. Empirical demonstrations affirm that our algorithm exhibits significantly accelerated convergence",
    "checked": true,
    "id": "9601b71af3a36548386eb8f7458d6428b3343675",
    "semantic_title": "adaptively perturbed mirror descent for learning in games",
    "citation_count": 3,
    "authors": [
      "Kenshi Abe",
      "Kaito Ariu",
      "Mitsuki Sakamoto",
      "Atsushi Iwasaki"
    ]
  },
  "https://proceedings.mlr.press/v235/abhyankar24a.html": {
    "title": "InferCept: Efficient Intercept Support for Augmented Large Language Model Inference",
    "volume": "main",
    "abstract": "Large language models are increasingly integrated with external environments, tools, and agents like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat each external interaction as the end of LLM generation and form a new request when the interaction finishes, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents InferCept, the first LLM inference framework targeting augmented LLMs and supporting the efficient interception of LLM generation. InferCept minimizes the GPU resource waste caused by LLM interceptions and dedicates saved memory for serving more requests.InferCept improves the overall serving throughput by 1.6x-2x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems",
    "checked": true,
    "id": "daed41785efecb393898af61b27934ffa5421230",
    "semantic_title": "infercept: efficient intercept support for augmented large language model inference",
    "citation_count": 2,
    "authors": [
      "Reyna Abhyankar",
      "Zijian He",
      "Vikranth Srivatsa",
      "Hao Zhang",
      "Yiying Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/acharya24a.html": {
    "title": "Balancing Feature Similarity and Label Variability for Optimal Size-Aware One-shot Subset Selection",
    "volume": "main",
    "abstract": "Subset or core-set selection offers a data-efficient way for training deep learning models. One-shot subset selection poses additional challenges as subset selection is only performed once and full set data become unavailable after the selection. However, most existing methods tend to choose either diverse or difficult data samples, which fail to faithfully represent the joint data distribution that is comprised of both feature and label information. The selection is also performed independently from the subset size, which plays an essential role in choosing what types of samples. To address this critical gap, we propose to conduct Feature similarity and Label variability Balanced One-shot Subset Selection (BOSS), aiming to construct an optimal size-aware subset for data-efficient deep learning. We show that a novel balanced core-set loss bound theoretically justifies the need to simultaneously consider both diversity and difficulty to form an optimal subset. It also reveals how the subset size influences the bound. We further connect the inaccessible bound to a practical surrogate target which is tailored to subset sizes and varying levels of overall difficulty. We design a novel Beta-scoring importance function to delicately control the optimal balance of diversity and difficulty. Comprehensive experiments conducted on both synthetic and real data justify the important theoretical properties and demonstrate the superior performance of BOSS as compared with the competitive baselines",
    "checked": true,
    "id": "b79cb99965bc5b7a0d1a321d0794aaf31b21859d",
    "semantic_title": "balancing feature similarity and label variability for optimal size-aware one-shot subset selection",
    "citation_count": 0,
    "authors": [
      "Abhinab Acharya",
      "Dayou Yu",
      "Qi Yu",
      "Xumin Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/achituve24a.html": {
    "title": "Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning",
    "volume": "main",
    "abstract": "As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel. Multi-task learning (MTL) addresses this challenge by learning a single model that solves several tasks simultaneously and efficiently. Often optimizing MTL models entails first computing the gradient of the loss for each task, and then aggregating all the gradients to obtain a combined update direction. However, common methods following this approach do not consider an important aspect, the sensitivity in the dimensions of the gradients. Some dimensions may be more lenient for changes while others may be more restrictive. Here, we introduce a novel gradient aggregation procedure using Bayesian inference. We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks. This valuable information allows us to quantify the uncertainty associated with each of the gradients' dimensions which is factored in when aggregating them. We empirically demonstrate the benefits of our approach in a variety of datasets, achieving state-of-the-art performance",
    "checked": true,
    "id": "5274e0a0445a5bfd9ff74503820a2390ba73865d",
    "semantic_title": "bayesian uncertainty for gradient aggregation in multi-task learning",
    "citation_count": 0,
    "authors": [
      "Idan Achituve",
      "Idit Diamant",
      "Arnon Netzer",
      "Gal Chechik",
      "Ethan Fetaya"
    ]
  },
  "https://proceedings.mlr.press/v235/achtibat24a.html": {
    "title": "AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers",
    "volume": "main",
    "abstract": "Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a single backward pass. Through extensive evaluations against existing methods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an LRP library at https://github.com/rachtibat/LRP-eXplains-Transformers",
    "checked": true,
    "id": "a2539713a31ee08cc8116c1355da9f96896df145",
    "semantic_title": "attnlrp: attention-aware layer-wise relevance propagation for transformers",
    "citation_count": 10,
    "authors": [
      "Reduan Achtibat",
      "Sayed Mohammad Vakilzadeh Hatefi",
      "Maximilian Dreyer",
      "Aakriti Jain",
      "Thomas Wiegand",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ]
  },
  "https://proceedings.mlr.press/v235/adcock24a.html": {
    "title": "A Unified Framework for Learning with Nonlinear Model Classes from Arbitrary Linear Samples",
    "volume": "main",
    "abstract": "This work considers the fundamental problem of learning an unknown object from training data using a given model class. We introduce a framework that allows for objects in arbitrary Hilbert spaces, general types of (random) linear measurements as training data and general types of nonlinear model classes. We establish a series of learning guarantees for this framework, which provide explicit relations between the amount of training data and the model class to ensure near-best generalization bounds. In doing so, we introduce the key notion of the variation of a model class with respect to a distribution of sampling operators. We show that this framework can accommodate many different types of well-known problems of interest, such as matrix sketching by random sampling, compressed sensing with isotropic vectors, active learning in regression and compressed sensing with generative models. In all cases, known results become straightforward corollaries of our general theory. Hence, this work provides a powerful framework for studying and analyzing many different types of learning problems",
    "checked": true,
    "id": "e700eacdea27a645cfbf7f294d666a92387e5483",
    "semantic_title": "a unified framework for learning with nonlinear model classes from arbitrary linear samples",
    "citation_count": 0,
    "authors": [
      "Ben Adcock",
      "Juan M. Cardenas",
      "Nick Dexter"
    ]
  },
  "https://proceedings.mlr.press/v235/adepu24a.html": {
    "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers",
    "volume": "main",
    "abstract": "Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if desired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. The code is available at https://github.com/vsingh-group/FrameQuant",
    "checked": true,
    "id": "cb2c05ab5fd155af6acf57d7b229f16e04bf3e1b",
    "semantic_title": "framequant: flexible low-bit quantization for transformers",
    "citation_count": 2,
    "authors": [
      "Harshavardhan Adepu",
      "Zhanpeng Zeng",
      "Li Zhang",
      "Vikas Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/adhikary24a.html": {
    "title": "BeigeMaps: Behavioral Eigenmaps for Reinforcement Learning from Images",
    "volume": "main",
    "abstract": "Training reinforcement learning (RL) agents directly from high-dimensional image observations continues to be a challenging problem. Recent line of work on behavioral distances proposes to learn representations that encode behavioral similarities quantified by the bisimulation metric. By learning an isometric mapping to a lower dimensional space that preserves this metric, such methods attempt to learn representations that group together functionally similar states. However, such an isometric mapping may not exist, making the learning objective ill-defined. We propose an alternative objective that allows distortions in long-range distances, while preserving local metric structure – inducing representations that highlight natural clusters in the state space. This leads to new representations, which we term Behavioral Eigenmaps (BeigeMaps), corresponding to the eigenfunctions of similarity kernels induced by behavioral distances. We empirically demonstrate that when added as a drop-in modification, BeigeMaps improve the policy performance of prior behavioral distance based RL algorithms",
    "checked": true,
    "id": "a230b99280c6c6d0a9ffb493b08bca40ef602e1a",
    "semantic_title": "beigemaps: behavioral eigenmaps for reinforcement learning from images",
    "citation_count": 0,
    "authors": [
      "Sandesh Adhikary",
      "Anqi Li",
      "Byron Boots"
    ]
  },
  "https://proceedings.mlr.press/v235/adila24a.html": {
    "title": "Discovering Bias in Latent Space: An Unsupervised Debiasing Approach",
    "volume": "main",
    "abstract": "The question-answering (QA) capabilities of foundation models are highly sensitive to prompt variations, rendering their performance susceptible to superficial, non-meaning-altering changes. This vulnerability often stems from the model's preference or bias towards specific input characteristics, such as option position or superficial image features in multi-modal settings. We propose to rectify this bias directly in the model's internal representation. Our approach, SteerFair, finds the bias direction in the model's representation space and steers activation values away from it during inference. Specifically, we exploit the observation that bias often adheres to simple association rules, such as the spurious association between the first option and correctness likelihood. Next, we construct demonstrations of these rules from unlabeled samples and use them to identify the bias directions. We empirically show that SteerFair significantly reduces instruction-tuned model performance variance across prompt modifications on three benchmark tasks. Remarkably, our approach surpasses a supervised baseline with 100 labels by an average of 10.86% accuracy points and 12.95 score points and matches the performance with 500 labels",
    "checked": true,
    "id": "5cbc3f8433576fcfd977a2f58af6968661cb9d8c",
    "semantic_title": "discovering bias in latent space: an unsupervised debiasing approach",
    "citation_count": 2,
    "authors": [
      "Dyah Adila",
      "Shuai Zhang",
      "Boran Han",
      "Bernie Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/afshani24a.html": {
    "title": "Optimal Coresets for Low-Dimensional Geometric Median",
    "volume": "main",
    "abstract": "We investigate coresets for approximating the cost with respect to median queries. In this problem, we are given a set of points $P\\subset \\mathbb{R}^d$ and median queries are $\\sum_{p\\in P} ||p-c||$ for any point $c\\in \\mathbb{R}^d$. Our goal is to compute a small weighted summary $S\\subset P$ such that the cost of any median query is approximated within a multiplicative $(1\\pm\\varepsilon)$ factor. We provide matching upper and lower bounds on the number of points contained in $S$ of the order $\\tilde{\\Theta}\\left(\\varepsilon^{-d/(d+1)}\\right)$",
    "checked": true,
    "id": "167b007a6a1513902f76a32daa7f11b4bd21e1eb",
    "semantic_title": "optimal coresets for low-dimensional geometric median",
    "citation_count": 0,
    "authors": [
      "Peyman Afshani",
      "Chris Schwiegelshohn"
    ]
  },
  "https://proceedings.mlr.press/v235/afzal24a.html": {
    "title": "REST: Efficient and Accelerated EEG Seizure Analysis through Residual State Updates",
    "volume": "main",
    "abstract": "EEG-based seizure detection models face challenges in terms of inference speed and memory efficiency, limiting their real-time implementation in clinical devices. This paper introduces a novel graph-based residual state update mechanism (REST) for real-time EEG signal analysis in applications such as epileptic seizure detection. By leveraging a combination of graph neural networks and recurrent structures, REST efficiently captures both non-Euclidean geometry and temporal dependencies within EEG data. Our model demonstrates high accuracy in both seizure detection and classification tasks. Notably, REST achieves a remarkable 9-fold acceleration in inference speed compared to state-of-the-art models, while simultaneously demanding substantially less memory than the smallest model employed for this task. These attributes position REST as a promising candidate for real-time implementation in clinical devices, such as Responsive Neurostimulation or seizure alert systems",
    "checked": true,
    "id": "6dcfaacb4333afb240719af712366a059ccc67a3",
    "semantic_title": "rest: efficient and accelerated eeg seizure analysis through residual state updates",
    "citation_count": 1,
    "authors": [
      "Arshia Afzal",
      "Grigorios Chrysos",
      "Volkan Cevher",
      "Mahsa Shoaran"
    ]
  },
  "https://proceedings.mlr.press/v235/agarwal24a.html": {
    "title": "CHAI: Clustered Head Attention for Efficient LLM Inference",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-head attention is one of the key components of LLMs, which can for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered HeadAttention ( CHAI ). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73× without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets",
    "checked": true,
    "id": "7b46d6e2e00a4b4693dbadbbee3a2586bbae3c2f",
    "semantic_title": "chai: clustered head attention for efficient llm inference",
    "citation_count": 3,
    "authors": [
      "Saurabh Agarwal",
      "Bilge Acun",
      "Basil Hosmer",
      "Mostafa Elhoushi",
      "Yejin Lee",
      "Shivaram Venkataraman",
      "Dimitris Papailiopoulos",
      "Carole-Jean Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/agarwal24b.html": {
    "title": "Learning to Play Atari in a World of Tokens",
    "volume": "main",
    "abstract": "Model-based reinforcement learning agents utilizing transformers have shown improved sample efficiency due to their ability to model extended context, resulting in more accurate world models. However, for complex reasoning and planning tasks, these methods primarily rely on continuous representations. This complicates modeling of discrete properties of the real world such as disjoint object classes between which interpolation is not plausible. In this work, we introduce discrete abstract representations for transformer-based learning (DART), a sample-efficient method utilizing discrete representations for modeling both the world and learning behavior. We incorporate a transformer-decoder for auto-regressive world modeling and a transformer-encoder for learning behavior by attending to task-relevant cues in the discrete representation of the world model. For handling partial observability, we aggregate information from past time steps as memory tokens. DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games. We release our code at https://pranaval.github.io/DART/",
    "checked": true,
    "id": "c91e2861c2b09215ed053fe064b0cbf3b9a9d353",
    "semantic_title": "learning to play atari in a world of tokens",
    "citation_count": 0,
    "authors": [
      "Pranav Agarwal",
      "Sheldon Andrews",
      "Samira Ebrahimi Kahou"
    ]
  },
  "https://proceedings.mlr.press/v235/agarwal24c.html": {
    "title": "Probabilistic Generating Circuits - Demystified",
    "volume": "main",
    "abstract": "Zhang et al. (ICML 2021, PLMR 139, pp. 12447–12457) introduced probabilistic generating circuits (PGCs) as a probabilistic model to unify probabilistic circuits (PCs) and determinantal point processes (DPPs). At a first glance, PGCs store a distribution in a very different way, they compute the probability generating polynomial instead of the probability mass function and it seems that this is the main reason why PGCs are more powerful than PCs or DPPs. However, PGCs also allow for negative weights, whereas classical PCs assume that all weights are nonnegative. One main insight of this work is that the negative weights are the cause for the power of PGCs and not the different representation. PGCs are PCs in disguise: we show how to transform any PGC on binary variables into a PC with negative weights with only polynomial blowup. PGCs were defined by Zhang et al. only for binary random variables. As our second main result, we show that there is a good reason for this: we prove that PGCs for categorical variables with larger image size do not support tractable marginalization unless NP=P. On the other hand, we show that we can model categorical variables with larger image size as PC with negative weights computing set-multilinear polynomials. These allow for tractable marginalization. In this sense, PCs with negative weights strictly subsume PGCs",
    "checked": true,
    "id": "ca724adee2c54c66db260885217a5d62f478130d",
    "semantic_title": "probabilistic generating circuits - demystified",
    "citation_count": 0,
    "authors": [
      "Sanyam Agarwal",
      "Markus Bläser"
    ]
  },
  "https://proceedings.mlr.press/v235/agarwal24d.html": {
    "title": "Improved Differentially Private and Lazy Online Convex Optimization: Lower Regret without Smoothness Requirements",
    "volume": "main",
    "abstract": "We design differentially private regret-minimizing algorithms in the online convex optimization (OCO) framework. Unlike recent results, our algorithms and analyses do not require smoothness, thus yielding the first private regret bounds with an optimal leading-order term for non-smooth loss functions. Additionally, even for smooth losses, the resulting regret guarantees improve upon previous results in terms their dependence of dimension. Our results provide the best known rates for DP-OCO in all practical regimes of the privacy parameter, barring when it is exceptionally small. The principal innovation in our algorithm design is the use of sampling from strongly log-concave densities which satisfy the Log-Sobolev Inequality. The resulting concentration of measure allows us to obtain a better trade-off for the dimension factors than prior work, leading to improved results. Following previous works on DP-OCO, the proposed algorithm explicitly limits the number of switches via rejection sampling. Thus, independently of privacy constraints, the algorithm also provides improved results for online convex optimization with a switching budget",
    "checked": true,
    "id": "06eefa46ed9fdf62dfc3365d0fe366084c496b3f",
    "semantic_title": "improved differentially private and lazy online convex optimization: lower regret without smoothness requirements",
    "citation_count": 0,
    "authors": [
      "Naman Agarwal",
      "Satyen Kale",
      "Karan Singh",
      "Abhradeep Guha Thakurta"
    ]
  },
  "https://proceedings.mlr.press/v235/agarwal24e.html": {
    "title": "The Non-linear $F$-Design and Applications to Interactive Learning",
    "volume": "main",
    "abstract": "We propose a generalization of the classical G-optimal design concept to non-linear function classes. The criterion, termed F -design, coincides with G-design in the linear case. We compute the value of the optimal design, termed the F-condition number, for several non-linear function classes. We further provide algorithms to construct designs with a bounded F -condition number. Finally, we employ the F-design in a variety of interactive machine learning tasks, where the design is naturally useful for data collection or exploration. We show that in four diverse settings of confidence band construction, contextual bandits, model-free reinforcement learning, and active learning, F-design can be combined with existing approaches in a black-box manner to yield state-of-the-art results in known problem settings as well as to generalize to novel ones",
    "checked": false,
    "id": "146c7b63449dc4a54e9a9d829ee994bc6d99a985",
    "semantic_title": "the non-linear f-design and applications to interactive learning",
    "citation_count": 1,
    "authors": [
      "Alekh Agarwal",
      "Jian Qian",
      "Alexander Rakhlin",
      "Tong Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/agnihotri24a.html": {
    "title": "ACPO: A Policy Optimization Algorithm for Average MDPs with Constraints",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) for constrained MDPs (CMDPs) is an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average-CMDPs (ACMDPs) remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by trust region-based policy optimization algorithms. We develop basic sensitivity theory for average CMDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging OpenAI Gym environments, show its superior empirical performance when compared to other state-of-the-art algorithms adapted for the ACMDPs",
    "checked": true,
    "id": "3dbc8d2a5f4bd8792c5de9bfda879845c4200b66",
    "semantic_title": "acpo: a policy optimization algorithm for average mdps with constraints",
    "citation_count": 1,
    "authors": [
      "Akhil Agnihotri",
      "Rahul Jain",
      "Haipeng Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/agnihotri24b.html": {
    "title": "CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks",
    "volume": "main",
    "abstract": "While neural networks allow highly accurate predictions in many tasks, their lack of robustness towards even slight input perturbations often hampers their deployment. Adversarial attacks such as the seminal projected gradient descent (PGD) offer an effective means to evaluate a model's robustness and dedicated solutions have been proposed for attacks on semantic segmentation or optical flow estimation. While they attempt to increase the attack's efficiency, a further objective is to balance its effect, so that it acts on the entire image domain instead of isolated point-wise predictions. This often comes at the cost of optimization stability and thus efficiency. Here, we propose CosPGD, an attack that encourages more balanced errors over the entire image domain while increasing the attack's overall efficiency. To this end, CosPGD leverages a simple alignment score computed from any pixel-wise prediction and its target to scale the loss in a smooth and fully differentiable way. It leads to efficient evaluations of a model's robustness for semantic segmentation as well as regression models (such as optical flow, disparity estimation, or image restoration), and it allows it to outperform the previous SotA attack on semantic segmentation. We provide code for the CosPGD algorithm and example usage at https://github.com/shashankskagnihotri/cospgd",
    "checked": true,
    "id": "ba9442b00d7cc6bb44404afdab983dabd9e450ed",
    "semantic_title": "cospgd: an efficient white-box adversarial attack for pixel-wise prediction tasks",
    "citation_count": 17,
    "authors": [
      "Shashank Agnihotri",
      "Steffen Jung",
      "Margret Keuper"
    ]
  },
  "https://proceedings.mlr.press/v235/agostinelli-iii24a.html": {
    "title": "LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions",
    "volume": "main",
    "abstract": "A promising approach to preserving model performance in linearized transformers is to employ position-based re-weighting functions. However, state-of-the-art re-weighting functions rely heavily on target sequence lengths, making it difficult or impossible to apply them to autoregressive and simultaneous tasks, where the target and sometimes even the input sequence length are unknown. To address this issue, we propose Learned Proportions (LeaP) and LeaPformers. Our contribution is built on two major components. First, we generalize the dependence on explicit positional representations and sequence lengths into dependence on sequence proportions for re-weighting. Second, we replace static positional representations with dynamic proportions derived via a compact module, enabling more flexible attention concentration patterns. We evaluate LeaPformer against eight representative efficient transformers on the Long-Range Arena benchmark, where we show that LeaPformer achieves the best quality-throughput trade-off, as well as apply LeaPformer to Wikitext-103b autoregressive language modeling and simultaneous speech-to-text translation for two language pairs, achieving competitive results in both tasks",
    "checked": true,
    "id": "fdf9545ed4c9970a003d9a856101b606ba01fce2",
    "semantic_title": "leapformer: enabling linear transformers for autoregressive and simultaneous tasks via learned proportions",
    "citation_count": 0,
    "authors": [
      "Victor Agostinelli Iii",
      "Sanghyun Hong",
      "Lizhong Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/agrawal24a.html": {
    "title": "Policy Evaluation for Variance in Average Reward Reinforcement Learning",
    "volume": "main",
    "abstract": "We consider an average reward reinforcement learning (RL) problem and work with asymptotic variance as a risk measure to model safety-critical applications. We design a temporal-difference (TD) type algorithm tailored for policy evaluation in this context. Our algorithm is based on linear stochastic approximation of an equivalent formulation of the asymptotic variance in terms of the solution of the Poisson equation. We consider both the tabular and linear function approximation settings, and establish $\\tilde {O}(1/k)$ finite time convergence rate, where $k$ is the number of steps of the algorithm. Our work paves the way for developing actor-critic style algorithms for variance-constrained RL. To the best of our knowledge, our result provides the first sequential estimator for asymptotic variance of a Markov chain with provable finite sample guarantees, which is of independent interest",
    "checked": true,
    "id": "8559788fc018e47a1aec80752aef1f94c6e9cb17",
    "semantic_title": "policy evaluation for variance in average reward reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Shubhada Agrawal",
      "Prashanth L A",
      "Siva Theja Maguluri"
    ]
  },
  "https://proceedings.mlr.press/v235/ahdritz24a.html": {
    "title": "Distinguishing the Knowable from the Unknowable with Language Models",
    "volume": "main",
    "abstract": "We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings. Code can be found at: https://github.com/KempnerInstitute/llm_uncertainty",
    "checked": true,
    "id": "4862ea8e9bd568a7ea5d3347ebb6d2a8d7f80ecc",
    "semantic_title": "distinguishing the knowable from the unknowable with language models",
    "citation_count": 12,
    "authors": [
      "Gustaf Ahdritz",
      "Tian Qin",
      "Nikhil Vyas",
      "Boaz Barak",
      "Benjamin L. Edelman"
    ]
  },
  "https://proceedings.mlr.press/v235/ahmadian24a.html": {
    "title": "Unmasking Vulnerabilities: Cardinality Sketches under Adaptive Inputs",
    "volume": "main",
    "abstract": "Cardinality sketches are popular data structures that enhance the efficiency of working with large data sets. The sketches are randomized representations of sets that are only of logarithmic size but can support set merges and approximate cardinality (i.e., distinct count) queries. When queries are not adaptive, that is, they do not depend on preceding query responses, the design provides strong guarantees of correctly answering a number of queries exponential in the sketch size $k$. In this work, we investigate the performance of cardinality sketches in adaptive settings and unveil inherent vulnerabilities. We design an attack against the \"standard\" estimators that constructs an adversarial input by post-processing responses to a set of simple non-adaptive queries of size linear in the sketch size $k$. Empirically, our attack used only $4k$ queries with the widely used HyperLogLog (HLL++) Flajolet et al., 2007; Heule et al., 2013) sketch. The simple attack technique suggests it can be effective with post-processed natural workloads. Finally and importantly, we demonstrate that the vulnerability is inherent as any estimator applied to known sketch structures can be attacked using a number of queries that is quadratic in $k$, matching a generic upper bound",
    "checked": true,
    "id": "abd19ce1ab86a64293873cc02d778f9ea12833b2",
    "semantic_title": "unmasking vulnerabilities: cardinality sketches under adaptive inputs",
    "citation_count": 2,
    "authors": [
      "Sara Ahmadian",
      "Edith Cohen"
    ]
  },
  "https://proceedings.mlr.press/v235/ahmaditeshnizi24a.html": {
    "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models",
    "volume": "main",
    "abstract": "Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20$% and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30$%. The implementation and the datasets are available at https://github.com/teshnizi/OptiMUS",
    "checked": true,
    "id": "6857a1439cfd5057d7e129eff16e5a58ab94bf14",
    "semantic_title": "optimus: scalable optimization modeling with (mi)lp solvers and large language models",
    "citation_count": 8,
    "authors": [
      "Ali Ahmaditeshnizi",
      "Wenzhi Gao",
      "Madeleine Udell"
    ]
  },
  "https://proceedings.mlr.press/v235/ahn24a.html": {
    "title": "How to Escape Sharp Minima with Random Perturbations",
    "volume": "main",
    "abstract": "Modern machine learning applications have witnessed the remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this design choice, we undertake a formal study that (i) formulates the notion of flat minima, and (ii) studies the complexity of finding them. Specifically, we adopt the trace of the Hessian of the cost function as a measure of flatness, and use it to formally define the notion of approximate flat minima. Under this notion, we then analyze algorithms that find approximate flat minima efficiently. For general cost functions, we discuss a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, supporting its success in practice",
    "checked": true,
    "id": "68f33a6c37fd77517bfb24a9b35cbb072057f05f",
    "semantic_title": "how to escape sharp minima with random perturbations",
    "citation_count": 6,
    "authors": [
      "Kwangjun Ahn",
      "Ali Jadbabaie",
      "Suvrit Sra"
    ]
  },
  "https://proceedings.mlr.press/v235/ahn24b.html": {
    "title": "Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise",
    "volume": "main",
    "abstract": "Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited. In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD. In this work, we provide a different perspective based on online learning that underscores the importance of Adam's algorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates/increments, where we choose the updates/increments of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective",
    "checked": true,
    "id": "b28941cb31500f65c2e2ae33be03144dbf4a53e0",
    "semantic_title": "understanding adam optimizer via online learning of updates: adam is ftrl in disguise",
    "citation_count": 8,
    "authors": [
      "Kwangjun Ahn",
      "Zhiyu Zhang",
      "Yunbum Kook",
      "Yan Dai"
    ]
  },
  "https://proceedings.mlr.press/v235/ai24a.html": {
    "title": "Not all distributional shifts are equal: Fine-grained robust conformal inference",
    "volume": "main",
    "abstract": "We introduce a fine-grained framework for uncertainty quantification of predictive models under distributional shifts. This framework distinguishes the shift in covariate distributions from that in the conditional relationship between the outcome ($Y$) and the covariates ($X$). We propose to reweight the training samples to adjust for an identifiable shift in covariate distribution while protecting against the worst-case conditional distribution shift bounded in an $f$-divergence ball. Based on ideas from conformal inference and distributionally robust learning, we present an algorithm that outputs (approximately) valid and efficient prediction intervals in the presence of distributional shifts. As a use case, we apply the framework to sensitivity analysis of individual treatment effects with hidden confounding. The proposed methods are evaluated in simulations and four real data applications, demonstrating superior robustness and efficiency compared with existing benchmarks",
    "checked": true,
    "id": "c419b6952b204798b5a36d04e4a82b75aabeb581",
    "semantic_title": "not all distributional shifts are equal: fine-grained robust conformal inference",
    "citation_count": 2,
    "authors": [
      "Jiahao Ai",
      "Zhimei Ren"
    ]
  },
  "https://proceedings.mlr.press/v235/akbari24a.html": {
    "title": "Triple Changes Estimator for Targeted Policies",
    "volume": "main",
    "abstract": "The renowned difference-in-differences (DiD) estimator relies on the assumption of 'parallel trends,' which may not hold in many practical applications. To address this issue, economists are increasingly considering the triple difference estimator as a more credible alternative. Both DiD and triple difference are limited to assessing average effects exclusively. An alternative avenue is offered by the changes-in-changes (CiC) estimator, which provides an estimate of the entire counterfactual distribution by relying on assumptions imposed on the distribution of potential outcomes. In this work, we extend the triple difference estimator to accommodate the CiC framework, presenting the ‘triple changes estimator' and its identification assumptions, thereby expanding the scope of the CiC paradigm. Subsequently, we empirically evaluate the proposed framework and apply it to a study examining the impact of Medicaid expansion on children's preventive care",
    "checked": true,
    "id": "0a7b47b3d3309db8f1520f2192a6dc078107b7db",
    "semantic_title": "triple changes estimator for targeted policies",
    "citation_count": 0,
    "authors": [
      "Sina Akbari",
      "Negar Kiyavash"
    ]
  },
  "https://proceedings.mlr.press/v235/akbarian24a.html": {
    "title": "Improving Computational Complexity in Statistical Models with Local Curvature Information",
    "volume": "main",
    "abstract": "It is known that when the statistical models are singular, i.e., the Fisher information matrix at the true parameter is degenerate, the fixed step-size gradient descent algorithm takes polynomial number of steps in terms of the sample size $n$ to converge to a final statistical radius around the true parameter, which can be unsatisfactory for the practical application. To further improve that computational complexity, we consider utilizing the local curvature information for parameter estimation. Even though there is a rich literature in using the local curvature information for optimization, the statistical rate of these methods in statistical models, to the best of our knowledge, has not been studied rigorously. The major challenge of this problem is due to the non-convex nature of sample loss function. To shed light on these problems, we specifically study the normalized gradient descent (NormGD) algorithm, a variant of gradient descent algorithm whose step size is scaled by the maximum eigenvalue of the Hessian matrix of the empirical loss function, and deal with the aforementioned issue with a population-to-sample analysis. When the population loss function is homogeneous, the NormGD iterates reach a final statistical radius around the true parameter after a logarithmic number of iterations in terms of $n$. Therefore, for fixed dimension $d$, the NormGD algorithm achieves the optimal computational complexity $\\mathcal{O}(n)$ to reach the final statistical radius, which is cheaper than the complexity $\\mathcal{O}(n^{\\tau})$ of the fixed step-size gradient descent algorithm for some $\\tau > 1$",
    "checked": true,
    "id": "55ae6e1733950dac04fd8de2c0e9f776d927a907",
    "semantic_title": "improving computational complexity in statistical models with local curvature information",
    "citation_count": 0,
    "authors": [
      "Pedram Akbarian",
      "Tongzheng Ren",
      "Jiacheng Zhuo",
      "Sujay Sanghavi",
      "Nhat Ho"
    ]
  },
  "https://proceedings.mlr.press/v235/akeweje24a.html": {
    "title": "Learning Mixtures of Gaussian Processes through Random Projection",
    "volume": "main",
    "abstract": "We propose an ensemble clustering framework to uncover latent cluster labels in functional data generated from a Gaussian process mixture. Our method exploits the fact that the projection coefficients of the functional data onto any given projection function follow a univariate Gaussian mixture model (GMM). By conducting multiple one-dimensional projections and learning a univariate GMM for each, we create an ensemble of GMMs. Each GMM serves as a base clustering, and applying ensemble clustering yields a consensus clustering. Our approach significantly reduces computational complexity compared to state-of-the-art methods, and we provide theoretical guarantees on the identifiability and learnability of Gaussian process mixtures. Extensive experiments on synthetic and real datasets confirm the superiority of our method over existing techniques",
    "checked": true,
    "id": "7a9569c063c91f25451a0608f12f9b888e120098",
    "semantic_title": "learning mixtures of gaussian processes through random projection",
    "citation_count": 0,
    "authors": [
      "Emmanuel Akeweje",
      "Mimi Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/akhauri24a.html": {
    "title": "Encodings for Prediction-based Neural Architecture Search",
    "volume": "main",
    "abstract": "Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization. The efficacy of these predictors is largely influenced by the method of encoding neural network architectures. While traditional encodings used an adjacency matrix describing the graph structure of a neural network, novel encodings embrace a variety of approaches from unsupervised pretraining of latent representations to vectors of zero-cost proxies. In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based. Furthermore, we extend these encodings and introduce unified encodings, that extend NAS predictors to multiple search spaces. Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101. Building on our study, we present our predictor FLAN: Flow Attention for NAS. FLAN integrates critical insights on predictor design, transfer learning, and unified encodings to enable more than an order of magnitude cost reduction for training NAS accuracy predictors. Our implementation and encodings for all neural networks are open-sourced at https://github.com/abdelfattah-lab/flan_nas",
    "checked": true,
    "id": "972767946a5dde9d7da5c8545c172455331b186b",
    "semantic_title": "encodings for prediction-based neural architecture search",
    "citation_count": 1,
    "authors": [
      "Yash Akhauri",
      "Mohamed S Abdelfattah"
    ]
  },
  "https://proceedings.mlr.press/v235/akhound-sadegh24a.html": {
    "title": "Iterated Denoising Energy Matching for Sampling from Boltzmann Densities",
    "volume": "main",
    "abstract": "Efficiently generating statistically independent samples from an unnormalized probability distribution, such as equilibrium samples of many-body systems, is a foundational problem in science. In this paper, we propose Iterated Denoising Energy Matching (iDEM), an iterative algorithm that uses a novel stochastic score matching objective leveraging solely the energy function and its gradient—and no data samples—to train a diffusion-based sampler. Specifically, iDEM alternates between (I) sampling regions of high model density from a diffusion-based sampler and (II) using these samples in our stochastic matching objective to further improve the sampler. iDEM is scalable to high dimensions as the inner matching objective, is simulation-free, and requires no MCMC samples. Moreover, by leveraging the fast mode mixing behavior of diffusion, iDEM smooths out the energy landscape enabling efficient exploration and learning of an amortized sampler. We evaluate iDEM on a suite of tasks ranging from standard synthetic energy functions to invariant $n$-body particle systems. We show that the proposed approach achieves state-of-the-art performance on all metrics and trains $2-5\\times$ faster, which allows it to be the first method to train using energy on the challenging $55$-particle Lennard-Jones system",
    "checked": true,
    "id": "9f0f134bc53f9aea130f1a561acb5fa6b08ae4ce",
    "semantic_title": "iterated denoising energy matching for sampling from boltzmann densities",
    "citation_count": 21,
    "authors": [
      "Tara Akhound-Sadegh",
      "Jarrid Rector-Brooks",
      "Joey Bose",
      "Sarthak Mittal",
      "Pablo Lemos",
      "Cheng-Hao Liu",
      "Marcin Sendera",
      "Siamak Ravanbakhsh",
      "Gauthier Gidel",
      "Yoshua Bengio",
      "Nikolay Malkin",
      "Alexander Tong"
    ]
  },
  "https://proceedings.mlr.press/v235/akyurek24a.html": {
    "title": "In-Context Language Learning: Architectures and Algorithms",
    "volume": "main",
    "abstract": "Some neural language models (LMs) exhibit a remarkable capacity for in-context learning (ICL): they can fit predictors to datasets provided as input. While the mechanisms underlying ICL are well-studied in the context of synthetic problems like in-context linear regression, there is still some divergence between these model problems and the \"real\" ICL exhibited by LMs trained on large text corpora. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in- context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models on regular ICLL tasks. We first show that Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks. Next, we provide evidence that they do so by computing in-context n-gram statistics using specialized attention heads. Finally, we show that hard-wiring these heads into neural models improves performance not just on synthetic ICLL, but natural language modeling, reducing the perplexity of 340M-parameter Transformers by up to 1.14 points (6.7%) on the SlimPajama dataset. Our results highlight the usefulness of in-context formal language learning as a tool for understanding ICL in models of natural text",
    "checked": true,
    "id": "85447eeb6e5276e713957835125a2273f9ac0694",
    "semantic_title": "in-context language learning: architectures and algorithms",
    "citation_count": 27,
    "authors": [
      "Ekin Akyürek",
      "Bailin Wang",
      "Yoon Kim",
      "Jacob Andreas"
    ]
  },
  "https://proceedings.mlr.press/v235/al-jarrah24a.html": {
    "title": "Nonlinear Filtering with Brenier Optimal Transport Maps",
    "volume": "main",
    "abstract": "This paper is concerned with the problem of nonlinear filtering, i.e., computing the conditional distribution of the state of a stochastic dynamical system given a history of noisy partial observations. Conventional sequential importance resampling (SIR) particle filters suffer from fundamental limitations, in scenarios involving degenerate likelihoods or high-dimensional states, due to the weight degeneracy issue. In this paper, we explore an alternative method, which is based on estimating the Brenier optimal transport (OT) map from the current prior distribution of the state to the posterior distribution at the next time step. Unlike SIR particle filters, the OT formulation does not require the analytical form of the likelihood. Moreover, it allows us to harness the approximation power of neural networks to model complex and multi-modal distributions and employ stochastic optimization algorithms to enhance scalability. Extensive numerical experiments are presented that compare the OT method to the SIR particle filter and the ensemble Kalman filter, evaluating the performance in terms of sample efficiency, high-dimensional scalability, and the ability to capture complex and multi-modal distributions",
    "checked": true,
    "id": "1ffd2ff11b5b65a83c2224c56731d3a9c01d196c",
    "semantic_title": "nonlinear filtering with brenier optimal transport maps",
    "citation_count": 1,
    "authors": [
      "Mohammad Al-Jarrah",
      "Niyizhen Jin",
      "Bamdad Hosseini",
      "Amirhossein Taghvaei"
    ]
  },
  "https://proceedings.mlr.press/v235/alacaoglu24a.html": {
    "title": "Revisiting Inexact Fixed-Point Iterations for Min-Max Problems: Stochasticity and Structured Nonconvexity",
    "volume": "main",
    "abstract": "We focus on constrained, $L$-smooth, potentially stochastic and nonconvex-nonconcave min-max problems either satisfying $\\rho$-cohypomonotonicity or admitting a solution to the $\\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\\rho>0$ correspond to a greater degree of nonconvexity. These problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail. It has been conjectured that first-order methods can tolerate a value of $\\rho$ no larger than $\\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\\rho < \\frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\\rho < \\frac{1}{L}$. First main insight for the improvements in the convergence analyses is to harness the recently proposed conic nonexpansiveness property of operators. Second, we provide a refined analysis for inexact Halpern iteration that relaxes the required inexactness level to improve some state-of-the-art complexity results even for constrained stochastic convex-concave min-max problems. Third, we analyze a stochastic inexact Krasnosel'skii-Mann iteration with a multilevel Monte Carlo estimator when the assumptions only hold with respect to a solution",
    "checked": true,
    "id": "5ec146087bec8961407a7ec2c8de287e3f7ee201",
    "semantic_title": "revisiting inexact fixed-point iterations for min-max problems: stochasticity and structured nonconvexity",
    "citation_count": 5,
    "authors": [
      "Ahmet Alacaoglu",
      "Donghwan Kim",
      "Stephen Wright"
    ]
  },
  "https://proceedings.mlr.press/v235/alain24a.html": {
    "title": "Gaussian Processes on Cellular Complexes",
    "volume": "main",
    "abstract": "In recent years, there has been considerable interest in developing machine learning models on graphs to account for topological inductive biases. In particular, recent attention has been given to Gaussian processes on such structures since they can additionally account for uncertainty. However, graphs are limited to modelling relations between two vertices. In this paper, we go beyond this dyadic setting and consider polyadic relations that include interactions between vertices, edges and one of their generalisations, known as cells. Specifically, we propose Gaussian processes on cellular complexes, a generalisation of graphs that captures interactions between these higher-order cells. One of our key contributions is the derivation of two novel kernels, one that generalises the graph Matérn kernel and one that additionally mixes information of different cell types",
    "checked": true,
    "id": "2a130f61ce5ab6c2367b49082f680386f3eb86f1",
    "semantic_title": "gaussian processes on cellular complexes",
    "citation_count": 7,
    "authors": [
      "Mathieu Alain",
      "So Takao",
      "Brooks Paige",
      "Marc Peter Deisenroth"
    ]
  },
  "https://proceedings.mlr.press/v235/alamdari24a.html": {
    "title": "Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making",
    "volume": "main",
    "abstract": "Fair decision making has largely been studied with respect to a single decision. Here we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points within the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We explore the interplay between non-Markovian fairness and memory and how memory can support construction of fair policies. Finally, we introduce the FairQCM algorithm, which can automatically augment its training data to improve sample efficiency in the synthesis of fair policies via reinforcement learning",
    "checked": true,
    "id": "47e49c575eee9607b873c2f6e80816ec4616f521",
    "semantic_title": "remembering to be fair: non-markovian fairness in sequential decision making",
    "citation_count": 2,
    "authors": [
      "Parand A. Alamdari",
      "Toryn Q. Klassen",
      "Elliot Creager",
      "Sheila A. Mcilraith"
    ]
  },
  "https://proceedings.mlr.press/v235/albergo24a.html": {
    "title": "Stochastic Interpolants with Data-Dependent Couplings",
    "volume": "main",
    "abstract": "Generative models inspired by dynamical transport of measure – such as flows and diffusions – construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to couple the base and the target densities, whereby samples from the base are computed conditionally given samples from the target in a way that is different from (but does not preclude) incorporating information about class labels or continuous embeddings. This enables us to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting. The code is available at https://github.com/interpolants/couplings",
    "checked": true,
    "id": "c34cc8723ab763caab3596c395e44d755f533bad",
    "semantic_title": "stochastic interpolants with data-dependent couplings",
    "citation_count": 19,
    "authors": [
      "Michael Samuel Albergo",
      "Mark Goldstein",
      "Nicholas Matthew Boffi",
      "Rajesh Ranganath",
      "Eric Vanden-Eijnden"
    ]
  },
  "https://proceedings.mlr.press/v235/albuquerque24a.html": {
    "title": "Evaluating Model Bias Requires Characterizing its Mistakes",
    "volume": "main",
    "abstract": "The ability to properly benchmark model performance in the face of spurious correlations is important to both build better predictors and increase confidence that models are operating as intended. We demonstrate that characterizing (as opposed to simply quantifying) model mistakes across subgroups is pivotal to properly reflect model biases, which are ignored by standard metrics such as worst-group accuracy or accuracy gap. Inspired by the hypothesis testing framework, we introduce SkewSize, a principled and flexible metric that captures bias from mistakes in a model's predictions. It can be used in multi-class settings or generalised to the open vocabulary setting of generative models. SkewSize is an aggregation of the effect size of the interaction between two categorical variables: the spurious variable representing the bias attribute the model's prediction. We demonstrate the utility of SkewSize in multiple settings including: standard vision models trained on synthetic data, vision models trained on ImageNet, and large scale vision-and-language models from the BLIP-2 family. In each case, the proposed SkewSize is able to highlight biases not captured by other metrics, while also providing insights on the impact of recently proposed techniques, such as instruction tuning",
    "checked": true,
    "id": "23e62f1c4b79c070d180eebba853326d942cb66b",
    "semantic_title": "evaluating model bias requires characterizing its mistakes",
    "citation_count": 1,
    "authors": [
      "Isabela Albuquerque",
      "Jessica Schrouff",
      "David Warde-Farley",
      "Ali Taylan Cemgil",
      "Sven Gowal",
      "Olivia Wiles"
    ]
  },
  "https://proceedings.mlr.press/v235/alder24a.html": {
    "title": "Energy-Efficient Gaussian Processes Using Low-Precision Arithmetic",
    "volume": "main",
    "abstract": "The widespread use of artificial intelligence requires finding energy-efficient paradigms for the field. We propose to reduce the energy consumption of Gaussian process regression using low-precision floating-point representations. We explore how low-precision representations impact the results of Gaussian process regression and how data set properties, implementation approach, model performance, and energy consumption interact. Our findings show that a well-conditioned kernel matrix allows reducing the energy consumption by up to 89.01% for 98.08% of arithmetic operations with little to no impact on model performance. Our findings are relevant whenever one needs to invert a symmetric full-rank matrix",
    "checked": true,
    "id": "2b17481e44cd0ce1bbeb7507b6336956c4e46688",
    "semantic_title": "energy-efficient gaussian processes using low-precision arithmetic",
    "citation_count": 0,
    "authors": [
      "Nicolas Alder",
      "Ralf Herbrich"
    ]
  },
  "https://proceedings.mlr.press/v235/alfarra24a.html": {
    "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
    "volume": "main",
    "abstract": "This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods, which penalizes slower methods by providing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Though many effective methods have been proposed, their impressive performance usually comes at the cost of significantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA methods, where data is received in an online fashion from a constant-speed data stream, thereby accounting for the method's adaptation speed. We apply our proposed protocol to benchmark several TTA methods on multiple datasets and scenarios. Extensive experiments shows that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower methods. For example, SHOT from 2020, outperforms the state-of-the-art method SAR from 2023 under our online setting. Our results reveal the importance of developing practical TTA methods that are both accurate and efficient",
    "checked": true,
    "id": "7a59f44f9e9ce84d642d24f03c15860e19cbfa87",
    "semantic_title": "evaluation of test-time adaptation under computational time constraints",
    "citation_count": 3,
    "authors": [
      "Motasem Alfarra",
      "Hani Itani",
      "Alejandro Pardo",
      "Shyma Yaser Alhuwaider",
      "Merey Ramazanova",
      "Juan Camilo Perez",
      "Zhipeng Cai",
      "Matthias Müller",
      "Bernard Ghanem"
    ]
  },
  "https://proceedings.mlr.press/v235/ali-mehmeti-gopel24a.html": {
    "title": "On the Weight Dynamics of Deep Normalized Networks",
    "volume": "main",
    "abstract": "Recent studies have shown that high disparities in effective learning rates (ELRs) across layers in deep neural networks can negatively affect trainability. We formalize how these disparities evolve over time by modeling weight dynamics (evolution of expected gradient and weight norms) of networks with normalization layers, predicting the evolution of layer-wise ELR ratios. We prove that when training with any constant learning rate, ELR ratios converge to 1, despite initial gradient explosion. We identify a \"critical learning rate\" beyond which ELR disparities widen, which only depends on current ELRs. To validate our findings, we devise a hyper-parameter-free warm-up method that successfully minimizes ELR spread quickly in theory and practice. Our experiments link ELR spread with trainability, a relationship that is most evident in very deep networks with significant gradient magnitude excursions",
    "checked": true,
    "id": "ce6ab017cde568a6bf87316bdb4ce72029cc1318",
    "semantic_title": "on the weight dynamics of deep normalized networks",
    "citation_count": 0,
    "authors": [
      "Christian H.X. Ali Mehmeti-Göpel",
      "Michael Wand"
    ]
  },
  "https://proceedings.mlr.press/v235/alishahi24a.html": {
    "title": "No Dimensional Sampling Coresets for Classification",
    "volume": "main",
    "abstract": "We refine and generalize what is known about coresets for classification problems via the sensitivity sampling framework. Such coresets seek the smallest possible subsets of input data, so one can optimize a loss function on the coreset and ensure approximation guarantees with respect to the original data. Our analysis provides the first no dimensional coresets, so the size does not depend on the dimension. Moreover, our results are general, apply for distributional input and can use iid samples, so provide sample complexity bounds, and work for a variety of loss functions. A key tool we develop is a Radamacher complexity version of the main sensitivity sampling approach, which can be of independent interest",
    "checked": true,
    "id": "cdda6d099fa12e87faa6fec000452180b4152eb5",
    "semantic_title": "no dimensional sampling coresets for classification",
    "citation_count": 0,
    "authors": [
      "Meysam Alishahi",
      "Jeff M. Phillips"
    ]
  },
  "https://proceedings.mlr.press/v235/allamanis24a.html": {
    "title": "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness",
    "volume": "main",
    "abstract": "To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader set of domains and tasks which was not previously possible without costly human annotations",
    "checked": true,
    "id": "9dbdc804fc37c55bfff4f595b043770262810533",
    "semantic_title": "unsupervised evaluation of code llms with round-trip correctness",
    "citation_count": 7,
    "authors": [
      "Miltiadis Allamanis",
      "Sheena Panthaplackel",
      "Pengcheng Yin"
    ]
  },
  "https://proceedings.mlr.press/v235/allen-zhu24a.html": {
    "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
    "volume": "main",
    "abstract": "Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g., \"What is Abraham Lincoln's birthday?\"). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various diversity measures of the training data. Essentially, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling) during pretraining. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and how the model internally encodes knowledge — whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. This paper provides several key recommendations for LLM pretraining in the industry: (1) rewrite the pretraining data — using small, auxiliary models — to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late",
    "checked": true,
    "id": "f29f8b8aa2b7e608199b65d3cf751969d4024132",
    "semantic_title": "physics of language models: part 3.1, knowledge storage and extraction",
    "citation_count": 70,
    "authors": [
      "Zeyuan Allen-Zhu",
      "Yuanzhi Li"
    ]
  },
  "https://proceedings.mlr.press/v235/allouah24a.html": {
    "title": "Byzantine-Robust Federated Learning: Impact of Client Subsampling and Local Updates",
    "volume": "main",
    "abstract": "The possibility of adversarial (a.k.a., Byzantine) clients makes federated learning (FL) prone to arbitrary manipulation. The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\\mathsf{FedAvg}$ algorithm by a robust averaging rule. While a significant amount of work has been devoted to studying the convergence of federated robust averaging (which we denote by $\\mathsf{FedRo}$), prior work has largely ignored the impact of client subsampling and local steps, two fundamental FL characteristics. While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients. Consequently, a careless deployment of $\\mathsf{FedRo}$ could yield poor performance. We validate this observation by presenting an in-depth analysis of $\\mathsf{FedRo}$ tightly analyzing the impact of client subsampling and local steps. Specifically, we present a sufficient condition on client subsampling for nearly-optimal convergence of $\\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show that the rate of improvement in learning accuracy diminishes with respect to the number of clients subsampled, as soon as the sample size exceeds a threshold value. Interestingly, we also observe that under a careful choice of step-sizes, the learning error due to Byzantine clients decreases with the number of local steps. We validate our theory by experiments on the FEMNIST and CIFAR-$10$ image classification tasks",
    "checked": true,
    "id": "926bb777511301840ae598a1325974fbc8da70a8",
    "semantic_title": "byzantine-robust federated learning: impact of client subsampling and local updates",
    "citation_count": 0,
    "authors": [
      "Youssef Allouah",
      "Sadegh Farhadkhani",
      "Rachid Guerraoui",
      "Nirupam Gupta",
      "Rafael Pinot",
      "Geovani Rizk",
      "Sasha Voitovych"
    ]
  },
  "https://proceedings.mlr.press/v235/allouah24b.html": {
    "title": "The Privacy Power of Correlated Noise in Decentralized Learning",
    "volume": "main",
    "abstract": "Decentralized learning is appealing as it enables the scalable usage of large amounts of distributed data and resources without resorting to any central entity, while promoting privacy since every user minimizes the direct exposure of their data. Yet, without additional precautions, curious users can still leverage models obtained from their peers to violate privacy. In this paper, we propose Decor, a variant of decentralized SGD with differential privacy (DP) guarantees. Essentially, in Decor, users securely exchange randomness seeds in one communication round to generate pairwise-canceling correlated Gaussian noises, which are injected to protect local models at every communication round. We theoretically and empirically show that, for arbitrary connected graphs, Decor matches the central DP optimal privacy-utility trade-off. We do so under SecLDP, our new relaxation of local DP, which protects all user communications against an external eavesdropper and curious users, assuming that every pair of connected users shares a secret, i.e., an information hidden to all others. The main theoretical challenge is to control the accumulation of non-canceling correlated noise due to network sparsity. We also propose a companion SecLDP privacy accountant for public use",
    "checked": true,
    "id": "93d0f10655b716128ecbf90c742910c88f89ffaf",
    "semantic_title": "the privacy power of correlated noise in decentralized learning",
    "citation_count": 3,
    "authors": [
      "Youssef Allouah",
      "Anastasia Koloskova",
      "Aymane El Firdoussi",
      "Martin Jaggi",
      "Rachid Guerraoui"
    ]
  },
  "https://proceedings.mlr.press/v235/alonso-campana24a.html": {
    "title": "Predicting Dose-Response Curves with Deep Neural Networks",
    "volume": "main",
    "abstract": "Dose-response curves characterize the relationship between the concentration of drugs and their inhibitory effect on the growth of specific types of cells. The predominant Hill-equation model of an ideal enzymatic inhibition unduly simplifies the biochemical reality of many drugs; and for these drugs the widely-used drug performance indicator of the half-inhibitory concentration $IC_{50}$ can lead to poor therapeutic recommendations and poor selections of promising drug candidates. We develop a neural model that uses an embedding of the interaction between drug molecules and the tissue transcriptome to estimate the entire dose-response curve rather than a scalar aggregate. We find that, compared to the prior state of the art, this model excels at interpolating and extrapolating the inhibitory effect of untried concentrations. Unlike prevalent parametric models, it it able to accurately predict dose-response curves of drugs on previously unseen tumor tissues as well as of previously untested drug molecules on established tumor cell lines",
    "checked": true,
    "id": "37bcced60df59353883957ea824be02d17d9cbd3",
    "semantic_title": "predicting dose-response curves with deep neural networks",
    "citation_count": 0,
    "authors": [
      "Pedro Alonso Campana",
      "Paul Prasse",
      "Tobias Scheffer"
    ]
  },
  "https://proceedings.mlr.press/v235/altamirano24a.html": {
    "title": "Robust and Conjugate Gaussian Process Regression",
    "volume": "main",
    "abstract": "To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this paper, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes",
    "checked": true,
    "id": "173c4373744d772be88f99dc500fb13ef9d42229",
    "semantic_title": "robust and conjugate gaussian process regression",
    "citation_count": 7,
    "authors": [
      "Matias Altamirano",
      "Francois-Xavier Briol",
      "Jeremias Knoblauch"
    ]
  },
  "https://proceedings.mlr.press/v235/altieri24a.html": {
    "title": "Beyond the Norms: Detecting Prediction Errors in Regression Models",
    "volume": "main",
    "abstract": "This paper tackles the challenge of detecting unreliable behavior in regression algorithms, which may arise from intrinsic variability (e.g., aleatoric uncertainty) or modeling errors (e.g., model uncertainty). First, we formally introduce the notion of unreliability in regression, i.e., when the output of the regressor exceeds a specified discrepancy (or error). Then, using powerful tools for probabilistic modeling, we estimate the discrepancy density, and we measure its statistical diversity using our proposed metric for statistical dissimilarity. In turn, this allows us to derive a data-driven score that expresses the uncertainty of the regression outcome. We show empirical improvements in error detection for multiple regression tasks, consistently outperforming popular baseline approaches, and contributing to the broader field of uncertainty quantification and safe machine learning systems",
    "checked": true,
    "id": "cfd96aefb10f644a7aa435afb416846cff58ceb6",
    "semantic_title": "beyond the norms: detecting prediction errors in regression models",
    "citation_count": 0,
    "authors": [
      "Andres Altieri",
      "Marco Romanelli",
      "Georg Pichler",
      "Florence Alberge",
      "Pablo Piantanida"
    ]
  },
  "https://proceedings.mlr.press/v235/altmeyer24a.html": {
    "title": "Position: Stop Making Unscientific AGI Performance Claims",
    "volume": "main",
    "abstract": "Developments in the field of Artificial Intelligence (AI), and particularly large language models (LLMs), have created a 'perfect storm' for observing 'sparks' of Artificial General Intelligence (AGI) that are spurious. Like simpler models, LLMs distill meaningful representations in their latent embeddings that have been shown to correlate with external variables. Nonetheless, the correlation of such representations has often been linked to human-like intelligence in the latter but not the former. We probe models of varying complexity including random projections, matrix decompositions, deep autoencoders and transformers: all of them successfully distill information that can be used to predict latent or external variables and yet none of them have previously been linked to AGI. We argue and empirically demonstrate that the finding of meaningful patterns in latent spaces of models cannot be seen as evidence in favor of AGI. Additionally, we review literature from the social sciences that shows that humans are prone to seek such patterns and anthropomorphize. We conclude that both the methodological setup and common public image of AI are ideal for the misinterpretation that correlations between model representations and some variables of interest are 'caused' by the model's understanding of underlying 'ground truth' relationships. We, therefore, call for the academic community to exercise extra caution, and to be keenly aware of principles of academic integrity, in interpreting and communicating about AI research outcomes",
    "checked": true,
    "id": "ffffc62cf48c40b24db0c9fc49c67ed65589bb83",
    "semantic_title": "position: stop making unscientific agi performance claims",
    "citation_count": 1,
    "authors": [
      "Patrick Altmeyer",
      "Andrew M. Demetriou",
      "Antony Bartlett",
      "Cynthia C. S. Liem"
    ]
  },
  "https://proceedings.mlr.press/v235/alvarado24a.html": {
    "title": "Hyperbolic Optimizer as a Dynamical System",
    "volume": "main",
    "abstract": "During the last few years, the field of dynamical systems has been developing innovative tools to study the asymptotic behavior of different optimizers in the context of neural networks. In this work, we redefine an extensively studied optimizer, employing classical techniques from hyperbolic geometry. This new definition is linked to a non-linear differential equation as a continuous limit. Additionally, by utilizing Lyapunov stability concepts, we analyze the asymptotic behavior of its critical points",
    "checked": true,
    "id": "3cdcec020c2e42a09f853433b3bc645091099297",
    "semantic_title": "hyperbolic optimizer as a dynamical system",
    "citation_count": 0,
    "authors": [
      "Nico Alvarado",
      "Hans Lobel"
    ]
  },
  "https://proceedings.mlr.press/v235/ambrogioni24a.html": {
    "title": "Stationarity without mean reversion in improper Gaussian processes",
    "volume": "main",
    "abstract": "The behavior of a GP regression depends on the choice of covariance function. Stationary covariance functions are preferred in machine learning applications. However, (non-periodic) stationary covariance functions are always mean reverting and can therefore exhibit pathological behavior when applied to data that does not relax to a fixed global mean value. In this paper we show that it is possible to use improper GP priors with infinite variance to define processes that are stationary but not mean reverting. To this aim, we use of non-positive kernels that can only be defined in this limit regime. The resulting posterior distributions can be computed analytically and it involves a simple correction of the usual formulas. The main contribution of the paper is the introduction of a large family of smooth non-reverting covariance functions that closely resemble the kernels commonly used in the GP literature (e.g. squared exponential and Matérn class). By analyzing both synthetic and real data, we demonstrate that these non-positive kernels solve some known pathologies of mean reverting GP regression while retaining most of the favorable properties of ordinary smooth stationary kernels",
    "checked": true,
    "id": "fc82c667ea6cc9ace4621df20e13e2b78cc6940d",
    "semantic_title": "stationarity without mean reversion in improper gaussian processes",
    "citation_count": 0,
    "authors": [
      "Luca Ambrogioni"
    ]
  },
  "https://proceedings.mlr.press/v235/ameen24a.html": {
    "title": "Robust Graph Matching when Nodes are Corrupt",
    "volume": "main",
    "abstract": "Two models are introduced to study the problem of matching two correlated graphs when some of the nodes are corrupt. In the weak model, a random subset of nodes in one or both graphs can interact randomly with their network. For this model, it is shown that no estimator can correctly recover a positive fraction of the corrupt nodes. Necessary conditions for any estimator to correctly identify and match all the uncorrupt nodes are derived, and it is shown that these conditions are also sufficient for the k-core estimator. In the strong model, an adversarially selected subset of nodes in one or both graphs can interact arbitrarily with their network. For this model, detection of corrupt nodes is impossible. Even so, we show that if only one of the networks is compromised, then under appropriate conditions, the maximum overlap estimator can correctly match a positive fraction of nodes albeit without explicitly identifying them",
    "checked": true,
    "id": "4217cc7896a6b2a8f1967bda311ae2301356130b",
    "semantic_title": "robust graph matching when nodes are corrupt",
    "citation_count": 2,
    "authors": [
      "Taha Ameen",
      "Bruce Hajek"
    ]
  },
  "https://proceedings.mlr.press/v235/ameranis24a.html": {
    "title": "Fast Algorithms for Hypergraph PageRank with Applications to Semi-Supervised Learning",
    "volume": "main",
    "abstract": "A fundamental approach to semi-supervised learning is to leverage the structure of the sample space to diffuse label information from annotated examples to unlabeled points. Traditional methods model the input data points as a graph and rely on fast algorithms for solving Laplacian systems of equations, such as those defining PageRank. However, previous work has demonstrated that graph-based models fail to capture higher-order relations, such as group membership, which are better modeled by hypergraphs. Unfortunately, the scalable application of hypergraph models has been hampered by the non-linearity of the hypergraph Laplacian. In this paper, we present highly scalable algorithms for hypergraph primitives, such as hypergraph PageRank vectors and hypergraph Laplacian systems, over general families of hypergraphs. In addition to giving strong theoretical guarantees, we empirically showcase the speed of our algorithms on benchmark instances of semi-supervised learning on categorical data. We exploit their generality to improve semi-supervised manifold clustering via hypergraph models. By providing significant speed-ups on fundamental hypergraph tasks, our algorithms enable the deployment of hypergraph models on a massive scale",
    "checked": true,
    "id": "bd6bd6cfe3e48065dedf6542e5ccde9298ab3d22",
    "semantic_title": "fast algorithms for hypergraph pagerank with applications to semi-supervised learning",
    "citation_count": 0,
    "authors": [
      "Konstantinos Ameranis",
      "Adela Frances Depavia",
      "Lorenzo Orecchia",
      "Erasmo Tani"
    ]
  },
  "https://proceedings.mlr.press/v235/amin24a.html": {
    "title": "Scalable and Flexible Causal Discovery with an Efficient Test for Adjacency",
    "volume": "main",
    "abstract": "To make accurate predictions, understand mechanisms, and design interventions in systems of many variables, we wish to learn causal graphs from large scale data. Unfortunately the space of all possible causal graphs is enormous so scalably and accurately searching for the best fit to the data is a challenge. In principle we could substantially decrease the search space, or learn the graph entirely, by testing the conditional independence of variables. However, deciding if two variables are adjacent in a causal graph may require an exponential number of tests. Here we build a scalable and flexible method to evaluate if two variables are adjacent in a causal graph, the Differentiable Adjacency Test (DAT). DAT replaces an exponential number of tests with a provably equivalent relaxed problem. It then solves this problem by training two neural networks. We build a graph learning method based on DAT, DAT-Graph, that can also learn from data with interventions. DAT-Graph can learn graphs of 1000 variables with state of the art accuracy. Using the graph learned by DAT-Graph, we also build models that make much more accurate predictions of the effects of interventions on large scale RNA sequencing data",
    "checked": true,
    "id": "aba9e483fc80b2246e0043768867a60815d1c888",
    "semantic_title": "scalable and flexible causal discovery with an efficient test for adjacency",
    "citation_count": 0,
    "authors": [
      "Alan Nawzad Amin",
      "Andrew Gordon Wilson"
    ]
  },
  "https://proceedings.mlr.press/v235/aminian24a.html": {
    "title": "Generalization Error of Graph Neural Networks in the Mean-field Regime",
    "volume": "main",
    "abstract": "This work provides a theoretical framework for assessing the generalization error of graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our understanding of their performance",
    "checked": true,
    "id": "658dfd1f241166ccb49c3e19af454012d61b7257",
    "semantic_title": "generalization error of graph neural networks in the mean-field regime",
    "citation_count": 2,
    "authors": [
      "Gholamali Aminian",
      "Yixuan He",
      "Gesine Reinert",
      "Lukasz Szpruch",
      "Samuel N. Cohen"
    ]
  },
  "https://proceedings.mlr.press/v235/amortila24a.html": {
    "title": "Scalable Online Exploration via Coverability",
    "volume": "main",
    "abstract": "Exploration is a major challenge in reinforcement learning, especially for high-dimensional domains that require function approximation. We propose exploration objectives—policy optimization objectives that enable downstream maximization of any reward function—as a conceptual framework to systematize the study of exploration. We introduce a new objective, L1-Coverage, which generalizes previous exploration schemes and supports three fundamental desiderata: 1. Intrinsic complexity control. L1-Coverage is associated with a structural parameter, L1-Coverability, which reflects the intrinsic statistical difficulty of the underlying MDP, subsuming Block and Low-Rank MDPs. 2. Efficient planning. For a known MDP, L1-Coverage efficiently reduces to standard policy optimization, allowing flexible integration with off-the-shelf methods such as policy gradient and Q-learning approaches. 3. Efficient exploration. L1-Coverage enables the first computationally efficient model-based and model-free algorithms for online (reward-free or reward-driven) reinforcement learning in MDPs with low coverability. Empirically, we find that L1-Coverage effectively drives off-the-shelf policy optimization algorithms to explore the state space",
    "checked": true,
    "id": "e7683184a3636add2e736bd7aa4b56d084af8451",
    "semantic_title": "scalable online exploration via coverability",
    "citation_count": 2,
    "authors": [
      "Philip Amortila",
      "Dylan J Foster",
      "Akshay Krishnamurthy"
    ]
  },
  "https://proceedings.mlr.press/v235/an24a.html": {
    "title": "WAVES: Benchmarking the Robustness of Image Watermarks",
    "volume": "main",
    "abstract": "In the burgeoning age of generative AI, watermarks act as identifiers of provenance and artificial content. We present WAVES (Watermark Analysis via Enhanced Stress-testing), a benchmark for assessing image watermark robustness, overcoming the limitations of current evaluation methods. WAVES integrates detection and identification tasks and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced, novel variations of diffusive, and adversarial attacks. Our evaluation examines two pivotal dimensions: the degree of image quality degradation and the efficacy of watermark detection after attacks. Our novel, comprehensive evaluation reveals previously undetected vulnerabilities of several modern watermarking algorithms. We envision WAVES as a toolkit for the future development of robust watermarks",
    "checked": false,
    "id": "51ee6e799c1faae57b1736941d9d289fcce72b61",
    "semantic_title": "benchmarking the robustness of image watermarks",
    "citation_count": 22,
    "authors": [
      "Bang An",
      "Mucong Ding",
      "Tahseen Rabbani",
      "Aakriti Agrawal",
      "Yuancheng Xu",
      "Chenghao Deng",
      "Sicheng Zhu",
      "Abdirisak Mohamed",
      "Yuxin Wen",
      "Tom Goldstein",
      "Furong Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/an24b.html": {
    "title": "Training-Free Long-Context Scaling of Large Language Models",
    "volume": "main",
    "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose a training-free approach named Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of up to 100k tokens. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of models built through continual training. All code and data used in this work are released at https://github.com/HKUNLP/ChunkLlama",
    "checked": true,
    "id": "cf7ab5df804575bad88a9fcf0fbf7707bf500944",
    "semantic_title": "training-free long-context scaling of large language models",
    "citation_count": 20,
    "authors": [
      "Chenxin An",
      "Fei Huang",
      "Jun Zhang",
      "Shansan Gong",
      "Xipeng Qiu",
      "Chang Zhou",
      "Lingpeng Kong"
    ]
  },
  "https://proceedings.mlr.press/v235/anagnostidis24a.html": {
    "title": "Navigating Scaling Laws: Compute Optimality in Adaptive Model Training",
    "volume": "main",
    "abstract": "In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a 'compute-optimal' model, i.e. a model that allocates a given level of compute during training optimally to maximize performance. In this work, we extend the concept of optimality by allowing for an 'adaptive' model, i.e. a model that can change its shape during training. By doing so, we can design adaptive models that optimally traverse between the underlying scaling laws and outpace their ‘static' counterparts, leading to a significant reduction in the required compute to reach a given target performance. We show that our approach generalizes across modalities and different shape parameters",
    "checked": true,
    "id": "c0075372464eb68011f68d0293849bcab9159355",
    "semantic_title": "navigating scaling laws: compute optimality in adaptive model training",
    "citation_count": 1,
    "authors": [
      "Sotiris Anagnostidis",
      "Gregor Bachmann",
      "Imanol Schlag",
      "Thomas Hofmann"
    ]
  },
  "https://proceedings.mlr.press/v235/anani24a.html": {
    "title": "Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing",
    "volume": "main",
    "abstract": "Certification for machine learning is proving that no adversarial sample can evade a model within a range under certain conditions, a necessity for safety-critical domains. Common certification methods for segmentation use a flat set of fine-grained classes, leading to high abstain rates due to model uncertainty across many classes. We propose a novel, more practical setting, which certifies pixels within a multi-level hierarchy, and adaptively relaxes the certification to a coarser level for unstable components classic methods would abstain from, effectively lowering the abstain rate whilst providing more certified semantically meaningful information. We mathematically formulate the problem setup, introduce an adaptive hierarchical certification algorithm and prove the correctness of its guarantees. Since certified accuracy does not take the loss of information into account for coarser classes, we introduce the Certified Information Gain ($\\mathrm{CIG}$) metric, which is proportional to the class granularity level. Our extensive experiments on the datasets Cityscapes, PASCAL-Context, ACDC and COCO-Stuff demonstrate that our adaptive algorithm achieves a higher $\\mathrm{CIG}$ and lower abstain rate compared to the current state-of-the-art certification method. Our code can be found here: https://github.com/AlaaAnani/adaptive-certify",
    "checked": true,
    "id": "940515e9f7d7d1953b853e9b894a53f9051c24ba",
    "semantic_title": "adaptive hierarchical certification for segmentation using randomized smoothing",
    "citation_count": 0,
    "authors": [
      "Alaa Anani",
      "Tobias Lorenz",
      "Bernt Schiele",
      "Mario Fritz"
    ]
  },
  "https://proceedings.mlr.press/v235/anders24a.html": {
    "title": "Adaptive Observation Cost Control for Variational Quantum Eigensolvers",
    "volume": "main",
    "abstract": "The objective to be minimized in the variational quantum eigensolver (VQE) has a restricted form, which allows a specialized sequential minimal optimization (SMO) that requires only a few observations in each iteration. However, the SMO iteration is still costly due to the observation noise—one observation at a point typically requires averaging over hundreds to thousands of repeated quantum measurement shots for achieving a reasonable noise level. In this paper, we propose an adaptive cost control method, named subspace in confident region (SubsCoRe), for SMO. SubsCoRe uses the Gaussian process (GP) surrogate, and requires it to have low uncertainty over the subspace being updated, so that optimization in each iteration is performed with guaranteed accuracy. Adaptive cost control is performed by setting the required accuracy according to the progress of the optimization, and identifying the minimum number of measurement shots, as well as their distribution, satisfying the SubsCoRe requirement",
    "checked": true,
    "id": "296b2d698947ded0ad0c55ed2f759149b4f22464",
    "semantic_title": "adaptive observation cost control for variational quantum eigensolvers",
    "citation_count": 0,
    "authors": [
      "Christopher J. Anders",
      "Kim Andrea Nicoli",
      "Bingting Wu",
      "Naima Elosegui",
      "Samuele Pedrielli",
      "Lena Funcke",
      "Karl Jansen",
      "Stefan Kühn",
      "Shinichi Nakajima"
    ]
  },
  "https://proceedings.mlr.press/v235/angell24a.html": {
    "title": "Fast, Scalable, Warm-Start Semidefinite Programming with Spectral Bundling and Sketching",
    "volume": "main",
    "abstract": "While semidefinite programming (SDP) has traditionally been limited to moderate-sized problems, recent algorithms augmented with matrix sketching techniques have enabled solving larger SDPs. However, these methods achieve scalability at the cost of an increase in the number of necessary iterations, resulting in slower convergence as the problem size grows. Furthermore, they require iteration-dependent parameter schedules that prohibit effective utilization of warm-start initializations important in practical applications with incrementally-arriving data or mixed-integer programming. We present Unified Spectral Bundling with Sketching (USBS), a provably correct, fast and scalable algorithm for solving massive SDPs that can leverage a warm-start initialization to further accelerate convergence. Our proposed algorithm is a spectral bundle method for solving general SDPs containing both equality and inequality constraints. Moveover, when augmented with an optional matrix sketching technique, our algorithm achieves the dramatically improved scalability of previous work while sustaining convergence speed. We empirically demonstrate the effectiveness of our method across multiple applications, with and without warm-starting. For example, USBS provides a 500x speed-up over the state-of-the-art scalable SDP solver on an instance with over 2 billion decision variables. We make our implementation in pure JAX publicly available",
    "checked": true,
    "id": "2c4bb353d06d3b8870961b2462821c00d59392d0",
    "semantic_title": "fast, scalable, warm-start semidefinite programming with spectral bundling and sketching",
    "citation_count": 1,
    "authors": [
      "Rico Angell",
      "Andrew Mccallum"
    ]
  },
  "https://proceedings.mlr.press/v235/angelopoulos24a.html": {
    "title": "Online conformal prediction with decaying step sizes",
    "volume": "main",
    "abstract": "We introduce a method for online conformal prediction with decaying step sizes. Like previous methods, ours possesses a retrospective guarantee of coverage for arbitrary sequences. However, unlike previous methods, we can simultaneously estimate a population quantile when it exists. Our theory and experiments indicate substantially improved practical properties: in particular, when the distribution is stable, the coverage is close to the desired level for every time point, not just on average over the observed sequence",
    "checked": true,
    "id": "25b73b441a04b60d7339d7d26e857bfff933051d",
    "semantic_title": "online conformal prediction with decaying step sizes",
    "citation_count": 11,
    "authors": [
      "Anastasios Nikolas Angelopoulos",
      "Rina Barber",
      "Stephen Bates"
    ]
  },
  "https://proceedings.mlr.press/v235/apostolopoulou24a.html": {
    "title": "A Rate-Distortion View of Uncertainty Quantification",
    "volume": "main",
    "abstract": "In supervised learning, understanding an input's proximity to the training data can help a model decide whether it has sufficient evidence for reaching a reliable prediction. While powerful probabilistic models such as Gaussian Processes naturally have this property, deep neural networks often lack it. In this paper, we introduce Distance Aware Bottleneck (DAB), i.e., a new method for enriching deep neural networks with this property. Building on prior information bottleneck approaches, our method learns a codebook that stores a compressed representation of all inputs seen during training. The distance of a new example from this codebook can serve as an uncertainty estimate for the example. The resulting model is simple to train and provides deterministic uncertainty estimates by a single forward pass. Finally, our method achieves better out-of-distribution (OOD) detection and misclassification prediction than prior methods, including expensive ensemble methods, deep kernel Gaussian Processes, and approaches based on the standard information bottleneck",
    "checked": true,
    "id": "8af0861cb38a5a335d69eebefc8d1917222be960",
    "semantic_title": "a rate-distortion view of uncertainty quantification",
    "citation_count": 0,
    "authors": [
      "Ifigeneia Apostolopoulou",
      "Benjamin Eysenbach",
      "Frank Nielsen",
      "Artur Dubrawski"
    ]
  },
  "https://proceedings.mlr.press/v235/archer24a.html": {
    "title": "Practical Performance Guarantees for Pipelined DNN Inference",
    "volume": "main",
    "abstract": "We optimize pipeline parallelism for deep neural network (DNN) inference by partitioning model graphs into $k$ stages and minimizing the running time of the bottleneck stage, including communication. We give practical and effective algorithms for this NP-hard problem, but our emphasis is on tackling the practitioner's dilemma of deciding when a solution is good enough. To this end, we design novel mixed integer programming (MIP) relaxations for proving lower bounds. Applying these methods to a diverse testbed of 369 production models, for $k \\in \\\\{2, 4, 8, 16, 32, 64\\\\}$, we empirically show that these lower bounds are strong enough to be useful in practice. Our lower bounds are substantially stronger than standard combinatorial bounds. For example, evaluated via geometric means across a production testbed with $k = 16$ pipeline stages, our MIP formulations raise the lower bound from 0.4598 to 0.9452, expressed as a fraction of the best partition found. In other words, our improved lower bounds close the optimality gap by a factor of 9.855x",
    "checked": true,
    "id": "288f945833238aca0e63e247eb06eb578da81013",
    "semantic_title": "practical performance guarantees for pipelined dnn inference",
    "citation_count": 0,
    "authors": [
      "Aaron Archer",
      "Matthew Fahrbach",
      "Kuikui Liu",
      "Prakash Prabhu"
    ]
  },
  "https://proceedings.mlr.press/v235/arefin24a.html": {
    "title": "Unsupervised Concept Discovery Mitigates Spurious Correlations",
    "volume": "main",
    "abstract": "Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring subgroups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the benchmark datasets for sub-population shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation. Code is available at https://github.com/rarefin/CoBalT",
    "checked": true,
    "id": "2a870dec6c17331b2c6eadd6a4687b035ecdf796",
    "semantic_title": "unsupervised concept discovery mitigates spurious correlations",
    "citation_count": 3,
    "authors": [
      "Md Rifat Arefin",
      "Yan Zhang",
      "Aristide Baratin",
      "Francesco Locatello",
      "Irina Rish",
      "Dianbo Liu",
      "Kenji Kawaguchi"
    ]
  },
  "https://proceedings.mlr.press/v235/arisaka24a.html": {
    "title": "Accelerating Legacy Numerical Solvers by Non-intrusive Gradient-based Meta-solving",
    "volume": "main",
    "abstract": "Scientific computing is an essential tool for scientific discovery and engineering design, and its computational cost is always a main concern in practice. To accelerate scientific computing, it is a promising approach to use machine learning (especially meta-learning) techniques for selecting hyperparameters of traditional numerical methods. There have been numerous proposals to this direction, but many of them require automatic-differentiable numerical methods. However, in reality, many practical applications still depend on well-established but non-automatic-differentiable legacy codes, which prevents practitioners from applying the state-of-the-art research to their own problems. To resolve this problem, we propose a non-intrusive methodology with a novel gradient estimation technique to combine machine learning and legacy numerical codes without any modification. We theoretically and numerically show the advantage of the proposed method over other baselines and present applications of accelerating established non-automatic-differentiable numerical solvers implemented in PETSc, a widely used open-source numerical software library",
    "checked": true,
    "id": "5caa0a22e7f5ef4b3d8788e92a18746ef2f38449",
    "semantic_title": "accelerating legacy numerical solvers by non-intrusive gradient-based meta-solving",
    "citation_count": 0,
    "authors": [
      "Sohei Arisaka",
      "Qianxiao Li"
    ]
  },
  "https://proceedings.mlr.press/v235/armengol-urpi-24a.html": {
    "title": "Causal Action Influence Aware Counterfactual Data Augmentation",
    "volume": "main",
    "abstract": "Offline data are both valuable and practical resources for teaching robots complex behaviors. Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize beyond the training distribution. However, the complexity of real-world scenarios typically requires huge amounts of data to prevent neural network policies from picking up on spurious correlations and learning non-causal relationships. We propose CAIAC, a data augmentation method that can create feasible synthetic transitions from a fixed dataset without having access to online environment interactions. By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping $\\textit{action}$-unaffected parts of the state-space between independent trajectories in the dataset. We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift",
    "checked": true,
    "id": "ea9542d9cec6dca54664b4a58787c61ed35f2362",
    "semantic_title": "causal action influence aware counterfactual data augmentation",
    "citation_count": 1,
    "authors": [
      "Núria Armengol Urpı́",
      "Marco Bagatella",
      "Marin Vlastelica",
      "Georg Martius"
    ]
  },
  "https://proceedings.mlr.press/v235/arnaboldi24a.html": {
    "title": "Online Learning and Information Exponents: The Importance of Batch size & Time/Complexity Tradeoffs",
    "volume": "main",
    "abstract": "We study the impact of the batch size $n_b$ on the iteration time $T$ of training two-layer neural networks with one-pass stochastic gradient descent (SGD) on multi-index target functions of isotropic covariates. We characterize the optimal batch size minimizing the iteration time as a function of the hardness of the target, as characterized by the information exponents. We show that performing gradient updates with large batches $n_b \\lesssim d^{\\frac{\\ell}{2}}$ minimizes the training time without changing the total sample complexity, where $\\ell$ is the information exponent of the target to be learned and $d$ is the input dimension. However, larger batch sizes than $n_b \\gg d^{\\frac{\\ell}{2}}$ are detrimental for improving the time complexity of SGD. We provably overcome this fundamental limitation via a different training protocol, Correlation loss SGD, which suppresses the auto-correlation terms in the loss function. We show that one can track the training progress by a system of low-dimensional ordinary differential equations (ODEs). Finally, we validate our theoretical results with numerical experiments",
    "checked": true,
    "id": "f7a16717a4273acf7d9d781ec7b1b1a05d24688c",
    "semantic_title": "online learning and information exponents: the importance of batch size & time/complexity tradeoffs",
    "citation_count": 1,
    "authors": [
      "Luca Arnaboldi",
      "Yatin Dandi",
      "Florent Krzakala",
      "Bruno Loureiro",
      "Luca Pesce",
      "Ludovic Stephan"
    ]
  },
  "https://proceedings.mlr.press/v235/arora24a.html": {
    "title": "Simple linear attention language models balance the recall-throughput tradeoff",
    "volume": "main",
    "abstract": "Recent work has shown that attention-based language models excel at \"recall\", the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's recurrent state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the Pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to $1.3$b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 10.36 accuracy points. We further develop IO-aware algorithms that enable BASED to provide 24× higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Overall, BASED expands the Pareto frontier of the throughput-recall tradeoff space beyond prior architectures",
    "checked": true,
    "id": "cde66097f4123a62bf3e28d48c764648e8c69f72",
    "semantic_title": "simple linear attention language models balance the recall-throughput tradeoff",
    "citation_count": 35,
    "authors": [
      "Simran Arora",
      "Sabri Eyuboglu",
      "Michael Zhang",
      "Aman Timalsina",
      "Silas Alberti",
      "James Zou",
      "Atri Rudra",
      "Christopher Re"
    ]
  },
  "https://proceedings.mlr.press/v235/arpino24a.html": {
    "title": "Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing",
    "volume": "main",
    "abstract": "We consider the problem of localizing change points in high-dimensional linear regression. We propose an Approximate Message Passing (AMP) algorithm for estimating both the signals and the change point locations. Assuming Gaussian covariates, we give an exact asymptotic characterization of its estimation performance in the limit where the number of samples grows proportionally to the signal dimension. Our algorithm can be tailored to exploit any prior information on the signal, noise, and change points. It also enables uncertainty quantification in the form of an efficiently computable approximate posterior distribution, whose asymptotic form we characterize exactly. We validate our theory via numerical experiments, and demonstrate the favorable performance of our estimators on both synthetic data and images",
    "checked": true,
    "id": "5e5506a9d443fe29c51da699e7511908b8e10e49",
    "semantic_title": "inferring change points in high-dimensional linear regression via approximate message passing",
    "citation_count": 3,
    "authors": [
      "Gabriel Arpino",
      "Xiaoqi Liu",
      "Ramji Venkataramanan"
    ]
  },
  "https://proceedings.mlr.press/v235/arruda24a.html": {
    "title": "An amortized approach to non-linear mixed-effects modeling based on neural posterior estimation",
    "volume": "main",
    "abstract": "Non-linear mixed-effects models are a powerful tool for studying heterogeneous populations in various fields, including biology, medicine, economics, and engineering. Here, the aim is to find a distribution over the parameters that describe the whole population using a model that can generate simulations for an individual of that population. However, fitting these distributions to data is computationally challenging if the description of individuals is complex and the population is large. To address this issue, we propose a novel machine learning-based approach: We exploit neural density estimation based on conditional normalizing flows to approximate individual-specific posterior distributions in an amortized fashion, thereby allowing for efficient inference of population parameters. Applying this approach to problems from cell biology and pharmacology, we demonstrate its unseen flexibility and scalability to large data sets compared to established methods",
    "checked": true,
    "id": "c22240f10717a49c763ca1d8a122429d62f2e18c",
    "semantic_title": "an amortized approach to non-linear mixed-effects modeling based on neural posterior estimation",
    "citation_count": 2,
    "authors": [
      "Jonas Arruda",
      "Yannik Schälte",
      "Clemens Peiter",
      "Olga Teplytska",
      "Ulrich Jaehde",
      "Jan Hasenauer"
    ]
  },
  "https://proceedings.mlr.press/v235/asadi24a.html": {
    "title": "Learning the Target Network in Function Space",
    "volume": "main",
    "abstract": "We focus on the task of learning the value function in the reinforcement learning (RL) setting. This task is often solved by updating a pair of online and target networks while ensuring that the parameters of these two networks are equivalent. We propose Lookahead-Replicate (LR), a new value-function approximation algorithm that is agnostic to this parameter-space equivalence. Instead, the LR algorithm is designed to maintain an equivalence between the two networks in the function space. This value-based equivalence is obtained by employing a new target-network update. We show that LR leads to a convergent behavior in learning the value function. We also present empirical results demonstrating that LR-based target-network updates significantly improve deep RL on the Atari benchmark",
    "checked": true,
    "id": "cf0df4e6ac66ea3e76562c13375a92b1139e68b1",
    "semantic_title": "learning the target network in function space",
    "citation_count": 0,
    "authors": [
      "Kavosh Asadi",
      "Yao Liu",
      "Shoham Sabach",
      "Ming Yin",
      "Rasool Fakoor"
    ]
  },
  "https://proceedings.mlr.press/v235/ashman24a.html": {
    "title": "Translation Equivariant Transformer Neural Processes",
    "volume": "main",
    "abstract": "The effectiveness of neural processes (NPs) in modelling posterior prediction maps—the mapping from data to posterior predictive distributions—has significantly improved since their inception. This improvement can be attributed to two principal factors: (1) advancements in the architecture of permutation invariant set functions, which are intrinsic to all NPs; and (2) leveraging symmetries present in the true posterior predictive map, which are problem dependent. Transformers are a notable development in permutation invariant set functions, and their utility within NPs has been demonstrated through the family of models we refer to as TNPs. Despite significant interest in TNPs, little attention has been given to incorporating symmetries. Notably, the posterior prediction maps for data that are stationary—a common assumption in spatio-temporal modelling—exhibit translation equivariance. In this paper, we introduce of a new family of translation equivariant TNPs that incorporate translation equivariance. Through an extensive range of experiments on synthetic and real-world spatio-temporal data, we demonstrate the effectiveness of TE-TNPs relative to their non-translation-equivariant counterparts and other NP baselines",
    "checked": true,
    "id": "f3c1a350aad63f37c1f0f4ee88a96cfd774f1a43",
    "semantic_title": "translation equivariant transformer neural processes",
    "citation_count": 2,
    "authors": [
      "Matthew Ashman",
      "Cristiana Diaconu",
      "Junhyuck Kim",
      "Lakee Sivaraya",
      "Stratis Markou",
      "James Requeima",
      "Wessel P Bruinsma",
      "Richard E. Turner"
    ]
  },
  "https://proceedings.mlr.press/v235/asi24a.html": {
    "title": "Private Vector Mean Estimation in the Shuffle Model: Optimal Rates Require Many Messages",
    "volume": "main",
    "abstract": "We study the problem of private vector mean estimation in the shuffle model of privacy where $n$ users each have a unit vector $v^{(i)} \\in \\mathbb{R}^d$. We propose a new multi-message protocol that achieves the optimal error using $O(\\min(n\\varepsilon^2,d))$ messages per user. Moreover, we show that any (unbiased) protocol that achieves optimal error must require each user to send $\\Omega(\\min(n\\varepsilon^2,d)/\\log(n))$ messages, demonstrating the optimality of our message complexity up to logarithmic factors. Additionally, we study the single-message setting and design a protocol that achieves mean squared error $O(dn^{d/(d+2)}\\varepsilon^{-4/(d+2)})$. Moreover, we show that any single-message protocol must incur mean squared error $\\Omega(dn^{d/(d+2)})$, showing that our protocol is optimal in the standard setting where $\\varepsilon = \\Theta(1)$. Finally, we study robustness to malicious users and show that malicious users can incur large additive error with a single shuffler",
    "checked": true,
    "id": "f2dee78e687df06f92b0a3d0a7d3babe0b9f0641",
    "semantic_title": "private vector mean estimation in the shuffle model: optimal rates require many messages",
    "citation_count": 4,
    "authors": [
      "Hilal Asi",
      "Vitaly Feldman",
      "Jelani Nelson",
      "Huy Nguyen",
      "Kunal Talwar",
      "Samson Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/athiwaratkun24a.html": {
    "title": "Bifurcated Attention for Single-Context Large-Batch Sampling",
    "volume": "main",
    "abstract": "In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation without substantially increasing latency, enhancing performance when integrated with post-processing techniques such as reranking",
    "checked": true,
    "id": "796d26457fcf195ed9f18b7db035c08aa9f17a1d",
    "semantic_title": "bifurcated attention for single-context large-batch sampling",
    "citation_count": 1,
    "authors": [
      "Ben Athiwaratkun",
      "Sujan Kumar Gonugondla",
      "Sanjay Krishna Gouda",
      "Haifeng Qian",
      "Hantian Ding",
      "Qing Sun",
      "Jun Wang",
      "Jiacheng Guo",
      "Liangfu Chen",
      "Parminder Bhatia",
      "Ramesh Nallapati",
      "Sudipta Sengupta",
      "Bing Xiang"
    ]
  },
  "https://proceedings.mlr.press/v235/attali24a.html": {
    "title": "Delaunay Graph: Addressing Over-Squashing and Over-Smoothing Using Delaunay Triangulation",
    "volume": "main",
    "abstract": "GNNs rely on the exchange of messages to distribute information along the edges of the graph. This approach makes the efficiency of architectures highly dependent on the specific structure of the input graph. Certain graph topologies lead to inefficient information propagation, resulting in a phenomenon known as over-squashing. While the majority of existing methods address over-squashing by rewiring the input graph, our novel approach involves constructing a graph directly from features using Delaunay Triangulation. We posit that the topological properties of the resulting graph prove advantageous for mitigate oversmoothing and over-squashing. Our extensive experimentation demonstrates that our method consistently outperforms established graph rewiring methods",
    "checked": true,
    "id": "69f2efce2bed7b58c281a601ad290265e5f8bb81",
    "semantic_title": "delaunay graph: addressing over-squashing and over-smoothing using delaunay triangulation",
    "citation_count": 1,
    "authors": [
      "Hugo Attali",
      "Davide Buscaldi",
      "Nathalie Pernelle"
    ]
  },
  "https://proceedings.mlr.press/v235/attia24a.html": {
    "title": "How Free is Parameter-Free Stochastic Optimization?",
    "volume": "main",
    "abstract": "We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered \"partially\" parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free stochastic convex optimization infeasible, and provide a method which is (partially) parameter-free up to the limit indicated by our lower bound",
    "checked": true,
    "id": "5e59454d79797d6f426dfe07abb405006addb4c9",
    "semantic_title": "how free is parameter-free stochastic optimization?",
    "citation_count": 4,
    "authors": [
      "Amit Attia",
      "Tomer Koren"
    ]
  },
  "https://proceedings.mlr.press/v235/attias24a.html": {
    "title": "Information Complexity of Stochastic Convex Optimization: Applications to Generalization, Memorization, and Tracing",
    "volume": "main",
    "abstract": "In this work, we investigate the interplay between memorization and learning in the context of stochastic convex optimization (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz–bounded setting and under strong convexity, every learner with an excess error $\\epsilon$ has CMI bounded below by $\\Omega(1/\\epsilon^2)$ and $\\Omega(1/\\epsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems. Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems",
    "checked": true,
    "id": "d6e2803b933e32f6f0e932a5d99e7dca08ba6666",
    "semantic_title": "information complexity of stochastic convex optimization: applications to generalization, memorization, and tracing",
    "citation_count": 1,
    "authors": [
      "Idan Attias",
      "Gintare Karolina Dziugaite",
      "Mahdi Haghifam",
      "Roi Livni",
      "Daniel M. Roy"
    ]
  },
  "https://proceedings.mlr.press/v235/attias24b.html": {
    "title": "Agnostic Sample Compression Schemes for Regression",
    "volume": "main",
    "abstract": "We obtain the first positive results for bounded sample compression in the agnostic regression setting with the $\\ell_p$ loss, where $p\\in [1,\\infty]$. We construct a generic approximate sample compression scheme for real-valued function classes exhibiting exponential size in the fat-shattering dimension but independent of the sample size. Notably, for linear regression, an approximate compression of size linear in the dimension is constructed. Moreover, for $\\ell_1$ and $\\ell_\\infty$ losses, we can even exhibit an efficient exact sample compression scheme of size linear in the dimension. We further show that for every other $\\ell_p$ loss, $p\\in (1,\\infty)$, there does not exist an exact agnostic compression scheme of bounded size. This refines and generalizes a negative result of David, Moran, and Yehudayoff (2016) for the $\\ell_2$ loss. We close by posing general open questions: for agnostic regression with $\\ell_1$ loss, does every function class admit an exact compression scheme of polynomial size in the pseudo-dimension? For the $\\ell_2$ loss, does every function class admit an approximate compression scheme of polynomial size in the fat-shattering dimension? These questions generalize Warmuth's classic sample compression conjecture for realizable-case classification (Warmuth, 2003)",
    "checked": false,
    "id": "83b409c5d93069e31aa5b760cc258505ab797b2f",
    "semantic_title": "sample compression scheme reductions",
    "citation_count": 0,
    "authors": [
      "Idan Attias",
      "Steve Hanneke",
      "Aryeh Kontorovich",
      "Menachem Sadigurschi"
    ]
  },
  "https://proceedings.mlr.press/v235/axiotis24a.html": {
    "title": "Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond",
    "volume": "main",
    "abstract": "We study the data selection problem, whose aim is to select a small representative subset of data that can be used to efficiently train a machine learning model. We present a new data selection approach based on $k$-means clustering and sensitivity sampling. Assuming access to an embedding representation of the data with respect to which the model loss is Holder continuous, our approach provably allows selecting a set of \"typical\" $k + 1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an additive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means cost for the input embeddings and $\\lambda$ is the Holder constant. We furthermore demonstrate the performance and scalability of our approach on fine-tuning foundation models and show that it outperforms state-of-the-art methods. We also show how it can be applied on linear regression, leading to a new sampling strategy that surprisingly matches the performance of leverage score sampling, while being conceptually simpler and more scalable",
    "checked": true,
    "id": "7107b458202530e03deaebfb01a0df3cb005f8db",
    "semantic_title": "data-efficient learning via clustering-based sensitivity sampling: foundation models and beyond",
    "citation_count": 2,
    "authors": [
      "Kyriakos Axiotis",
      "Vincent Cohen-Addad",
      "Monika Henzinger",
      "Sammy Jerome",
      "Vahab Mirrokni",
      "David Saulpic",
      "David Woodruff",
      "Michael Wunder"
    ]
  },
  "https://proceedings.mlr.press/v235/ayme24a.html": {
    "title": "Random features models: a way to study the success of naive imputation",
    "volume": "main",
    "abstract": "Constant (naive) imputation is still widely used in practice as this is a first easy-to-use technique to deal with missing data. Yet, this simple method could be expected to induce a large bias for prediction purposes, as the imputed input may strongly differ from the true underlying data. However, recent works suggest that this bias is low in the context of high-dimensional linear predictors when data is supposed to be missing completely at random (MCAR). This paper completes the picture for linear predictors by confirming the intuition that the bias is negligible and that surprisingly naive imputation also remains relevant in very low dimension. To this aim, we consider a unique underlying random features model, which offers a rigorous framework for studying predictive performances, whilst the dimension of the observed features varies. Building on these theoretical results, we establish finite-sample bounds on stochastic gradient (SGD) predictors applied to zero-imputed data, a strategy particularly well suited for large-scale learning. If the MCAR assumption appears to be strong, we show that similar favorable behaviors occur for more complex missing data scenarios",
    "checked": true,
    "id": "fc90e6147b7184dc4ce9784dfd219fb3413e6b3a",
    "semantic_title": "random features models: a way to study the success of naive imputation",
    "citation_count": 2,
    "authors": [
      "Alexis Ayme",
      "Claire Boyer",
      "Aymeric Dieuleveut",
      "Erwan Scornet"
    ]
  },
  "https://proceedings.mlr.press/v235/ayoub24a.html": {
    "title": "Switching the Loss Reduces the Cost in Batch Reinforcement Learning",
    "volume": "main",
    "abstract": "We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving small-cost bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal",
    "checked": true,
    "id": "3a42a517858ed4ce70e60d14f8b2bfec3636b3a7",
    "semantic_title": "switching the loss reduces the cost in batch reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Alex Ayoub",
      "Kaiwen Wang",
      "Vincent Liu",
      "Samuel Robertson",
      "James Mcinerney",
      "Dawen Liang",
      "Nathan Kallus",
      "Csaba Szepesvari"
    ]
  },
  "https://proceedings.mlr.press/v235/azarmehr24a.html": {
    "title": "Bipartite Matching in Massive Graphs: A Tight Analysis of EDCS",
    "volume": "main",
    "abstract": "Maximum matching is one of the most fundamental combinatorial optimization problems with applications in various contexts such as balanced clustering, data mining, resource allocation, and online advertisement. In many of these applications, the input graph is massive. The sheer size of these inputs makes it impossible to store the whole graph in the memory of a single machine and process it there. Graph sparsification has been an extremely powerful tool to alleviate this problem. In this paper, we study a highly successful and versatile sparsifier for the matching problem: the edge-degree constrained subgraph (EDCS) introduced first by Bernstein & Stein 2015 The EDCS has a parameter $\\beta \\geq 2$ which controls the density of the sparsifier. It has been shown through various proofs in the literature that by picking a subgraph with $O(n\\beta)$ edges, the EDCS includes a matching of size at least $2/3-O(1/\\beta)$ times the maximum matching size. As such, by increasing $\\beta$ the approximation ratio of EDCS gets closer and closer to $2/3$. In this paper, we propose a new approach for analyzing the approximation ratio of EDCS. Our analysis is tight for any value of $\\beta$. Namely, we pinpoint the precise approximation ratio of EDCS for any sparsity parameter $\\beta$. Our analysis reveals that one does not necessarily need to increase $\\beta$ to improve approximation, as suggested by previous analysis. In particular, the best choice turns out to be $\\beta = 6$, which achieves an approximation ratio of $.677$! This is arguably surprising as it is even better than $2/3 \\sim .666$, the bound that was widely believed to be the limit for EDCS",
    "checked": true,
    "id": "cbd50dd9eb4708e7433baf9572a22fec9a07b27e",
    "semantic_title": "bipartite matching in massive graphs: a tight analysis of edcs",
    "citation_count": 0,
    "authors": [
      "Amir Azarmehr",
      "Soheil Behnezhad",
      "Mohammad Roghani"
    ]
  },
  "https://proceedings.mlr.press/v235/azizian24a.html": {
    "title": "What is the Long-Run Distribution of Stochastic Gradient Descent? A Large Deviations Analysis",
    "volume": "main",
    "abstract": "In this paper, we examine the long-run distribution of stochastic gradient descent (SGD) in general, non-convex problems. Specifically, we seek to understand which regions of the problem's state space are more likely to be visited by SGD, and by how much. Using an approach based on the theory of large deviations and randomly perturbed dynamical systems, we show that the long-run distribution of SGD resembles the Boltzmann-Gibbs distribution of equilibrium thermodynamics with temperature equal to the method's step-size and energy levels determined by the problem's objective and the statistics of the noise. In particular, we show that, in the long run, (a) the problem's critical region is visited exponentially more often than any non-critical region; (b) the iterates of SGD are exponentially concentrated around the problem's minimum energy state (which does not always coincide with the global minimum of the objective); (c) all other connected components of critical points are visited with frequency that is exponentially proportional to their energy level; and, finally, (d) any component of local maximizers or saddle points is \"dominated\" by a component of local minimizers which is visited exponentially more often",
    "checked": true,
    "id": "ead0cb3fdc3a0aa6bfd1684af48ba504ea73c51f",
    "semantic_title": "what is the long-run distribution of stochastic gradient descent? a large deviations analysis",
    "citation_count": 2,
    "authors": [
      "Waı̈ss Azizian",
      "Franck Iutzeler",
      "Jerome Malick",
      "Panayotis Mertikopoulos"
    ]
  },
  "https://proceedings.mlr.press/v235/babu24a.html": {
    "title": "HyperFields: Towards Zero-Shot Generation of NeRFs from Text",
    "volume": "main",
    "abstract": "We introduce HyperFields, a method for generating text-conditioned Neural Radiance Fields (NeRFs) with a single forward pass and (optionally) some fine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns a smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF distillation training, which distills scenes encoded in individual NeRFs into one dynamic hypernetwork. These techniques enable a single network to fit over a hundred unique scenes. We further demonstrate that HyperFields learns a more general map between text and NeRFs, and consequently is capable of predicting novel in-distribution and out-of-distribution scenes — either zero-shot or with a few finetuning steps. Finetuning HyperFields benefits from accelerated convergence thanks to the learned general map, and is capable of synthesizing novel scenes 5 to 10 times faster than existing neural optimization-based methods. Our ablation experiments show that both the dynamic architecture and NeRF distillation are critical to the expressivity of HyperFields",
    "checked": true,
    "id": "65cb8085e9541e2cf228687a3cf1b53d1c6e6209",
    "semantic_title": "hyperfields: towards zero-shot generation of nerfs from text",
    "citation_count": 7,
    "authors": [
      "Sudarshan Babu",
      "Richard Liu",
      "Avery Zhou",
      "Michael Maire",
      "Greg Shakhnarovich",
      "Rana Hanocka"
    ]
  },
  "https://proceedings.mlr.press/v235/baby24a.html": {
    "title": "Online Matrix Completion: A Collaborative Approach with Hott Items",
    "volume": "main",
    "abstract": "We investigate the low rank matrix completion problem in an online setting with ${M}$ users, ${N}$ items, ${T}$ rounds, and an unknown rank-$r$ reward matrix ${R}\\in \\mathbb{R}^{{M}\\times {N}}$. This problem has been well-studied in the literature and has several applications in practice. In each round, we recommend ${S}$ carefully chosen distinct items to every user and observe noisy rewards. In the regime where ${M},{N} >> {T}$, we propose two distinct computationally efficient algorithms for recommending items to users and analyze them under the benign hott items assumption 1) First, for ${S}=1$, under additional incoherence/smoothness assumptions on ${R}$, we propose the phased algorithm PhasedClusterElim. Our algorithm obtains a near-optimal per-user regret of $\\tilde{O}({N}{M}^{-1}(\\Delta^{-1}+\\Delta_{\\text{hott}}^{-2}))$ where $\\Delta_{\\text{hott}},\\Delta$ are problem-dependent gap parameters with $\\Delta_{\\text{hott}} >> \\Delta$ almost always. 2) Second, we consider a simplified setting with ${S}=r$ where we make significantly milder assumptions on ${R}$. Here, we introduce another phased algorithm, DeterminantElim, to derive a regret guarantee of $\\tilde{O}({N}{M}^{-1/r}\\Delta_\\text{det}^{-1}))$ where $\\Delta_{\\text{det}}$ is another problem-dependent gap. Both algorithms crucially use collaboration among users to jointly eliminate sub-optimal items for groups of users successively in phases, but with distinctive and novel approaches",
    "checked": true,
    "id": "50796bda1c112e366c00897c3c93b32a0b01ee22",
    "semantic_title": "online matrix completion: a collaborative approach with hott items",
    "citation_count": 0,
    "authors": [
      "Dheeraj Baby",
      "Soumyabrata Pal"
    ]
  },
  "https://proceedings.mlr.press/v235/bacellar24a.html": {
    "title": "Differentiable Weightless Neural Networks",
    "volume": "main",
    "abstract": "We introduce the Differentiable Weightless Neural Network (DWN), a model based on interconnected lookup tables. Training of DWNs is enabled by a novel Extended Finite Difference technique for approximate differentiation of binary values. We propose Learnable Mapping, Learnable Reduction, and Spectral Regularization to further improve the accuracy and efficiency of these models. We evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardware accelerator, where they demonstrate superior latency, throughput, energy efficiency, and model area compared to state-of-the-art solutions, (2) a low-power microcontroller, where they achieve preferable accuracy to XGBoost while subject to stringent memory constraints, and (3) ultra-low-cost chips, where they consistently outperform small models in both accuracy and projected hardware area. DWNs also compare favorably against leading approaches for tabular datasets, with higher average rank. Overall, our work positions DWNs as a pioneering solution for edge-compatible high-throughput neural networks",
    "checked": true,
    "id": "87e91f6afbf89095ea05ae3612ccc81e3232b5db",
    "semantic_title": "differentiable weightless neural networks",
    "citation_count": 2,
    "authors": [
      "Alan Tendler Leibel Bacellar",
      "Zachary Susskind",
      "Mauricio Breternitz Jr",
      "Eugene John",
      "Lizy Kurian John",
      "Priscila Machado Vieira Lima",
      "Felipe M.G. França"
    ]
  },
  "https://proceedings.mlr.press/v235/bachmann24a.html": {
    "title": "The Pitfalls of Next-Token Prediction",
    "volume": "main",
    "abstract": "Can a mere next-token predictor faithfully model human thinking? Our work is aimed at crystallizing this intuitive concern, which is currently fragmented in the literature. First, we emphasize isolating the two phases of next-token prediction that are often conflated: autoregression during inference vs. teacher-forcing during training. We argue that the previously-identified problem of \"exponential error accumulation\" is a symptom of autoregressive inference. But more concerningly, we identify that teacher-forcing can let the model fit the training data by cheating, causing total in-distribution failure. We design a minimal planning task where empirically both the Transformer and the Mamba architecture fail in this manner - remarkably, despite the task being easy to learn. Overall, our work consolidates these and other essential arguments surrounding next-token prediction. We hope this effort can ground future discussions and inspire explorations beyond the next-token prediction paradigm",
    "checked": true,
    "id": "a01e1138600499f65462ed3d51c3e76af1aad18c",
    "semantic_title": "the pitfalls of next-token prediction",
    "citation_count": 31,
    "authors": [
      "Gregor Bachmann",
      "Vaishnavh Nagarajan"
    ]
  },
  "https://proceedings.mlr.press/v235/back-de-luca24a.html": {
    "title": "Simulation of Graph Algorithms with Looped Transformers",
    "volume": "main",
    "abstract": "The execution of graph algorithms using neural networks has recently attracted significant interest due to promising empirical progress. This motivates further understanding of how neural networks can replicate reasoning steps with relational data. In this work, we study the ability of transformer networks to simulate algorithms on graphs from a theoretical perspective. The architecture we use is a looped transformer with extra attention heads that interact with the graph. We prove by construction that this architecture can simulate individual algorithms such as Dijkstra's shortest path, Breadth- and Depth-First Search, and Kosaraju's strongly connected components, as well as multiple algorithms simultaneously. The number of parameters in the networks does not increase with the input graph size, which implies that the networks can simulate the above algorithms for any graph. Despite this property, we show a limit to simulation in our solution due to finite precision. Finally, we show a Turing Completeness result with constant width when the extra attention heads are utilized",
    "checked": true,
    "id": "53310cd49893559b93f7ca2bb65f8749092a8ec6",
    "semantic_title": "simulation of graph algorithms with looped transformers",
    "citation_count": 9,
    "authors": [
      "Artur Back De Luca",
      "Kimon Fountoulakis"
    ]
  },
  "https://proceedings.mlr.press/v235/bai24a.html": {
    "title": "QBMK: Quantum-based Matching Kernels for Un-attributed Graphs",
    "volume": "main",
    "abstract": "In this work, we develop a new Quantum-based Matching Kernel (QBMK) for un-attributed graphs, by computing the kernel-based similarity between the quantum Shannon entropies of aligned vertices through the Continuous-time Quantum Walk (CTQW). The theoretical analysis reveals that the proposed QBMK kernel not only addresses the shortcoming of neglecting the structural correspondence information between graphs arising in existing R-convolution graph kernels, but also overcomes the problem of neglecting the structural differences between pairs of aligned vertices arising in existing vertex-based matching kernels. Moreover, the proposed QBMK kernel can simultaneously capture both global and local structural characteristics through the quantum Shannon entropies. Experimental evaluations on standard graph datasets demonstrate that the proposed QBMK kernel is able to outperform state-of-the-art graph kernels and graph deep learning approaches",
    "checked": true,
    "id": "ff65551a9aac8cbe2b3e3c819d298a120b3745b6",
    "semantic_title": "qbmk: quantum-based matching kernels for un-attributed graphs",
    "citation_count": 0,
    "authors": [
      "Lu Bai",
      "Lixin Cui",
      "Ming Li",
      "Yue Wang",
      "Edwin Hancock"
    ]
  },
  "https://proceedings.mlr.press/v235/bai24b.html": {
    "title": "Diffusion Models Demand Contrastive Guidance for Adversarial Purification to Advance",
    "volume": "main",
    "abstract": "In adversarial defense, adversarial purification can be viewed as a special generation task with the purpose to remove adversarial attacks and diffusion models excel in adversarial purification for their strong generative power. With different predetermined generation requirements, various types of guidance have been proposed, but few of them focuses on adversarial purification. In this work, we propose to guide diffusion models for adversarial purification using contrastive guidance. We theoretically derive the proper noise level added in the forward process diffusion models for adversarial purification from a feature learning perspective. For the reverse process, it is implied that the role of contrastive loss guidance is to facilitate the evolution towards the signal direction. From the theoretical findings and implications, we design the forward process with the proper amount of Gaussian noise added and the reverse process with the gradient of contrastive loss as the guidance of diffusion models for adversarial purification. Empirically, extensive experiments on CIFAR-10, CIFAR-100, the German Traffic Sign Recognition Benchmark and ImageNet datasets with ResNet and WideResNet classifiers show that our method outperforms most of current adversarial training and adversarial purification methods by a large improvement",
    "checked": true,
    "id": "96b9bb5a5215028739949ff31a1a2921844bfeca",
    "semantic_title": "diffusion models demand contrastive guidance for adversarial purification to advance",
    "citation_count": 0,
    "authors": [
      "Mingyuan Bai",
      "Wei Huang",
      "Tenghui Li",
      "Andong Wang",
      "Junbin Gao",
      "Cesar F Caiafa",
      "Qibin Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/bai24c.html": {
    "title": "On the Complexity of Finite-Sum Smooth Optimization under the Polyak–Łojasiewicz Condition",
    "volume": "main",
    "abstract": "This paper considers the optimization problem of the form $\\min_{{\\bf x}\\in{\\mathbb R}^d} f({\\bf x})\\triangleq \\frac{1}{n}\\sum_{i=1}^n f_i({\\bf x})$, where $f(\\cdot)$ satisfies the Polyak–Łojasiewicz (PL) condition with parameter $\\mu$ and $\\{f_i(\\cdot)\\}_{i=1}^n$ is $L$-mean-squared smooth. We show that any gradient method requires at least $\\Omega(n+\\kappa\\sqrt{n}\\log(1/\\epsilon))$ incremental first-order oracle (IFO) calls to find an $\\epsilon$-suboptimal solution, where $\\kappa\\triangleq L/\\mu$ is the condition number of the problem. This result nearly matches upper bounds of IFO complexity for best-known first-order methods. We also study the problem of minimizing the PL function in the distributed setting such that the individuals $f_1(\\cdot),…,f_n(\\cdot)$ are located on a connected network of $n$ agents. We provide lower bounds of $\\Omega(\\kappa/\\sqrt{\\gamma}\\log(1/\\epsilon))$, $\\Omega((\\kappa+\\tau\\kappa/\\sqrt{\\gamma})\\log(1/\\epsilon))$ and $\\Omega\\big(n+\\kappa\\sqrt{n}\\log(1/\\epsilon)\\big)$ for communication rounds, time cost and local first-order oracle calls respectively, where $\\gamma\\in(0,1]$ is the spectral gap of the mixing matrix associated with the network and $\\tau>0$ is the time cost of per communication round. Furthermore, we propose a decentralized first-order method that nearly matches above lower bounds in expectation",
    "checked": false,
    "id": "5b1ee0c2d34089f03a70a1f2c3e17c1cb6152085",
    "semantic_title": "on the complexity of finite-sum smooth optimization under the polyak-łojasiewicz condition",
    "citation_count": 0,
    "authors": [
      "Yunyan Bai",
      "Yuxing Liu",
      "Luo Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/bai24d.html": {
    "title": "Constrained Ensemble Exploration for Unsupervised Skill Discovery",
    "volume": "main",
    "abstract": "Unsupervised Reinforcement Learning (RL) provides a promising paradigm for learning useful behaviors via reward-free per-training. Existing methods for unsupervised RL mainly conduct empowerment-driven skill discovery or entropy-based exploration. However, empowerment often leads to static skills, and pure exploration only maximizes the state coverage rather than learning useful behaviors. In this paper, we propose a novel unsupervised RL framework via an ensemble of skills, where each skill performs partition exploration based on the state prototypes. Thus, each skill can explore the clustered area locally, and the ensemble skills maximize the overall state coverage. We adopt state-distribution constraints for the skill occupancy and the desired cluster for learning distinguishable skills. Theoretical analysis is provided for the state entropy and the resulting skill distributions. Based on extensive experiments on several challenging tasks, we find our method learns well-explored ensemble skills and achieves superior performance in various downstream tasks compared to previous methods",
    "checked": true,
    "id": "d39b12a480a7c01e1515b6358eb5cab882338a35",
    "semantic_title": "constrained ensemble exploration for unsupervised skill discovery",
    "citation_count": 0,
    "authors": [
      "Chenjia Bai",
      "Rushuai Yang",
      "Qiaosheng Zhang",
      "Kang Xu",
      "Yi Chen",
      "Ting Xiao",
      "Xuelong Li"
    ]
  },
  "https://proceedings.mlr.press/v235/bailey24a.html": {
    "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
    "volume": "main",
    "abstract": "Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour matching to craft hijacks for four types of attack: forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations",
    "checked": true,
    "id": "5bdaadb84db0cbf72aaebda9f55f4288b63c6e9b",
    "semantic_title": "image hijacks: adversarial images can control generative models at runtime",
    "citation_count": 52,
    "authors": [
      "Luke Bailey",
      "Euan Ong",
      "Stuart Russell",
      "Scott Emmons"
    ]
  },
  "https://proceedings.mlr.press/v235/baker24a.html": {
    "title": "An Explicit Frame Construction for Normalizing 3D Point Clouds",
    "volume": "main",
    "abstract": "Many real-world datasets are represented as 3D point clouds – yet they often lack a predefined reference frame, posing a challenge for machine learning or general data analysis. Traditional methods for determining reference frames and normalizing 3D point clouds often struggle with specific inputs, lack theoretical guarantees, or require massive data. We introduce a new algorithm that overcomes these limitations and guarantees both universality and compatibility with any learnable framework for 3D point cloud analysis. Our algorithm works with any input point cloud and performs consistently regardless of input complexities, unlike data-driven methods that are susceptible to biases or limited training data. Empirically, our algorithm outperforms existing methods in effectiveness and generalizability across diverse benchmark datasets. Code is available at https://github.com/Utah-Math-Data-Science/alignment",
    "checked": true,
    "id": "62dd3dadac4d15cb2bc155bd2f77c946660787fb",
    "semantic_title": "an explicit frame construction for normalizing 3d point clouds",
    "citation_count": 2,
    "authors": [
      "Justin Baker",
      "Shih-Hsin Wang",
      "Tommaso De Fernex",
      "Bao Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/balabin24a.html": {
    "title": "Disentanglement Learning via Topology",
    "volume": "main",
    "abstract": "We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding a multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art methods are based on VAE and encourage the joint distribution of latent variables to be factorized. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement learning. Our experiments have shown that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score, and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality. Our method works in an unsupervised manner, permitting us to apply it to problems without labeled factors of variation. The TopDis loss works even when factors of variation are correlated. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN",
    "checked": true,
    "id": "d7ff83cb989e5bdf7b9de766e235c37394614189",
    "semantic_title": "disentanglement learning via topology",
    "citation_count": 2,
    "authors": [
      "Nikita Balabin",
      "Daria Voronkova",
      "Ilya Trofimov",
      "Evgeny Burnaev",
      "Serguei Barannikov"
    ]
  },
  "https://proceedings.mlr.press/v235/balasubramanian24a.html": {
    "title": "Adversarial Attacks on Combinatorial Multi-Armed Bandits",
    "volume": "main",
    "abstract": "We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, a notion to capture the vulnerability and robustness of CMAB. The attackability condition depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanning tree, cascading bandits for online ranking, and online shortest path",
    "checked": true,
    "id": "bdc8312b405eb603bc430e28f9f1dff3dfb369e2",
    "semantic_title": "adversarial attacks on combinatorial multi-armed bandits",
    "citation_count": 2,
    "authors": [
      "Rishab Balasubramanian",
      "Jiawei Li",
      "Prasad Tadepalli",
      "Huazheng Wang",
      "Qingyun Wu",
      "Haoyu Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/balazevic24a.html": {
    "title": "Memory Consolidation Enables Long-Context Video Understanding",
    "volume": "main",
    "abstract": "Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. We propose to instead re-purpose existing pre-trained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memory-consolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters",
    "checked": true,
    "id": "3c23f28bac6c9387573a645673622172ea8b50a5",
    "semantic_title": "memory consolidation enables long-context video understanding",
    "citation_count": 13,
    "authors": [
      "Ivana Balazevic",
      "Yuge Shi",
      "Pinelopi Papalampidi",
      "Rahma Chaabouni",
      "Skanda Koppula",
      "Olivier J Henaff"
    ]
  },
  "https://proceedings.mlr.press/v235/balestriero24a.html": {
    "title": "Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) drive current AI breakthroughs despite very little being known about their internal representations. In this work, we propose to shed the light on LLMs inner mechanisms through the lens of geometry. In particular, we develop in closed form $(i)$ the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and $(ii)$ the partition and per-region affine mappings of the feedforward (MLP) network of LLMs' layers. Our theoretical findings further enable the design of novel principled solutions applicable to state-of-the-art LLMs. First, we show that, through our geometric understanding, we can bypass LLMs' RLHF protection by controlling the embedding's intrinsic dimension through informed prompt manipulation. Second, we derive interpretable geometrical features that can be extracted from any (pre-trained) LLM, providing a rich abstract representation of their inputs. We observe that these features are sufficient to help solve toxicity detection, and even allow the identification of various types of toxicity. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in LLMs. Code: https://github.com/RandallBalestriero/SplineLLM",
    "checked": true,
    "id": "399bd4f36c1bc0228463bd45473c5176e29d5c6a",
    "semantic_title": "characterizing large language model geometry helps solve toxicity detection and generation",
    "citation_count": 1,
    "authors": [
      "Randall Balestriero",
      "Romain Cosentino",
      "Sarath Shekkizhar"
    ]
  },
  "https://proceedings.mlr.press/v235/balestriero24b.html": {
    "title": "How Learning by Reconstruction Produces Uninformative Features For Perception",
    "volume": "main",
    "abstract": "Input space reconstruction is an attractive representation learning paradigm. Despite interpretability benefit of reconstruction and generation, we identify a misalignment between learning to reconstruct, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance–a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90% of the pixel variance can be solved with 45% test accuracy. Using the bottom subspace instead, accounting for only 20% of the pixel variance, reaches 55% test accuracy. Learning by reconstruction is also wasteful as the features for perception are learned last, pushing the need for long training schedules. We finally prove that learning by denoising can alleviate that misalignment for some noise strategies, e.g., masking. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide a solution to detect if a noise strategy is never beneficial regardless of the perception task, e.g., additive Gaussian noise",
    "checked": true,
    "id": "dee50de1da3d917e97ccde8ca103600842612383",
    "semantic_title": "how learning by reconstruction produces uninformative features for perception",
    "citation_count": 0,
    "authors": [
      "Randall Balestriero",
      "Yann Lecun"
    ]
  },
  "https://proceedings.mlr.press/v235/balmaseda24a.html": {
    "title": "Combinatorial Approximations for Cluster Deletion: Simpler, Faster, and Better",
    "volume": "main",
    "abstract": "Cluster deletion is an NP-hard graph clustering objective with applications in computational biology and social network analysis, where the goal is to delete a minimum number of edges to partition a graph into cliques. We first provide a tighter analysis of two previous approximation algorithms, improving their approximation guarantees from 4 to 3. Moreover, we show that both algorithms can be derandomized in a surprisingly simple way, by greedily taking a vertex of maximum degree in an auxiliary graph and forming a cluster around it. One of these algorithms relies on solving a linear program. Our final contribution is to design a new and purely combinatorial approach for doing so that is far more scalable in theory and practice",
    "checked": true,
    "id": "852831ea70ebf7cd23be549e2685df8457a22c61",
    "semantic_title": "combinatorial approximations for cluster deletion: simpler, faster, and better",
    "citation_count": 2,
    "authors": [
      "Vicente Balmaseda",
      "Ying Xu",
      "Yixin Cao",
      "Nate Veldt"
    ]
  },
  "https://proceedings.mlr.press/v235/balseiro24a.html": {
    "title": "A Field Guide for Pacing Budget and ROS Constraints",
    "volume": "main",
    "abstract": "Budget pacing is a popular service that has been offered by major internet advertising platforms since their inception. In the past few years, autobidding products that provide real-time bidding as a service to advertisers have seen a prominent rise in adoption. A popular autobidding stategy is value maximization subject to return-on-spend (ROS) constraints. For historical or business reasons, the systems that govern these two services, namely budget pacing and ROS pacing, are not necessarily always a single unified and coordinated entity that optimizes a global objective subject to both constraints. The purpose of this work is to theoretically and empirically compare algorithms with different degrees of coordination between these two pacing systems. In particular, we compare (a) a fully-decoupled sequential algorithm; (b) a minimally-coupled min-pacing algorithm; (c) a fully-coupled dual-based algorithm. Our main contribution is to theoretically analyze the min-pacing algorithm and show that it attains similar guarantees to the fully-coupled canonical dual-based algorithm. On the other hand, we show that the sequential algorithm, even though appealing by virtue of being fully decoupled, could badly violate the constraints. We validate our theoretical findings empirically by showing that the min-pacing algorithm performs almost as well as the canonical dual-based algorithm on a semi-synthetic dataset that was generated from a large online advertising platform's auction data",
    "checked": true,
    "id": "9bf98329553ec74ff2615a33b1bfaf412e347457",
    "semantic_title": "a field guide for pacing budget and ros constraints",
    "citation_count": 0,
    "authors": [
      "Santiago R. Balseiro",
      "Kshipra Bhawalkar",
      "Zhe Feng",
      "Haihao Lu",
      "Vahab Mirrokni",
      "Balasubramanian Sivan",
      "Di Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/balsells-rodas24a.html": {
    "title": "On the Identifiability of Switching Dynamical Systems",
    "volume": "main",
    "abstract": "The identifiability of latent variable models has received increasing attention due to its relevance in interpretability and out-of-distribution generalisation. In this work, we study the identifiability of Switching Dynamical Systems, taking an initial step toward extending identifiability analysis to sequential latent variable models. We first prove the identifiability of Markov Switching Models, which commonly serve as the prior distribution for the continuous latent variables in Switching Dynamical Systems. We present identification conditions for first-order Markov dependency structures, whose transition distribution is parametrised via non-linear Gaussians. We then establish the identifiability of the latent variables and non-linear mappings in Switching Dynamical Systems up to affine transformations, by leveraging identifiability analysis techniques from identifiable deep latent variable models. We finally develop estimation algorithms for identifiable Switching Dynamical Systems. Throughout empirical studies, we demonstrate the practicality of identifiable Switching Dynamical Systems for segmenting high-dimensional time series such as videos, and showcase the use of identifiable Markov Switching Models for regime-dependent causal discovery in climate data",
    "checked": true,
    "id": "da49a44176fc13dda06d60b10238ac0d3536c29e",
    "semantic_title": "on the identifiability of switching dynamical systems",
    "citation_count": 2,
    "authors": [
      "Carles Balsells-Rodas",
      "Yixin Wang",
      "Yingzhen Li"
    ]
  },
  "https://proceedings.mlr.press/v235/bamas24a.html": {
    "title": "Analyzing $D^α$ seeding for $k$-means",
    "volume": "main",
    "abstract": "One of the most popular clustering algorithms is the celebrated $D^\\alpha$ seeding algorithm (also know as $k$-means++ when $\\alpha=2$) by Arthur and Vassilvitskii (2007), who showed that it guarantees in expectation an $O(2^{2\\alpha}\\cdot \\log k)$-approximate solution to the ($k$,$\\alpha$)-clustering cost (where distances are raised to the power $\\alpha$) for any $\\alpha\\ge 1$. More recently, Balcan, Dick, and White (2018) observed experimentally that using $D^\\alpha$ seeding with $\\alpha>2$ can lead to a better solution with respect to the standard $k$-means objective (i.e. the $(k,2)$-clustering cost). In this paper, we provide a rigorous understanding of this phenomenon. For any $\\alpha>2$, we show that $D^\\alpha$ seeding guarantees in expectation an approximation factor of \\begin{equation*} O_\\alpha \\left(\\left(\\frac{\\sigma_{\\textrm{max}}}{\\sigma_{\\textrm{min}}}\\right)^{2-4/\\alpha}\\cdot (g_\\alpha \\cdot \\min \\lbrace\\ell,\\log k\\rbrace)^{2/\\alpha}\\right) \\end{equation*} with respect to the standard $k$-means cost of any underlying clustering; where $g_\\alpha$ is a parameter capturing the concentration of the points in each cluster, $\\sigma_{\\textrm{max}}$ and $\\sigma_{\\textrm{min}}$ are the maximum and minimum standard deviation of the clusters around their center, and $\\ell$ is the number of distinct mixing weights in the underlying clustering (after rounding them to the nearest power of $2$). For instance, if the underlying clustering is defined by a mixture of $k$ Gaussian distributions with equal cluster variance (up to a constant-factor), then our result implies that: (1) if there are a constant number of mixing weights, any constant $\\alpha>2$ yields a constant-factor approximation; (2) if the mixing weights are arbitrary, any constant $\\alpha>2$ yields an $O\\left(\\log^{2/\\alpha}k\\right)$-approximation, and $\\alpha=\\Theta(\\log\\log k)$ yields an $O(\\log\\log k)^3$-approximation. We complement these results by some lower bounds showing that the dependency on $g_\\alpha$ and $\\sigma_{\\textrm{max}}/\\sigma_{\\textrm{min}}$ is tight. Finally, we provide an experimental validation of the effects of the aforementioned parameters when using $D^\\alpha$ seeding",
    "checked": false,
    "id": "ccf1b4914745cae65a95dc493724fe5de128ea1c",
    "semantic_title": "an analysis of $d^α$ seeding for $k$-means",
    "citation_count": 0,
    "authors": [
      "Etienne Bamas",
      "Sai Ganesh Nagarajan",
      "Ola Svensson"
    ]
  },
  "https://proceedings.mlr.press/v235/bampis24a.html": {
    "title": "Parsimonious Learning-Augmented Approximations for Dense Instances of $\\mathcalNP$-hard Problems",
    "volume": "main",
    "abstract": "The classical work of (Arora et al., 1999) provides a scheme that gives, for any $\\epsilon>0$, a polynomial time $1-\\epsilon$ approximation algorithm for dense instances of a family of $\\mathcal{NP}$-hard problems, such as Max-CUT and Max-$k$-SAT. In this paper we extend and speed up this scheme using a logarithmic number of one-bit predictions. We propose a learning augmented framework which aims at finding fast algorithms which guarantees approximation consistency, smoothness and robustness with respect to the prediction error. We provide such algorithms, which moreover use predictions parsimoniously, for dense instances of various optimization problems",
    "checked": false,
    "id": "5c27b35f69e783caba3086890ee5f324e0f8fef0",
    "semantic_title": "parsimonious learning-augmented approximations for dense instances of np-hard problems",
    "citation_count": 0,
    "authors": [
      "Evripidis Bampis",
      "Bruno Escoffier",
      "Michalis Xefteris"
    ]
  },
  "https://proceedings.mlr.press/v235/ban24a.html": {
    "title": "Fair Resource Allocation in Multi-Task Learning",
    "volume": "main",
    "abstract": "By jointly learning multiple tasks, multi-task learning (MTL) can leverage the shared knowledge across tasks, resulting in improved data efficiency and generalization performance. However, a major challenge in MTL lies in the presence of conflicting gradients, which can hinder the fair optimization of some tasks and subsequently impede MTL's ability to achieve better overall performance. Inspired by fair resource allocation in communication networks, we formulate the optimization of MTL as a utility maximization problem, where the loss decreases across tasks are maximized under different fairness measurements. To address the problem, we propose FairGrad, a novel optimization objective. FairGrad not only enables flexible emphasis on certain tasks but also achieves a theoretical convergence guarantee. Extensive experiments demonstrate that our method can achieve state-of-the-art performance among gradient manipulation methods on a suite of multi-task benchmarks in supervised learning and reinforcement learning. Furthermore, we incorporate the idea of $\\alpha$-fairness into the loss functions of various MTL methods. Extensive empirical studies demonstrate that their performance can be significantly enhanced. Code is available at https://github.com/OptMN-Lab/fairgrad",
    "checked": true,
    "id": "73e5e31ac39c66ebf02396f06d38f4cef15fd2b0",
    "semantic_title": "fair resource allocation in multi-task learning",
    "citation_count": 1,
    "authors": [
      "Hao Ban",
      "Kaiyi Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/band24a.html": {
    "title": "Linguistic Calibration of Long-Form Generations",
    "volume": "main",
    "abstract": "Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce long-form text with calibrated confidence statements. Through the lens of decision-making, we define linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as \"I estimate a 30% chance of...\" or \"I am certain that...\", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under significant domain shifts to scientific and biomedical questions and to an entirely held-out person biography generation task. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making",
    "checked": true,
    "id": "947687643a7d26f7ee370aa40e7ff54d29ab00ea",
    "semantic_title": "linguistic calibration of long-form generations",
    "citation_count": 11,
    "authors": [
      "Neil Band",
      "Xuechen Li",
      "Tengyu Ma",
      "Tatsunori Hashimoto"
    ]
  },
  "https://proceedings.mlr.press/v235/banerjee24a.html": {
    "title": "Relational DNN Verification With Cross Executional Bound Refinement",
    "volume": "main",
    "abstract": "We focus on verifying relational properties defined over deep neural networks (DNNs) such as robustness against universal adversarial perturbations (UAP), certified worst-case hamming distance for binary string classifications, etc. Precise verification of these properties requires reasoning about multiple executions of the same DNN. However, most of the existing works in DNN verification only handle properties defined over single executions and as a result, are imprecise for relational properties. Though few recent works for relational DNN verification, capture linear dependencies between the inputs of multiple executions, they do not leverage dependencies between the outputs of hidden layers producing imprecise results. We develop a scalable relational verifier RACoon that utilizes cross-execution dependencies at all layers of the DNN gaining substantial precision over SOTA baselines on a wide range of datasets, networks, and relational properties",
    "checked": true,
    "id": "7a463fb6447e3b231efd7bf8fb88bd8a705e1d25",
    "semantic_title": "relational dnn verification with cross executional bound refinement",
    "citation_count": 2,
    "authors": [
      "Debangshu Banerjee",
      "Gagandeep Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/banihashem24a.html": {
    "title": "A Dynamic Algorithm for Weighted Submodular Cover Problem",
    "volume": "main",
    "abstract": "We initiate the study of the submodular cover problem in a dynamic setting where the elements of the ground set are inserted and deleted. In the classical submodular cover problem, we are given a monotone submodular function $f : 2^{V} \\to \\mathbb{R}^{\\ge 0}$ and the goal is to obtain a set $S \\subseteq V$ that minimizes the cost subject to the constraint $f(S) = f(V)$. This is a classical problem in computer science and generalizes the Set Cover problem, 2-Set Cover, and dominating set problem among others. We consider this problem in a dynamic setting where there are updates to our set $V$, in the form of insertions and deletions of elements from a ground set $\\mathcal{V}$, and the goal is to maintain an approximately optimal solution with low query complexity per update. For this problem, we propose a randomized algorithm that, in expectation, obtains a $(1-O(\\epsilon), O(\\epsilon^{-1}))$-bicriteria approximation using polylogarithmic query complexity per update",
    "checked": true,
    "id": "6ebc4ab1d0a46aa0e7d2474f2a5e30dda1c25991",
    "semantic_title": "a dynamic algorithm for weighted submodular cover problem",
    "citation_count": 0,
    "authors": [
      "Kiarash Banihashem",
      "Samira Goudarzi",
      "Mohammadtaghi Hajiaghayi",
      "Peyman Jabbarzade",
      "Morteza Monemizadeh"
    ]
  },
  "https://proceedings.mlr.press/v235/banihashem24b.html": {
    "title": "Dynamic Metric Embedding into lp Space",
    "volume": "main",
    "abstract": "We give the first non-trivial decremental dynamic embedding of a weighted, undirected graph $G$ into $\\ell_p$ space. Given a weighted graph $G$ undergoing a sequence of edge weight increases, the goal of this problem is to maintain a (randomized) mapping $\\phi: (G,d) \\to (X,\\ell_p)$ from the set of vertices of the graph to the $\\ell_p$ space such that for every pair of vertices $u$ and $v$, the expected distance between $\\phi(u)$ and $\\phi(v)$ in the $\\ell_p$ metric is within a small multiplicative factor, referred to as the distortion, of their distance in $G$. Our main result is a dynamic algorithm with expected distortion $O(\\log^2 n)$ and total update time $O\\left((m^{1+o(1)} \\log^2 W + Q)\\log(nW) \\right)$, where $W$ is the maximum weight of the edges, $Q$ is the total number of updates and $n, m$ denote the number of vertices and edges in $G$ respectively. This is the first result of its kind, extending the seminal result of Bourgain '85 to the expanding field of dynamic algorithms. Moreover, we demonstrate that in the fully dynamic regime, where we tolerate edge insertions as well as deletions, no algorithm can explicitly maintain an embedding into $\\ell_p$ space that has a low distortion with high probability",
    "checked": true,
    "id": "004b03fb82c77b2fe4f0a25f124ac191b3588cc6",
    "semantic_title": "dynamic metric embedding into lp space",
    "citation_count": 1,
    "authors": [
      "Kiarash Banihashem",
      "Mohammadtaghi Hajiaghayi",
      "Dariusz Rafal Kowalski",
      "Jan Olkowski",
      "Max Springer"
    ]
  },
  "https://proceedings.mlr.press/v235/baninajjar24a.html": {
    "title": "VNN: Verification-Friendly Neural Networks with Hard Robustness Guarantees",
    "volume": "main",
    "abstract": "Machine learning techniques often lack formal correctness guarantees, evidenced by the widespread adversarial examples that plague most deep-learning applications. This lack of formal guarantees resulted in several research efforts that aim at verifying Deep Neural Networks (DNNs), with a particular focus on safety-critical applications. However, formal verification techniques still face major scalability and precision challenges. The over-approximation introduced during the formal verification process to tackle the scalability challenge often results in inconclusive analysis. To address this challenge, we propose a novel framework to generate Verification-Friendly Neural Networks (VNNs). We present a post-training optimization framework to achieve a balance between preserving prediction performance and verification-friendliness. Our proposed framework results in VNNs that are comparable to the original DNNs in terms of prediction performance, while amenable to formal verification techniques. This essentially enables us to establish robustness for more VNNs than their DNN counterparts, in a time-efficient manner",
    "checked": true,
    "id": "f8c34e3630d2f4d6b99142d0134a07d2c00996cd",
    "semantic_title": "vnn: verification-friendly neural networks with hard robustness guarantees",
    "citation_count": 1,
    "authors": [
      "Anahita Baninajjar",
      "Ahmed Rezine",
      "Amir Aminifar"
    ]
  },
  "https://proceedings.mlr.press/v235/bao24a.html": {
    "title": "Provable Benefits of Local Steps in Heterogeneous Federated Learning for Neural Networks: A Feature Learning Perspective",
    "volume": "main",
    "abstract": "Local steps are crucial for Federated Learning (FL) algorithms and have witnessed great empirical success in reducing communication costs and improving the generalization performance of deep neural networks. However, there are limited studies on the effect of local steps on heterogeneous FL. A few works investigate this problem from the optimization perspective. Woodworth et al. (2020a) showed that the iteration complexity of Local SGD, the most popular FL algorithm, is dominated by the baseline mini-batch SGD, which does not show the benefits of local steps. In addition, Levy (2023) proposed a new local update method that provably benefits over mini-batch SGD. However, in the same setting, there is still no work analyzing the effects of local steps to generalization in a heterogeneous FL setting. Motivated by our experimental findings where Local SGD learns more distinguishing features than parallel SGD, this paper studies the generalization benefits of local steps from a feature learning perspective. We propose a novel federated data model that exhibits a new form of data heterogeneity, under which we show that a convolutional neural network (CNN) trained by GD with global updates will miss some pattern-related features, while the network trained by GD with local updates can learn all features in polynomial time. Consequently, local steps help CNN generalize better in our data model. In a different parameter setting, we also prove that Local GD with one-shot model averaging can learn all features and generalize well in all clients. Our experimental results also confirm the benefits of local steps in improving test accuracy on real-world data",
    "checked": true,
    "id": "f4f881bacf35461a8a74876c483056d2e5d7772e",
    "semantic_title": "provable benefits of local steps in heterogeneous federated learning for neural networks: a feature learning perspective",
    "citation_count": 0,
    "authors": [
      "Yajie Bao",
      "Michael Crawshaw",
      "Mingrui Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/bao24b.html": {
    "title": "Self-attention Networks Localize When QK-eigenspectrum Concentrates",
    "volume": "main",
    "abstract": "The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability",
    "checked": true,
    "id": "b6dd8671099ed834850290530a6dc347303adfbf",
    "semantic_title": "self-attention networks localize when qk-eigenspectrum concentrates",
    "citation_count": 2,
    "authors": [
      "Han Bao",
      "Ryuichiro Hataya",
      "Ryo Karakida"
    ]
  },
  "https://proceedings.mlr.press/v235/bao24c.html": {
    "title": "Graph Out-of-Distribution Detection Goes Neighborhood Shaping",
    "volume": "main",
    "abstract": "Despite the rich line of research works on out-of-distribution (OOD) detection on images, the literature on OOD detection for interdependent data, e.g., graphs, is still relatively limited. To fill this gap, we introduce TopoOOD as a principled approach that accommodates graph topology and neighborhood context for detecting OOD node instances on graphs. Meanwhile, we enrich the experiment settings by splitting in-distribution (ID) and OOD data based on distinct topological distributions, which presents new benchmarks for a more comprehensive analysis of graph-based OOD detection. The latter is designed to thoroughly assess the performance of these discriminators under distribution shifts involving structural information, providing a rigorous evaluation of methods in the emerging area of OOD detection on graphs. Our experimental results show the competitiveness of the proposed model across multiple datasets, as evidenced by up to a 15% increase in the AUROC and a 50% decrease in the FPR compared to existing state-of-the-art methods",
    "checked": true,
    "id": "571062267e70a4e667704104dd74cbf66374d2f4",
    "semantic_title": "graph out-of-distribution detection goes neighborhood shaping",
    "citation_count": 0,
    "authors": [
      "Tianyi Bao",
      "Qitian Wu",
      "Zetian Jiang",
      "Yiting Chen",
      "Jiawei Sun",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/bar24a.html": {
    "title": "Stochastic positional embeddings improve masked image modeling",
    "volume": "main",
    "abstract": "Masked Image Modeling (MIM) is a promising self-supervised learning approach that enables learning from unlabeled images. Despite its recent success, learning good representations through MIM remains challenging because it requires predicting the right semantic content in accurate locations. For example, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose to incorporate location uncertainty to MIM by using stochastic positional embeddings (StoP). Specifically, we condition the model on stochastic masked token positions drawn from a gaussian distribution. We show that using StoP reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainties. Quantitatively, using StoP improves downstream MIM performance on a variety of downstream tasks. For example, linear probing on ImageNet using ViT-B is improved by $+1.7%$, and by $2.5%$ for ViT-H using 1% of the data",
    "checked": true,
    "id": "da136b4651e035c2bafd0bfd34433faf7af2619e",
    "semantic_title": "stochastic positional embeddings improve masked image modeling",
    "citation_count": 1,
    "authors": [
      "Amir Bar",
      "Florian Bordes",
      "Assaf Shocher",
      "Mido Assran",
      "Pascal Vincent",
      "Nicolas Ballas",
      "Trevor Darrell",
      "Amir Globerson",
      "Yann Lecun"
    ]
  },
  "https://proceedings.mlr.press/v235/bar-shalom24a.html": {
    "title": "Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products",
    "volume": "main",
    "abstract": "In the realm of Graph Neural Networks (GNNs), two exciting research directions have recently emerged: Subgraph GNNs and Graph Transformers. In this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, message-passing mechanisms, and aggregation schemes from Subgraph GNNs with attention and positional encodings, arguably the most important components in Graph Transformers. Our method is based on an intriguing new connection we reveal between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the graph with itself. We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product graph. Following this, we propose a novel and efficient positional encoding scheme for Subgraph GNNs, which we derive as a positional encoding for the product graph. Our experimental results demonstrate significant performance improvements over both Subgraph GNNs and Graph Transformers on a wide range of datasets",
    "checked": true,
    "id": "a951bd5f667a63459c4f2f70f39724074634b89a",
    "semantic_title": "subgraphormer: unifying subgraph gnns and graph transformers via graph products",
    "citation_count": 3,
    "authors": [
      "Guy Bar-Shalom",
      "Beatrice Bevilacqua",
      "Haggai Maron"
    ]
  },
  "https://proceedings.mlr.press/v235/barbarani24a.html": {
    "title": "Scale-Free Image Keypoints Using Differentiable Persistent Homology",
    "volume": "main",
    "abstract": "In computer vision, keypoint detection is a fundamental task, with applications spanning from robotics to image retrieval; however, existing learning-based methods suffer from scale dependency, and lack flexibility. This paper introduces a novel approach that leverages Morse theory and persistent homology, powerful tools rooted in algebraic topology. We propose a novel loss function based on the recent introduction of a notion of subgradient in persistent homology, paving the way towards topological learning. Our detector, MorseDet, is the first topology-based learning model for feature detection, which achieves competitive performance in keypoint repeatability and introduces a principled and theoretically robust approach to the problem",
    "checked": true,
    "id": "1910e6f1fac8d886dbf65cb9d9daf26b14f2b24e",
    "semantic_title": "scale-free image keypoints using differentiable persistent homology",
    "citation_count": 0,
    "authors": [
      "Giovanni Barbarani",
      "Francesco Vaccarino",
      "Gabriele Trivigno",
      "Marco Guerra",
      "Gabriele Berton",
      "Carlo Masone"
    ]
  },
  "https://proceedings.mlr.press/v235/barbulescu24a.html": {
    "title": "To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models",
    "volume": "main",
    "abstract": "LLMs have been found to memorize training textual sequences and regurgitate verbatim said sequences during text generation time. This fact is known to be the cause of privacy and related (e.g., copyright) problems. Unlearning in LLMs then takes the form of devising new algorithms that will properly deal with these side-effects of memorized data, while not hurting the model's utility. We offer a fresh perspective towards this goal, namely, that each textual sequence to be forgotten should be treated differently when being unlearned based on its degree of memorization within the LLM. We contribute a new metric for measuring unlearning quality, an adversarial attack showing that SOTA algorithms lacking this perspective fail for privacy, and two new unlearning methods based on Gradient Ascent and Task Arithmetic, respectively. A comprehensive performance evaluation across an extensive suite of NLP tasks then mapped the solution space, identifying the best solutions under different scales in model capacities and forget set sizes and quantified the gains of the new approaches",
    "checked": true,
    "id": "5606211736f4e6d564d9e4b2a03bc884b43307bd",
    "semantic_title": "to each (textual sequence) its own: improving memorized-data unlearning in large language models",
    "citation_count": 10,
    "authors": [
      "George-Octavian Bărbulescu",
      "Peter Triantafillou"
    ]
  },
  "https://proceedings.mlr.press/v235/bardone24a.html": {
    "title": "Sliding Down the Stairs: How Correlated Latent Variables Accelerate Learning with Neural Networks",
    "volume": "main",
    "abstract": "Neural networks extract features from data using stochastic gradient descent (SGD). In particular, higher-order input cumulants (HOCs) are crucial for their performance. However, extracting information from the $p$th cumulant of $d$-dimensional inputs is computationally hard: the number of samples required to recover a single direction from an order-$p$ tensor (tensor PCA) using SGD grows as $d^{p−1}$, which is prohibitive for high-dimensional inputs. This result raises the question of how neural networks extract relevant directions from the HOCs of their inputs efficiently. Here, we show that correlations between latent variables along the directions encoded in different input cumulants speed up learning from higher-order correlations. We show this effect analytically by deriving nearly sharp thresholds for the number of samples required by a single neuron to recover these directions using online SGD from a random start in high dimensions. Our analytical results are confirmed in simulations of two-layer neural networks and unveil a new mechanism for hierarchical learning in neural networks",
    "checked": true,
    "id": "7ee614992f4cad589f1caa4bdc02e9b731605d60",
    "semantic_title": "sliding down the stairs: how correlated latent variables accelerate learning with neural networks",
    "citation_count": 4,
    "authors": [
      "Lorenzo Bardone",
      "Sebastian Goldt"
    ]
  },
  "https://proceedings.mlr.press/v235/bartoldson24a.html": {
    "title": "Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies",
    "volume": "main",
    "abstract": "This paper revisits the simple, long-studied, yet still unsolved problem of making image classifiers robust to imperceptible perturbations. Taking CIFAR10 as an example, SOTA clean accuracy is about $100$%, but SOTA robustness to $\\ell_{\\infty}$-norm bounded perturbations barely exceeds $70$%. To understand this gap, we analyze how model size, dataset size, and synthetic data quality affect robustness by developing the first scaling laws for adversarial training. Our scaling laws reveal inefficiencies in prior art and provide actionable feedback to advance the field. For instance, we discovered that SOTA methods diverge notably from compute-optimal setups, using excess compute for their level of robustness. Leveraging a compute-efficient setup, we surpass the prior SOTA with $20$% ($70$%) fewer training (inference) FLOPs. We trained various compute-efficient models, with our best achieving $74$% AutoAttack accuracy ($+3$% gain). However, our scaling laws also predict robustness slowly grows then plateaus at $90$%: dwarfing our new SOTA by scaling is impractical, and perfect robustness is impossible. To better understand this predicted limit, we carry out a small-scale human evaluation on the AutoAttack data that fools our top-performing model. Concerningly, we estimate that human performance also plateaus near $90$%, which we show to be attributable to $\\ell_{\\infty}$-constrained attacks' generation of invalid images not consistent with their original labels. Having characterized limiting roadblocks, we outline promising paths for future research",
    "checked": true,
    "id": "b651b4c2f693801171d9a70c6b50db2e91bb6fd6",
    "semantic_title": "adversarial robustness limits via scaling-law and human-alignment studies",
    "citation_count": 7,
    "authors": [
      "Brian R. Bartoldson",
      "James Diffenderfer",
      "Konstantinos Parasyris",
      "Bhavya Kailkhura"
    ]
  },
  "https://proceedings.mlr.press/v235/bartosh24a.html": {
    "title": "Neural Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs through experiments on many image generation benchmarks, including MNIST, CIFAR-10, downsampled versions of ImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms of likelihood, achieving state-of-the-art results on ImageNet and CelebA-HQ, and produces high-quality samples",
    "checked": true,
    "id": "0dab896e3659e1a61724ecaa552071949e6cc73f",
    "semantic_title": "neural diffusion models",
    "citation_count": 4,
    "authors": [
      "Grigory Bartosh",
      "Dmitry Vetrov",
      "Christian A. Naesseth"
    ]
  },
  "https://proceedings.mlr.press/v235/barzilai24a.html": {
    "title": "Generalization in Kernel Regression Under Realistic Assumptions",
    "volume": "main",
    "abstract": "It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. When applied to common kernels, our results imply benign overfitting in high input dimensions, nearly tempered overfitting in fixed dimensions, and explicit convergence rates for regularized regression. As a by-product, we obtain time-dependent bounds for neural networks trained in the kernel regime. Our results rely on new relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the kernel implicitly leads to good generalization",
    "checked": true,
    "id": "ee6b02a348c920ec9ff09b081614cd24ca45feb4",
    "semantic_title": "generalization in kernel regression under realistic assumptions",
    "citation_count": 11,
    "authors": [
      "Daniel Barzilai",
      "Ohad Shamir"
    ]
  },
  "https://proceedings.mlr.press/v235/bassan24a.html": {
    "title": "Local vs. Global Interpretability: A Computational Complexity Perspective",
    "volume": "main",
    "abstract": "The local and global interpretability of various ML models has been studied extensively in recent years. However, despite significant progress in the field, many known results remain informal or lack sufficient mathematical rigor. We propose a framework for bridging this gap, by using computational complexity theory to assess local and global perspectives of interpreting ML models. We begin by proposing proofs for two novel insights that are essential for our analysis: (1) a duality between local and global forms of explanations; and (2) the inherent uniqueness of certain global explanation forms. We then use these insights to evaluate the complexity of computing explanations, across three model types representing the extremes of the interpretability spectrum: (1) linear models; (2) decision trees; and (3) neural networks. Our findings offer insights into both the local and global interpretability of these models. For instance, under standard complexity assumptions such as P != NP, we prove that selecting global sufficient subsets in linear models is computationally harder than selecting local subsets. Interestingly, with neural networks and decision trees, the opposite is true: it is harder to carry out this task locally than globally. We believe that our findings demonstrate how examining explainability through a computational complexity lens can help us develop a more rigorous grasp of the inherent interpretability of ML models",
    "checked": true,
    "id": "f99b4dbd359fdce2f20ac6cb3ed42076782507f4",
    "semantic_title": "local vs. global interpretability: a computational complexity perspective",
    "citation_count": 1,
    "authors": [
      "Shahaf Bassan",
      "Guy Amir",
      "Guy Katz"
    ]
  },
  "https://proceedings.mlr.press/v235/bassily24a.html": {
    "title": "Differentially Private Domain Adaptation with Theoretical Guarantees",
    "volume": "main",
    "abstract": "In many applications, the labeled data at the learner's disposal is subject to privacy constraints and is relatively limited. To derive a more accurate predictor for the target domain, it is often beneficial to leverage publicly available labeled data from an alternative domain, somewhat close to the target domain. This is the modern problem of supervised domain adaptation from a public source to a private target domain. We present two $(\\epsilon, \\delta)$-differentially private adaptation algorithms for supervised adaptation, for which we make use of a general optimization problem, recently shown to benefit from favorable theoretical learning guarantees. Our first algorithm is designed for regression with linear predictors and shown to solve a convex optimization problem. Our second algorithm is a more general solution for loss functions that may be non-convex but Lipschitz and smooth. While our main objective is a theoretical analysis, we also report the results of several experiments. We first show that the non-private versions of our algorithms match state-of-the-art performance in supervised adaptation and that for larger values of the target sample size or $\\epsilon$, the performance of our private algorithms remains close to that of their non-private counterparts",
    "checked": true,
    "id": "f0bae3345eba8b6c1160ce564f72ccf01f16f8cb",
    "semantic_title": "differentially private domain adaptation with theoretical guarantees",
    "citation_count": 0,
    "authors": [
      "Raef Bassily",
      "Corinna Cortes",
      "Anqi Mao",
      "Mehryar Mohri"
    ]
  },
  "https://proceedings.mlr.press/v235/basu24a.html": {
    "title": "A Statistical Framework for Data-dependent Retrieval-Augmented Models",
    "volume": "main",
    "abstract": "Modern ML systems increasingly augment input instances with additional relevant information to enhance final prediction. Despite growing interest in such retrieval-augmented models, their fundamental properties and training are not well understood. We propose a statistical framework to study such models with two components: 1) a retriever to identify the relevant information out of a large corpus via a data-dependent metric; and 2) a predictor that consumes the input instances along with the retrieved information to make the final predictions. We present a principled method for end-to-end training of both components and draw connections with various training approaches in the literature. Furthermore, we establish excess risk bounds for retrieval-augmented models while delineating the contributions of both retriever and predictor towards the model performance.We validate the utility of our proposed training methods along with the key takeaways from our statistical analysis on open domain question answering task where retrieval augmentation is important",
    "checked": true,
    "id": "aca5b6de9d2f4a764fb50d330ca0c756b2673b9b",
    "semantic_title": "a statistical framework for data-dependent retrieval-augmented models",
    "citation_count": 0,
    "authors": [
      "Soumya Basu",
      "Ankit Singh Rawat",
      "Manzil Zaheer"
    ]
  },
  "https://proceedings.mlr.press/v235/basu24b.html": {
    "title": "On Mechanistic Knowledge Localization in Text-to-Image Generative Models",
    "volume": "main",
    "abstract": "Identifying layers within text-to-image models which control visual attributes can facilitate efficient model editing through closed-form updates. Recent work, leveraging causal tracing show that early Stable-Diffusion variants confine knowledge primarily to the first layer of the CLIP text-encoder, while it diffuses throughout the UNet. Extending this framework, we observe that for recent models (e.g., SD-XL, DeepFloyd), causal tracing fails in pinpointing localized knowledge, highlighting challenges in model editing. To address this issue, we introduce the concept of mechanistic localization in text-to-image models, where knowledge about various visual attributes (e.g., \"style\", \"objects\", \"facts\") can be mechanistically localized to a small fraction of layers in the UNet, thus facilitating efficient model editing. We localize knowledge using our method LocoGen which measures the direct effect of intermediate layers to output generation by performing interventions in the cross-attention layers of the UNet. We then employ LocoEdit, a fast closed-form editing method across popular open-source text-to-image models (including the latest SD-XL) and explore the possibilities of neuron-level model editing. Using mechanistic localization, our work offers a better view of successes and failures in localization-based text-to-image model editing",
    "checked": true,
    "id": "c0362d0ef4f31ec24b2fdc9a71039cf1a5b23f7d",
    "semantic_title": "on mechanistic knowledge localization in text-to-image generative models",
    "citation_count": 6,
    "authors": [
      "Samyadeep Basu",
      "Keivan Rezaei",
      "Priyatham Kattakinda",
      "Vlad I Morariu",
      "Nanxuan Zhao",
      "Ryan A. Rossi",
      "Varun Manjunatha",
      "Soheil Feizi"
    ]
  },
  "https://proceedings.mlr.press/v235/bechavod24a.html": {
    "title": "Monotone Individual Fairness",
    "volume": "main",
    "abstract": "We revisit the problem of online learning with individual fairness, where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly. We first extend the frameworks of Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human auditors regarding fairness violations, to allow for auditing schemes that can aggregate feedback from any number of auditors, using a rich class we term monotone aggregation functions, for which we also prove a useful characterization. Using our generalized framework, we present an oracle-efficient algorithm guaranteeing a bound of $\\mathcal{O}(T^\\frac{3}{4})$ simultaneously for regret and number of fairness violations. We then study an online classification setting where label feedback is available for positively-predicted individuals only, and present an algorithm guaranteeing a bound of $\\mathcal{O}(T^\\frac{5}{6})$ simultaneously for regret and number of fairness violations. In both settings, our algorithms improve on the best known bounds for oracle-efficient algorithms. Furthermore, our algorithms offer significant improvements in computational efficiency, greatly reducing the number of required calls to an (offline) optimization oracle, as opposed to previous algorithms which required $T$ such calls every round",
    "checked": true,
    "id": "b275157661c7d18448d95426a54d24aa3126b559",
    "semantic_title": "monotone individual fairness",
    "citation_count": 0,
    "authors": [
      "Yahav Bechavod"
    ]
  },
  "https://proceedings.mlr.press/v235/bechler-speicher24a.html": {
    "title": "Graph Neural Networks Use Graphs When They Shouldn't",
    "volume": "main",
    "abstract": "Predictions over graphs play a crucial role in various domains, including social networks and medicine. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Although a graph-structure is provided as input to the GNN, in some cases the best solution can be obtained by ignoring it. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the given graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We analyze the implicit bias of gradient-descent learning of GNNs and prove that when the ground truth function does not use the graphs, GNNs are not guaranteed to learn a solution that ignores the graph, even with infinite data. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting. We also prove that within the family of regular graphs, GNNs are guaranteed to extrapolate when learning with gradient descent. Finally, based on our empirical and theoretical findings, we demonstrate on real-data how regular graphs can be leveraged to reduce graph overfitting and enhance performance",
    "checked": true,
    "id": "c52c7c0d72d882f9c7f7659e9e77b78e6093fdfb",
    "semantic_title": "graph neural networks use graphs when they shouldn't",
    "citation_count": 8,
    "authors": [
      "Maya Bechler-Speicher",
      "Ido Amos",
      "Ran Gilad-Bachrach",
      "Amir Globerson"
    ]
  },
  "https://proceedings.mlr.press/v235/beck24a.html": {
    "title": "Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations",
    "volume": "main",
    "abstract": "Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin–Huxley model with a practically relevant number of parameters",
    "checked": true,
    "id": "f1ef492883f7104554c62985deb712aada8c1931",
    "semantic_title": "diffusion tempering improves parameter estimation with probabilistic integrators for ordinary differential equations",
    "citation_count": 4,
    "authors": [
      "Jonas Beck",
      "Nathanael Bosch",
      "Michael Deistler",
      "Kyra L. Kadhim",
      "Jakob H. Macke",
      "Philipp Hennig",
      "Philipp Berens"
    ]
  },
  "https://proceedings.mlr.press/v235/becker24a.html": {
    "title": "Standardized Interpretable Fairness Measures for Continuous Risk Scores",
    "volume": "main",
    "abstract": "We propose a standardized version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed standardized fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-based fairness measures miss",
    "checked": true,
    "id": "e63508f60393d4c3938e19d0084e3a9a9f4ed4bf",
    "semantic_title": "standardized interpretable fairness measures for continuous risk scores",
    "citation_count": 0,
    "authors": [
      "Ann-Kristin Becker",
      "Oana Dumitrasc",
      "Klaus Broelemann"
    ]
  },
  "https://proceedings.mlr.press/v235/behrouz24a.html": {
    "title": "Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity",
    "volume": "main",
    "abstract": "Effective brain representation learning is a key step toward the understanding of cognitive processes and diagnosis of neurological diseases/disorders. Existing studies have focused on either (1) voxel-level activity, where only a single weight relating the voxel activity to the task (i.e., aggregation of voxel activity over a time window) is considered, missing their temporal dynamics, or (2) functional connectivity of the brain in the level of region of interests, missing voxel-level activities. We bridge this gap and design BrainMixer, an unsupervised learning framework that effectively utilizes both functional connectivity and associated time series of voxels to learn voxel-level representation in an unsupervised manner. BrainMixer employs two simple yet effective MLP-based encoders to simultaneously learn the dynamics of voxel-level signals and their functional correlations. To encode voxel activity, BrainMixer fuses information across both time and voxel dimensions via a dynamic attention mechanism. To learn the structure of the functional connectivity, BrainMixer presents a temporal graph patching and encodes each patch by combining its nodes' features via a new adaptive temporal pooling. Our experiments show that BrainMixer attains outstanding performance and outperforms 14 baselines in different downstream tasks and setups",
    "checked": true,
    "id": "6daa30531a828976a4bf59d372749d5812d83bca",
    "semantic_title": "unsupervised representation learning of brain activity via bridging voxel activity and functional connectivity",
    "citation_count": 5,
    "authors": [
      "Ali Behrouz",
      "Parsa Delavari",
      "Farnoosh Hashemi"
    ]
  },
  "https://proceedings.mlr.press/v235/belrose24a.html": {
    "title": "Neural Networks Learn Statistics of Increasing Complexity",
    "volume": "main",
    "abstract": "The distributional simplicity bias (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time",
    "checked": true,
    "id": "4563516164acbd3978d6c32811f0edb9297131c9",
    "semantic_title": "neural networks learn statistics of increasing complexity",
    "citation_count": 7,
    "authors": [
      "Nora Belrose",
      "Quintin Pope",
      "Lucia Quirke",
      "Alex Troy Mallen",
      "Xiaoli Fern"
    ]
  },
  "https://proceedings.mlr.press/v235/ben-basat24a.html": {
    "title": "Accelerating Federated Learning with Quick Distributed Mean Estimation",
    "volume": "main",
    "abstract": "Distributed Mean Estimation (DME), in which $n$ clients communicate vectors to a parameter server that estimates their average, is a fundamental building block in communication-efficient federated learning. In this paper, we improve on previous DME techniques that achieve the optimal $O(1/n)$ Normalized Mean Squared Error (NMSE) guarantee by asymptotically improving the complexity for either encoding or decoding (or both). To achieve this, we formalize the problem in a novel way that allows us to use off-the-shelf mathematical solvers to design the quantization. Using various datasets and training tasks, we demonstrate how QUIC-FL achieves state of the art accuracy with faster encoding and decoding times compared to other DME methods",
    "checked": true,
    "id": "fdec23777fcd380e658d2c0253d895f082a1dccd",
    "semantic_title": "accelerating federated learning with quick distributed mean estimation",
    "citation_count": 1,
    "authors": [
      "Ran Ben-Basat",
      "Shay Vargaftik",
      "Amit Portnoy",
      "Gil Einziger",
      "Yaniv Ben-Itzhak",
      "Michael Mitzenmacher"
    ]
  },
  "https://proceedings.mlr.press/v235/ben-dov24a.html": {
    "title": "The Role of Learning Algorithms in Collective Action",
    "volume": "main",
    "abstract": "Collective action in machine learning is the study of the control that a coordinated group can have over machine learning algorithms. While previous research has concentrated on assessing the impact of collectives against Bayes (sub-)optimal classifiers, this perspective is limited in that it does not account for the choice of learning algorithm. Since classifiers seldom behave like Bayes classifiers and are influenced by the choice of learning algorithms along with their inherent biases, in this work we initiate the study of how the choice of the learning algorithm plays a role in the success of a collective in practical settings. Specifically, we focus on distributionally robust optimization (DRO), popular for improving a worst group error, and on the ubiquitous stochastic gradient descent (SGD), due to its inductive bias for \"simpler\" functions. Our empirical results, supported by a theoretical foundation, show that the effective size and success of the collective are highly dependent on properties of the learning algorithm. This highlights the necessity of taking the learning algorithm into account when studying the impact of collective action in machine learning",
    "checked": true,
    "id": "de0e9e602f236650b589f302e93ad59a7b0f3ef3",
    "semantic_title": "the role of learning algorithms in collective action",
    "citation_count": 2,
    "authors": [
      "Omri Ben-Dov",
      "Jake Fawkes",
      "Samira Samadi",
      "Amartya Sanyal"
    ]
  },
  "https://proceedings.mlr.press/v235/ben-hamu24a.html": {
    "title": "D-Flow: Differentiating through Flows for Controlled Generation",
    "volume": "main",
    "abstract": "Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all",
    "checked": true,
    "id": "20a9fdf9e3b77be8d182fb04646ffc11c9708367",
    "semantic_title": "d-flow: differentiating through flows for controlled generation",
    "citation_count": 15,
    "authors": [
      "Heli Ben-Hamu",
      "Omri Puny",
      "Itai Gat",
      "Brian Karrer",
      "Uriel Singer",
      "Yaron Lipman"
    ]
  },
  "https://proceedings.mlr.press/v235/benkert24a.html": {
    "title": "Transitional Uncertainty with Layered Intermediate Predictions",
    "volume": "main",
    "abstract": "In this paper, we discuss feature engineering for single-pass uncertainty estimation. For accurate uncertainty estimates, neural networks must extract differences in the feature space that quantify uncertainty. This could be achieved by current single-pass approaches that maintain feature distances between data points as they traverse the network. While initial results are promising, maintaining feature distances within the network representations frequently inhibits information compression and opposes the learning objective. We study this effect theoretically and empirically to arrive at a simple conclusion: preserving feature distances in the output is beneficial when the preserved features contribute to learning the label distribution and act in opposition otherwise. We then propose Transitional Uncertainty with Layered Intermediate Predictions (TULIP) as a simple approach to address the shortcomings of current single-pass estimators. Specifically, we implement feature preservation by extracting features from intermediate representations before information is collapsed by subsequent layers. We refer to the underlying preservation mechanism as transitional feature preservation. We show that TULIP matches or outperforms current single-pass methods on standard benchmarks and in practical settings where these methods are less reliable (imbalances, complex architectures, medical modalities)",
    "checked": true,
    "id": "739806cafc54bf7e80ca3c4a26e2e94b30a3922d",
    "semantic_title": "transitional uncertainty with layered intermediate predictions",
    "citation_count": 0,
    "authors": [
      "Ryan Benkert",
      "Mohit Prabhushankar",
      "Ghassan Alregib"
    ]
  },
  "https://proceedings.mlr.press/v235/benomar24a.html": {
    "title": "Non-clairvoyant Scheduling with Partial Predictions",
    "volume": "main",
    "abstract": "The non-clairvoyant scheduling problem has gained new interest within learning-augmented algorithms, where the decision-maker is equipped with predictions without any quality guarantees. In practical settings, access to predictions may be reduced to specific instances, due to cost or data limitations. Our investigation focuses on scenarios where predictions for only $B$ job sizes out of $n$ are available to the algorithm. We first establish near-optimal lower bounds and algorithms in the case of perfect predictions. Subsequently, we present a learning-augmented algorithm satisfying the robustness, consistency, and smoothness criteria, and revealing a novel tradeoff between consistency and smoothness inherent in the scenario with a restricted number of predictions",
    "checked": true,
    "id": "1df1f4808a8dc151f678e0a1f65222a05ba8c80f",
    "semantic_title": "non-clairvoyant scheduling with partial predictions",
    "citation_count": 4,
    "authors": [
      "Ziyad Benomar",
      "Vianney Perchet"
    ]
  },
  "https://proceedings.mlr.press/v235/berman24a.html": {
    "title": "Sequential Disentanglement by Extracting Static Information From A Single Sequence Element",
    "volume": "main",
    "abstract": "One of the fundamental representation learning tasks is unsupervised sequential disentanglement, where latent codes of inputs are decomposed to a single static factor and a sequence of dynamic factors. To extract this latent information, existing methods condition the static and dynamic codes on the entire input sequence. Unfortunately, these models often suffer from information leakage, i.e., the dynamic vectors encode both static and dynamic information, or vice versa, leading to a non-disentangled representation. Attempts to alleviate this problem via reducing the dynamic dimension and auxiliary loss terms gain only partial success. Instead, we propose a novel and simple architecture that mitigates information leakage by offering a simple and effective subtraction inductive bias while conditioning on a single sample. Remarkably, the resulting variational framework is simpler in terms of required loss terms, hyper-parameters, and data augmentation. We evaluate our method on multiple data-modality benchmarks including general time series, video, and audio, and we show beyond state-of-the-art results on generation and prediction tasks in comparison to several strong baselines",
    "checked": true,
    "id": "242344a5e543a645d5f761b0120f90be23c91b95",
    "semantic_title": "sequential disentanglement by extracting static information from a single sequence element",
    "citation_count": 2,
    "authors": [
      "Nimrod Berman",
      "Ilan Naiman",
      "Idan Arbiv",
      "Gal Fadlon",
      "Omri Azencot"
    ]
  },
  "https://proceedings.mlr.press/v235/berman24b.html": {
    "title": "CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations",
    "volume": "main",
    "abstract": "This work introduces reduced models based on Continuous Low Rank Adaptation (CoLoRA) that pre-train neural networks for a given partial differential equation and then continuously adapt low-rank weights in time to rapidly predict the evolution of solution fields at new physics parameters and new initial conditions. The adaptation can be either purely data-driven or via an equation-driven variational approach that provides Galerkin-optimal approximations. Because CoLoRA approximates solution fields locally in time, the rank of the weights can be kept small, which means that only few training trajectories are required offline so that CoLoRA is well suited for data-scarce regimes. Predictions with CoLoRA are orders of magnitude faster than with classical methods and their accuracy and parameter efficiency is higher compared to other neural network approaches",
    "checked": true,
    "id": "3f029640bad3c71ebb366b316b156c211f2090e5",
    "semantic_title": "colora: continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations",
    "citation_count": 7,
    "authors": [
      "Jules Berman",
      "Benjamin Peherstorfer"
    ]
  },
  "https://proceedings.mlr.press/v235/bertolotti24a.html": {
    "title": "By Tying Embeddings You Are Assuming the Distributional Hypothesis",
    "volume": "main",
    "abstract": "In this work, we analyze both theoretically and empirically the effect of tied input-output embeddings—a popular technique that reduces the model size while often improving training. Interestingly, we found that this technique is connected to Harris (1954)'s distributional hypothesis—often portrayed by the famous Firth (1957)'s quote \"a word is characterized by the company it keeps\". Specifically, our findings indicate that words (or, more broadly, symbols) with similar semantics tend to be encoded in similar input embeddings, while words that appear in similar contexts are encoded in similar output embeddings (thus explaining the semantic space arising in input and output embedding of foundational language models). As a consequence of these findings, the tying of the input and output embeddings is encouraged only when the distributional hypothesis holds for the underlying data. These results also provide insight into the embeddings of foundation language models (which are known to be semantically organized). Further, we complement the theoretical findings with several experiments supporting the claims",
    "checked": true,
    "id": "1d3fe01bfaae24428e900740bce7b73cb590ab5f",
    "semantic_title": "by tying embeddings you are assuming the distributional hypothesis",
    "citation_count": 0,
    "authors": [
      "Francesco Bertolotti",
      "Walter Cazzola"
    ]
  },
  "https://proceedings.mlr.press/v235/bettini24a.html": {
    "title": "Controlling Behavioral Diversity in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "The study of behavioral diversity in Multi-Agent Reinforcement Learning (MARL) is a nascent yet promising field. In this context, the present work deals with the question of how to control the diversity of a multi-agent system. With no existing approaches to control diversity to a set value, current solutions focus on blindly promoting it via intrinsic rewards or additional loss functions, effectively changing the learning objective and lacking a principled measure for it. To address this, we introduce Diversity Control (DiCo), a method able to control diversity to an exact value of a given metric by representing policies as the sum of a parameter-shared component and dynamically scaled per-agent components. By applying constraints directly to the policy architecture, DiCo leaves the learning objective unchanged, enabling its applicability to any actor-critic MARL algorithm. We theoretically prove that DiCo achieves the desired diversity, and we provide several experiments, both in cooperative and competitive tasks, that show how DiCo can be employed as a novel paradigm to increase performance and sample efficiency in MARL. Multimedia results are available on the paper's website: https://sites.google.com/view/dico-marl",
    "checked": true,
    "id": "35d56612c266518ccb64951e8bc91e26e7bea456",
    "semantic_title": "controlling behavioral diversity in multi-agent reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Matteo Bettini",
      "Ryan Kortvelesy",
      "Amanda Prorok"
    ]
  },
  "https://proceedings.mlr.press/v235/beukman24a.html": {
    "title": "Refining Minimax Regret for Unsupervised Environment Design",
    "volume": "main",
    "abstract": "In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there may be possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We further introduce an algorithm, ReMiDi, that results in a BLP policy at convergence. We empirically demonstrate that training on levels from a minimax regret adversary causes learning to prematurely stagnate, but that ReMiDi continues learning",
    "checked": true,
    "id": "2e1931dc2df81877bca8eeeafc1958c9b4536551",
    "semantic_title": "refining minimax regret for unsupervised environment design",
    "citation_count": 3,
    "authors": [
      "Michael Beukman",
      "Samuel Coward",
      "Michael Matthews",
      "Mattie Fellows",
      "Minqi Jiang",
      "Michael D Dennis",
      "Jakob Nicolaus Foerster"
    ]
  },
  "https://proceedings.mlr.press/v235/beurer-kellner24a.html": {
    "title": "Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation",
    "volume": "main",
    "abstract": "To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding methods propose to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods often incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\\times$ speedup over unconstrained decoding – thereby outperforming existing approaches by a wide margin. We release DOMINO as open source at https://github.com/eth-sri/domino",
    "checked": true,
    "id": "b95ca121a606da32180ab8bb0c0c58bf19b1499b",
    "semantic_title": "guiding llms the right way: fast, non-invasive constrained generation",
    "citation_count": 8,
    "authors": [
      "Luca Beurer-Kellner",
      "Marc Fischer",
      "Martin Vechev"
    ]
  },
  "https://proceedings.mlr.press/v235/beurer-kellner24b.html": {
    "title": "Prompt Sketching for Large Language Models",
    "volume": "main",
    "abstract": "Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially – first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders as part of https://github.com/eth-sri/lmql",
    "checked": true,
    "id": "c0de6d873e96cbf466164f477884cb882d4163fe",
    "semantic_title": "prompt sketching for large language models",
    "citation_count": 0,
    "authors": [
      "Luca Beurer-Kellner",
      "Mark Niklas Mueller",
      "Marc Fischer",
      "Martin Vechev"
    ]
  },
  "https://proceedings.mlr.press/v235/bewley24a.html": {
    "title": "Counterfactual Metarules for Local and Global Recourse",
    "volume": "main",
    "abstract": "We introduce T-CREx, a novel model-agnostic method for local and global counterfactual explanation (CE), which summarises recourse options for both individuals and groups in the form of generalised rules. It leverages tree-based surrogate models to learn the counterfactual rules, alongside metarules denoting their regimes of optimality, providing both a global analysis of model behaviour and diverse recourse options for users. Experiments indicate that T-CREx achieves superior aggregate performance over existing rule-based baselines on a range of CE desiderata, while being orders of magnitude faster to run",
    "checked": true,
    "id": "e76b99496d73c673e182293872957b7c3d3f7dec",
    "semantic_title": "counterfactual metarules for local and global recourse",
    "citation_count": 1,
    "authors": [
      "Tom Bewley",
      "Salim I. Amoukou",
      "Saumitra Mishra",
      "Daniele Magazzeni",
      "Manuela Veloso"
    ]
  },
  "https://proceedings.mlr.press/v235/beznosikov24a.html": {
    "title": "Sarah Frank-Wolfe: Methods for Constrained Optimization with Best Rates and Practical Features",
    "volume": "main",
    "abstract": "The Frank-Wolfe (FW) method is a popular approach for solving optimization problems with structured constraints that arise in machine learning applications. In recent years, stochastic versions of FW have gained popularity, motivated by large datasets for which the computation of the full gradient is prohibitively expensive. In this paper, we present two new variants of the FW algorithms for stochastic finite-sum minimization. Our algorithms have the best convergence guarantees of existing stochastic FW approaches for both convex and non-convex objective functions. Our methods do not have the issue of permanently collecting large batches, which is common to many stochastic projection-free approaches. Moreover, our second approach does not require either large batches or full deterministic gradients, which is a typical weakness of many techniques for finite-sum problems. The faster theoretical rates of our approaches are confirmed experimentally",
    "checked": true,
    "id": "0adb96554efb918bb232a5ae25c681335b67fcc4",
    "semantic_title": "sarah frank-wolfe: methods for constrained optimization with best rates and practical features",
    "citation_count": 3,
    "authors": [
      "Aleksandr Beznosikov",
      "David Dobre",
      "Gauthier Gidel"
    ]
  },
  "https://proceedings.mlr.press/v235/bharadhwaj24a.html": {
    "title": "Position: Scaling Simulation is Neither Necessary Nor Sufficient for In-the-Wild Robot Manipulation",
    "volume": "main",
    "abstract": "In this paper, we develop a structured critique of robotic simulations for real-world manipulation, by arguing that scaling simulators is neither necessary nor sufficient for making progress in general-purpose real-world robotic manipulation agents that are compliant with human preferences. With the ubiquity of robotic simulators, and recent efforts to scale them for diverse tasks, and at the same time the interest in generally capable real-world manipulation systems, we believe it is important to address the limitations of using simulation for real-world manipulation, so that as a community, we can focus our collective resources, energy, and time on approaches that have more principled odds of success. We further demonstrate the unique challenges that real-world manipulation presents, and show through examples and arguments why scaling simulation doesn't get us closer to solving these challenges required for diverse real-world deployment",
    "checked": true,
    "id": "3b0e5fc30fdd112c7084d2642234bfaea8ae5b95",
    "semantic_title": "position: scaling simulation is neither necessary nor sufficient for in-the-wild robot manipulation",
    "citation_count": 0,
    "authors": [
      "Homanga Bharadhwaj"
    ]
  },
  "https://proceedings.mlr.press/v235/bhattacharya24a.html": {
    "title": "Dynamic Facility Location in High Dimensional Euclidean Spaces",
    "volume": "main",
    "abstract": "We study the facility location problem in the dynamic setting, where the goal is to efficiently process an intermixed sequence of point insertions and deletions while maintaining a high quality and stable solution. Although the problem has been studied in the context of general metrics and low-dimensional spaces, much remains unknown concerning dynamic facility location in high dimensional spaces. In this work, we present the first fully dynamic algorithm for facility location in high-dimensional spaces $\\mathbb{R}^{d}$. For any $c \\geq 1$, our algorithm achieves $O(c)$-approximation, supports point updates in $\\tilde{O}(\\mathrm{poly}(d)n^{1/c + o(1)})$ amortized time and incurs $O(1)$ amortized recourse. More generally, our result shows that despite the linear-time lower bound on the update time for general metrics, it is possible to achieve sub-linear update times for metric spaces that admit dynamic nearest neighbour oracles. Experiments on real datasets confirm that our algorithm achieves high-quality solutions with low running time, and incurs minimal recourse",
    "checked": true,
    "id": "65342bf91a60f4291dbd1954b48611235b070722",
    "semantic_title": "dynamic facility location in high dimensional euclidean spaces",
    "citation_count": 1,
    "authors": [
      "Sayan Bhattacharya",
      "Gramoz Goranci",
      "Shaofeng H.-C. Jiang",
      "Yi Qian",
      "Yubo Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/bhattacharyya24a.html": {
    "title": "Total Variation Distance Meets Probabilistic Inference",
    "volume": "main",
    "abstract": "In this paper, we establish a novel connection between total variation (TV) distance estimation and probabilistic inference. In particular, we present an efficient, structure-preserving reduction from relative approximation of TV distance to probabilistic inference over directed graphical models. This reduction leads to a fully polynomial randomized approximation scheme (FPRAS) for estimating TV distances between same-structure distributions over any class of Bayes nets for which there is an efficient probabilistic inference algorithm. In particular, it leads to an FPRAS for estimating TV distances between distributions that are defined over a common Bayes net of small treewidth. Prior to this work, such approximation schemes only existed for estimating TV distances between product distributions. Our approach employs a new notion of partial couplings of high-dimensional distributions, which might be of independent interest",
    "checked": true,
    "id": "9670144c141ef3b70af5ad188bb017d4355cd6a8",
    "semantic_title": "total variation distance meets probabilistic inference",
    "citation_count": 1,
    "authors": [
      "Arnab Bhattacharyya",
      "Sutanu Gayen",
      "Kuldeep S. Meel",
      "Dimitrios Myrisiotis",
      "A. Pavan",
      "N. V. Vinodchandran"
    ]
  },
  "https://proceedings.mlr.press/v235/bhirangi24a.html": {
    "title": "Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling",
    "volume": "main",
    "abstract": "Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io",
    "checked": true,
    "id": "a0b8309c31731e9f7c3368247e73ca843d001d94",
    "semantic_title": "hierarchical state space models for continuous sequence-to-sequence modeling",
    "citation_count": 8,
    "authors": [
      "Raunaq Bhirangi",
      "Chenyu Wang",
      "Venkatesh Pattabiraman",
      "Carmel Majidi",
      "Abhinav Gupta",
      "Tess Hellebrekers",
      "Lerrel Pinto"
    ]
  },
  "https://proceedings.mlr.press/v235/bhowal24a.html": {
    "title": "Why do Variational Autoencoders Really Promote Disentanglement?",
    "volume": "main",
    "abstract": "Despite not being designed for this purpose, the use of variational autoencoders (VAEs) has proven remarkably effective for disentangled representation learning (DRL). Recent research attributes this success to certain characteristics of the loss function that prevent latent space rotation, or hypothesize about the orthogonality properties of the decoder by drawing parallels with principal component analysis (PCA). This hypothesis, however, has only been tested experimentally for linear VAEs, and the theoretical justification still remains an open problem. Moreover, since real-world VAEs are often inherently non-linear due to the use of neural architectures, understanding DRL capabilities of real-world VAEs remains a critical task. Our work takes a step towards understanding disentanglement in real-world VAEs to theoretically establish how the orthogonality properties of the decoder promotes disentanglement in practical applications. Complementary to our theoretical contributions, our experimental results corroborate our analysis. Code is available at https://github.com/criticalml-uw/Disentanglement-in-VAE",
    "checked": true,
    "id": "a8f39bc26d1db2532bbb2204ae26c085732b107d",
    "semantic_title": "why do variational autoencoders really promote disentanglement?",
    "citation_count": 2,
    "authors": [
      "Pratik Bhowal",
      "Achint Soni",
      "Sirisha Rambhatla"
    ]
  },
  "https://proceedings.mlr.press/v235/bhuyan24a.html": {
    "title": "Best of Both Worlds Guarantees for Smoothed Online Quadratic Optimization",
    "volume": "main",
    "abstract": "We study the smoothed online quadratic optimization (SOQO) problem where, at each round $t$, a player plays an action $x_t$ in response to a quadratic hitting cost and an additional squared $\\ell_2$-norm cost for switching actions. This problem class has strong connections to a wide range of application domains including smart grid management, adaptive control, and data center management, where switching-efficient algorithms are highly sought after. We study the SOQO problem in both adversarial and stochastic settings, and in this process, perform the first stochastic analysis of this class of problems. We provide the online optimal algorithm when the minimizers of the hitting cost function evolve as a general stochastic process, which, for the case of martingale process, takes the form of a distribution-agnostic dynamic interpolation algorithm that we call Lazy Adaptive Interpolation (LAI). Next, we present the stochastic-adversarial trade-off by proving an $\\Omega(T)$ expected regret for the adversarial optimal algorithm in the literature (ROBD) with respect to LAI and, a sub-optimal competitive ratio for LAI in the adversarial setting. Finally, we present a best-of-both-worlds algorithm that obtains a robust adversarial performance while simultaneously achieving a near-optimal stochastic performance",
    "checked": true,
    "id": "ae0a8e8522cb2f2557890c7acd938047da7a2bee",
    "semantic_title": "best of both worlds guarantees for smoothed online quadratic optimization",
    "citation_count": 1,
    "authors": [
      "Neelkamal Bhuyan",
      "Debankur Mukherjee",
      "Adam Wierman"
    ]
  },
  "https://proceedings.mlr.press/v235/bian24a.html": {
    "title": "Multi-Patch Prediction: Adapting Language Models for Time Series Representation Learning",
    "volume": "main",
    "abstract": "In this study, we present $\\text{aL\\small{LM}4T\\small{S}}$, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mastering temporal patch-based representations. $\\text{aL\\small{LM}4T\\small{S}}$ demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of LLMs for time-series analysis",
    "checked": true,
    "id": "2fafad06b457cdc7f132b78c06e5fbfb68bd16eb",
    "semantic_title": "multi-patch prediction: adapting language models for time series representation learning",
    "citation_count": 1,
    "authors": [
      "Yuxuan Bian",
      "Xuan Ju",
      "Jiangtong Li",
      "Zhijian Xu",
      "Dawei Cheng",
      "Qiang Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/bian24b.html": {
    "title": "Naive Bayes Classifiers over Missing Data: Decision and Poisoning",
    "volume": "main",
    "abstract": "We study the certifiable robustness of ML classifiers on dirty datasets that could contain missing values. A test point is certifiably robust for an ML classifier if the classifier returns the same prediction for that test point, regardless of which cleaned version (among exponentially many) of the dirty dataset the classifier is trained on. In this paper, we show theoretically that for Naive Bayes Classifiers (NBC) over dirty datasets with missing values: (i) there exists an efficient polynomial time algorithm to decide whether multiple input test points are all certifiably robust over a dirty dataset; and (ii) the data poisoning attack, which aims to make all input test points certifiably non-robust by inserting missing cells to the clean dataset, is in polynomial time for single test points but NP-complete for multiple test points. Extensive experiments demonstrate that our algorithms are efficient and outperform existing baselines",
    "checked": true,
    "id": "4fc8096945613737ddb756f44ee02eee789970a6",
    "semantic_title": "naive bayes classifiers over missing data: decision and poisoning",
    "citation_count": 2,
    "authors": [
      "Song Bian",
      "Xiating Ouyang",
      "Zhiwei Fan",
      "Paraschos Koutris"
    ]
  },
  "https://proceedings.mlr.press/v235/bianchi24a.html": {
    "title": "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis",
    "volume": "main",
    "abstract": "Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20% when negotiating against the standard GPT-4. We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans. Together, NegotiationArena offers a new environment to investigate LLM interactions, enabling new insights into LLM's theory of mind, irrationality, and reasoning abilities",
    "checked": true,
    "id": "b92d30286e7984a54cf1f537b4b72b2dd1b168a7",
    "semantic_title": "how well can llms negotiate? negotiationarena platform and analysis",
    "citation_count": 15,
    "authors": [
      "Federico Bianchi",
      "Patrick John Chia",
      "Mert Yuksekgonul",
      "Jacopo Tagliabue",
      "Dan Jurafsky",
      "James Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/bianchi24b.html": {
    "title": "Scalable Safe Policy Improvement for Factored Multi-Agent MDPs",
    "volume": "main",
    "abstract": "In this work, we focus on safe policy improvement in multi-agent domains where current state-of-the-art methods cannot be effectively applied because of large state and action spaces. We consider recent results using Monte Carlo Tree Search for Safe Policy Improvement with Baseline Bootstrapping and propose a novel algorithm that scales this approach to multi-agent domains, exploiting the factorization of the transition model and value function. Given a centralized behavior policy and a dataset of trajectories, our algorithm generates an improved policy by selecting joint actions using a novel extension of Max-Plus (or Variable Elimination) that constrains local actions to guarantee safety criteria. An empirical evaluation on multi-agent SysAdmin and multi-UAV Delivery shows that the approach scales to very large domains where state-of-the-art methods cannot work",
    "checked": true,
    "id": "59a8aaaab89517df4db77ccd92d13aac4a673174",
    "semantic_title": "scalable safe policy improvement for factored multi-agent mdps",
    "citation_count": 0,
    "authors": [
      "Federico Bianchi",
      "Edoardo Zorzi",
      "Alberto Castellini",
      "Thiago D. Simão",
      "Matthijs T. J. Spaan",
      "Alessandro Farinelli"
    ]
  },
  "https://proceedings.mlr.press/v235/bica24a.html": {
    "title": "Improving fine-grained understanding in image-text pre-training",
    "volume": "main",
    "abstract": "We introduce SPARse fine-grained Contrastive alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives, i.e., more detailed information is encoded in a computationally inexpensive way. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thoroughly evaluate SPARC and show improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g., retrieval, object detection, segmentation while also improving model faithfulness and captioning in foundational vision-language models",
    "checked": true,
    "id": "603242e88244aff1135ce6cb3baf53d2e95ed427",
    "semantic_title": "improving fine-grained understanding in image-text pre-training",
    "citation_count": 11,
    "authors": [
      "Ioana Bica",
      "Anastasija Ilic",
      "Matthias Bauer",
      "Goker Erdogan",
      "Matko Bošnjak",
      "Christos Kaplanis",
      "Alexey A. Gritsenko",
      "Matthias Minderer",
      "Charles Blundell",
      "Razvan Pascanu",
      "Jovana Mitrovic"
    ]
  },
  "https://proceedings.mlr.press/v235/biecek24a.html": {
    "title": "Position: Explain to Question not to Justify",
    "volume": "main",
    "abstract": "Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. We separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). This position paper argues that the area of RED XAI is currently under-explored, i.e., more methods for explainability are desperately needed to question models (e.g., extract knowledge from well-performing models as well as spotting and fixing bugs in faulty models), and the area of RED XAI hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Przemyslaw Biecek",
      "Wojciech Samek"
    ]
  },
  "https://proceedings.mlr.press/v235/bini24a.html": {
    "title": "ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections",
    "volume": "main",
    "abstract": "Parameter-efficient finetuning (PEFT) has become ubiquitous to adapt foundation models to downstream task requirements while retaining their generalization ability. However, the amount of additionally introduced parameters and compute for successful adaptation and hyperparameter searches can explode quickly, especially when deployed at scale to serve numerous individual requests. To ensure effective, parameter-efficient, and hyperparameter-robust adaptation, we propose the ETHER transformation family, which performs Efficient fineTuning via HypErplane Reflections. By design, ETHER transformations require a minimal number of parameters, are less likely to deteriorate model performance, and exhibit robustness to hyperparameter and learning rate choices. In particular, we introduce ETHER and its relaxation ETHER+, which match or outperform existing PEFT methods with significantly fewer parameters ($\\sim$$10$-$100$ times lower than LoRA or OFT) across multiple image synthesis and natural language tasks without exhaustive hyperparameter tuning. Finally, we investigate the recent emphasis on Hyperspherical Energy retention for adaptation and raise questions on its practical utility. The code is available at https://github.com/mwbini/ether",
    "checked": true,
    "id": "e6abda46720fd6923ddbd886ee3301b203f1be25",
    "semantic_title": "ether: efficient finetuning of large-scale models with hyperplane reflections",
    "citation_count": 1,
    "authors": [
      "Massimo Bini",
      "Karsten Roth",
      "Zeynep Akata",
      "Anna Khoreva"
    ]
  },
  "https://proceedings.mlr.press/v235/biparva24a.html": {
    "title": "Incorporating Information into Shapley Values: Reweighting via a Maximum Entropy Approach",
    "volume": "main",
    "abstract": "Both the marginal contributions needed for the computation of Shapley values and the graph produced by Pearl-Verma theorem rely on the choice of an ordering of the variables. For Shapley values, the marginal contributions are averaged over all orderings, while in causal inference methods, the typical approach is to select orderings producing a graph with a minimal number of edges. We reconcile both approaches by reinterpreting them from a maximum entropy perspective. Namely, Shapley values assume no prior knowledge about the orderings and treat them as equally likely, while causal inference approaches apply Occam's razor and consider only orderings producing the simplest explanatory graphs. We find that the blind application of Occam's razor to Shapley values does not produce fully satisfactory explanations. Hence, we propose two variations of Shapley values based on entropy maximization to appropriately incorporate prior information about the model. Hence, we propose a variation of Shapley values based on entropy maximization to appropriately incorporate prior information about the model",
    "checked": true,
    "id": "6fe0e2f55d0ed49b6be43f738fa777b810504df9",
    "semantic_title": "incorporating information into shapley values: reweighting via a maximum entropy approach",
    "citation_count": 0,
    "authors": [
      "Darya Biparva",
      "Donatello Materassi"
    ]
  },
  "https://proceedings.mlr.press/v235/blaauwbroek24a.html": {
    "title": "Graph2Tac: Online Representation Learning of Formal Math Concepts",
    "volume": "main",
    "abstract": "In proof assistants, the physical proximity between two formal mathematical concepts is a strong predictor of their mutual relevance. Furthermore, lemmas with close proximity regularly exhibit similar proof structures. We show that this locality property can be exploited through online learning techniques to obtain solving agents that far surpass offline learners when asked to prove theorems in an unseen mathematical setting. We extensively benchmark two such online solvers implemented in the Tactician platform for the Coq proof assistant: First, Tactician's online $k$-nearest neighbor solver, which can learn from recent proofs, shows a $1.72\\times$ improvement in theorems proved over an offline equivalent. Second, we introduce a graph neural network, Graph2Tac, with a novel approach to build hierarchical representations for new definitions. Graph2Tac's online definition task realizes a $1.5\\times$ improvement in theorems solved over an offline baseline. The $k$-NN and Graph2Tac solvers rely on orthogonal online data, making them highly complementary. Their combination improves $1.27\\times$ over their individual performances. Both solvers outperform all other general purpose provers for Coq, including CoqHammer, Proverbot9001, and a transformer baseline by at least $1.48\\times$ and are available for practical use by end-users",
    "checked": true,
    "id": "955420033e82248e16344a256b39e4030aea2da6",
    "semantic_title": "graph2tac: online representation learning of formal math concepts",
    "citation_count": 5,
    "authors": [
      "Lasse Blaauwbroek",
      "Mirek Olšák",
      "Jason Rute",
      "Fidel Ivan Schaposnik Massolo",
      "Jelle Piepenbrock",
      "Vasily Pestun"
    ]
  },
  "https://proceedings.mlr.press/v235/black24a.html": {
    "title": "Biharmonic Distance of Graphs and its Higher-Order Variants: Theoretical Properties with Applications to Centrality and Clustering",
    "volume": "main",
    "abstract": "Effective resistance is a distance between vertices of a graph that is both theoretically interesting and useful in applications. We study a variant of effective resistance called the biharmonic distance. While the effective resistance measures how well-connected two vertices are, we prove several theoretical results supporting the idea that the biharmonic distance measures how important an edge is to the global topology of the graph. Our theoretical results connect the biharmonic distance to well-known measures of connectivity of a graph like its total resistance and sparsity. Based on these results, we introduce two clustering algorithms using the biharmonic distance. Finally, we introduce a further generalization of the biharmonic distance that we call the $k$-harmonic distance. We empirically study the utility of biharmonic and $k$-harmonic distance for edge centrality and graph clustering",
    "checked": true,
    "id": "1c02b61bb37453ab5d3c4f956c824ff85a34691b",
    "semantic_title": "biharmonic distance of graphs and its higher-order variants: theoretical properties with applications to centrality and clustering",
    "citation_count": 0,
    "authors": [
      "Mitchell Black",
      "Lucy Lin",
      "Weng-Keen Wong",
      "Amir Nayyeri"
    ]
  },
  "https://proceedings.mlr.press/v235/black24b.html": {
    "title": "Comparing Graph Transformers via Positional Encodings",
    "volume": "main",
    "abstract": "The distinguishing power of graph transformers is tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., shortest-path distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in their ability to distinguish non-isomorphic graphs. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in terms of graph transformers. However, in the case of graphs with node features, we show that RPEs may have an advantage over APEs. Based on our theoretical results, we provide a study of different APEs and RPEs—including the shortest-path and resistance distance and the recently introduced stable and expressive positional encoding (SPE)—and compare their distinguishing power in terms of transformers. We believe our work will help navigate the vast number of positional encoding choices and provide guidance on the future design of positional encodings for graph transformers",
    "checked": true,
    "id": "08e8fafd5aa80eb118a5dab36fa27fe38a62c582",
    "semantic_title": "comparing graph transformers via positional encodings",
    "citation_count": 5,
    "authors": [
      "Mitchell Black",
      "Zhengchao Wan",
      "Gal Mishne",
      "Amir Nayyeri",
      "Yusu Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/blanchet24a.html": {
    "title": "Stability Evaluation through Distributional Perturbation Analysis",
    "volume": "main",
    "abstract": "The performance of learning models often deteriorates when deployed in out-of-sample environments. To ensure reliable deployment, we propose a stability evaluation criterion based on distributional perturbations. Conceptually, our stability evaluation criterion is defined as the minimal perturbation required on our observed dataset to induce a prescribed deterioration in risk evaluation. In this paper, we utilize the optimal transport (OT) discrepancy with moment constraints on the (sample, density) space to quantify this perturbation. Therefore, our stability evaluation criterion can address both data corruptions and sub-population shifts—the two most common types of distribution shifts in real-world scenarios. To further realize practical benefits, we present a series of tractable convex formulations and computational methods tailored to different classes of loss functions. The key technical tool to achieve this is the strong duality theorem provided in this paper. Empirically, we validate the practical utility of our stability evaluation criterion across a host of real-world applications. These empirical studies showcase the criterion's ability not only to compare the stability of different learning models and features but also to provide valuable guidelines and strategies to further improve models",
    "checked": true,
    "id": "f9a716347de56c7800c5f946ec8c209844f587c8",
    "semantic_title": "stability evaluation through distributional perturbation analysis",
    "citation_count": 0,
    "authors": [
      "Jose Blanchet",
      "Peng Cui",
      "Jiajin Li",
      "Jiashuo Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/bleistein24a.html": {
    "title": "Dynamic Survival Analysis with Controlled Latent States",
    "volume": "main",
    "abstract": "We consider the task of learning individual-specific intensities of counting processes from a set of static variables and irregularly sampled time series. We introduce a novel modelization approach in which the intensity is the solution to a controlled differential equation. We first design a neural estimator by building on neural controlled differential equations. In a second time, we show that our model can be linearized in the signature space under sufficient regularity conditions, yielding a signature-based estimator which we call CoxSig. We provide theoretical learning guarantees for both estimators, before showcasing the performance of our models on a vast array of simulated and real-world datasets from finance, predictive maintenance and food supply chain management",
    "checked": false,
    "id": "1b2bb1954927b52461993b72b47ac6b0be80a241",
    "semantic_title": "dynamical survival analysis with controlled latent states",
    "citation_count": 1,
    "authors": [
      "Linus Bleistein",
      "Van Tuan Nguyen",
      "Adeline Fermanian",
      "Agathe Guilloux"
    ]
  },
  "https://proceedings.mlr.press/v235/blessing24a.html": {
    "title": "Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling",
    "volume": "main",
    "abstract": "Monte Carlo methods, Variational Inference, and their combinations play a pivotal role in sampling from intractable probability distributions. However, current studies lack a unified evaluation framework, relying on disparate performance measures and limited method comparisons across diverse tasks, complicating the assessment of progress and hindering the decision-making of practitioners. In response to these challenges, our work introduces a benchmark that evaluates sampling methods using a standardized task suite and a broad range of performance criteria. Moreover, we study existing metrics for quantifying mode collapse and introduce novel metrics for this purpose. Our findings provide insights into strengths and weaknesses of existing sampling methods, serving as a valuable reference for future developments",
    "checked": true,
    "id": "ff9ab30cf541dec4ad7e135c603994ba33caca28",
    "semantic_title": "beyond elbos: a large-scale evaluation of variational methods for sampling",
    "citation_count": 6,
    "authors": [
      "Denis Blessing",
      "Xiaogang Jia",
      "Johannes Esslinger",
      "Francisco Vargas",
      "Gerhard Neumann"
    ]
  },
  "https://proceedings.mlr.press/v235/bok24a.html": {
    "title": "Shifted Interpolation for Differential Privacy",
    "volume": "main",
    "abstract": "Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the \"privacy amplification by iteration\" phenomenon in the unifying framework of $f$-differential privacy—which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\\varepsilon,\\delta)$-DP and Rényi DP. Our key technical insight is the construction of shifted interpolated processes that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first exact privacy analysis in the foundational setting of strongly convex optimization. Our techniques extend to many settings: convex/strongly convex, constrained/unconstrained, full/cyclic/stochastic batches, and all combinations thereof. As an immediate corollary, we recover the $f$-DP characterization of the exponential mechanism for strongly convex optimization in Gopi et al. (2022), and moreover extend this result to more general settings",
    "checked": true,
    "id": "6ba8972d91654cd11fdc919103f46aa462ce100c",
    "semantic_title": "shifted interpolation for differential privacy",
    "citation_count": 7,
    "authors": [
      "Jinho Bok",
      "Weijie J Su",
      "Jason Altschuler"
    ]
  },
  "https://proceedings.mlr.press/v235/bombari24a.html": {
    "title": "How Spurious Features are Memorized: Precise Analysis for Random and NTK Features",
    "volume": "main",
    "abstract": "Deep learning models are known to overfit and memorize spurious features in the training dataset. While numerous empirical studies have aimed at understanding this phenomenon, a rigorous theoretical framework to quantify it is still missing. In this paper, we consider spurious features that are uncorrelated with the learning task, and we provide a precise characterization of how they are memorized via two separate terms: (i) the stability of the model with respect to individual training samples, and (ii) the feature alignment between the spurious pattern and the full sample. While the first term is well established in learning theory and it is connected to the generalization error in classical work, the second one is, to the best of our knowledge, novel. Our key technical result gives a precise characterization of the feature alignment for the two prototypical settings of random features (RF) and neural tangent kernel (NTK) regression. We prove that the memorization of spurious features weakens as the generalization capability increases and, through the analysis of the feature alignment, we unveil the role of the model and of its activation function. Numerical experiments show the predictive power of our theory on standard datasets (MNIST, CIFAR-10)",
    "checked": true,
    "id": "8cbc260a6f78adedadcbf0ad0ac59ca408aad915",
    "semantic_title": "how spurious features are memorized: precise analysis for random and ntk features",
    "citation_count": 3,
    "authors": [
      "Simone Bombari",
      "Marco Mondelli"
    ]
  },
  "https://proceedings.mlr.press/v235/bombari24b.html": {
    "title": "Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features",
    "volume": "main",
    "abstract": "Understanding the reasons behind the exceptional success of transformers requires a better analysis of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. We then translate these results on the word sensitivity into generalization bounds: due to their low WS, random features provably cannot learn to distinguish between two sentences that differ only in a single word; in contrast, due to their high WS, random attention features have higher generalization capabilities. We validate our theoretical results with experimental evidence over the BERT-Base word embeddings of the imdb review dataset",
    "checked": true,
    "id": "3c537970f5dc1d7b75df15912d9668b06e1d3fd1",
    "semantic_title": "towards understanding the word sensitivity of attention layers: a study via random features",
    "citation_count": 3,
    "authors": [
      "Simone Bombari",
      "Marco Mondelli"
    ]
  },
  "https://proceedings.mlr.press/v235/bonel24a.html": {
    "title": "Position: Machine Learning-powered Assessments of the EU Digital Services Act Aid Quantify Policy Impacts on Online Harms",
    "volume": "main",
    "abstract": "While machine learning shows promise in automated knowledge generation, current techniques such as large language models and micro-targeted influence operations can be exploited for harmful purposes like the proliferation of disinformation. The European Union's Digital Services Act (DSA) is an exemplary policy response addressing these harms generated by online platforms. In this regard, it necessitates a comprehensive evaluation of its impact on curbing the harmful downstream effects of these opaque practices. Despite their harmful applications, we argue that machine learning techniques offer immense, yet under-exploited, potential for unraveling the impacts of regulations like the DSA. Following an analysis that reveals possible limitations in the DSA's provisions, we call for resolute efforts to address methodological barriers around appropriate data access, isolating marginal regulatory effects, and facilitating generalization across different contexts. Given the identified advantages of data-driven approaches to regulatory delivery, we advocate for machine learning research to help quantify the policy impacts on online harms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleonora Bonel",
      "Luca Nannini",
      "Davide Bassi",
      "Michele Joshua Maggini"
    ]
  },
  "https://proceedings.mlr.press/v235/bordelon24a.html": {
    "title": "A Dynamical Model of Neural Scaling Laws",
    "volume": "main",
    "abstract": "On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-width dynamics at a rate $1/\\text{width}$ but at late time exhibit a rate $\\text{width}^{-c}$, where $c$ depends on the structure of the architecture and task. We show that our model exhibits this behavior. Lastly, our theory shows how the gap between training and test loss can gradually build up over time due to repeated reuse of data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Blake Bordelon",
      "Alexander Atanasov",
      "Cengiz Pehlevan"
    ]
  },
  "https://proceedings.mlr.press/v235/boschi24a.html": {
    "title": "A New Computationally Efficient Algorithm to solve Feature Selection for Functional Data Classification in High-dimensional Spaces",
    "volume": "main",
    "abstract": "This paper introduces a novel methodology for Feature Selection for Functional Classification, FSFC, that addresses the challenge of jointly performing feature selection and classification of functional data in scenarios with categorical responses and multivariate longitudinal features. FSFC tackles a newly defined optimization problem that integrates logistic loss and functional features to identify the most crucial variables for classification. To address the minimization procedure, we employ functional principal components and develop a new adaptive version of the Dual Augmented Lagrangian algorithm. The computational efficiency of FSFC enables handling high-dimensional scenarios where the number of features may considerably exceed the number of statistical units. Simulation experiments demonstrate that FSFC outperforms other machine learning and deep learning methods in computational time and classification accuracy. Furthermore, the FSFC feature selection capability can be leveraged to significantly reduce the problem's dimensionality and enhance the performances of other classification algorithms. The efficacy of FSFC is also demonstrated through a real data application, analyzing relationships between four chronic diseases and other health and demographic factors. FSFC source code is publicly available at https://github.com/IBM/funGCN",
    "checked": true,
    "id": "44f21bcd0e031d94325b67a47c5952b3b822e4fa",
    "semantic_title": "a new computationally efficient algorithm to solve feature selection for functional data classification in high-dimensional spaces",
    "citation_count": 3,
    "authors": [
      "Tobia Boschi",
      "Francesca Bonin",
      "Rodrigo Ordonez-Hurtado",
      "Alessandra Pascale",
      "Jonathan P Epperlein"
    ]
  },
  "https://proceedings.mlr.press/v235/bouchard24a.html": {
    "title": "Random matrix theory improved Fréchet mean of symmetric positive definite matrices",
    "volume": "main",
    "abstract": "In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fréchet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory based method that estimates Fréchet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florent Bouchard",
      "Ammar Mian",
      "Malik Tiomoko",
      "Guillaume Ginolhac",
      "Frederic Pascal"
    ]
  },
  "https://proceedings.mlr.press/v235/bouchiat24a.html": {
    "title": "Improving Neural Additive Models with Bayesian Principles",
    "volume": "main",
    "abstract": "Neural additive models (NAMs) enhance the transparency of deep neural networks by handling input features in separate additive sub-networks. However, they lack inherent mechanisms that provide calibrated uncertainties and enable selection of relevant features and interactions. Approaching NAMs from a Bayesian perspective, we augment them in three primary ways, namely by a) providing credible intervals for the individual additive sub-networks; b) estimating the marginal likelihood to perform an implicit selection of features via an empirical Bayes procedure; and c) facilitating the ranking of feature pairs as candidates for second-order interaction in fine-tuned models. In particular, we develop Laplace-approximated NAMs (LA-NAMs), which show improved empirical performance on tabular datasets and challenging real-world medical tasks",
    "checked": true,
    "id": "0fe8eeec26610ae47df9804ddfe36114c953a592",
    "semantic_title": "improving neural additive models with bayesian principles",
    "citation_count": 3,
    "authors": [
      "Kouroche Bouchiat",
      "Alexander Immer",
      "Hugo Yèche",
      "Gunnar Ratsch",
      "Vincent Fortuin"
    ]
  },
  "https://proceedings.mlr.press/v235/bounoua24a.html": {
    "title": "S$Ω$I: Score-based O-INFORMATION Estimation",
    "volume": "main",
    "abstract": "The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is O-information, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce S$\\Omega$I, which allows to compute O-information without restrictive assumptions about the system while leveraging a unique model. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of S$\\Omega$I in the context of a real-world use case",
    "checked": false,
    "id": "e959b547aad7777891b67a2863e39e9baf299f88",
    "semantic_title": "sωi: score-based o-information estimation",
    "citation_count": 0,
    "authors": [
      "Mustapha Bounoua",
      "Giulio Franzese",
      "Pietro Michiardi"
    ]
  },
  "https://proceedings.mlr.press/v235/bravo24a.html": {
    "title": "On dimensionality of feature vectors in MPNNs",
    "volume": "main",
    "abstract": "We revisit the result of Morris et al. (AAAI'19) that message-passing graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler–Leman (WL) isomorphism test. Morris et al. show their result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the size of the graph. Recently, by introducing randomness into the architecture, Aamand et al. (NeurIPS'22) improved this bound to $O(\\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect simulation only with high probability. In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs. However, architectures used in practice have feature vectors of constant dimension. Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice. In this paper we close this gap by showing that, for any non-polynomial analytic (like the sigmoid) activation function, to guarantee that MPNNs are equivalent to the WL test, feature vectors of dimension $d=1$ is all we need, independently of the size of the graphs. Our main technical insight is that for simulating multi-sets in the WL-test, it is enough to use linear independence of feature vectors over rationals instead of reals. Countability of the set of rationals together with nice properties of analytic functions allow us to carry out the simulation invariant over the iterations of the WL test without increasing the dimension of the feature vectors",
    "checked": true,
    "id": "6da35d3c112485a9ca4cef3dc3967dbdc4da89f5",
    "semantic_title": "on dimensionality of feature vectors in mpnns",
    "citation_count": 5,
    "authors": [
      "César Bravo",
      "Alexander Kozachinskiy",
      "Cristobal Rojas"
    ]
  },
  "https://proceedings.mlr.press/v235/brenner24a.html": {
    "title": "Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics",
    "volume": "main",
    "abstract": "Many, if not most, systems of interest in science are naturally described as nonlinear dynamical systems. Empirically, we commonly access these systems through time series measurements. Often such time series may consist of discrete random variables rather than continuous measurements, or may be composed of measurements from multiple data modalities observed simultaneously. For instance, in neuroscience we may have behavioral labels in addition to spike counts and continuous physiological recordings. While by now there is a burgeoning literature on deep learning for dynamical systems reconstruction (DSR), multimodal data integration has hardly been considered in this context. Here we provide such an efficient and flexible algorithmic framework that rests on a multimodal variational autoencoder for generating a sparse teacher signal that guides training of a reconstruction model, exploiting recent advances in DSR training techniques. It enables to combine various sources of information for optimal reconstruction, even allows for reconstruction from symbolic data (class labels) alone, and connects different types of observations within a common latent dynamics space. In contrast to previous multimodal data integration techniques for scientific applications, our framework is fully generative, producing, after training, trajectories with the same geometrical and temporal structure as those of the ground truth system",
    "checked": true,
    "id": "49dabd925e020e28ac33fdcb41b8b0961f2f40ff",
    "semantic_title": "integrating multimodal data for joint generative modeling of complex dynamics",
    "citation_count": 8,
    "authors": [
      "Manuel Brenner",
      "Florian Hess",
      "Georgia Koppe",
      "Daniel Durstewitz"
    ]
  },
  "https://proceedings.mlr.press/v235/bressan24a.html": {
    "title": "Fully-Dynamic Approximate Decision Trees With Worst-Case Update Time Guarantees",
    "volume": "main",
    "abstract": "We study the problem of maintaining a decision tree in the fully-dynamic setting, where the dataset is updated by an adversarial sequence of insertions and deletions. We present the first algorithm with strong guarantees on both the quality of the tree and the worst-case update time (the maximum time spent between two consecutive dataset updates). For instance, we can maintain a tree where each node has Gini gain within $\\beta$ of the optimum, while guaranteeing an update time $O(d \\beta^{-3} \\log^4 n )$, where $d$ is the number of features and $n$ the maximum size of the dataset. This is optimal up to polylogarithmic factors, as any dynamic algorithm must have update time in $\\Omega(d)$. Similar guarantees hold for the variance and information gain, for classification and regression, and even for boosted trees. This shows that many popular decision trees such as ID3 or C4.5 can be efficiently be made dynamic, answering an open question of Bressan, Damay and Sozio (AAAI 2023). We also show that, under the 3SUM conjecture or the Orthogonal Vectors Hypothesis, the update time must be polynomial in $1/\\beta$",
    "checked": true,
    "id": "40164ee5748e99f3d8523bb6a15c20deaa56bf65",
    "semantic_title": "fully-dynamic approximate decision trees with worst-case update time guarantees",
    "citation_count": 0,
    "authors": [
      "Marco Bressan",
      "Mauro Sozio"
    ]
  },
  "https://proceedings.mlr.press/v235/brilliantov24a.html": {
    "title": "Applying language models to algebraic topology: generating simplicial cycles using multi-labeling in Wu's formula",
    "volume": "main",
    "abstract": "Computing homotopy groups of spheres has long been a fundamental objective in algebraic topology. Various theoretical and algorithmic approaches have been developed to tackle this problem. In this paper we take a step towards the goal of comprehending the group-theoretic structure of the generators of these homotopy groups by leveraging the power of machine learning. Specifically, in the simplicial group setting of Wu's formula, we reformulate the problem of generating simplicial cycles as a problem of sampling from the intersection of algorithmic datasets related to Dyck languages. We present and evaluate language modelling approaches that employ multi-label information for input sequences, along with the necessary group-theoretic toolkit and non-neural baselines",
    "checked": true,
    "id": "b7bc5f258bf271e8635002a548acd354d199cc03",
    "semantic_title": "applying language models to algebraic topology: generating simplicial cycles using multi-labeling in wu's formula",
    "citation_count": 0,
    "authors": [
      "Kirill Brilliantov",
      "Fedor Pavutnitskiy",
      "Dmitry Pasechnyuk",
      "German Magai"
    ]
  },
  "https://proceedings.mlr.press/v235/brown24a.html": {
    "title": "Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation",
    "volume": "main",
    "abstract": "We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss. Under modest assumptions on the input, we characterize the distribution of the iterate at each time step. Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data. This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023). Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set. We validate our theorems through experiments on synthetic data",
    "checked": true,
    "id": "3e80d492e909d1bed8fd02270f9addc86ab29f48",
    "semantic_title": "private gradient descent for linear regression: tighter error bounds and instance-specific uncertainty estimation",
    "citation_count": 2,
    "authors": [
      "Gavin R Brown",
      "Krishnamurthy Dj Dvijotham",
      "Georgina Evans",
      "Daogao Liu",
      "Adam Smith",
      "Abhradeep Guha Thakurta"
    ]
  },
  "https://proceedings.mlr.press/v235/brown-cohen24a.html": {
    "title": "Scalable AI Safety via Doubly-Efficient Debate",
    "volume": "main",
    "abstract": "The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al (2018). proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate deterministic AI systems for an exponential number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps",
    "checked": true,
    "id": "50d1eeb8678a267d4759bd7418457998c0135d90",
    "semantic_title": "scalable ai safety via doubly-efficient debate",
    "citation_count": 7,
    "authors": [
      "Jonah Brown-Cohen",
      "Geoffrey Irving",
      "Georgios Piliouras"
    ]
  },
  "https://proceedings.mlr.press/v235/bruce24a.html": {
    "title": "Genie: Generative Interactive Environments",
    "volume": "main",
    "abstract": "We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future",
    "checked": true,
    "id": "c064de2c71ebc5cf05493f49dc312b033c36b3b9",
    "semantic_title": "genie: generative interactive environments",
    "citation_count": 66,
    "authors": [
      "Jake Bruce",
      "Michael D Dennis",
      "Ashley Edwards",
      "Jack Parker-Holder",
      "Yuge Shi",
      "Edward Hughes",
      "Matthew Lai",
      "Aditi Mavalankar",
      "Richie Steigerwald",
      "Chris Apps",
      "Yusuf Aytar",
      "Sarah Maria Elisabeth Bechtle",
      "Feryal Behbahani",
      "Stephanie C.Y. Chan",
      "Nicolas Heess",
      "Lucy Gonzalez",
      "Simon Osindero",
      "Sherjil Ozair",
      "Scott Reed",
      "Jingwei Zhang",
      "Konrad Zolna",
      "Jeff Clune",
      "Nando De Freitas",
      "Satinder Singh",
      "Tim Rocktäschel"
    ]
  },
  "https://proceedings.mlr.press/v235/bryutkin24a.html": {
    "title": "HAMLET: Graph Transformer Neural Operator for Partial Differential Equations",
    "volume": "main",
    "abstract": "We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs",
    "checked": true,
    "id": "3a6d0954edf0c8e7bb09a7f36706dbb318614e6f",
    "semantic_title": "hamlet: graph transformer neural operator for partial differential equations",
    "citation_count": 3,
    "authors": [
      "Andrey Bryutkin",
      "Jiahao Huang",
      "Zhongying Deng",
      "Guang Yang",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://proceedings.mlr.press/v235/bu24a.html": {
    "title": "Provably Neural Active Learning Succeeds via Prioritizing Perplexing Samples",
    "volume": "main",
    "abstract": "Neural Network-based active learning (NAL) is a cost-effective data selection technique that utilizes neural networks to select and train on a small subset of samples. While existing work successfully develops various effective or theory-justified NAL algorithms, the understanding of the two commonly used query criteria of NAL: uncertainty-based and diversity-based, remains in its infancy. In this work, we try to move one step forward by offering a unified explanation for the success of both query criteria-based NAL from a feature learning view. Specifically, we consider a feature-noise data model comprising easy-to-learn or hard-to-learn features disrupted by noise, and conduct analysis over 2-layer NN-based NALs in the pool-based scenario. We provably show that both uncertainty-based and diversity-based NAL are inherently amenable to one and the same principle, i.e., striving to prioritize samples that contain yet-to-be-learned features. We further prove that this shared principle is the key to their success-achieve small test error within a small labeled set. Contrastingly, the strategy-free passive learning exhibits a large test error due to the inadequate learning of yet-to-be-learned features, necessitating resort to a significantly larger label complexity for a sufficient test error reduction. Experimental results validate our findings",
    "checked": true,
    "id": "cc4d9d2149b355c73ee2c69b9b4efe6b3f3be883",
    "semantic_title": "provably neural active learning succeeds via prioritizing perplexing samples",
    "citation_count": 1,
    "authors": [
      "Dake Bu",
      "Wei Huang",
      "Taiji Suzuki",
      "Ji Cheng",
      "Qingfu Zhang",
      "Zhiqiang Xu",
      "Hau-San Wong"
    ]
  },
  "https://proceedings.mlr.press/v235/bu24b.html": {
    "title": "Tackling Prevalent Conditions in Unsupervised Combinatorial Optimization: Cardinality, Minimum, Covering, and More",
    "volume": "main",
    "abstract": "Combinatorial optimization (CO) is naturally discrete, making machine-learning techniques based on differentiable optimization inapplicable. Karalias & Loukas (2020) adapted the probabilistic method by Erdős & Spencer (1974), to incorporate CO into differentiable optimization. Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization. However, each component confronts unique challenges. First, deriving objectives under complex conditions and constraints is nontrivial. Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding. In this work, we aim to tackle complex conditions in unsupervised CO. First, we concretize the targets for probabilistic objective construction and derandomization with theoretical justification. Then, for various complex conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets. Finally, we apply the derivations to various CO problems. Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t. both optimization quality and speed",
    "checked": true,
    "id": "f7daa82747a13314c42a1df0c8eb2d4958ac27d6",
    "semantic_title": "tackling prevalent conditions in unsupervised combinatorial optimization: cardinality, minimum, covering, and more",
    "citation_count": 2,
    "authors": [
      "Fanchen Bu",
      "Hyeonsoo Jo",
      "Soo Yong Lee",
      "Sungsoo Ahn",
      "Kijung Shin"
    ]
  },
  "https://proceedings.mlr.press/v235/bu24c.html": {
    "title": "Differentially Private Bias-Term Fine-tuning of Foundation Models",
    "volume": "main",
    "abstract": "We study the problem of differentially private (DP) fine-tuning of large pre-trained models — a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the efficiency of the standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture), parameter efficient (only training about 0.1% of the parameters), and computation efficient (almost removing the overhead caused by DP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiT is 2 - 30X faster and uses 2 - 8X less memory than DP full fine-tuning, even faster than the standard full fine-tuning. This amazing efficiency enables us to conduct DP fine-tuning on language and vision tasks with long-sequence texts and high-resolution images, which were computationally difficult using existing methods",
    "checked": true,
    "id": "573234fca785a474154b4b54e9c0914647cc8563",
    "semantic_title": "differentially private bias-term fine-tuning of foundation models",
    "citation_count": 42,
    "authors": [
      "Zhiqi Bu",
      "Yu-Xiang Wang",
      "Sheng Zha",
      "George Karypis"
    ]
  },
  "https://proceedings.mlr.press/v235/buathong24a.html": {
    "title": "Bayesian Optimization of Function Networks with Partial Evaluations",
    "volume": "main",
    "abstract": "Bayesian optimization is a powerful framework for optimizing functions that are expensive or time-consuming to evaluate. Recent work has considered Bayesian optimization of function networks (BOFN), where the objective function is given by a network of functions, each taking as input the output of previous nodes in the network as well as additional parameters. Leveraging this network structure has been shown to yield significant performance improvements. Existing BOFN algorithms for general-purpose networks evaluate the full network at each iteration. However, many real-world applications allow for evaluating nodes individually. To exploit this, we propose a novel knowledge gradient acquisition function that chooses which node and corresponding inputs to evaluate in a cost-aware manner, thereby reducing query costs by evaluating only on a part of the network at each step. We provide an efficient approach to optimizing our acquisition function and show that it outperforms existing BOFN methods and other benchmarks across several synthetic and real-world problems. Our acquisition function is the first to enable cost-aware optimization of a broad class of function networks",
    "checked": true,
    "id": "0b14fe3d669111919332d1fdf5ff9752a90c0b36",
    "semantic_title": "bayesian optimization of function networks with partial evaluations",
    "citation_count": 2,
    "authors": [
      "Poompol Buathong",
      "Jiayue Wan",
      "Raul Astudillo",
      "Sam Daulton",
      "Maximilian Balandat",
      "Peter I. Frazier"
    ]
  },
  "https://proceedings.mlr.press/v235/buchholz24a.html": {
    "title": "Robustness of Nonlinear Representation Learning",
    "volume": "main",
    "abstract": "We study the problem of unsupervised representation learning in slightly misspecified settings, and thus formalize the study of robustness of nonlinear representation learning. We focus on the case where the mixing is close to a local isometry in a suitable distance and show based on existing rigidity results that the mixing can be identified up to linear transformations and small errors. In a second step, we investigate Independent Component Analysis (ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an invertible mixing matrix and $h$ a small perturbation. We show that we can approximately recover the matrix $A$ and the independent components. Together, these two results show approximate identifiability of nonlinear ICA with almost isometric mixing functions. Those results are a step towards identifiability results for unsupervised representation learning for real-world data that do not follow restrictive model classes",
    "checked": true,
    "id": "f2d5331114188860db08b940a774a4fcc2093f1f",
    "semantic_title": "robustness of nonlinear representation learning",
    "citation_count": 0,
    "authors": [
      "Simon Buchholz",
      "Bernhard Schölkopf"
    ]
  },
  "https://proceedings.mlr.press/v235/bui24a.html": {
    "title": "Density-Softmax: Efficient Test-time Model for Uncertainty Estimation and Robustness under Distribution Shifts",
    "volume": "main",
    "abstract": "Sampling-based methods, e.g., Deep Ensembles and Bayesian Neural Nets have become promising approaches to improve the quality of uncertainty estimation and robust generalization. However, they suffer from a large model size and high latency at test time, which limits the scalability needed for low-resource devices and real-time applications. To resolve these computational issues, we propose Density-Softmax, a sampling-free deterministic framework via combining a density function built on a Lipschitz-constrained feature extractor with the softmax layer. Theoretically, we show that our model is the solution of minimax uncertainty risk and is distance-aware on feature space, thus reducing the over-confidence of the standard softmax under distribution shifts. Empirically, our method enjoys competitive results with state-of-the-art techniques in terms of uncertainty and robustness, while having a lower number of model parameters and a lower latency at test time",
    "checked": true,
    "id": "69640b4f08dbac192d1e2643a007fb48b88069d5",
    "semantic_title": "density-softmax: efficient test-time model for uncertainty estimation and robustness under distribution shifts",
    "citation_count": 4,
    "authors": [
      "Ha Manh Bui",
      "Anqi Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/bui24b.html": {
    "title": "Explaining Graph Neural Networks via Structure-aware Interaction Index",
    "volume": "main",
    "abstract": "The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods",
    "checked": true,
    "id": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
    "semantic_title": "explaining graph neural networks via structure-aware interaction index",
    "citation_count": 1,
    "authors": [
      "Ngoc Bui",
      "Hieu Trung Nguyen",
      "Viet Anh Nguyen",
      "Rex Ying"
    ]
  },
  "https://proceedings.mlr.press/v235/bulian24a.html": {
    "title": "Assessing Large Language Models on Climate Information",
    "volume": "main",
    "abstract": "As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication",
    "checked": true,
    "id": "aec95e6330033e0ec39fb5a069d647288c03b945",
    "semantic_title": "assessing large language models on climate information",
    "citation_count": 10,
    "authors": [
      "Jannis Bulian",
      "Mike S. Schäfer",
      "Afra Amini",
      "Heidi Lam",
      "Massimiliano Ciaramita",
      "Ben Gaiarin",
      "Michelle Chen Huebscher",
      "Christian Buck",
      "Niels G. Mede",
      "Markus Leippold",
      "Nadine Strauss"
    ]
  },
  "https://proceedings.mlr.press/v235/burns24a.html": {
    "title": "Semantically-correlated memories in a dense associative model",
    "volume": "main",
    "abstract": "I introduce a novel associative memory model named Correlated Dense Associative Memory (CDAM), which integrates both auto- and hetero-association in a unified framework for continuous-valued memory patterns. Employing an arbitrary graph structure to semantically link memory patterns, CDAM is theoretically and numerically analysed, revealing four distinct dynamical modes: auto-association, narrow hetero-association, wide hetero-association, and neutral quiescence. Drawing inspiration from inhibitory modulation studies, I employ anti-Hebbian learning rules to control the range of hetero-association, extract multi-scale representations of community structures in graphs, and stabilise the recall of temporal sequences. Experimental demonstrations showcase CDAM's efficacy in handling real-world data, replicating a classical neuroscience experiment, performing image retrieval, and simulating arbitrary finite automata",
    "checked": true,
    "id": "b44e41fcaaead1b50f20fda0c78d395b51002f7f",
    "semantic_title": "semantically-correlated memories in a dense associative model",
    "citation_count": 5,
    "authors": [
      "Thomas F Burns"
    ]
  },
  "https://proceedings.mlr.press/v235/burns24b.html": {
    "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
    "volume": "main",
    "abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models",
    "checked": true,
    "id": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
    "semantic_title": "weak-to-strong generalization: eliciting strong capabilities with weak supervision",
    "citation_count": 180,
    "authors": [
      "Collin Burns",
      "Pavel Izmailov",
      "Jan Hendrik Kirchner",
      "Bowen Baker",
      "Leo Gao",
      "Leopold Aschenbrenner",
      "Yining Chen",
      "Adrien Ecoffet",
      "Manas Joglekar",
      "Jan Leike",
      "Ilya Sutskever",
      "Jeffrey Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/butt24a.html": {
    "title": "CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay",
    "volume": "main",
    "abstract": "Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach the ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the program output given input) to the output actually produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-symbolic approach that scales to the full ARC evaluation dataset. Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art performance and outperforming existing neural and symbolic baselines. Our code is available at https://github.com/Qualcomm-AI-research/codeit",
    "checked": true,
    "id": "0de191d3d94927ab4c68b7fa746ccbe0b120fb7f",
    "semantic_title": "codeit: self-improving language models with prioritized hindsight replay",
    "citation_count": 13,
    "authors": [
      "Natasha Butt",
      "Blazej Manczak",
      "Auke Wiggers",
      "Corrado Rainone",
      "David W. Zhang",
      "Michaël Defferrard",
      "Taco Cohen"
    ]
  },
  "https://proceedings.mlr.press/v235/buzaglo24a.html": {
    "title": "How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers",
    "volume": "main",
    "abstract": "A main theoretical puzzle is why over-parameterized Neural Networks (NNs) generalize well when trained to zero loss (i.e., so they interpolate the data). Usually, the NN is trained with Stochastic Gradient Descent (SGD) or one of its variants. However, recent empirical work examined the generalization of a random NN that interpolates the data: the NN was sampled from a seemingly uniform prior over the parameters, conditioned on that the NN perfectly classifying the training set. Interestingly, such a NN sample typically generalized as well as SGD-trained NNs. We prove that such a random NN interpolator typically generalizes well if there exists an underlying narrow \"teacher NN\" that agrees with the labels. Specifically, we show that such a ‘flat' prior over the NN parametrization induces a rich prior over the NN functions, due to the redundancy in the NN structure. In particular, this creates a bias towards simpler functions, which require less relevant parameters to represent — enabling learning with a sample complexity approximately proportional to the complexity of the teacher (roughly, the number of non-redundant parameters), rather than the student's",
    "checked": true,
    "id": "2290956f27d5e90245db99268ee559ffa26fcf3d",
    "semantic_title": "how uniform random weights induce non-uniform bias: typical interpolating neural networks generalize with narrow teachers",
    "citation_count": 0,
    "authors": [
      "Gon Buzaglo",
      "Itamar Harel",
      "Mor Shpigel Nacson",
      "Alon Brutzkus",
      "Nathan Srebro",
      "Daniel Soudry"
    ]
  },
  "https://proceedings.mlr.press/v235/byambadalai24a.html": {
    "title": "Estimating Distributional Treatment Effects in Randomized Experiments: Machine Learning for Variance Reduction",
    "volume": "main",
    "abstract": "We propose a novel regression adjustment method designed for estimating distributional treatment effect parameters in randomized experiments. Randomized experiments have been extensively used to estimate treatment effects in various scientific fields. However, to gain deeper insights, it is essential to estimate distributional treatment effects rather than relying solely on average effects. Our approach incorporates pre-treatment covariates into a distributional regression framework, utilizing machine learning techniques to improve the precision of distributional treatment effect estimators. The proposed approach can be readily implemented with off-the-shelf machine learning methods and remains valid as long as the nuisance components are reasonably well estimated. Also, we establish the asymptotic properties of the proposed estimator and present a uniformly valid inference method. Through simulation results and real data analysis, we demonstrate the effectiveness of integrating machine learning techniques in reducing the variance of distributional treatment effect estimators in finite samples",
    "checked": true,
    "id": "c69c4d392caece545f1252bdc135f6d7e41f0ef5",
    "semantic_title": "estimating distributional treatment effects in randomized experiments: machine learning for variance reduction",
    "citation_count": 0,
    "authors": [
      "Undral Byambadalai",
      "Tatsushi Oka",
      "Shota Yasui"
    ]
  },
  "https://proceedings.mlr.press/v235/cabannes24a.html": {
    "title": "Learning Associative Memories with Gradient Descent",
    "volume": "main",
    "abstract": "This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings. We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings. Through theory and experiments, we provide several insights. In overparameterized regimes, we obtain logarithmic growth of the \"classification margins.\" Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes. The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence. We also find that underparameterized regimes lead to suboptimal memorization schemes. Finally, we assess the validity of our findings on small Transformer models",
    "checked": true,
    "id": "cbecf4a9370587c6d4210ab339749d870b432a6b",
    "semantic_title": "learning associative memories with gradient descent",
    "citation_count": 5,
    "authors": [
      "Vivien Cabannes",
      "Berfin Simsek",
      "Alberto Bietti"
    ]
  },
  "https://proceedings.mlr.press/v235/cachet24a.html": {
    "title": "Bridging Environments and Language with Rendering Functions and Vision-Language Models",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) have tremendous potential for grounding language, and thus enabling language-conditioned agents (LCAs) to perform diverse tasks specified with text. This has motivated the study of LCAs based on reinforcement learning (RL) with rewards given by rendering images of an environment and evaluating those images with VLMs. If single-task RL is employed, such approaches are limited by the cost and time required to train a policy for each new task. Multi-task RL (MTRL) is a natural alternative, but requires a carefully designed corpus of training tasks and does not always generalize reliably to new tasks. Therefore, this paper introduces a novel decomposition of the problem of building an LCA: first find an environment configuration that has a high VLM score for text describing a task; then use a (pretrained) goal-conditioned policy to reach that configuration. We also explore several enhancements to the speed and quality of VLM-based LCAs, notably, the use of distilled models, and the evaluation of configurations from multiple viewpoints to resolve the ambiguities inherent in a single 2D view. We demonstrate our approach on the Humanoid environment, showing that it results in LCAs that outperform MTRL baselines in zero-shot generalization, without requiring any textual task descriptions or other forms of environment-specific annotation during training",
    "checked": true,
    "id": "2c8e9873122cefab6e12802e0c5348583637c813",
    "semantic_title": "bridging environments and language with rendering functions and vision-language models",
    "citation_count": 0,
    "authors": [
      "Theo Cachet",
      "Christopher R Dance",
      "Olivier Sigaud"
    ]
  },
  "https://proceedings.mlr.press/v235/cai24a.html": {
    "title": "Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions",
    "volume": "main",
    "abstract": "In recent years, deep learning-based sequence modelings, such as language models, have received much attention and success, which pushes researchers to explore the possibility of transforming non-sequential problems into a sequential form. Following this thought, deep neural networks can be represented as composite functions of a sequence of mappings, linear or nonlinear, where each composition can be viewed as a word. However, the weights of linear mappings are undetermined and hence require an infinite number of words. In this article, we investigate the finite case and constructively prove the existence of a finite vocabulary $V$=$\\phi_i: \\mathbb{R}^d \\to \\mathbb{R}^d | i=1,...,n$ with $n=O(d^2)$ for the universal approximation. That is, for any continuous mapping $f: \\mathbb{R}^d \\to \\mathbb{R}^d$, compact domain $\\Omega$ and $\\varepsilon>0$, there is a sequence of mappings $\\phi_{i_1}, ..., \\phi_{i_m} \\in V, m \\in \\mathbb{Z}^+$, such that the composition $\\phi_{i_m} \\circ ... \\circ \\phi_{i_1} $ approximates $f$ on $\\Omega$ with an error less than $\\varepsilon$. Our results demonstrate an unusual approximation power of mapping compositions and motivate a novel compositional model for regular languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqiang Cai"
    ]
  },
  "https://proceedings.mlr.press/v235/cai24b.html": {
    "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the model's capabilities. Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2$\\times$ speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-2.8$\\times$",
    "checked": true,
    "id": "57e7af0b69325fafb371ef5d502e39ef9c90ef7e",
    "semantic_title": "medusa: simple llm inference acceleration framework with multiple decoding heads",
    "citation_count": 149,
    "authors": [
      "Tianle Cai",
      "Yuhong Li",
      "Zhengyang Geng",
      "Hongwu Peng",
      "Jason D. Lee",
      "Deming Chen",
      "Tri Dao"
    ]
  },
  "https://proceedings.mlr.press/v235/cai24c.html": {
    "title": "Enhancing Cross-Modal Fine-Tuning with Gradually Intermediate Modality Generation",
    "volume": "main",
    "abstract": "Large-scale pretrained models have proven immensely valuable in handling data-intensive modalities like text and image. However, fine-tuning these models for certain specialized modalities, such as protein sequence and cosmic ray, poses challenges due to the significant modality discrepancy and scarcity of labeled data. In this paper, we propose an end-to-end method, PaRe, to enhance cross-modal fine-tuning, aiming to transfer a large-scale pretrained model to various target modalities. PaRe employs a gating mechanism to select key patches from both source and target data. Through a modality-agnostic Patch Replacement scheme, these patches are preserved and combined to construct data-rich intermediate modalities ranging from easy to hard. By gradually intermediate modality generation, we can not only effectively bridge the modality gap to enhance stability and transferability of cross-modal fine-tuning, but also address the challenge of limited data in the target modality by leveraging enriched intermediate modality data. Compared with hand-designed, general-purpose, task-specific, and state-of-the-art cross-modal fine-tuning approaches, PaRe demonstrates superior performance across three challenging benchmarks, encompassing more than ten modalities",
    "checked": true,
    "id": "92197d5da5a401aec67e996bc9d448124bf7d232",
    "semantic_title": "enhancing cross-modal fine-tuning with gradually intermediate modality generation",
    "citation_count": 0,
    "authors": [
      "Lincan Cai",
      "Shuang Li",
      "Wenxuan Ma",
      "Jingxuan Kang",
      "Binhui Xie",
      "Zixun Sun",
      "Chengwei Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/cai24d.html": {
    "title": "Batch and match: black-box variational inference with a score-based divergence",
    "volume": "main",
    "abstract": "Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates and their sensitivity to hyperparameters. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM typically converges in fewer (and sometimes significantly fewer) gradient evaluations than leading implementations of BBVI based on ELBO maximization",
    "checked": true,
    "id": "a1e6b79a6e4f0fce311d11c8ed97047b48047eb2",
    "semantic_title": "batch and match: black-box variational inference with a score-based divergence",
    "citation_count": 6,
    "authors": [
      "Diana Cai",
      "Chirag Modi",
      "Loucas Pillaud-Vivien",
      "Charles Margossian",
      "Robert M. Gower",
      "David Blei",
      "Lawrence K. Saul"
    ]
  },
  "https://proceedings.mlr.press/v235/cai24e.html": {
    "title": "Flextron: Many-in-One Flexible Large Language Model",
    "volume": "main",
    "abstract": "Training modern LLMs is extremely resource intensive, and customizing them for various deployment scenarios characterized by limited compute and memory resources through repeated training is impractical. In this paper, we introduce Flextron, a network architecture and post-training model optimization framework supporting flexible model deployment. The Flextron architecture utilizes a nested elastic structure to rapidly adapt to specific user-defined latency and accuracy targets during inference with no additional fine-tuning required. It is also input-adaptive, and can automatically route tokens through its sub-networks for improved performance and efficiency. We present a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model. We evaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance over multiple end-to-end trained variants and other state-of-the-art elastic networks, all with a single pretraining run that consumes a mere 7.63% tokens compared to original pretraining",
    "checked": true,
    "id": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
    "semantic_title": "flextron: many-in-one flexible large language model",
    "citation_count": 5,
    "authors": [
      "Ruisi Cai",
      "Saurav Muralidharan",
      "Greg Heinrich",
      "Hongxu Yin",
      "Zhangyang Wang",
      "Jan Kautz",
      "Pavlo Molchanov"
    ]
  },
  "https://proceedings.mlr.press/v235/cai24f.html": {
    "title": "Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion",
    "volume": "main",
    "abstract": "We study constrained comonotone min-max optimization, a structured class of nonconvex-nonconcave min-max optimization problems, and their generalization to comonotone inclusion. In our first contribution, we extend the Extra Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu (2021) for unconstrained min-max optimization, to constrained comonotone min-max optimization and comonotone inclusion, achieving an optimal convergence rate of $O\\left(\\frac{1}{T}\\right)$ among all first-order methods. Additionally, we prove that the algorithm's iterations converge to a point in the solution set. In our second contribution, we extend the Fast Extra Gradient (FEG) algorithm, as developed by Lee and Kim (2021), to constrained comonotone min-max optimization and comonotone inclusion, achieving the same $O\\left(\\frac{1}{T}\\right)$ convergence rate. This rate is applicable to the broadest set of comonotone inclusion problems yet studied in the literature. Our analyses are based on simple potential function arguments, which might be useful for analyzing other accelerated algorithms",
    "checked": true,
    "id": "3fc133a0b16a4e737e5685760e7295288f9e50fa",
    "semantic_title": "accelerated algorithms for constrained nonconvex-nonconcave min-max optimization and comonotone inclusion",
    "citation_count": 15,
    "authors": [
      "Yang Cai",
      "Argyris Oikonomou",
      "Weiqiang Zheng"
    ]
  },
  "https://proceedings.mlr.press/v235/cai24g.html": {
    "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
    "volume": "main",
    "abstract": "This paper tackles the memory hurdle of of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for Long Context Compression (LoCoCo). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward \"drop-in\" integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to $3482$ tokens into a $128$-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to $0.2791$ compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruisi Cai",
      "Yuandong Tian",
      "Zhangyang Wang",
      "Beidi Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/cai24h.html": {
    "title": "On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box",
    "volume": "main",
    "abstract": "Attribution methods shed light on the explainability of data-driven approaches such as deep learning models by uncovering the most influential features in a to-be-explained decision. While determining feature attributions via gradients delivers promising results, the internal access required for acquiring gradients can be impractical under safety concerns, thus limiting the applicability of gradient-based approaches. In response to such limited flexibility, this paper presents GEEX (gradient-estimation-based explanation), a method that produces gradient-like explanations through only query-level access. The proposed approach holds a set of fundamental properties for attribution methods, which are mathematically rigorously proved, ensuring the quality of its explanations. In addition to the theoretical analysis, with a focus on image data, the experimental results empirically demonstrate the superiority of the proposed method over state-of-the-art black-box methods and its competitive performance compared to methods with full access",
    "checked": true,
    "id": "360ab7a9947226557bd25fab6024296ae8a30673",
    "semantic_title": "on gradient-like explanation under a black-box setting: when black-box explanations become as good as white-box",
    "citation_count": 0,
    "authors": [
      "Yi Cai",
      "Gerhard Wunder"
    ]
  },
  "https://proceedings.mlr.press/v235/cai24i.html": {
    "title": "Sample-specific Masks for Visual Reprogramming-based Prompting",
    "volume": "main",
    "abstract": "Visual reprogramming (VR) is a prompting technique that aims to re-purpose a pre-trained model (e.g., a classifier on ImageNet) to target tasks (e.g., medical data prediction) by learning a small-scale pattern added into input images instead of tuning considerable parameters within the model. The location of the pattern within input samples is usually determined by a pre-defined mask shared across all samples. In this paper, we show that the shared mask potentially limits VR's generalization and increases its approximation error due to the lack of sample-level adaptation. Motivated by this finding, we design a new framework for VR called sample-specific multi-channel masks (SMM). Specifically, SMM employs a lightweight ConvNet and patch-wise interpolation to generate sample-specific three-channel masks instead of a shared and pre-defined mask. Since we generate different masks for individual samples, SMM is theoretically shown to reduce approximation error for the target tasks compared with existing state-of-the-art VR methods. We also empirically demonstrate its performance gain on both ResNet and ViT. The success of SMM further highlights the broader applicability of VR in leveraging the latent knowledge of pre-trained models for various target tasks. Our code is available at https://github.com/tmlr-group/SMM",
    "checked": true,
    "id": "77ba3fc35d8272090922f0e7fd4f51c186350d51",
    "semantic_title": "sample-specific masks for visual reprogramming-based prompting",
    "citation_count": 1,
    "authors": [
      "Chengyi Cai",
      "Zesheng Ye",
      "Lei Feng",
      "Jianzhong Qi",
      "Feng Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/calandriello24a.html": {
    "title": "Human Alignment of Large Language Models through Online Preference Optimisation",
    "volume": "main",
    "abstract": "Ensuring alignment of language model's outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD. This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generations are sampled by the online policy and annotated by a trained preference model. Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play. Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy (between the online and reference policy) similarly as the general Nash-MD algorithm. We compare online-IPO and IPO-MD to different online versions of existing losses on preference data such as DPO and SLiC on a summarisation task",
    "checked": true,
    "id": "46566ef7e51987cd101bf2b275c650cb3be21995",
    "semantic_title": "human alignment of large language models through online preference optimisation",
    "citation_count": 39,
    "authors": [
      "Daniele Calandriello",
      "Zhaohan Daniel Guo",
      "Remi Munos",
      "Mark Rowland",
      "Yunhao Tang",
      "Bernardo Avila Pires",
      "Pierre Harvey Richemond",
      "Charline Le Lan",
      "Michal Valko",
      "Tianqi Liu",
      "Rishabh Joshi",
      "Zeyu Zheng",
      "Bilal Piot"
    ]
  },
  "https://proceedings.mlr.press/v235/calvo-ordonez24a.html": {
    "title": "Partially Stochastic Infinitely Deep Bayesian Neural Networks",
    "volume": "main",
    "abstract": "In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the computational efficiency of existing architectures at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational complexity. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators. Lastly, empirical evaluations across multiple tasks show that our proposed architectures achieve better downstream task performance and uncertainty quantification than their counterparts while being significantly more efficient. The code can be found at https://github.com/Sergio20f/part_stoch_inf_deep",
    "checked": true,
    "id": "37b7554a7841e257401bcc76f5f967d112e30f51",
    "semantic_title": "partially stochastic infinitely deep bayesian neural networks",
    "citation_count": 1,
    "authors": [
      "Sergio Calvo Ordoñez",
      "Matthieu Meunier",
      "Francesco Piatti",
      "Yuantao Shi"
    ]
  },
  "https://proceedings.mlr.press/v235/campbell24a.html": {
    "title": "Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design",
    "volume": "main",
    "abstract": "Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a multimodal flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or structure",
    "checked": true,
    "id": "b30ce57128b672945b3e24f98aee63b2b3881ee0",
    "semantic_title": "generative flows on discrete state-spaces: enabling multimodal flows with applications to protein co-design",
    "citation_count": 40,
    "authors": [
      "Andrew Campbell",
      "Jason Yim",
      "Regina Barzilay",
      "Tom Rainforth",
      "Tommi Jaakkola"
    ]
  },
  "https://proceedings.mlr.press/v235/candido-ramos24a.html": {
    "title": "Mimicking Better by Matching the Approximate Action Distribution",
    "volume": "main",
    "abstract": "In this paper, we introduce MAAD, a novel, sample-efficient on-policy algorithm for Imitation Learning from Observations. MAAD utilizes a surrogate reward signal, which can be derived from various sources such as adversarial games, trajectory matching objectives, or optimal transport criteria. To compensate for the non-availability of expert actions, we rely on an inverse dynamics model that infers plausible actions distribution given the expert's state-state transitions; we regularize the imitator's policy by aligning it to the inferred action distribution. MAAD leads to significantly improved sample efficiency and stability. We demonstrate its effectiveness in a number of MuJoCo environments, both int the OpenAI Gym and the DeepMind Control Suite. We show that it requires considerable fewer interactions to achieve expert performance, outperforming current state-of-the-art on-policy methods. Remarkably, MAAD often stands out as the sole method capable of attaining expert performance levels, underscoring its simplicity and efficacy",
    "checked": true,
    "id": "7fc5aa48d4e8020b3580f050c309bf46bc1d52e4",
    "semantic_title": "mimicking better by matching the approximate action distribution",
    "citation_count": 1,
    "authors": [
      "Joao Candido Ramos",
      "Lionel Blondé",
      "Naoya Takeishi",
      "Alexandros Kalousis"
    ]
  },
  "https://proceedings.mlr.press/v235/canturk24a.html": {
    "title": "Graph Positional and Structural Encoder",
    "volume": "main",
    "abstract": "Positional and structural encodings (PSE) enable better identifiability of nodes within a graph, rendering them essential tools for empowering modern GNNs, and in particular graph Transformers. However, designing PSEs that work optimally for all graph prediction tasks is a challenging and unsolved problem. Here, we present the Graph Positional and Structural Encoder (GPSE), the first-ever graph encoder designed to capture rich PSE representations for augmenting any GNN. GPSE learns an efficient common latent representation for multiple PSEs, and is highly transferable: The encoder trained on a particular graph dataset can be used effectively on datasets drawn from markedly different distributions and modalities. We show that across a wide range of benchmarks, GPSE-enhanced models can significantly outperform those that employ explicitly computed PSEs, and at least match their performance in others. Our results pave the way for the development of foundational pre-trained graph encoders for extracting positional and structural information, and highlight their potential as a more powerful and efficient alternative to explicitly computed PSEs and existing self-supervised pre-training approaches. Our framework and pre-trained models are publicly available at https://github.com/G-Taxonomy-Workgroup/GPSE. For convenience, GPSE has also been integrated into the PyG library to facilitate downstream applications",
    "checked": true,
    "id": "7fdce9a00ae132cc8bb3f893788a19d0c1649e6a",
    "semantic_title": "graph positional and structural encoder",
    "citation_count": 6,
    "authors": [
      "Semih Cantürk",
      "Renming Liu",
      "Olivier Lapointe-Gagné",
      "Vincent Létourneau",
      "Guy Wolf",
      "Dominique Beaini",
      "Ladislav Rampášek"
    ]
  },
  "https://proceedings.mlr.press/v235/cao24a.html": {
    "title": "Successor Features for Efficient Multi-Subject Controlled Text Generation",
    "volume": "main",
    "abstract": "While large language models (LLMs) have achieved impressive performance in generating fluent and realistic text, controlling the generated text so that it exhibits properties such as safety, factuality, and non-toxicity remains challenging. Existing decoding-based controllable text generation methods are static in terms of the dimension of control; if the target subject is changed, they require new training. Moreover, it can quickly become prohibitive to concurrently control multiple subjects. To address these challenges, we first show that existing methods can be framed as a reinforcement learning problem, where an action-value function estimates the likelihood of a desired attribute appearing in the generated text. Then, we introduce a novel approach named SF-Gen, which leverages the concept of successor features to decouple the dynamics of LLMs from task-specific rewards. By employing successor features, our method proves to be memory-efficient and computationally efficient for both training and decoding, especially when dealing with multiple target subjects. To the best of our knowledge, our research represents the first application of successor features in text generation. In addition to its computational efficiency, the resultant language produced by our method is comparable to the SOTA (and outperforms baselines) in both control measures as well as language quality, which we demonstrate through a series of experiments in various controllable text generation tasks",
    "checked": false,
    "id": "34fda38263a4af9d0be684ae564ce1c3120c3314",
    "semantic_title": "successor features for efficient multisubject controlled text generation",
    "citation_count": 0,
    "authors": [
      "Meng Cao",
      "Mehdi Fatemi",
      "Jackie Ck Cheung",
      "Samira Shabanian"
    ]
  },
  "https://proceedings.mlr.press/v235/cao24b.html": {
    "title": "Limited Preference Aided Imitation Learning from Imperfect Demonstrations",
    "volume": "main",
    "abstract": "Imitation learning mimics high-quality policies from expert data for sequential decision-making tasks. However, its efficacy is hindered in scenarios where optimal demonstrations are unavailable, and only imperfect demonstrations are present. To address this issue, introducing additional limited human preferences is a suitable approach as it can be obtained in a human-friendly manner, offering a promising way to learn the policy that exceeds the performance of imperfect demonstrations. In this paper, we propose a novel imitation learning (IL) algorithm, Preference Aided Imitation Learning from imperfect demonstrations (PAIL). Specifically, PAIL learns a preference reward by querying experts for limited preferences from imperfect demonstrations. This serves two purposes during training: 1) Reweighting imperfect demonstrations with the preference reward for higher quality. 2) Selecting explored trajectories with high cumulative preference rewards to augment imperfect demonstrations. The dataset with continuously improving quality empowers the performance of PAIL to transcend the initial demonstrations. Comprehensive empirical results across a synthetic task and two locomotion benchmarks show that PAIL surpasses baselines by 73.2% and breaks through the performance bottleneck of imperfect demonstrations",
    "checked": true,
    "id": "50f6afa8002711aace580ae62511468e0fa3cfad",
    "semantic_title": "limited preference aided imitation learning from imperfect demonstrations",
    "citation_count": 0,
    "authors": [
      "Xingchen Cao",
      "Fan-Ming Luo",
      "Junyin Ye",
      "Tian Xu",
      "Zhilong Zhang",
      "Yang Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/cao24c.html": {
    "title": "Predictive Dynamic Fusion",
    "volume": "main",
    "abstract": "Multimodal fusion is crucial in joint decision-making systems for rendering holistic judgments. Since multimodal data changes in open environments, dynamic fusion has emerged and achieved remarkable progress in numerous applications. However, most existing dynamic multimodal fusion methods lack theoretical guarantees and easily fall into suboptimal problems, yielding unreliability and instability. To address this issue, we propose a Predictive Dynamic Fusion (PDF) framework for multimodal learning. We proceed to reveal the multimodal fusion from a generalization perspective and theoretically derive the predictable Collaborative Belief (Co-Belief) with Mono- and Holo-Confidence, which provably reduces the upper bound of generalization error. Accordingly, we further propose a relative calibration strategy to calibrate the predicted Co-Belief for potential uncertainty. Extensive experiments on multiple benchmarks confirm our superiority. Our code is available at https://github.com/Yinan-Xia/PDF",
    "checked": true,
    "id": "ea8a2ea9d27d86dd5f2118e571fb51340f40d49c",
    "semantic_title": "predictive dynamic fusion",
    "citation_count": 0,
    "authors": [
      "Bing Cao",
      "Yinan Xia",
      "Yi Ding",
      "Changqing Zhang",
      "Qinghua Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/cao24d.html": {
    "title": "Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Detecting out-of-distribution (OOD) samples is essential when deploying machine learning models in open-world scenarios. Zero-shot OOD detection, requiring no training on in-distribution (ID) data, has been possible with the advent of vision-language models like CLIP. Existing methods build a text-based classifier with only closed-set labels. However, this largely restricts the inherent capability of CLIP to recognize samples from large and open label space. In this paper, we propose to tackle this constraint by leveraging the expert knowledge and reasoning capability of large language models (LLM) to Envision potential Outlier Exposure, termed EOE, without access to any actual OOD data. Owing to better adaptation to open-world scenarios, EOE can be generalized to different tasks, including far, near, and fine-grained OOD detection. Technically, we design (1) LLM prompts based on visual similarity to generate potential outlier class labels specialized for OOD detection, as well as (2) a new score function based on potential outlier penalty to distinguish hard OOD samples effectively. Empirically, EOE achieves state-of-the-art performance across different OOD tasks and can be effectively scaled to the ImageNet-1K dataset. The code is publicly available at: https://github.com/tmlr-group/EOE",
    "checked": true,
    "id": "14cfe2588311870325e2770c5159d3100d7031ea",
    "semantic_title": "envisioning outlier exposure by large language models for out-of-distribution detection",
    "citation_count": 5,
    "authors": [
      "Chentao Cao",
      "Zhun Zhong",
      "Zhanke Zhou",
      "Yang Liu",
      "Tongliang Liu",
      "Bo Han"
    ]
  },
  "https://proceedings.mlr.press/v235/caragiannis24a.html": {
    "title": "Can a Few Decide for Many? The Metric Distortion of Sortition",
    "volume": "main",
    "abstract": "Recent works have studied the design of algorithms for selecting representative sortition panels. However, the most central question remains unaddressed: Do these panels reflect the entire population's opinion? We present a positive answer by adopting the concept of metric distortion from computational social choice, which aims to quantify how much a panel's decision aligns with the ideal decision of the population when preferences and agents lie on a metric space. We show that uniform selection needs only logarithmically many agents in terms of the number of alternatives to achieve almost optimal distortion. We also show that Fair Greedy Capture, a selection algorithm introduced recently by Ebadian and Micha (2024), matches uniform selection's guarantees of almost optimal distortion and also achieves constant ex-post distortion, ensuring a \"best of both worlds\" performance",
    "checked": true,
    "id": "b7f8dcb79abd1ff80150b7fc9a3c6ac7241a4651",
    "semantic_title": "can a few decide for many? the metric distortion of sortition",
    "citation_count": 2,
    "authors": [
      "Ioannis Caragiannis",
      "Evi Micha",
      "Jannik Peters"
    ]
  },
  "https://proceedings.mlr.press/v235/carlini24a.html": {
    "title": "Stealing part of a production language model",
    "volume": "main",
    "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under $20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the GPT-3.5-turbo model, and estimate it would cost under \\$2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack",
    "checked": true,
    "id": "b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
    "semantic_title": "stealing part of a production language model",
    "citation_count": 44,
    "authors": [
      "Nicholas Carlini",
      "Daniel Paleka",
      "Krishnamurthy Dj Dvijotham",
      "Thomas Steinke",
      "Jonathan Hayase",
      "A. Feder Cooper",
      "Katherine Lee",
      "Matthew Jagielski",
      "Milad Nasr",
      "Arthur Conmy",
      "Eric Wallace",
      "David Rolnick",
      "Florian Tramèr"
    ]
  },
  "https://proceedings.mlr.press/v235/carroll24a.html": {
    "title": "AI Alignment with Changing and Influenceable Reward Functions",
    "volume": "main",
    "abstract": "Existing AI alignment approaches assume that preferences are static, which is unrealistic: our preferences change, and may even be influenced by our interactions with AI systems themselves. To clarify the consequences of incorrectly assuming static preferences, we introduce Dynamic Reward Markov Decision Processes (DR-MDPs), which explicitly model preference changes and the AI's influence on them. We show that despite its convenience, the static-preference assumption may undermine the soundness of existing alignment techniques, leading them to implicitly reward AI systems for influencing user preferences in ways users may not truly want. We then explore potential solutions. First, we offer a unifying perspective on how an agent's optimization horizon may partially help reduce undesirable AI influence. Then, we formalize different notions of AI alignment that account for preference change from the outset. Comparing the strengths and limitations of 8 such notions of alignment, we find that they all either err towards causing undesirable AI influence, or are overly risk-averse, suggesting that a straightforward solution to the problems of changing preferences may not exist. As there is no avoiding grappling with changing preferences in real-world settings, this makes it all the more important to handle these issues with care, balancing risks and capabilities. We hope our work can provide conceptual clarity and constitute a first step towards AI alignment practices which explicitly account for (and contend with) the changing and influenceable nature of human preferences",
    "checked": true,
    "id": "e4eee21ef4e11d3d0ad444b049c6c758b5240187",
    "semantic_title": "ai alignment with changing and influenceable reward functions",
    "citation_count": 8,
    "authors": [
      "Micah Carroll",
      "Davis Foote",
      "Anand Siththaranjan",
      "Stuart Russell",
      "Anca Dragan"
    ]
  },
  "https://proceedings.mlr.press/v235/cassel24a.html": {
    "title": "Near-Optimal Regret in Linear MDPs with Aggregate Bandit Feedback",
    "volume": "main",
    "abstract": "In many real-world applications, it is hard to provide a reward signal in each step of a Reinforcement Learning (RL) process and more natural to give feedback when an episode ends. To this end, we study the recently proposed model of RL with Aggregate Bandit Feedback (RL-ABF), where the agent only observes the sum of rewards at the end of an episode instead of each reward individually. Prior work studied RL-ABF only in tabular settings, where the number of states is assumed to be small. In this paper, we extend ABF to linear function approximation and develop two efficient algorithms with near-optimal regret guarantees: a value-based optimistic algorithm built on a new randomization technique with a Q-functions ensemble, and a policy optimization algorithm that uses a novel hedging scheme over the ensemble",
    "checked": true,
    "id": "fef73ec58077f1cc6e4c1d7f04b4ce14341c9504",
    "semantic_title": "near-optimal regret in linear mdps with aggregate bandit feedback",
    "citation_count": 2,
    "authors": [
      "Asaf Cassel",
      "Haipeng Luo",
      "Aviv Rosenberg",
      "Dmitry Sotnikov"
    ]
  },
  "https://proceedings.mlr.press/v235/castiglioni24a.html": {
    "title": "Online Learning under Budget and ROI Constraints via Weak Adaptivity",
    "volume": "main",
    "abstract": "We study online learning problems in which a decision maker has to make a sequence of costly decisions, with the goal of maximizing their expected reward while adhering to budget and return-on-investment (ROI) constraints. Existing primal-dual algorithms designed for constrained online learning problems under adversarial inputs rely on two fundamental assumptions. First, the decision maker must know beforehand the value of parameters related to the degree of strict feasibility of the problem (i.e. Slater parameters). Second, a strictly feasible solution to the offline optimization problem must exist at each round. Both requirements are unrealistic for practical applications such as bidding in online ad auctions. In this paper, we show how such assumptions can be circumvented by endowing standard primal-dual templates with weakly adaptive regret minimizers. This results in a \"dual-balancing\" framework which ensures that dual variables stay sufficiently small, even in the absence of knowledge about Slater's parameter. We prove the first best-of-both-worlds no-regret guarantees which hold in absence of the two aforementioned assumptions, under stochastic and adversarial inputs. Finally, we show how to instantiate the framework to optimally bid in various mechanisms of practical relevance, such as first- and second-price auctions",
    "checked": true,
    "id": "7b76f68f6b16634f3fa6669fee4becfe91bad580",
    "semantic_title": "online learning under budget and roi constraints via weak adaptivity",
    "citation_count": 0,
    "authors": [
      "Matteo Castiglioni",
      "Andrea Celli",
      "Christian Kroer"
    ]
  },
  "https://proceedings.mlr.press/v235/castin24a.html": {
    "title": "How Smooth Is Attention?",
    "volume": "main",
    "abstract": "Self-attention and masked self-attention are at the heart of Transformers' outstanding success. Still, our mathematical understanding of attention, in particular of its Lipschitz properties — which are key when it comes to analyzing robustness and expressive power — is incomplete. We provide a detailed study of the Lipschitz constant of self-attention in several practical scenarios, discussing the impact of the sequence length $n$ and layer normalization on the local Lipschitz constant of both unmasked and masked self-attention. In particular, we show that for inputs of length $n$ in any compact set, the Lipschitz constant of self-attention is bounded by $\\sqrt{n}$ up to a constant factor and that this bound is tight for reasonable sequence lengths. When the sequence length $n$ is too large for the previous bound to be tight, which we refer to as the mean-field regime, we provide an upper bound and a matching lower bound which are independent of $n$. Our mean-field framework for masked self-attention is novel and of independent interest. Our experiments on pretrained and randomly initialized BERT and GPT-2 support our theoretical findings",
    "checked": true,
    "id": "2cb91c1a9488c3bdb05eee7668ac4a77c7e18a3b",
    "semantic_title": "how smooth is attention?",
    "citation_count": 5,
    "authors": [
      "Valérie Castin",
      "Pierre Ablin",
      "Gabriel Peyré"
    ]
  },
  "https://proceedings.mlr.press/v235/catalano24a.html": {
    "title": "Hierarchical Integral Probability Metrics: A distance on random probability measures with low sample complexity",
    "volume": "main",
    "abstract": "Random probabilities are a key component to many nonparametric methods in Statistics and Machine Learning. To quantify comparisons between different laws of random probabilities several works are starting to use the elegant Wasserstein over Wasserstein distance. In this paper we prove that the infinite dimensionality of the space of probabilities drastically deteriorates its sample complexity, which is slower than any polynomial rate in the sample size. We propose a new distance that preserves many desirable properties of the former while achieving a parametric rate of convergence. In particular, our distance 1) metrizes weak convergence; 2) can be estimated numerically through samples with low complexity; 3) can be bounded analytically from above and below. The main ingredient are integral probability metrics, which lead to the name hierarchical IPM",
    "checked": true,
    "id": "0bc3d57b1e3f5f2b818e0c9fdf886cc05736c43b",
    "semantic_title": "hierarchical integral probability metrics: a distance on random probability measures with low sample complexity",
    "citation_count": 0,
    "authors": [
      "Marta Catalano",
      "Hugo Lavenant"
    ]
  },
  "https://proceedings.mlr.press/v235/cattaneo24a.html": {
    "title": "On the Implicit Bias of Adam",
    "volume": "main",
    "abstract": "In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different \"norm\" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, conversely, impede its reduction (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization",
    "checked": true,
    "id": "7b444736da839085f423609e22284b9882f85c5b",
    "semantic_title": "on the implicit bias of adam",
    "citation_count": 8,
    "authors": [
      "Matias D. Cattaneo",
      "Jason Matthew Klusowski",
      "Boris Shigida"
    ]
  },
  "https://proceedings.mlr.press/v235/celik24a.html": {
    "title": "Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose Diverse Skill Learning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective. We show on challenging robot simulation tasks that Di-SkilL can learn diverse and performant skills",
    "checked": true,
    "id": "dd483e4956eafba4bc82af26a29e0075a696f788",
    "semantic_title": "acquiring diverse skills using curriculum reinforcement learning with mixture of experts",
    "citation_count": 3,
    "authors": [
      "Onur Celik",
      "Aleksandar Taranovic",
      "Gerhard Neumann"
    ]
  },
  "https://proceedings.mlr.press/v235/celis24a.html": {
    "title": "Centralized Selection with Preferences in the Presence of Biases",
    "volume": "main",
    "abstract": "This paper considers the scenario in which there are multiple institutions, each with a limited capacity for candidates, and candidates, each with preferences over the institutions. A central entity evaluates the utility of each candidate to the institutions, and the goal is to select candidates for each institution in a way that maximizes utility while also considering the candidates' preferences. The paper focuses on the setting in which candidates are divided into multiple groups and the observed utilities of candidates in some groups are biased–systematically lower than their true utilities. The first result is that, in these biased settings, prior algorithms can lead to selections with sub-optimal true utility and significant discrepancies in the fraction of candidates from each group that get their preferred choices. Subsequently, an algorithm is presented along with proof that it produces selections that achieve near-optimal group fairness with respect to preferences while also nearly maximizing the true utility under distributional assumptions. Further, extensive empirical validation of these results in real-world and synthetic settings, in which the distributional assumptions may not hold, are presented",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "L. Elisa Celis",
      "Amit Kumar",
      "Nisheeth K. Vishnoi",
      "Andrew Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/cen24a.html": {
    "title": "Using Left and Right Brains Together: Towards Vision and Language Planning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) and Large Multi-modality Models (LMMs) have demonstrated remarkable decision masking capabilities on a variety of tasks. However, they inherently operate planning within the language space, lacking the vision and spatial imagination ability. In contrast, humans utilize both left and right hemispheres of the brain for language and visual planning during the thinking process. Therefore, we introduce a novel vision-language planning framework in this work to perform concurrent visual and language planning for tasks with inputs of any form. Our framework incorporates visual planning to capture intricate environmental details, while language planning enhances the logical coherence of the overall system. We evaluate the effectiveness of our framework across vision-language tasks, vision-only tasks, and language-only tasks. The results demonstrate the superior performance of our approach, indicating that the integration of visual and language planning yields better contextually aware task execution",
    "checked": true,
    "id": "83d201d503b863fec7d1225f00a141e722e03f17",
    "semantic_title": "using left and right brains together: towards vision and language planning",
    "citation_count": 2,
    "authors": [
      "Jun Cen",
      "Chenfei Wu",
      "Xiao Liu",
      "Shengming Yin",
      "Yixuan Pei",
      "Jinglong Yang",
      "Qifeng Chen",
      "Nan Duan",
      "Jianguo Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/cen24b.html": {
    "title": "Feasibility Consistent Representation Learning for Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "In the field of safe reinforcement learning (RL), finding a balance between satisfying safety constraints and optimizing reward performance presents a significant challenge. A key obstacle in this endeavor is the estimation of safety constraints, which is typically more difficult than estimating a reward metric due to the sparse nature of the constraint signals. To address this issue, we introduce a novel framework named Feasibility Consistent Safe Reinforcement Learning (FCSRL). This framework combines representation learning with feasibility-oriented objectives to identify and extract safety-related information from the raw state for safe RL. Leveraging self-supervised learning techniques and a more learnable safety metric, our approach enhances the policy learning and constraint estimation. Empirical evaluations across a range of vector-state and image-based tasks demonstrate that our method is capable of learning a better safety-aware embedding and achieving superior performance than previous representation learning baselines",
    "checked": true,
    "id": "f23bd924fd676d2f45d26f71ccd9498b6f53dce8",
    "semantic_title": "feasibility consistent representation learning for safe reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Zhepeng Cen",
      "Yihang Yao",
      "Zuxin Liu",
      "Ding Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/cetin24a.html": {
    "title": "Simple Ingredients for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning algorithms have proven effective on datasets highly connected to the target downstream task. Yet, by leveraging a novel testbed (MOOD) in which trajectories come from heterogeneous sources, we show that existing methods struggle with diverse data: their performance considerably deteriorates as data collected for related but different tasks is simply added to the offline buffer. In light of this finding, we conduct a large empirical study where we formulate and test several hypotheses to explain this failure. Surprisingly, we find that targeted scale, more than algorithmic considerations, is the key factor influencing performance. We show that simple methods like AWAC and IQL with increased policy size overcome the paradoxical failure modes from the inclusion of additional data in MOOD, and notably outperform prior state-of-the-art algorithms on the canonical D4RL benchmark",
    "checked": true,
    "id": "d24fabfd0fd96951af968e53530aefdd4744bd28",
    "semantic_title": "simple ingredients for offline reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Edoardo Cetin",
      "Andrea Tirinzoni",
      "Matteo Pirotta",
      "Alessandro Lazaric",
      "Yann Ollivier",
      "Ahmed Touati"
    ]
  },
  "https://proceedings.mlr.press/v235/cha24a.html": {
    "title": "Regularizing with Pseudo-Negatives for Continual Self-Supervised Learning",
    "volume": "main",
    "abstract": "We introduce a novel Pseudo-Negative Regularization (PNR) framework for effective continual self-supervised learning (CSSL). Our PNR leverages pseudo-negatives obtained through model-based augmentation in a way that newly learned representations may not contradict what has been learned in the past. Specifically, for the InfoNCE-based contrastive learning methods, we define symmetric pseudo-negatives obtained from current and previous models and use them in both main and regularization loss terms. Furthermore, we extend this idea to non-contrastive learning methods which do not inherently rely on negatives. For these methods, a pseudo-negative is defined as the output from the previous model for a differently augmented version of the anchor sample and is asymmetrically applied to the regularization term. Extensive experimental results demonstrate that our PNR framework achieves state-of-the-art performance in representation learning during CSSL by effectively balancing the trade-off between plasticity and stability",
    "checked": true,
    "id": "41fdc913008d7d74779827108319fdce46dadfbd",
    "semantic_title": "regularizing with pseudo-negatives for continual self-supervised learning",
    "citation_count": 2,
    "authors": [
      "Sungmin Cha",
      "Kyunghyun Cho",
      "Taesup Moon"
    ]
  },
  "https://proceedings.mlr.press/v235/chadha24a.html": {
    "title": "Auditing Private Prediction",
    "volume": "main",
    "abstract": "Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of an algorithm, while empirical auditing establishes a practical lower bound. Auditing techniques exist for DP training algorithms. However machine learning can also be made private at inference. We propose the first framework for auditing private prediction where we instantiate adversaries with varying poisoning and query capabilities. This enables us to study the privacy leakage of four private prediction algorithms: PATE (Papernot et al., 2016), CaPC (Choquette-Choo et al., 2020), PromptPATE (Duan et al., 2023), and Private-kNN (Zhu et al., 2020). To conduct our audit, we introduce novel techniques to empirically evaluate privacy leakage in terms of Renyi DP. Our experiments show that (i) the privacy analysis of private prediction can be improved, (ii) algorithms which are easier to poison lead to much higher privacy leakage, and (iii) the privacy leakage is significantly lower for adversaries without query control than those with full control",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karan Chadha",
      "Matthew Jagielski",
      "Nicolas Papernot",
      "Christopher A. Choquette-Choo",
      "Milad Nasr"
    ]
  },
  "https://proceedings.mlr.press/v235/chakraborty24a.html": {
    "title": "Position: On the Possibilities of AI-Generated Text Detection",
    "volume": "main",
    "abstract": "Our study addresses the challenge of distinguishing human-written text from Large Language Model (LLM) outputs. We provide evidence that this differentiation is consistently feasible, except when human and machine text distributions are indistinguishable across their entire support. Employing information theory, we show that while detecting machine-generated text becomes harder as it nears human quality, it remains possible with adequate text data. We introduce guidelines on the required text data quantity, either through sample size or sequence length, for reliable AI text detection, through derivations of sample complexity bounds. This research paves the way for advanced detection methods. Our comprehensive empirical tests, conducted across various datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) and with several state-of-the-art text generators (GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, Llama-2-70B-Chat-HF), assess the viability of enhanced detection methods against detectors like RoBERTa-Large/Base-Detector and GPTZero, with increasing sample sizes and sequence lengths. Our findings align with OpenAI's empirical data related to sequence length, marking the first theoretical substantiation for these observations",
    "checked": true,
    "id": "d7ca9a27a1694119480076cdfd3a2c566b492188",
    "semantic_title": "position: on the possibilities of ai-generated text detection",
    "citation_count": 0,
    "authors": [
      "Souradip Chakraborty",
      "Amrit Bedi",
      "Sicheng Zhu",
      "Bang An",
      "Dinesh Manocha",
      "Furong Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/chakraborty24b.html": {
    "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
    "volume": "main",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, the single reward model overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. Next, we propose to learn a mixture of reward models via an expectation-maximization algorithm and solve a MaxMin alignment objective inspired by the Egalitarian principle in social choice theory to better honor diverse human preferences. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language (with Tulu2-7B)) and show the efficacy of the proposed approach in the presence of diversity among human preferences. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Souradip Chakraborty",
      "Jiahao Qiu",
      "Hui Yuan",
      "Alec Koppel",
      "Dinesh Manocha",
      "Furong Huang",
      "Amrit Bedi",
      "Mengdi Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/chan24a.html": {
    "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback",
    "volume": "main",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many \"actions\" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima",
    "checked": true,
    "id": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
    "semantic_title": "dense reward for free in reinforcement learning from human feedback",
    "citation_count": 17,
    "authors": [
      "Alex James Chan",
      "Hao Sun",
      "Samuel Holt",
      "Mihaela Van Der Schaar"
    ]
  },
  "https://proceedings.mlr.press/v235/chan24b.html": {
    "title": "Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation",
    "volume": "main",
    "abstract": "Scribble-supervised semantic segmentation presents a cost-effective training method that utilizes annotations generated through scribbling. It is valued in attaining high performance while minimizing annotation costs, which has made it highly regarded among researchers. Scribble supervision propagates information from labeled pixels to the surrounding unlabeled pixels, enabling semantic segmentation for the entire image. However, existing methods often ignore the features of classified pixels during feature propagation. To address these limitations, this paper proposes a prototype-based feature augmentation method that leverages feature prototypes to augment scribble supervision. Experimental results demonstrate that our approach achieves state-of-the-art performance on the PASCAL VOC 2012 dataset in scribble-supervised semantic segmentation tasks. The code is available at https://github.com/TranquilChan/PFA",
    "checked": true,
    "id": "7755adfc681aa859475a1649bb29be65cfa90861",
    "semantic_title": "scribble-supervised semantic segmentation with prototype-based feature augmentation",
    "citation_count": 0,
    "authors": [
      "Guiyang Chan",
      "Pengcheng Zhang",
      "Hai Dong",
      "Shunhui Ji",
      "Bainian Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/chang24a.html": {
    "title": "Feature Importance Disparities for Data Bias Investigations",
    "volume": "main",
    "abstract": "It is widely held that one cause of downstream bias in classifiers is bias present in the training data. Rectifying such biases may involve context-dependent interventions such as training separate models on subgroups, removing features with bias in the collection process, or even conducting real-world experiments to ascertain sources of bias. Despite the need for such data bias investigations, few automated methods exist to assist practitioners in these efforts. In this paper, we present one such method that given a dataset $X$ consisting of protected and unprotected features, outcomes $y$, and a regressor $h$ that predicts $y$ given $X$, outputs a tuple $(f_j, g)$, with the following property: $g$ corresponds to a subset of the training dataset $(X, y)$, such that the $j^{th}$ feature $f_j$ has much larger (or smaller) influence in the subgroup $g$, than on the dataset overall, which we call feature importance disparity (FID). We show across $4$ datasets and $4$ common feature importance methods of broad interest to the machine learning community that we can efficiently find subgroups with large FID values even over exponentially large subgroup classes and in practice these groups correspond to subgroups with potentially serious bias issues as measured by standard fairness metrics",
    "checked": true,
    "id": "bd7b25fd9aad7cf6093fcc30e19b27d1a81eb1ba",
    "semantic_title": "feature importance disparities for data bias investigations",
    "citation_count": 0,
    "authors": [
      "Peter W Chang",
      "Leor Fishman",
      "Seth Neel"
    ]
  },
  "https://proceedings.mlr.press/v235/chang24b.html": {
    "title": "Inferring Dynamic Networks from Marginals with Iterative Proportional Fitting",
    "volume": "main",
    "abstract": "A common network inference problem, arising from real-world data constraints, is how to infer a dynamic network from its time-aggregated adjacency matrix and time-varying marginals (i.e., row and column sums). Prior approaches to this problem have repurposed the classic iterative proportional fitting (IPF) procedure, also known as Sinkhorn's algorithm, with promising empirical results. However, the statistical foundation for using IPF has not been well understood: under what settings does IPF provide principled estimation of a dynamic network from its marginals, and how well does it estimate the network? In this work, we establish such a setting, by identifying a generative network model whose maximum likelihood estimates are recovered by IPF. Our model both reveals implicit assumptions on the use of IPF in such settings and enables new analyses, such as structure-dependent error bounds on IPF's parameter estimates. When IPF fails to converge on sparse network data, we introduce a principled algorithm that guarantees IPF converges under minimal changes to the network structure. Finally, we conduct experiments with synthetic and real-world data, which demonstrate the practical value of our theoretical and algorithmic contributions",
    "checked": true,
    "id": "00e840044e1b0ad14d336e17a9f5a8120bcfbe2c",
    "semantic_title": "inferring dynamic networks from marginals with iterative proportional fitting",
    "citation_count": 2,
    "authors": [
      "Serina Chang",
      "Frederic Koehler",
      "Zhaonan Qu",
      "Jure Leskovec",
      "Johan Ugander"
    ]
  },
  "https://proceedings.mlr.press/v235/chang24c.html": {
    "title": "LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits",
    "volume": "main",
    "abstract": "In the realm of electronic and electrical engineering, automation of analog circuit is increasingly vital given the complexity and customized requirements of modern applications. However, existing methods only develop search-based algorithms that require many simulation iterations to design a custom circuit topology, which is usually a time-consuming process. To this end, we introduce LaMAGIC, a pioneering language model-based topology generation model that leverages supervised finetuning for automated analog circuit design. LaMAGIC can efficiently generate an optimized circuit design from the custom specification in a single pass. Our approach involves a meticulous development and analysis of various input and output formulations for circuit. These formulations can ensure canonical representations of circuits and align with the autoregressive nature of LMs to effectively addressing the challenges of representing analog circuits as graphs. The experimental results show that LaMAGIC achieves a success rate of up to 96% under a strict tolerance of 0.01. We also examine the scalability and adaptability of LaMAGIC, specifically testing its performance on more complex circuits. Our findings reveal the enhanced effectiveness of our adjacency matrix-based circuit formulation with floating-point input, suggesting its suitability for handling intricate circuit designs. This research not only demonstrates the potential of language models in graph generation, but also builds a foundational framework for future explorations in automated analog circuit design",
    "checked": true,
    "id": "e84ee3ecf073e413556ef80d0d23ee08980f53ad",
    "semantic_title": "lamagic: language-model-based topology generation for analog integrated circuits",
    "citation_count": 0,
    "authors": [
      "Chen-Chia Chang",
      "Yikang Shen",
      "Shaoze Fan",
      "Jing Li",
      "Shun Zhang",
      "Ningyuan Cao",
      "Yiran Chen",
      "Xin Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/chang24d.html": {
    "title": "MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion",
    "volume": "main",
    "abstract": "In this work, we propose MagicPose, a diffusion-based model for 2D human pose and facial expression retargeting. Specifically, given a reference image, we aim to generate a person's new images by controlling the poses and facial expressions while keeping the identity unchanged. To this end, we propose a two-stage training strategy to disentangle human motions and appearance (e.g., facial expressions, skin tone, and dressing), consisting of (1) the pre-training of an appearance-control block and (2) learning appearance-disentangled pose control. Our novel design enables robust appearance control over generated human images, including body, facial attributes, and even background. By leveraging the prior knowledge of image diffusion models, MagicPose generalizes well to unseen human identities and complex poses without the need for additional fine-tuning. Moreover, the proposed model is easy to use and can be considered as a plug-in module/extension to Stable Diffusion. The project website is here. The code is available here",
    "checked": true,
    "id": "30bad170cc8a0834f0d54f1e89a863a6361f15bd",
    "semantic_title": "magicpose: realistic human poses and facial expressions retargeting with identity-aware diffusion",
    "citation_count": 19,
    "authors": [
      "Di Chang",
      "Yichun Shi",
      "Quankai Gao",
      "Hongyi Xu",
      "Jessica Fu",
      "Guoxian Song",
      "Qing Yan",
      "Yizhe Zhu",
      "Xiao Yang",
      "Mohammad Soleymani"
    ]
  },
  "https://proceedings.mlr.press/v235/chang24e.html": {
    "title": "From Biased Selective Labels to Pseudo-Labels: An Expectation-Maximization Framework for Learning from Biased Decisions",
    "volume": "main",
    "abstract": "Selective labels occur when label observations are subject to a decision-making process; e.g., diagnoses that depend on the administration of laboratory tests. We study a clinically-inspired selective label problem called disparate censorship, where labeling biases vary across subgroups and unlabeled individuals are imputed as \"negative\" (i.e., no diagnostic test = no illness). Machine learning models naively trained on such labels could amplify labeling bias. Inspired by causal models of selective labels, we propose Disparate Censorship Expectation-Maximization (DCEM), an algorithm for learning in the presence of disparate censorship. We theoretically analyze how DCEM mitigates the effects of disparate censorship on model performance. We validate DCEM on synthetic data, showing that it improves bias mitigation (area between ROC curves) without sacrificing discriminative performance (AUC) compared to baselines. We achieve similar results in a sepsis classification task using clinical data",
    "checked": true,
    "id": "48a2bcd3943d73aff3e5bfeec68a3a894008a954",
    "semantic_title": "from biased selective labels to pseudo-labels: an expectation-maximization framework for learning from biased decisions",
    "citation_count": 0,
    "authors": [
      "Trenton Chang",
      "Jenna Wiens"
    ]
  },
  "https://proceedings.mlr.press/v235/chanpuriya24a.html": {
    "title": "On the Role of Edge Dependency in Graph Generative Models",
    "volume": "main",
    "abstract": "We investigate the trade-off between the representation power of graph generative models and model overlap, i.e., the degree to which the model generates diverse outputs versus regurgitating its training data. In particular, we delineate a nested hierarchy of graph generative models categorized into three levels of complexity: edge independent, node independent, and arbitrarily dependent models. This hierarchy encapsulates a wide range of prevalent methods. We derive theoretical bounds on the number of triangles and other short-length cycles producible by each level of the hierarchy, finding that more complex dependency structure allows an improved trade-off between representation power and overlap. We provide instances demonstrating the asymptotic optimality of our bounds. Furthermore, we introduce new generative models for each of the three hierarchical levels, leveraging dense subgraph discovery. Our evaluation, conducted on real-world datasets, focuses on assessing the output quality and overlap of our proposed models in comparison to other popular models. Our results indicate that our simple, interpretable models provide competitive baselines to popular generative models. Through this investigation, we offer a structured and robust evaluation scheme, thereby facilitating the development of models capable of generating accurate and edge-diverse graphs",
    "checked": true,
    "id": "98cd956fc985ecf516f53fc9a9091932d9fcd41e",
    "semantic_title": "on the role of edge dependency in graph generative models",
    "citation_count": 0,
    "authors": [
      "Sudhanshu Chanpuriya",
      "Cameron N Musco",
      "Konstantinos Sotiropoulos",
      "Charalampos Tsourakakis"
    ]
  },
  "https://proceedings.mlr.press/v235/chattopadhyay24a.html": {
    "title": "Performance Bounds for Active Binary Testing with Information Maximization",
    "volume": "main",
    "abstract": "In many applications like experimental design, group testing, and medical diagnosis, the state of a random variable $Y$ is revealed by successively observing the outcomes of binary tests about $Y$. New tests are selected adaptively based on the history of outcomes observed so far. If the number of states of $Y$ is finite, the process ends when $Y$ can be predicted with a desired level of confidence or all available tests have been used. Finding the strategy that minimizes the expected number of tests needed to predict $Y$ is virtually impossible in most real applications. Therefore, the commonly used strategy is the greedy heuristic of Information Maximization (InfoMax) that selects tests sequentially in order of information gain. Despite its widespread use, existing guarantees on its performance are often vacuous when compared to its empirical efficiency. In this paper, for the first time to the best of our knowledge, we establish tight non-vacuous bounds on InfoMax's performance. Our analysis is based on the assumption that at any iteration of the greedy strategy, there is always a binary test available whose conditional probability of being 'true', given the history, is within $\\delta$ units of one-half. This assumption is motivated by practical applications where the available set of tests often satisfies this property for modest values of $\\delta$, say, ${0.1 \\leq \\delta \\leq 0.4}$. Specifically, we analyze two distinct scenarios: (i) all tests are functions of $Y$, and (ii) test outcomes are corrupted by a binary symmetric channel. For both cases, our bounds guarantee the near-optimal performance of InfoMax for modest $\\delta$ values. It requires only a small multiplicative factor of the entropy of $Y$, in terms of the average number of tests needed to make accurate predictions",
    "checked": true,
    "id": "45087160896803e1e04a4e835a0e48663e708860",
    "semantic_title": "performance bounds for active binary testing with information maximization",
    "citation_count": 1,
    "authors": [
      "Aditya Chattopadhyay",
      "Benjamin David Haeffele",
      "Rene Vidal",
      "Donald Geman"
    ]
  },
  "https://proceedings.mlr.press/v235/che24a.html": {
    "title": "Target Networks and Over-parameterization Stabilize Off-policy Bootstrapping with Function Approximation",
    "volume": "main",
    "abstract": "We prove that the combination of a target network and over-parameterized linear function approximation establishes a weaker convergence condition for bootstrapped value estimation in certain cases, even with off-policy data. Our condition is naturally satisfied for expected updates over the entire state-action space or learning with a batch of complete trajectories from episodic Markov decision processes. Notably, using only a target network or an over-parameterized model does not provide such a convergence guarantee. Additionally, we extend our results to learning with truncated trajectories, showing that convergence is achievable for all tasks with minor modifications, akin to value truncation for the final states in trajectories. Our primary result focuses on temporal difference estimation for prediction, providing high-probability value estimation error bounds and empirical analysis on Baird's counterexample and a Four-room task. Furthermore, we explore the control setting, demonstrating that similar convergence conditions apply to Q-learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengdi Che",
      "Chenjun Xiao",
      "Jincheng Mei",
      "Bo Dai",
      "Ramki Gummadi",
      "Oscar A Ramirez",
      "Christopher K Harris",
      "A. Rupam Mahmood",
      "Dale Schuurmans"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24a.html": {
    "title": "PlanDQ: Hierarchical Plan Orchestration via D-Conductor and Q-Performer",
    "volume": "main",
    "abstract": "Despite the recent advancements in offline RL, no unified algorithm could achieve superior performance across a broad range of tasks. Offline value function learning, in particular, struggles with sparse-reward, long-horizon tasks due to the difficulty of solving credit assignment and extrapolation errors that accumulates as the horizon of the task grows. On the other hand, models that can perform well in long-horizon tasks are designed specifically for goal-conditioned tasks, which commonly perform worse than value function learning methods on short-horizon, dense-reward scenarios. To bridge this gap, we propose a hierarchical planner designed for offline RL called PlanDQ. PlanDQ incorporates a diffusion-based planner at the high level, named D-Conductor, which guides the low-level policy through sub-goals. At the low level, we used a Q-learning based approach called the Q-Performer to accomplish these sub-goals. Our experimental results suggest that PlanDQ can achieve superior or competitive performance on D4RL continuous control benchmark tasks as well as AntMaze, Kitchen, and Calvin as long-horizon tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Chen",
      "Junyeob Baek",
      "Fei Deng",
      "Kenji Kawaguchi",
      "Caglar Gulcehre",
      "Sungjin Ahn"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24b.html": {
    "title": "How Interpretable Are Interpretable Graph Neural Networks?",
    "volume": "main",
    "abstract": "Interpretable graph neural networks (XGNNs ) are widely adopted in various scientific applications involving graph-structured data. Existing XGNNs predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph. However, the representational properties and limitations of these methods remain inadequately explored. In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs. To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT. We empirically validate our theoretical findings on a number of graph classification benchmarks. The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability across 12 regular and geometric graph benchmarks",
    "checked": true,
    "id": "854342cf063eef4428a5441c8d317dfbabb8117f",
    "semantic_title": "how interpretable are interpretable graph neural networks?",
    "citation_count": 0,
    "authors": [
      "Yongqiang Chen",
      "Yatao Bian",
      "Bo Han",
      "James Cheng"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24c.html": {
    "title": "Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning",
    "volume": "main",
    "abstract": "Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilin Chen",
      "Ruichu Cai",
      "Zeqin Yang",
      "Jie Qiao",
      "Yuguang Yan",
      "Zijian Li",
      "Zhifeng Hao"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24d.html": {
    "title": "Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation",
    "volume": "main",
    "abstract": "We investigate the problem of explainability for machine learning models, focusing on Feature Attribution Methods (FAMs) that evaluate feature importance through perturbation tests. Despite their utility, FAMs struggle to distinguish the contributions of different features, when their prediction changes are similar after perturbation. To enhance FAMs' discriminative power, we introduce Feature Attribution with Necessity and Sufficiency (FANS), which find a neighborhood of the input such that perturbing samples within this neighborhood have a high Probability of being Necessity and Sufficiency (PNS) cause for the change in predictions, and use this PNS as the importance of the feature. Specifically, FANS compute this PNS via a heuristic strategy for estimating the neighborhood and a perturbation test involving two stages (factual and interventional) for counterfactual reasoning. To generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Please refer to the source code via https://github.com/DMIRLAB-Group/FANS",
    "checked": true,
    "id": "428c018f23e0d746b90d5414d4aacd92f002bd99",
    "semantic_title": "feature attribution with necessity and sufficiency via dual-stage perturbation test for causal explanation",
    "citation_count": 1,
    "authors": [
      "Xuexin Chen",
      "Ruichu Cai",
      "Zhengting Huang",
      "Yuxuan Zhu",
      "Julien Horwood",
      "Zhifeng Hao",
      "Zijian Li",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24e.html": {
    "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) are instruction followers but the performance varies under different instructions. It is challenging to create the best instruction, especially for black-box LLMs on which backpropagation is forbidden. Instead of directly optimizing the discrete instruction, we optimize a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM. In each optimization step of the proposed method InstructZero, a soft prompt is converted into an instruction by the open-source LLM, which is then submitted to the black-box LLM for zero-shot evaluation, whose result is sent to Bayesian optimization to produce new soft prompts improving the zero-shot performance. We evaluate InstructZero on different combinations of open-source LLMs and APIs including Vicuna and ChatGPT. InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks",
    "checked": true,
    "id": "d6fb1c21a46fb8b0f3f4383fd467b21e5b58c55f",
    "semantic_title": "instructzero: efficient instruction optimization for black-box large language models",
    "citation_count": 36,
    "authors": [
      "Lichang Chen",
      "Jiuhai Chen",
      "Tom Goldstein",
      "Heng Huang",
      "Tianyi Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24f.html": {
    "title": "MaSS: Multi-attribute Selective Suppression for Utility-preserving Data Transformation from an Information-theoretic Perspective",
    "volume": "main",
    "abstract": "The growing richness of large-scale datasets has been crucial in driving the rapid advancement and wide adoption of machine learning technologies. The massive collection and usage of data, however, pose an increasing risk for people's private and sensitive information due to either inadvertent mishandling or malicious exploitation. Besides legislative solutions, many technical approaches have been proposed towards data privacy protection. However, they bear various limitations such as leading to degraded data availability and utility, or relying on heuristics and lacking solid theoretical bases. To overcome these limitations, we propose a formal information-theoretic definition for this utility-preserving privacy protection problem, and design a data-driven learnable data transformation framework that is capable of selectively suppressing sensitive attributes from target datasets while preserving the other useful attributes, regardless of whether or not they are known in advance or explicitly annotated for preservation. We provide rigorous theoretical analyses on the operational bounds for our framework, and carry out comprehensive experimental evaluations using datasets of a variety of modalities, including facial images, voice audio clips, and human activity motion sensor signals. Results demonstrate the effectiveness and generalizability of our method under various configurations on a multitude of tasks. Our source code is available at this URL",
    "checked": true,
    "id": "b7cb2a1f9879f99ef6b21968254b527e8b23dad7",
    "semantic_title": "mass: multi-attribute selective suppression for utility-preserving data transformation from an information-theoretic perspective",
    "citation_count": 0,
    "authors": [
      "Yizhuo Chen",
      "Chun-Fu Chen",
      "Hsiang Hsu",
      "Shaohan Hu",
      "Marco Pistoia",
      "Tarek F. Abdelzaher"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24g.html": {
    "title": "Policy-conditioned Environment Models are More Generalizable",
    "volume": "main",
    "abstract": "In reinforcement learning, it is crucial to have an accurate environment dynamics model to evaluate different policies' value in downstream tasks like offline policy optimization and policy evaluation. However, the learned model is known to be inaccurate in predictions when evaluating target policies different from data-collection policies. In this work, we found that utilizing policy representation for model learning, called policy-conditioned model (PCM) learning, is useful to mitigate the problem, especially when the offline dataset is collected from diversified behavior policies. The reason beyond that is in this case, PCM becomes a meta-dynamics model that is trained to be aware of and focus on the evaluation policies that on-the-fly adjust the model to be suitable to the evaluation policies' state-action distribution, thus improving the prediction accuracy. Based on that intuition, we propose an easy-to-implement yet effective algorithm of PCM for accurate model learning. We also give a theoretical analysis and experimental evidence to demonstrate the feasibility of reducing value gaps by adapting the dynamics model under different policies. Experiment results show that PCM outperforms the existing SOTA off-policy evaluation methods in the DOPE benchmark by a large margin, and derives significantly better policies in offline policy selection and model predictive control compared with the standard model learning method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruifeng Chen",
      "Xiong-Hui Chen",
      "Yihao Sun",
      "Siyuan Xiao",
      "Minhui Li",
      "Yang Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24h.html": {
    "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, a closer examination reveals persistent challenges in the evaluative capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: https://mllm-judge.github.io/",
    "checked": true,
    "id": "1036f6dfe75af06fbcdb3447dbe9be8613bf857c",
    "semantic_title": "mllm-as-a-judge: assessing multimodal llm-as-a-judge with vision-language benchmark",
    "citation_count": 25,
    "authors": [
      "Dongping Chen",
      "Ruoxi Chen",
      "Shilin Zhang",
      "Yaochen Wang",
      "Yinuo Liu",
      "Huichi Zhou",
      "Qihui Zhang",
      "Yao Wan",
      "Pan Zhou",
      "Lichao Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24i.html": {
    "title": "Premise Order Matters in Reasoning with Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that even if the model performance is decent on the optimal order, permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyun Chen",
      "Ryan Andrew Chi",
      "Xuezhi Wang",
      "Denny Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24j.html": {
    "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
    "volume": "main",
    "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiang Chen",
      "Yihe Deng",
      "Huizhuo Yuan",
      "Kaixuan Ji",
      "Quanquan Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24k.html": {
    "title": "Robust Classification via a Single Diffusion Model",
    "volume": "main",
    "abstract": "Diffusion models have been applied to improve adversarial robustness of image classifiers by purifying the adversarial noises or generating realistic data for adversarial training. However, diffusion-based purification can be evaded by stronger adaptive attacks while adversarial training does not perform well under unseen threats, exhibiting inevitable limitations of these methods. To better harness the expressive power of diffusion models, this paper proposes Robust Diffusion Classifier (RDC), a generative classifier that is constructed from a pre-trained diffusion model to be adversarially robust. RDC first maximizes the data likelihood of a given input and then predicts the class probabilities of the optimized input using the conditional likelihood estimated by the diffusion model through Bayes' theorem. To further reduce the computational cost, we propose a new diffusion backbone called multi-head diffusion and develop efficient sampling strategies. As RDC does not require training on particular adversarial attacks, we demonstrate that it is more generalizable to defend against multiple unseen threats. In particular, RDC achieves $75.67%$ robust accuracy against various $\\ell_\\infty$ norm-bounded adaptive attacks with $\\epsilon_\\infty=8/255$ on CIFAR-10, surpassing the previous state-of-the-art adversarial training models by $+4.77%$. The results highlight the potential of generative classifiers by employing pre-trained diffusion models for adversarial robustness compared with the commonly studied discriminative classifiers",
    "checked": true,
    "id": "762b42dd957071dd7f5add9435d29edb3b4cbefc",
    "semantic_title": "robust classification via a single diffusion model",
    "citation_count": 34,
    "authors": [
      "Huanran Chen",
      "Yinpeng Dong",
      "Zhengyi Wang",
      "Xiao Yang",
      "Chengqi Duan",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24l.html": {
    "title": "Relational Learning in Pre-Trained Models: A Theory from Hypergraph Recovery Perspective",
    "volume": "main",
    "abstract": "Foundation Models (FMs) have demonstrated remarkable insights into the relational dynamics of the world, leading to the crucial question: how do these models acquire an understanding of world hybrid relations? Traditional statistical learning, particularly for prediction problems, may overlook the rich and inherently structured information from the data, especially regarding the relationships between objects. We introduce a mathematical model that formalizes relational learning as hypergraph recovery to study pre-training of FMs. In our framework, the world is represented as a hypergraph, with data abstracted as random samples from hyperedges. We theoretically examine the feasibility of a Pre-Trained Model (PTM) to recover this hypergraph and analyze the data efficiency in a minimax near-optimal style. By integrating rich graph theories into the realm of PTMs, our mathematical framework offers powerful tools for an in-depth understanding of pre-training from a unique perspective and can be used under various scenarios. As an example, we extend the framework to entity alignment in multimodal learning",
    "checked": true,
    "id": "65eccb6d41024968bdc52efcae49c70b5d022140",
    "semantic_title": "relational learning in pre-trained models: a theory from hypergraph recovery perspective",
    "citation_count": 0,
    "authors": [
      "Yang Chen",
      "Cong Fang",
      "Zhouchen Lin",
      "Bing Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24m.html": {
    "title": "Towards AutoAI: Optimizing a Machine Learning System with Black-box and Differentiable Components",
    "volume": "main",
    "abstract": "Machine learning (ML) models in the real world typically do not exist in isolation. They are usually part of a complex system (e.g., healthcare systems, self-driving cars) containing multiple ML and black-box components. The problem of optimizing such systems, which we refer to as automated AI (AutoAI), requires us to jointly train all ML components together and presents a significant challenge because the number of system parameters is extremely high and the system has no analytical form. To circumvent this, we introduce a novel algorithm called A-BAD-BO which uses each ML component's local loss as an auxiliary indicator for system performance. A-BAD-BO uses Bayesian optimization (BO) to optimize the local loss configuration of a system in a smaller dimensional space and exploits the differentiable structure of ML components to recover optimal system parameters from the optimized configuration. We show A-BAD-BO converges to optimal system parameters by showing that it is asymptotically no regret. We use A-BAD-BO to optimize several synthetic and real-world complex systems, including a prompt engineering pipeline for large language models containing millions of system parameters. Our results demonstrate that A-BAD-BO yields better system optimality than gradient-driven baselines and is more sample-efficient than pure BO algorithms",
    "checked": true,
    "id": "fbd613dfe27c49e0f3cce722e6fac38250f39518",
    "semantic_title": "towards autoai: optimizing a machine learning system with black-box and differentiable components",
    "citation_count": 0,
    "authors": [
      "Zhiliang Chen",
      "Chuan-Sheng Foo",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24n.html": {
    "title": "Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes",
    "volume": "main",
    "abstract": "We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be learned efficiently by square loss regression over the time-series data. We show that the drift and the diffusion coefficients of this SDE can be adjusted after training, and that a specific choice that minimizes the impact of the estimation error gives a Föllmer process. We highlight the utility of our approach on several complex, high-dimensional forecasting problems, including stochastically forced Navier-Stokes and video prediction on the KTH and CLEVRER datasets. The code is available at https://github.com/interpolants/forecasting",
    "checked": true,
    "id": "22296ea460bd88c1b331ac95f046715b71e6a0ca",
    "semantic_title": "probabilistic forecasting with stochastic interpolants and föllmer processes",
    "citation_count": 8,
    "authors": [
      "Yifan Chen",
      "Mark Goldstein",
      "Mengjian Hua",
      "Michael Samuel Albergo",
      "Nicholas Matthew Boffi",
      "Eric Vanden-Eijnden"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24o.html": {
    "title": "CogDPM: Diffusion Probabilistic Models via Cognitive Predictive Coding",
    "volume": "main",
    "abstract": "Predictive Coding (PC) is a theoretical framework in cognitive science suggesting that the human brain processes cognition through spatiotemporal prediction of visual world. Existing studies have developed spatiotemporal prediction neural networks based on the PC theroy, emulating its two core mechanisms: Correcting predictions from residuals and Hierarchical learning. However, these models do not show the enhancement of prediction skills on real-world forecasting tasks, and ignore the Precision Weighting mechanism of PC theory. Precision weight posits that the brain allocates more attention to signals with lower Precision, contributing to the the cognitive ability of human brains. This work introduces the Cognitive Diffusion Probabilistic Models (CogDPM) which demonstrates the connection between diffusion probabilistic models and PC theory. CogDPM features a precision estimation method based on the hierarchical sampling capabilities of diffusion models, and allocate the guidance with precision weights estimated by the inherent property of diffusion models. We experimentally show that the precision weights is an estimator of model's predictability on the rigid body and fluid motion dataset. We also apply CogDPM to real-world prediction tasks using the U.K. precipitation and ERA surface wind datasets. Our results demonstrate that CogDPM outperforms both existing domain-specific operational models and general deep prediction models in providing more proficient forecasting",
    "checked": true,
    "id": "4b94f642ed62081f3e4dee67fa805c02524c892c",
    "semantic_title": "cogdpm: diffusion probabilistic models via cognitive predictive coding",
    "citation_count": 0,
    "authors": [
      "Kaiyuan Chen",
      "Xingzhuo Guo",
      "Yu Zhang",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24p.html": {
    "title": "On Interpolating Experts and Multi-Armed Bandits",
    "volume": "main",
    "abstract": "Learning with expert advice and multi-armed bandit are two classic online decision problems which differ on how the information is observed in each round of the game. We study a family of problems interpolating the two. For a vector $\\mathbf{m}=(m_1,…,m_K)\\in \\mathbb N^K$, an instance of $\\mathbf m$-MAB indicates that the arms are partitioned into $K$ groups and the $i$-th group contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same group are observed. We prove tight minimax regret bounds for $\\mathbf m$-MAB and design an optimal PAC algorithm for its pure exploration version, $\\mathbf m$-BAI, where the goal is to identify the arm with minimum loss with as few rounds as possible. We show that the minimax regret of $\\mathbf m$-MAB is $\\Theta\\left(\\sqrt{T\\sum_{k=1}^K\\log (m_k+1)}\\right)$ and the minimum number of pulls for an $(\\varepsilon,0.05)$-PAC algorithm of $\\mathbf m$-BAI is $\\Theta\\left(\\frac{1}{\\varepsilon^2}\\cdot \\sum_{k=1}^K\\log (m_k+1)\\right)$. Both our upper bounds and lower bounds for $\\mathbf m$-MAB can be extended to a more general setting, namely the bandit with graph feedback, in terms of the clique cover and related graph parameters. As consequences, we obtained tight minimax regret bounds for several families of feedback graphs",
    "checked": true,
    "id": "9ba58ae11b0f566a9e479a4d4d52fc1c330191d8",
    "semantic_title": "on interpolating experts and multi-armed bandits",
    "citation_count": 2,
    "authors": [
      "Houshuang Chen",
      "Yuchen He",
      "Chihao Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24q.html": {
    "title": "Bagged Deep Image Prior for Recovering Images in the Presence of Speckle Noise",
    "volume": "main",
    "abstract": "We investigate both the theoretical and algorithmic aspects of likelihood-based methods for recovering a complex-valued signal from multiple sets of measurements, referred to as looks, affected by speckle (multiplicative) noise. Our theoretical contributions include establishing the first existing theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our theoretical results capture the dependence of MSE upon the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we introduce the concept of bagged Deep Image Priors (Bagged-DIP) and integrate them with projected gradient descent. Furthermore, we show how employing Newton-Schulz algorithm for calculating matrix inverses within the iterations of PGD reduces the computational complexity of the algorithm. We will show that this method achieves the state-of-the-art performance",
    "checked": true,
    "id": "5027af7d45ae01c023b6713985a3a1a1789f80a7",
    "semantic_title": "bagged deep image prior for recovering images in the presence of speckle noise",
    "citation_count": 1,
    "authors": [
      "Xi Chen",
      "Zhewen Hou",
      "Christopher Metzler",
      "Arian Maleki",
      "Shirin Jalali"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24r.html": {
    "title": "Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers",
    "volume": "main",
    "abstract": "In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms",
    "checked": true,
    "id": "a6d3ca7bd056a16d879fad619c2fd0874ff2536f",
    "semantic_title": "exact conversion of in-context learning to model weights in linearized-attention transformers",
    "citation_count": 0,
    "authors": [
      "Brian K Chen",
      "Tianyang Hu",
      "Hui Jin",
      "Hwee Kuan Lee",
      "Kenji Kawaguchi"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24s.html": {
    "title": "Accelerated Policy Gradient for s-rectangular Robust MDPs with Large State Spaces",
    "volume": "main",
    "abstract": "Robust Markov decision process (robust MDP) is an important machine learning framework to make a reliable policy that is robust to environmental perturbation. Despite empirical success and popularity of policy gradient methods, existing policy gradient methods require at least iteration complexity $\\mathcal{O}(\\epsilon^{-4})$ to converge to the global optimal solution of s-rectangular robust MDPs with $\\epsilon$-accuracy and are limited to deterministic setting with access to exact gradients and small state space that are impractical in many applications. In this work, we propose an accelerated policy gradient algorithm with iteration complexity $\\mathcal{O}(\\epsilon^{-3}\\ln\\epsilon^{-1})$ in the deterministic setting using entropy regularization. Furthermore, we extend this algorithm to stochastic setting with access to only stochastic gradients and large state space which achieves the sample complexity $\\mathcal{O}(\\epsilon^{-7}\\ln\\epsilon^{-1})$. In the meantime, our algorithms are also the first scalable policy gradient methods to entropy-regularized robust MDPs, which provide an important but underexplored machine learning framework",
    "checked": true,
    "id": "6a9b10dfa71a85ba1e20b55dabe6ab3809aaeb85",
    "semantic_title": "accelerated policy gradient for s-rectangular robust mdps with large state spaces",
    "citation_count": 0,
    "authors": [
      "Ziyi Chen",
      "Heng Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24t.html": {
    "title": "Accelerated Policy Gradient: On the Convergence Rates of the Nesterov Momentum for Reinforcement Learning",
    "volume": "main",
    "abstract": "Various acceleration approaches for Policy Gradient (PG) have been analyzed within the realm of Reinforcement Learning (RL). However, the theoretical understanding of the widely used momentum-based acceleration method on PG remains largely open. In response to this gap, we adapt the celebrated Nesterov's accelerated gradient (NAG) method to policy optimization in RL, termed Accelerated Policy Gradient (APG). To demonstrate the potential of APG in achieving fast convergence, we formally prove that with the true gradient and under the softmax policy parametrization, APG converges to an optimal policy at rates: (i) $\\tilde{O}(1/t^2)$ with nearly constant step sizes; (ii) $O(e^{-ct})$ with time-varying step sizes. To the best of our knowledge, this is the first characterization of the convergence rates of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the parameter initialization, APG ends up entering a locally nearly-concave regime, where APG can significantly benefit from the momentum, within finite iterations. Through numerical validation and experiments on the Atari 2600 benchmarks, we confirm that APG exhibits a $\\tilde{O}(1/t^2)$ rate with nearly constant step sizes and a linear convergence rate with time-varying step sizes, significantly improving convergence over the standard PG",
    "checked": true,
    "id": "1cb9ecfadd9e34604ac5266700c4c14a89d2caf5",
    "semantic_title": "accelerated policy gradient: on the convergence rates of the nesterov momentum for reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Yen-Ju Chen",
      "Nai-Chieh Huang",
      "Ching-Pei Lee",
      "Ping-Chun Hsieh"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24u.html": {
    "title": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language Models with Pinpoint Tuning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) tend to prioritize adherence to user prompts over providing veracious responses, leading to the sycophancy issue. When challenged by users, LLMs tend to admit mistakes and provide inaccurate responses even if they initially provided the correct answer. Recent works propose to employ supervised fine-tuning (SFT) to mitigate the sycophancy issue, while it typically leads to the degeneration of LLMs' general capability. To address the challenge, we propose a novel supervised pinpoint tuning (SPT), where the region-of-interest modules are tuned for a given objective. Specifically, SPT first reveals and verifies a small percentage ($<$5%) of the basic modules, which significantly affect a particular behavior of LLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified modules while freezing the rest. To verify the effectiveness of the proposed SPT, we conduct comprehensive experiments, demonstrating that SPT significantly mitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT introduces limited or even no side effects on the general capability of LLMs. Our results shed light on how to precisely, effectively, and efficiently explain and improve the targeted ability of LLMs",
    "checked": true,
    "id": "f681a03a43885e696675825917baff9203d7b90c",
    "semantic_title": "from yes-men to truth-tellers: addressing sycophancy in large language models with pinpoint tuning",
    "citation_count": 1,
    "authors": [
      "Wei Chen",
      "Zhen Huang",
      "Liang Xie",
      "Binbin Lin",
      "Houqiang Li",
      "Le Lu",
      "Xinmei Tian",
      "Deng Cai",
      "Yonggang Zhang",
      "Wenxiao Wang",
      "Xu Shen",
      "Jieping Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24v.html": {
    "title": "Improved Communication-Privacy Trade-offs in $L_2$ Mean Estimation under Streaming Differential Privacy",
    "volume": "main",
    "abstract": "We study $L_2$ mean estimation under central differential privacy and communication constraints, and address two key challenges: firstly, existing mean estimation schemes that simultaneously handle both constraints are usually optimized for $L_\\infty$ geometry and rely on random rotation or Kashin's representation to adapt to $L_2$ geometry, resulting in suboptimal leading constants in mean square errors (MSEs); secondly, schemes achieving order-optimal communication-privacy trade-offs do not extend seamlessly to streaming differential privacy (DP) settings (e.g., tree aggregation or matrix factorization), rendering them incompatible with DP-FTRL type optimizers. In this work, we tackle these issues by introducing a novel privacy accounting method for the sparsified Gaussian mechanism that incorporates the randomness inherent in sparsification into the DP noise. Unlike previous approaches, our accounting algorithm directly operates in $L_2$ geometry, yielding MSEs that fast converge to those of the uncompressed Gaussian mechanism. Additionally, we extend the sparsification scheme to the matrix factorization framework under streaming DP and provide a precise accountant tailored for DP-FTRL type optimizers. Empirically, our method demonstrates at least a 100x improvement of compression for DP-SGD across various FL tasks",
    "checked": false,
    "id": "80f33e2da153d56ba496f1075912a80b8df84f2c",
    "semantic_title": "improved communication-privacy trade-offs in l2 mean estimation under streaming differential privacy",
    "citation_count": 0,
    "authors": [
      "Wei-Ning Chen",
      "Berivan Isik",
      "Peter Kairouz",
      "Albert No",
      "Sewoong Oh",
      "Zheng Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24w.html": {
    "title": "Offline Transition Modeling via Contrastive Energy Learning",
    "volume": "main",
    "abstract": "Learning a high-quality transition model is of great importance for sequential decision-making tasks, especially in offline settings. Nevertheless, the complex behaviors of transition dynamics in real-world environments pose challenges for the standard forward models because of their inductive bias towards smooth regressors, conflicting with the inherent nature of transitions such as discontinuity or large curvature. In this work, we propose to model the transition probability implicitly through a scalar-value energy function, which enables not only flexible distribution prediction but also capturing complex transition behaviors. The Energy-based Transition Models (ETM) are shown to accurately fit the discontinuous transition functions and better generalize to out-of-distribution transition data. Furthermore, we demonstrate that energy-based transition models improve the evaluation accuracy and significantly outperform other off-policy evaluation methods in DOPE benchmark. Finally, we show that energy-based transition models also benefit reinforcement learning and outperform prior offline RL algorithms in D4RL Gym-Mujoco tasks",
    "checked": true,
    "id": "054d4ab6d204b4a71c1a5437d86a7413811f68fd",
    "semantic_title": "offline transition modeling via contrastive energy learning",
    "citation_count": 0,
    "authors": [
      "Ruifeng Chen",
      "Chengxing Jia",
      "Zefang Huang",
      "Tian-Shuo Liu",
      "Xu-Hui Liu",
      "Yang Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24x.html": {
    "title": "Efficient Pareto Manifold Learning with Low-Rank Structure",
    "volume": "main",
    "abstract": "Multi-task learning, which optimizes performance across multiple tasks, is inherently a multi-objective optimization problem. Various algorithms are developed to provide discrete trade-off solutions on the Pareto front. Recently, continuous Pareto front approximations using a linear combination of base networks have emerged as a compelling strategy. However, it suffers from scalability issues when the number of tasks is large. To address this issue, we propose a novel approach that integrates a main network with several low-rank matrices to efficiently learn the Pareto manifold. It significantly reduces the number of parameters and facilitates the extraction of shared features. We also introduce orthogonal regularization to further bolster performance. Extensive experimental results demonstrate that the proposed approach outperforms state-of-the-art baselines, especially on datasets with a large number of tasks",
    "checked": true,
    "id": "767f44cb7f1de56c09a248c4c75c32a1cd082da8",
    "semantic_title": "efficient pareto manifold learning with low-rank structure",
    "citation_count": 1,
    "authors": [
      "Weiyu Chen",
      "James Kwok"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24y.html": {
    "title": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback",
    "volume": "main",
    "abstract": "Large language models (LLMs) have been routinely used to solve various tasks using step-by-step reasoning. However, the structure of intermediate reasoning steps, or thoughts, is rigid and unidirectional, such as chains, trees, or acyclic-directed graphs. Consequently, the resulting inflexible and forward-only reasoning may not address challenging tasks and fail when the LLM frequently gives false responses, i.e., hallucinations. This paper proposes a new reasoning framework, called Thought Rollback (TR), allowing LLMs to adaptively build thought structure while maintaining effective reasoning toward problem-solving under hallucinations. The core mechanism of TR is rolling back thoughts, which allows LLMs to perform error analysis on thoughts, and thus roll back to any previously mistaken thought for revision. Subsequently, by including such trial-and-error in the prompt to guide the LLM, each rollback leads to one more reliable reasoning path. Therefore, starting with a simple prompt without human annotations, LLM with TR adaptively and gradually explores thoughts for a correct solution. Comprehensive experiments on mathematical problems and multi-task reasoning demonstrate the state-of-the-art performance of TR in terms of problem-solving rate and interaction cost. For instance, the solving rate of GPT-4 with TR outperforms the current best by $9%$ on the MATH dataset. The source code is available under the folder examples/ThoughtRollback of https://github.com/iQua/llmpebase",
    "checked": true,
    "id": "3d5705323b62d28f83805c42d8b7dd87ebc5e599",
    "semantic_title": "toward adaptive reasoning in large language models with thought rollback",
    "citation_count": 2,
    "authors": [
      "Sijia Chen",
      "Baochun Li"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24z.html": {
    "title": "Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank",
    "volume": "main",
    "abstract": "Unbiased Learning to Rank (ULTR) aims to train unbiased ranking models from biased click logs, by explicitly modeling a generation process for user behavior and fitting click data based on examination hypothesis. Previous research found empirically that the true latent relevance is mostly recoverable through click fitting. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. This research investigates the conditions under which relevance can be recovered from click data in the first principle. We initially characterize a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, a criterion sufficient for the pairwise ranking objective. Subsequently, we investigate an equivalent condition for identifiability, articulated as a graph connectivity test problem: the recovery of relevance is feasible if and only if the identifiability graph (IG), derived from the underlying structure of the dataset, is connected. The presence of a disconnected IG may lead to degenerate cases and suboptimal ranking performance. To tackle this challenge, we introduce two methods, namely node intervention and node merging, designed to modify the dataset and restore the connectivity of the IG. Empirical results derived from a simulated dataset and two real-world LTR benchmark datasets not only validate our proposed theory, but also demonstrate the effectiveness of our methods in alleviating data bias when the relevance model is unidentifiable",
    "checked": true,
    "id": "b432794e96e57bfbd899f2e2cbb5428eeb096a79",
    "semantic_title": "identifiability matters: revealing the hidden recoverable condition in unbiased learning to rank",
    "citation_count": 2,
    "authors": [
      "Mouxiang Chen",
      "Chenghao Liu",
      "Zemin Liu",
      "Zhuo Li",
      "Jianling Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24aa.html": {
    "title": "High-Dimensional Kernel Methods under Covariate Shift: Data-Dependent Implicit Regularization",
    "volume": "main",
    "abstract": "This paper studies kernel ridge regression in high dimensions under covariate shifts and analyzes the role of importance re-weighting. We first derive the asymptotic expansion of high dimensional kernels under covariate shifts. By a bias-variance decomposition, we theoretically demonstrate that the re-weighting strategy allows for decreasing the variance. For bias, we analyze the regularization of the arbitrary or well-chosen scale, showing that the bias can behave very differently under different regularization scales. In our analysis, the bias and variance can be characterized by the spectral decay of a data-dependent regularized kernel: the original kernel matrix associated with an additional re-weighting matrix, and thus the re-weighting strategy can be regarded as a data-dependent regularization for better understanding. Besides, our analysis provides asymptotic expansion of kernel functions/vectors under covariate shift, which has its own interest",
    "checked": true,
    "id": "2a761cc6b99aa80b81ebdffcd05be5e9ff61abd1",
    "semantic_title": "high-dimensional kernel methods under covariate shift: data-dependent implicit regularization",
    "citation_count": 0,
    "authors": [
      "Yihang Chen",
      "Fanghui Liu",
      "Taiji Suzuki",
      "Volkan Cevher"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ab.html": {
    "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
    "volume": "main",
    "abstract": "In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced training costs and much faster inference speeds. Our DiJiang-7B achieves comparable performance with LLaMA2-7B on various benchmark while requires only about 1/50 training cost. Code is available at https://github.com/YuchuanTian/DiJiang",
    "checked": true,
    "id": "ef8846c0b9eb9e915d44e18bf06fda51f9b5e2fa",
    "semantic_title": "dijiang: efficient large language models through compact kernelization",
    "citation_count": 2,
    "authors": [
      "Hanting Chen",
      "Liu Zhicheng",
      "Xutao Wang",
      "Yuchuan Tian",
      "Yunhe Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ac.html": {
    "title": "GeoMFormer: A General Architecture for Geometric Molecular Representation Learning",
    "volume": "main",
    "abstract": "Molecular modeling, a central topic in quantum mechanics, aims to accurately calculate the properties and simulate the behaviors of molecular systems. The molecular model is governed by physical laws, which impose geometric constraints such as invariance and equivariance to coordinate rotation and translation. While numerous deep learning approaches have been developed to learn molecular representations under these constraints, most of them are built upon heuristic and costly modules. We argue that there is a strong need for a general and flexible framework for learning both invariant and equivariant features. In this work, we introduce a novel Transformer-based molecular model called GeoMFormer to achieve this goal. Using the standard Transformer modules, two separate streams are developed to maintain and learn invariant and equivariant representations. Carefully designed cross-attention modules bridge the two streams, allowing information fusion and enhancing geometric modeling in each stream. As a general and flexible architecture, we show that many previous architectures can be viewed as special instantiations of GeoMFormer. Extensive experiments are conducted to demonstrate the power of GeoMFormer. All empirical results show that GeoMFormer achieves strong performance on both invariant and equivariant tasks of different types and scales. Code and models will be made publicly available at https://github.com/c-tl/GeoMFormer",
    "checked": true,
    "id": "8751f5b1394f38b19893f668c907c5c74703c853",
    "semantic_title": "geomformer: a general architecture for geometric molecular representation learning",
    "citation_count": 4,
    "authors": [
      "Tianlang Chen",
      "Shengjie Luo",
      "Di He",
      "Shuxin Zheng",
      "Tie-Yan Liu",
      "Liwei Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ad.html": {
    "title": "TENG: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets Toward Machine Precision",
    "volume": "main",
    "abstract": "Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering. The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems. In this paper, we introduce the Time-Evolving Natural Gradient (TENG), generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions. Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency. TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving machine precision in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Chen",
      "Jacob Mccarran",
      "Esteban Vizcaino",
      "Marin Soljacic",
      "Di Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ae.html": {
    "title": "EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism",
    "volume": "main",
    "abstract": "We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared to standard LLM training, as well as outstanding inference speedup without compromising output quality. To facilitate further research and adoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM",
    "checked": true,
    "id": "89d3d19ad717cd534bdd1866d28e2da253147705",
    "semantic_title": "ee-llm: large-scale training and inference of early-exit large language models with 3d parallelism",
    "citation_count": 15,
    "authors": [
      "Yanxi Chen",
      "Xuchen Pan",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24af.html": {
    "title": "TimeMIL: Advancing Multivariate Time Series Classification via a Time-aware Multiple Instance Learning",
    "volume": "main",
    "abstract": "Deep neural networks, including transformers and convolutional neural networks (CNNs), have significantly improved multivariate time series classification (MTSC). However, these methods often rely on supervised learning, which does not fully account for the sparsity and locality of patterns in time series data (e.g., quantification of diseases-related anomalous points in ECG and abnormal detection in signal). To address this challenge, we formally discuss and reformulate MTSC as a weakly supervised problem, introducing a novel multiple-instance learning (MIL) framework for better localization of patterns of interest and modeling time dependencies within time series. Our novel approach, TimeMIL, formulates the temporal correlation and ordering within a time-aware MIL pooling, leveraging a tokenized transformer with a specialized learnable wavelet positional token. The proposed method surpassed 26 recent state-of-the-art MTSC methods, underscoring the effectiveness of the weakly supervised TimeMIL in MTSC. The code is available https://github.com/xiwenc1/TimeMIL",
    "checked": true,
    "id": "e081cca01598a5a6f97cbaea71928294f2d6dfdf",
    "semantic_title": "timemil: advancing multivariate time series classification via a time-aware multiple instance learning",
    "citation_count": 1,
    "authors": [
      "Xiwen Chen",
      "Peijie Qiu",
      "Wenhui Zhu",
      "Huayu Li",
      "Hao Wang",
      "Aristeidis Sotiras",
      "Yalin Wang",
      "Abolfazl Razi"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ag.html": {
    "title": "AegisFL: Efficient and Flexible Privacy-Preserving Byzantine-Robust Cross-silo Federated Learning",
    "volume": "main",
    "abstract": "Privacy attacks and poisoning attacks are two of the thorniest problems in federation learning (FL). Homomorphic encryption (HE), which allows certain mathematical operations to be done in the ciphertext state, provides a way to solve these two problems simultaneously. However, existing Paillier-based and CKKS-based privacy-preserving byzantine-robust FL (PBFL) solutions not only suffer from low efficiency but also expose the final model to the server. Additionally, these methods are limited to one robust aggregation algorithm (AGR) and are therefore vulnerable to AGR-tailored poisoning attacks. In this paper, we present AegisFL, an efficient PBLF system that provides the flexibility to change the AGR. We first observe that the core of the existing advanced AGRs is to calculate the inner products, $L_2$ norms and mean values for vectors. Based on this observation, we tailor a packing scheme for PBFL, which fits perfectly with RLWE-based fully homomorphic encryption. Under this packing scheme, the server only needs to perform one ciphertext multiplication to construct any required AGR, while the global model only belongs to honest clients. Finally, we conduct extensive experiments on different datasets and adversary settings, which also confirm the effectiveness and efficiency of our scheme",
    "checked": true,
    "id": "d54cd90ebea10b6de42ea38cde4d9956391eea0c",
    "semantic_title": "aegisfl: efficient and flexible privacy-preserving byzantine-robust cross-silo federated learning",
    "citation_count": 0,
    "authors": [
      "Dong Chen",
      "Hongyuan Qu",
      "Guangwu Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ah.html": {
    "title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
    "volume": "main",
    "abstract": "Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers. Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers. We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency – an inference technique that relies on model diversity",
    "checked": true,
    "id": "8b5bdbbe470913e828aaa703ffbc3c02af698654",
    "semantic_title": "magdi: structured distillation of multi-agent interaction graphs improves reasoning in smaller language models",
    "citation_count": 5,
    "authors": [
      "Justin Chen",
      "Swarnadeep Saha",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ai.html": {
    "title": "CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process",
    "volume": "main",
    "abstract": "Identifying the underlying time-delayed latent causal processes in sequential data is vital for grasping temporal dynamics and making downstream reasoning. While some recent methods can robustly identify these latent causal variables, they rely on strict assumptions about the invertible generation process from latent variables to observed data. However, these assumptions are often hard to satisfy in real-world applications containing information loss. For instance, the visual perception process translates a 3D space into 2D images, or the phenomenon of persistence of vision incorporates historical data into current perceptions. To address this challenge, we establish an identifiability theory that allows for the recovery of independent latent components even when they come from a nonlinear and non-invertible mix. Using this theory as a foundation, we propose a principled approach, CaRiNG, to learn the Causal Representation of Non-invertible Generative temporal data with identifiability guarantees. Specifically, we utilize temporal context to recover lost latent information and apply the conditions in our theory to guide the training process. Through experiments conducted on synthetic datasets, we validate that our CaRiNG method reliably identifies the causal process, even when the generation process is non-invertible. Moreover, we demonstrate that our approach considerably improves temporal understanding and reasoning in practical applications",
    "checked": true,
    "id": "d1e58774877071848dac227c08eb65843c190beb",
    "semantic_title": "caring: learning temporal causal representation under non-invertible generation process",
    "citation_count": 3,
    "authors": [
      "Guangyi Chen",
      "Yifan Shen",
      "Zhenhao Chen",
      "Xiangchen Song",
      "Yuewen Sun",
      "Weiran Yao",
      "Xiao Liu",
      "Kun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24aj.html": {
    "title": "GRATH: Gradual Self-Truthifying for Large Language Models",
    "volume": "main",
    "abstract": "Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate pairwise truthfulness training data with each pair containing a question and its correct and incorrect answers, and then optimizes the model via direct preference optimization (DPO) to learn from the truthfulness difference between answer pairs. GRATH iteratively refines truthfulness data and updates the model, leading to a gradual improvement in model truthfulness in a self-supervised manner. Empirically, we evaluate GRATH using different 7B-LLMs and compare with LLMs with similar or even larger sizes on benchmark datasets. Our results show that GRATH effectively improves LLMs' truthfulness without compromising other core capabilities. Notably, GRATH achieves state-of-the-art performance on TruthfulQA, with MC1 accuracy of 54.71% and MC2 accuracy of 69.10%, which even surpass those on 70B-LLMs. The code is available at https://github.com/chenweixin107/GRATH",
    "checked": true,
    "id": "6d9d552af11f333b56158b4c4a3ccc236820eca1",
    "semantic_title": "grath: gradual self-truthifying for large language models",
    "citation_count": 4,
    "authors": [
      "Weixin Chen",
      "Dawn Song",
      "Bo Li"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ak.html": {
    "title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts",
    "volume": "main",
    "abstract": "Recent successes suggest that parameter-efficient fine-tuning of foundation models is becoming the state-of-the-art method for transfer learning in vision, gradually replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-distribution (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-distribution and out-of-distribution generalization. Our code and models are publicly available",
    "checked": true,
    "id": "5c5adfbeb53b2e7668e60b4dd9612d671e707f81",
    "semantic_title": "unleashing the power of meta-tuning for few-shot generalization through sparse interpolated experts",
    "citation_count": 1,
    "authors": [
      "Shengzhuang Chen",
      "Jihoon Tack",
      "Yunqiao Yang",
      "Yee Whye Teh",
      "Jonathan Richard Schwarz",
      "Ying Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24al.html": {
    "title": "Performative Prediction with Bandit Feedback: Learning through Reparameterization",
    "volume": "main",
    "abstract": "Performative prediction, as introduced by Perdomo et al., is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work in this field usually hinges on three assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, that the mapping from the model to the data distribution is known to the model designer in advance, and the first-order information of the performative risk is available. In this paper, we initiate the study of performative prediction problems that do not require these assumptions. Specifically, we develop a parameterization framework that parametrizes the performative prediction objective as a function of the induced data distribution. We also develop a two-level zeroth-order optimization procedure, where the first level performs iterative optimization on the distribution parameter space, and the second level learns the model that induced a particular target distribution parameter at each iteration. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and achieve provable regret guarantees. In particular, we provide a regret bound that is sublinear in the total number of performative samples taken and is only polynomial in the dimension of the model parameter",
    "checked": true,
    "id": "4231512270369a9795f7ff20515eb2c9e2251ea7",
    "semantic_title": "performative prediction with bandit feedback: learning through reparameterization",
    "citation_count": 3,
    "authors": [
      "Yatong Chen",
      "Wei Tang",
      "Chien-Ju Ho",
      "Yang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24am.html": {
    "title": "Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes",
    "volume": "main",
    "abstract": "While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteriors can be based on the inversion of a diagonal matrix containing singular values, contributing to a reduction in time complexity; iii) an evidence lower bound is derived so that variational parameters and network weights can be optimized with it. Experiments verify our excellent performances and efficiency on in-distribution, distribution-shift and out-of-distribution benchmarks",
    "checked": true,
    "id": "b0e6147369d84fd5e369d5343977b1070244ad4a",
    "semantic_title": "self-attention through kernel-eigen pair sparse variational gaussian processes",
    "citation_count": 1,
    "authors": [
      "Yingyi Chen",
      "Qinghua Tao",
      "Francesco Tonin",
      "Johan Suykens"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24an.html": {
    "title": "Recovering Labels from Local Updates in Federated Learning",
    "volume": "main",
    "abstract": "Gradient inversion (GI) attacks present a threat to the privacy of clients in federated learning (FL) by aiming to enable reconstruction of the clients' data from communicated model updates. A number of such techniques attempts to accelerate data recovery by first reconstructing labels of the samples used in local training. However, existing label extraction methods make strong assumptions that typically do not hold in realistic FL settings. In this paper we present a novel label recovery scheme, Recovering Labels from Local Updates (RLU), which provides near-perfect accuracy when attacking untrained (most vulnerable) models. More significantly, RLU achieves high performance even in realistic real-world settings where the clients in an FL system run multiple local epochs, train on heterogeneous data, and deploy various optimizers to minimize different objective functions. Specifically, RLU estimates labels by solving a least-square problem that emerges from the analysis of the correlation between labels of the data points used in a training round and the resulting update of the output layer. The experimental results on several datasets, architectures, and data heterogeneity scenarios demonstrate that the proposed method consistently outperforms existing baselines, and helps improve quality of the reconstructed images in GI attacks in terms of both PSNR and LPIPS",
    "checked": true,
    "id": "8379adbbffb632c3794172c0259581ceaa02557a",
    "semantic_title": "recovering labels from local updates in federated learning",
    "citation_count": 3,
    "authors": [
      "Huancheng Chen",
      "Haris Vikalo"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ao.html": {
    "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
    "volume": "main",
    "abstract": "How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings open avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets",
    "checked": true,
    "id": "a72955288e6468b8342eff6f8f0281ceaefc526a",
    "semantic_title": "selfie: self-interpretation of large language model embeddings",
    "citation_count": 12,
    "authors": [
      "Haozhe Chen",
      "Carl Vondrick",
      "Chengzhi Mao"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ap.html": {
    "title": "Locally Differentially Private Decentralized Stochastic Bilevel Optimization with Guaranteed Convergence Accuracy",
    "volume": "main",
    "abstract": "Decentralized bilevel optimization based machine learning techniques are achieving remarkable success in a wide variety of domains. However, the intensive exchange of information (involving nested-loops of consensus or communication iterations) in existing decentralized bilevel optimization algorithms leads to a great challenge to ensure rigorous differential privacy, which, however, is necessary to bring the benefits of machine learning to domains where involved data are sensitive. By proposing a new decentralized stochastic bilevel-optimization algorithm which avoids nested-loops of information-exchange iterations, we achieve, for the first time, both differential privacy and accurate convergence in decentralized bilevel optimization. This is significant since even for single-level decentralized optimization and learning, existing differential-privacy solutions have to sacrifice convergence accuracy for privacy. Besides characterizing the convergence rate under nonconvex/convex/strongly convex conditions, we also rigorously quantify the price of differential privacy in the convergence rate. Experimental results on machine learning models confirm the efficacy of our algorithm",
    "checked": true,
    "id": "73648e6e3a0c2e87069d847aefa1ea9df5b16afb",
    "semantic_title": "locally differentially private decentralized stochastic bilevel optimization with guaranteed convergence accuracy",
    "citation_count": 1,
    "authors": [
      "Ziqin Chen",
      "Yongqiang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24aq.html": {
    "title": "Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments",
    "volume": "main",
    "abstract": "Learning policies for multi-entity systems in 3D environments is far more complicated against single-entity scenarios, due to the exponential expansion of the global state space as the number of entities increases. One potential solution of alleviating the exponential complexity is dividing the global space into independent local views that are invariant to transformations including translations and rotations. To this end, this paper proposes Subequivariant Hierarchical Neural Networks (SHNN) to facilitate multi-entity policy learning. In particular, SHNN first dynamically decouples the global space into local entity-level graphs via task assignment. Second, it leverages subequivariant message passing over the local entity-level graphs to devise local reference frames, remarkably compressing the representation redundancy, particularly in gravity-affected environments. Furthermore, to overcome the limitations of existing benchmarks in capturing the subtleties of multi-entity systems under the Euclidean symmetry, we propose the Multi-entity Benchmark (MEBEN), a new suite of environments tailored for exploring a wide range of multi-entity reinforcement learning. Extensive experiments demonstrate significant advancements of SHNN on the proposed benchmarks compared to existing methods. Comprehensive ablations are conducted to verify the indispensability of task assignment and subequivariance",
    "checked": true,
    "id": "a2ca3f792bd8ec928303dcb70bfeffc7a7ab4add",
    "semantic_title": "subequivariant reinforcement learning in 3d multi-entity physical environments",
    "citation_count": 1,
    "authors": [
      "Runfa Chen",
      "Ling Wang",
      "Yu Du",
      "Tianrui Xue",
      "Fuchun Sun",
      "Jianwei Zhang",
      "Wenbing Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ar.html": {
    "title": "A General Framework for Learning from Weak Supervision",
    "volume": "main",
    "abstract": "Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enhances the scalability of machine learning models but also demonstrates superior performance and versatility across 11 weak supervision scenarios. We hope our work paves the way for further advancements and practical deployment in this field",
    "checked": true,
    "id": "9262890ea10dbf0746cb6b0c34215aafe930fe83",
    "semantic_title": "a general framework for learning from weak supervision",
    "citation_count": 1,
    "authors": [
      "Hao Chen",
      "Jindong Wang",
      "Lei Feng",
      "Xiang Li",
      "Yidong Wang",
      "Xing Xie",
      "Masashi Sugiyama",
      "Rita Singh",
      "Bhiksha Raj"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24as.html": {
    "title": "Diffusion Model-Augmented Behavioral Cloning",
    "volume": "main",
    "abstract": "Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a). Despite the simplicity of modeling the conditional probability with BC, it usually struggles with generalization. While modeling the joint probability can improve generalization performance, the inference procedure is often time-consuming, and the model can suffer from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed Diffusion Model-Augmented Behavioral Cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to optimize both the BC loss (conditional) and our proposed diffusion model loss (joint). DBC outperforms baselines in various continuous control tasks in navigation, robot arm manipulation, dexterous manipulation, and locomotion. We design additional experiments to verify the limitations of modeling either the conditional probability or the joint probability of the expert distribution, as well as compare different generative models. Ablation studies justify the effectiveness of our design choices",
    "checked": true,
    "id": "e134ec1e834b0d56945916cdc02df653dc4e175d",
    "semantic_title": "diffusion model-augmented behavioral cloning",
    "citation_count": 20,
    "authors": [
      "Shang-Fu Chen",
      "Hsiang-Chun Wang",
      "Ming-Hao Hsu",
      "Chun-Mao Lai",
      "Shao-Hua Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24at.html": {
    "title": "AutoOS: Make Your OS More Powerful by Exploiting Large Language Models",
    "volume": "main",
    "abstract": "With the rapid development of Artificial Intelligence of Things (AIoT), customizing and optimizing operating system (OS) kernel configurations for various AIoT application scenarios is crucial for maximizing system performance. However, existing approaches falter due to the overwhelming problem complexity (i.e., over 15,000 configuration options in the Linux kernel), together with the huge evaluation costs and error-prone options that may result in OS boot-up failure, which all make it an unresolved problem to optimize the Linux kernel automatically. In this paper, we introduce AutoOS, a novel framework exploiting Large Language Models for customizing and optimizing OS kernel configurations automatically for various AIoT application scenarios.Inspired by the inherently directory-structured kernel configuration process, we first formulate our research problem as optimizing on a dynamic tree. We then propose a novel framework integrating a state machine-based traversal algorithm as the observe-prune-propose-act-correct loop, which can effectively refine the optimization space and ensure a successful OS boot-up.Experimental results show that AutoOS can automatically customize and optimize the OS kernel configurations without human effort. More importantly, AutoOS even achieves better performance by up to 25% than vendor-provided configuration",
    "checked": true,
    "id": "2aded1bfb1d0f4a376cf05101a16f0e491ac8d97",
    "semantic_title": "autoos: make your os more powerful by exploiting large language models",
    "citation_count": 0,
    "authors": [
      "Huilai Chen",
      "Yuanbo Wen",
      "Limin Cheng",
      "Shouxu Kuang",
      "Yumeng Liu",
      "Weijia Li",
      "Ling Li",
      "Rui Zhang",
      "Xinkai Song",
      "Wei Li",
      "Qi Guo",
      "Yunji Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24au.html": {
    "title": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning",
    "volume": "main",
    "abstract": "Operator learning for Partial Differential Equations (PDEs) is rapidly emerging as a promising approach for surrogate modeling of intricate systems. Transformers with the self-attention mechanism—a powerful tool originally designed for natural language processing—have recently been adapted for operator learning. However, they confront challenges, including high computational demands and limited interpretability. This raises a critical question: Is there a more efficient attention mechanism for Transformer-based operator learning? This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning. Position-attention draws inspiration from numerical methods for PDEs. Different from self-attention, position-attention is induced by only the spatial interrelations of sampling positions for input functions of the operators, and does not rely on the input function values themselves, thereby greatly boosting efficiency. PiT exhibits superior performance over current state-of-the-art neural operators in a variety of complex operator learning tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator",
    "checked": true,
    "id": "f07fccb9dc24559d81ae9afbd2e634f065252683",
    "semantic_title": "positional knowledge is all you need: position-induced transformer (pit) for operator learning",
    "citation_count": 0,
    "authors": [
      "Junfeng Chen",
      "Kailiang Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24av.html": {
    "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation",
    "volume": "main",
    "abstract": "Large language models (LLMs) frequently hallucinate, e.g., making factual errors, yet our understanding of why they make these errors remains limited. In this study, we aim to understand the underlying mechanisms of LLM hallucinations from the perspective of inner representations. We discover a pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to that of the incorrect generations. Leveraging this signal, we propose an entropy-based metric to quantify the sharpness among the in-context hidden states and incorporate it into the decoding process, i.e, use the entropy value to adjust the next token prediction distribution to improve the factuality and overall quality of the generated text. Experiments on knowledge-seeking datasets (Natural Questions, HotpotQA, TriviaQA) and hallucination benchmark (TruthfulQA) demonstrate our consistent effectiveness, e.g., up to 8.6 absolute points on TruthfulQA. We believe this study can improve our understanding of hallucinations and serve as a practical solution for hallucination mitigation",
    "checked": true,
    "id": "ed8f46f493abce9498851e647d900a4628f6ff7f",
    "semantic_title": "in-context sharpness as alerts: an inner representation perspective for hallucination mitigation",
    "citation_count": 12,
    "authors": [
      "Shiqi Chen",
      "Miao Xiong",
      "Junteng Liu",
      "Zhengxuan Wu",
      "Teng Xiao",
      "Siyang Gao",
      "Junxian He"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24aw.html": {
    "title": "Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting",
    "volume": "main",
    "abstract": "Uncertainty estimation is crucial for deep learning models to detect out-of-distribution (OOD) inputs. However, the naive deep learning classifiers produce uncalibrated uncertainty for OOD data. Improving the uncertainty estimation typically requires external data for OOD-aware training or considerable costs to build an ensemble. In this work, we improve on uncertainty estimation without extra OOD data or additional inference costs using an alternative Split-Ensemble method. Specifically, we propose a novel subtask-splitting ensemble training objective where a task is split into several complementary subtasks based on feature similarity. Each subtask considers part of the data as in distribution while all the rest as OOD data. Diverse submodels can therefore be trained on each subtask with OOD-aware objectives, learning generalizable uncertainty estimation. To avoid overheads, we enable low-level feature sharing among submodels, building a tree-like Split-Ensemble architecture via iterative splitting and pruning. Empirical study shows Split-Ensemble, without additional computational cost, improves accuracy over a single model by 0.8%, 1.8%, and 25.5% on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively. OOD detection for the same backbone and in-distribution datasets surpasses a single model baseline by 2.2%, 8.1%, and 29.6% in mean AUROC, respectively",
    "checked": true,
    "id": "49dc776c7348db2f5512e163ae3214d027f42804",
    "semantic_title": "split-ensemble: efficient ood-aware ensemble via task and model splitting",
    "citation_count": 2,
    "authors": [
      "Anthony Chen",
      "Huanrui Yang",
      "Yulu Gan",
      "Denis A Gudovskiy",
      "Zhen Dong",
      "Haofan Wang",
      "Tomoyuki Okuno",
      "Yohei Nakata",
      "Kurt Keutzer",
      "Shanghang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ax.html": {
    "title": "Deep Demonstration Tracing: Learning Generalizable Imitator Policy for Runtime Imitation from a Single Demonstration",
    "volume": "main",
    "abstract": "One-shot imitation learning (OSIL) is to learn an imitator agent that can execute multiple tasks with only a single demonstration. In real-world scenario, the environment is dynamic, e.g., unexpected changes can occur after demonstration. Thus, achieving generalization of the imitator agent is crucial as agents would inevitably face situations unseen in the provided demonstrations. While traditional OSIL methods excel in relatively stationary settings, their adaptability to such unforeseen changes, which asking for a higher level of generalization ability for the imitator agents, is limited and rarely discussed. In this work, we present a new algorithm called Deep Demonstration Tracing (DDT). In DDT, we propose a demonstration transformer architecture to encourage agents to adaptively trace suitable states in demonstrations. Besides, it integrates OSIL into a meta-reinforcement-learning training paradigm, providing regularization for policies in unexpected situations. We evaluate DDT on a new navigation task suite and robotics tasks, demonstrating its superior performance over existing OSIL methods across all evaluated tasks in dynamic environments with unforeseen changes. The project page is in https://osil-ddt.github.io",
    "checked": true,
    "id": "703768f9780278140a3eb2ec3e0110363cd54bc6",
    "semantic_title": "deep demonstration tracing: learning generalizable imitator policy for runtime imitation from a single demonstration",
    "citation_count": 0,
    "authors": [
      "Xiong-Hui Chen",
      "Junyin Ye",
      "Hang Zhao",
      "Yi-Chen Li",
      "Xu-Hui Liu",
      "Haoran Shi",
      "Yu-Yan Xu",
      "Zhihao Ye",
      "Si-Hang Yang",
      "Yang Yu",
      "Anqi Huang",
      "Kai Xu",
      "Zongzhang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ay.html": {
    "title": "DRCT: Diffusion Reconstruction Contrastive Training towards Universal Detection of Diffusion Generated Images",
    "volume": "main",
    "abstract": "Diffusion models have made significant strides in visual content generation but also raised increasing demands on generated image detection. Existing detection methods have achieved considerable progress, but they usually suffer a significant decline in accuracy when detecting images generated by an unseen diffusion model. In this paper, we seek to address the generalizability of generated image detectors from the perspective of hard sample classification. The basic idea is that if a classifier can distinguish generated images that closely resemble real ones, then it can also effectively detect less similar samples, potentially even those produced by a different diffusion model. Based on this idea, we propose Diffusion Reconstruction Contrastive Learning (DRCT), a universal framework to enhance the generalizability of the existing detectors. DRCT generates hard samples by high-quality diffusion reconstruction and adopts contrastive training to guide the learning of diffusion artifacts. In addition, we have built a million-scale dataset, DRCT-2M, including 16 types diffusion models for the evaluation of generalizability of detection methods. Extensive experimental results show that detectors enhanced with DRCT achieve over a 10% accuracy improvement in cross-set tests. The code, models, and dataset will soon be available at https://github.com/beibuwandeluori/DRCT",
    "checked": true,
    "id": "a5b6416bbea65a62b4356dbdd6b2056bd8dbc013",
    "semantic_title": "drct: diffusion reconstruction contrastive training towards universal detection of diffusion generated images",
    "citation_count": 3,
    "authors": [
      "Baoying Chen",
      "Jishen Zeng",
      "Jianquan Yang",
      "Rui Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24az.html": {
    "title": "$\\rm E(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns",
    "checked": false,
    "id": "39b3c23c3154005a8230524499057af08f995a25",
    "semantic_title": "e(3)-equivariant actor-critic methods for cooperative multi-agent reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Dingyang Chen",
      "Qi Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24ba.html": {
    "title": "FedMBridge: Bridgeable Multimodal Federated Learning",
    "volume": "main",
    "abstract": "Multimodal Federated Learning (MFL) addresses the setup of multiple clients with diversified modality types (e.g. image, text, video, and audio) working together to improve their local personal models in a data-privacy manner. Prior MFL works rely on restrictive compositional neural architecture designs to ensure inter-client information sharing via blockwise model aggregation, limiting their applicability in the real-world Architecture-personalized MFL (AMFL) scenarios, where clients may have distinguished multimodal interaction strategies and there is no restriction on local architecture design. The key challenge in AMFL is how to automatically and efficiently tackle the two heterogeneity patterns–statistical and architecture heterogeneity–while maximizing the beneficial information sharing among clients. To solve this challenge, we propose FedMBridge, which leverages a topology-aware hypernetwork to act as a bridge that can automatically balance and digest the two heterogeneity patterns in a communication-efficient manner. Our experiments on four AMFL simulations demonstrate the efficiency and effectiveness of our proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Chen",
      "Aidong Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bb.html": {
    "title": "Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness",
    "volume": "main",
    "abstract": "Robustness is a vital aspect to consider when deploying deep learning models into the wild. Numerous studies have been dedicated to the study of the robustness of vision transformers (ViTs), which have dominated as the mainstream backbone choice for vision tasks since the dawn of 2020s. Recently, some large kernel convnets make a comeback with impressive performance and efficiency. However, it still remains unclear whether large kernel networks are robust and the attribution of their robustness. In this paper, we first conduct a comprehensive evaluation of large kernel convnets' robustness and their differences from typical small kernel counterparts and ViTs on six diverse robustness benchmark datasets. Then to analyze the underlying factors behind their strong robustness, we design experiments from both quantitative and qualitative perspectives to reveal large kernel convnets' intriguing properties that are completely different from typical convnets. Our experiments demonstrate for the first time that pure CNNs can achieve exceptional robustness comparable or even superior to that of ViTs. Our analysis on occlusion invariance, kernel attention patterns and frequency characteristics provide novel insights into the source of robustness. Code available at: https://github.com/Lauch1ng/LKRobust",
    "checked": true,
    "id": "e0041dea44e20e99dc55f82063dbb6407e46b4cf",
    "semantic_title": "revealing the dark secrets of extremely large kernel convnets on robustness",
    "citation_count": 0,
    "authors": [
      "Honghao Chen",
      "Yurong Zhang",
      "Xiaokun Feng",
      "Xiangxiang Chu",
      "Kaiqi Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bc.html": {
    "title": "One for All: A Universal Generator for Concept Unlearnability via Multi-Modal Alignment",
    "volume": "main",
    "abstract": "The abundance of free internet data offers unprecedented opportunities for researchers and developers, but it also poses privacy risks. Utilizing data without explicit consent raises critical challenges in protecting personal information.Unlearnable examples have emerged as a feasible protection approach, which renders the data unlearnable, i.e., useless to third parties, by injecting imperceptible perturbations. However, these perturbations only exhibit unlearnable effects on either a particular dataset or label-consistent scenarios, thereby lacking broad applicability. To address both issues concurrently, we propose a universal perturbation generator that harnesses data with concept unlearnability, thereby broadening the scope of unlearnability beyond specific datasets or labels. Specifically, we leverage multi-modal pre-trained models to establish a connection between the data concepts in a shared embedding space. This connection enables the information transformation from image data to text concepts. Consequently, we can align the text embedding using concept-wise discriminant loss, and render the data unlearnable. Extensive experiments conducted on real-world datasets demonstrate the concept unlearnability, i.e., cross-dataset transferability and label-agnostic utility, of our proposed unlearnable examples, as well as their robustness against attacks",
    "checked": true,
    "id": "3c260518c36c9e74d63b10f70330a97a707bbcc9",
    "semantic_title": "one for all: a universal generator for concept unlearnability via multi-modal alignment",
    "citation_count": 1,
    "authors": [
      "Chaochao Chen",
      "Jiaming Zhang",
      "Yuyuan Li",
      "Zhongxuan Han"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bd.html": {
    "title": "Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have become a building block in graph data processing, with wide applications in critical domains. The growing needs to deploy GNNs in high-stakes applications necessitate explainability for users in the decision-making processes. A popular paradigm for the explainability of GNNs is to identify explainable subgraphs by comparing their labels with the ones of original graphs. This task is challenging due to the substantial distributional shift from the original graphs in the training set to the set of explainable subgraphs, which prevents accurate prediction of labels with the subgraphs. To address it, in this paper, we propose a novel method that generates proxy graphs for explainable subgraphs that are in the distribution of training data. We introduce a parametric method that employs graph generators to produce proxy graphs. A new training objective based on information theory is designed to ensure that proxy graphs not only adhere to the distribution of training data but also preserve explanatory factors. Such generated proxy graphs can be reliably used to approximate the predictions of the labels of explainable subgraphs. Empirical evaluations across various datasets demonstrate our method achieves more accurate explanations for GNNs",
    "checked": true,
    "id": "8fd8cf39a8c70c386c938535a1fe02ce19e431e2",
    "semantic_title": "generating in-distribution proxy graphs for explaining graph neural networks",
    "citation_count": 1,
    "authors": [
      "Zhuomin Chen",
      "Jiaxing Zhang",
      "Jingchao Ni",
      "Xiaoting Li",
      "Yuchen Bian",
      "Md Mezbahul Islam",
      "Ananda Mondal",
      "Hua Wei",
      "Dongsheng Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24be.html": {
    "title": "Diffusive Gibbs Sampling",
    "volume": "main",
    "abstract": "The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics",
    "checked": true,
    "id": "0fb93b93790d353ed7e5f87717d947305a813d61",
    "semantic_title": "diffusive gibbs sampling",
    "citation_count": 3,
    "authors": [
      "Wenlin Chen",
      "Mingtian Zhang",
      "Brooks Paige",
      "José Miguel Hernández-Lobato",
      "David Barber"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bf.html": {
    "title": "Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation",
    "volume": "main",
    "abstract": "In the realm of reinforcement learning (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount. In this paper, we introduce a general framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation. Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity. We design two innovative meta-algorithms: RS-DisRL-M, a model-based strategy for model-based function approximation, and RS-DisRL-V, a model-free approach for general value function approximation. With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in distributional RL with augmented Markov Decision Process (MDP), we derive the first $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ dependency of the regret upper bound for RSRL with static LRM, marking a pioneering contribution towards statistically efficient algorithms in this domain",
    "checked": true,
    "id": "167585781e8a63577e2ed2ee163d531a6a406443",
    "semantic_title": "provable risk-sensitive distributional reinforcement learning with general function approximation",
    "citation_count": 2,
    "authors": [
      "Yu Chen",
      "Xiangcheng Zhang",
      "Siwei Wang",
      "Longbo Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bg.html": {
    "title": "$\\textttMoE-RBench$: Towards Building Reliable Language Models with Sparse Mixture-of-Experts",
    "volume": "main",
    "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new domains such as in fine-tuning MoE models sometimes underperform their dense counterparts. Motivated by the research gap and counter-intuitive phenomenon, we propose $\\texttt{MoE-RBench}$, the first comprehensive assessment of SMoE reliability from three aspects: $\\textit{(i)}$ safety and hallucination, $\\textit{(ii)}$ resilience to adversarial attacks, and $\\textit{(iii)}$ out-of-distribution robustness. Extensive models and datasets are tested to compare the MoE to dense networks from these reliability dimensions. Our empirical observations suggest that with appropriate hyperparameters, training recipes, and inference techniques, we can build the MoE model more reliably than the dense LLM. In particular, we find that the robustness of SMoE is sensitive to the basic training settings. We hope that this study can provide deeper insights into how to adapt the pre-trained MoE model to other tasks with higher-generation security, quality, and stability. Codes are available at https://github.com/UNITES-Lab/MoE-RBench",
    "checked": false,
    "id": "ac325eae5b183a4ad192529d69a71b1fdaf85040",
    "semantic_title": "moe-rbench: towards building reliable language models with sparse mixture-of-experts",
    "citation_count": 4,
    "authors": [
      "Guanjie Chen",
      "Xinyu Zhao",
      "Tianlong Chen",
      "Yu Cheng"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bh.html": {
    "title": "LLaGA: Large Language and Graph Assistant",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the the Large Language and Graph Assistant (LLaGA), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs. Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art graph models in both supervised and zero-shot scenarios",
    "checked": true,
    "id": "0fa9b8a28cf379360ad56b7c778ce7ffc43bea5e",
    "semantic_title": "llaga: large language and graph assistant",
    "citation_count": 29,
    "authors": [
      "Runjin Chen",
      "Tong Zhao",
      "Ajay Kumar Jaiswal",
      "Neil Shah",
      "Zhangyang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bi.html": {
    "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
    "volume": "main",
    "abstract": "While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate HALC's effectiveness in reducing OH, outperforming state-of-the-arts across four benchmarks. Code is released at https://github.com/BillChan226/HALC",
    "checked": true,
    "id": "7751f6cdec0f4473c1733eec91699744a7d5176f",
    "semantic_title": "halc: object hallucination reduction via adaptive focal-contrast decoding",
    "citation_count": 27,
    "authors": [
      "Zhaorun Chen",
      "Zhuokai Zhao",
      "Hongyin Luo",
      "Huaxiu Yao",
      "Bo Li",
      "Jiawei Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bj.html": {
    "title": "Compact Optimality Verification for Optimization Proxies",
    "volume": "main",
    "abstract": "Recent years have witnessed increasing interest in optimization proxies, i.e., machine learning models that approximate the input-output mapping of parametric optimization problems and return near-optimal feasible solutions. Following recent work by (Nellikkath & Chatzivasileiadis, 2021), this paper reconsiders the optimality verification problem for optimization proxies, i.e., the determination of the worst-case optimality gap over the instance distribution. The paper proposes a compact formulation for optimality verification and a gradient-based primal heuristic that brings significant computational benefits to the original formulation. The compact formulation is also more general and applies to non-convex optimization problems. The benefits of the compact formulation are demonstrated on large-scale DC Optimal Power Flow and knapsack problems",
    "checked": true,
    "id": "b2ad769c6bf1431d0209f1f74660a2d0f055b817",
    "semantic_title": "compact optimality verification for optimization proxies",
    "citation_count": 0,
    "authors": [
      "Wenbo Chen",
      "Haoruo Zhao",
      "Mathieu Tanneau",
      "Pascal Van Hentenryck"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bk.html": {
    "title": "Enhancing Implicit Shape Generators Using Topological Regularizations",
    "volume": "main",
    "abstract": "A fundamental problem in learning 3D shapes generative models is that when the generative model is simply fitted to the training data, the resulting synthetic 3D models can present various artifacts. Many of these artifacts are topological in nature, e.g., broken legs, unrealistic thin structures, and small holes. In this paper, we introduce a principled approach that utilizes topological regularization losses on an implicit shape generator to rectify topological artifacts. The objectives are two-fold. The first is to align the persistent diagram (PD) distribution of the training shapes with that of synthetic shapes. The second ensures that the PDs are smooth among adjacent synthetic shapes. We show how to achieve these two objectives using two simple but effective formulations. Specifically, distribution alignment is achieved to learn a generative model of PDs and align this generator with PDs of synthetic shapes. We show how to handle discrete and continuous variabilities of PDs by using a shape-regularization term when performing PD alignment. Moreover, we enforce the smoothness of the PDs using a smoothness loss on the PD generator, which further improves the behavior of PD distribution alignment. Experimental results on ShapeNet show that our approach leads to much better generalization behavior than state-of-the-art implicit shape generators",
    "checked": true,
    "id": "c0aa4f995e2055f850ae542438e5a8cc8b5bd11d",
    "semantic_title": "enhancing implicit shape generators using topological regularizations",
    "citation_count": 1,
    "authors": [
      "Liyan Chen",
      "Yan Zheng",
      "Yang Li",
      "Lohit Anirudh Jagarapu",
      "Haoxiang Li",
      "Hao Kang",
      "Gang Hua",
      "Qixing Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bl.html": {
    "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
    "volume": "main",
    "abstract": "Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers \"$\\textit{yes}$\" to the input question \"$\\textit{Can eagles fly?}$\" with the explanation \"$\\textit{all birds can fly}$\", then humans would infer from the explanation that it would also answer \"$\\textit{yes}$\" to the counterfactual input \"$\\textit{Can penguins fly?}$\". If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may be insufficient",
    "checked": true,
    "id": "69f2ba0f33a54e01de32c616b64e85d5d7194067",
    "semantic_title": "do models explain themselves? counterfactual simulatability of natural language explanations",
    "citation_count": 39,
    "authors": [
      "Yanda Chen",
      "Ruiqi Zhong",
      "Narutatsu Ri",
      "Chen Zhao",
      "He He",
      "Jacob Steinhardt",
      "Zhou Yu",
      "Kathleen Mckeown"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bm.html": {
    "title": "On the Trajectory Regularity of ODE-based Diffusion Sampling",
    "volume": "main",
    "abstract": "Diffusion-based generative models use stochastic differential equations (SDEs) and their equivalent ordinary differential equations (ODEs) to establish a smooth connection between a complex data distribution and a tractable prior distribution. In this paper, we identify several intriguing trajectory properties in the ODE-based sampling process of diffusion models. We characterize an implicit denoising trajectory and discuss its vital role in forming the coupled sampling trajectory with a strong shape regularity, regardless of the generated content. We also describe a dynamic programming-based scheme to make the time schedule in sampling better fit the underlying trajectory structure. This simple strategy requires minimal modification to any given ODE-based numerical solvers and incurs negligible computational cost, while delivering superior performance in image generation, especially in $5\\sim 10$ function evaluations",
    "checked": true,
    "id": "48856ff99053bb9769c900c93aea2272c764cb30",
    "semantic_title": "on the trajectory regularity of ode-based diffusion sampling",
    "citation_count": 6,
    "authors": [
      "Defang Chen",
      "Zhenyu Zhou",
      "Can Wang",
      "Chunhua Shen",
      "Siwei Lyu"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bn.html": {
    "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
    "volume": "main",
    "abstract": "In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators and achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads to predict the preference, one trained to correlate with length and the other trained to decorrelate with length and therefore focusing more on the actual content. We then discard the length head in RL to ignore the spurious length reward. Experiments demonstrate that our approach eliminates the reward correlation with length, and improves the obtained policy by a significant margin",
    "checked": true,
    "id": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
    "semantic_title": "odin: disentangled reward mitigates hacking in rlhf",
    "citation_count": 26,
    "authors": [
      "Lichang Chen",
      "Chen Zhu",
      "Jiuhai Chen",
      "Davit Soselia",
      "Tianyi Zhou",
      "Tom Goldstein",
      "Heng Huang",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bo.html": {
    "title": "Stacking Deep Set Networks and Pooling by Quantiles",
    "volume": "main",
    "abstract": "We propose Stacked Deep Sets and Quantile Pooling for learning tasks on set data. We introduce Quantile Pooling, a novel permutation-invariant pooling operation that synergizes max and average pooling. Just like max pooling, quantile pooling emphasizes the most salient features of the data. Like average pooling, it captures the overall distribution and subtle features of the data. Like both, it is lightweight and fast. We demonstrate the effectiveness of our approach in a variety of tasks, showing that quantile pooling can outperform both max and average pooling in each of their respective strengths. We also introduce a variant of deep set networks that is more expressive and universal. While Quantile Pooling balances robustness and sensitivity, Stacked Deep Sets enhances learning with depth",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuojun Chen",
      "Xinghua Zhu",
      "Dongzhe Su",
      "Justin C. I. Chuang"
    ]
  },
  "https://proceedings.mlr.press/v235/chen24bp.html": {
    "title": "What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks",
    "volume": "main",
    "abstract": "We study the capabilities of the transformer architecture with varying depth. Specifically, we designed a novel set of sequence learning tasks to systematically evaluate and comprehend how the depth of transformer affects its ability to perform memorization, reasoning, generalization, and contextual generalization. We show a transformer with only one attention layer can excel in memorization but falls short in other tasks. Then, we show that exhibiting reasoning and generalization ability requires the transformer to have at least two attention layers, while context generalization ability may necessitate three attention layers. Additionally, we identify a class of simple operations that a single attention layer can execute, and show that the complex tasks can be approached as the combinations of these simple operations and thus can be resolved by stacking multiple attention layers. This sheds light on studying more practical and complex tasks beyond our design. Numerical experiments corroborate our theoretical findings",
    "checked": true,
    "id": "ebbd118d7f6383f8a08eb6ea6fbfd24a19639505",
    "semantic_title": "what can transformer learn with varying depth? case studies on sequence learning tasks",
    "citation_count": 5,
    "authors": [
      "Xingwu Chen",
      "Difan Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24a.html": {
    "title": "Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context",
    "volume": "main",
    "abstract": "Many neural network architectures are known to be Turing Complete, and can thus, in principle implement arbitrary algorithms. However, Transformers are unique in that they can implement gradient-based learning algorithms under simple parameter configurations. This paper provides theoretical and empirical evidence that (non-linear) Transformers naturally learn to implement gradient descent in function space, which in turn enable them to learn non-linear functions in context. Our results apply to a broad class of combinations of non-linear architectures and non-linear in-context learning tasks. Additionally, we show that the optimal choice of non-linear activation depends in a natural way on the class of functions that need to be learned",
    "checked": true,
    "id": "7723b4da2f4565e81c5e7543cfdccc18311375b8",
    "semantic_title": "transformers implement functional gradient descent to learn non-linear functions in context",
    "citation_count": 30,
    "authors": [
      "Xiang Cheng",
      "Yuxin Chen",
      "Suvrit Sra"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24b.html": {
    "title": "Layerwise Change of Knowledge in Neural Networks",
    "volume": "main",
    "abstract": "This paper aims to explain how a deep neural network (DNN) gradually extracts new knowledge and forgets noisy features through layers in forward propagation. Up to now, although how to define knowledge encoded by the DNN has not reached a consensus so far, previous studies have derived a series of mathematical evidences to take interactions as symbolic primitive inference patterns encoded by a DNN. We extend the definition of interactions and, for the first time, extract interactions encoded by intermediate layers. We quantify and track the newly emerged interactions and the forgotten interactions in each layer during the forward propagation, which shed new light on the learning behavior of DNNs. The layer-wise change of interactions also reveals the change of the generalization capacity and instability of feature representations of a DNN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Cheng",
      "Lei Cheng",
      "Zhaoran Peng",
      "Yang Xu",
      "Tian Han",
      "Quanshi Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24c.html": {
    "title": "Reference Neural Operators: Learning the Smooth Dependence of Solutions of PDEs on Geometric Deformations",
    "volume": "main",
    "abstract": "For partial differential equations on domains of arbitrary shapes, existing works of neural operators attempt to learn a mapping from geometries to solutions. It often requires a large dataset of geometry-solution pairs in order to obtain a sufficiently accurate neural operator. However, for many industrial applications, e.g., engineering design optimization, it can be prohibitive to satisfy the requirement since even a single simulation may take hours or days of computation. To address this issue, we propose reference neural operators (RNO), a novel way of implementing neural operators, i.e., to learn the smooth dependence of solutions on geometric deformations. Specifically, given a reference solution, RNO can predict solutions corresponding to arbitrary deformations of the referred geometry. This approach turns out to be much more data efficient. Through extensive experiments, we show that RNO can learn the dependence across various types and different numbers of geometry objects with relatively small datasets. RNO outperforms baseline models in accuracy by a large lead and achieves up to 80% error reduction",
    "checked": true,
    "id": "6f517042ead7a4318d7529708b74af013e171734",
    "semantic_title": "reference neural operators: learning the smooth dependence of solutions of pdes on geometric deformations",
    "citation_count": 1,
    "authors": [
      "Ze Cheng",
      "Zhongkai Hao",
      "Xiaoqiang Wang",
      "Jianing Huang",
      "Youjia Wu",
      "Xudan Liu",
      "Yiru Zhao",
      "Songming Liu",
      "Hang Su"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24d.html": {
    "title": "Causal Inference out of Control: Estimating Performativity without Treatment Randomization",
    "volume": "main",
    "abstract": "Regulators and academics are increasingly interested in the causal effect that algorithmic actions of a digital platform have on user consumption. In pursuit of estimating this effect from observational data, we identify a set of assumptions that permit causal identifiability without assuming randomized platform actions. Our results are applicable to platforms that rely on machine-learning-powered predictions and leverage knowledge from historical data. The key novelty of our approach is to explicitly model the dynamics of consumption over time, exploiting the repeated interaction of digital platforms with their participants to prove our identifiability results. By viewing the platform as a controller acting on a dynamical system, we can show that exogenous variation in consumption and appropriately responsive algorithmic control actions are sufficient for identifying the causal effect of interest. We complement our claims with an analysis of ready-to-use finite sample estimators and empirical investigations. More broadly, our results deriving identifiability conditions tailored to digital platform settings illustrate a fruitful interplay of control theory and causal inference",
    "checked": true,
    "id": "f406e80d109fee5a7dee377f014275055603e910",
    "semantic_title": "causal inference out of control: estimating performativity without treatment randomization",
    "citation_count": 0,
    "authors": [
      "Gary Cheng",
      "Moritz Hardt",
      "Celestine Mendler-Dünner"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24e.html": {
    "title": "BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks",
    "volume": "main",
    "abstract": "Pixel-wise regression tasks (e.g., monocular depth estimation (MDE) and optical flow estimation (OFE)) have been widely involved in our daily life in applications like autonomous driving, augmented reality and video composition. Although certain applications are security-critical or bear societal significance, the adversarial robustness of such models are not sufficiently studied, especially in the black-box scenario. In this work, we introduce the first unified black-box adversarial patch attack framework against pixel-wise regression tasks, aiming to identify the vulnerabilities of these models under query-based black-box attacks. We propose a novel square-based adversarial patch optimization framework and employ probabilistic square sampling and score-based gradient estimation techniques to generate the patch effectively and efficiently, overcoming the scalability problem of previous black-box patch attacks. Our attack prototype, named BadPart, is evaluated on both MDE and OFE tasks, utilizing a total of 7 models. BadPart surpasses 3 baseline methods in terms of both attack performance and efficiency. We also apply BadPart on the Google online service for portrait depth estimation, causing 43.5% relative distance error with 50K queries. State-of-the-art (SOTA) countermeasures cannot defend our attack effectively",
    "checked": true,
    "id": "40162f78cd2a2eeaf28c2fe77628e0d2d26ef87f",
    "semantic_title": "badpart: unified black-box adversarial patch attacks against pixel-wise regression tasks",
    "citation_count": 3,
    "authors": [
      "Zhiyuan Cheng",
      "Zhaoyi Liu",
      "Tengda Guo",
      "Shiwei Feng",
      "Dongfang Liu",
      "Mingjie Tang",
      "Xiangyu Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24f.html": {
    "title": "GaussianPro: 3D Gaussian Splatting with Progressive Propagation",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized the field of neural rendering with its high fidelity and efficiency. However, 3DGS heavily depends on the initialized point cloud produced by Structure-from-Motion (SfM) techniques. When tackling large-scale scenes that unavoidably contain texture-less surfaces, SfM techniques fail to produce enough points in these surfaces and cannot provide good initialization for 3DGS. As a result, 3DGS suffers from difficult optimization and low-quality renderings. In this paper, inspired by classic multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that applies a progressive propagation strategy to guide the densification of the 3D Gaussians. Compared to the simple split and clone strategies used in 3DGS, our method leverages the priors of the existing reconstructed geometries of the scene and utilizes patch matching to produce new Gaussians with accurate positions and orientations. Experiments on both large-scale and small-scale scenes validate the effectiveness of our method. Our method significantly surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in terms of PSNR. Codes and data are available at https://github.com/kcheng1021/GaussianPro",
    "checked": true,
    "id": "e22c03e7ca21ee07e2253960dcd4670cf955a3de",
    "semantic_title": "gaussianpro: 3d gaussian splatting with progressive propagation",
    "citation_count": 54,
    "authors": [
      "Kai Cheng",
      "Xiaoxiao Long",
      "Kaizhi Yang",
      "Yao Yao",
      "Wei Yin",
      "Yuexin Ma",
      "Wenping Wang",
      "Xuejin Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24g.html": {
    "title": "Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum",
    "volume": "main",
    "abstract": "We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel. Our conclusion is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new random matrix theory (RMT) techniques with recent tools in the kernel ridge regression (KRR) literature",
    "checked": true,
    "id": "02963059bd6028c00abefabe0b9cebc0564cbb1b",
    "semantic_title": "characterizing overfitting in kernel ridgeless regression through the eigenspectrum",
    "citation_count": 5,
    "authors": [
      "Tin Sum Cheng",
      "Aurelien Lucchi",
      "Anastasis Kratsios",
      "David Belius"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24h.html": {
    "title": "Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior",
    "volume": "main",
    "abstract": "This paper studies the challenging black-box adversarial attack that aims to generate adversarial examples against a black-box model by only using output feedback of the model to input queries. Some previous methods improve the query efficiency by incorporating the gradient of a surrogate white-box model into query-based attacks due to the adversarial transferability. However, the localized gradient is not informative enough, making these methods still query-intensive. In this paper, we propose a Prior-guided Bayesian Optimization (P-BO) algorithm that leverages the surrogate model as a global function prior in black-box adversarial attacks. As the surrogate model contains rich prior information of the black-box one, P-BO models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss. Our theoretical analysis on the regret bound indicates that the performance of P-BO may be affected by a bad prior. Therefore, we further propose an adaptive integration strategy to automatically adjust a coefficient on the function prior by minimizing the regret bound. Extensive experiments on image classifiers and large vision-language models demonstrate the superiority of the proposed algorithm in reducing queries and improving attack success rates compared with the state-of-the-art black-box attacks. Code is available at https://github.com/yibo-miao/PBO-Attack",
    "checked": true,
    "id": "3065ffa18d7f9988ec26c5306ac07f3a3b9162bd",
    "semantic_title": "efficient black-box adversarial attacks via bayesian optimization guided by a function prior",
    "citation_count": 2,
    "authors": [
      "Shuyu Cheng",
      "Yibo Miao",
      "Yinpeng Dong",
      "Xiao Yang",
      "Xiao-Shan Gao",
      "Jun Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24i.html": {
    "title": "Can AI Assistants Know What They Don't Know?",
    "volume": "main",
    "abstract": "AI assistants powered by Large Language Models (LLMs) have demonstrated impressive performance in various tasks. However, LLMs still make factual errors in knowledge-intensive tasks such as open-domain question answering. These untruthful responses from AI assistants can pose significant risks in practical applications. Therefore, in this paper, we ask the question Can AI assistants know what they don't know and express this awareness through natural language? To investigate this, we construct a model-specific \"I don't know\" (Idk) dataset. This dataset includes Supervised Fine-tuning data and preference data, categorizing questions based on whether the assistant knows or does not know the answers. Then, we align the assistant with its corresponding Idk dataset using different alignment methods, including Supervised Fine-tuning and preference optimization. Experimental results show that, after alignment with the Idk dataset, the assistant is more capable of declining to answer questions outside its knowledge scope. The assistant aligned with the Idk dataset shows significantly higher truthfulness than the original assistant",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyuan Cheng",
      "Tianxiang Sun",
      "Xiangyang Liu",
      "Wenwei Zhang",
      "Zhangyue Yin",
      "Shimin Li",
      "Linyang Li",
      "Zhengfu He",
      "Kai Chen",
      "Xipeng Qiu"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24j.html": {
    "title": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation",
    "volume": "main",
    "abstract": "Deep reinforcement learning (DRL) is playing an increasingly important role in real-world applications. However, obtaining an optimally performing DRL agent for complex tasks, especially with sparse rewards, remains a significant challenge. The training of a DRL agent can be often trapped in a bottleneck without further progress. In this paper, we propose RICE, an innovative refining scheme for reinforcement learning that incorporates explanation methods to break through the training bottlenecks. The high-level idea of RICE is to construct a new initial state distribution that combines both the default initial states and critical states identified through explanation methods, thereby encouraging the agent to explore from the mixed initial states. Through careful design, we can theoretically guarantee that our refining scheme has a tighter sub-optimality bound. We evaluate RICE in various popular RL environments and real-world applications. The results demonstrate that RICE significantly outperforms existing refining schemes in enhancing agent performance",
    "checked": true,
    "id": "a8cf219c83b66995f5fa535078e4a9e3f3e68a01",
    "semantic_title": "rice: breaking through the training bottlenecks of reinforcement learning with explanation",
    "citation_count": 1,
    "authors": [
      "Zelei Cheng",
      "Xian Wu",
      "Jiahao Yu",
      "Sabrina Yang",
      "Gang Wang",
      "Xinyu Xing"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24k.html": {
    "title": "RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences",
    "volume": "main",
    "abstract": "Preference-based Reinforcement Learning (PbRL) circumvents the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL methods excessively depend on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method utilizes a sample selection-based discriminator to dynamically filter out noise and ensure robust training. To counteract the cumulative error stemming from incorrect selection, we suggest a warm start for the reward model, which additionally bridges the performance gap during the transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the state-of-the-art PbRL method. Code is available at https://github.com/CJReinforce/RIME_ICML2024",
    "checked": true,
    "id": "87f1b39c320e1fc71584a231855523167a5588ff",
    "semantic_title": "rime: robust preference-based reinforcement learning with noisy preferences",
    "citation_count": 4,
    "authors": [
      "Jie Cheng",
      "Gang Xiong",
      "Xingyuan Dai",
      "Qinghai Miao",
      "Yisheng Lv",
      "Fei-Yue Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/cheng24l.html": {
    "title": "Kernel Semi-Implicit Variational Inference",
    "volume": "main",
    "abstract": "Semi-implicit variational inference (SIVI) extends traditional variational families with semi-implicit distributions defined in a hierarchical manner. Due to the intractable densities of semi-implicit distributions, classical SIVI often resorts to surrogates of evidence lower bound (ELBO) that would introduce biases for training. A recent advancement in SIVI, named SIVI-SM, utilizes an alternative score matching objective made tractable via a minimax formulation, albeit requiring an additional lower-level optimization. In this paper, we propose kernel SIVI (KSIVI), a variant of SIVI-SM that eliminates the need for the lower-level optimization through kernel tricks. Specifically, we show that when optimizing over a reproducing kernel Hilbert space (RKHS), the lower-level problem has an explicit solution. This way, the upper-level objective becomes the kernel Stein discrepancy (KSD), which is readily computable for stochastic gradient descent due to the hierarchical structure of semi-implicit variational distributions. An upper bound for the variance of the Monte Carlo gradient estimators of the KSD objective is derived, which allows us to establish novel convergence guarantees of KSIVI. We demonstrate the effectiveness and efficiency of KSIVI on both synthetic distributions and a variety of real data Bayesian inference tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Cheng",
      "Longlin Yu",
      "Tianyu Xie",
      "Shiyue Zhang",
      "Cheng Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/cherep24a.html": {
    "title": "Creative Text-to-Audio Generation via Synthesizer Programming",
    "volume": "main",
    "abstract": "Neural audio synthesis methods now allow specifying ideas in natural language. However, these methods produce results that cannot be easily tweaked, as they are based on large latent spaces and up to billions of uninterpretable parameters. We propose a text-to-audio generation method that leverages a virtual modular sound synthesizer with only 78 parameters. Synthesizers have long been used by skilled sound designers for media like music and film due to their flexibility and intuitive controls. Our method, CTAG, iteratively updates a synthesizer's parameters to produce high-quality audio renderings of text prompts that can be easily inspected and tweaked. Sounds produced this way are also more abstract, capturing essential conceptual features over fine-grained acoustic details, akin to how simple sketches can vividly convey visual concepts. Our results show how CTAG produces sounds that are distinctive, perceived as artistic, and yet similarly identifiable to recent neural audio synthesis models, positioning it as a valuable and complementary tool",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Cherep",
      "Nikhil Singh",
      "Jessica Shand"
    ]
  },
  "https://proceedings.mlr.press/v235/cheung24a.html": {
    "title": "Leveraging (Biased) Information: Multi-armed Bandits with Offline Data",
    "volume": "main",
    "abstract": "We leverage offline data to facilitate online learning in stochastic multi-armed bandits. The probability distributions that govern the offline data and the online rewards can be different. Without any non-trival upper bound on their difference, we show that no non-anticipatory policy can out-perform the UCB policy by (Auer et al. 2002), even in the presence of offline data. In complement, we propose an online policy MIN-UCB, which outperforms UCB when a non-trivial upper bound is given. MIN-UCB adaptively chooses to utilize the offline data when they are deemed informative, and to ignore them otherwise. MIN-UCB is shown to be tight in terms of both instance indepedent and dependent regret bounds. Finally, we corroborate the theoretical results with numerical experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Chi Cheung",
      "Lixing Lyu"
    ]
  },
  "https://proceedings.mlr.press/v235/chevalier24a.html": {
    "title": "Language Models as Science Tutors",
    "volume": "main",
    "abstract": "NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized in math have a 32K-token context window, and they excel at TutorEval while performing strongly on GSM8K and MATH. Our datasets build on open-source materials, and we release our models, data, and evaluations publicly",
    "checked": true,
    "id": "57a8333365bf99b65591e6b2176eacf8fd85d5da",
    "semantic_title": "language models as science tutors",
    "citation_count": 5,
    "authors": [
      "Alexis Chevalier",
      "Jiayi Geng",
      "Alexander Wettig",
      "Howard Chen",
      "Sebastian Mizera",
      "Toni Annala",
      "Max Aragon",
      "Arturo Rodriguez Fanlo",
      "Simon Frieder",
      "Simon Machado",
      "Akshara Prabhakar",
      "Ellie Thieu",
      "Jiachen T. Wang",
      "Zirui Wang",
      "Xindi Wu",
      "Mengzhou Xia",
      "Wenhan Xia",
      "Jiatong Yu",
      "Junjie Zhu",
      "Zhiyong Ren",
      "Sanjeev Arora",
      "Danqi Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/chiang24a.html": {
    "title": "Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning",
    "volume": "main",
    "abstract": "In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where acquiring multiple expert demonstrations is costly or infeasible and the ground truth reward function is not available. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existing IL approaches and achieves expert-level performance in the single-demonstration IL setting across five widely adopted MuJoCo benchmarks as well as the \"Adroit Door\" robotic environment",
    "checked": true,
    "id": "66e8e0bb744fd7d3d55499ba9a5b6f79ee8da548",
    "semantic_title": "expert proximity as surrogate rewards for single demonstration imitation learning",
    "citation_count": 0,
    "authors": [
      "Chia-Cheng Chiang",
      "Li-Cheng Lan",
      "Wei-Fang Sun",
      "Chien Feng",
      "Cho-Jui Hsieh",
      "Chun-Yi Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/chiang24b.html": {
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowd-sourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. The platform is publicly available at https://chat.lmsys.org",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Lin Chiang",
      "Lianmin Zheng",
      "Ying Sheng",
      "Anastasios Nikolas Angelopoulos",
      "Tianle Li",
      "Dacheng Li",
      "Banghua Zhu",
      "Hao Zhang",
      "Michael Jordan",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ]
  },
  "https://proceedings.mlr.press/v235/chib24a.html": {
    "title": "MS-TIP: Imputation Aware Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "Pedestrian trajectory prediction aims to predict future trajectories based on observed trajectories. Current state-of-the-art methods often assume that the observed sequences of agents are complete, which is a strong assumption that overlooks inherent uncertainties. Understanding pedestrian behavior when dealing with missing values in the observed sequence is crucial for enhancing the performance of predictive models. In this work, we propose the MultiScale hypergraph for Trajectory Imputation and Prediction (MS-TIP), a novel approach that simultaneously addresses the imputation of missing observations and the prediction of future trajectories. Specifically, we leverage transformers with diagonal masked self-attention to impute incomplete observations. Further, our approach promotes complex interaction modeling through multi-scale hypergraphs, optimizing our trajectory prediction module to capture different types of interactions. With the inclusion of scenic attention, we learn contextual scene information, instead of sole reliance on coordinates. Additionally, our approach utilizes an intermediate control point and refinement module to infer future trajectories accurately. Extensive experiments validate the efficacy of MS-TIP in precisely predicting pedestrian future trajectories. Code is publicly available at https://github.com/Pranav-chib/MS-TIP",
    "checked": true,
    "id": "8ffe85684e74f2cb43dfab09779f89830df61900",
    "semantic_title": "ms-tip: imputation aware pedestrian trajectory prediction",
    "citation_count": 1,
    "authors": [
      "Pranav Singh Chib",
      "Achintya Nath",
      "Paritosh Kabra",
      "Ishu Gupta",
      "Pravendra Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/chib24b.html": {
    "title": "Enhancing Trajectory Prediction through Self-Supervised Waypoint Distortion Prediction",
    "volume": "main",
    "abstract": "Trajectory prediction is an important task that involves modeling the indeterminate nature of agents to forecast future trajectories given the observed trajectory sequences. The task of predicting trajectories poses significant challenges, as agents not only move individually through time but also interact spatially. The learning of complex spatio-temporal representations stands as a fundamental challenge in trajectory prediction. To this end, we propose a novel approach called SSWDP (Self-Supervised Waypoint Distortion Prediction). We propose a simple yet highly effective self-supervised task of predicting distortion present in the observed trajectories to improve the representation learning of the model. Our approach can complement existing trajectory prediction methods. The experimental results highlight a significant improvement with relative percentage differences of 22.7%/38.9%, 33.8%/36.4%, and 16.60%/23.20% in ADE/FDE for the NBA, TrajNet++, and ETH-UCY datasets, respectively, compared to the baseline methods. Our approach also demonstrates a significant improvement over baseline methods with relative percentage differences of 76.8%/82.5% and 61.0%/36.1% in ADE/FDE for TrajNet++ and NBA datasets in distorted environments, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Singh Chib",
      "Pravendra Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/chidambaram24a.html": {
    "title": "How Flawed Is ECE? An Analysis via Logit Smoothing",
    "volume": "main",
    "abstract": "Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction. By far the most common method in the literature for measuring calibration is the expected calibration error (ECE). Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors. In this work, we ask: how fundamental are these issues, and what are their impacts on existing results? Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces. We then use the nature of these discontinuities to motivate a novel continuous, easily estimated miscalibration metric, which we term Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical pathologies of ECE may be avoidable in practice",
    "checked": true,
    "id": "684f6205f83f4cbf8985d2b0e6dbfd4391567f3d",
    "semantic_title": "how flawed is ece? an analysis via logit smoothing",
    "citation_count": 0,
    "authors": [
      "Muthu Chidambaram",
      "Holden Lee",
      "Colin Mcswiggen",
      "Semon Rezchikov"
    ]
  },
  "https://proceedings.mlr.press/v235/chien24a.html": {
    "title": "Safe Exploration in Dose Finding Clinical Trials with Heterogeneous Participants",
    "volume": "main",
    "abstract": "In drug development, early phase dose-finding clinical trials are carried out to identify an optimal dose to administer to patients in larger confirmatory clinical trials. Standard trial procedures do not optimize for participant benefit and do not consider participant heterogeneity, despite consequences to participants' health and downstream impacts to under-represented population subgroups. Many novel drugs also do not obey parametric modelling assumptions made in common dose-finding procedures. We present Safe Allocation for Exploration of Treatments SAFE-T, a procedure for adaptive dose-finding that adheres to safety constraints, improves utility for heterogeneous participants, and works well with small sample sizes. SAFE-T flexibly learns non-parametric multi-output Gaussian process models for dose toxicity and efficacy, using Bayesian optimization, and provides accurate final dose recommendations. We provide theoretical guarantees for the satisfaction of safety constraints. Using a comprehensive set of realistic synthetic scenarios, we demonstrate empirically that SAFE-T generally outperforms comparable methods and maintains performance across variations in sample size and subgroup distribution. Finally, we extend SAFE-T to a new adaptive setting, demonstrating its potential to improve traditional clinical trial procedures",
    "checked": true,
    "id": "1e4790eaecb37bcf4680ab87f9b44ca8da12872f",
    "semantic_title": "safe exploration in dose finding clinical trials with heterogeneous participants",
    "citation_count": 0,
    "authors": [
      "Isabel Chien",
      "Wessel P Bruinsma",
      "Javier Gonzalez",
      "Richard E. Turner"
    ]
  },
  "https://proceedings.mlr.press/v235/chin24a.html": {
    "title": "Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts",
    "volume": "main",
    "abstract": "Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows that around half of prompts in existing safe prompting benchmarks which were originally considered \"safe\" can actually be manipulated to bypass many deployed safety mechanisms, including concept removal, negative prompt, and safety guidance. Our findings suggest that, without comprehensive testing, the evaluations on limited safe prompting benchmarks can lead to a false sense of safety for text-to-image models",
    "checked": true,
    "id": "f4b88a1a6a877cbeb9faaf99eccc5e1ed9f2ea6b",
    "semantic_title": "prompting4debugging: red-teaming text-to-image diffusion models by finding problematic prompts",
    "citation_count": 41,
    "authors": [
      "Zhi-Yi Chin",
      "Chieh Ming Jiang",
      "Ching-Chun Huang",
      "Pin-Yu Chen",
      "Wei-Chen Chiu"
    ]
  },
  "https://proceedings.mlr.press/v235/cho24a.html": {
    "title": "Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams",
    "volume": "main",
    "abstract": "We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, peeking with expectation-based averaged capital (PEAK), builds upon the testing-by-betting framework and provides a non-asymptotic $\\alpha$-level test across any stopping time. Our contributions are two-fold: (1) we propose a novel betting scheme and provide theoretical guarantees on type-I error control, power, and asymptotic growth rate/$e$-power in the setting of a single data stream; (2) we introduce PEAK, a generalization of this betting scheme to multiple streams, that (i) avoids using wasteful union bounds via averaging, (ii) is a test of power one under mild regularity conditions on the sampling scheme of the streams, and (iii) reduces computational overhead when applying the testing-as-betting approaches for pure-exploration bandit problems. We illustrate the practical benefits of PEAK using both synthetic and real-world HeartSteps datasets. Our experiments show that PEAK provides up to an 85% reduction in the number of samples before stopping compared to existing stopping rules for pure-exploration bandit problems, and matches the performance of state-of-the-art sequential tests while improving upon computational complexity",
    "checked": true,
    "id": "af83dd5b0a95f951698c10dc7178bd457c8ef0f8",
    "semantic_title": "peeking with peak: sequential, nonparametric composite hypothesis tests for means of multiple data streams",
    "citation_count": 3,
    "authors": [
      "Brian M Cho",
      "Kyra Gan",
      "Nathan Kallus"
    ]
  },
  "https://proceedings.mlr.press/v235/cho24b.html": {
    "title": "Parameterized Physics-informed Neural Networks for Parameterized PDEs",
    "volume": "main",
    "abstract": "Complex physical systems are often described by partial differential equations (PDEs) that depend on parameters such as the Raynolds number in fluid mechanics. In applications such as design optimization or uncertainty quantification, solutions of those PDEs need to be evaluated at numerous points in the parameter space. While physics-informed neural networks (PINNs) have emerged as a new strong competitor as a surrogate, their usage in this scenario remains underexplored due to the inherent need for repetitive and time-consuming training. In this paper, we address this problem by proposing a novel extension, parameterized physics-informed neural networks (P$^2$INNs). P$^2$INNs enable modeling the solutions of parameterized PDEs via explicitly encoding a latent representation of PDE parameters. With the extensive empirical evaluation, we demonstrate that P$^2$INNs outperform the baselines both in accuracy and parameter efficiency on benchmark 1D and 2D parameterized PDEs and are also effective in overcoming the known \"failure modes\"",
    "checked": true,
    "id": "4ca0ec3e858b6f249c48d169370dee08ae38e713",
    "semantic_title": "parameterized physics-informed neural networks for parameterized pdes",
    "citation_count": 4,
    "authors": [
      "Woojin Cho",
      "Minju Jo",
      "Haksoo Lim",
      "Kookjin Lee",
      "Dongeun Lee",
      "Sanghyun Hong",
      "Noseong Park"
    ]
  },
  "https://proceedings.mlr.press/v235/cho24c.html": {
    "title": "Kernel Debiased Plug-in Estimation: Simultaneous, Automated Debiasing without Influence Functions for Many Target Parameters",
    "volume": "main",
    "abstract": "When estimating target parameters in nonparametric models with nuisance parameters, substituting the unknown nuisances with nonparametric estimators can introduce \"plug-in bias.\" Traditional methods addressing this suboptimal bias-variance trade-off rely on the influence function (IF) of the target parameter. When estimating multiple target parameters, these methods require debiasing the nuisance parameter multiple times using the corresponding IFs, which poses analytical and computational challenges. In this work, we leverage the targeted maximum likelihood estimation (TMLE) framework to propose a novel method named kernel debiased plug-in estimation (KDPE). KDPE refines an initial estimate through regularized likelihood maximization steps, employing a nonparametric model based on reproducing kernel Hilbert spaces. We show that KDPE: (i) simultaneously debiases all pathwise differentiable target parameters that satisfy our regularity conditions, (ii) does not require the IF for implementation, and (iii) remains computationally tractable. We numerically illustrate the use of KDPE and validate our theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian M Cho",
      "Yaroslav Mukhin",
      "Kyra Gan",
      "Ivana Malenica"
    ]
  },
  "https://proceedings.mlr.press/v235/cho24d.html": {
    "title": "Hard Tasks First: Multi-Task Reinforcement Learning Through Task Scheduling",
    "volume": "main",
    "abstract": "Multi-task reinforcement learning (RL) faces the significant challenge of varying task difficulties, often leading to negative transfer when simpler tasks overshadow the learning of more complex ones. To overcome this challenge, we propose a novel algorithm, Scheduled Multi-Task Training (SMT), that strategically prioritizes more challenging tasks, thereby enhancing overall learning efficiency. SMT introduces a dynamic task prioritization strategy, underpinned by an effective metric for assessing task difficulty. This metric ensures an efficient and targeted allocation of training resources, significantly improving learning outcomes. Additionally, SMT incorporates a reset mechanism that periodically reinitializes key network parameters to mitigate the simplicity bias, further enhancing the adaptability and robustness of the learning process across diverse tasks. The efficacy of SMT's scheduling method is validated by significantly improving performance on challenging Meta-World benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myungsik Cho",
      "Jongeui Park",
      "Suyoung Lee",
      "Youngchul Sung"
    ]
  },
  "https://proceedings.mlr.press/v235/cho24e.html": {
    "title": "KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation",
    "volume": "main",
    "abstract": "Large Language Model or LLM inference has two phases, the prompt (or prefill) phase to output the first token and the extension (or decoding) phase to the generate subsequent tokens. In this work, we propose an efficient parallelization scheme, KV-Runahead to accelerate the prompt phase. The key observation is that the extension phase generates tokens faster than the prompt phase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes the prompt phase by orchestrating multiple processes to populate the KV-cache and minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache scheme has two main benefits. First, since KV-cache is designed to leverage the causal attention map, we minimize computation and computation automatically. Second, since it already exists for the extension phase, KV-Runahead is easy to implement. We further propose context-level load-balancing to handle uneven KV-cache generation (due to the causal attention) and to optimize TTFT. Compared with an existing parallelization scheme such as tensor or sequential parallelization where keys and values are locally generated and exchanged via all-gather collectives, our experimental results demonstrate that KV-Runahead can offer over 1.4× and 1.6× speedups for Llama 7B and Falcon 7B respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsik Cho",
      "Mohammad Rastegari",
      "Devang Naik"
    ]
  },
  "https://proceedings.mlr.press/v235/cho24f.html": {
    "title": "Neurodegenerative Brain Network Classification via Adaptive Diffusion with Temporal Regularization",
    "volume": "main",
    "abstract": "Analysis of neurodegenerative diseases on brain connectomes is important in facilitating early diagnosis and predicting its onset. However, investigation of the progressive and irreversible dynamics of these diseases remains underexplored in cross-sectional studies as its diagnostic groups are considered independent. Also, as in many real-world graphs, brain networks exhibit intricate structures with both homophily and heterophily. To address these challenges, we propose Adaptive Graph diffusion network with Temporal regularization (AGT). AGT introduces node-wise convolution to adaptively capture low (i.e., homophily) and high-frequency (i.e., heterophily) characteristics within an optimally tailored range for each node. Moreover, AGT captures sequential variations within progressive diagnostic groups with a novel temporal regularization, considering the relative feature distance between the groups in the latent space. As a result, our proposed model yields interpretable results at both node-level and group-level. The superiority of our method is validated on two neurodegenerative disease benchmarks for graph classification: Alzheimer's Disease Neuroimaging Initiative (ADNI) and Parkinson's Progression Markers Initiative (PPMI) datasets",
    "checked": true,
    "id": "af6e5c1722937adb5520f088de5193833004c6f3",
    "semantic_title": "neurodegenerative brain network classification via adaptive diffusion with temporal regularization",
    "citation_count": 1,
    "authors": [
      "Hyuna Cho",
      "Jaeyoon Sim",
      "Guorong Wu",
      "Won Hwa Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/cho24g.html": {
    "title": "Tilt and Average : Geometric Adjustment of the Last Layer for Recalibration",
    "volume": "main",
    "abstract": "After the revelation that neural networks tend to produce overconfident predictions, the problem of calibration, which aims to align confidence with accuracy to enhance the reliability of predictions, has gained significant importance. Several solutions based on calibration maps have been proposed to address the problem of recalibrating a trained classifier using additional datasets. In this paper, we offer an algorithm that transforms the weights of the last layer of the classifier, distinct from the calibration-map-based approach. We concentrate on the geometry of the final linear layer, specifically its angular aspect, and adjust the weights of the corresponding layer. We name the method Tilt and Average, and validate the calibration effect empirically and theoretically. Through this, we demonstrate that our approach, in addition to the existing calibration-map-based techniques, can yield improved calibration performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyusang Cho",
      "Chan-Hyun Youn"
    ]
  },
  "https://proceedings.mlr.press/v235/choi24a.html": {
    "title": "Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "Wasserstein gradient flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrized transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.42 on CelebA-HQ-256, which are comparable to state-of-the-art image generative models",
    "checked": true,
    "id": "4aff6ed1511dbe413ea791db92cc3f4bff4702e5",
    "semantic_title": "scalable wasserstein gradient flow for generative modeling through unbalanced optimal transport",
    "citation_count": 2,
    "authors": [
      "Jaemoo Choi",
      "Jaewoong Choi",
      "Myungjoo Kang"
    ]
  },
  "https://proceedings.mlr.press/v235/choi24b.html": {
    "title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning",
    "volume": "main",
    "abstract": "In Reinforcement Learning (RL), designing precise reward functions remains to be a challenge, particularly when aligning with human intent. Preference-based RL (PbRL) was introduced to address this problem by learning reward models from human feedback. However, existing PbRL methods have limitations as they often overlook the second-order preference that indicates the relative strength of preference. In this paper, we propose Listwise Reward Estimation (LiRE), a novel approach for offline PbRL that leverages second-order preference information by constructing a Ranked List of Trajectories (RLT), which can be efficiently built by using the same ternary feedback type as traditional methods. To validate the effectiveness of LiRE, we propose a new offline PbRL dataset that objectively reflects the effect of the estimated rewards. Our extensive experiments on the dataset demonstrate the superiority of LiRE, i.e., outperforming state-of-the-art baselines even with modest feedback budgets and enjoying robustness with respect to the number of feedbacks and feedback noise. Our code is available at https://github.com/chwoong/LiRE",
    "checked": true,
    "id": "8525434adbf25984e55c78063c71bcb958d364e4",
    "semantic_title": "listwise reward estimation for offline preference-based reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Heewoong Choi",
      "Sangwon Jung",
      "Hongjoon Ahn",
      "Taesup Moon"
    ]
  },
  "https://proceedings.mlr.press/v235/choi24c.html": {
    "title": "BWS: Best Window Selection Based on Sample Scores for Data Pruning across Broad Ranges",
    "volume": "main",
    "abstract": "Data subset selection aims to find a smaller yet informative subset of a large dataset that can approximate the full-dataset training, addressing challenges associated with training neural networks on large-scale datasets. However, existing methods tend to specialize in either high or low selection ratio regimes, lacking a universal approach that consistently achieves competitive performance across a broad range of selection ratios. We introduce a universal and efficient data subset selection method, Best Window Selection (BWS), by proposing a method to choose the best window subset from samples ordered based on their difficulty scores. This approach offers flexibility by allowing the choice of window intervals that span from easy to difficult samples. Furthermore, we provide an efficient mechanism for selecting the best window subset by evaluating its quality using kernel ridge regression. Our experimental results demonstrate the superior performance of BWS compared to other baselines across a broad range of selection ratios over datasets, including CIFAR-10/100 and ImageNet, and the scenarios involving training from random initialization or fine-tuning of pre-trained models",
    "checked": true,
    "id": "be525ad66a6ddd23ab293468190ad5f37f45e218",
    "semantic_title": "bws: best window selection based on sample scores for data pruning across broad ranges",
    "citation_count": 1,
    "authors": [
      "Hoyong Choi",
      "Nohyun Ki",
      "Hye Won Chung"
    ]
  },
  "https://proceedings.mlr.press/v235/choi24d.html": {
    "title": "Embodied CoT Distillation From LLM To Off-the-shelf Agents",
    "volume": "main",
    "abstract": "We address the challenge of utilizing large language models (LLMs) for complex embodied tasks, in the environment where decision-making systems operate timely on capacity-limited, off-the-shelf devices. We present DeDer, a framework for decomposing and distilling the embodied reasoning capabilities from LLMs to efficient, small language model (sLM)-based policies. In DeDer, the decision-making process of LLM-based strategies is restructured into a hierarchy with a reasoning-policy and planning-policy. The reasoning-policy is distilled from the data that is generated through the embodied in-context learning and self-verification of an LLM, so it can produce effective rationales. The planning-policy, guided by the rationales, can render optimized plans efficiently. In turn, DeDer allows for adopting sLMs for both policies, deployed on off-the-shelf devices. Furthermore, to enhance the quality of intermediate rationales, specific to embodied tasks, we devise the embodied knowledge graph, and to generate multiple rationales timely through a single inference, we also use the contrastively prompted attention model. Our experiments with the ALFRED benchmark demonstrate that DeDer surpasses leading language planning and distillation approaches, indicating the applicability and efficiency of sLM-based embodied policies derived through DeDer",
    "checked": true,
    "id": "89435c392b611b1e804c12d0176661e91ea54ea1",
    "semantic_title": "embodied cot distillation from llm to off-the-shelf agents",
    "citation_count": 1,
    "authors": [
      "Wonje Choi",
      "Woo Kyung Kim",
      "Minjong Yoo",
      "Honguk Woo"
    ]
  },
  "https://proceedings.mlr.press/v235/choi24e.html": {
    "title": "PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are trained on massive text corpora, which are encoded with diverse personality traits. This triggers an interesting goal of eliciting a desired personality trait from the LLM, and probing its behavioral preferences. Accordingly, we formalize the persona elicitation task, aiming to customize LLM behaviors to align with a target persona. We present Persona In-Context Learning (PICLe), a novel persona elicitation framework grounded in Bayesian inference. At the core, PICLe introduces a new ICL example selection criterion based on likelihood ratio, which is designed to optimally guide the model in eliciting a specific target persona. We demonstrate the effectiveness of PICLe through extensive comparisons against baseline methods across three contemporary LLMs. Code is available at https://github.com/deeplearning-wisc/picle",
    "checked": true,
    "id": "dd5ef6ceed4e77eb2702b40ba4afece05c1b0af8",
    "semantic_title": "picle: eliciting diverse behaviors from large language models with persona in-context learning",
    "citation_count": 8,
    "authors": [
      "Hyeong Kyu Choi",
      "Yixuan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/choi24f.html": {
    "title": "PANDA: Expanded Width-Aware Message Passing Beyond Rewiring",
    "volume": "main",
    "abstract": "Recent research in the field of graph neural network (GNN) has identified a critical issue known as \"over-squashing,\" resulting from the bottleneck phenomenon in graph structures, which impedes the propagation of long-range information. Prior works have proposed a variety of graph rewiring concepts that aim at optimizing the spatial or spectral properties of graphs to promote the signal propagation. However, such approaches inevitably deteriorate the original graph topology, which may lead to a distortion of information flow. To address this, we introduce an expanded width-aware (PANDA) message passing, a new message passing paradigm where nodes with high centrality, a potential source of over-squashing, are selectively expanded in width to encapsulate the growing influx of signals from distant nodes. Experimental results show that our method outperforms existing rewiring methods, suggesting that selectively expanding the hidden state of nodes can be a compelling alternative to graph rewiring for addressing the over-squashing",
    "checked": true,
    "id": "8832ab4d6aee5695cce6e1a2b6ff1d614075f156",
    "semantic_title": "panda: expanded width-aware message passing beyond rewiring",
    "citation_count": 1,
    "authors": [
      "Jeongwhan Choi",
      "Sumin Park",
      "Hyowon Wi",
      "Sung-Bae Cho",
      "Noseong Park"
    ]
  },
  "https://proceedings.mlr.press/v235/choo24a.html": {
    "title": "Online bipartite matching with imperfect advice",
    "volume": "main",
    "abstract": "We study the problem of online unweighted bipartite matching with $n$ offline vertices and $n$ online vertices where one wishes to be competitive against the optimal offline algorithm. While the classic RANKING algorithm of (Karp et al., 1990) provably attains competitive ratio of $1-1/e > 1/2$, we show that no learning-augmented method can be both 1-consistent and strictly better than 1/2-robust under the adversarial arrival model. Meanwhile, under the random arrival model, we show how one can utilize methods from distribution testing to design an algorithm that takes in external advice about the online vertices and provably achieves competitive ratio interpolating between any ratio attainable by advice-free methods and the optimal ratio of 1, depending on the advice quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davin Choo",
      "Themistoklis Gouleakis",
      "Chun Kai Ling",
      "Arnab Bhattacharyya"
    ]
  },
  "https://proceedings.mlr.press/v235/chopin24a.html": {
    "title": "A connection between Tempering and Entropic Mirror Descent",
    "volume": "main",
    "abstract": "This paper explores the connections between tempering (for Sequential Monte Carlo; SMC) and entropic mirror descent to sample from a target probability distribution whose unnormalized density is known. We establish that tempering SMC corresponds to entropic mirror descent applied to the reverse Kullback-Leibler (KL) divergence and obtain convergence rates for the tempering iterates. Our result motivates the tempering iterates from an optimization point of view, showing that tempering can be seen as a descent scheme of the KL divergence with respect to the Fisher-Rao geometry, in contrast to Langevin dynamics that perform descent of the KL with respect to the Wasserstein-2 geometry. We exploit the connection between tempering and mirror descent iterates to justify common practices in SMC and derive adaptive tempering rules that improve over other alternative benchmarks in the literature",
    "checked": true,
    "id": "b7db5f78fc061081e896dc00c8ad2c60f248902c",
    "semantic_title": "a connection between tempering and entropic mirror descent",
    "citation_count": 10,
    "authors": [
      "Nicolas Chopin",
      "Francesca Crucinio",
      "Anna Korba"
    ]
  },
  "https://proceedings.mlr.press/v235/choukroun24a.html": {
    "title": "Learning Linear Block Error Correction Codes",
    "volume": "main",
    "abstract": "Error correction codes are a crucial part of the physical communication layer, ensuring the reliable transfer of data over noisy channels. The design of optimal linear block codes capable of being efficiently decoded is of major concern, especially for short block lengths. While neural decoders have recently demonstrated their advantage over classical decoding techniques, the neural design of the codes remains a challenge. In this work, we propose for the first time a unified encoder-decoder training of binary linear block codes. To this end, we adapt the coding setting to support efficient and differentiable training of the code for end-to-end optimization over the order two Galois field. We also propose a novel Transformer model in which the self-attention masking is performed in a differentiable fashion for the efficient backpropagation of the code gradient. Our results show that (i) the proposed decoder outperforms existing neural decoding on conventional codes, (ii) the suggested framework generates codes that outperform the analogous conventional codes, and (iii) the codes we developed not only excel with our decoder but also show enhanced performance with traditional decoding techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoni Choukroun",
      "Lior Wolf"
    ]
  },
  "https://proceedings.mlr.press/v235/chowdhury24a.html": {
    "title": "A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts",
    "volume": "main",
    "abstract": "The sparsely gated mixture of experts (MoE) architecture sends different inputs to different subnetworks (experts), through trainable routers. MoE reduces the training computation significantly for large models, but its deployment can be still memory/computation expensive for some downstream tasks. Model pruning is a popular approach to reduce inference computation, but its application in MoE architecture is largely unexplored. To the best of our knowledge, this paper provides the first provably efficient technique for pruning experts in fine-tuned MoE models. We theoretically prove that prioritizing the pruning of the experts with a smaller change of the router's $l_2$ norm from the pre-trained model guarantees the preservation of test accuracy, while significantly reducing the model size and the computational requirements. Although our theoretical analysis is centered on binary classification tasks on simplified MoE architecture, our expert pruning method is verified on large vision MoE models such as V-MoE and $\\text{E}^3$-MoE fine-tuned on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Nowaz Rabbani Chowdhury",
      "Meng Wang",
      "Kaoutar El Maghraoui",
      "Naigang Wang",
      "Pin-Yu Chen",
      "Christopher Carothers"
    ]
  },
  "https://proceedings.mlr.press/v235/chu24a.html": {
    "title": "SPABA: A Single-Loop and Probabilistic Stochastic Bilevel Algorithm Achieving Optimal Sample Complexity",
    "volume": "main",
    "abstract": "While stochastic bilevel optimization methods have been extensively studied for addressing large-scale nested optimization problems in machine learning, it remains an open question whether the optimal complexity bounds for solving bilevel optimization are the same as those in single-level optimization. Our main result resolves this question: SPABA, an adaptation of the PAGE method for nonconvex optimization in (Li et al., 2021) to the bilevel setting, can achieve optimal sample complexity in both the finite-sum and expectation settings. We show the optimality of SPABA by proving that there is no gap in complexity analysis between stochastic bilevel and single-level optimization when implementing PAGE. Notably, as indicated by the results of (Dagréou et al., 2022), there might exist a gap in complexity analysis when implementing other stochastic gradient estimators, like SGD and SAGA. In addition to SPABA, we propose several other single-loop stochastic bilevel algorithms, that either match or improve the state-of-the-art sample complexity results, leveraging our convergence rate and complexity analysis. Numerical experiments demonstrate the superior practical performance of the proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianshu Chu",
      "Dachuan Xu",
      "Wei Yao",
      "Jin Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/chua24a.html": {
    "title": "How Private are DP-SGD Implementations?",
    "volume": "main",
    "abstract": "We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling-based DP-SGD is more commonly used in practical implementations, it has not been amenable to easy privacy analysis, either analytically or even numerically. On the other hand, Poisson subsampling-based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling-based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lynn Chua",
      "Badih Ghazi",
      "Pritish Kamath",
      "Ravi Kumar",
      "Pasin Manurangsi",
      "Amer Sinha",
      "Chiyuan Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/chung24a.html": {
    "title": "Sampling-based Multi-dimensional Recalibration",
    "volume": "main",
    "abstract": "Calibration of probabilistic forecasts in the regression setting has been widely studied in the single dimensional case, where the output variables are assumed to be univariate. In many problem settings, however, the output variables are multi-dimensional, and in the presence of dependence across the output dimensions, measuring calibration and performing recalibration for each dimension separately can be both misleading and detrimental. In this work, we focus on representing predictive uncertainties via samples, and propose a recalibration method which accounts for the joint distribution across output dimensions to produce calibrated samples. Based on the concept of highest density regions (HDR), we define the notion of HDR calibration, and show that our recalibration method produces samples which are HDR calibrated. We demonstrate the performance of our method and the quality of the recalibrated samples on a suite of benchmark datasets in multi-dimensional regression, a real-world dataset in modeling plasma dynamics during nuclear fusion reactions, and on a decision-making application in forecasting demand",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngseog Chung",
      "Ian Char",
      "Jeff Schneider"
    ]
  },
  "https://proceedings.mlr.press/v235/chung24b.html": {
    "title": "Prompt-tuning Latent Diffusion Models for Inverse Problems",
    "volume": "main",
    "abstract": "We propose a new method for solving imaging inverse problems using text-to-image latent diffusion models as general priors. Existing methods using latent diffusion models for inverse problems typically rely on simple null text prompts, which can lead to suboptimal performance. To improve upon this, we introduce a method for prompt tuning, which jointly optimizes the text embedding on-the-fly while running the reverse diffusion. This allows us to generate images that are more faithful to the diffusion prior. Specifically, our approach involves a unified optimization framework that simultaneously considers the prompt, latent, and pixel values through alternating minimization. This significantly diminishes image artifacts - a major problem when using latent diffusion models instead of pixel-based diffusion ones. Our method, called P2L, outperforms both pixel- and latent-diffusion model-based inverse problem solvers on a variety of tasks, such as super-resolution, deblurring, and inpainting. Furthermore, P2L demonstrates remarkable scalability to higher resolutions without artifacts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungjin Chung",
      "Jong Chul Ye",
      "Peyman Milanfar",
      "Mauricio Delbracio"
    ]
  },
  "https://proceedings.mlr.press/v235/cideron24a.html": {
    "title": "MusicRL: Aligning Music Generation to Human Preferences",
    "volume": "main",
    "abstract": "We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat workout music\" can map to a retro guitar solo or a technopop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM model of discrete audio tokens finetuned with reinforcement learning to maximize sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models. Samples can be found at google-research.github.io/seanet/musiclm/rlhf/",
    "checked": true,
    "id": "11626a6709e24e3f4c4351acca22f73335309a84",
    "semantic_title": "musicrl: aligning music generation to human preferences",
    "citation_count": 8,
    "authors": [
      "Geoffrey Cideron",
      "Sertan Girgin",
      "Mauro Verzetti",
      "Damien Vincent",
      "Matej Kastelic",
      "Zalán Borsos",
      "Brian Mcwilliams",
      "Victor Ungureanu",
      "Olivier Bachem",
      "Olivier Pietquin",
      "Matthieu Geist",
      "Leonard Hussenot",
      "Neil Zeghidour",
      "Andrea Agostinelli"
    ]
  },
  "https://proceedings.mlr.press/v235/cini24a.html": {
    "title": "Graph-based Time Series Clustering for End-to-End Hierarchical Forecasting",
    "volume": "main",
    "abstract": "Relationships among time series can be exploited as inductive biases in learning effective forecasting models. In hierarchical time series, relationships among subsets of sequences induce hard constraints (hierarchical inductive biases) on the predicted values. In this paper, we propose a graph-based methodology to unify relational and hierarchical inductive biases in the context of deep learning for time series forecasting. In particular, we model both types of relationships as dependencies in a pyramidal graph structure, with each pyramidal layer corresponding to a level of the hierarchy. By exploiting modern - trainable - graph pooling operators we show that the hierarchical structure, if not available as a prior, can be learned directly from data, thus obtaining cluster assignments aligned with the forecasting objective. A differentiable reconciliation stage is incorporated into the processing architecture, allowing hierarchical constraints to act both as an architectural bias as well as a regularization element for predictions. Simulation results on representative datasets show that the proposed method compares favorably against the state of the art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Cini",
      "Danilo Mandic",
      "Cesare Alippi"
    ]
  },
  "https://proceedings.mlr.press/v235/clarke24a.html": {
    "title": "Studying K-FAC Heuristics by Viewing Adam through a Second-Order Lens",
    "volume": "main",
    "abstract": "Research into optimisation for deep learning is characterised by a tension between the computational efficiency of first-order, gradient-based methods (such as SGD and Adam) and the theoretical efficiency of second-order, curvature-based methods (such as quasi-Newton methods and K-FAC). Noting that second-order methods often only function effectively with the addition of stabilising heuristics (such as Levenberg-Marquardt damping), we ask how much these (as opposed to the second-order curvature model) contribute to second-order algorithms' performance. We thus study AdamQLR: an optimiser combining damping and learning rate selection techniques from K-FAC (Martens & Grosse, 2015) with the update directions proposed by Adam, inspired by considering Adam through a second-order lens. We evaluate AdamQLR on a range of regression and classification tasks at various scales and hyperparameter tuning methodologies, concluding K-FAC's adaptive heuristics are of variable standalone general effectiveness, and finding an untuned AdamQLR setting can achieve comparable performance vs runtime to tuned benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ross M Clarke",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://proceedings.mlr.press/v235/clavier24a.html": {
    "title": "$\\mathttVITS$ : Variational Inference Thompson Sampling for contextual bandits",
    "volume": "main",
    "abstract": "In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference TS $\\mathtt{VITS}$, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that $\\mathtt{VITS}$ achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we demonstrate experimentally the effectiveness of $\\mathtt{VITS}$ on both synthetic and real world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Clavier",
      "Tom Huix",
      "Alain Oliviero Durmus"
    ]
  },
  "https://proceedings.mlr.press/v235/coda-forno24a.html": {
    "title": "CogBench: a large language model walks into a psychology lab",
    "volume": "main",
    "abstract": "Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 40 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Coda-Forno",
      "Marcel Binz",
      "Jane X Wang",
      "Eric Schulz"
    ]
  },
  "https://proceedings.mlr.press/v235/cohen24a.html": {
    "title": "Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices",
    "volume": "main",
    "abstract": "Text-to-image (T2I) diffusion models achieve state-of-the-art results in image synthesis and editing. However, leveraging such pre-trained models for video editing is considered a major challenge. Many existing works attempt to enforce temporal consistency in the edited video through explicit correspondence mechanisms, either in pixel space or between deep features. These methods, however, struggle with strong nonrigid motion. In this paper, we introduce a fundamentally different approach, which is based on the observation that spatiotemporal slices of natural videos exhibit similar characteristics to natural images. Thus, the same T2I diffusion model that is normally used only as a prior on video frames, can also serve as a strong prior for enhancing temporal consistency by applying it on spatiotemporal slices. Based on this observation, we present Slicedit, a method for text-based video editing that utilizes a pre-trained T2I diffusion model to process both spatial and spatiotemporal slices. Our method generates videos that retain the structure and motion of the original video while adhering to the target text. Through extensive experiments, we demonstrate Slicedit's ability to edit a wide range of real-world videos, confirming its clear advantages compared to existing baselines",
    "checked": true,
    "id": "971e40bdf959afe5591f7a1a5ea1f8a82e60b629",
    "semantic_title": "slicedit: zero-shot video editing with text-to-image diffusion models using spatio-temporal slices",
    "citation_count": 0,
    "authors": [
      "Nathaniel Cohen",
      "Vladimir Kulikov",
      "Matan Kleiner",
      "Inbar Huberman-Spiegelglas",
      "Tomer Michaeli"
    ]
  },
  "https://proceedings.mlr.press/v235/cohen24b.html": {
    "title": "Improving Token-Based World Models with Parallel Observation Prediction",
    "volume": "main",
    "abstract": "Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours. Our code is available at https://github.com/leor-c/REM",
    "checked": true,
    "id": "d755b1b9c0d0efebbe69be4e8bd96cd65c35be95",
    "semantic_title": "improving token-based world models with parallel observation prediction",
    "citation_count": 1,
    "authors": [
      "Lior Cohen",
      "Kaixin Wang",
      "Bingyi Kang",
      "Shie Mannor"
    ]
  },
  "https://proceedings.mlr.press/v235/cohen-addad24a.html": {
    "title": "Perturb-and-Project: Differentially Private Similarities and Marginals",
    "volume": "main",
    "abstract": "We revisit the objective perturbations framework for differential privacy where noise is added to the input $A\\in \\mathcal{S}$ and the result is then projected back to the space of admissible datasets $\\mathcal{S}$. Through this framework, we first design novel efficient algorithms to privately release pair-wise cosine similarities. Second, we derive a novel algorithm to compute $k$-way marginal queries over $n$ features. Prior work could achieve comparable guarantees only for $k$ even. Furthermore, we extend our results to $t$-sparse datasets, where our efficient algorithms yields novel, stronger guarantees whenever $t\\le n^{5/6}/\\log n.$ Finally, we provide a theoretical perspective on why fast input perturbation algorithms works well in practice. The key technical ingredients behind our results are tight sum-of-squares certificates upper bounding the Gaussian complexity of sets of solutions",
    "checked": true,
    "id": "b6258d3aa1e99808e1dce164818e86da931df727",
    "semantic_title": "perturb-and-project: differentially private similarities and marginals",
    "citation_count": 0,
    "authors": [
      "Vincent Cohen-Addad",
      "Tommaso D’Orsi",
      "Alessandro Epasto",
      "Vahab Mirrokni",
      "Peilin Zhong"
    ]
  },
  "https://proceedings.mlr.press/v235/cohen-addad24b.html": {
    "title": "Multi-View Stochastic Block Models",
    "volume": "main",
    "abstract": "Graph clustering is a central topic in unsupervised learning with a multitude of practical applications. In recent years, multi-view graph clustering has gained a lot of attention for its applicability to real-world instances where one often has access to multiple data sources. In this paper we formalize a new family of models, called multi-view stochastic block models that capture this setting. For this model, we first study efficient algorithms that naively work on the union of multiple graphs. Then, we introduce a new efficient algorithm that provably outperforms previous approaches by analyzing the structure of each graph separately. Finally, we complement our results with an information-theoretic lower bound studying the limits of what can be done in this model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Cohen-Addad",
      "Tommaso D’Orsi",
      "Silvio Lattanzi",
      "Rajai Nasser"
    ]
  },
  "https://proceedings.mlr.press/v235/cohen-addad24c.html": {
    "title": "A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case Graph Clustering",
    "volume": "main",
    "abstract": "We consider the semi-random graph model of [Makarychev, Makarychev and Vijayaraghavan, STOC'12], where, given a random bipartite graph with $\\alpha$ edges and an unknown bipartition $(A, B)$ of the vertex set, an adversary can add arbitrary edges inside each community and remove arbitrary edges from the cut $(A, B)$ (i.e. all adversarial changes are monotone with respect to the bipartition). For this model, a polynomial time algorithm [MMV'12] is known to approximate the Balanced Cut problem up to value $O(\\alpha)$ as long as the cut $(A, B)$ has size $\\Omega(\\alpha)$. However, it consists of slow subroutines requiring optimal solutions for logarithmically many semidefinite programs. We study the fine-grained complexity of the problem and present the first near-linear time algorithm that achieves similar performances to that of [MMV'12]. Our algorithm runs in time $O(|V(G)|^{1+o(1)} + |E(G)|^{1+o(1)})$ and finds a balanced cut of value $O(\\alpha).$ Our approach appears easily extendible to related problem, such as Sparsest Cut, and also yields an near-linear time $O(1)$-approximation to Dagupta's objective function for hierarchical clustering [Dasgupta, STOC'16] for the semi-random hierarchical stochastic block model inputs of [Cohen-Addad, Kanade, Mallmann-Trenn, Mathieu, JACM'19]",
    "checked": true,
    "id": "34357b40cccc9bb170e85fee493853b0abc84dcb",
    "semantic_title": "a near-linear time approximation algorithm for beyond-worst-case graph clustering",
    "citation_count": 0,
    "authors": [
      "Vincent Cohen-Addad",
      "Tommaso D’Orsi",
      "Aida Mousavifar"
    ]
  },
  "https://proceedings.mlr.press/v235/cohen-addad24d.html": {
    "title": "Dynamic Correlation Clustering in Sublinear Update Time",
    "volume": "main",
    "abstract": "We study the classic problem of correlation clustering in dynamic vertex streams. In this setting, vertices are either added or randomly deleted over time, and each vertex pair is connected by a positive or negative edge. The objective is to continuously find a partition which minimizes the sum of positive edges crossing clusters and negative edges within clusters. We present an algorithm that maintains an $O(1)$-approximation with $O(\\text{polylog} n)$ amortized update time. Prior to our work Behnezhad et al. in SODA 2023 achieved a $5$-approximation with $O(1)$ expected update time in edge streams which translates in vertex streams to an $O(D)$-update time where $D$ is the maximum possible degree. Finally we complement our theoretical analysis with experiments on real world data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Cohen-Addad",
      "Silvio Lattanzi",
      "Andreas Maggiori",
      "Nikos Parotsidis"
    ]
  },
  "https://proceedings.mlr.press/v235/colbert24a.html": {
    "title": "A2Q+: Improving Accumulator-Aware Weight Quantization",
    "volume": "main",
    "abstract": "Quantization techniques commonly reduce the inference costs of neural networks by restricting the precision of weights and activations. Recent studies show that also reducing the precision of the accumulator can further improve hardware efficiency at the risk of numerical overflow, which introduces arithmetic errors that can degrade model accuracy. To avoid numerical overflow while maintaining accuracy, recent work proposed accumulator-aware quantization (A2Q)—a quantization-aware training method that constrains model weights during training to safely use a target accumulator bit width during inference. Although this shows promise, we demonstrate that A2Q relies on an overly restrictive constraint and a sub-optimal weight initialization strategy that each introduce superfluous quantization error. To address these shortcomings, we introduce: (1) an improved bound that alleviates accumulator constraints without compromising overflow avoidance; and (2) a new strategy for initializing quantized weights from pre-trained floating-point checkpoints. We combine these contributions with weight normalization to introduce A2Q+. We identify and characterize the various tradeoffs that arise as a consequence of accumulator constraints and support our analysis with experiments that show A2Q+ significantly improves these trade-offs when compared to prior methods",
    "checked": true,
    "id": "80041f0c16b84bcb0acb24161da08dfebe93135d",
    "semantic_title": "a2q+: improving accumulator-aware weight quantization",
    "citation_count": 1,
    "authors": [
      "Ian Colbert",
      "Alessandro Pappalardo",
      "Jakoba Petri-Koenig",
      "Yaman Umuroglu"
    ]
  },
  "https://proceedings.mlr.press/v235/collins24a.html": {
    "title": "Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks",
    "volume": "main",
    "abstract": "An increasingly popular machine learning paradigm is to pretrain a neural network (NN) on many tasks offline, then adapt it to downstream tasks, often by re-training only the last linear layer of the network. This approach yields strong downstream performance in a variety of contexts, demonstrating that multitask pretraining leads to effective feature learning. Although several recent theoretical studies have shown that shallow NNs learn meaningful features when either (i) they are trained on a single task or (ii) they are linear, very little is known about the closer-to-practice case of nonlinear NNs trained on multiple tasks. In this work, we present the first results proving that feature learning occurs during training with a nonlinear model on multiple tasks. Our key insight is that multi-task pretraining induces a pseudo-contrastive loss that favors representations that align points that typically have the same label across tasks. Using this observation, we show that when the tasks are binary classification tasks with labels depending on the projection of the data onto an $r$-dimensional subspace within the $d\\gg r$-dimensional input space, a simple gradient-based multitask learning algorithm on a two-layer ReLU NN recovers this projection, allowing for generalization to downstream tasks with sample and neuron complexity independent of $d$. In contrast, we show that with high probability over the draw of a single task, training on this single task cannot guarantee to learn all $r$ ground-truth features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liam Collins",
      "Hamed Hassani",
      "Mahdi Soltanolkotabi",
      "Aryan Mokhtari",
      "Sanjay Shakkottai"
    ]
  },
  "https://proceedings.mlr.press/v235/conitzer24a.html": {
    "title": "Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback",
    "volume": "main",
    "abstract": "Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise problematic behavior, such as helping to commit crimes or producing racist text. One approach to fine-tuning, called reinforcement learning from human feedback, learns from humans' expressed preferences over multiple outputs. Another approach is constitutional AI, in which the input from humans is a list of high-level principles. But how do we deal with potentially diverging input from humans? How can we aggregate the input into consistent data about \"collective\" preferences or otherwise use it to make collective choices about model behavior? In this paper, we argue that the field of social choice is well positioned to address these questions, and we discuss ways forward for this agenda, drawing on discussions in a recent workshop on Social Choice for AI Ethics and Safety held in Berkeley, CA, USA in December 2023",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Conitzer",
      "Rachel Freedman",
      "Jobst Heitzig",
      "Wesley H. Holliday",
      "Bob M. Jacobs",
      "Nathan Lambert",
      "Milan Mosse",
      "Eric Pacuit",
      "Stuart Russell",
      "Hailey Schoelkopf",
      "Emanuel Tewolde",
      "William S. Zwicker"
    ]
  },
  "https://proceedings.mlr.press/v235/cortes-gomez24a.html": {
    "title": "Statistical Inference Under Constrained Selection Bias",
    "volume": "main",
    "abstract": "Large-scale datasets are increasingly being used to inform decision making. While this effort aims to ground policy in real-world evidence, challenges have arisen as selection bias and other forms of distribution shifts often plague observational data. Previous attempts to provide robust inference have given guarantees depending on a user-specified amount of possible distribution shift (e.g., the maximum KL divergence between the observed and target distributions). However, decision makers will often have additional knowledge about the target distribution which constrains the kind of possible shifts. To leverage such information, we propose a framework that enables statistical inference in the presence of selection bias which obeys user-specified constraints in the form of functions whose expectation is known under the target distribution. The output is high-probability bounds on the value of an estimand for the target distribution. Hence, our method leverages domain knowledge in order to partially identify a wide class of estimands. We analyze the computational and statistical properties of methods to estimate these bounds and show that our method can produce informative bounds on a variety of simulated and semisynthetic tasks, as well as in a real-world use case",
    "checked": true,
    "id": "3d799df17c4d68788b83ff603be6707ddcd41433",
    "semantic_title": "statistical inference under constrained selection bias",
    "citation_count": 0,
    "authors": [
      "Santiago Cortes-Gomez",
      "Mateo Dulce Rubio",
      "Carlos Miguel Patiño",
      "Bryan Wilder"
    ]
  },
  "https://proceedings.mlr.press/v235/covert24a.html": {
    "title": "Scaling Laws for the Value of Individual Data Points in Machine Learning",
    "volume": "main",
    "abstract": "Recent works have shown that machine learning models improve at a predictable rate with the amount of training data, leading to scaling laws that describe the relationship between error and dataset size. These scaling laws can help determine a model's training dataset, but they take an aggregate view of the data by only considering the dataset's size. We consider a new perspective by investigating scaling behavior for the value of individual data points: we find that a data point's contribution to model's performance shrinks predictably with the size of the dataset in a log-linear manner. Interestingly, there is significant variability in the scaling exponent among different data points, indicating that certain points are more valuable in small datasets and other points are relatively more useful as a part of large datasets. We provide learning theory support for our scaling laws and we observe empirically that it holds across several model classes. We further propose a maximum likelihood estimator and an amortized estimator to efficiently learn the individualized scaling behaviors from a small number of noisy observations per data point. Using our efficient estimators, we provide insights into factors that influence the scaling behavior of different data points. Finally we demonstrate applications of the individualized scaling laws to data valuation and data subset selection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Connick Covert",
      "Wenlong Ji",
      "Tatsunori Hashimoto",
      "James Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/crabbe24a.html": {
    "title": "Time Series Diffusion in the Frequency Domain",
    "volume": "main",
    "abstract": "Fourier analysis has been an instrumental tool in the development of signal processing. This leads us to wonder whether this framework could similarly benefit generative modelling. In this paper, we explore this question through the scope of time series diffusion models. More specifically, we analyze whether representing time series in the frequency domain is a useful inductive bias for score-based diffusion models. By starting from the canonical SDE formulation of diffusion in the time domain, we show that a dual diffusion process occurs in the frequency domain with an important nuance: Brownian motions are replaced by what we call mirrored Brownian motions, characterized by mirror symmetries among their components. Building on this insight, we show how to adapt the denoising score matching approach to implement diffusion models in the frequency domain. This results in frequency diffusion models, which we compare to canonical time diffusion models. Our empirical evaluation on real-world datasets, covering various domains like healthcare and finance, shows that frequency diffusion models better capture the training distribution than time diffusion models. We explain this observation by showing that time series from these datasets tend to be more localized in the frequency domain than in the time domain, which makes them easier to model in the former case. All our observations point towards impactful synergies between Fourier analysis and diffusion models",
    "checked": true,
    "id": "712d5b54c70679c2ea96861878e4b9ab36bc30f8",
    "semantic_title": "time series diffusion in the frequency domain",
    "citation_count": 5,
    "authors": [
      "Jonathan Crabbé",
      "Nicolas Huynh",
      "Jan Pawel Stanczuk",
      "Mihaela Van Der Schaar"
    ]
  },
  "https://proceedings.mlr.press/v235/cresswell24a.html": {
    "title": "Conformal Prediction Sets Improve Human Decision Making",
    "volume": "main",
    "abstract": "In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams",
    "checked": true,
    "id": "b6e698f0f17506715c8cfc1fa5d27c7b589b51ba",
    "semantic_title": "conformal prediction sets improve human decision making",
    "citation_count": 11,
    "authors": [
      "Jesse C. Cresswell",
      "Yi Sui",
      "Bhargava Kumar",
      "Noël Vouitsis"
    ]
  },
  "https://proceedings.mlr.press/v235/crispino24a.html": {
    "title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners",
    "volume": "main",
    "abstract": "We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. To enable this, our agent only needs to generate a single set of instructions for each task. These instructions turn out to be extremely effective for improving the reasoning process of different large language models across all task instances. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b, Llama-2-70b-chat, and GPT-3.5 Turbo. Compared to zero-shot chain of thought, our improvement in reasoning is striking. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo significantly",
    "checked": true,
    "id": "2a9a5008c333017d7929fd3d7a46f6142fb62ba7",
    "semantic_title": "agent instructs large language models to be general zero-shot reasoners",
    "citation_count": 0,
    "authors": [
      "Nicholas Crispino",
      "Kyle Montgomery",
      "Fankun Zeng",
      "Dawn Song",
      "Chenguang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/crowson24a.html": {
    "title": "Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers",
    "volume": "main",
    "abstract": "We present the Hourglass Diffusion Transformer (HDiT), an image-generative model that exhibits linear scaling with pixel count, supporting training at high resolution (e.g. $1024 \\times 1024$) directly in pixel-space. Building on the Transformer architecture, which is known to scale to billions of parameters, it bridges the gap between the efficiency of convolutional U-Nets and the scalability of Transformers. HDiT trains successfully without typical high-resolution training techniques such as multiscale architectures, latent autoencoders or self-conditioning. We demonstrate that HDiT performs competitively with existing models on ImageNet $256^2$, and sets a new state-of-the-art for diffusion models on FFHQ-$1024^2$. Code is available at https://github.com/crowsonkb/k-diffusion",
    "checked": true,
    "id": "9b91b3031ea159e4964d18b2ce703168660ecf46",
    "semantic_title": "scalable high-resolution pixel-space image synthesis with hourglass diffusion transformers",
    "citation_count": 25,
    "authors": [
      "Katherine Crowson",
      "Stefan Andreas Baumann",
      "Alex Birch",
      "Tanishq Mathew Abraham",
      "Daniel Z Kaplan",
      "Enrico Shippole"
    ]
  },
  "https://proceedings.mlr.press/v235/csillag24a.html": {
    "title": "Generalization Bounds for Causal Regression: Insights, Guarantees and Sensitivity Analysis",
    "volume": "main",
    "abstract": "Many algorithms have been recently proposed for causal machine learning. Yet, there is little to no theory on their quality, especially considering finite samples. In this work, we propose a theory based on generalization bounds that provides such guarantees. By introducing a novel change-of-measure inequality, we are able to tightly bound the model loss in terms of the deviation of the treatment propensities over the population, which we show can be empirically limited. Our theory is fully rigorous and holds even in the face of hidden confounding and violations of positivity. We demonstrate our bounds on semi-synthetic and real data, showcasing their remarkable tightness and practical utility",
    "checked": true,
    "id": "626b0f3d18a8ac1de403e02542f118347c2e4504",
    "semantic_title": "generalization bounds for causal regression: insights, guarantees and sensitivity analysis",
    "citation_count": 0,
    "authors": [
      "Daniel Csillag",
      "Claudio Jose Struchiner",
      "Guilherme Tegoni Goedert"
    ]
  },
  "https://proceedings.mlr.press/v235/cui24a.html": {
    "title": "Major-Minor Mean Field Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Multi-agent reinforcement learning (MARL) remains difficult to scale to many agents. Recent MARL using Mean Field Control (MFC) provides a tractable and rigorous approach to otherwise difficult cooperative MARL. However, the strict MFC assumption of many independent, weakly-interacting agents is too inflexible in practice. We generalize MFC to instead simultaneously model many similar and few complex agents – as Major-Minor Mean Field Control (M3FC). Theoretically, we give approximation results for finite agent control, and verify the sufficiency of stationary policies for optimality together with a dynamic programming principle. Algorithmically, we propose Major-Minor Mean Field MARL (M3FMARL) for finite agent systems instead of the limiting system. The algorithm is shown to approximate the policy gradient of the underlying M3FC MDP. Finally, we demonstrate its capabilities experimentally in various scenarios. We observe a strong performance in comparison to state-of-the-art policy gradient MARL methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Cui",
      "Christian Fabian",
      "Anam Tahir",
      "Heinz Koeppl"
    ]
  },
  "https://proceedings.mlr.press/v235/cui24b.html": {
    "title": "Learning Latent Space Hierarchical EBM Diffusion Models",
    "volume": "main",
    "abstract": "This work studies the learning problem of the energy-based prior model and the multi-layer generator model. The multi-layer generator model, which contains multiple layers of latent variables organized in a top-down hierarchical structure, typically assumes the Gaussian prior model. Such a prior model can be limited in modelling expressivity, which results in a gap between the generator posterior and the prior model, known as the prior hole problem. Recent works have explored learning the energy-based (EBM) prior model as a second-stage, complementary model to bridge the gap. However, the EBM defined on a multi-layer latent space can be highly multi-modal, which makes sampling from such marginal EBM prior challenging in practice, resulting in ineffectively learned EBM. To tackle the challenge, we propose to leverage the diffusion probabilistic scheme to mitigate the burden of EBM sampling and thus facilitate EBM learning. Our extensive experiments demonstrate a superior performance of our diffusion-learned EBM prior on various challenging tasks",
    "checked": true,
    "id": "b70e7d754429b9718a6928bf7b4ff9cf67297d02",
    "semantic_title": "learning latent space hierarchical ebm diffusion models",
    "citation_count": 0,
    "authors": [
      "Jiali Cui",
      "Tian Han"
    ]
  },
  "https://proceedings.mlr.press/v235/cui24c.html": {
    "title": "Harmonizing Generalization and Personalization in Federated Prompt Learning",
    "volume": "main",
    "abstract": "Federated Prompt Learning (FPL) incorporates large pre-trained Vision-Language models (VLM) into federated learning through prompt tuning. The transferable representations and remarkable generalization capacity of VLM make them highly compatible with the integration of federated learning. Addressing data heterogeneity in federated learning requires personalization, but excessive focus on it across clients could compromise the model's ability to generalize effectively. To preserve the impressive generalization capability of VLM, it is crucial to strike a balance between personalization and generalization in FPL. To tackle this challenge, we proposed Federated Prompt Learning with CLIP Generalization and low-rank Personalization (FedPGP), which employs pre-trained CLIP to provide knowledge-guidance on the global prompt for improved generalization and incorporates a low-rank adaptation term to personalize the global prompt. Further, FedPGP integrates a prompt-wise contrastive loss to achieve knowledge guidance and personalized adaptation simultaneously, enabling a harmonious balance between personalization and generalization in FPL. We conduct extensive experiments on various datasets to explore base-to-novel generalization in both category-level and domain-level scenarios with heterogeneous data, showing the superiority of FedPGP in balancing generalization and personalization",
    "checked": true,
    "id": "398e4c7e4a8f09b19740cd5375df7953f10a71c5",
    "semantic_title": "harmonizing generalization and personalization in federated prompt learning",
    "citation_count": 3,
    "authors": [
      "Tianyu Cui",
      "Hongxia Li",
      "Jingya Wang",
      "Ye Shi"
    ]
  },
  "https://proceedings.mlr.press/v235/cui24d.html": {
    "title": "Asymptotics of feature learning in two-layer networks after one gradient-step",
    "volume": "main",
    "abstract": "In this manuscript, we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging the insight from (Ba et al., 2022), we model the trained network by a spiked Random Features (sRF) model. Further building on recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error of the sRF in the high-dimensional limit where the number of samples, the width, and the input dimension grow at a proportional rate. The resulting characterization for sRFs also captures closely the learning curves of the original network model. This enables us to understand how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient - where at initialization it can only express linear functions in this regime",
    "checked": true,
    "id": "ecc98cc49d0e30355ceeb887dbcb6c5c3fa0f6e3",
    "semantic_title": "asymptotics of feature learning in two-layer networks after one gradient-step",
    "citation_count": 11,
    "authors": [
      "Hugo Cui",
      "Luca Pesce",
      "Yatin Dandi",
      "Florent Krzakala",
      "Yue Lu",
      "Lenka Zdeborova",
      "Bruno Loureiro"
    ]
  },
  "https://proceedings.mlr.press/v235/cui24e.html": {
    "title": "Ameliorate Spurious Correlations in Dataset Condensation",
    "volume": "main",
    "abstract": "Dataset Condensation has emerged as a technique for compressing large datasets into smaller synthetic counterparts, facilitating downstream training tasks. In this paper, we study the impact of bias inside the original dataset on the performance of dataset condensation. With a comprehensive empirical evaluation on canonical datasets with color, corruption and background biases, we found that color and background biases in the original dataset will be amplified through the condensation process, resulting in a notable decline in the performance of models trained on the condensed dataset, while corruption bias is suppressed through the condensation process. To reduce bias amplification in dataset condensation, we introduce a simple yet highly effective approach based on a sample reweighting scheme utilizing kernel density estimation. Empirical results on multiple real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Notably, on CMNIST with 5% bias-conflict ratio and IPC 50, our method achieves 91.5% test accuracy compared to 23.8% from vanilla DM, boosting the performance by 67.7%, whereas applying state-of-the-art debiasing method on the same dataset only achieves 53.7% accuracy. Our findings highlight the importance of addressing biases in dataset condensation and provide a promising avenue to address bias amplification in the process",
    "checked": true,
    "id": "0149bf6c34cd06b9bc89022212099a91611ebf31",
    "semantic_title": "ameliorate spurious correlations in dataset condensation",
    "citation_count": 0,
    "authors": [
      "Justin Cui",
      "Ruochen Wang",
      "Yuanhao Xiong",
      "Cho-Jui Hsieh"
    ]
  },
  "https://proceedings.mlr.press/v235/cui24f.html": {
    "title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback",
    "volume": "main",
    "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality AI feedback automatically for a scalable alternative. Specifically, we identify scale and diversity as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present UltraFeedback, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon UltraFeedback, we align a LLaMA-based model by best-of-$n$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research",
    "checked": true,
    "id": "976ca858496d2d10246b943a44fe75d1ac477639",
    "semantic_title": "ultrafeedback: boosting language models with scaled ai feedback",
    "citation_count": 30,
    "authors": [
      "Ganqu Cui",
      "Lifan Yuan",
      "Ning Ding",
      "Guanming Yao",
      "Bingxiang He",
      "Wei Zhu",
      "Yuan Ni",
      "Guotong Xie",
      "Ruobing Xie",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/cullen24a.html": {
    "title": "Et Tu Certifications: Robustness Certificates Yield Better Adversarial Examples",
    "volume": "main",
    "abstract": "In guaranteeing the absence of adversarial examples in an instance's neighbourhood, certification mechanisms play an important role in demonstrating neural net robustness. In this paper, we ask if these certifications can compromise the very models they help to protect? Our new Certification Aware Attack exploits certifications to produce computationally efficient norm-minimising adversarial examples $74$% more often than comparable attacks, while reducing the median perturbation norm by more than $10$%. While these attacks can be used to assess the tightness of certification bounds, they also highlight that releasing certifications can paradoxically reduce security",
    "checked": true,
    "id": "1e95b6860e76d55efe03bf5f023e8b81406cec0b",
    "semantic_title": "et tu certifications: robustness certificates yield better adversarial examples",
    "citation_count": 0,
    "authors": [
      "Andrew Craig Cullen",
      "Shijie Liu",
      "Paul Montague",
      "Sarah Monazam Erfani",
      "Benjamin I. P. Rubinstein"
    ]
  },
  "https://proceedings.mlr.press/v235/cyffers24a.html": {
    "title": "Differentially Private Decentralized Learning with Random Walks",
    "volume": "main",
    "abstract": "The popularity of federated learning comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty. Unfortunately, sharing model updates also creates a new privacy attack surface. In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication graph. Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities. Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other. We supplement our theoretical results with empirical evaluation on synthetic and real-world graphs and datasets",
    "checked": true,
    "id": "ba0bc51abfca63f53ffc89126615283a679a66c5",
    "semantic_title": "differentially private decentralized learning with random walks",
    "citation_count": 2,
    "authors": [
      "Edwige Cyffers",
      "Aurélien Bellet",
      "Jalaj Upadhyay"
    ]
  },
  "https://proceedings.mlr.press/v235/dagan24a.html": {
    "title": "Getting the most out of your tokenizer for pre-training and domain adaptation",
    "volume": "main",
    "abstract": "Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to a wide range of use-cases. We find that when fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of a pre-trained LLM to obtain large gains in generation speed and effective context size",
    "checked": true,
    "id": "a640215755e23e2649f4b3d3246a47b14fea93f7",
    "semantic_title": "getting the most out of your tokenizer for pre-training and domain adaptation",
    "citation_count": 7,
    "authors": [
      "Gautier Dagan",
      "Gabriel Synnaeve",
      "Baptiste Roziere"
    ]
  },
  "https://proceedings.mlr.press/v235/dahan24a.html": {
    "title": "Fault Tolerant ML: Efficient Meta-Aggregation and Synchronous Training",
    "volume": "main",
    "abstract": "In this paper, we investigate the challenging framework of Byzantine-robust training in distributed machine learning (ML) systems, focusing on enhancing both efficiency and practicality. As distributed ML systems become integral for complex ML tasks, ensuring resilience against Byzantine failures—where workers may contribute incorrect updates due to malice or error—gains paramount importance. Our first contribution is the introduction of the Centered Trimmed Meta Aggregator (CTMA), an efficient meta-aggregator that upgrades baseline aggregators to optimal performance levels, while requiring low computational demands. Additionally, we propose harnessing a recently developed gradient estimation technique based on a double-momentum strategy within the Byzantine context. Our paper highlights its theoretical and practical advantages for Byzantine-robust training, especially in simplifying the tuning process and reducing the reliance on numerous hyperparameters. The effectiveness of this technique is supported by theoretical insights within the stochastic convex optimization (SCO) framework and corroborated by empirical evidence",
    "checked": true,
    "id": "7f13183746c65b7164be80b27cb0bc23afaccaf5",
    "semantic_title": "fault tolerant ml: efficient meta-aggregation and synchronous training",
    "citation_count": 0,
    "authors": [
      "Tehila Dahan",
      "Kfir Yehuda Levy"
    ]
  },
  "https://proceedings.mlr.press/v235/dai24a.html": {
    "title": "Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis",
    "volume": "main",
    "abstract": "What is agency, and why does it matter? In this work, we draw from the political science and philosophy literature and give two competing visions of what it means to be an (ethical) agent. The first view, which we term mechanistic, is commonly— and implicitly—assumed in AI research, yet it is a fundamentally limited means to understand the ethical characteristics of AI. Under the second view, which we term volitional, AI can no longer be considered an ethical agent. We discuss the implications of each of these views for two critical questions: first, what the ideal system \"ought\" to look like, and second, how accountability may be achieved. In light of this discussion, we ultimately argue that, in the context of ethically-significant behavior, AI should be viewed not as an agent but as the outcome of political processes",
    "checked": true,
    "id": "f5189689120fec50aeea45c119222916130ffa99",
    "semantic_title": "position: beyond personhood: agency, accountability, and the limits of anthropomorphic ethical analysis",
    "citation_count": 0,
    "authors": [
      "Jessica Dai"
    ]
  },
  "https://proceedings.mlr.press/v235/dai24b.html": {
    "title": "Multi-View Clustering by Inter-cluster Connectivity Guided Reward",
    "volume": "main",
    "abstract": "Multi-view clustering has been widely explored for its effectiveness in harmonizing heterogeneity along with consistency in different views of data. Despite the significant progress made by recent works, the performance of most existing methods is heavily reliant on strong priori information regarding the true cluster number $\\textit{K}$, which is rarely feasible in real-world scenarios. In this paper, we propose a novel graph-based multi-view clustering algorithm to infer unknown $\\textit{K}$ through a graph consistency reward mechanism. To be specific, we evaluate the cluster indicator matrix during each iteration with respect to diverse $\\textit{K}$. We formulate the inference process of unknown $\\textit{K}$ as a parsimonious reinforcement learning paradigm, where the reward is measured by inter-cluster connectivity. As a result, our approach is capable of independently producing the final clustering result, free from the input of a predefined cluster number. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach in comparison to existing state-of-the-art methods",
    "checked": true,
    "id": "8e5b88bcdce6106354e78c02bc61733c869574ed",
    "semantic_title": "multi-view clustering by inter-cluster connectivity guided reward",
    "citation_count": 0,
    "authors": [
      "Hao Dai",
      "Yang Liu",
      "Peng Su",
      "Hecheng Cai",
      "Shudong Huang",
      "Jiancheng Lv"
    ]
  },
  "https://proceedings.mlr.press/v235/dai24c.html": {
    "title": "High-Order Contrastive Learning with Fine-grained Comparative Levels for Sparse Ordinal Tensor Completion",
    "volume": "main",
    "abstract": "Contrastive learning is a powerful paradigm for representation learning with prominent success in computer vision and NLP, but how to extend its success to high-dimensional tensors remains a challenge. This is because tensor data often exhibit high-order mode-interactions that are hard to profile and with negative samples growing combinatorially faster than second-order contrastive learning; furthermore, many real-world tensors have ordinal entries that necessitate more delicate comparative levels. To solve the challenge, we propose High-Order Contrastive Tensor Completion (HOCTC), an innovative network to extend contrastive learning to sparse ordinal tensor data. HOCTC employs a novel attention-based strategy with query-expansion to capture high-order mode interactions even in case of very limited tokens, which transcends beyond second-order learning scenarios. Besides, it extends two-level comparisons (positive-vs-negative) to fine-grained contrast-levels using ordinal tensor entries as a natural guidance. Efficient sampling scheme is proposed to enforce such delicate comparative structures, generating comprehensive self-supervised signals for high-order representation learning. Extensive experiments show that HOCTC has promising results in sparse tensor completion in traffic/recommender applications",
    "checked": true,
    "id": "8647411c48ed03e982b4cfd1042ff627e141d4d4",
    "semantic_title": "high-order contrastive learning with fine-grained comparative levels for sparse ordinal tensor completion",
    "citation_count": 0,
    "authors": [
      "Yu Dai",
      "Junchen Shen",
      "Zijie Zhai",
      "Danlin Liu",
      "Jingyang Chen",
      "Yu Sun",
      "Ping Li",
      "Jie Zhang",
      "Kai Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/dai24d.html": {
    "title": "Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation",
    "volume": "main",
    "abstract": "A key aspect of Safe Reinforcement Learning (Safe RL) involves estimating the constraint condition for the next policy, which is crucial for guiding the optimization of safe policy updates. However, the existing Advantage-based Estimation (ABE) method relies on the infinite-horizon discounted advantage function. This dependence leads to catastrophic errors in finite-horizon scenarios with non-discounted constraints, resulting in safety-violation updates. In response, we propose the first estimation method for finite-horizon non-discounted constraints in deep Safe RL, termed Gradient-based Estimation (GBE), which relies on the analytic gradient derived along trajectories. Our theoretical and empirical analyses demonstrate that GBE can effectively estimate constraint changes over a finite horizon. Constructing a surrogate optimization problem with GBE, we developed a novel Safe RL algorithm called Constrained Gradient-based Policy Optimization (CGPO). CGPO identifies feasible optimal policies by iteratively resolving sub-problems within trust regions. Our empirical results reveal that CGPO, unlike baseline algorithms, successfully estimates the constraint functions of subsequent policies, thereby ensuring the efficiency and feasibility of each update",
    "checked": true,
    "id": "8efc9c76034295cd3e87c7c805919c78c7ea5fd8",
    "semantic_title": "safe reinforcement learning using finite-horizon gradient-based estimation",
    "citation_count": 0,
    "authors": [
      "Juntao Dai",
      "Yaodong Yang",
      "Qian Zheng",
      "Gang Pan"
    ]
  },
  "https://proceedings.mlr.press/v235/daley24a.html": {
    "title": "Averaging $n$-step Returns Reduces Variance in Reinforcement Learning",
    "volume": "main",
    "abstract": "Multistep returns, such as $n$-step returns and $\\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns—weighted averages of $n$-step returns—to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing that compound returns often increase the sample efficiency of $n$-step deep RL agents like DQN and PPO",
    "checked": false,
    "id": "579ad8a01018173c33f251c65be472aa31f85722",
    "semantic_title": "averaging n-step returns reduces variance in reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Brett Daley",
      "Martha White",
      "Marlos C. Machado"
    ]
  },
  "https://proceedings.mlr.press/v235/dalirrooyfard24a.html": {
    "title": "Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models",
    "volume": "main",
    "abstract": "Given a graph with positive and negative edge labels, the correlation clustering problem aims to cluster the nodes so to minimize the total number of between-cluster positive and within-cluster negative edges. This problem has many applications in data mining, particularly in unsupervised learning. Inspired by the prevalence of large graphs and constantly changing data in modern applications, we study correlation clustering in dynamic, parallel (MPC), and local computation (LCA) settings. We design an approach that improves state-of-the-art runtime complexities in all these settings. In particular, we provide the first fully dynamic algorithm that runs in an expected amortized constant time, without any dependence on the graph size. Moreover, our algorithm essentially matches the approximation guarantee of the celebrated Pivot algorithm",
    "checked": true,
    "id": "c32b0a6abdaa6c069224549df5150a0cba3c5f81",
    "semantic_title": "pruned pivot: correlation clustering algorithm for dynamic, parallel, and local computation models",
    "citation_count": 1,
    "authors": [
      "Mina Dalirrooyfard",
      "Konstantin Makarychev",
      "Slobodan Mitrovic"
    ]
  },
  "https://proceedings.mlr.press/v235/dalton24a.html": {
    "title": "Physics and Lie symmetry informed Gaussian processes",
    "volume": "main",
    "abstract": "Physics-informed machine learning (PIML) has established itself as a new scientific paradigm which enables the seamless integration of observational data with partial differential equation (PDE) based physics models. A powerful tool for the analysis, reduction and solution of PDEs is the Lie symmetry method. Nevertheless, only recently has the integration of such symmetries into PIML frameworks begun to be explored. The present work adds to this growing literature by introducing an approach for incorporating a Lie symmetry into a physics-informed Gaussian process (GP) model. The symmetry is introduced as a constraint on the GP; either in a soft manner via virtual observations of an induced PDE called the invariant surface condition, or explicitly through the design of the kernel. Experimental results demonstrate that the use of symmetry constraints improves the performance of the GP for both forward and inverse problems, and that our approach offers competitive performance with neural networks in the low-data environment",
    "checked": true,
    "id": "06c3b36ad0c84ad037adcd4e7e06435a835258bb",
    "semantic_title": "physics and lie symmetry informed gaussian processes",
    "citation_count": 1,
    "authors": [
      "David Dalton",
      "Dirk Husmeier",
      "Hao Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/dan24a.html": {
    "title": "Exploring the Enigma of Neural Dynamics Through A Scattering-Transform Mixer Landscape for Riemannian Manifold",
    "volume": "main",
    "abstract": "The human brain is a complex inter-wired system that emerges spontaneous functional fluctuations. In spite of tremendous success in the experimental neuroscience field, a system-level understanding of how brain anatomy supports various neural activities remains elusive. Capitalizing on the unprecedented amount of neuroimaging data, we present a physics-informed deep model to uncover the coupling mechanism between brain structure and function through the lens of data geometry that is rooted in the widespread wiring topology of connections between distant brain regions. Since deciphering the puzzle of self-organized patterns in functional fluctuations is the gateway to understanding the emergence of cognition and behavior, we devise a geometric deep model to uncover manifold mapping functions that characterize the intrinsic feature representations of evolving functional fluctuations on the Riemannian manifold. In lieu of learning unconstrained mapping functions, we introduce a set of graph-harmonic scattering transforms to impose the brain-wide geometry on top of manifold mapping functions, which allows us to cast the manifold-based deep learning into a reminiscent of MLP-Mixer architecture (in computer vision) for Riemannian manifold. As a proof-of-concept approach, we explore a neural-manifold perspective to understand the relationship between (static) brain structure and (dynamic) function, challenging the prevailing notion in cognitive neuroscience by proposing that neural activities are essentially excited by brain-wide oscillation waves living on the geometry of human connectomes, instead of being confined to focal areas",
    "checked": true,
    "id": "d66e5f5d53deaeaa431719b98b70a520725e74c3",
    "semantic_title": "exploring the enigma of neural dynamics through a scattering-transform mixer landscape for riemannian manifold",
    "citation_count": 1,
    "authors": [
      "Tingting Dan",
      "Ziquan Wei",
      "Won Hwa Kim",
      "Guorong Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/dandi24a.html": {
    "title": "The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents",
    "volume": "main",
    "abstract": "We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamical process of the low-dimensional projections of the weights, and numerical experiments illustrating the theory",
    "checked": true,
    "id": "9ae8a940451fd24d87c278324531f36206b492a1",
    "semantic_title": "the benefits of reusing batches for gradient descent in two-layer networks: breaking the curse of information and leap exponents",
    "citation_count": 17,
    "authors": [
      "Yatin Dandi",
      "Emanuele Troiani",
      "Luca Arnaboldi",
      "Luca Pesce",
      "Lenka Zdeborova",
      "Florent Krzakala"
    ]
  },
  "https://proceedings.mlr.press/v235/dang24a.html": {
    "title": "Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Features Model",
    "volume": "main",
    "abstract": "The current paradigm of training deep neural networks for classification tasks includes minimizing the empirical risk, pushing the training loss value towards zero even after the training classification error has vanished. In this terminal phase of training, it has been observed that the last-layer features collapse to their class-means and these class-means converge to the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural Collapse ($\\mathcal{NC}$). However, this characterization only holds in class-balanced datasets where every class has the same number of training samples. When the training dataset is class-imbalanced, some $\\mathcal{NC}$ properties will no longer hold true, for example, the geometry of class-means will skew away from the simplex ETF. In this paper, we generalize $\\mathcal{NC}$ to imbalanced regime for cross-entropy loss under the unconstrained ReLU features model. We demonstrate that while the within-class features collapse property still holds in this setting, the class-means will converge to a structure consisting of orthogonal vectors with lengths dependent on the number of training samples. Furthermore, we find that the classifier weights (i.e., the last-layer linear classifier) are aligned to the scaled and centered class-means, with scaling factors dependent on the number of training samples of each class. This generalizes $\\mathcal{NC}$ in the class-balanced setting. We empirically validate our results through experiments on practical architectures and dataset",
    "checked": false,
    "id": "274bb76e14a27af18fae2ca10b0f9b87ca98ed86",
    "semantic_title": "neural collapse for cross-entropy class-imbalanced learning with unconstrained relu feature model",
    "citation_count": 9,
    "authors": [
      "Hien Dang",
      "Tho Tran Huu",
      "Tan Minh Nguyen",
      "Nhat Ho"
    ]
  },
  "https://proceedings.mlr.press/v235/dao24a.html": {
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
    "volume": "main",
    "abstract": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8$\\times$ faster, while continuing to be competitive with Transformers on language modeling",
    "checked": true,
    "id": "ca9f5b3bf0f54ad97513e6175b30497873670fed",
    "semantic_title": "transformers are ssms: generalized models and efficient algorithms through structured state space duality",
    "citation_count": 179,
    "authors": [
      "Tri Dao",
      "Albert Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/dao24b.html": {
    "title": "Boosting Offline Optimizers with Surrogate Sensitivity",
    "volume": "main",
    "abstract": "Offline optimization is an important task in numerous material engineering domains where online experimentation to collect data is too expensive and needs to be replaced by an in silico maximization of a surrogate of the black-box function. Although such a surrogate can be learned from offline data, its prediction might not be reliable outside the offline data regime, which happens when the surrogate has narrow prediction margin and is (therefore) sensitive to small perturbations of its parameterization. This raises the following questions: (1) how to regulate the sensitivity of a surrogate model; and (2) whether conditioning an offline optimizer with such less sensitive surrogate will lead to better optimization performance. To address these questions, we develop an optimizable sensitivity measurement for the surrogate model, which then inspires a sensitivity-informed regularizer that is applicable to a wide range of offline optimizers. This development is both orthogonal and synergistic to prior research on offline optimization, which is demonstrated in our extensive experiment benchmark",
    "checked": true,
    "id": "f6fdd91502e96ba260de26d38bf6d647f37ce043",
    "semantic_title": "boosting offline optimizers with surrogate sensitivity",
    "citation_count": 1,
    "authors": [
      "Manh Cuong Dao",
      "Phi Le Nguyen",
      "Thao Nguyen Truong",
      "Trong Nghia Hoang"
    ]
  },
  "https://proceedings.mlr.press/v235/daras24a.html": {
    "title": "Consistent Diffusion Meets Tweedie: Training Exact Ambient Diffusion Models with Noisy Data",
    "volume": "main",
    "abstract": "Ambient diffusion is a recently proposed framework for training diffusion models using corrupted data. Both Ambient Diffusion and alternative SURE-based approaches for learning diffusion models from corrupted data resort to approximations which deteriorate performance. We present the first framework for training diffusion models that provably sample from the uncorrupted distribution given only noisy training data, solving an open problem in Ambient diffusion. Our key technical contribution is a method that uses a double application of Tweedie's formula and a consistency loss function that allows us to extend sampling at noise levels below the observed data noise. We also provide further evidence that diffusion models memorize from their training sets by identifying extremely corrupted images that are almost perfectly reconstructed, raising copyright and privacy concerns. Our method for training using corrupted samples can be used to mitigate this problem. We demonstrate this by fine-tuning Stable Diffusion XL to generate samples from a distribution using only noisy samples. Our framework reduces the amount of memorization of the fine-tuning dataset, while maintaining competitive performance",
    "checked": true,
    "id": "f7b4211d53a05737b5efc161da69c78e2e7b8850",
    "semantic_title": "consistent diffusion meets tweedie: training exact ambient diffusion models with noisy data",
    "citation_count": 8,
    "authors": [
      "Giannis Daras",
      "Alex Dimakis",
      "Constantinos Costis Daskalakis"
    ]
  },
  "https://proceedings.mlr.press/v235/das24a.html": {
    "title": "Larimar: Large Language Models with Episodic Memory Control",
    "volume": "main",
    "abstract": "Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed—yielding speed-ups of 8-10x depending on the base LLM —as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at https://github.com/IBM/larimar",
    "checked": true,
    "id": "5372f4364fe5b7991c81ea19ee944c717afe6e0a",
    "semantic_title": "larimar: large language models with episodic memory control",
    "citation_count": 8,
    "authors": [
      "Payel Das",
      "Subhajit Chaudhury",
      "Elliot Nelson",
      "Igor Melnyk",
      "Sarathkrishna Swaminathan",
      "Sihui Dai",
      "Aurelie Lozano",
      "Georgios Kollias",
      "Vijil Chenthamarakshan",
      "Jiri Navratil",
      "Soham Dan",
      "Pin-Yu Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/das24b.html": {
    "title": "Understanding the Training Speedup from Sampling with Approximate Losses",
    "volume": "main",
    "abstract": "It is well known that selecting samples with large losses/gradients can significantly reduce the number of training steps. However, the selection overhead is often too high to yield any meaningful gains in terms of overall training time. In this work, we focus on the greedy approach of selecting samples with large approximate losses instead of exact losses in order to reduce the selection overhead. For smooth convex losses, we show that such a greedy strategy can converge to a constant factor of the minimum value of the average loss in fewer iterations than the standard approach of random selection. We also theoretically quantify the effect of the approximation level. We then develop SIFT which uses early exiting to obtain approximate losses with an intermediate layer's representations for sample selection. We evaluate SIFT on the task of training a 110M parameter 12 layer BERT base model, and show significant gains (in terms of training hours and number of backpropagation steps) without any optimized implementation over vanilla training. For e.g., to reach 64% validation accuracy, SIFT with exit at the first layer takes $\\sim$ 43 hours compared to $\\sim$ 57 hours of vanilla training",
    "checked": true,
    "id": "e96e8db4eaed629090dff1e04d24ba45a57ebf5d",
    "semantic_title": "understanding the training speedup from sampling with approximate losses",
    "citation_count": 0,
    "authors": [
      "Rudrajit Das",
      "Xi Chen",
      "Bertram Ieong",
      "Parikshit Bansal",
      "Sujay Sanghavi"
    ]
  },
  "https://proceedings.mlr.press/v235/das24c.html": {
    "title": "A decoder-only foundation model for time-series forecasting",
    "volume": "main",
    "abstract": "Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a decoder style attention model with input patching, using a large time-series corpus comprising both real-world and synthetic datasets. Experiments on a diverse set of previously unseen forecasting datasets suggests that the model can yield accurate zero-shot forecasts across different domains, forecasting horizons and temporal granularities",
    "checked": true,
    "id": "f45f85fa1beaa795c24c4ff86f1f2deece72252f",
    "semantic_title": "a decoder-only foundation model for time-series forecasting",
    "citation_count": 104,
    "authors": [
      "Abhimanyu Das",
      "Weihao Kong",
      "Rajat Sen",
      "Yichen Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/das24d.html": {
    "title": "Disparate Impact on Group Accuracy of Linearization for Private Inference",
    "volume": "main",
    "abstract": "Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the finetuning step for linearized models can serve as an effective mitigation strategy",
    "checked": true,
    "id": "18a226b794e89d3cf8d93734724b42275c9c4c0d",
    "semantic_title": "disparate impact on group accuracy of linearization for private inference",
    "citation_count": 2,
    "authors": [
      "Saswat Das",
      "Marco Romanelli",
      "Ferdinando Fioretto"
    ]
  },
  "https://proceedings.mlr.press/v235/dasgupta24a.html": {
    "title": "New Bounds on the Cohesion of Complete-link and Other Linkage Methods for Agglomerative Clustering",
    "volume": "main",
    "abstract": "Linkage methods are among the most popular algorithms for hierarchical clustering. Despite their relevance, the current knowledge regarding the quality of the clustering produced by these methods is limited. Here, we improve the currently available bounds on the maximum diameter of the clustering obtained by complete-link for metric spaces. One of our new bounds, in contrast to the existing ones, allows us to separate complete-link from single-link in terms of approximation for the diameter, which corroborates the common perception that the former is more suitable than the latter when the goal is producing compact clusters. We also show that our techniques can be employed to derive upper bounds on the cohesion of a class of linkage methods that includes the quite popular average-link",
    "checked": true,
    "id": "c980b9886bab06e9e62abfed9e22f865bab18333",
    "semantic_title": "new bounds on the cohesion of complete-link and other linkage methods for agglomerative clustering",
    "citation_count": 0,
    "authors": [
      "Sanjoy Dasgupta",
      "Eduardo Sany Laber"
    ]
  },
  "https://proceedings.mlr.press/v235/de-santi24a.html": {
    "title": "Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction",
    "volume": "main",
    "abstract": "How can a scientist use a Reinforcement Learning (RL) algorithm to design experiments over a dynamical system's state space? In the case of finite and Markovian systems, an area called Active Exploration (AE) relaxes the optimization problem of experiments design into Convex RL, a generalization of RL admitting a wider notion of reward. Unfortunately, this framework is currently not scalable and the potential of AE is hindered by the vastness of experiments spaces typical of scientific discovery applications. However, these spaces are often endowed with natural geometries, e.g., permutation invariance in molecular design, that an agent could leverage to improve the statistical and computational efficiency of AE. To achieve this, we bridge AE and MDP homomorphisms, which offer a way to exploit known geometric structures via abstraction. Towards this goal, we make two fundamental contributions: we extend MDP homomorphisms formalism to Convex RL, and we present, to the best of our knowledge, the first analysis that formally captures the benefit of abstraction via homomorphisms on sample efficiency. Ultimately, we propose the Geometric Active Exploration (GAE) algorithm, which we analyse theoretically and experimentally in environments motivated by problems in scientific discovery",
    "checked": true,
    "id": "fb2bce3ecbdbdfb9b0589a010f899562ac31af51",
    "semantic_title": "geometric active exploration in markov decision processes: the benefit of abstraction",
    "citation_count": 0,
    "authors": [
      "Riccardo De Santi",
      "Federico Arangath Joseph",
      "Noah Liniger",
      "Mirco Mutti",
      "Andreas Krause"
    ]
  },
  "https://proceedings.mlr.press/v235/de-santi24b.html": {
    "title": "Global Reinforcement Learning : Beyond Linear and Convex Rewards via Submodular Semi-gradient Methods",
    "volume": "main",
    "abstract": "In classic Reinforcement Learning (RL), the agent maximizes an additive objective of the visited states, e.g., a value function. Unfortunately, objectives of this type cannot model many real-world applications such as experiment design, exploration, imitation learning, and risk-averse RL to name a few. This is due to the fact that additive objectives disregard interactions between states that are crucial for certain tasks. To tackle this problem, we introduce Global RL (GRL), where rewards are globally defined over trajectories instead of locally over states. Global rewards can capture negative interactions among states, e.g., in exploration, via submodularity, positive interactions, e.g., synergetic effects, via supermodularity, while mixed interactions via combinations of them. By exploiting ideas from submodular optimization, we propose a novel algorithmic scheme that converts any GRL problem to a sequence of classic RL problems and solves it efficiently with curvature-dependent approximation guarantees. We also provide hardness of approximation results and empirically demonstrate the effectiveness of our method on several GRL instances",
    "checked": false,
    "id": "cb7ea0176eec1f809f3bc3a34aeb279d44dda612",
    "semantic_title": "global reinforcement learning: beyond linear and convex rewards via submodular semi-gradient methods",
    "citation_count": 3,
    "authors": [
      "Riccardo De Santi",
      "Manish Prajapat",
      "Andreas Krause"
    ]
  },
  "https://proceedings.mlr.press/v235/decker24a.html": {
    "title": "Provably Better Explanations with Optimized Aggregation of Feature Attributions",
    "volume": "main",
    "abstract": "Using feature attributions for post-hoc explanations is a common practice to understand and verify the predictions of opaque machine learning models. Despite the numerous techniques available, individual methods often produce inconsistent and unstable results, putting their overall reliability into question. In this work, we aim to systematically improve the quality of feature attributions by combining multiple explanations across distinct methods or their variations. For this purpose, we propose a novel approach to derive optimal convex combinations of feature attributions that yield provable improvements of desired quality criteria such as robustness or faithfulness to the model behavior. Through extensive experiments involving various model architectures and popular feature attribution techniques, we demonstrate that our combination strategy consistently outperforms individual methods and existing baselines",
    "checked": true,
    "id": "83dcfb6bd217c71f7cc4ba287ded382d9386b2e9",
    "semantic_title": "provably better explanations with optimized aggregation of feature attributions",
    "citation_count": 1,
    "authors": [
      "Thomas Decker",
      "Ananta R. Bhattarai",
      "Jindong Gu",
      "Volker Tresp",
      "Florian Buettner"
    ]
  },
  "https://proceedings.mlr.press/v235/dedieu24a.html": {
    "title": "Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments",
    "volume": "main",
    "abstract": "Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains the near-perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest path problems exponentially faster. Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context path planning problems",
    "checked": true,
    "id": "61e81c9a357b96fecbbc7a7bbae9704645ca73b8",
    "semantic_title": "learning cognitive maps from transformer representations for efficient planning in partially observed environments",
    "citation_count": 1,
    "authors": [
      "Antoine Dedieu",
      "Wolfgang Lehrach",
      "Guangyao Zhou",
      "Dileep George",
      "Miguel Lazaro-Gredilla"
    ]
  },
  "https://proceedings.mlr.press/v235/deep24a.html": {
    "title": "Asymptotically Optimal and Computationally Efficient Average Treatment Effect Estimation in A/B testing",
    "volume": "main",
    "abstract": "Motivated by practical applications in clinical trials and online platforms, we study A/B testing with the aim of estimating a confidence interval (CI) for the average treatment effect (ATE) using the minimum expected sample size. This CI should have a width at most $\\epsilon$ while ensuring that the probability of the CI not containing the true ATE is at most $\\delta$. To answer this, we first establish a lower bound on the expected sample size needed for any adaptive policy which constructs a CI of ATE with desired properties. Specifically, we prove that the lower bound is based on the solution to a max-min non-convex optimization problem for small $\\delta$. Tailoring the \"plug-in\" approach for the ATE problem, we construct an adaptive policy that is asymptotically optimal, i.e., matches the lower bound on the expected sample size for small $\\delta$. Interestingly, we find that, for small $\\epsilon$ and $\\delta$, the asymptotically optimal fraction of treatment assignment for A and B is proportional to the standard deviation of the outcome distributions of treatments A and B, respectively. However, as the proposed approach can be computationally intensive, we propose an alternative adaptive policy. This new policy, informed by insights from our lower bound analysis, is computationally efficient while remaining asymptotically optimal for small values of $\\epsilon$ and $\\delta$. Numerical comparisons demonstrate that both policies perform similarly across practical values of $\\epsilon$ and $\\delta$, offering efficient solutions for A/B testing",
    "checked": true,
    "id": "e80bc605204e761a76a1fc07f4fd6691e6a136cc",
    "semantic_title": "asymptotically optimal and computationally efficient average treatment effect estimation in a/b testing",
    "citation_count": 0,
    "authors": [
      "Vikas Deep",
      "Achal Bassamboo",
      "Sandeep Kumar Juneja"
    ]
  },
  "https://proceedings.mlr.press/v235/demelas24a.html": {
    "title": "Predicting Lagrangian Multipliers for Mixed Integer Linear Programs",
    "volume": "main",
    "abstract": "Lagrangian Relaxation stands among the most efficient approaches for solving Mixed Integer Linear Programs (MILPs) with difficult constraints. Given any duals for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on the optimal value of the MILP, and Lagrangian methods seek the LMs giving the best such bound. But these methods generally rely on iterative algorithms resembling gradient descent to maximize the concave piecewise linear dual function: the computational burden grows quickly with the number of relaxed constraints. We introduce a deep learning approach that bypasses the descent, effectively amortizing per instance optimization. A probabilistic encoder based on a graph neural network computes, given a MILP instance and its Continuous Relaxation (CR) solution, high-dimensional representations of relaxed constraints, which are turned into LMs by a decoder. We train the encoder and the decoder jointly by directly optimizing the bound obtained from the predicted multipliers. Our method is applicable to any problem with a compact MILP formulation, and to any Lagrangian Relaxation providing a tighter bound than CR. Experiments on two widely known problems, Multi-Commodity Network Design and Generalized Assignment, show that our approach closes up to 85% of the gap between the continuous relaxation and the best Lagrangian bound, and provides a high-quality warm-start for descent-based Lagrangian methods",
    "checked": true,
    "id": "47d931a73f108e1de7a78f7f56fff7f5795ff7a2",
    "semantic_title": "predicting lagrangian multipliers for mixed integer linear programs",
    "citation_count": 0,
    "authors": [
      "Francesco Demelas",
      "Joseph Le Roux",
      "Mathieu Lacroix",
      "Axel Parmentier"
    ]
  },
  "https://proceedings.mlr.press/v235/demirel24a.html": {
    "title": "Prediction-powered Generalization of Causal Inferences",
    "volume": "main",
    "abstract": "Causal inferences from a randomized controlled trial (RCT) may not pertain to a target population where some effect modifiers have a different distribution. Prior work studies generalizing the results of a trial to a target population with no outcome but covariate data available. We show how the limited size of trials makes generalization a statistically infeasible task, as it requires estimating complex nuisance functions. We develop generalization algorithms that supplement the trial data with a prediction model learned from an additional observational study (OS), without making any assumptions on the OS. We theoretically and empirically show that our methods facilitate better generalization when the OS is \"high-quality\", and remain robust when it is not, and e.g., have unmeasured confounding",
    "checked": true,
    "id": "e6f27904230ea2a2d8aa5316e27a9b5344d3316a",
    "semantic_title": "prediction-powered generalization of causal inferences",
    "citation_count": 1,
    "authors": [
      "Ilker Demirel",
      "Ahmed Alaa",
      "Anthony Philippakis",
      "David Sontag"
    ]
  },
  "https://proceedings.mlr.press/v235/demirel24b.html": {
    "title": "An Unsupervised Approach for Periodic Source Detection in Time Series",
    "volume": "main",
    "abstract": "Detection of periodic patterns of interest within noisy time series data plays a critical role in various tasks, spanning from health monitoring to behavior analysis. Existing learning techniques often rely on labels or clean versions of signals for detecting the periodicity, and those employing self-supervised methods are required to apply proper augmentations, which is already challenging for time series and can result in collapse—all representations collapse to a single point due to strong augmentation. In this work, we propose a novel method to detect the periodicity in time series without the need for any labels or requiring tailored positive or negative data generation mechanisms. We mitigate the collapse issue by ensuring the learned representations retain information from the original samples without imposing any variance constraints on the batch. Our experiments in three time-series tasks against state-of-the-art learning methods show that the proposed approach consistently outperforms prior works, achieving performance improvements of more than 45–50%, showing its effectiveness",
    "checked": true,
    "id": "aabdc15bf7c4dd4c42575e1a50cbe4760d7e2063",
    "semantic_title": "an unsupervised approach for periodic source detection in time series",
    "citation_count": 0,
    "authors": [
      "Berken Utku Demirel",
      "Christian Holz"
    ]
  },
  "https://proceedings.mlr.press/v235/deng24a.html": {
    "title": "Multi-group Learning for Hierarchical Groups",
    "volume": "main",
    "abstract": "The multi-group learning model formalizes the learning scenario in which a single predictor must generalize well on multiple, possibly overlapping subgroups of interest. We extend the study of multi-group learning to the natural case where the groups are hierarchically structured. We design an algorithm for this setting that outputs an interpretable and deterministic decision tree predictor with near-optimal sample complexity. We then conduct an empirical evaluation of our algorithm and find that it achieves attractive generalization properties on real datasets with hierarchical group structure",
    "checked": true,
    "id": "f182ee040232c099851461a4f83fcc57d7a0b12b",
    "semantic_title": "multi-group learning for hierarchical groups",
    "citation_count": 1,
    "authors": [
      "Samuel Deng",
      "Daniel Hsu"
    ]
  },
  "https://proceedings.mlr.press/v235/deng24b.html": {
    "title": "A3S: A General Active Clustering Method with Pairwise Constraints",
    "volume": "main",
    "abstract": "Active clustering aims to boost the clustering performance by integrating human-annotated pairwise constraints through strategic querying. Conventional approaches with semi-supervised clustering schemes encounter high query costs when applied to large datasets with numerous classes. To address these limitations, we propose a novel Adaptive Active Aggregation and Splitting (A3S) framework, falling within the cluster-adjustment scheme in active clustering. A3S features strategic active clustering adjustment on the initial cluster result, which is obtained by an adaptive clustering algorithm. In particular, our cluster adjustment is inspired by the quantitative analysis of Normalized mutual information gain under the information theory framework and can provably improve the clustering quality. The proposed A3S framework significantly elevates the performance and scalability of active clustering. In extensive experiments across diverse real-world datasets, A3S achieves desired results with significantly fewer human queries compared with existing methods",
    "checked": true,
    "id": "e507abd43bc8b64375637485d0f0bac9dce145b3",
    "semantic_title": "a3s: a general active clustering method with pairwise constraints",
    "citation_count": 0,
    "authors": [
      "Xun Deng",
      "Junlong Liu",
      "Han Zhong",
      "Fuli Feng",
      "Chen Shen",
      "Xiangnan He",
      "Jieping Ye",
      "Zheng Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/deng24c.html": {
    "title": "Variational Schrödinger Diffusion Models",
    "volume": "main",
    "abstract": "Schrödinger bridge (SB) has emerged as the go-to method for optimizing transportation plans in diffusion models. However, SB requires estimating the intractable forward score functions, inevitably resulting in the (costly) implicit training loss based on simulated trajectories. To improve the scalability while preserving efficient transportation plans, we leverage variational inference to linearize the forward score functions (variational scores) of SB and restore simulation-free properties in training backward scores. We propose the variational Schrödinger diffusion model (VSDM), where the forward process is a multivariate diffusion and the variational scores are adaptively optimized for efficient transport. Theoretically, we use stochastic approximation to prove the convergence of the variational scores and show the convergence of the adaptively generated samples based on the optimal variational scores. Empirically, we test the algorithm in simulated examples and observe that VSDM is efficient in generations of anisotropic shapes and yields straighter sample trajectories compared to the single-variate diffusion. We also verify the scalability of the algorithm in real-world data and achieve competitive unconditional generation performance in CIFAR10 and conditional generation in time series modeling. Notably, VSDM no longer depends on warm-up initializations required by SB",
    "checked": true,
    "id": "d55282fda0159d8c3692bf46586cd4426d535dbf",
    "semantic_title": "variational schrödinger diffusion models",
    "citation_count": 2,
    "authors": [
      "Wei Deng",
      "Weijian Luo",
      "Yixin Tan",
      "Marin Biloš",
      "Yu Chen",
      "Yuriy Nevmyvaka",
      "Ricky T. Q. Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/deng24d.html": {
    "title": "Collaborative Learning with Different Labeling Functions",
    "volume": "main",
    "abstract": "We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions. We show that, when the data distributions satisfy a weaker realizability assumption, which appeared in (Crammer & Mansour, 2012) in the context of multi-task learning, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class. In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is $\\mathsf{NP}$-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, we give learners that are both sample- and computationally-efficient",
    "checked": true,
    "id": "bea2bf3ab7bf48e4798cda82ad3356d76149491c",
    "semantic_title": "collaborative learning with different labeling functions",
    "citation_count": 0,
    "authors": [
      "Yuyang Deng",
      "Mingda Qiao"
    ]
  },
  "https://proceedings.mlr.press/v235/deng24e.html": {
    "title": "Exploring the Low-Pass Filtering Behavior in Image Super-Resolution",
    "volume": "main",
    "abstract": "Deep neural networks for image super-resolution (ISR) have shown significant advantages over traditional approaches like the interpolation. However, they are often criticized as 'black boxes' compared to traditional approaches with solid mathematical foundations. In this paper, we attempt to interpret the behavior of deep neural networks in ISR using theories from the field of signal processing. First, we report an intriguing phenomenon, referred to as ‘the sinc phenomenon.' It occurs when an impulse input is fed to a neural network. Then, building on this observation, we propose a method named Hybrid Response Analysis (HyRA) to analyze the behavior of neural networks in ISR tasks. Specifically, HyRA decomposes a neural network into a parallel connection of a linear system and a non-linear system and demonstrates that the linear system functions as a low-pass filter while the non-linear system injects high-frequency information. Finally, to quantify the injected high-frequency information, we introduce a metric for image-to-image tasks called Frequency Spectrum Distribution Similarity (FSDS). FSDS reflects the distribution similarity of different frequency components and can capture nuances that traditional metrics may overlook. Code, videos and raw experimental results for this paper can be found in: https://github.com/RisingEntropy/LPFInISR",
    "checked": true,
    "id": "db4a61d14de1e7fc2740db65a277e2fd7878736b",
    "semantic_title": "exploring the low-pass filtering behavior in image super-resolution",
    "citation_count": 0,
    "authors": [
      "Haoyu Deng",
      "Zijing Xu",
      "Yule Duan",
      "Xiao Wu",
      "Wenjie Shu",
      "Liang-Jian Deng"
    ]
  },
  "https://proceedings.mlr.press/v235/deng24f.html": {
    "title": "Network Tight Community Detection",
    "volume": "main",
    "abstract": "Conventional community detection methods often categorize all nodes into clusters. However, the presumed community structure of interest may only be valid for a subset of nodes (named as ‘tight nodes'), while the rest of the network may consist of noninformative \"scattered nodes\". For example, a protein-protein network often contains proteins that do not belong to specific biological functional modules but are involved in more general processes, or act as bridges between different functional modules. Forcing each of these proteins into a single cluster introduces unwanted biases and obscures the underlying biological implication. To address this issue, we propose a tight community detection (TCD) method to identify tight communities excluding scattered nodes. The algorithm enjoys a strong theoretical guarantee of tight node identification accuracy and is scalable for large networks. The superiority of the proposed method is demonstrated by various synthetic and real experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Deng",
      "Xiaodong Yang",
      "Jun Yu",
      "Jun Liu",
      "Zhaiming Shen",
      "Danyang Huang",
      "Huimin Cheng"
    ]
  },
  "https://proceedings.mlr.press/v235/deschenaux24a.html": {
    "title": "Going beyond Compositions, DDPMs Can Produce Zero-Shot Interpolations",
    "volume": "main",
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) exhibit remarkable capabilities in image generation, with studies suggesting that they can generalize by composing latent factors learned from the training data. In this work, we go further and study DDPMs trained on strictly separate subsets of the data distribution with large gaps on the support of the latent factors. We show that such a model can effectively generate images in the unexplored, intermediate regions of the distribution. For instance, when trained on clearly smiling and non-smiling faces, we demonstrate a sampling procedure which can generate slightly smiling faces without reference images (zero-shot interpolation). We replicate these findings for other attributes as well as other datasets. Our code is available on GitHub",
    "checked": true,
    "id": "04cf695745efa8b3645c01b731f09f038470921a",
    "semantic_title": "going beyond compositions, ddpms can produce zero-shot interpolations",
    "citation_count": 1,
    "authors": [
      "Justin Deschenaux",
      "Igor Krawczuk",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://proceedings.mlr.press/v235/detommaso24a.html": {
    "title": "Multicalibration for Confidence Scoring in LLMs",
    "volume": "main",
    "abstract": "This paper proposes the use of \"multicalibration\": to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods",
    "checked": true,
    "id": "71d517ea75fac9f9fc62aefca4ad02eaa7cc0c76",
    "semantic_title": "multicalibration for confidence scoring in llms",
    "citation_count": 9,
    "authors": [
      "Gianluca Detommaso",
      "Martin Andres Bertran",
      "Riccardo Fogliato",
      "Aaron Roth"
    ]
  },
  "https://proceedings.mlr.press/v235/deuschel24a.html": {
    "title": "Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning",
    "volume": "main",
    "abstract": "Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models force a tradeoff between accuracy and interpretability, limiting data-driven interpretations of human decision-making processes. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically under different contexts. Thus, we develop Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem, where each context poses a unique task and complex decision policies can be constructed piece-wise from many simple context-specific policies. CPR models each context-specific policy as a linear map, and generates new policy models on-demand as contexts are updated with new observations. We provide two flavors of the CPR framework: one focusing on exact local interpretability, and one retaining full global interpretability. We assess CPR through studies on simulated and real data, achieving state-of-the-art performance on predicting antibiotic prescription in intensive care units ($+22$% AUROC vs. previous SOTA) and predicting MRI prescription for Alzheimer's patients ($+7.7$% AUROC vs. previous SOTA). With this improvement, CPR closes the accuracy gap between interpretable and black-box methods, allowing high-resolution exploration and analysis of context-specific decision models",
    "checked": true,
    "id": "5d17963ceb279be116e7a1207542ea94f1b2a8c8",
    "semantic_title": "contextualized policy recovery: modeling and interpreting medical decisions with adaptive imitation learning",
    "citation_count": 2,
    "authors": [
      "Jannik Deuschel",
      "Caleb Ellington",
      "Yingtao Luo",
      "Ben Lengerich",
      "Pascal Friederich",
      "Eric P. Xing"
    ]
  },
  "https://proceedings.mlr.press/v235/devic24a.html": {
    "title": "Stability and Multigroup Fairness in Ranking with Uncertain Predictions",
    "volume": "main",
    "abstract": "Rankings are ubiquitous across many applications, from search engines to hiring committees. In practice, many rankings are derived from the output of predictors. However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings. Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups. Not only is stability an important requirement for its own sake, but — as we show — it composes harmoniously with individual fairness in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al. (2021) are stable. Our main result is that UA rankings also achieve group fairness through successful composition with multiaccurate or multicalibrated predictors. Our work demonstrates that UA rankings naturally interpolate between group and individual level fairness guarantees, while simultaneously satisfying stability guarantees important whenever machine-learned predictions are used",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddartha Devic",
      "Aleksandra Korolova",
      "David Kempe",
      "Vatsal Sharan"
    ]
  },
  "https://proceedings.mlr.press/v235/deweese24a.html": {
    "title": "Locally Interdependent Multi-Agent MDP: Theoretical Framework for Decentralized Agents with Dynamic Dependencies",
    "volume": "main",
    "abstract": "Many multi-agent systems in practice are decentralized and have dynamically varying dependencies. There has been a lack of attempts in the literature to analyze these systems theoretically. In this paper, we propose and theoretically analyze a decentralized model with dynamically varying dependencies called the Locally Interdependent Multi-Agent MDP. This model can represent problems in many disparate domains such as cooperative navigation, obstacle avoidance, and formation control. Despite the intractability that general partially observable multi-agent systems suffer from, we propose three closed-form policies that are theoretically near-optimal in this setting and can be scalable to compute and store. Consequentially, we reveal a fundamental property of Locally Interdependent Multi-Agent MDP's that the partially observable decentralized solution is exponentially close to the fully observable solution with respect to the visibility radius. We then discuss extensions of our closed-form policies to further improve tractability. We conclude by providing simulations to investigate some long horizon behaviors of our closed-form policies",
    "checked": true,
    "id": "da8093d677760ec4837f68a413ac7a76c7ca2d5b",
    "semantic_title": "locally interdependent multi-agent mdp: theoretical framework for decentralized agents with dynamic dependencies",
    "citation_count": 1,
    "authors": [
      "Alex Deweese",
      "Guannan Qu"
    ]
  },
  "https://proceedings.mlr.press/v235/dhir24a.html": {
    "title": "Bivariate Causal Discovery using Bayesian Model Selection",
    "volume": "main",
    "abstract": "Much of the causal discovery literature prioritises guaranteeing the identifiability of causal direction in statistical models. For structures within a Markov equivalence class, this requires strong assumptions which may not hold in real-world datasets, ultimately limiting the usability of these methods. Building on previous attempts, we show how to incorporate causal assumptions within the Bayesian framework. Identifying causal direction then becomes a Bayesian model selection problem. This enables us to construct models with realistic assumptions, and consequently allows for the differentiation between Markov equivalent causal structures. We analyse why Bayesian model selection works in situations where methods based on maximum likelihood fail. To demonstrate our approach, we construct a Bayesian non-parametric model that can flexibly model the joint distribution. We then outperform previous methods on a wide range of benchmark datasets with varying data generating assumptions",
    "checked": true,
    "id": "18e204166b5b458fba3dd6a648780f42a9654d02",
    "semantic_title": "bivariate causal discovery using bayesian model selection",
    "citation_count": 0,
    "authors": [
      "Anish Dhir",
      "Samuel Power",
      "Mark Van Der Wilk"
    ]
  },
  "https://proceedings.mlr.press/v235/dhurandhar24a.html": {
    "title": "Trust Regions for Explanations via Black-Box Probabilistic Certification",
    "volume": "main",
    "abstract": "Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\\ell_{\\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a trust region has multiple benefits: i) insight into model behavior in a region, with a guarantee; ii) ascertained stability of the explanation; iii) explanation reuse, which can save time, energy and money by not having to find explanations for every example; and iv) a possible meta-metric to compare explanation methods. Our contributions include formalizing this problem, proposing solutions, providing theoretical guarantees for these solutions that are computable, and experimentally showing their efficacy on synthetic and real data",
    "checked": true,
    "id": "a9030189e167909ca960153588842ec1e0498122",
    "semantic_title": "trust regions for explanations via black-box probabilistic certification",
    "citation_count": 1,
    "authors": [
      "Amit Dhurandhar",
      "Swagatam Haldar",
      "Dennis Wei",
      "Karthikeyan Natesan Ramamurthy"
    ]
  },
  "https://proceedings.mlr.press/v235/di24a.html": {
    "title": "Double Stochasticity Gazes Faster: Snap-Shot Decentralized Stochastic Gradient Tracking Methods",
    "volume": "main",
    "abstract": "In decentralized optimization, $m$ agents form a network and only communicate with their neighbors, which gives advantages in data ownership, privacy, and scalability. At the same time, decentralized stochastic gradient descent ($\\texttt{SGD}$) methods, as popular decentralized algorithms for training large-scale machine learning models, have shown their superiority over centralized counterparts. Distributed stochastic gradient tracking $\\texttt{DSGT}$ has been recognized as the popular and state-of-the-art decentralized $\\texttt{SGD}$ method due to its proper theoretical guarantees. However, the theoretical analysis of $\\texttt{DSGT}$ shows that its iteration complexity is $\\tilde{\\mathcal{O}} \\left(\\frac{\\bar{\\sigma}^2}{m\\mu \\varepsilon} + \\frac{\\sqrt{L}\\bar{\\sigma}}{\\mu(1 - \\lambda_2(W))^{1/2} C_W \\sqrt{\\varepsilon} }\\right)$, where the doubly stochastic matrix $W$ represents the network topology and $ C_W $ is a parameter that depends on $W$. Thus, it indicates that the convergence property of $\\texttt{DSGT}$ is heavily affected by the topology of the communication network. To overcome the weakness of $\\texttt{DSGT}$, we resort to the snap-shot gradient tracking skill and propose two novel algorithms, snap-shot $\\texttt{DSGT}$ ($\\texttt{SS-DSGT}$) and accelerated snap-shot $\\texttt{DSGT}$ ($\\texttt{ASS-DSGT}$). We further justify that $\\texttt{SS-DSGT}$ exhibits a lower iteration complexity compared to $\\texttt{DSGT}$ in the general communication network topology. Additionally, $\\texttt{ASS-DSGT}$ matches $\\texttt{DSGT}$'s iteration complexity $\\mathcal{O}\\left( \\frac{\\bar{\\sigma}^2}{m\\mu \\varepsilon} + \\frac{\\sqrt{L}\\bar{\\sigma}}{\\mu (1 - \\lambda_2(W))^{1/2}\\sqrt{\\varepsilon}} \\right)$ under the same conditions as $\\texttt{DSGT}$. Numerical experiments validate $\\texttt{SS-DSGT}$'s superior performance performance in the general communication network topology and exhibit better practical performance of $\\texttt{ASS-DSGT}$ on the specified $W$ compared to $\\texttt{DSGT}$",
    "checked": true,
    "id": "11b665250e45ec3aa494c67da0fe87b86ecf5d1c",
    "semantic_title": "double stochasticity gazes faster: snap-shot decentralized stochastic gradient tracking methods",
    "citation_count": 1,
    "authors": [
      "Hao Di",
      "Haishan Ye",
      "Xiangyu Chang",
      "Guang Dai",
      "Ivor Tsang"
    ]
  },
  "https://proceedings.mlr.press/v235/di24b.html": {
    "title": "Double Variance Reduction: A Smoothing Trick for Composite Optimization Problems without First-Order Gradient",
    "volume": "main",
    "abstract": "Variance reduction techniques are designed to decrease the sampling variance, thereby accelerating convergence rates of first-order (FO) and zeroth-order (ZO) optimization methods. However, in composite optimization problems, ZO methods encounter an additional variance called the coordinate-wise variance, which stems from the random gradient estimation. To reduce this variance, prior works require estimating all partial derivatives, essentially approximating FO information. This approach demands $\\mathcal{O}(d)$ function evaluations ($d$ is the dimension size), which incurs substantial computational costs and is prohibitive in high-dimensional scenarios. This paper proposes the Zeroth-order Proximal Double Variance Reduction ($\\texttt{ZPDVR}$) method, which utilizes the averaging trick to reduce both sampling and coordinate-wise variances. Compared to prior methods, $\\texttt{ZPDVR}$ relies solely on random gradient estimates, calls the stochastic zeroth-order oracle (SZO) in expectation $\\mathcal{O}(1)$ times per iteration, and achieves the optimal $\\mathcal{O}(d(n + \\kappa)\\log (\\frac{1}{\\epsilon}))$ SZO query complexity in the strongly convex and smooth setting, where $\\kappa$ represents the condition number and $\\epsilon$ is the desired accuracy. Empirical results validate $\\texttt{ZPDVR}$'s linear convergence and demonstrate its superior performance over other related methods",
    "checked": true,
    "id": "59c16ccad89e3982f33413666c7c882766faad71",
    "semantic_title": "double variance reduction: a smoothing trick for composite optimization problems without first-order gradient",
    "citation_count": 1,
    "authors": [
      "Hao Di",
      "Haishan Ye",
      "Yueling Zhang",
      "Xiangyu Chang",
      "Guang Dai",
      "Ivor Tsang"
    ]
  },
  "https://proceedings.mlr.press/v235/diakonikolas24a.html": {
    "title": "Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination",
    "volume": "main",
    "abstract": "We study Gaussian sparse estimation tasks in Huber's contamination model with a focus on mean estimation, PCA, and linear regression. For each of these tasks, we give the first sample and computationally efficient robust estimators with optimal error guarantees, within constant factors. All prior efficient algorithms for these tasks incur quantitatively suboptimal error. Concretely, for Gaussian robust $k$-sparse mean estimation on $\\mathbb{R}^d$ with corruption rate $\\epsilon>0$, our algorithm has sample complexity $(k^2/\\epsilon ^2)\\mathrm{polylog}(d/\\epsilon)$, runs in sample polynomial time, and approximates the target mean within $\\ell_2$-error $O(\\epsilon)$. Previous efficient algorithms inherently incur error $\\Omega(\\epsilon \\sqrt{\\log(1/\\epsilon)})$. At the technical level, we develop a novel multidimensional filtering method in the sparse regime that may find other applications",
    "checked": true,
    "id": "7c130e1ec0e725608a33895ecf6662e2c7d6a0dd",
    "semantic_title": "robust sparse estimation for gaussians with optimal error under huber contamination",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Daniel Kane",
      "Sushrut Karmalkar",
      "Ankit Pensia",
      "Thanasis Pittas"
    ]
  },
  "https://proceedings.mlr.press/v235/diakonikolas24b.html": {
    "title": "Fast Co-Training under Weak Dependence via Stream-Based Active Learning",
    "volume": "main",
    "abstract": "Co-training is a classical semi-supervised learning method which only requires a small number of labeled examples for learning, under reasonable assumptions. Despite extensive literature on the topic, very few hypothesis classes are known to be provably efficiently learnable via co-training, even under very strong distributional assumptions. In this work, we study the co-training problem in the stream-based active learning model. We show that a range of natural concept classes are efficiently learnable via co-training, in terms of both label efficiency and computational efficiency. We provide an efficient reduction of co-training under the standard assumption of weak dependence, in the stream-based active model, to online classification. As a corollary, we obtain efficient co-training algorithms with error independent label complexity for every concept class class efficiently learnable in the mistake bound online model. Our framework also gives co-training algorithms with label complexity $\\tilde{O}(d\\log (1/\\epsilon))$ for any concept class with VC dimension $d$, though in general this reduction is not computationally efficient. Finally, using additional ideas from online learning, we design the first efficient co-training algorithms with label complexity $\\tilde{O}(d^2\\log (1/\\epsilon))$ for several concept classes, including unions of intervals and homogeneous halfspaces",
    "checked": true,
    "id": "7a1679ee52403312c8fa378ecfa06e6b4b31a474",
    "semantic_title": "fast co-training under weak dependence via stream-based active learning",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Mingchen Ma",
      "Lisheng Ren",
      "Christos Tzamos"
    ]
  },
  "https://proceedings.mlr.press/v235/dickens24a.html": {
    "title": "Convex and Bilevel Optimization for Neural-Symbolic Inference and Learning",
    "volume": "main",
    "abstract": "We leverage convex and bilevel optimization techniques to develop a general gradient-based parameter learning framework for neural-symbolic (NeSy) systems. We demonstrate our framework with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over $100 \\times$ learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a $16$% point prediction performance improvement over alternative learning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Andrew Dickens",
      "Changyu Gao",
      "Connor Pryor",
      "Stephen Wright",
      "Lise Getoor"
    ]
  },
  "https://proceedings.mlr.press/v235/dimitriou24a.html": {
    "title": "Structure Your Data: Towards Semantic Graph Counterfactuals",
    "volume": "main",
    "abstract": "Counterfactual explanations (CEs) based on concepts are explanations that consider alternative scenarios to understand which high-level semantic features contributed to particular model predictions. In this work, we propose CEs based on the semantic graphs accompanying input data to achieve more descriptive, accurate, and human-aligned explanations. Building upon state-of-the-art (SotA) conceptual attempts, we adopt a model-agnostic edit-based approach and introduce leveraging GNNs for efficient Graph Edit Distance (GED) computation. With a focus on the visual domain, we represent images as scene graphs and obtain their GNN embeddings to bypass solving the NP-hard graph similarity problem for all input pairs, an integral part of CE computation process. We apply our method to benchmark and real-world datasets with varying difficulty and availability of semantic annotations. Testing on diverse classifiers, we find that our CEs outperform previous SotA explanation models based on semantics, including both white and black-box as well as conceptual and pixel-level approaches. Their superiority is proven quantitatively and qualitatively, as validated by human subjects, highlighting the significance of leveraging semantic edges in the presence of intricate relationships. Our model-agnostic graph-based approach is widely applicable and easily extensible, producing actionable explanations across different contexts. The code is available at https://github.com/aggeliki-dimitriou/SGCE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angeliki Dimitriou",
      "Maria Lymperaiou",
      "Georgios Filandrianos",
      "Konstantinos Thomas",
      "Giorgos Stamou"
    ]
  },
  "https://proceedings.mlr.press/v235/ding24a.html": {
    "title": "Efficient Algorithms for Sum-Of-Minimum Optimization",
    "volume": "main",
    "abstract": "In this work, we propose a novel optimization model termed \"sum-of-minimum\" optimization. This model seeks to minimize the sum or average of $N$ objective functions over $k$ parameters, where each objective takes the minimum value of a predefined sub-function with respect to the $k$ parameters. This universal framework encompasses numerous clustering applications in machine learning and related fields. We develop efficient algorithms for solving sum-of-minimum optimization problems, inspired by a randomized initialization algorithm for the classic $k$-means (Arthur & Vassilvitskii, 2007) and Lloyd's algorithm (Lloyd, 1982). We establish a new tight bound for the generalized initialization algorithm and prove a gradient-descent-like convergence rate for generalized Lloyd's algorithm. The efficiency of our algorithms is numerically examined on multiple tasks, including generalized principal component analysis, mixed linear regression, and small-scale neural network training. Our approach compares favorably to previous ones based on simpler-but-less-precise optimization reformulations",
    "checked": true,
    "id": "5a6f518936907b0d1f8012d912a22e0b6fb5eee9",
    "semantic_title": "efficient algorithms for sum-of-minimum optimization",
    "citation_count": 1,
    "authors": [
      "Lisang Ding",
      "Ziang Chen",
      "Xinshang Wang",
      "Wotao Yin"
    ]
  },
  "https://proceedings.mlr.press/v235/ding24b.html": {
    "title": "AMPA: Adaptive Mixed Precision Allocation for Low-Bit Integer Training",
    "volume": "main",
    "abstract": "Low-bit integer training emerges as a promising approach to mitigate the heavy burden during network training by quantizing the weights, activations, and gradients. However, existing methods cannot well achieve mixed-precision quantization for low-bit training and are commonly limited to INT8 precision. In this paper, we propose a novel low-bit integer training framework that, for the first time, achieves adaptive mixed-precision allocation (AMPA) for weights, activations, and gradients, and pushes the boundaries to a precision level below INT8. We develop a novel magnitude-based sensitivity measurement with regard to the quantization losses of weight, activation, and gradient quantization and the average gradient magnitudes, which is demonstrated as an upper bound of quantization influence in theory. We further design a layer-wise precision update strategy under observations on the quantization losses and their effects on model performance in low-bit training. Extensive experiments on different backbones and datasets show that, compared to INT8 quantization, the proposed method can achieve more than 38% BitOPs reduction with a tolerable loss below 2% in image classification, image segmentation, and language modeling",
    "checked": true,
    "id": "fe16e3a4592619370cd3c9aafd17a41643568e56",
    "semantic_title": "ampa: adaptive mixed precision allocation for low-bit integer training",
    "citation_count": 1,
    "authors": [
      "Li Ding",
      "Wen Fei",
      "Yuyang Huang",
      "Shuangrui Ding",
      "Wenrui Dai",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://proceedings.mlr.press/v235/ding24c.html": {
    "title": "Understanding Forgetting in Continual Learning with Linear Regression",
    "volume": "main",
    "abstract": "Continual learning, focused on sequentially learning multiple tasks, has gained significant attention recently. Despite the tremendous progress made in the past, the theoretical understanding, especially factors contributing to $\\textit{catastrophic forgetting}$, remains relatively unexplored. In this paper, we provide a general theoretical analysis of forgetting in the linear regression model via Stochastic Gradient Descent (SGD) applicable to both under-parameterized and overparameterized regimes. Our theoretical framework reveals some interesting insights into the intricate relationship between task sequence and algorithmic parameters, an aspect not fully captured in previous studies due to their restrictive assumptions. Specifically, we demonstrate that, given a sufficiently large data size, the arrangement of tasks in a sequence—where tasks with larger eigenvalues in their population data covariance matrices are trained later—tends to result in increased forgetting. Additionally, our findings highlight that an appropriate choice of step size will help mitigate forgetting in both under-parameterized and overparameterized settings. To validate our theoretical analysis, we conducted simulation experiments on both linear regression models and Deep Neural Networks (DNNs). Results from these simulations substantiate our theoretical findings",
    "checked": true,
    "id": "43c16086367d3b8aedbbfc2b4f812fd8aae7343f",
    "semantic_title": "understanding forgetting in continual learning with linear regression",
    "citation_count": 4,
    "authors": [
      "Meng Ding",
      "Kaiyi Ji",
      "Di Wang",
      "Jinhui Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/ding24d.html": {
    "title": "Recurrent Distance Filtering for Graph Representation Learning",
    "volume": "main",
    "abstract": "Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing the information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but lack graph inductive bias and have to rely on ad-hoc positional encoding. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a linear RNN to encode the sequence of hop representations. The linear RNN is parameterized in a particular diagonal form for stable long-range signal propagation and is theoretically expressive enough to encode the neighborhood hierarchy. With no need for positional encoding, we empirically show that the performance of our model is comparable to or better than that of state-of-the-art graph transformers on various benchmarks, with a significantly reduced computational cost. Our code is open-source at https://github.com/skeletondyh/GRED",
    "checked": true,
    "id": "92f38084de7ed74305ce7b9c3a3a3f845182fd82",
    "semantic_title": "recurrent distance filtering for graph representation learning",
    "citation_count": 2,
    "authors": [
      "Yuhui Ding",
      "Antonio Orvieto",
      "Bobby He",
      "Thomas Hofmann"
    ]
  },
  "https://proceedings.mlr.press/v235/ding24e.html": {
    "title": "Robust Stable Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) are gaining popularity in deep learning due to their low energy budget on neuromorphic hardware. However, they still face challenges in lacking sufficient robustness to guard safety-critical applications such as autonomous driving. Many studies have been conducted to defend SNNs from the threat of adversarial attacks. This paper aims to uncover the robustness of SNN through the lens of the stability of nonlinear systems. We are inspired by the fact that searching for parameters altering the leaky integrate-and-fire dynamics can enhance their robustness. Thus, we dive into the dynamics of membrane potential perturbation and simplify the formulation of the dynamics. We present that membrane potential perturbation dynamics can reliably convey the intensity of perturbation. Our theoretical analyses imply that the simplified perturbation dynamics satisfy input-output stability. Thus, we propose a training framework with modified SNN neurons and to reduce the mean square of membrane potential perturbation aiming at enhancing the robustness of SNN. Finally, we experimentally verify the effectiveness of the framework in the setting of Gaussian noise training and adversarial training on the image classification task. Please refer to https://github.com/DingJianhao/stable-snn for our code implementation",
    "checked": true,
    "id": "29514e6b13f80e0c1e686960df368476b73d5ad5",
    "semantic_title": "robust stable spiking neural networks",
    "citation_count": 1,
    "authors": [
      "Jianhao Ding",
      "Zhiyu Pan",
      "Yujia Liu",
      "Zhaofei Yu",
      "Tiejun Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/ding24f.html": {
    "title": "Fewer Truncations Improve Language Modeling",
    "volume": "main",
    "abstract": "In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity—it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hantian Ding",
      "Zijian Wang",
      "Giovanni Paolini",
      "Varun Kumar",
      "Anoop Deoras",
      "Dan Roth",
      "Stefano Soatto"
    ]
  },
  "https://proceedings.mlr.press/v235/ding24g.html": {
    "title": "Delving into Differentially Private Transformer",
    "volume": "main",
    "abstract": "Deep learning with differential privacy (DP) has garnered significant attention over the past years, leading to the development of numerous methods aimed at enhancing model accuracy and training efficiency. This paper delves into the problem of training Transformer models with differential privacy. Our treatment is modular: the logic is to 'reduce' the problem of training DP Transformer to the more basic problem of training DP vanilla neural nets. The latter is better understood and amenable to many model-agnostic methods. Such 'reduction' is done by first identifying the hardness unique to DP Transformer training: the attention distraction phenomenon and a lack of compatibility with existing techniques for efficient gradient clipping. To deal with these two issues, we propose the Re-Attention Mechanism and Phantom Clipping, respectively. We believe that our work not only casts new light on training DP Transformers but also promotes a modular treatment to advance research in the field of differentially private deep learning",
    "checked": true,
    "id": "be6c26ba4a0e1e821e260fe05f7c9366099f1d7f",
    "semantic_title": "delving into differentially private transformer",
    "citation_count": 4,
    "authors": [
      "Youlong Ding",
      "Xueyang Wu",
      "Yining Meng",
      "Yonggang Luo",
      "Hao Wang",
      "Weike Pan"
    ]
  },
  "https://proceedings.mlr.press/v235/ding24h.html": {
    "title": "Quality Diversity through Human Feedback: Towards Open-Ended Diversity-Driven Optimization",
    "volume": "main",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown potential in qualitative tasks where easily defined performance measures are lacking. However, there are drawbacks when RLHF is commonly used to optimize for average human preferences, especially in generative tasks that demand diverse model responses. Meanwhile, Quality Diversity (QD) algorithms excel at identifying diverse and high-quality solutions but often rely on manually crafted diversity metrics. This paper introduces Quality Diversity through Human Feedback (QDHF), a novel approach that progressively infers diversity metrics from human judgments of similarity among solutions, thereby enhancing the applicability and effectiveness of QD algorithms in complex and open-ended domains. Empirical studies show that QDHF significantly outperforms state-of-the-art methods in automatic diversity discovery and matches the efficacy of QD with manually crafted diversity metrics on standard benchmarks in robotics and reinforcement learning. Notably, in open-ended generative tasks, QDHF substantially enhances the diversity of text-to-image generation from a diffusion model and is more favorably received in user studies. We conclude by analyzing QDHF's scalability, robustness, and quality of derived diversity metrics, emphasizing its strength in open-ended optimization tasks. Code and tutorials are available at https://liding.info/qdhf",
    "checked": false,
    "id": "c47785aa96672fdb1d2da3515a5c9a0cf8997e69",
    "semantic_title": "quality diversity through human feedback",
    "citation_count": 7,
    "authors": [
      "Li Ding",
      "Jenny Zhang",
      "Jeff Clune",
      "Lee Spector",
      "Joel Lehman"
    ]
  },
  "https://proceedings.mlr.press/v235/ding24i.html": {
    "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",
    "volume": "main",
    "abstract": "Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations. Code is available at https://github.com/microsoft/LongRoPE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Ding",
      "Li Lyna Zhang",
      "Chengruidong Zhang",
      "Yuanyuan Xu",
      "Ning Shang",
      "Jiahang Xu",
      "Fan Yang",
      "Mao Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/dodd24a.html": {
    "title": "Learning-Rate-Free Stochastic Optimization over Riemannian Manifolds",
    "volume": "main",
    "abstract": "In recent years, interest in gradient-based optimization over Riemannian manifolds has surged. However, a significant challenge lies in the reliance on hyperparameters, especially the learning rate, which requires meticulous tuning by practitioners to ensure convergence at a suitable rate. In this work, we introduce innovative learning-rate-free algorithms for stochastic optimization over Riemannian manifolds, eliminating the need for hand-tuning and providing a more robust and user-friendly approach. We establish high probability convergence guarantees that are optimal, up to logarithmic factors, compared to the best-known optimally tuned rate in the deterministic setting. Our approach is validated through numerical experiments, demonstrating competitive performance against learning-rate-dependent algorithms",
    "checked": true,
    "id": "ce823da4c040675af4ab27910b9c1b7ccd0a5b58",
    "semantic_title": "learning-rate-free stochastic optimization over riemannian manifolds",
    "citation_count": 0,
    "authors": [
      "Daniel Dodd",
      "Louis Sharrock",
      "Christopher Nemeth"
    ]
  },
  "https://proceedings.mlr.press/v235/dohmatob24a.html": {
    "title": "Consistent Adversarially Robust Linear Classification: Non-Parametric Setting",
    "volume": "main",
    "abstract": "For binary classification in $d$ dimensions, it is known that with a sample size of $n$, an excess adversarial risk of $O(d/n)$ is achievable under strong parametric assumptions about the underlying data distribution (e.g., assuming a Gaussian mixture model). In the case of well-separated distributions, this rate can be further refined to $O(1/n)$. Our work studies the non-parametric setting, where very little is known. With only mild regularity conditions on the conditional distribution of the features, we examine adversarial attacks with respect to arbitrary norms and introduce a straightforward yet effective estimator with provable consistency w.r.t adversarial risk. Our estimator is given by minimizing a series of smoothed versions of the robust 0/1 loss, with a smoothing bandwidth that adapts to both $n$ and $d$. Furthermore, we demonstrate that our estimator can achieve the minimax excess adversarial risk of $\\widetilde O(\\sqrt{d/n})$ for linear classifiers, at the cost of solving possibly rougher optimization problems",
    "checked": true,
    "id": "ef82f000b0063c5736d73fbd217ad6c065ba81e6",
    "semantic_title": "consistent adversarially robust linear classification: non-parametric setting",
    "citation_count": 0,
    "authors": [
      "Elvis Dohmatob"
    ]
  },
  "https://proceedings.mlr.press/v235/dohmatob24b.html": {
    "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
    "volume": "main",
    "abstract": "As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the \"un-learning\" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2",
    "checked": true,
    "id": "837b55eefbeee72e580e97a7b3c7136e714134b4",
    "semantic_title": "a tale of tails: model collapse as a change of scaling laws",
    "citation_count": 38,
    "authors": [
      "Elvis Dohmatob",
      "Yunzhen Feng",
      "Pu Yang",
      "Francois Charton",
      "Julia Kempe"
    ]
  },
  "https://proceedings.mlr.press/v235/dohmatob24c.html": {
    "title": "Precise Accuracy / Robustness Tradeoffs in Regression: Case of General Norms",
    "volume": "main",
    "abstract": "In this paper, we investigate the impact of test-time adversarial attacks on linear regression models and determine the optimal level of robustness that any model can reach while maintaining a given level of standard predictive performance (accuracy). Through quantitative estimates, we uncover fundamental tradeoffs between adversarial robustness and accuracy in different regimes. We obtain a precise characterization which distinguishes between regimes where robustness is achievable without hurting standard accuracy and regimes where a tradeoff might be unavoidable. Our findings are empirically confirmed with simple experiments that represent a variety of settings. This work covers feature covariance matrices and attack norms of any nature, extending previous works in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elvis Dohmatob",
      "Meyer Scetbon"
    ]
  },
  "https://proceedings.mlr.press/v235/doikov24a.html": {
    "title": "Spectral Preconditioning for Gradient Methods on Graded Non-convex Functions",
    "volume": "main",
    "abstract": "The performance of optimization methods is often tied to the spectrum of the objective Hessian. Yet, conventional assumptions, such as smoothness, do often not enable us to make finely-grained convergence statements—particularly not for non-convex problems. Striving for a more intricate characterization of complexity, we introduce a unique concept termed graded non-convexity. This allows to partition the class of non-convex problems into a nested chain of subclasses. Interestingly, many traditional non-convex objectives, including partially convex problems, matrix factorizations, and neural networks, fall within these subclasses. As a second contribution, we propose gradient methods with spectral preconditioning, which employ inexact top eigenvectors of the Hessian to address the ill-conditioning of the problem, contingent on the grade. Our analysis reveals that these new methods provide provably superior convergence rates compared to basic gradient descent on applicable problem classes, particularly when large gaps exist between the top eigenvalues of the Hessian. Our theory is validated by numerical experiments executed on multiple practical machine learning problems",
    "checked": true,
    "id": "fcfe8b33490c91ed49fb2df9fb4a10f3d6d68a59",
    "semantic_title": "spectral preconditioning for gradient methods on graded non-convex functions",
    "citation_count": 1,
    "authors": [
      "Nikita Doikov",
      "Sebastian U Stich",
      "Martin Jaggi"
    ]
  },
  "https://proceedings.mlr.press/v235/donahue24a.html": {
    "title": "Impact of Decentralized Learning on Player Utilities in Stackelberg Games",
    "volume": "main",
    "abstract": "When deployed in the world, a learning agent such as a recommender system or a chatbot often repeatedly interacts with another learning agent (such as a user) over time. In many such two-agent systems, each agent learns separately and the rewards of the two agents are not perfectly aligned. To better understand such cases, we examine the learning dynamics of the two-agent system and the implications for each agent's objective. We model these systems as Stackelberg games with decentralized learning and show that standard regret benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case linear regret for at least one player. To better capture these systems, we construct a relaxed regret benchmark that is tolerant to small learning errors by agents. We show that standard learning algorithms fail to provide sublinear regret, and we develop algorithms to achieve near-optimal $\\mathcal{O}(T^{2/3})$ regret for both players with respect to these benchmarks. We further design relaxed environments under which faster learning ($\\mathcal{O}(\\sqrt{T})$) is possible. Altogether, our results take a step towards assessing how two-agent interactions in sequential and decentralized learning environments affect the utility of both agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kate Donahue",
      "Nicole Immorlica",
      "Meena Jagadeesan",
      "Brendan Lucier",
      "Aleksandrs Slivkins"
    ]
  },
  "https://proceedings.mlr.press/v235/dong24a.html": {
    "title": "Towards Generalization beyond Pointwise Learning: A Unified Information-theoretic Perspective",
    "volume": "main",
    "abstract": "The recent surge in contrastive learning has intensified the interest in understanding the generalization of non-pointwise learning paradigms. While information-theoretic analysis achieves remarkable success in characterizing the generalization behavior of learning algorithms, its applicability is largely confined to pointwise learning, with extensions to the simplest pairwise settings remaining unexplored due to the challenges of non-i.i.d losses and dimensionality explosion. In this paper, we develop the first series of information-theoretic bounds extending beyond pointwise scenarios, encompassing pointwise, pairwise, triplet, quadruplet, and higher-order scenarios, all within a unified framework. Specifically, our hypothesis-based bounds elucidate the generalization behavior of iterative and noisy learning algorithms via gradient covariance analysis, and our prediction-based bounds accurately estimate the generalization gap with computationally tractable low-dimensional information metrics. Comprehensive numerical studies then demonstrate the effectiveness of our bounds in capturing the generalization dynamics across diverse learning scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Dong",
      "Tieliang Gong",
      "Hong Chen",
      "Zhongjiang He",
      "Mengxiang Li",
      "Shuangyong Song",
      "Chen Li"
    ]
  },
  "https://proceedings.mlr.press/v235/dong24b.html": {
    "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
    "volume": "main",
    "abstract": "Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively expensive and computationally demanding. Recently, post-training pruning approaches introduced novel metrics, enabling the pruning of LLMs without retraining. However, these metrics require the involvement of human experts and tedious trial and error. To efficiently identify superior pruning metrics, we develop an automatic framework for searching symbolic pruning metrics using genetic programming. In particular, we devise an elaborate search space encompassing the existing pruning metrics to discover the potential symbolic pruning metric. We propose an opposing operation simplification strategy to increase the diversity of the population. In this way, Pruner-Zero allows auto-generation of symbolic pruning metrics. Based on the searched results, we explore the correlation between pruning metrics and performance after pruning and summarize some principles. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate that our Pruner-Zero obtains superior performance than SOTA post-training pruning methods. Code at: https://github.com/pprp/Pruner-Zero",
    "checked": true,
    "id": "a7919a3c6dbdcc524776a3102110d637836ad2e0",
    "semantic_title": "pruner-zero: evolving symbolic pruning metric from scratch for large language models",
    "citation_count": 9,
    "authors": [
      "Peijie Dong",
      "Lujun Li",
      "Zhenheng Tang",
      "Xiang Liu",
      "Xinglin Pan",
      "Qiang Wang",
      "Xiaowen Chu"
    ]
  },
  "https://proceedings.mlr.press/v235/dong24c.html": {
    "title": "Position: Building Guardrails for Large Language Models Requires Systematic Design",
    "volume": "main",
    "abstract": "As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and testing to ensure the utmost quality of the final product",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Dong",
      "Ronghui Mu",
      "Gaojie Jin",
      "Yi Qi",
      "Jinwei Hu",
      "Xingyu Zhao",
      "Jie Meng",
      "Wenjie Ruan",
      "Xiaowei Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/dong24d.html": {
    "title": "Accelerating PDE Data Generation via Differential Operator Action in Solution Space",
    "volume": "main",
    "abstract": "Recent advancements in data-driven approaches, such as Neural Operator (NO), have demonstrated their effectiveness in reducing the solving time of Partial Differential Equations (PDEs). However, one major challenge faced by these approaches is the requirement for a large amount of high-precision training data, which needs significant computational costs during the generation process. To address this challenge, we propose a novel PDE dataset generation algorithm, namely Differential Operator Action in Solution space (DiffOAS), which speeds up the data generation process and enhances the precision of the generated data simultaneously. Specifically, DiffOAS obtains a few basic PDE solutions and then combines them to get solutions. It applies differential operators on these solutions, a process we call 'operator action', to efficiently generate precise PDE data points. Theoretical analysis shows that the time complexity of DiffOAS method is one order lower than the existing generation method. Experimental results show that DiffOAS accelerates the generation of large-scale datasets with 10,000 instances by 300 times. Even with just 5% of the generation time, NO trained on the data generated by DiffOAS exhibits comparable performance to that using the existing generation method, which highlights the efficiency of DiffOAS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanshuo Dong",
      "Hong Wang",
      "Haoyang Liu",
      "Jian Luo",
      "Jie Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/dong24e.html": {
    "title": "TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling",
    "volume": "main",
    "abstract": "Time series pre-training has recently garnered wide attention for its potential to reduce labeling expenses and benefit various downstream tasks. Prior methods are mainly based on pre-training techniques well-acknowledged in vision or language, such as masked modeling and contrastive learning. However, randomly masking time series or calculating series-wise similarity will distort or neglect inherent temporal correlations crucial in time series data. To emphasize temporal correlation modeling, this paper proposes TimeSiam as a simple but effective self-supervised pre-training framework for Time series based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to capture intrinsic temporal correlations between randomly sampled past and current subseries. With a simple data augmentation method (e.g. masking), TimeSiam can benefit from diverse augmented subseries and learn internal time-dependent representations through a past-to-current reconstruction. Moreover, learnable lineage embeddings are also introduced to distinguish temporal distance between sampled series and further foster the learning of diverse temporal correlations. TimeSiam consistently outperforms extensive advanced pre-training baselines, demonstrating superior forecasting and classification capabilities across 13 standard benchmarks in both intra- and cross-domain scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiang Dong",
      "Haixu Wu",
      "Yuxuan Wang",
      "Yun-Zhong Qiu",
      "Li Zhang",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://proceedings.mlr.press/v235/dong24f.html": {
    "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
    "volume": "main",
    "abstract": "Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS",
    "checked": true,
    "id": "ef1b02dc1b82f9955fc4760fcefd92c0fff9f227",
    "semantic_title": "get more with less: synthesizing recurrence with kv cache compression for efficient llm inference",
    "citation_count": 26,
    "authors": [
      "Harry Dong",
      "Xinyu Yang",
      "Zhenyu Zhang",
      "Zhangyang Wang",
      "Yuejie Chi",
      "Beidi Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/donhauser24a.html": {
    "title": "Privacy-Preserving Data Release Leveraging Optimal Transport and Particle Gradient Descent",
    "volume": "main",
    "abstract": "We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Donhauser",
      "Javier Abad",
      "Neha Hulkund",
      "Fanny Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/doran24a.html": {
    "title": "Spike Distance Function as a Learning Objective for Spike Prediction",
    "volume": "main",
    "abstract": "Approaches to predicting neuronal spike responses commonly use a Poisson learning objective. This objective quantizes responses into spike counts within a fixed summation interval, typically on the order of 10 to 100 milliseconds in duration; however, neuronal responses are often time accurate down to a few milliseconds, and Poisson models struggle to precisely model them at these timescales. We propose the concept of a spike distance function that maps points in time to the temporal distance to the nearest spike. We show that neural networks can be trained to approximate spike distance functions, and we present an efficient algorithm for inferring spike trains from the outputs of these models. Using recordings of chicken and frog retinal ganglion cells responding to visual stimuli, we compare the performance of our approach to that of Poisson models trained with various summation intervals. We show that our approach outperforms the use of Poisson models at spike train inference",
    "checked": true,
    "id": "e00f214667781453f21ddf15064f09185f4cd1b6",
    "semantic_title": "spike distance function as a learning objective for spike prediction",
    "citation_count": 0,
    "authors": [
      "Kevin Doran",
      "Marvin Seifert",
      "Carola A. M. Yovanovich",
      "Tom Baden"
    ]
  },
  "https://proceedings.mlr.press/v235/dorfman24a.html": {
    "title": "Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers",
    "volume": "main",
    "abstract": "Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques focus on the static setting, wherein the identity of Byzantine workers remains unchanged throughout the learning process. This assumption fails to capture real-world dynamic Byzantine behaviors, which may include intermittent malfunctions or targeted, time-limited attacks. Addressing this limitation, we propose DynaBRO – a new method capable of withstanding any sub-linear number of identity changes across rounds. Specifically, when the number of such changes is $\\mathcal{O}(\\sqrt{T})$ (where $T$ is the total number of training rounds), DynaBRO nearly matches the state-of-the-art asymptotic convergence rate of the static setting. Our method utilizes a multi-level Monte Carlo (MLMC) gradient estimation technique applied at the server to robustly aggregated worker updates. By additionally leveraging an adaptive learning rate, we circumvent the need for prior knowledge of the fraction of Byzantine workers",
    "checked": true,
    "id": "3144176620cf1f6483c64b163bcf0faf9faa1f34",
    "semantic_title": "dynamic byzantine-robust learning: adapting to switching byzantine workers",
    "citation_count": 0,
    "authors": [
      "Ron Dorfman",
      "Naseem Amin Yehya",
      "Kfir Yehuda Levy"
    ]
  },
  "https://proceedings.mlr.press/v235/dorner24a.html": {
    "title": "Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget",
    "volume": "main",
    "abstract": "We study how to best spend a budget of noisy labels to compare the accuracy of two binary classifiers. It's common practice to collect and aggregate multiple noisy labels for a given data point into a less noisy label via a majority vote. We prove a theorem that runs counter to conventional wisdom. If the goal is to identify the better of two classifiers, we show it's best to spend the budget on collecting a single label for more samples. Our result follows from a non-trivial application of Cramér's theorem, a staple in the theory of large deviations. We discuss the implications of our work for the design of machine learning benchmarks, where they overturn some time-honored recommendations. In addition, our results provide sample size bounds superior to what follows from Hoeffding's bound",
    "checked": true,
    "id": "deb20bee3776e79c109581c84c97c1472a53427d",
    "semantic_title": "don't label twice: quantity beats quality when comparing binary classifiers on a budget",
    "citation_count": 2,
    "authors": [
      "Florian E. Dorner",
      "Moritz Hardt"
    ]
  },
  "https://proceedings.mlr.press/v235/dotzel24a.html": {
    "title": "Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs",
    "volume": "main",
    "abstract": "The increasing size of large language models (LLMs) traditionally requires low-precision integer formats to meet strict latency and power demands. Yet recently, alternative formats such as Normal Float (NF4) have increased model accuracy at the cost of increased chip area. In this work, we first conduct a large-scale analysis of LLM weights and activations across 30 networks and conclude that most distributions follow a Student's t-distribution. We then derive a new theoretically optimal format, Student Float (SF4), that improves over NF4 across modern LLMs, for example increasing the average accuracy on LLaMA2-7B by 0.76% across tasks. Using this format as a high-accuracy reference, we then propose augmenting E2M1 with two variants of supernormal support for higher model accuracy. Finally, we explore the quality and efficiency frontier across 11 datatypes by evaluating their model accuracy and hardware complexity. We discover a Pareto curve composed of INT4, E2M1, and E2M1 with supernormal support, which offers a continuous tradeoff between model accuracy and chip area. For example, E2M1 with supernormal support increases the accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more LLM-based applications to be run at four bits. The supporting code is hosted at https://github.com/cornell-zhang/llm-datatypes",
    "checked": true,
    "id": "afff235c6d8e196be4d2dafaee01a03595a66de1",
    "semantic_title": "learning from students: applying t-distributions to explore accurate and efficient formats for llms",
    "citation_count": 4,
    "authors": [
      "Jordan Dotzel",
      "Yuzong Chen",
      "Bahaa Kotb",
      "Sushma Prasad",
      "Gang Wu",
      "Sheng Li",
      "Mohamed S Abdelfattah",
      "Zhiru Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/dou24a.html": {
    "title": "Theory of Consistency Diffusion Models: Distribution Estimation Meets Fast Sampling",
    "volume": "main",
    "abstract": "Diffusion models have revolutionized various application domains, including computer vision and audio generation. Despite the state-of-the-art performance, diffusion models are known for their slow sample generation due to the extensive number of steps involved. In response, consistency models have been developed to merge multiple steps in the sampling process, thereby significantly boosting the speed of sample generation without compromising quality. This paper contributes towards the first statistical theory for consistency models, formulating their training as a distribution discrepancy minimization problem. Our analysis yields statistical estimation rates based on the Wasserstein distance for consistency models, matching those of vanilla diffusion models. Additionally, our results encompass the training of consistency models through both distillation and isolation methods, demystifying their underlying advantage",
    "checked": true,
    "id": "df477d708b7528ec2dbffbb53086e550b11d2582",
    "semantic_title": "theory of consistency diffusion models: distribution estimation meets fast sampling",
    "citation_count": 0,
    "authors": [
      "Zehao Dou",
      "Minshuo Chen",
      "Mengdi Wang",
      "Zhuoran Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/draxler24a.html": {
    "title": "On the Universality of Volume-Preserving and Coupling-Based Normalizing Flows",
    "volume": "main",
    "abstract": "We present a novel theoretical framework for understanding the expressive power of normalizing flows. Despite their prevalence in scientific applications, a comprehensive understanding of flows remains elusive due to their restricted architectures. Existing theorems fall short as they require the use of arbitrarily ill-conditioned neural networks, limiting practical applicability. We propose a distributional universality theorem for well-conditioned coupling-based normalizing flows such as RealNVP. In addition, we show that volume-preserving normalizing flows are not universal, what distribution they learn instead, and how to fix their expressivity. Our results support the general wisdom that affine and related couplings are expressive and in general outperform volume-preserving flows, bridging a gap between empirical results and theoretical understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Draxler",
      "Stefan Wahl",
      "Christoph Schnoerr",
      "Ullrich Koethe"
    ]
  },
  "https://proceedings.mlr.press/v235/drouin24a.html": {
    "title": "WorkArena: How Capable are Web Agents at Solving Common Knowledge Work Tasks?",
    "volume": "main",
    "abstract": "We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 33 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Drouin",
      "Maxime Gasse",
      "Massimo Caccia",
      "Issam H. Laradji",
      "Manuel Del Verme",
      "Tom Marty",
      "David Vazquez",
      "Nicolas Chapados",
      "Alexandre Lacoste"
    ]
  },
  "https://proceedings.mlr.press/v235/du24a.html": {
    "title": "Principled Gradient-Based MCMC for Conditional Sampling of Text",
    "volume": "main",
    "abstract": "We consider the problem of sampling text from an energy-based model. This arises, for example, when sampling text from a neural language model subject to soft constraints. Although the target distribution is discrete, the internal computations of the energy function (given by the language model) are differentiable, so one would like to exploit gradient information within a method such as MCMC. Alas, all previous attempts to generalize gradient-based MCMC to text sampling fail to sample correctly from the target distribution. We propose a solution, along with variants, and study its theoretical properties. Through experiments on various forms of text generation, we demonstrate that our unbiased samplers are able to generate more fluent text while better adhering to the control objectives. The same methods could be used to sample from discrete energy-based models unrelated to text",
    "checked": true,
    "id": "dfdc45be2f0a01b08c428aa786be3712ea18417c",
    "semantic_title": "principled gradient-based mcmc for conditional sampling of text",
    "citation_count": 1,
    "authors": [
      "Li Du",
      "Afra Amini",
      "Lucas Torroba Hennigen",
      "Xinyan Velocity Yu",
      "Holden Lee",
      "Jason Eisner",
      "Ryan Cotterell"
    ]
  },
  "https://proceedings.mlr.press/v235/du24b.html": {
    "title": "SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) method by separating the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization phase, leading to the formulation of a Bayes classifier. The Bayes classifier, in turn, enhances the quality of pseudo-labels in the expectation phase. Remarkably, the SimPro framework is not only straightforward to implement but also comes with theoretical guarantees. Moreover, we introduce two novel class distributions broadening the scope of the evaluation. Our method showcases consistent state-of-the-art performance across diverse benchmarks and data distribution scenarios. benchmarks and data distribution scenarios. Our code is available at https://github.com/LeapLabTHU/SimPro",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoqun Du",
      "Yizeng Han",
      "Gao Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/du24c.html": {
    "title": "GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding",
    "volume": "main",
    "abstract": "Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cunxiao Du",
      "Jing Jiang",
      "Xu Yuanchen",
      "Jiawei Wu",
      "Sicheng Yu",
      "Yongqi Li",
      "Shenggui Li",
      "Kai Xu",
      "Liqiang Nie",
      "Zhaopeng Tu",
      "Yang You"
    ]
  },
  "https://proceedings.mlr.press/v235/du24d.html": {
    "title": "Position: Compositional Generative Modeling: A Single Model is Not All You Need",
    "volume": "main",
    "abstract": "Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data",
    "checked": true,
    "id": "94e1717e6314db89ce4f2d5b3e4f103e66e8f7fd",
    "semantic_title": "position: compositional generative modeling: a single model is not all you need",
    "citation_count": 1,
    "authors": [
      "Yilun Du",
      "Leslie Pack Kaelbling"
    ]
  },
  "https://proceedings.mlr.press/v235/du24e.html": {
    "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such \"society of minds\" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding",
    "checked": true,
    "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
    "semantic_title": "improving factuality and reasoning in language models through multiagent debate",
    "citation_count": 388,
    "authors": [
      "Yilun Du",
      "Shuang Li",
      "Antonio Torralba",
      "Joshua B. Tenenbaum",
      "Igor Mordatch"
    ]
  },
  "https://proceedings.mlr.press/v235/du24f.html": {
    "title": "Learning Iterative Reasoning through Energy Diffusion",
    "volume": "main",
    "abstract": "We introduce iterative reasoning through energy diffusion (IRED), a novel framework for learning to reason for a variety of tasks by formulating reasoning and decision-making problems with energy-based optimization. IRED learns energy functions to represent the constraints between input conditions and desired outputs. After training, IRED adapts the number of optimization steps during inference based on problem difficulty, enabling it to solve problems outside its training distribution — such as more complex Sudoku puzzles, matrix completion with large value magnitudes, and path finding in larger graphs. Key to our method's success is two novel techniques: learning a sequence of annealed energy landscapes for easier inference and a combination of score function and energy landscape supervision for faster and more stable training. Our experiments show that IRED outperforms existing methods in continuous-space reasoning, discrete-space reasoning, and planning tasks, particularly in more challenging scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Du",
      "Jiayuan Mao",
      "Joshua B. Tenenbaum"
    ]
  },
  "https://proceedings.mlr.press/v235/du24g.html": {
    "title": "When and How Does In-Distribution Label Help Out-of-Distribution Detection?",
    "volume": "main",
    "abstract": "Detecting data points deviating from the training distribution is pivotal for ensuring reliable machine learning. Extensive research has been dedicated to the challenge, spanning classical anomaly detection techniques to contemporary out-of-distribution (OOD) detection approaches. While OOD detection commonly relies on supervised learning from a labeled in-distribution (ID) dataset, anomaly detection may treat the entire ID data as a single class and disregard ID labels. This fundamental distinction raises a significant question that has yet to be rigorously explored: when and how does ID label help OOD detection? This paper bridges this gap by offering a formal understanding to theoretically delineate the impact of ID labels on OOD detection. We employ a graph-theoretic approach, rigorously analyzing the separability of ID data from OOD data in a closed-form manner. Key to our approach is the characterization of data representations through spectral decomposition on the graph. Leveraging these representations, we establish a provable error bound that compares the OOD detection performance with and without ID labels, unveiling conditions for achieving enhanced OOD detection. Lastly, we present empirical results on both simulated and real datasets, validating theoretical guarantees and reinforcing our insights",
    "checked": true,
    "id": "87268ea5825cd65c1c3151d6ecc0973f267b3c68",
    "semantic_title": "when and how does in-distribution label help out-of-distribution detection?",
    "citation_count": 3,
    "authors": [
      "Xuefeng Du",
      "Yiyou Sun",
      "Yixuan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/du24h.html": {
    "title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls",
    "volume": "main",
    "abstract": "We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.5% in terms of average pass rate on ToolBench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Du",
      "Fangyun Wei",
      "Hongyang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/du24i.html": {
    "title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization",
    "volume": "main",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level elliptical potential analysis technique used to infer reward function parameters when comparison queries rather than reward observations are used. We provide and analyze algorithms in two settings: linear and neural function approximation, PG-RLHF and NN-PG-RLHF, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Du",
      "Anna Winnicki",
      "Gal Dalal",
      "Shie Mannor",
      "R. Srikant"
    ]
  },
  "https://proceedings.mlr.press/v235/du24j.html": {
    "title": "Bottleneck-Minimal Indexing for Generative Document Retrieval",
    "volume": "main",
    "abstract": "We apply an information-theoretic perspective to reconsider generative document retrieval (GDR), in which a document $x \\in \\mathcal{X}$ is indexed by $t \\in \\mathcal{T}$, and a neural autoregressive model is trained to map queries $\\mathcal{Q}$ to $\\mathcal{T}$. GDR can be considered to involve information transmission from documents $\\mathcal{X}$ to queries $\\mathcal{Q}$, with the requirement to transmit more bits via the indexes $\\mathcal{T}$. By applying Shannon's rate-distortion theory, the optimality of indexing can be analyzed in terms of the mutual information, and the design of the indexes $\\mathcal{T}$ can then be regarded as a bottleneck in GDR. After reformulating GDR from this perspective, we empirically quantify the bottleneck underlying GDR. Finally, using the NQ320K and MARCO datasets, we evaluate our proposed bottleneck-minimal indexing method in comparison with various previous indexing methods, and we show that it outperforms those methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Du",
      "Lixin Xiu",
      "Kumiko Tanaka-Ishii"
    ]
  },
  "https://proceedings.mlr.press/v235/duan24a.html": {
    "title": "MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance, and organizations are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search. However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs. In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving. The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources. MuxServe formally formulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal colocations and maximize utilization. MuxServe designs a unified resource manager to enable flexible and efficient multiplexing. Evaluation results show that MuxServe can achieves up to $1.8\\times$ higher throughput or processes $2.9\\times$ more requests within $99%$ SLO attainment. The code is available at: https://github.com/hao-ai-lab/MuxServe",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangfei Duan",
      "Runyu Lu",
      "Haojie Duanmu",
      "Xiuhong Li",
      "Xingcheng Zhang",
      "Dahua Lin",
      "Ion Stoica",
      "Hao Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/duan24b.html": {
    "title": "MF-CLR: Multi-Frequency Contrastive Learning Representation for Time Series",
    "volume": "main",
    "abstract": "Learning a decent representation from unlabeled time series is a challenging task, especially when the time series data is derived from diverse channels at different sampling rates. Our motivation stems from the financial domain, where sparsely labeled covariates are commonly collected at different frequencies, e.g., daily stock market index, monthly unemployment rate and quarterly net revenue of a certain listed corporation. This paper presents Multi-Frequency Contrastive Learning Representation (MF-CLR), aimed at learning a good representation of multi-frequency time series in a self-supervised paradigm by leveraging the ability of contrastive learning. MF-CLR introduces a hierarchical mechanism that spans across different frequencies along the feature dimension. Within each contrastive block, two groups of subseries with adjacent frequencies are embedded based on our proposed cross-frequency consistency. To validate the effectiveness of MF-CLR, we conduct extensive experiments on five downstream tasks, including long-term and short-term forecasting, classification, anomaly detection and imputation. Experimental evidence shows that MF-CLR delivers a leading performance in all the downstream tasks and keeps consistent performance across different target dataset scales in the transfer learning scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jufang Duan",
      "Wei Zheng",
      "Yangzhou Du",
      "Wenfa Wu",
      "Haipeng Jiang",
      "Hongsheng Qi"
    ]
  },
  "https://proceedings.mlr.press/v235/duarte24a.html": {
    "title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data",
    "volume": "main",
    "abstract": "How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content is included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP outperforms the prior best method by 8.6% in detection accuracy (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give approximately 0% accuracy. The code and datasets are available at https://github.com/LeiLiLab/DE-COP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "André Vicente Duarte",
      "Xuandong Zhao",
      "Arlindo L. Oliveira",
      "Lei Li"
    ]
  },
  "https://proceedings.mlr.press/v235/dubrovsky24a.html": {
    "title": "Unveiling the Potential of AI for Nanomaterial Morphology Prediction",
    "volume": "main",
    "abstract": "Creation of nanomaterials with specific morphology remains a complex experimental process, even though there is a growing demand for these materials in various industry sectors. This study explores the potential of AI to predict the morphology of nanoparticles within the data availability constraints. For that, we first generated a new multi-modal dataset that is double the size of analogous studies. Then, we systematically evaluated performance of classical machine learning and large language models in prediction of nanomaterial shapes and sizes. Finally, we prototyped a text-to-image system, discussed the obtained empirical results, as well as the limitations and promises of existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Dubrovsky",
      "Andrei Dmitrenko",
      "Aleksei Dmitrenko",
      "Nikita Serov",
      "Vladimir Vinogradov"
    ]
  },
  "https://proceedings.mlr.press/v235/duetting24a.html": {
    "title": "Consistent Submodular Maximization",
    "volume": "main",
    "abstract": "Maximizing monotone submodular functions under cardinality constraints is a classic optimization task with several applications in data mining and machine learning. In this paper, we study this problem in a dynamic environment with consistency constraints: elements arrive in a streaming fashion, and the goal is maintaining a constant approximation to the optimal solution while having a stable solution (i.e., the number of changes between two consecutive solutions is bounded). In this setting, we provide algorithms with different trade-offs between consistency and approximation quality. We also complement our theoretical results with an experimental analysis showing the effectiveness of our algorithms in real-world instances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Duetting",
      "Federico Fusco",
      "Silvio Lattanzi",
      "Ashkan Norouzi-Fard",
      "Morteza Zadimoghaddam"
    ]
  },
  "https://proceedings.mlr.press/v235/dunefsky24a.html": {
    "title": "Observable Propagation: Uncovering Feature Vectors in Transformers",
    "volume": "main",
    "abstract": "A key goal of current mechanistic interpretability research in NLP is to find linear features (also called \"feature vectors\") for transformers: directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data – both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called \"observable propagation\" (in short: ObProp), for finding linear features used by transformer language models in computing a given task – using almost no data. Our paradigm centers on the concept of \"observables\", linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors, including a similarity metric between feature vectors called the coupling coefficient which estimates the degree to which one feature's output correlates with another's. We use ObProp to perform extensive qualitative investigations into several tasks, including gendered occupational bias, political party prediction, and programming language detection. Our results suggest that ObProp surpasses traditional approaches for finding feature vectors in the low-data regime, and that ObProp can be used to better understand the mechanisms responsible for bias in large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Dunefsky",
      "Arman Cohan"
    ]
  },
  "https://proceedings.mlr.press/v235/dung24a.html": {
    "title": "Sharpness-Aware Data Generation for Zero-shot Quantization",
    "volume": "main",
    "abstract": "Zero-shot quantization aims to learn a quantized model from a pre-trained full-precision model with no access to original real training data. The common idea in zero-shot quantization approaches is to generate synthetic data for quantizing the full-precision model. While it is well-known that deep neural networks with low sharpness have better generalization ability, none of the previous zero-shot quantization works considers the sharpness of the quantized model as a criterion for generating training data. This paper introduces a novel methodology that takes into account quantized model sharpness in synthetic data generation to enhance generalization. Specifically, we first demonstrate that sharpness minimization can be attained by maximizing gradient matching between the reconstruction loss gradients computed on synthetic and real validation data, under certain assumptions. We then circumvent the problem of the gradient matching without real validation set by approximating it with the gradient matching between each generated sample and its neighbors. Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the superiority of the proposed method over the state-of-the-art techniques in low-bit quantization settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Anh Dung",
      "Cuong Pham",
      "Trung Le",
      "Jianfei Cai",
      "Thanh-Toan Do"
    ]
  },
  "https://proceedings.mlr.press/v235/dupre-la-tour24a.html": {
    "title": "Making Old Things New: A Unified Algorithm for Differentially Private Clustering",
    "volume": "main",
    "abstract": "As a staple of data analysis and unsupervised learning, the problem of private clustering has been widely studied, under various privacy models. Centralized differential privacy is the first of them, and the problem has also been studied for the local and the shuffle variation. In each case, the goal is to design an algorithm that computes privately a clustering, with the smallest possible error. The study of each variation gave rise to new algorithm: the landscape of private clustering algorithm is therefore quite intricate. In this paper, we show that a 20 year-old algorithm can be slightly modified to work for any of those models. This provides a unified picture: while matching almost all previously known results, it allows us to improve some of them, and extend to a new privacy model, the continual observation setting, where the input is changing over time and the algorithm must output a new solution at each time step",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Dupre La Tour",
      "Monika Henzinger",
      "David Saulpic"
    ]
  },
  "https://proceedings.mlr.press/v235/dupuis24a.html": {
    "title": "Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation",
    "volume": "main",
    "abstract": "Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to prior art. Our results further identify a phase transition phenomenon, which suggests that heavy tails can be either beneficial or harmful depending on the problem structure. We support our theory with experiments conducted in a variety of settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Dupuis",
      "Umut Simsekli"
    ]
  },
  "https://proceedings.mlr.press/v235/duran-martin24a.html": {
    "title": "Outlier-robust Kalman Filtering through Generalised Bayes",
    "volume": "main",
    "abstract": "We derive a novel, provably robust, efficient, and closed-form Bayesian update rule for online filtering in state-space models in the presence of outliers and misspecified measurement models. Our method combines generalised Bayesian inference with filtering methods such as the extended and ensemble Kalman filter. We use the former to show robustness and the latter to ensure computational efficiency in the case of nonlinear models. Our method matches or outperforms other robust filtering methods (such as those based on variational Bayes) at a much lower computational cost. We show this empirically on a range of filtering problems with outlier measurements, such as object tracking, state estimation in high-dimensional chaotic systems, and online learning of neural networks",
    "checked": true,
    "id": "a15056c2ff1aaec5f7dc4dad4394a7ad767ca04b",
    "semantic_title": "outlier-robust kalman filtering through generalised bayes",
    "citation_count": 6,
    "authors": [
      "Gerardo Duran-Martin",
      "Matias Altamirano",
      "Alex Shestopaloff",
      "Leandro Sánchez-Betancourt",
      "Jeremias Knoblauch",
      "Matt Jones",
      "Francois-Xavier Briol",
      "Kevin Patrick Murphy"
    ]
  },
  "https://proceedings.mlr.press/v235/durasov24a.html": {
    "title": "Enabling Uncertainty Estimation in Iterative Neural Networks",
    "volume": "main",
    "abstract": "Turning pass-through network architectures into iterative ones, which use their own output as input, is a well-known approach for boosting performance. In this paper, we argue that such architectures offer an additional benefit: The convergence rate of their successive outputs is highly correlated with the accuracy of the value to which they converge. Thus, we can use the convergence rate as a useful proxy for uncertainty. This results in an approach to uncertainty estimation that provides state-of-the-art estimates at a much lower computational cost than techniques like Ensembles, and without requiring any modifications to the original iterative model. We demonstrate its practical value by embedding it in two application domains: road detection in aerial images and the estimation of aerodynamic properties of 2D and 3D shapes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Durasov",
      "Doruk Oner",
      "Jonathan Donier",
      "Hieu Le",
      "Pascal Fua"
    ]
  },
  "https://proceedings.mlr.press/v235/dvurechensky24a.html": {
    "title": "Barrier Algorithms for Constrained Non-Convex Optimization",
    "volume": "main",
    "abstract": "In this paper we theoretically show that interior-point methods based on self-concordant barriers possess favorable global complexity beyond their standard application area of convex optimization. To do that we propose first- and second-order methods for non-convex optimization problems with general convex set constraints and linear constraints. Our methods attain a suitably defined class of approximate first- or second-order KKT points with the worst-case iteration complexity similar to unconstrained problems, namely $O(\\varepsilon^{-2})$ (first-order) and $O(\\varepsilon^{-3/2})$ (second-order), respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Dvurechensky",
      "Mathias Staudigl"
    ]
  },
  "https://proceedings.mlr.press/v235/dwaracherla24a.html": {
    "title": "Efficient Exploration for LLMs",
    "volume": "main",
    "abstract": "We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vikranth Dwaracherla",
      "Seyed Mohammad Asghari",
      "Botao Hao",
      "Benjamin Van Roy"
    ]
  },
  "https://proceedings.mlr.press/v235/dym24a.html": {
    "title": "Equivariant Frames and the Impossibility of Continuous Canonicalization",
    "volume": "main",
    "abstract": "Canonicalization provides an architecture-agnostic method for enforcing equivariance, with generalizations such as frame-averaging recently gaining prominence as a lightweight and flexible alternative to equivariant architectures. Recent works have found an empirical benefit to using probabilistic frames instead, which learn weighted distributions over group elements. In this work, we provide strong theoretical justification for this phenomenon: for commonly-used groups, there is no efficiently computable choice of frame that preserves continuity of the function being averaged. In other words, unweighted frame-averaging can turn a smooth, non-symmetric function into a discontinuous, symmetric function. To address this fundamental robustness problem, we formally define and construct weighted frames, which provably preserve continuity, and demonstrate their utility by constructing efficient and continuous weighted frames for the actions of $SO(d)$, $O(d)$, and $S_n$ on point clouds",
    "checked": true,
    "id": "77bf137205838e5dc4b3c824b2be2a808421c250",
    "semantic_title": "equivariant frames and the impossibility of continuous canonicalization",
    "citation_count": 11,
    "authors": [
      "Nadav Dym",
      "Hannah Lawrence",
      "Jonathan W. Siegel"
    ]
  },
  "https://proceedings.mlr.press/v235/eckman24a.html": {
    "title": "Position: Insights from Survey Methodology can Improve Training Data",
    "volume": "main",
    "abstract": "Whether future AI models are fair, trustworthy, and aligned with the public's interests rests in part on our ability to collect accurate data about what we want the models to do. However, collecting high-quality data is difficult, and few AI/ML researchers are trained in data collection methods. Recent research in data-centric AI has show that higher quality training data leads to better performing models, making this the right moment to introduce AI/ML researchers to the field of survey methodology, the science of data collection. We summarize insights from the survey methodology literature and discuss how they can improve the quality of training and feedback data. We also suggest collaborative research ideas into how biases in data collection can be mitigated, making models more accurate and human-centric",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephanie Eckman",
      "Barbara Plank",
      "Frauke Kreuter"
    ]
  },
  "https://proceedings.mlr.press/v235/egiazarian24a.html": {
    "title": "Extreme Compression of Large Language Models via Additive Quantization",
    "volume": "main",
    "abstract": "The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of \"extreme\" LLM compression—defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter—from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint",
    "checked": true,
    "id": "2209dd35db8098b6c80caeda705f75339f141e22",
    "semantic_title": "extreme compression of large language models via additive quantization",
    "citation_count": 47,
    "authors": [
      "Vage Egiazarian",
      "Andrei Panferov",
      "Denis Kuznedelev",
      "Elias Frantar",
      "Artem Babenko",
      "Dan Alistarh"
    ]
  },
  "https://proceedings.mlr.press/v235/egorov24a.html": {
    "title": "Ai-sampler: Adversarial Learning of Markov kernels with involutive maps",
    "volume": "main",
    "abstract": "Markov chain Monte Carlo methods have become popular in statistics as versatile techniques to sample from complicated probability distributions. In this work, we propose a method to parameterize and train transition kernels of Markov chains to achieve efficient sampling and good mixing. This training procedure minimizes the total variation distance between the stationary distribution of the chain and the empirical distribution of the data. Our approach leverages involutive Metropolis-Hastings kernels constructed from reversible neural networks that ensure detailed balance by construction. We find that reversibility also implies $C_2$-equivariance of the discriminator function which can be used to restrict its function space",
    "checked": true,
    "id": "bb2c2b241f3606c0a708fc8e220c1815e4d59186",
    "semantic_title": "ai-sampler: adversarial learning of markov kernels with involutive maps",
    "citation_count": 1,
    "authors": [
      "Evgenii Egorov",
      "Riccardo Valperga",
      "Stratis Gavves"
    ]
  },
  "https://proceedings.mlr.press/v235/eiras24a.html": {
    "title": "Efficient Error Certification for Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "Recent work provides promising evidence that Physics-Informed Neural Networks (PINN) can efficiently solve partial differential equations (PDE). However, previous works have failed to provide guarantees on the worst-case residual error of a PINN across the spatio-temporal domain - a measure akin to the tolerance of numerical solvers - focusing instead on point-wise comparisons between their solution and the ones obtained by a solver on a set of inputs. In real-world applications, one cannot consider tests on a finite set of points to be sufficient grounds for deployment, as the performance could be substantially worse on a different set. To alleviate this issue, we establish guaranteed error-based conditions for PINNs over their continuous applicability domain. To verify the extent to which they hold, we introduce $\\partial$-CROWN: a general, efficient and scalable post-training framework to bound PINN residual errors. We demonstrate its effectiveness in obtaining tight certificates by applying it to two classically studied PINNs – Burgers' and Schrödinger's equations –, and two more challenging ones with real-world applications – the Allan-Cahn and Diffusion-Sorption equations",
    "checked": true,
    "id": "165933ebf6156bebaabf733555a4f0e434ccf2cb",
    "semantic_title": "efficient error certification for physics-informed neural networks",
    "citation_count": 2,
    "authors": [
      "Francisco Eiras",
      "Adel Bibi",
      "Rudy R Bunel",
      "Krishnamurthy Dj Dvijotham",
      "Philip Torr",
      "M. Pawan Kumar"
    ]
  },
  "https://proceedings.mlr.press/v235/eiras24b.html": {
    "title": "Position: Near to Mid-term Risks and Opportunities of Open-Source Generative AI",
    "volume": "main",
    "abstract": "In the next few years, applications of Generative AI are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about potential risks and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. While regulation is important, it is key that it does not put at risk the budding field of open-source Generative AI. We argue for the responsible open sourcing of generative AI models in the near and medium term. To set the stage, we first introduce an AI openness taxonomy system and apply it to 40 current large language models. We then outline differential benefits and risks of open versus closed source AI and present potential risk mitigation, ranging from best practices to calls for technical and scientific contributions. We hope that this report will add a much needed missing voice to the current public discourse on near to mid-term AI safety and other societal impact",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisco Eiras",
      "Aleksandar Petrov",
      "Bertie Vidgen",
      "Christian Schroeder De Witt",
      "Fabio Pizzati",
      "Katherine Elkins",
      "Supratik Mukhopadhyay",
      "Adel Bibi",
      "Botos Csaba",
      "Fabro Steibel",
      "Fazl Barez",
      "Genevieve Smith",
      "Gianluca Guadagni",
      "Jon Chun",
      "Jordi Cabot",
      "Joseph Marvin Imperial",
      "Juan A. Nolazco-Flores",
      "Lori Landay",
      "Matthew Thomas Jackson",
      "Paul Rottger",
      "Philip Torr",
      "Trevor Darrell",
      "Yong Suk Lee",
      "Jakob Nicolaus Foerster"
    ]
  },
  "https://proceedings.mlr.press/v235/el-nouby24a.html": {
    "title": "Scalable Pre-training of Large Autoregressive Image Models",
    "volume": "main",
    "abstract": "This paper introduces AIM, a collection of vision models pre-trained with an autoregressive objective. These models are inspired by their textual counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling properties. Specifically, we highlight two key findings: (1) the performance of the visual features scale with both the model capacity and the quantity of data, (2) the value of the objective function correlates with the performance of the model on downstream tasks. We illustrate the practical implication of these findings by pre-training a 7 billion parameter AIM on 2 billion images, that achieves 84.0% on ImageNet-1k with a frozen trunk. Interestingly, even at this scale, we observe no sign of saturation in performance, suggesting that AIM potentially represents a new frontier for training large-scale vision models. The pre-training of AIM is similar to the pre-training of LLMs, and does not require any image-specific strategy to stabilize the training at scale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alaaeldin El-Nouby",
      "Michal Klein",
      "Shuangfei Zhai",
      "Miguel Ángel Bautista",
      "Vaishaal Shankar",
      "Alexander T Toshev",
      "Joshua M. Susskind",
      "Armand Joulin"
    ]
  },
  "https://proceedings.mlr.press/v235/elahi24a.html": {
    "title": "Adaptive Online Experimental Design for Causal Discovery",
    "volume": "main",
    "abstract": "Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on interventional data efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Qasim Elahi",
      "Lai Wei",
      "Murat Kocaoglu",
      "Mahsa Ghasemi"
    ]
  },
  "https://proceedings.mlr.press/v235/eldele24a.html": {
    "title": "TSLANet: Rethinking Transformers for Time Series Representation Learning",
    "volume": "main",
    "abstract": "Time series data, characterized by its intrinsic long and short-range dependencies, poses a unique challenge across analytical applications. While Transformer-based models excel at capturing long-range dependencies, they face limitations in noise sensitivity, computational efficiency, and overfitting with smaller datasets. In response, we introduce a novel Time Series Lightweight Adaptive Network (TSLANet), as a universal convolutional model for diverse time series tasks. Specifically, we propose an Adaptive Spectral Block, harnessing Fourier analysis to enhance feature representation and to capture both long-term and short-term interactions while mitigating noise via adaptive thresholding. Additionally, we introduce an Interactive Convolution Block and leverage self-supervised learning to refine the capacity of TSLANet for decoding complex temporal patterns and improve its robustness on different datasets. Our comprehensive experiments demonstrate that TSLANet outperforms state-of-the-art models in various tasks spanning classification, forecasting, and anomaly detection, showcasing its resilience and adaptability across a spectrum of noise levels and data sizes. The code is available at https://github.com/emadeldeen24/TSLANet",
    "checked": true,
    "id": "fe0d1bea9a852b35261ef7b9975429d2e61a2485",
    "semantic_title": "tslanet: rethinking transformers for time series representation learning",
    "citation_count": 10,
    "authors": [
      "Emadeldeen Eldele",
      "Mohamed Ragab",
      "Zhenghua Chen",
      "Min Wu",
      "Xiaoli Li"
    ]
  },
  "https://proceedings.mlr.press/v235/elhamod24a.html": {
    "title": "Neuro-Visualizer: A Novel Auto-Encoder-Based Loss Landscape Visualization Method With an Application in Knowledge-Guided Machine Learning",
    "volume": "main",
    "abstract": "In recent years, there has been a growing interest in visualizing the loss landscape of neural networks. Linear landscape visualization methods, such as principal component analysis, have become widely used as they intuitively help researchers study neural networks and their training process. However, these linear methods suffer from limitations and drawbacks due to their lack of flexibility and low fidelity at representing the high dimensional landscape. In this paper, we present a novel auto-encoder-based non-linear landscape visualization method called Neuro-Visualizer that addresses these shortcoming and provides useful insights about neural network loss landscapes. To demonstrate its potential, we run experiments on a variety of problems in two separate applications of knowledge-guided machine learning (KGML). Our findings show that Neuro-Visualizer outperforms other linear and non-linear baselines and helps corroborate, and sometime challenge, claims proposed by machine learning community. All code and data used in the experiments of this paper can be found at the link below",
    "checked": true,
    "id": "63bbef639e0a868e1ec38ddd45a373b5519b058d",
    "semantic_title": "neuro-visualizer: a novel auto-encoder-based loss landscape visualization method with an application in knowledge-guided machine learning",
    "citation_count": 0,
    "authors": [
      "Mohannad Elhamod",
      "Anuj Karpatne"
    ]
  },
  "https://proceedings.mlr.press/v235/elsayed24a.html": {
    "title": "Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning",
    "volume": "main",
    "abstract": "Second-order information is valuable for many applications but challenging to compute. Several works focus on computing or approximating Hessian diagonals, but even this simplification introduces significant additional costs compared to computing a gradient. In the absence of efficient exact computation schemes for Hessian diagonals, we revisit an early approximation scheme proposed by Becker and LeCun (1989, BL89), which has a cost similar to gradients and appears to have been overlooked by the community. We introduce HesScale, an improvement over BL89, which adds negligible extra computation. On small networks, we find that this improvement is of higher quality than all alternatives, even those with theoretical guarantees, such as unbiasedness, while being much cheaper to compute. We use this insight in reinforcement learning problems where small networks are used and demonstrate HesScale in second-order optimization and scaling the step-size parameter. In our experiments, HesScale optimizes faster than existing methods and improves stability through step-size scaling. These findings are promising for scaling second-order methods in larger models in the future",
    "checked": true,
    "id": "36c8ca6fce0c299ca128faaaa8c4386cd254a434",
    "semantic_title": "revisiting scalable hessian diagonal approximations for applications in reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Mohamed Elsayed",
      "Homayoon Farrahi",
      "Felix Dangel",
      "A. Rupam Mahmood"
    ]
  },
  "https://proceedings.mlr.press/v235/engels24a.html": {
    "title": "Approximate Nearest Neighbor Search with Window Filters",
    "volume": "main",
    "abstract": "We define and investigate the problem of c-approximate window search: approximate nearest neighbor search where each point in the dataset has a numeric label, and the goal is to find nearest neighbors to queries within arbitrary label ranges. Many semantic search problems, such as image and document search with timestamp filters, or product search with cost filters, are natural examples of this problem. We propose and theoretically analyze a modular tree-based framework for transforming an index that solves the traditional c-approximate nearest neighbor problem into a data structure that solves window search. On standard nearest neighbor benchmark datasets equipped with random label values, adversarially constructed embeddings, and image search embeddings with real timestamps, we obtain up to a $75\\times$ speedup over existing solutions at the same level of recall",
    "checked": true,
    "id": "d0a904e8a0dea199338396fa652da1b73cd7bcf5",
    "semantic_title": "approximate nearest neighbor search with window filters",
    "citation_count": 1,
    "authors": [
      "Joshua Engels",
      "Ben Landrum",
      "Shangdi Yu",
      "Laxman Dhulipala",
      "Julian Shun"
    ]
  },
  "https://proceedings.mlr.press/v235/engstrom24a.html": {
    "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
    "volume": "main",
    "abstract": "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with \"high quality\" data sources may not increase (and can even hurt) performance compared to randomly selecting data. To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods",
    "checked": true,
    "id": "d6a546b23c3b67aa2dc2d03ef3e7692f031ebd2d",
    "semantic_title": "dsdm: model-aware dataset selection with datamodels",
    "citation_count": 28,
    "authors": [
      "Logan Engstrom",
      "Axel Feldmann",
      "Aleksander Madry"
    ]
  },
  "https://proceedings.mlr.press/v235/entesari24a.html": {
    "title": "Compositional Curvature Bounds for Deep Neural Networks",
    "volume": "main",
    "abstract": "A key challenge that threatens the widespread use of neural networks in safety-critical applications is their vulnerability to adversarial attacks. In this paper, we study the second-order behavior of continuously differentiable deep neural networks, focusing on robustness against adversarial perturbations. First, we provide a theoretical analysis of robustness and attack certificates for deep classifiers by leveraging local gradients and upper bounds on the second derivative (curvature constant). Next, we introduce a novel algorithm to analytically compute provable upper bounds on the second derivative of neural networks. This algorithm leverages the compositional structure of the model to propagate the curvature bound layer-by-layer, giving rise to a scalable and modular approach. The proposed bound can serve as a differentiable regularizer to control the curvature of neural networks during training, thereby enhancing robustness. Finally, we demonstrate the efficacy of our method on classification tasks using the MNIST and CIFAR-10 datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taha Entesari",
      "Sina Sharifi",
      "Mahyar Fazlyab"
    ]
  },
  "https://proceedings.mlr.press/v235/epstein24a.html": {
    "title": "Disentangled 3D Scene Generation with Layout Learning",
    "volume": "main",
    "abstract": "We introduce a method to generate 3D scenes that are disentangled into their component objects. This disentanglement is unsupervised, relying only on the knowledge of a large pretrained text-to-image model. Our key insight is that objects can be discovered by finding parts of a 3D scene that, when rearranged spatially, still produce valid configurations of the same scene. Concretely, our method jointly optimizes multiple NeRFs—each representing its own object—along with a set of layouts that composite these objects into scenes. We then encourage these composited scenes to be in-distribution according to the image generator. We show that despite its simplicity, our approach successfully generates 3D scenes decomposed into individual objects, enabling new capabilities in text-to-3D content creation",
    "checked": true,
    "id": "66dcc6ec983fe0447b440c44a3f631bea418ba3d",
    "semantic_title": "disentangled 3d scene generation with layout learning",
    "citation_count": 13,
    "authors": [
      "Dave Epstein",
      "Ben Poole",
      "Ben Mildenhall",
      "Alexei A Efros",
      "Aleksander Holynski"
    ]
  },
  "https://proceedings.mlr.press/v235/eringis24a.html": {
    "title": "PAC-Bayesian Error Bound, via Rényi Divergence, for a Class of Linear Time-Invariant State-Space Models",
    "volume": "main",
    "abstract": "In this paper we derive a PAC-Bayesian error bound for a class of stochastic dynamical systems with inputs, namely, for linear time-invariant stochastic state-space models (stochastic LTI systems for short). This class of systems is widely used in control engineering and econometrics, in particular, they represent a special case of recurrent neural networks. In this paper we 1) formalize the learning problem for stochastic LTI systems with inputs, 2) derive a PAC-Bayesian error bound for such systems, and 3) discuss various consequences of this error bound",
    "checked": true,
    "id": "8b0ff5869e215f25cb3f1d7f2a3717c438bc4dc5",
    "semantic_title": "pac-bayesian error bound, via rényi divergence, for a class of linear time-invariant state-space models",
    "citation_count": 0,
    "authors": [
      "Deividas Eringis",
      "John Leth",
      "Zheng-Hua Tan",
      "Rafal Wisniewski",
      "Mihaly Petreczky"
    ]
  },
  "https://proceedings.mlr.press/v235/esfandiari24a.html": {
    "title": "High-Dimensional Geometric Streaming for Nearly Low Rank Data",
    "volume": "main",
    "abstract": "We study streaming algorithms for the $\\ell_p$ subspace approximation problem. Given points $a_1, \\ldots, a_n$ as an insertion-only stream and a rank parameter $k$, the $\\ell_p$ subspace approximation problem is to find a $k$-dimensional subspace $V$ such that $(\\sum_{i=1}^n d(a_i, V)^p)^{1/p}$ is minimized, where $d(a, V)$ denotes the Euclidean distance between $a$ and $V$ defined as $\\min_{v \\in V} ||a - v||$. When $p = \\infty$, we need to find a subspace $V$ that minimizes $\\max_i d(a_i, V)$. For $\\ell_{\\infty}$ subspace approximation, we give a deterministic strong coreset construction algorithm and show that it can be used to compute a $\\mathrm{poly}(k, \\log n)$ approximate solution. We show that the distortion obtained by our coreset is nearly tight for any sublinear space algorithm. For $\\ell_p$ subspace approximation, we show that suitably scaling the points and then using our $\\ell_{\\infty}$ coreset construction, we can compute a $\\mathrm{poly}(k, \\log n)$ approximation. Our algorithms are easy to implement and run very fast on large datasets. We also use our strong coreset construction to improve the results in a recent work of Woodruff and Yasuda (FOCS 2022) which gives streaming algorithms for high-dimensional geometric problems such as width estimation, convex hull estimation, and volume estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Esfandiari",
      "Praneeth Kacham",
      "Vahab Mirrokni",
      "David Woodruff",
      "Peilin Zhong"
    ]
  },
  "https://proceedings.mlr.press/v235/esser24a.html": {
    "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
    "volume": "main",
    "abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models. Stability AI is considering making experimental data, code, and model weights publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Esser",
      "Sumith Kulal",
      "Andreas Blattmann",
      "Rahim Entezari",
      "Jonas Müller",
      "Harry Saini",
      "Yam Levi",
      "Dominik Lorenz",
      "Axel Sauer",
      "Frederic Boesel",
      "Dustin Podell",
      "Tim Dockhorn",
      "Zion English",
      "Robin Rombach"
    ]
  },
  "https://proceedings.mlr.press/v235/ethayarajh24a.html": {
    "title": "Model Alignment as Prospect Theoretic Optimization",
    "volume": "main",
    "abstract": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases—the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kawin Ethayarajh",
      "Winnie Xu",
      "Niklas Muennighoff",
      "Dan Jurafsky",
      "Douwe Kiela"
    ]
  },
  "https://proceedings.mlr.press/v235/evans24a.html": {
    "title": "Fast Timing-Conditioned Latent Audio Diffusion",
    "volume": "main",
    "abstract": "Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. It is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. The generative model is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. It is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, the proposed model is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds",
    "checked": true,
    "id": "c281c4d6f9c60b424dcc86ca27807afd3c9cdc94",
    "semantic_title": "fast timing-conditioned latent audio diffusion",
    "citation_count": 58,
    "authors": [
      "Zach Evans",
      "Cj Carr",
      "Josiah Taylor",
      "Scott H. Hawley",
      "Jordi Pons"
    ]
  },
  "https://proceedings.mlr.press/v235/everett24a.html": {
    "title": "Scaling Exponents Across Parameterizations and Optimizers",
    "volume": "main",
    "abstract": "Robust and effective scaling of models from small to large width typically requires the precise adjustment of many algorithmic and architectural details, such as parameterization and optimizer choices. In this work, we propose a new perspective on parameterization by investigating a key assumption in prior work about the alignment between parameters and data and derive new theoretical results under weaker assumptions and a broader set of optimizers. Our extensive empirical investigation includes tens of thousands of models trained with all combinations of three optimizers, four parameterizations, several alignment assumptions, more than a dozen learning rates, and fourteen model sizes up to 27B parameters. We find that the best learning rate scaling prescription would often have been excluded by the assumptions in prior work. Our results show that all parameterizations, not just maximal update parameterization (muP), can achieve hyperparameter transfer; moreover, our novel per-layer learning rate prescription for standard parameterization outperforms muP. Finally, we demonstrate that an overlooked aspect of parameterization, the epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow and propose Adam-atan2, a new numerically stable, scale-invariant version of Adam that eliminates the epsilon hyperparameter entirely",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katie E Everett",
      "Lechao Xiao",
      "Mitchell Wortsman",
      "Alexander A Alemi",
      "Roman Novak",
      "Peter J Liu",
      "Izzeddin Gur",
      "Jascha Sohl-Dickstein",
      "Leslie Pack Kaelbling",
      "Jaehoon Lee",
      "Jeffrey Pennington"
    ]
  },
  "https://proceedings.mlr.press/v235/eyre24a.html": {
    "title": "Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift",
    "volume": "main",
    "abstract": "Designing deep neural network classifiers that perform robustly on distributions differing from the available training data is an active area of machine learning research. However, out-of-distribution generalization for regression—the analogous problem for modeling continuous targets—remains relatively unexplored. To tackle this problem, we return to first principles and analyze how the closed-form solution for Ordinary Least Squares (OLS) regression is sensitive to covariate shift. We characterize the out-of-distribution risk of the OLS model in terms of the eigenspectrum decomposition of the source and target data. We then use this insight to propose a method called Spectral Adapted Regressor (SpAR) for adapting the weights of the last layer of a pre-trained neural regression model to perform better on input data originating from a different distribution. We demonstrate how this lightweight spectral adaptation procedure can improve out-of-distribution performance for synthetic and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Eyre",
      "Elliot Creager",
      "David Madras",
      "Vardan Papyan",
      "Richard Zemel"
    ]
  },
  "https://proceedings.mlr.press/v235/fabian24a.html": {
    "title": "Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models",
    "volume": "main",
    "abstract": "Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the ground truth signal structure, the severity of the degradation and the complex interactions between the above. This results in natural sample-by-sample variation in the difficulty of a reconstruction problem. Our key observation is that most existing inverse problem solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in subpar performance and wasteful resource allocation. We propose a novel method, severity encoding, to estimate the degradation severity of corrupted signals in the latent space of an autoencoder. We show that the estimated severity has strong correlation with the true corruption level and can provide useful hints on the difficulty of reconstruction problems on a sample-by-sample basis. Furthermore, we propose a reconstruction method based on latent diffusion models that leverages the predicted degradation severities to fine-tune the reverse diffusion sampling trajectory and thus achieve sample-adaptive inference times. Our framework, Flash-Diffusion, acts as a wrapper that can be combined with any latent diffusion-based baseline solver, imbuing it with sample-adaptivity and acceleration. We perform experiments on both linear and nonlinear inverse problems and demonstrate that our technique greatly improves the performance of the baseline solver and achieves up to $10\\times$ acceleration in mean sampling speed",
    "checked": true,
    "id": "b13c89699a98d6dacf8d46a8fc635388e00da618",
    "semantic_title": "adapt and diffuse: sample-adaptive reconstruction via latent diffusion models",
    "citation_count": 3,
    "authors": [
      "Zalan Fabian",
      "Berk Tinaz",
      "Mahdi Soltanolkotabi"
    ]
  },
  "https://proceedings.mlr.press/v235/fabian24b.html": {
    "title": "DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency",
    "volume": "main",
    "abstract": "Diffusion models have established new state of the art in a multitude of computer vision tasks, including image restoration. Diffusion-based inverse problem solvers generate reconstructions of exceptional visual quality from heavily corrupted measurements. However, in what is widely known as the perception-distortion trade-off, the price of perceptually appealing reconstructions is often paid in declined distortion metrics, such as PSNR. Distortion metrics measure faithfulness to the observation, a crucial requirement in inverse problems. In this work, we propose a novel framework for inverse problem solving, namely we assume that the observation comes from a stochastic degradation process that gradually degrades and noises the original clean image. We learn to reverse the degradation process in order to recover the clean image. Our technique maintains consistency with the original measurement throughout the reverse process, and allows for great flexibility in trading off perceptual quality for improved distortion metrics and sampling speedup via early-stopping. We demonstrate the efficiency of our method on different high-resolution datasets and inverse problems, achieving great improvements over other state-of-the-art diffusion-based methods with respect to both perceptual and distortion metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zalan Fabian",
      "Berk Tinaz",
      "Mahdi Soltanolkotabi"
    ]
  },
  "https://proceedings.mlr.press/v235/falck24a.html": {
    "title": "Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective",
    "volume": "main",
    "abstract": "In-context learning (ICL) has emerged as a particularly remarkable characteristic of Large Language Models (LLM): given a pretrained LLM and an observed dataset, LLMs can make predictions for new data points from the same distribution without fine-tuning. Numerous works have postulated ICL as approximately Bayesian inference, rendering this a natural hypothesis. In this work, we analyse this hypothesis from a new angle through the martingale property, a fundamental requirement of a Bayesian learning system for exchangeable data. We show that the martingale property is a necessary condition for unambiguous predictions in such scenarios, and enables a principled, decomposed notion of uncertainty vital in trustworthy, safety-critical systems. We derive actionable checks with corresponding theory and test statistics which must hold if the martingale property is satisfied. We also examine if uncertainty in LLMs decreases as expected in Bayesian learning when more data is observed. In three experiments, we provide evidence for violations of the martingale property, and deviations from a Bayesian scaling behaviour of uncertainty, falsifying the hypothesis that ICL is Bayesian",
    "checked": true,
    "id": "8d49a00ac9829942e9b4dd2cf7683490a9161a87",
    "semantic_title": "is in-context learning in large language models bayesian? a martingale perspective",
    "citation_count": 8,
    "authors": [
      "Fabian Falck",
      "Ziyu Wang",
      "Christopher C. Holmes"
    ]
  },
  "https://proceedings.mlr.press/v235/fan24a.html": {
    "title": "On the Recoverability of Causal Relations from Temporally Aggregated I.I.D. Data",
    "volume": "main",
    "abstract": "We consider the effect of temporal aggregation on instantaneous (non-temporal) causal discovery in general setting. This is motivated by the observation that the true causal time lag is often considerably shorter than the observational interval. This discrepancy leads to high aggregation, causing time-delay causality to vanish and instantaneous dependence to manifest. Although we expect such instantaneous dependence has consistency with the true causal relation in certain sense to make the discovery results meaningful, it remains unclear what type of consistency we need and when will such consistency be satisfied. We proposed functional consistency and conditional independence consistency in formal way correspond functional causal model-based methods and conditional independence-based methods respectively and provide the conditions under which these consistencies will hold. We show theoretically and experimentally that causal discovery results may be seriously distorted by aggregation especially in complete nonlinear case and we also find causal relationship still recoverable from aggregated data if we have partial linearity or appropriate prior. Our findings suggest community should take a cautious and meticulous approach when interpreting causal discovery results from such data and show why and when aggregation will distort the performance of causal discovery methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunxing Fan",
      "Mingming Gong",
      "Kun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/fan24b.html": {
    "title": "On the Convergence of Projected Bures-Wasserstein Gradient Descent under Euclidean Strong Convexity",
    "volume": "main",
    "abstract": "The Bures-Wasserstein (BW) gradient descent method has gained considerable attention in various domains, including Gaussian barycenter, matrix recovery and variational inference problems, due to its alignment with the Wasserstein geometry of normal distributions. Despite its popularity, existing convergence analysis are often contingent upon specific loss functions, and the exploration of constrained settings within this framework remains limited. In this work, we make an attempt to bridge this gap by providing a general convergence rate guarantee for BW gradient descent when the Euclidean strong convexity of the loss and the constraints is assumed. In an effort to advance practical implementations, we also derive a closed-form solution for the projection onto BW distance-constrained sets, which enables the fast implementation of projected BW gradient descent for problems that arise in the constrained barycenter and distributionally robust optimization literature. Experimental results demonstrate significant improvements in computational efficiency and convergence speed, underscoring the efficacy of our method in practical scenarios",
    "checked": true,
    "id": "45022fb882387dd17ef40b0dc990eae2a63d9657",
    "semantic_title": "on the convergence of projected bures-wasserstein gradient descent under euclidean strong convexity",
    "citation_count": 0,
    "authors": [
      "Junyi Fan",
      "Yuxuan Han",
      "Zijian Liu",
      "Jian-Feng Cai",
      "Yang Wang",
      "Zhengyuan Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/fan24c.html": {
    "title": "Locally Estimated Global Perturbations are Better than Local Perturbations for Federated Sharpness-aware Minimization",
    "volume": "main",
    "abstract": "In federated learning (FL), the multi-step update and data heterogeneity among clients often lead to a loss landscape with sharper minima, degenerating the performance of the resulted global model. Prevalent federated approaches incorporate sharpness-aware minimization (SAM) into local training to mitigate this problem. However, the local loss landscapes may not accurately reflect the flatness of global loss landscape in heterogeneous environments; as a result, minimizing local sharpness and calculating perturbations on client data might not align the efficacy of SAM in FL with centralized training. To overcome this challenge, we propose FedLESAM, a novel algorithm that locally estimates the direction of global perturbation on client side as the difference between global models received in the previous active and current rounds. Besides the improved quality, FedLESAM also speed up federated SAM-based approaches since it only performs once backpropagation in each iteration. Theoretically, we prove a slightly tighter bound than its original FedSAM by ensuring consistent perturbation. Empirically, we conduct comprehensive experiments on four federated benchmark datasets under three partition strategies to demonstrate the superior performance and efficiency of FedLESAM",
    "checked": true,
    "id": "627fcbbc030c7dd7c89457e436741ec79c5025f7",
    "semantic_title": "locally estimated global perturbations are better than local perturbations for federated sharpness-aware minimization",
    "citation_count": 8,
    "authors": [
      "Ziqing Fan",
      "Shengchao Hu",
      "Jiangchao Yao",
      "Gang Niu",
      "Ya Zhang",
      "Masashi Sugiyama",
      "Yanfeng Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/fan24d.html": {
    "title": "Revisit the Essence of Distilling Knowledge through Calibration",
    "volume": "main",
    "abstract": "Knowledge Distillation (KD) has evolved into a practical technology for transferring knowledge from a well-performing model (teacher) to a weak model (student). A counter-intuitive phenomenon known as capacity mismatch has been identified, wherein KD performance may not be good when a better teacher instructs the student. Various preliminary methods have been proposed to alleviate capacity mismatch, but a unifying explanation for its cause remains lacking. In this paper, we propose a unifying analytical framework to pinpoint the core of capacity mismatch based on calibration. Through extensive analytical experiments, we observe a positive correlation between the calibration of the teacher model and the KD performance with original KD methods. As this correlation arises due to the sensitivity of metrics (e.g., KL divergence) to calibration, we recommend employing measurements insensitive to calibration such as ranking-based loss. Our experiments demonstrate that ranking-based loss can effectively replace KL divergence, aiding large models with poor calibration to teach better",
    "checked": true,
    "id": "abf3e8830b80b594646a16af3b4a598b305c3252",
    "semantic_title": "revisit the essence of distilling knowledge through calibration",
    "citation_count": 1,
    "authors": [
      "Wen-Shu Fan",
      "Su Lu",
      "Xin-Chun Li",
      "De-Chuan Zhan",
      "Le Gan"
    ]
  },
  "https://proceedings.mlr.press/v235/fan24e.html": {
    "title": "DOGE: Domain Reweighting with Generalization Estimation",
    "volume": "main",
    "abstract": "The coverage and composition of the pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). Despite its importance, recent LLMs still rely on heuristics and trial and error to increase or reduce the influence of data-domains. We propose DOmain reweighting with Generalization Estimation (DoGE), which optimizes the probability of sampling from each domain (domain weights) in a principled way. Our approach is a two stage process consisting (i) training a proxy model to obtain domain weights using a bi-level optimization algorithm; (ii) training a larger base model by sampling training domains according to the learnt domain weights. In our experiments, we extensively show how DoGE improves the generalization of the base model to any target data mixture. On the SlimPajama dataset, our base model gets a better perplexity and few-shot reasoning accuracies across 6 tasks compared to baseline methods. Moreover, aiming to generalize to out-of-domain target tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can effectively identify inter-domain dependencies, consistently achieves better test perplexity on the target domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simin Fan",
      "Matteo Pagliardini",
      "Martin Jaggi"
    ]
  },
  "https://proceedings.mlr.press/v235/fan24f.html": {
    "title": "Path-Guided Particle-based Sampling",
    "volume": "main",
    "abstract": "Particle-based Bayesian inference methods by sampling from a partition-free target (posterior) distribution, e.g., Stein variational gradient descent (SVGD), have attracted significant attention. We propose a path-guided particle-based sampling (PGPS) method based on a novel Log-weighted Shrinkage (LwS) density path linking an initial distribution to the target distribution. We propose to utilize a Neural network to learn a vector field motivated by the Fokker-Planck equation of the designed density path. Particles, initiated from the initial distribution, evolve according to the ordinary differential equation defined by the vector field. The distribution of these particles is guided along a density path from the initial distribution to the target distribution. The proposed LwS density path allows for an efficient search of modes of the target distribution while canonical methods fail. We theoretically analyze the Wasserstein distance of the distribution of the PGPS-generated samples and the target distribution due to approximation and discretization errors. Practically, the proposed PGPS-LwS method demonstrates higher Bayesian inference accuracy and better calibration ability in experiments conducted on both synthetic and real-world Bayesian learning tasks, compared to baselines, such as SVGD and Langevin dynamics, etc",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhou Fan",
      "Ruida Zhou",
      "Chao Tian",
      "Xiaoning Qian"
    ]
  },
  "https://proceedings.mlr.press/v235/fang24a.html": {
    "title": "Bayesian Knowledge Distillation: A Bayesian Perspective of Distillation with Uncertainty Quantification",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) has been widely used for model compression and deployment acceleration. Nonetheless, the statistical insight of the remarkable performance of KD remains elusive, and methods for evaluating the uncertainty of the distilled model/student model are lacking. To address these issues, we establish a close connection between KD and a Bayesian model. In particular, we develop an innovative method named Bayesian Knowledge Distillation (BKD) to provide a transparent interpretation of the working mechanism of KD, and a suite of Bayesian inference tools for the uncertainty quantification of the student model. In BKD, the regularization imposed by the teacher model in KD is formulated as a teacher-informed prior for the student model's parameters. Consequently, we establish the equivalence between minimizing the KD loss and estimating the posterior mode in BKD. Efficient Bayesian inference algorithms are developed based on the stochastic gradient Langevin Monte Carlo and examined with extensive experiments on uncertainty ranking and credible intervals construction for predicted class probabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luyang Fang",
      "Yongkai Chen",
      "Wenxuan Zhong",
      "Ping Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/fang24b.html": {
    "title": "Exploring Correlations of Self-Supervised Tasks for Graphs",
    "volume": "main",
    "abstract": "Graph self-supervised learning has sparked a research surge in training informative representations without accessing any labeled data. However, our understanding of graph self-supervised learning remains limited, and the inherent relationships between various self-supervised tasks are still unexplored. Our paper aims to provide a fresh understanding of graph self-supervised learning based on task correlations. Specifically, we evaluate the performance of the representations trained by one specific task on other tasks and define correlation values to quantify task correlations. Through this process, we unveil the task correlations between various self-supervised tasks and can measure their expressive capabilities, which are closely related to downstream performance. By analyzing the correlation values between tasks across various datasets, we reveal the complexity of task correlations and the limitations of existing multi-task learning methods. To obtain more capable representations, we propose Graph Task Correlation Modeling (GraphTCM) to illustrate the task correlations and utilize it to enhance graph self-supervised training. The experimental results indicate that our method significantly outperforms existing methods across various downstream tasks",
    "checked": true,
    "id": "785e2342858e09ef97739288f11e919c0805ebb3",
    "semantic_title": "exploring correlations of self-supervised tasks for graphs",
    "citation_count": 4,
    "authors": [
      "Taoran Fang",
      "Wei Chow",
      "Yifei Sun",
      "Kaiqiao Han",
      "Lvbin Ma",
      "Yang Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/fang24c.html": {
    "title": "INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer",
    "volume": "main",
    "abstract": "Recently, deep reinforcement learning has shown promising results for learning fast heuristics to solve routing problems. Meanwhile, most of the solvers suffer from generalizing to an unseen distribution or distributions with different scales. To address this issue, we propose a novel architecture, called Invariant Nested View Transformer (INViT), which is designed to enforce a nested design together with invariant views inside the encoders to promote the generalizability of the learned solver. It applies a modified policy gradient algorithm enhanced with data augmentations. We demonstrate that the proposed INViT achieves a dominant generalization performance on both TSP and CVRP problems with various distributions and different problem scales. Our source code and datasets are available in supplementary materials",
    "checked": true,
    "id": "a83438a6fa241241330a8eb134732d7cc08b2b01",
    "semantic_title": "invit: a generalizable routing problem solver with invariant nested view transformer",
    "citation_count": 4,
    "authors": [
      "Han Fang",
      "Zhihao Song",
      "Paul Weng",
      "Yutong Ban"
    ]
  },
  "https://proceedings.mlr.press/v235/fang24d.html": {
    "title": "BayOTIDE: Bayesian Online Multivariate Time Series Imputation with Functional Decomposition",
    "volume": "main",
    "abstract": "In real-world scenarios such as traffic and energy management, we frequently encounter large volumes of time-series data characterized by missing values, noise, and irregular sampling patterns. While numerous imputation methods have been proposed, the majority tend to operate within a local horizon, which involves dividing long sequences into batches of fixed-length segments for model training. This local horizon often leads to the overlooking of global trends and periodic patterns. More importantly, most methods assume the observations are sampled at regular timestamps, and fail to handle complex irregular sampled time series in various applications. Additionally, most existing methods are learned in an offline manner. Thus, it is not suitable for applications with rapidly arriving streaming data. To address these challenges, we propose BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. Our method conceptualizes multivariate time series as the weighted combination of groups of low-rank temporal factors with different patterns. We employ a suite of Gaussian Processes (GPs),each with a unique kernel, as functional priors to model these factors. For computational efficiency, we further convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE), and developing a scalable algorithm for online inference. The proposed method can not only handle imputation over arbitrary timestamps, but also offer uncertainty quantification and interpretability for the downstream application. We evaluate our method on both synthetic and real-world datasets. We release the code at https://github.com/xuangu-fang/BayOTIDE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikai Fang",
      "Qingsong Wen",
      "Yingtao Luo",
      "Shandian Zhe",
      "Liang Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/fang24e.html": {
    "title": "StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation",
    "volume": "main",
    "abstract": "WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics",
    "checked": true,
    "id": "fa83074f19beab66f6e1b1f3e428fdf7730b33af",
    "semantic_title": "stacksight: unveiling webassembly through large language models and neurosymbolic chain-of-thought decompilation",
    "citation_count": 0,
    "authors": [
      "Weike Fang",
      "Zhejian Zhou",
      "Junzhou He",
      "Weihang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/fani-24a.html": {
    "title": "Accelerating Heterogeneous Federated Learning with Closed-form Classifiers",
    "volume": "main",
    "abstract": "Federated Learning (FL) methods often struggle in highly statistically heterogeneous settings. Indeed, non-IID data distributions cause client drift and biased local solutions, particularly pronounced in the final classification layer, negatively impacting convergence speed and accuracy. To address this issue, we introduce Federated Recursive Ridge Regression (Fed3R). Our method fits a Ridge Regression classifier computed in closed form leveraging pre-trained features. Fed3R is immune to statistical heterogeneity and is invariant to the sampling order of the clients. Therefore, it proves particularly effective in cross-device scenarios. Furthermore, it is fast and efficient in terms of communication and computation costs, requiring up to two orders of magnitude fewer resources than the competitors. Finally, we propose to leverage the Fed3R parameters as an initialization for a softmax classifier and subsequently fine-tune the model using any FL algorithm (Fed3R with Fine-Tuning, Fed3R+FT). Our findings also indicate that maintaining a fixed classifier aids in stabilizing the training and learning more discriminative features in cross-device settings. Official website: https://fed-3r.github.io/",
    "checked": true,
    "id": "3f64d61184638e03f7e64c6dc7118d2cc52a581a",
    "semantic_title": "accelerating heterogeneous federated learning with closed-form classifiers",
    "citation_count": 0,
    "authors": [
      "Eros Fanı̀",
      "Raffaello Camoriano",
      "Barbara Caputo",
      "Marco Ciccone"
    ]
  },
  "https://proceedings.mlr.press/v235/farebrother24a.html": {
    "title": "Stop Regressing: Training Value Functions via Classification for Scalable Deep RL",
    "volume": "main",
    "abstract": "Value functions are an essential component in deep reinforcement learning (RL), that are typically trained via mean squared error regression to match bootstrapped target values. However, scaling value-based RL methods to large networks has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We show that training value functions with categorical cross-entropy significantly enhances performance and scalability across various domains, including single-task RL on Atari 2600 games, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that categorical cross-entropy mitigates issues inherent to value-based RL, such as noisy targets and non-stationarity. We argue that shifting to categorical cross-entropy for training value functions can substantially improve the scalability of deep RL at little-to-no cost",
    "checked": true,
    "id": "1d36dd3d333659d72df8eb5666edfd5cde54c84f",
    "semantic_title": "stop regressing: training value functions via classification for scalable deep rl",
    "citation_count": 32,
    "authors": [
      "Jesse Farebrother",
      "Jordi Orbay",
      "Quan Vuong",
      "Adrien Ali Taiga",
      "Yevgen Chebotar",
      "Ted Xiao",
      "Alex Irpan",
      "Sergey Levine",
      "Pablo Samuel Castro",
      "Aleksandra Faust",
      "Aviral Kumar",
      "Rishabh Agarwal"
    ]
  },
  "https://proceedings.mlr.press/v235/farnadi24a.html": {
    "title": "Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities",
    "volume": "main",
    "abstract": "The rise of foundation models holds immense promise for advancing AI, but this progress may amplify existing risks and inequalities, leaving marginalized communities behind. In this position paper, we discuss that disparities towards marginalized communities – performance, representation, privacy, robustness, interpretability and safety – are not isolated concerns but rather interconnected elements of a cascading disparity phenomenon. We contrast foundation models with traditional models and highlight the potential for exacerbated disparity against marginalized communities. Moreover, we emphasize the unique threat of cascading impacts in foundation models, where interconnected disparities can trigger long-lasting negative consequences, specifically to the people on the margin. We define marginalized communities within the machine learning context and explore the multifaceted nature of disparities. We analyze the sources of these disparities, tracing them from data creation, training and deployment procedures to highlight the complex technical and socio-technical landscape. To mitigate the pressing crisis, we conclude with a set of calls to action to mitigate disparity at its source",
    "checked": true,
    "id": "7a79d484cb01f988bc70a07d2f81e7407e07405c",
    "semantic_title": "position: cracking the code of cascading disparity towards marginalized communities",
    "citation_count": 0,
    "authors": [
      "Golnoosh Farnadi",
      "Mohammad Havaei",
      "Negar Rostamzadeh"
    ]
  },
  "https://proceedings.mlr.press/v235/farzam24a.html": {
    "title": "From Geometry to Causality- Ricci Curvature and the Reliability of Causal Inference on Networks",
    "volume": "main",
    "abstract": "Causal inference on networks faces challenges posed in part by violations of standard identification assumptions due to dependencies between treatment units. Although graph geometry fundamentally influences such dependencies, the potential of geometric tools for causal inference on networked treatment units is yet to be unlocked. Moreover, despite significant progress utilizing graph neural networks (GNNs) for causal inference on networks, methods for evaluating their achievable reliability without ground truth are lacking. In this work we establish for the first time a theoretical link between network geometry, the graph Ricci curvature in particular, and causal inference, formalizing the intrinsic challenges that negative curvature poses to estimating causal parameters. The Ricci curvature can then be used to assess the reliability of causal estimates in structured data, as we empirically demonstrate. Informed by this finding, we propose a method using the geometric Ricci flow to reduce causal effect estimation error in networked data, showcasing how this newfound connection between graph geometry and causal inference could improve GNN-based causal inference. Bridging graph geometry and causal inference, this paper opens the door to geometric techniques for improving causal estimation on networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhossein Farzam",
      "Allen Tannenbaum",
      "Guillermo Sapiro"
    ]
  },
  "https://proceedings.mlr.press/v235/fei24a.html": {
    "title": "Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition",
    "volume": "main",
    "abstract": "Existing research of video understanding still struggles to achieve in-depth comprehension and reasoning in complex videos, primarily due to the under-exploration of two key bottlenecks: fine-grained spatial-temporal perceptive understanding and cognitive-level video scene comprehension. This paper bridges the gap by presenting a novel solution. We first introduce a novel video Multimodal Large Language Model (MLLM), MotionEpic, which achieves fine-grained pixel-level spatial-temporal video grounding by integrating video spatial-temporal scene graph (STSG) representation. Building upon MotionEpic, we then develop a Video-of-Thought (VoT) reasoning framework. VoT inherits the Chain-of-Thought (CoT) core, breaking down a complex task into simpler and manageable sub-problems, and addressing them step-by-step from a low-level pixel perception to high-level cognitive interpretation. Extensive experiments across various complex video QA benchmarks demonstrate that our overall framework strikingly boosts existing state-of-the-art. To our knowledge, this is the first attempt at successfully implementing the CoT technique for achieving human-level video reasoning, where we show great potential in extending it to a wider range of video understanding scenarios. Systems and codes will be open later",
    "checked": true,
    "id": "6a08ff3d0821654b1fa92b2b63106946813e41a3",
    "semantic_title": "video-of-thought: step-by-step video reasoning from perception to cognition",
    "citation_count": 18,
    "authors": [
      "Hao Fei",
      "Shengqiong Wu",
      "Wei Ji",
      "Hanwang Zhang",
      "Meishan Zhang",
      "Mong-Li Lee",
      "Wynne Hsu"
    ]
  },
  "https://proceedings.mlr.press/v235/fellows24a.html": {
    "title": "Bayesian Exploration Networks",
    "volume": "main",
    "abstract": "Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. However theoretical understanding of model-free approaches is lacking. In this paper, we introduce a novel Bayesian model-free formulation and the first analysis showing that model-free approaches can yield Bayes-optimal policies. We show all existing model-free approaches make approximations that yield policies that can be arbitrarily Bayes-suboptimal. As a first step towards model-free Bayes optimality, we introduce the Bayesian exploration network (BEN) which uses normalising flows to model both the aleatoric uncertainty (via density estimation) and epistemic uncertainty (via variational inference) in the Bellman operator. In the limit of complete optimisation, BEN learns true Bayes-optimal policies, but like in variational expectation-maximisation, partial optimisation renders our approach tractable. Empirical results demonstrate that BEN can learn true Bayes-optimal policies in tasks where existing model-free approaches fail",
    "checked": true,
    "id": "03cdc3399cc1bdb38fad2d307570e5d9d0eff995",
    "semantic_title": "bayesian exploration networks",
    "citation_count": 3,
    "authors": [
      "Mattie Fellows",
      "Brandon Gary Kaplowitz",
      "Christian Schroeder De Witt",
      "Shimon Whiteson"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24a.html": {
    "title": "Auto-Linear Phenomenon in Subsurface Imaging",
    "volume": "main",
    "abstract": "Subsurface imaging involves solving full waveform inversion (FWI) to predict geophysical properties from measurements. This problem can be reframed as an image-to-image translation, with the usual approach being to train an encoder-decoder network using paired data from two domains: geophysical property and measurement. A recent seminal work (InvLINT) demonstrates there is only a linear mapping between the latent spaces of the two domains, and the decoder requires paired data for training. This paper extends this direction by demonstrating that only linear mapping necessitates paired data, while both the encoder and decoder can be learned from their respective domains through self-supervised learning. This unveils an intriguing phenomenon (named Auto-Linear) where the self-learned features of two separate domains are automatically linearly correlated. Compared with existing methods, our Auto-Linear has four advantages: (a) solving both forward and inverse modeling simultaneously, (b) reducing model size, (c) enhanced performance, especially when the paired data is limited, and (d) strong generalization ability of the trained encoder and decoder",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinan Feng",
      "Yinpeng Chen",
      "Peng Jin",
      "Shihang Feng",
      "Youzuo Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24b.html": {
    "title": "Resisting Stochastic Risks in Diffusion Planners with the Trajectory Aggregation Tree",
    "volume": "main",
    "abstract": "Diffusion planners have shown promise in handling long-horizon and sparse-reward tasks due to the non-autoregressive plan generation. However, their inherent stochastic risk of generating infeasible trajectories presents significant challenges to their reliability and stability. We introduce a novel approach, the Trajectory Aggregation Tree (TAT), to address this issue in diffusion planners. Compared to prior methods that rely solely on raw trajectory predictions, TAT aggregates information from both historical and current trajectories, forming a dynamic tree-like structure. Each trajectory is conceptualized as a branch and individual states as nodes. As the structure evolves with the integration of new trajectories, unreliable states are marginalized, and the most impactful nodes are prioritized for decision-making. TAT can be deployed without modifying the original training and sampling pipelines of diffusion planners, making it a training-free, ready-to-deploy solution. We provide both theoretical analysis and empirical evidence to support TAT's effectiveness. Our results highlight its remarkable ability to resist the risk from unreliable trajectories, guarantee the performance boosting of diffusion planners in 100% of tasks, and exhibit an appreciable tolerance margin for sample quality, thereby enabling planning with a more than $3\\times$ acceleration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lang Feng",
      "Pengjie Gu",
      "Bo An",
      "Gang Pan"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24c.html": {
    "title": "Fast White-Box Adversarial Streaming Without a Random Oracle",
    "volume": "main",
    "abstract": "Recently, the question of adversarially robust streaming, where the stream is allowed to depend on the randomness of the streaming algorithm, has gained a lot of attention. In this work, we consider a strong white-box adversarial model (Ajtai et al. PODS 2022), in which the adversary has access to all past random coins and the parameters used by the streaming algorithm. We focus on the sparse recovery problem and extend our result to other tasks such as distinct element estimation and low-rank approximation of matrices and tensors. The main drawback of previous work is that it requires a random oracle, which is especially problematic in the streaming model since the amount of randomness is counted in the space complexity of a streaming algorithm. Also, the previous work suffers from large update time. We construct a near-optimal solution for the sparse recovery problem in white-box adversarial streams, based on the subexponentially secure Learning with Errors assumption. Importantly, our solution does not require a random oracle and has a polylogarithmic per item processing time. We also give results in a related white-box adversarially robust distributed model. Our constructions are based on homomorphic encryption schemes satisfying very mild structural properties that are currently satisfied by most known schemes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Feng",
      "Aayush Jain",
      "David Woodruff"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24d.html": {
    "title": "DSD-DA: Distillation-based Source Debiasing for Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "Though feature-alignment based Domain Adaptive Object Detection (DAOD) methods have achieved remarkable progress, they ignore the source bias issue, i.e., the detector tends to acquire more source-specific knowledge, impeding its generalization capabilities in the target domain. Furthermore, these methods face a more formidable challenge in achieving consistent classification and localization in the target domain compared to the source domain. To overcome these challenges, we propose a novel Distillation-based Source Debiasing (DSD) framework for DAOD, which can distill domain-agnostic knowledge from a pre-trained teacher model, improving the detector's performance on both domains. In addition, we design a Target-Relevant Object Localization Network (TROLN), which can mine target-related localization information from source and target-style mixed data. Accordingly, we present a Domain-aware Consistency Enhancing (DCE) strategy, in which these information are formulated into a new localization representation to further refine classification scores in the testing stage, achieving a harmonization between classification and localization. Extensive experiments have been conducted to manifest the effectiveness of this method, which consistently improves the strong baseline by large margins, outperforming existing alignment-based works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongchao Feng",
      "Shiwei Li",
      "Yingjie Gao",
      "Ziyue Huang",
      "Yanan Zhang",
      "Qingjie Liu",
      "Yunhong Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24e.html": {
    "title": "Keypoint-based Progressive Chain-of-Thought Distillation for LLMs",
    "volume": "main",
    "abstract": "Chain-of-thought distillation is a powerful technique for transferring reasoning abilities from large language models (LLMs) to smaller student models. Previous methods typically require the student to mimic the step-by-step rationale produced by LLMs, often facing the following challenges: (i) Tokens within a rationale vary in significance, and treating them equally may fail to accurately mimic keypoint tokens, leading to reasoning errors. (ii) They usually distill knowledge by consistently predicting all the steps in a rationale, which falls short in distinguishing the learning order of step generation. This diverges from the human cognitive progression of starting with easy tasks and advancing to harder ones, resulting in sub-optimal outcomes. To this end, we propose a unified framework, called KPOD, to address these issues. Specifically, we propose a token weighting module utilizing mask learning to encourage accurate mimicry of keypoint tokens by the student during distillation. Besides, we develop an in-rationale progressive distillation strategy, starting with training the student to generate the final reasoning steps and gradually extending to cover the entire rationale. To accomplish this, a weighted token generation loss is proposed to assess step reasoning difficulty, and a value function is devised to schedule the progressive distillation by considering both step difficulty and question diversity. Extensive experiments on four reasoning benchmarks illustrate our KPOD outperforms previous methods by a large margin",
    "checked": true,
    "id": "3893c0516278a602c296242fb6755914b0a782c1",
    "semantic_title": "keypoint-based progressive chain-of-thought distillation for llms",
    "citation_count": 1,
    "authors": [
      "Kaituo Feng",
      "Changsheng Li",
      "Xiaolu Zhang",
      "Jun Zhou",
      "Ye Yuan",
      "Guoren Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24f.html": {
    "title": "UniCorn: A Unified Contrastive Learning Approach for Multi-view Molecular Representation Learning",
    "volume": "main",
    "abstract": "Recently, a noticeable trend has emerged in developing pre-trained foundation models in the domains of CV and NLP. However, for molecular pre-training, there lacks a universal model capable of effectively applying to various categories of molecular tasks, since existing prevalent pre-training methods exhibit effectiveness for specific types of downstream tasks. Furthermore, the lack of profound understanding of existing pre-training methods, including 2D graph masking, 2D-3D contrastive learning, and 3D denoising, hampers the advancement of molecular foundation models. In this work, we provide a unified comprehension of existing pre-training methods through the lens of contrastive learning. Thus their distinctions lie in clustering different views of molecules, which is shown beneficial to specific downstream tasks. To achieve a complete and general-purpose molecular representation, we propose a novel pre-training framework, named UniCorn, that inherits the merits of the three methods, depicting molecular views in three different levels. SOTA performance across quantum, physicochemical, and biological tasks, along with comprehensive ablation study, validate the universality and effectiveness of UniCorn",
    "checked": true,
    "id": "2f0d715676e56f340cfc734c8c819bc455d88340",
    "semantic_title": "unicorn: a unified contrastive learning approach for multi-view molecular representation learning",
    "citation_count": 3,
    "authors": [
      "Shikun Feng",
      "Yuyan Ni",
      "Minghao Li",
      "Yanwen Huang",
      "Zhi-Ming Ma",
      "Wei-Ying Ma",
      "Yanyan Lan"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24g.html": {
    "title": "Prediction Accuracy of Learning in Games : Follow-the-Regularized-Leader meets Heisenberg",
    "volume": "main",
    "abstract": "We investigate the accuracy of prediction in deterministic learning dynamics of zero-sum games with random initializations, specifically focusing on observer uncertainty and its relationship to the evolution of covariances. Zero-sum games are a prominent field of interest in machine learning due to their various applications. Concurrently, the accuracy of prediction in dynamical systems from mechanics has long been a classic subject of investigation since the discovery of the Heisenberg Uncertainty Principle. This principle employs covariance and standard deviation of particle states to measure prediction accuracy. In this study, we bring these two approaches together to analyze the Follow-the-Regularized-Leader (FTRL) algorithm in two-player zero-sum games. We provide growth rates of covariance information for continuous-time FTRL, as well as its two canonical discretization methods (Euler and Symplectic). A Heisenberg-type inequality is established for FTRL. Our analysis and experiments also show that employing Symplectic discretization enhances the accuracy of prediction in learning dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Feng",
      "Georgios Piliouras",
      "Xiao Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24h.html": {
    "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
    "volume": "main",
    "abstract": "Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanglun Feng",
      "Florian Tramèr"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24i.html": {
    "title": "Memory Efficient Neural Processes via Constant Memory Attention Block",
    "volume": "main",
    "abstract": "Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leo Feng",
      "Frederick Tung",
      "Hossein Hajimirsadeghi",
      "Yoshua Bengio",
      "Mohamed Osama Ahmed"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24j.html": {
    "title": "Improving Sample Efficiency of Model-Free Algorithms for Zero-Sum Markov Games",
    "volume": "main",
    "abstract": "The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $\\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/\\epsilon^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by leveraging the popular variance reduction technique based on the reference-advantage decomposition previously used only for single-agent RL. However, such a technique relies on a critical monotonicity property of the value function, which does not hold in Markov games due to the update of the policy via the coarse correlated equilibrium (CCE) oracle. Thus, to extend such a technique to Markov games, our algorithm features a key novel design of updating the reference value functions as the pair of optimistic and pessimistic value functions whose value difference is the smallest in the history in order to achieve the desired improvement in the sample efficiency",
    "checked": true,
    "id": "0d54b10fd45f138cb1feae8cf72d8cd96158c379",
    "semantic_title": "improving sample efficiency of model-free algorithms for zero-sum markov games",
    "citation_count": 0,
    "authors": [
      "Songtao Feng",
      "Ming Yin",
      "Yu-Xiang Wang",
      "Jing Yang",
      "Yingbin Liang"
    ]
  },
  "https://proceedings.mlr.press/v235/feng24k.html": {
    "title": "AquaLoRA: Toward White-box Protection for Customized Stable Diffusion Models via Watermark LoRA",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in generating high-quality images. Recently, the open-source models represented by Stable Diffusion (SD) are thriving and are accessible for customization, giving rise to a vibrant community of creators and enthusiasts. However, the widespread availability of customized SD models has led to copyright concerns, like unauthorized model distribution and unconsented commercial use. To address it, recent works aim to let SD models output watermarked content for post-hoc forensics. Unfortunately, none of them can achieve the challenging white-box protection, wherein the malicious user can easily remove or replace the watermarking module to fail the subsequent verification. For this, we propose AquaLoRA as the first implementation under this scenario. Briefly, we merge watermark information into the U-Net of Stable Diffusion Models via a watermark LowRank Adaptation (LoRA) module in a two-stage manner. For watermark LoRA module, we devise a scaling matrix to achieve flexible message updates without retraining. To guarantee fidelity, we design Prior Preserving Fine-Tuning (PPFT) to ensure watermark learning with minimal impacts on model distribution, validated by proofs. Finally, we conduct extensive experiments and ablation studies to verify our design. Our code is available at github.com/Georgefwt/AquaLoRA",
    "checked": true,
    "id": "dd0ebe7c71819a61ac6cc7a2d21665f83d52415d",
    "semantic_title": "aqualora: toward white-box protection for customized stable diffusion models via watermark lora",
    "citation_count": 4,
    "authors": [
      "Weitao Feng",
      "Wenbo Zhou",
      "Jiyan He",
      "Jie Zhang",
      "Tianyi Wei",
      "Guanlin Li",
      "Tianwei Zhang",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/ferber24a.html": {
    "title": "GenCO: Generating Diverse Designs with Combinatorial Constraints",
    "volume": "main",
    "abstract": "Deep generative models like GAN and VAE have shown impressive results in generating unconstrained objects like images. However, many design settings arising in industrial design, material science, computer graphics and more require that the generated objects satisfy hard combinatorial constraints or meet objectives in addition to modeling a data distribution. To address this, we propose GenCO, a generative framework that guarantees constraint satisfaction throughout training by leveraging differentiable combinatorial solvers to enforce feasibility. GenCO imposes the generative loss on provably feasible solutions rather than intermediate soft solutions, meaning that the deep generative network can focus on ensuring the generated objects match the data distribution without having to also capture feasibility. This shift enables practitioners to enforce hard constraints on the generated outputs during end-to-end training, enabling assessments of their feasibility and introducing additional combinatorial loss components to deep generative training. We demonstrate the effectiveness of our approach on a variety of generative combinatorial tasks, including game level generation, map creation for path planning, and photonic device design, consistently demonstrating its capability to yield diverse, high-quality solutions that verifiably adhere to user-specified combinatorial properties",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron M Ferber",
      "Arman Zharmagambetov",
      "Taoan Huang",
      "Bistra Dilkina",
      "Yuandong Tian"
    ]
  },
  "https://proceedings.mlr.press/v235/ferchichi24a.html": {
    "title": "Active Ranking and Matchmaking, with Perfect Matchings",
    "volume": "main",
    "abstract": "We address the challenge of actively ranking a set of items/players with varying values/strengths. The comparison outcomes are random, with a greater noise the closer the values. A crucial requirement is that, at each iteration of the algorithm, all items must be compared once, i.e., an iteration is a perfect matching. Furthermore, we presume that comparing two players with closely matched strengths incurs no cost and, in contrast, a unit cost is associated with comparing players whose strength difference is more substantial. Our secondary objective is to determine an optimal matching between players based on this cost function: we propose and analyze an algorithm that draws on concepts from both AKS sorting networks and bandit theory. Our algorithm achieves both objectives with high probability, and the total cost is optimal (up to logarithmic terms)",
    "checked": true,
    "id": "2c2f28204ec8bb51bd82afecba3def3ed6dcf148",
    "semantic_title": "active ranking and matchmaking, with perfect matchings",
    "citation_count": 0,
    "authors": [
      "Hafedh El Ferchichi",
      "Matthieu Lerasle",
      "Vianney Perchet"
    ]
  },
  "https://proceedings.mlr.press/v235/fernando24a.html": {
    "title": "Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution",
    "volume": "main",
    "abstract": "Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, evaluates them for fitness on a training set, and repeats this process over multiple generations to evolve task-prompts. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutation-prompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furthermore, Promptbreeder is able to evolve intricate task-prompts for the challenging problem of hate speech classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chrisantha Fernando",
      "Dylan Sunil Banarse",
      "Henryk Michalewski",
      "Simon Osindero",
      "Tim Rocktäschel"
    ]
  },
  "https://proceedings.mlr.press/v235/ferry24a.html": {
    "title": "Trained Random Forests Completely Reveal your Dataset",
    "volume": "main",
    "abstract": "We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest. Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn. To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective. We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming - an approach rooted in constraint propagation and solution-domain reduction. Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction. This holds true even with a small number of trees. Even with bootstrap aggregation, the majority of the data can also be reconstructed. These findings underscore a critical vulnerability inherent in widely adopted ensemble methods, warranting attention and mitigation. Although the potential for such reconstruction attacks has been discussed in privacy research, our study provides clear empirical evidence of their practicability",
    "checked": true,
    "id": "55a9feacb97fb0fcf85b3c6cbd760e9bb3ba5b70",
    "semantic_title": "trained random forests completely reveal your dataset",
    "citation_count": 3,
    "authors": [
      "Julien Ferry",
      "Ricardo Fukasawa",
      "Timothée Pascal",
      "Thibaut Vidal"
    ]
  },
  "https://proceedings.mlr.press/v235/ferte24a.html": {
    "title": "Reservoir Computing for Short High-Dimensional Time Series: an Application to SARS-CoV-2 Hospitalization Forecast",
    "volume": "main",
    "abstract": "In this work, we aimed at forecasting the number of SARS-CoV-2 hospitalized patients at 14 days to help anticipate the bed requirements of a large scale hospital using public data and electronic health records data. Previous attempts led to mitigated performance in this high-dimension setting; we introduce a novel approach to time series forecasting by providing an alternative to conventional methods to deal with high number of potential features of interest (409 predictors). We integrate Reservoir Computing (RC) with feature selection using a genetic algorithm (GA) to gather optimal non-linear combinations of inputs to improve prediction in sample-efficient context. We illustrate that the RC-GA combination exhibits excellent performance in forecasting SARS-CoV-2 hospitalizations. This approach outperformed the use of RC alone and other conventional methods: LSTM, Transformers, Elastic-Net, XGBoost. Notably, this work marks the pioneering use of RC (along with GA) in the realm of short and high-dimensional time series, positioning it as a competitive and innovative approach in comparison to standard methods",
    "checked": true,
    "id": "30d4e2ddc63d7a366bff4ee124fda86508274dbe",
    "semantic_title": "reservoir computing for short high-dimensional time series: an application to sars-cov-2 hospitalization forecast",
    "citation_count": 0,
    "authors": [
      "Thomas Ferté",
      "Dan Dutartre",
      "Boris P Hejblum",
      "Romain Griffier",
      "Vianney Jouhet",
      "Rodolphe Thiébaut",
      "Pierrick Legrand",
      "Xavier Hinaut"
    ]
  },
  "https://proceedings.mlr.press/v235/fey24a.html": {
    "title": "Position: Relational Deep Learning - Graph Representation Learning on Relational Databases",
    "volume": "main",
    "abstract": "Much of the world's most valued data is stored in relational databases and data warehouses, where the data is organized into tables connected by primary-foreign key relations. However, building machine learning models using this data is both challenging and time consuming because no ML algorithm can directly learn from multiple connected tables. Current approaches can only learn from a single table, so data must first be manually joined and aggregated into this format, the laborious process known as feature engineering. Feature engineering is slow, error prone and leads to suboptimal models. Here we introduce Relational Deep Learning (RDL), a blueprint for end-to-end learning on relational databases. The key is to represent relational databases as a temporal, heterogeneous graphs, with a node for each row in each table, and edges specified by primary-foreign key links. Graph Neural Networks then learn representations that leverage all input data, without any manual feature engineering. We also introduce RelBench, and benchmark and testing suite, demonstrating strong initial results. Overall, we define a new research area that generalizes graph machine learning and broadens its applicability",
    "checked": true,
    "id": "1181c36f7b932384defbc97c9b5d792618fda1a5",
    "semantic_title": "position: relational deep learning - graph representation learning on relational databases",
    "citation_count": 0,
    "authors": [
      "Matthias Fey",
      "Weihua Hu",
      "Kexin Huang",
      "Jan Eric Lenssen",
      "Rishabh Ranjan",
      "Joshua Robinson",
      "Rex Ying",
      "Jiaxuan You",
      "Jure Leskovec"
    ]
  },
  "https://proceedings.mlr.press/v235/fiedler24a.html": {
    "title": "On Statistical Learning Theory for Distributional Inputs",
    "volume": "main",
    "abstract": "Kernel-based statistical learning on distributional inputs appears in many relevant applications, from medical diagnostics to causal inference, and poses intriguing theoretical questions. While this learning scenario received considerable attention from the machine learning community recently, many gaps in the theory remain. In particular, most works consider only the distributional regression setting, and focus on the regularized least-squares algorithm for this problem. In this work, we start to fill these gaps. We prove two oracle inequalities for kernel machines in general distributional learning scenarios, as well as a generalization result based on algorithmic stability. Our main results are formulated in great generality, utilizing general Hilbertian embeddings, which makes them applicable to a wide array of approaches to distributional learning. Additionally, we specialize our results to the cases of kernel mean embeddings and of the recently introduced Hilbertian embeddings based on sliced Wasserstein distances, providing concrete instances of the general setup. Our results considerably enlarge the scope of theoretically grounded distributional learning, and provide many interesting avenues for future work",
    "checked": true,
    "id": "85b485415cbe5658028c688a71c2f0ed07335ab8",
    "semantic_title": "on statistical learning theory for distributional inputs",
    "citation_count": 0,
    "authors": [
      "Christian Fiedler",
      "Pierre-François Massiani",
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ]
  },
  "https://proceedings.mlr.press/v235/finkelshtein24a.html": {
    "title": "Cooperative Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either listen, broadcast, listen and broadcast, or to isolate. The standard message propagation scheme can then be viewed as a special case of this framework where every node listens and broadcasts to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on a synthetic and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Finkelshtein",
      "Xingyue Huang",
      "Michael M. Bronstein",
      "Ismail Ilkan Ceylan"
    ]
  },
  "https://proceedings.mlr.press/v235/fischer24a.html": {
    "title": "Critical feature learning in deep neural networks",
    "volume": "main",
    "abstract": "A key property of neural networks driving their success is their ability to learn features from data. Understanding feature learning from a theoretical viewpoint is an emerging field with many open questions. In this work we capture finite-width effects with a systematic theory of network kernels in deep non-linear neural networks. We show that the Bayesian prior of the network can be written in closed form as a superposition of Gaussian processes, whose kernels are distributed with a variance that depends inversely on the network width $N$. A large deviation approach, which is exact in the proportional limit for the number of data points $P=\\alpha N\\to\\infty$, yields a pair of forward-backward equations for the maximum a posteriori kernels in all layers at once. We study their solutions perturbatively, to demonstrate how the backward propagation across layers aligns kernels with the target. An alternative field-theoretic formulation shows that kernel adaptation of the Bayesian posterior at finite-width results from fluctuations in the prior: larger fluctuations correspond to a more flexible network prior and thus enable stronger adaptation to data. We thus find a bridge between the classical edge-of-chaos NNGP theory and feature learning, exposing an intricate interplay between criticality, response functions, and feature scale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirsten Fischer",
      "Javed Lindner",
      "David Dahmen",
      "Zohar Ringel",
      "Michael Krämer",
      "Moritz Helias"
    ]
  },
  "https://proceedings.mlr.press/v235/fischer24b.html": {
    "title": "Neuroexplicit Diffusion Models for Inpainting of Optical Flow Fields",
    "volume": "main",
    "abstract": "Deep learning has revolutionized the field of computer vision by introducing large scale neural networks with millions of parameters. Training these networks requires massive datasets and leads to intransparent models that can fail to generalize. At the other extreme, models designed from partial differential equations (PDEs) embed specialized domain knowledge into mathematical equations and usually rely on few manually chosen hyperparameters. This makes them transparent by construction and if designed and calibrated carefully, they can generalize well to unseen scenarios. In this paper, we show how to bring model- and data-driven approaches together by combining the explicit PDE-based approaches with convolutional neural networks to obtain the best of both worlds. We illustrate a joint architecture for the task of inpainting optical flow fields and show that the combination of model- and data-driven modeling leads to an effective architecture. Our model outperforms both fully explicit and fully data-driven baselines in terms of reconstruction quality, robustness and amount of required training data. Averaging the endpoint error across different mask densities, our method outperforms the explicit baselines by 11-27%, the GAN baseline by 47% and the Probabilisitic Diffusion baseline by 42%. With that, our method sets a new state of the art for inpainting of optical flow fields from random masks",
    "checked": true,
    "id": "19d789eaaf4ff9a61f52a234a30ee825306c2deb",
    "semantic_title": "neuroexplicit diffusion models for inpainting of optical flow fields",
    "citation_count": 0,
    "authors": [
      "Tom Fischer",
      "Pascal Peter",
      "Joachim Weickert",
      "Eddy Ilg"
    ]
  },
  "https://proceedings.mlr.press/v235/fisher24a.html": {
    "title": "Inverse-Variance Weighting for Estimation of Heterogeneous Treatment Effects",
    "volume": "main",
    "abstract": "Many methods for estimating conditional average treatment effects (CATEs) can be expressed as weighted pseudo-outcome regressions (PORs). Previous comparisons of POR techniques have paid careful attention to the choice of pseudo-outcome transformation. However, we argue that the dominant driver of performance is actually the choice of weights. For example, we point out that R-Learning implicitly performs a POR with inverse-variance weights (IVWs). In the CATE setting, IVWs mitigate the instability associated with inverse-propensity weights, and lead to convenient simplifications of bias terms. We demonstrate the superior performance of IVWs in simulations, and derive convergence rates for IVWs that are, to our knowledge, the fastest yet shown without assuming knowledge of the covariate distribution",
    "checked": true,
    "id": "9eedb583d92c3075a6177941bd51d96ab1ba09a5",
    "semantic_title": "inverse-variance weighting for estimation of heterogeneous treatment effects",
    "citation_count": 2,
    "authors": [
      "Aaron Fisher"
    ]
  },
  "https://proceedings.mlr.press/v235/fourati24a.html": {
    "title": "Stochastic Q-learning for Large Discrete Action Spaces",
    "volume": "main",
    "abstract": "In complex environments with large discrete action spaces, effective decision-making is critical in reinforcement learning (RL). Despite the widespread use of value-based RL approaches like Q-learning, they come with a computational burden, necessitating the maximization of a value function over all actions in each iteration. This burden becomes particularly challenging when addressing large-scale problems and using deep neural networks as function approximators. In this paper, we present stochastic value-based RL approaches which, in each iteration, as opposed to optimizing over the entire set of $n$ actions, only consider a variable stochastic set of a sublinear number of actions, possibly as small as $\\mathcal{O}(\\log(n))$. The presented stochastic value-based RL methods include, among others, Stochastic Q-learning, StochDQN, and StochDDQN, all of which integrate this stochastic approach for both value-function updates and action selection. The theoretical convergence of Stochastic Q-learning is established, while an analysis of stochastic maximization is provided. Moreover, through empirical validation, we illustrate that the various proposed approaches outperform the baseline methods across diverse environments, including different control problems, achieving near-optimal average returns in significantly reduced time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fares Fourati",
      "Vaneet Aggarwal",
      "Mohamed-Slim Alouini"
    ]
  },
  "https://proceedings.mlr.press/v235/fourati24b.html": {
    "title": "Federated Combinatorial Multi-Agent Multi-Armed Bandits",
    "volume": "main",
    "abstract": "This paper introduces a federated learning framework tailored for online combinatorial optimization with bandit feedback. In this setting, agents select subsets of arms, observe noisy rewards for these subsets without accessing individual arm information, and can cooperate and share information at specific intervals. Our framework transforms any offline resilient single-agent $(\\alpha-\\epsilon)$-approximation algorithm—having a complexity of $\\tilde{\\mathcal{O}}\\left(\\frac{\\psi}{\\epsilon^\\beta}\\right)$, where the logarithm is omitted, for some function $\\psi$ and constant $\\beta$—into an online multi-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of no more than $\\tilde{\\mathcal{O}}\\left(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta} T^\\frac{2+\\beta}{3+\\beta}\\right)$. Our approach not only eliminates the $\\epsilon$ approximation error but also ensures sublinear growth with respect to the time horizon $T$ and demonstrates a linear speedup with an increasing number of communicating agents. Additionally, the algorithm is notably communication-efficient, requiring only a sublinear number of communication rounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi T^\\frac{\\beta}{\\beta+1}\\right)$. Furthermore, the framework has been successfully applied to online stochastic submodular maximization using various offline algorithms, yielding the first results for both single-agent and multi-agent settings and recovering specialized single-agent theoretical guarantees. We empirically validate our approach to a stochastic data summarization problem, illustrating the effectiveness of the proposed framework, even in single-agent scenarios",
    "checked": true,
    "id": "46b8475b4b764376e79689d60fb279f9ac634f1f",
    "semantic_title": "federated combinatorial multi-agent multi-armed bandits",
    "citation_count": 1,
    "authors": [
      "Fares Fourati",
      "Mohamed-Slim Alouini",
      "Vaneet Aggarwal"
    ]
  },
  "https://proceedings.mlr.press/v235/francazi24a.html": {
    "title": "Initial Guessing Bias: How Untrained Networks Favor Some Classes",
    "volume": "main",
    "abstract": "Understanding and controlling biasing effects in neural networks is crucial for ensuring accurate and fair model performance. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a deep neural network (DNN) can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We prove that, besides dataset properties, the presence of this phenomenon, which we call Initial Guessing Bias (IGB), is influenced by model choices including dataset preprocessing methods, and architectural decisions, such as activation functions, max-pooling layers, and network depth. Our analysis of IGB provides information for architecture selection and model initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging and the non-trivial effects that depth has on the phenomenon",
    "checked": true,
    "id": "065e39b5b4e7cbef3dbbd9641d54ba340302767a",
    "semantic_title": "initial guessing bias: how untrained networks favor some classes",
    "citation_count": 3,
    "authors": [
      "Emanuele Francazi",
      "Aurelien Lucchi",
      "Marco Baity-Jesi"
    ]
  },
  "https://proceedings.mlr.press/v235/franceschi24a.html": {
    "title": "Explaining Probabilistic Models with Distributional Values",
    "volume": "main",
    "abstract": "A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for probabilistic models by generalising cooperative games and value operators. We introduce the distributional values, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models",
    "checked": true,
    "id": "95078a05a627a60c64d0640347738f886f411f2c",
    "semantic_title": "explaining probabilistic models with distributional values",
    "citation_count": 2,
    "authors": [
      "Luca Franceschi",
      "Michele Donini",
      "Cedric Archambeau",
      "Matthias Seeger"
    ]
  },
  "https://proceedings.mlr.press/v235/franco24a.html": {
    "title": "Hyperbolic Active Learning for Semantic Segmentation under Domain Shift",
    "volume": "main",
    "abstract": "We introduce a hyperbolic neural network approach to pixel-level active learning for semantic segmentation. Analysis of the data statistics leads to a novel interpretation of the hyperbolic radius as an indicator of data scarcity. In HALO (Hyperbolic Active Learning Optimization), for the first time, we propose the use of epistemic uncertainty as a data acquisition strategy, following the intuition of selecting data points that are the least known. The hyperbolic radius, complemented by the widely-adopted prediction entropy, effectively approximates epistemic uncertainty. We perform extensive experimental analysis based on two established synthetic-to-real benchmarks, i.e. GTAV $\\rightarrow$ Cityscapes and SYNTHIA $\\rightarrow$ Cityscapes. Additionally, we test HALO on Cityscape $\\rightarrow$ ACDC for domain adaptation under adverse weather conditions, and we benchmark both convolutional and attention-based backbones. HALO sets a new state-of-the-art in active learning for semantic segmentation under domain shift and it is the first active learning approach that surpasses the performance of supervised domain adaptation while using only a small portion of labels (i.e., 1%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Franco",
      "Paolo Mandica",
      "Konstantinos Kallidromitis",
      "Devin Guillory",
      "Yu-Teng Li",
      "Trevor Darrell",
      "Fabio Galasso"
    ]
  },
  "https://proceedings.mlr.press/v235/franks24a.html": {
    "title": "Weisfeiler-Leman at the margin: When more expressivity matters",
    "volume": "main",
    "abstract": "The Weisfeiler–Leman algorithm (1-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, the 1-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting 1-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we introduce variations of expressive 1-WL-based kernel and MPNN architectures with provable generalization properties. Our empirical study confirms the validity of our theoretical findings",
    "checked": true,
    "id": "03b4eb518db81999ad194560caa7eae57761cf9b",
    "semantic_title": "weisfeiler-leman at the margin: when more expressivity matters",
    "citation_count": 6,
    "authors": [
      "Billy Joe Franks",
      "Christopher Morris",
      "Ameya Velingker",
      "Floris Geerts"
    ]
  },
  "https://proceedings.mlr.press/v235/frans24a.html": {
    "title": "Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings",
    "volume": "main",
    "abstract": "Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Frans",
      "Seohong Park",
      "Pieter Abbeel",
      "Sergey Levine"
    ]
  },
  "https://proceedings.mlr.press/v235/frauen24a.html": {
    "title": "Fair Off-Policy Learning from Observational Data",
    "volume": "main",
    "abstract": "Algorithmic decision-making in practice must be fair for legal, ethical, and societal reasons. To achieve this, prior research has contributed various approaches that ensure fairness in machine learning predictions, while comparatively little effort has focused on fairness in decision-making, specifically off-policy learning. In this paper, we propose a novel framework for fair off-policy learning: we learn decision rules from observational data under different notions of fairness, where we explicitly assume that observational data were collected under a different – potentially discriminatory – behavioral policy. Importantly, our framework applies to different fairness notions for off-policy learning, where fairness is formalized based on actions or policy values. As our main contribution, we propose a neural network-based framework to learn optimal policies under different fairness notions. We further provide theoretical guarantees in the form of generalization bounds for the finite-sample version of our framework. We demonstrate the effectiveness of our framework through extensive numerical experiments using both simulated and real-world data. Altogether, our work enables algorithmic decision-making in a wide array of practical applications where fairness must be ensured",
    "checked": true,
    "id": "27578c0d8aa5941364e2a56917dd1c536a8178fc",
    "semantic_title": "fair off-policy learning from observational data",
    "citation_count": 1,
    "authors": [
      "Dennis Frauen",
      "Valentyn Melnychuk",
      "Stefan Feuerriegel"
    ]
  },
  "https://proceedings.mlr.press/v235/frauenknecht24a.html": {
    "title": "Trust the Model Where It Trusts Itself - Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption",
    "volume": "main",
    "abstract": "Dyna-style model-based reinforcement learning (MBRL) combines model-free agents with predictive transition models through model-based rollouts. This combination raises a critical question: \"When to trust your model?\"; i.e., which rollout length results in the model providing useful data? Janner et al. (2019) address this question by gradually increasing rollout lengths throughout the training. While theoretically tempting, uniform model accuracy is a fallacy that collapses at the latest when extrapolating. Instead, we propose asking the question \"Where to trust your model?\". Using inherent model uncertainty to consider local accuracy, we obtain the Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption (MACURA) algorithm. We propose an easy-to-tune rollout mechanism and demonstrate substantial improvements in data efficiency and performance compared to state-of-the-art deep MBRL methods on the MuJoCo benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernd Frauenknecht",
      "Artur Eisele",
      "Devdutt Subhasish",
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ]
  },
  "https://proceedings.mlr.press/v235/friedbaum24a.html": {
    "title": "Trustworthy Actionable Perturbations",
    "volume": "main",
    "abstract": "Counterfactuals, or modified inputs that lead to a different outcome, are an important tool for understanding the logic used by machine learning classifiers and how to change an undesirable classification. Even if a counterfactual changes a classifier's decision, however, it may not affect the true underlying class probabilities, i.e. the counterfactual may act like an adversarial attack and \"fool\" the classifier. We propose a new framework for creating modified inputs that change the true underlying probabilities in a beneficial way which we call Trustworthy Actionable Perturbations (TAP). This includes a novel verification procedure to ensure that TAP change the true class probabilities instead of acting adversarially. Our framework also includes new cost, reward, and goal definitions that are better suited to effectuating change in the real world. We present PAC-learnability results for our verification procedure and theoretically analyze our new method for measuring reward. We also develop a methodology for creating TAP and compare our results to those achieved by previous counterfactual methods",
    "checked": true,
    "id": "a6897e73394a77596007a559b0b28c0a08c41d6b",
    "semantic_title": "trustworthy actionable perturbations",
    "citation_count": 0,
    "authors": [
      "Jesse Friedbaum",
      "Sudarshan Adiga",
      "Ravi Tandon"
    ]
  },
  "https://proceedings.mlr.press/v235/friedman24a.html": {
    "title": "Interpretability Illusions in the Generalization of Simplified Models",
    "volume": "main",
    "abstract": "A common method to study deep learning systems is to use simplified model representations—for example, using singular value decomposition to visualize the model's hidden states in a lower dimensional space. This approach assumes that the results of these simplifications are faithful to the original model. Here, we illustrate an important caveat to this assumption: even if the simplified representations can accurately approximate the full model on the training set, they may fail to accurately capture the model's behavior out of distribution. We illustrate this by training Transformer models on controlled datasets with systematic generalization splits, including the Dyck balanced-parenthesis languages and a code completion task. We simplify these models using tools like dimensionality reduction and clustering, and then explicitly test how these simplified proxies match the behavior of the original model. We find consistent generalization gaps: cases in which the simplified proxies are more faithful to the original model on the in-distribution evaluations and less faithful on various tests of systematic generalization. This includes cases where the original model generalizes systematically but the simplified proxies fail, and cases where the simplified proxies generalize better. Together, our results raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Friedman",
      "Andrew Kyle Lampinen",
      "Lucas Dixon",
      "Danqi Chen",
      "Asma Ghandeharioun"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24a.html": {
    "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding",
    "volume": "main",
    "abstract": "Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichao Fu",
      "Peter Bailis",
      "Ion Stoica",
      "Hao Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24b.html": {
    "title": "A Touch, Vision, and Language Dataset for Multimodal Alignment",
    "volume": "main",
    "abstract": "Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild visiontouch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-visionlanguage (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) tactile-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code, checkpoints and data are available on https: //tactile-vlm.github.io",
    "checked": true,
    "id": "c4a1d57011307c575bfa6f41d0afe9dc75fed10b",
    "semantic_title": "a touch, vision, and language dataset for multimodal alignment",
    "citation_count": 16,
    "authors": [
      "Letian Fu",
      "Gaurav Datta",
      "Huang Huang",
      "William Chung-Ho Panitch",
      "Jaimyn Drake",
      "Joseph Ortiz",
      "Mustafa Mukadam",
      "Mike Lambeta",
      "Roberto Calandra",
      "Ken Goldberg"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24c.html": {
    "title": "Hyperbolic Geometric Latent Diffusion Model for Graph Generation",
    "volume": "main",
    "abstract": "Diffusion models have made significant contributions to computer vision, sparking a growing interest in the community recently regarding the application of it to graph generation. The existing discrete graph diffusion models exhibit heightened computational complexity and diminished training efficiency. A preferable and natural way is to directly diffuse the graph within the latent space. However, due to the non-Euclidean structure of graphs is not isotropic in the latent space, the existing latent diffusion models effectively make it difficult to capture and preserve the topological information of graphs. To address the above challenges, we propose a novel geometrically latent diffusion framework HypDiff. Specifically, we first establish a geometrically latent space with interpretability measures based on hyperbolic geometry, to define anisotropic latent diffusion processes for graphs. Then, we propose a geometrically latent diffusion process that is constrained by both radial and angular geometric properties, thereby ensuring the preservation of the original topological properties in the generative graphs. Extensive experimental results demonstrate the superior effectiveness of HypDiff for graph generation with various topologies",
    "checked": true,
    "id": "b1b6e6b34600df7ac53c10f034ca6794ce749ff8",
    "semantic_title": "hyperbolic geometric latent diffusion model for graph generation",
    "citation_count": 2,
    "authors": [
      "Xingcheng Fu",
      "Yisen Gao",
      "Yuecen Wei",
      "Qingyun Sun",
      "Hao Peng",
      "Jianxin Li",
      "Xianxian Li"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24d.html": {
    "title": "Data Engineering for Scaling Language Models to 128K Context",
    "volume": "main",
    "abstract": "We study continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular the ability to utilize information at arbitrary input locations, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training (e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the quantity and quality of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize domain balance and length upsampling. Concretely, naïvely upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance; a balanced domain mixture is equally important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Fu",
      "Rameswar Panda",
      "Xinyao Niu",
      "Xiang Yue",
      "Hannaneh Hajishirzi",
      "Yoon Kim",
      "Hao Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24e.html": {
    "title": "Language-guided Skill Learning with Temporal Variational Inference",
    "volume": "main",
    "abstract": "We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Fu",
      "Pratyusha Sharma",
      "Elias Stengel-Eskin",
      "George Konidaris",
      "Nicolas Le Roux",
      "Marc-Alexandre Côté",
      "Xingdi Yuan"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24f.html": {
    "title": "PinNet: Pinpoint Instructive Information for Retrieval Augmented Code-to-Text Generation",
    "volume": "main",
    "abstract": "Automatically generating high-quality code descriptions greatly improves the readability and maintainability of the codebase. Recently, retrieval augmented code-to-text generation has proven to be an effective solution, which has achieved state-of-the-art results on various benchmarks. It brings out the potential to leverage large unlabeled code descriptions to further improve the generation quality. Despite the promising performance, retrieval-augmented models however suffer from being deluded by inconducive retrieved references, due to irrelevant or even misleading information contained therein. To this end, we design PinNet, a new framework for code-to-text generation. PinNet relies on a discriminator to measure how well the retrievals match the semantics of the input code. Remarkably, the hidden representation of the reference before the output layer of the discriminator can be leveraged to significantly improve the code-to-text generation by modifying the attention weights. It essentially pays high attention to valuable information and eliminates misleadingness. To effectively execute this idea, we also propose a novel contrastive learning method to quantify the semantical similarities between unlabeled references. Using extensive experiments on code summarization and SQL-to-text generation, we demonstrate that the proposed method can significantly outperform all of the baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Fu",
      "Jian Tan",
      "Pinhan Zhang",
      "Feifei Li",
      "Jianling Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24g.html": {
    "title": "Mean-field Underdamped Langevin Dynamics and its Spacetime Discretization",
    "volume": "main",
    "abstract": "We propose a new method called the N-particle underdamped Langevin algorithm for optimizing a special class of non-linear functionals defined over the space of probability measures. Examples of problems with this formulation include training mean-field neural networks, maximum mean discrepancy minimization and kernel Stein discrepancy minimization. Our algorithm is based on a novel spacetime discretization of the mean-field underdamped Langevin dynamics, for which we provide a new, fast mixing guarantee. In addition, we demonstrate that our algorithm converges globally in total variation distance, bridging the theoretical gap between the dynamics and its practical implementation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Fu",
      "Ashia Camage Wilson"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24h.html": {
    "title": "Breaking through the learning plateaus of in-context learning in Transformer",
    "volume": "main",
    "abstract": "In-context learning, i.e., learning from context examples, is an impressive ability of Transformer. Training Transformers to possess this in-context learning skill is computationally intensive due to the occurrence of learning plateaus, which are periods within the training process where there is minimal or no enhancement in the model's in-context learning capability. To study the mechanism behind the learning plateaus, we conceptually separate a component within the model's internal representation that is exclusively affected by the model's weights. We call this the \"weights component\", and the remainder is identified as the \"context component\". By conducting meticulous and controlled experiments on synthetic tasks, we note that the persistence of learning plateaus correlates with compromised functionality of the weights component. Recognizing the impaired performance of the weights component as a fundamental behavior that drives learning plateaus, we have developed three strategies to expedite the learning of Transformers. The effectiveness of these strategies is further confirmed in natural language processing tasks. In conclusion, our research demonstrates the feasibility of cultivating a powerful in-context learning ability within AI systems in an eco-friendly manner",
    "checked": true,
    "id": "e63bc54f59bba688a5d2d79d842367c59465aeb1",
    "semantic_title": "breaking through the learning plateaus of in-context learning in transformer",
    "citation_count": 1,
    "authors": [
      "Jingwen Fu",
      "Tao Yang",
      "Yuwang Wang",
      "Yan Lu",
      "Nanning Zheng"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24i.html": {
    "title": "Towards Theoretical Understandings of Self-Consuming Generative Models",
    "volume": "main",
    "abstract": "This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training procedure impacts the data distributions learned by future models, including parametric and non-parametric models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios for diffusion models with a one-hidden-layer neural network score function. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declines beyond a threshold point. Finally, we present results for kernel density estimation, delivering nuanced insights such as the impact of mixed data training on error propagation",
    "checked": true,
    "id": "7938da5e88245d1769c23fbe0a3b32ce83fa25e1",
    "semantic_title": "towards theoretical understandings of self-consuming generative models",
    "citation_count": 6,
    "authors": [
      "Shi Fu",
      "Sen Zhang",
      "Yingjie Wang",
      "Xinmei Tian",
      "Dacheng Tao"
    ]
  },
  "https://proceedings.mlr.press/v235/fu24j.html": {
    "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning",
    "volume": "main",
    "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem of reward misalignment when applying VLM as a reward in RL tasks. To address this issue, we introduce a lightweight fine-tuning method, named Fuzzy VLM reward-aided RL (FuRL), based on reward alignment and relay RL. Specifically, we enhance the performance of SAC/DrQ baseline agents on sparse reward tasks by fine-tuning VLM representations and using relay RL to avoid local minima. Extensive experiments on the Meta-world benchmark tasks demonstrate the efficacy of the proposed method. Code is available at: https://github.com/fuyw/FuRL",
    "checked": true,
    "id": "761509e64bf8eb462b615e5fbcea866289ccc8af",
    "semantic_title": "furl: visual-language models as fuzzy rewards for reinforcement learning",
    "citation_count": 4,
    "authors": [
      "Yuwei Fu",
      "Haichao Zhang",
      "Di Wu",
      "Wei Xu",
      "Benoit Boulet"
    ]
  },
  "https://proceedings.mlr.press/v235/fuchsgruber24a.html": {
    "title": "Uncertainty for Active Learning on Graphs",
    "volume": "main",
    "abstract": "Uncertainty Sampling is an Active Learning strategy that aims to improve the data efficiency of machine learning models by iteratively acquiring labels of data points with the highest uncertainty. While it has proven effective for independent data its applicability to graphs remains under-explored. We propose the first extensive study of Uncertainty Sampling for node classification: (1) We benchmark Uncertainty Sampling beyond predictive uncertainty and highlight a significant performance gap to other Active Learning strategies. (2) We develop ground-truth Bayesian uncertainty estimates in terms of the data generating process and prove their effectiveness in guiding Uncertainty Sampling toward optimal queries. We confirm our results on synthetic data and design an approximate approach that consistently outperforms other uncertainty estimators on real datasets. (3) Based on this analysis, we relate pitfalls in modeling uncertainty to existing methods. Our analysis enables and informs the development of principled uncertainty estimation on graphs",
    "checked": true,
    "id": "dd77edcbe0f808a1389d65a72a1643a4d9b7e021",
    "semantic_title": "uncertainty for active learning on graphs",
    "citation_count": 4,
    "authors": [
      "Dominik Fuchsgruber",
      "Tom Wollschläger",
      "Bertrand Charpentier",
      "Antonio Oroz",
      "Stephan Günnemann"
    ]
  },
  "https://proceedings.mlr.press/v235/fumagalli24a.html": {
    "title": "KernelSHAP-IQ: Weighted Least Square Optimization for Shapley Interactions",
    "volume": "main",
    "abstract": "The Shapley value (SV) is a prevalent approach of allocating credit to machine learning (ML) entities to understand black box ML models. Enriching such interpretations with higher-order interactions is inevitable for complex systems, where the Shapley Interaction Index (SII) is a direct axiomatic extension of the SV. While it is well-known that the SV yields an optimal approximation of any game via a weighted least square (WLS) objective, an extension of this result to SII has been a long-standing open problem, which even led to the proposal of an alternative index. In this work, we characterize higher-order SII as a solution to a WLS problem, which constructs an optimal approximation via SII and k-Shapley values (k-SII). We prove this representation for the SV and pairwise SII and give empirically validated conjectures for higher orders. As a result, we propose KernelSHAP-IQ, a direct extension of KernelSHAP for SII, and demonstrate state-of-the-art performance for feature interactions",
    "checked": false,
    "id": "e8a643b38c20e6ee99e4c940c8a5aa1e41cf7a7e",
    "semantic_title": "kernelshap-iq: weighted least-square optimization for shapley interactions",
    "citation_count": 3,
    "authors": [
      "Fabian Fumagalli",
      "Maximilian Muschalik",
      "Patrick Kolpaczki",
      "Eyke Hüllermeier",
      "Barbara Hammer"
    ]
  },
  "https://proceedings.mlr.press/v235/gabidolla24a.html": {
    "title": "Beyond the ROC Curve: Classification Trees Using Cost-Optimal Curves, with Application to Imbalanced Datasets",
    "volume": "main",
    "abstract": "Important applications such as fraud or spam detection or churn prediction involve binary classification problems where the datasets are imbalanced and the cost of false positives greatly differs from the cost of false negatives. We focus on classification trees, in particular oblique trees, which subsume both the traditional axis-aligned trees and logistic regression, but are more accurate than both while providing interpretable models. Rather than using ROC curves, we advocate a loss based on minimizing the false negatives subject to a maximum false positive rate, which we prove to be equivalent to minimizing a weighted 0/1 loss. This yields a curve of classifiers that provably dominates the ROC curve, but is hard to optimize due to the 0/1 loss. We give the first algorithm that can iteratively update the tree parameters globally so that the weighted 0/1 loss decreases monotonically. Experiments on various datasets with class imbalance or class costs show this indeed dominates ROC-based classifiers and significantly improves over previous approaches to learn trees based on weighted purity criteria or over- or undersampling",
    "checked": true,
    "id": "bec6d84a584d421d93bb055d72e6e033ddc7e4fe",
    "semantic_title": "beyond the roc curve: classification trees using cost-optimal curves, with application to imbalanced datasets",
    "citation_count": 0,
    "authors": [
      "Magzhan Gabidolla",
      "Arman Zharmagambetov",
      "Miguel Á. Carreira-Perpiñán"
    ]
  },
  "https://proceedings.mlr.press/v235/gabor24a.html": {
    "title": "Positive Concave Deep Equilibrium Models",
    "volume": "main",
    "abstract": "Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks. These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks. However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established. As a result, DEQ models are potentially unstable in practice. To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models. Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant. By imposing these constraints, we can easily ensure the existence and uniqueness of the fixed point without relying on additional complex assumptions commonly found in the DEQ literature, such as those based on monotone operator theory in convex analysis. Furthermore, the fixed point can be computed with the standard fixed point algorithm, and we provide theoretical guarantees of its geometric convergence, which, in particular, simplifies the training process. Experiments demonstrate the competitiveness of our pcDEQ models against other implicit models",
    "checked": true,
    "id": "12a50648c8748c41a2d83f2025a595a7308f5d1e",
    "semantic_title": "positive concave deep equilibrium models",
    "citation_count": 1,
    "authors": [
      "Mateusz Gabor",
      "Tomasz Piotrowski",
      "Renato L. G. Cavalcante"
    ]
  },
  "https://proceedings.mlr.press/v235/gadetsky24a.html": {
    "title": "Let Go of Your Labels with Unsupervised Transfer",
    "volume": "main",
    "abstract": "Foundation vision-language models have enabled remarkable zero-shot transferability of the pre-trained representations to a wide range of downstream tasks. However, to solve a new task, zero-shot transfer still necessitates human guidance to define visual categories that appear in the data. Here, we show that fully unsupervised transfer emerges when searching for the labeling of a dataset that induces maximal margin classifiers in representation spaces of different foundation models. We present TURTLE, a fully unsupervised method that effectively employs this guiding principle to uncover the underlying labeling of a downstream dataset without any supervision and task-specific representation learning. We evaluate TURTLE on a diverse benchmark suite of 26 datasets and show that it achieves new state-of-the-art unsupervised performance. Furthermore, TURTLE, although being fully unsupervised, outperforms zero-shot transfer baselines on a wide range of datasets. In particular, TURTLE matches the average performance of CLIP zero-shot on 26 datasets by employing the same representation space, spanning a wide range of architectures and model sizes. By guiding the search for the underlying labeling using the representation spaces of two foundation models, TURTLE surpasses zero-shot transfer and unsupervised prompt tuning baselines, demonstrating the surprising power and effectiveness of unsupervised transfer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artyom Gadetsky",
      "Yulun Jiang",
      "Maria Brbic"
    ]
  },
  "https://proceedings.mlr.press/v235/gadot24a.html": {
    "title": "Bring Your Own (Non-Robust) Algorithm to Solve Robust MDPs by Estimating The Worst Kernel",
    "volume": "main",
    "abstract": "Robust Markov Decision Processes (RMDPs) provide a framework for sequential decision-making that is robust to perturbations on the transition kernel. However, current RMDP methods are often limited to small-scale problems, hindering their use in high-dimensional domains. To bridge this gap, we present EWoK, a novel online approach to solve RMDP that Estimates the Worst transition Kernel to learn robust policies. Unlike previous works that regularize the policy or value updates, EWoK achieves robustness by simulating the worst scenarios for the agent while retaining complete flexibility in the learning process. Notably, EWoK can be applied on top of any off-the-shelf non-robust RL algorithm, enabling easy scaling to high-dimensional domains. Our experiments, spanning from simple Cartpole to high-dimensional DeepMind Control Suite environments, demonstrate the effectiveness and applicability of the EWoK paradigm as a practical method for learning robust policies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uri Gadot",
      "Kaixin Wang",
      "Navdeep Kumar",
      "Kfir Yehuda Levy",
      "Shie Mannor"
    ]
  },
  "https://proceedings.mlr.press/v235/gala24a.html": {
    "title": "Leverage Class-Specific Accuracy to Guide Data Generation for Improving Image Classification",
    "volume": "main",
    "abstract": "In many image classification applications, the number of labeled training images is limited, which leads to model overfitting. To mitigate the lack of training data, deep generative models have been leveraged to generate synthetic training data. However, existing methods generate data for individual classes based on how much training data they have without considering their actual data needs. To address this limitation, we propose needs-aware image generation, which automatically identifies the different data needs of individual classes based on their classification performance and divides a limited data generation budget into these classes according to their needs. We propose a multi-level optimization based framework which performs four learning stages in an end-to-end manner. Experiments on both imbalanced and balanced classification datasets demonstrate the effectiveness of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Gala",
      "Pengtao Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/gan24a.html": {
    "title": "Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Semi-supervised learning (SSL) has witnessed remarkable progress, resulting in the emergence of numerous method variations. However, practitioners often encounter challenges when attempting to deploy these methods due to their subpar performance. In this paper, we present a novel SSL approach named FineSSL that significantly addresses this limitation by adapting pre-trained foundation models. We identify the aggregated biases and cognitive deviation problems inherent in foundation models, and propose a simple yet effective solution by imposing balanced margin softmax and decoupled label smoothing. Through extensive experiments, we demonstrate that FineSSL sets a new state of the art for SSL on multiple benchmark datasets, reduces the training cost by over six times, and can seamlessly integrate various fine-tuning and modern SSL algorithms. The source code is available at https://github.com/Gank0078/FineSSL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Gan",
      "Tong Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/gan24b.html": {
    "title": "Reflective Policy Optimization",
    "volume": "main",
    "abstract": "On-policy reinforcement learning methods, like Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), often demand extensive data per update, leading to sample inefficiency. This paper introduces Reflective Policy Optimization (RPO), a novel on-policy extension that amalgamates past and future state-action information for policy optimization. This approach empowers the agent for introspection, allowing modifications to its actions within the current state. Theoretical analysis confirms that policy performance is monotonically improved and contracts the solution space, consequently expediting the convergence procedure. Empirical results demonstrate RPO's feasibility and efficacy in two reinforcement learning benchmarks, culminating in superior sample efficiency. The source code of this work is available at https://github.com/Edgargan/RPO",
    "checked": true,
    "id": "b5e91a4d8d05148ee50ae346ac156e92882ebd63",
    "semantic_title": "reflective policy optimization",
    "citation_count": 1,
    "authors": [
      "Yaozhong Gan",
      "Renye Yan",
      "Zhe Wu",
      "Junliang Xing"
    ]
  },
  "https://proceedings.mlr.press/v235/ganapathi-subramanian24a.html": {
    "title": "Confidence Aware Inverse Constrained Reinforcement Learning",
    "volume": "main",
    "abstract": "In coming up with solutions to real-world problems, humans implicitly adhere to constraints that are too numerous and complex to be specified completely. However, reinforcement learning (RL) agents need these constraints to learn the correct optimal policy in these settings. The field of Inverse Constraint Reinforcement Learning (ICRL) deals with this problem and provides algorithms that aim to estimate the constraints from expert demonstrations collected offline. Practitioners prefer to know a measure of confidence in the estimated constraints, before deciding to use these constraints, which allows them to only use the constraints that satisfy a desired level of confidence. However, prior works do not allow users to provide the desired level of confidence for the inferred constraints. This work provides a principled ICRL method that can take a confidence level with a set of expert demonstrations and outputs a constraint that is at least as constraining as the true underlying constraint with the desired level of confidence. Further, unlike previous methods, this method allows a user to know if the number of expert trajectories is insufficient to learn a constraint with a desired level of confidence, and therefore collect more expert trajectories as required to simultaneously learn constraints with the desired level of confidence and a policy that achieves the desired level of performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriram Ganapathi Subramanian",
      "Guiliang Liu",
      "Mohammed Elmahgiubi",
      "Kasra Rezaee",
      "Pascal Poupart"
    ]
  },
  "https://proceedings.mlr.press/v235/gangrade24a.html": {
    "title": "Testing the Feasibility of Linear Programs with Bandit Feedback",
    "volume": "main",
    "abstract": "While the recent literature has seen a surge in the study of constrained bandit problems, all existing methods for these begin by assuming the feasibility of the underlying problem. We initiate the study of testing such feasibility assumptions, and in particular address the problem in the linear bandit setting, thus characterising the costs of feasibility testing for an unknown linear program using bandit feedback. Concretely, we test if $\\exists x: Ax \\ge 0$ for an unknown $A \\in \\mathbb{R}^{m \\times d}$, by playing a sequence of actions $x_t\\in \\mathbb{R}^d$, and observing $Ax_t + \\mathrm{noise}$ in response. By identifying the hypothesis as determining the sign of the value of a minimax game, we construct a novel test based on low-regret algorithms and a nonasymptotic law of iterated logarithms. We prove that this test is reliable, and adapts to the ‘signal level,' $\\Gamma,$ of any instance, with mean sample costs scaling as $\\widetilde{O}(d^2/\\Gamma^2)$. We complement this by a minimax lower bound of $\\Omega(d/\\Gamma^2)$ for sample costs of reliable tests, dominating prior asymptotic lower bounds by capturing the dependence on $d$, and thus elucidating a basic insight missing in the extant literature on such problems",
    "checked": true,
    "id": "14dfe8498f57f4af3830ae326a2430ab5d0f2acf",
    "semantic_title": "testing the feasibility of linear programs with bandit feedback",
    "citation_count": 0,
    "authors": [
      "Aditya Gangrade",
      "Aditya Gopalan",
      "Venkatesh Saligrama",
      "Clayton Scott"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24a.html": {
    "title": "A Doubly Recursive Stochastic Compositional Gradient Descent Method for Federated Multi-Level Compositional Optimization",
    "volume": "main",
    "abstract": "Federated compositional optimization has been actively studied in the past few years. However, existing methods mainly focus on the two-level compositional optimization problem, which cannot be directly applied to the multi-level counterparts. Moreover, the convergence rate of existing federated two-level compositional optimization learning algorithms fails to achieve linear speedup with respect to the number of workers under heterogeneous settings. After identifying the reason for this failure, we developed a novel federated stochastic multi-level compositional optimization algorithm by introducing a novel Jacobian-vector product estimator. This innovation mitigates both the heterogeneity issue and the communication efficiency issue simultaneously. We then theoretically proved that our algorithm can achieve the level-independent and linear speedup convergence rate for nonconvex problems. To our knowledge, this is the first time that a federated learning algorithm can achieve such a favorable convergence rate for multi-level compositional problems. Moreover, experimental results confirm the efficacy of our algorithm",
    "checked": true,
    "id": "998369aa432e0fa0f5181d58b1502ab1baaeac51",
    "semantic_title": "a doubly recursive stochastic compositional gradient descent method for federated multi-level compositional optimization",
    "citation_count": 0,
    "authors": [
      "Hongchang Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24b.html": {
    "title": "Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining",
    "volume": "main",
    "abstract": "Backdoor defense is crucial to ensure the safety and robustness of machine learning models when under attack. However, most existing methods specialize in either the detection or removal of backdoors, but seldom both. While few works have addressed both, these methods rely on strong assumptions or entail significant overhead costs, such as the need of task-specific samples for detection and model retraining for removal. Hence, the key challenge is how to reduce overhead and relax unrealistic assumptions. In this work, we propose two Energy-Based BAckdoor defense methods, called EBBA and EBBA+, that can achieve both backdoored model detection and backdoor removal with low overhead. Our contributions are twofold: First, we offer theoretical analysis for our observation that a predefined target label is more likely to occur among the top results for various samples. Inspired by this, we develop an enhanced energy-based technique, called EBBA, to detect backdoored models without task-specific samples (i.e., samples from any tasks). Secondly, we theoretically analyze that after data corruption, the original clean label of a poisoned sample is more likely to be predicted as a top output by the model, a sharp contrast to clean samples. Accordingly, we extend EBBA to develop EBBA+, a new transferred energy approach to efficiently detect poisoned images and remove backdoors without model retraining. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of our methods over baselines in both backdoor detection and removal. Notably, the proposed methods can effectively detect backdoored model and poisoned images as well as remove backdoors at the same time",
    "checked": true,
    "id": "c4c369f853271c3f050b0be5163f7bf87af12683",
    "semantic_title": "energy-based backdoor defense without task-specific samples and model retraining",
    "citation_count": 1,
    "authors": [
      "Yudong Gao",
      "Honglong Chen",
      "Peng Sun",
      "Zhe Li",
      "Junjian Li",
      "Huajie Shao"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24c.html": {
    "title": "An Intrinsic Vector Heat Network",
    "volume": "main",
    "abstract": "Vector fields are widely used to represent and model flows for many science and engineering applications. This paper introduces a novel neural network architecture for learning tangent vector fields that are intrinsically defined on manifold surfaces embedded in 3D. Previous approaches to learning vector fields on surfaces treat vectors as multi-dimensional scalar fields, using traditional scalar-valued architectures to process channels individually, thus fail to preserve fundamental intrinsic properties of the vector field. The core idea of this work is to introduce a trainable vector heat diffusion module to spatially propagate vector-valued feature data across the surface, which we incorporate into our proposed architecture that consists of vector-valued neurons. Our architecture is invariant to rigid motion of the input, isometric deformation, and choice of local tangent bases, and is robust to discretizations of the surface. We evaluate our Vector Heat Network on triangle meshes, and empirically validate its invariant properties. We also demonstrate the effectiveness of our method on the useful industrial application of quadrilateral mesh generation",
    "checked": true,
    "id": "9a9baa006d5fcbe47eb73dccc1c513060430b872",
    "semantic_title": "an intrinsic vector heat network",
    "citation_count": 0,
    "authors": [
      "Alexander Gao",
      "Maurice Chu",
      "Mubbasir Kapadia",
      "Ming Lin",
      "Hsueh-Ti Derek Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24d.html": {
    "title": "Stochastic Weakly Convex Optimization beyond Lipschitz Continuity",
    "volume": "main",
    "abstract": "This paper considers stochastic weakly convex optimization without the standard Lipschitz continuity assumption. Based on new adaptive regularization (stepsize) strategies, we show that a wide class of stochastic algorithms, including the stochastic subgradient method, preserve the $\\mathcal{O} ( 1 / \\sqrt{K})$ convergence rate with constant failure rate. Our analyses rest on rather weak assumptions: the Lipschitz parameter can be either bounded by a general growth function of $\\\\|x\\\\|$ or locally estimated through independent random samples. Numerical experiments demonstrate the efficiency and robustness of our proposed stepsize policies",
    "checked": true,
    "id": "93c8e4e31cead017b8d2e915626df38575c48a3f",
    "semantic_title": "stochastic weakly convex optimization beyond lipschitz continuity",
    "citation_count": 1,
    "authors": [
      "Wenzhi Gao",
      "Qi Deng"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24e.html": {
    "title": "A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer",
    "volume": "main",
    "abstract": "Can we model Non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The Non-Euclidean property have posed a long term challenge in graph modeling. Despite recent graph neural networks and graph transformers efforts encoding graphs as Euclidean vectors, recovering the original graph from vectors remains a challenge. In this paper, we introduce GraphsGPT, featuring an Graph2Seq encoder that transforms Non-Euclidean graphs into learnable Graph Words in the Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from Graph Words to ensure information equivalence. We pretrain GraphsGPT on $100$M molecules and yield some interesting findings: (1) The pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on $8/9$ graph classification and regression tasks. (2) The pretrained GraphGPT serves as a strong graph generator, demonstrated by its strong ability to perform both few-shot and conditional graph generation. (3) Graph2Seq+GraphGPT enables effective graph mixup in the Euclidean space, overcoming previously known Non-Euclidean challenges. (4) The edge-centric pretraining framework GraphsGPT demonstrates its efficacy in graph domain tasks, excelling in both representation and generation. Code is available at https://github.com/A4Bio/GraphsGPT",
    "checked": false,
    "id": "35cf5323e411d7717033892e62412f47c06c3eb2",
    "semantic_title": "a graph is worth k words: euclideanizing graph using pure transformer",
    "citation_count": 4,
    "authors": [
      "Zhangyang Gao",
      "Daize Dong",
      "Cheng Tan",
      "Jun Xia",
      "Bozhen Hu",
      "Stan Z. Li"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24f.html": {
    "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback",
    "volume": "main",
    "abstract": "The success of AI assistants based on Language Models (LLMs) hinges on Reinforcement Learning from Human Feedback (RLHF) to comprehend and align with user intentions. However, traditional alignment algorithms, such as PPO, are hampered by complex annotation and training requirements. This reliance limits the applicability of RLHF and hinders the development of professional assistants tailored to diverse human preferences. In this work, we introduce Linear Alignment, a novel algorithm that aligns language models with human preferences in one single inference step, eliminating the reliance on data annotation and model training. Linear alignment incorporates a new parameterization for policy optimization under divergence constraints, which enables the extraction of optimal policy in a closed-form manner and facilitates the direct estimation of the aligned response. Extensive experiments on both general and personalized preference datasets demonstrate that linear alignment significantly enhances the performance and efficiency of LLM alignment across diverse scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songyang Gao",
      "Qiming Ge",
      "Wei Shen",
      "Shihan Dou",
      "Junjie Ye",
      "Xiao Wang",
      "Rui Zheng",
      "Yicheng Zou",
      "Zhi Chen",
      "Hang Yan",
      "Qi Zhang",
      "Dahua Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24g.html": {
    "title": "Multi-Agent Reinforcement Learning Meets Leaf Sequencing in Radiotherapy",
    "volume": "main",
    "abstract": "In contemporary radiotherapy planning (RTP), a key module leaf sequencing is predominantly addressed by optimization-based approaches. In this paper, we propose a novel deep reinforcement learning (DRL) model termed as Reinforced Leaf Sequencer (RLS) in a multi-agent framework for leaf sequencing. The RLS model offers improvements to time-consuming iterative optimization steps via large-scale training and can control movement patterns through the design of reward mechanisms. We have conducted experiments on four datasets with four metrics and compared our model with a leading optimization sequencer. Our findings reveal that the proposed RLS model can achieve reduced fluence reconstruction errors, and potential faster convergence when integrated in an optimization planner. Additionally, RLS has shown promising results in a full artificial intelligence RTP pipeline. We hope this pioneer multi-agent RL leaf sequencer can foster future research on machine learning for RTP",
    "checked": true,
    "id": "b65638b62b490adc7b91abe71d85cab47ea6efb7",
    "semantic_title": "multi-agent reinforcement learning meets leaf sequencing in radiotherapy",
    "citation_count": 0,
    "authors": [
      "Riqiang Gao",
      "Florin-Cristian Ghesu",
      "Simon Arberet",
      "Shahab Basiri",
      "Esa Kuusela",
      "Martin Kraus",
      "Dorin Comaniciu",
      "Ali Kamen"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24h.html": {
    "title": "DMTG: One-Shot Differentiable Multi-Task Grouping",
    "volume": "main",
    "abstract": "We aim to address Multi-Task Learning (MTL) with a large number of tasks by Multi-Task Grouping (MTG). Given $N$ tasks, we propose to simultaneously identify the best task groups from $2^N$ candidates and train the model weights simultaneously in one-shot, with the high-order task-affinity fully exploited. This is distinct from the pioneering methods which sequentially identify the groups and train the model weights, where the group identification often relies on heuristics. As a result, our method not only improves the training efficiency, but also mitigates the objective bias introduced by the sequential procedures that potentially leads to a suboptimal solution. Specifically, we formulate MTG as a fully differentiable pruning problem on an adaptive network architecture determined by an unknown Categorical distribution. To categorize $N$ tasks into $K$ groups (represented by $K$ encoder branches), we initially set up $KN$ task heads, where each branch connects to all $N$ task heads to exploit the high-order task-affinity. Then, we gradually prune the $KN$ heads down to $N$ by learning a relaxed differentiable Categorical distribution, ensuring that each task is exclusively and uniquely categorized into only one branch. Extensive experiments on CelebA and Taskonomy datasets with detailed ablations show the promising performance and efficiency of our method. The codes are available at https://github.com/ethanygao/DMTG",
    "checked": true,
    "id": "d3ccb2630066c3081004356ca00e9404ba025e3b",
    "semantic_title": "dmtg: one-shot differentiable multi-task grouping",
    "citation_count": 1,
    "authors": [
      "Yuan Gao",
      "Shuguo Jiang",
      "Moran Li",
      "Jin-Gang Yu",
      "Gui-Song Xia"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24i.html": {
    "title": "Private Heterogeneous Federated Learning Without a Trusted Server Revisited: Error-Optimal and Communication-Efficient Algorithms for Convex Losses",
    "volume": "main",
    "abstract": "We revisit the problem of federated learning (FL) with private data from people who do not trust the server or other silos/clients. In this context, every silo (e.g. hospital) has data from several people (e.g. patients) and needs to protect the privacy of each person's data (e.g. health records), even if the server and/or other silos try to uncover this data. Inter-Silo Record-Level Differential Privacy (ISRL-DP) prevents each silo's data from being leaked, by requiring that silo $i$'s communications satisfy item-level differential privacy. Prior work (Lowy & Razaviyayn, 2023a) characterized the optimal excess risk bounds for ISRL-DP algorithms with homogeneous (i.i.d.) silo data and convex loss functions. However, two important questions were left open: 1) Can the same excess risk bounds be achieved with heterogeneous (non-i.i.d.) silo data? 2) Can the optimal risk bounds be achieved with fewer communication rounds? In this paper, we give positive answers to both questions. We provide novel ISRL-DP FL algorithms that achieve the optimal excess risk bounds in the presence of heterogeneous silo data. Moreover, our algorithms are more communication-efficient than the prior state-of-the-art. For smooth loss functions, our algorithm achieves the optimal excess risk bound and has communication complexity that matches the non-private lower bound. Additionally, our algorithms are more computationally efficient than the previous state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changyu Gao",
      "Andrew Lowy",
      "Xingyu Zhou",
      "Stephen Wright"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24j.html": {
    "title": "Speech Self-Supervised Learning Using Diffusion Model Synthetic Data",
    "volume": "main",
    "abstract": "While self-supervised learning (SSL) in speech has greatly reduced the reliance of speech processing systems on annotated corpora, the success of SSL still hinges on the availability of a large-scale unannotated corpus, which is still often impractical for many low-resource languages or under privacy concerns. Some existing work seeks to alleviate the problem by data augmentation, but most works are confined to introducing perturbations to real speech and do not introduce new variations in speech prosody, speakers, and speech content, which are important for SSL. Motivated by the recent finding that diffusion models have superior capabilities for modeling data distributions, we propose DiffS4L, a pretraining scheme that augments the limited unannotated data with synthetic data with different levels of variations, generated by a diffusion model trained on the limited unannotated data. Finally, an SSL model is pre-trained on the real and the synthetic speech. Our experiments show that DiffS4L can significantly improve the performance of SSL models, such as reducing the WER of the HuBERT pretrained model by 6.26 percentage points in the English ASR task. Notably, we find that the synthetic speech with all levels of variations, i.e. new prosody, new speakers, and even new content (despite the new content being mostly babble), accounts for significant performance improvement. The code is available at github.com/Hertin/DiffS4L",
    "checked": true,
    "id": "1c73ba4bf69817d085214d54771430e2f19d9e8c",
    "semantic_title": "speech self-supervised learning using diffusion model synthetic data",
    "citation_count": 1,
    "authors": [
      "Heting Gao",
      "Kaizhi Qian",
      "Junrui Ni",
      "Chuang Gan",
      "Mark A. Hasegawa-Johnson",
      "Shiyu Chang",
      "Yang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24k.html": {
    "title": "Rethinking Specificity in SBDD: Leveraging Delta Score and Energy-Guided Diffusion",
    "volume": "main",
    "abstract": "In the field of Structure-based Drug Design (SBDD), deep learning-based generative models have achieved outstanding performance in terms of docking score. However, further study shows that the existing molecular generative methods and docking scores both have lacked consideration in terms of specificity, which means that generated molecules bind to almost every protein pocket with high affinity. To address this, we introduce the Delta Score, a new metric for evaluating the specificity of molecular binding. To further incorporate this insight for generation, we develop an innovative energy-guided approach using contrastive learning, with active compounds as decoys, to direct generative models toward creating molecules with high specificity. Our empirical results show that this method not only enhances the delta score but also maintains or improves traditional docking scores, successfully bridging the gap between SBDD and real-world needs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Gao",
      "Minsi Ren",
      "Yuyan Ni",
      "Yanwen Huang",
      "Bo Qiang",
      "Zhi-Ming Ma",
      "Wei-Ying Ma",
      "Yanyan Lan"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24l.html": {
    "title": "Non-convex Stochastic Composite Optimization with Polyak Momentum",
    "volume": "main",
    "abstract": "The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Gao",
      "Anton Rodomanov",
      "Sebastian U Stich"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24m.html": {
    "title": "Adaptive-Gradient Policy Optimization: Enhancing Policy Learning in Non-Smooth Differentiable Simulations",
    "volume": "main",
    "abstract": "Recent advancements in differentiable simulators highlight the potential of policy optimization using simulation gradients. Yet, these approaches are largely contingent on the continuity and smoothness of the simulation, which precludes the use of certain simulation engines, such as Mujoco. To tackle this challenge, we introduce the adaptive analytic gradient. This method views the Q function as a surrogate for future returns, consistent with the Bellman equation. By analyzing the variance of batched gradients, our method can autonomously opt for a more resilient Q function to compute the gradient when encountering rough simulation transitions. We also put forth the Adaptive-Gradient Policy Optimization (AGPO) algorithm, which leverages our proposed method for policy learning. On the theoretical side, we demonstrate AGPO's convergence, emphasizing its stable performance under non-smooth dynamics due to low variance. On the empirical side, our results show that AGPO effectively mitigates the challenges posed by non-smoothness in policy learning through differentiable simulation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Gao",
      "Liangzhi Shi",
      "Shenao Zhang",
      "Zhaoran Wang",
      "Yi Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24n.html": {
    "title": "Decoupling Learning and Decision-Making: Breaking the $\\mathcalO(\\sqrtT)$ Barrier in Online Resource Allocation with First-Order Methods",
    "volume": "main",
    "abstract": "Online linear programming plays an important role in both revenue management and resource allocation, and recent research has focused on developing efficient first-order online learning algorithms. Despite the empirical success of first-order methods, they typically achieve regret no better than $\\mathcal{O}(\\sqrt{T})$, which is suboptimal compared to the $\\mathcal{O}(\\log T)$ result guaranteed by the state-of-the-art linear programming (LP)-based online algorithms. This paper establishes several important facts about online linear programming, which unveils the challenge for first-order online algorithms to achieve beyond $\\mathcal{O}(\\sqrt{T})$ regret. To address this challenge, we introduce a new algorithmic framework which decouples learning from decision-making. For the first time, we show that first-order methods can achieve regret $\\mathcal{O}(T^{1/3})$ with this new framework",
    "checked": false,
    "id": "ab8d84fb50ca262003e5569fb20e5a47f3042f54",
    "semantic_title": "decoupling learning and decision-making: breaking the o(√t) barrier in online resource allocation with first-order methods",
    "citation_count": 0,
    "authors": [
      "Wenzhi Gao",
      "Chunlin Sun",
      "Chenyu Xue",
      "Yinyu Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24o.html": {
    "title": "Parameter-Efficient Fine-Tuning with Discrete Fourier Transform",
    "volume": "main",
    "abstract": "Low-rank adaptation (LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats $\\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\\Delta W$. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. Our code is released at this link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Gao",
      "Qichao Wang",
      "Aochuan Chen",
      "Zijing Liu",
      "Bingzhe Wu",
      "Liang Chen",
      "Jia Li"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24p.html": {
    "title": "Fast-Slow Test-Time Adaptation for Online Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "The ability to accurately comprehend natural language instructions and navigate to the target location is essential for an embodied agent. Such agents are typically required to execute user instructions in an online manner, leading us to explore the use of unlabeled test samples for effective online model adaptation. However, for online Vision-and-Language Navigation (VLN), due to the intrinsic nature of inter-sample online instruction execution and intra-sample multi-step action decision, frequent updates can result in drastic changes in model parameters, while occasional updates can make the model ill-equipped to handle dynamically changing environments. Therefore, we propose a Fast-Slow Test-Time Adaptation (FSTTA) approach for online VLN by performing joint decomposition-accumulation analysis for both gradients and parameters in a unified framework. Extensive experiments show that our method obtains impressive performance gains on four popular benchmarks. Code is available at https://github.com/Feliciaxyao/ICML2024-FSTTA",
    "checked": true,
    "id": "a6946bdaf38f3b6e34d38a1a57483978777b3e66",
    "semantic_title": "fast-slow test-time adaptation for online vision-and-language navigation",
    "citation_count": 1,
    "authors": [
      "Junyu Gao",
      "Xuan Yao",
      "Changsheng Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24q.html": {
    "title": "Causal Customer Churn Analysis with Low-rank Tensor Block Hazard Model",
    "volume": "main",
    "abstract": "This study introduces an innovative method for analyzing the impact of various interventions on customer churn, using the potential outcomes framework. We present a new causal model, the tensorized latent factor block hazard model, which incorporates tensor completion methods for a principled causal analysis of customer churn. A crucial element of our approach is the formulation of a 1-bit tensor completion for the parameter tensor. This captures hidden customer characteristics and temporal elements from churn records, effectively addressing the binary nature of churn data and its time-monotonic trends. Our model also uniquely categorizes interventions by their similar impacts, enhancing the precision and practicality of implementing customer retention strategies. For computational efficiency, we apply a projected gradient descent algorithm combined with spectral clustering. We lay down the theoretical groundwork for our model, including its non-asymptotic properties. The efficacy and superiority of our model are further validated through comprehensive experiments on both simulated and real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyin Gao",
      "Zhiming Zhang",
      "Shu Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24r.html": {
    "title": "Prompt-based Visual Alignment for Zero-shot Policy Transfer",
    "volume": "main",
    "abstract": "Overfitting in RL has become one of the main obstacles to applications in reinforcement learning(RL). Existing methods do not provide explicit semantic constrain for the feature extractor, hindering the agent from learning a unified cross-domain representation and resulting in performance degradation on unseen domains. Besides, abundant data from multiple domains are needed. To address these issues, in this work, we propose prompt-based visual alignment (PVA), a robust framework to mitigate the detrimental domain bias in the image for zero-shot policy transfer. Inspired that Visual-Language Model (VLM) can serve as a bridge to connect both text space and image space, we leverage the semantic information contained in a text sequence as an explicit constraint to train a visual aligner. Thus, the visual aligner can map images from multiple domains to a unified domain and achieve good generalization performance. To better depict semantic information, prompt tuning is applied to learn a sequence of learnable tokens. With explicit constraints of semantic information, PVA can learn unified cross-domain representation under limited access to cross-domain data and achieves great zero-shot generalization ability in unseen domains. We verify PVA on a vision-based autonomous driving task with CARLA simulator. Experiments show that the agent generalizes well on unseen domains under limited access to multi-domain data",
    "checked": true,
    "id": "b57f4356a6a0e51cc97e032b89fdc3db0e66cc59",
    "semantic_title": "prompt-based visual alignment for zero-shot policy transfer",
    "citation_count": 0,
    "authors": [
      "Haihan Gao",
      "Rui Zhang",
      "Qi Yi",
      "Hantao Yao",
      "Haochen Li",
      "Jiaming Guo",
      "Shaohui Peng",
      "Yunkai Gao",
      "Qicheng Wang",
      "Xing Hu",
      "Yuanbo Wen",
      "Zihao Zhang",
      "Zidong Du",
      "Ling Li",
      "Qi Guo",
      "Yunji Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/gao24s.html": {
    "title": "Distribution Alignment Optimization through Neural Collapse for Long-tailed Classification",
    "volume": "main",
    "abstract": "A well-trained deep neural network on balanced datasets usually exhibits the Neural Collapse (NC) phenomenon, which is an informative indicator of the model achieving good performance. However, NC is usually hard to be achieved for a model trained on long-tailed datasets, leading to the deteriorated performance of test data. This work aims to induce the NC phenomenon in imbalanced learning from the perspective of distribution matching. By enforcing the distribution of last-layer representations to align the ideal distribution of the ETF structure, we develop a Distribution Alignment Optimization (DisA) loss, acting as a plug-and-play method can be combined with most of the existing long-tailed methods, we further instantiate it to the cases of fixing classifier and learning classifier. The extensive experiments show the effectiveness of DisA, providing a promising solution to the imbalanced issue. Our code is available at DisA",
    "checked": true,
    "id": "d82d8e69260992855b75020f916114effcbab09c",
    "semantic_title": "distribution alignment optimization through neural collapse for long-tailed classification",
    "citation_count": 3,
    "authors": [
      "Jintong Gao",
      "He Zhao",
      "Dan Dan Guo",
      "Hongyuan Zha"
    ]
  },
  "https://proceedings.mlr.press/v235/garber24a.html": {
    "title": "Projection-Free Online Convex Optimization with Time-Varying Constraints",
    "volume": "main",
    "abstract": "We consider the setting of online convex optimization with adversarial time-varying constraints in which actions must be feasible w.r.t. a fixed constraint set, and are also required on average to approximately satisfy additional time-varying constraints. Motivated by scenarios in which the fixed feasible set (hard constraint) is difficult to project on, we consider projection-free algorithms that access this set only through a linear optimization oracle (LOO). We present an algorithm that, on a sequence of length $T$ and using overall $T$ calls to the LOO, guarantees $\\tilde{O}(T^{3/4})$ regret w.r.t. the losses and $O(T^{7/8})$ constraints violation (ignoring all quantities except for $T$). In particular, these bounds hold w.r.t. any interval of the sequence. This algorithm however also requires access to an oracle for minimizing a strongly convex nonsmooth function over a Euclidean ball. We present a more efficient algorithm that does not require the latter optimization oracle but only first-order access to the time-varying constraints, and achieves similar bounds w.r.t. the entire sequence. We extend the latter to the setting of bandit feedback and obtain similar bounds (as a function of $T$) in expectation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Garber",
      "Ben Kretzu"
    ]
  },
  "https://proceedings.mlr.press/v235/garcin24a.html": {
    "title": "DRED: Zero-Shot Transfer in Reinforcement Learning via Data-Regularised Environment Design",
    "volume": "main",
    "abstract": "Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when these environments share characteristics with the ones they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which assume control over level generation. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce data-regularised environment design (DRED). DRED generates levels using a generative model trained to approximate the ground truth distribution of an initial set of level parameters. Through its grounding, DRED achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods",
    "checked": true,
    "id": "44115f2c0cdd038d7c5956a3e99e910ab020d448",
    "semantic_title": "dred: zero-shot transfer in reinforcement learning via data-regularised environment design",
    "citation_count": 6,
    "authors": [
      "Samuel Garcin",
      "James Doran",
      "Shangmin Guo",
      "Christopher G. Lucas",
      "Stefano V Albrecht"
    ]
  },
  "https://proceedings.mlr.press/v235/gardner24a.html": {
    "title": "LLark: A Multimodal Instruction-Following Language Model for Music",
    "volume": "main",
    "abstract": "Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, reasoning), we show that LLark matches or outperforms existing baselines in music understanding, and that humans show a high degree of agreement with its responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https://github.com/spotify-research/llark",
    "checked": true,
    "id": "91eaf9752002f6999a3bd8d33303cdcdbd135317",
    "semantic_title": "llark: a multimodal instruction-following language model for music",
    "citation_count": 7,
    "authors": [
      "Joshua P Gardner",
      "Simon Durand",
      "Daniel Stoller",
      "Rachel M Bittner"
    ]
  },
  "https://proceedings.mlr.press/v235/garg24a.html": {
    "title": "Memorization Through the Lens of Curvature of Loss Function Around Samples",
    "volume": "main",
    "abstract": "Deep neural networks are over-parameterized and easily overfit to and memorize the datasets that they train on. In the extreme case, it has been shown that networks can memorize a randomly labeled dataset. In this paper, we propose using the curvature of the loss function around each training sample, averaged over training epochs, as a measure of memorization of a sample. We show that this curvature metric effectively captures memorization statistics, both qualitatively and quantitatively in popular image datasets. We provide quantitative validation of the proposed metric against memorization scores released by Feldman & Zhang (2020). Further, experiments on mislabeled data detection show that corrupted samples are learned with high curvature and using curvature for identifying mislabelled examples outperforms existing approaches. Qualitatively, we find that high curvature samples correspond to long-tailed, mislabeled, or conflicting instances, indicating a likelihood of memorization. Notably, this analysis helps us find, to the best of our knowledge, a novel failure mode on the CIFAR100 and ImageNet datasets: that of duplicated images with differing labels",
    "checked": true,
    "id": "ae9a30ed24edab448cdb7a2e5269deb9713d2048",
    "semantic_title": "memorization through the lens of curvature of loss function around samples",
    "citation_count": 7,
    "authors": [
      "Isha Garg",
      "Deepak Ravikumar",
      "Kaushik Roy"
    ]
  },
  "https://proceedings.mlr.press/v235/gatmiry24a.html": {
    "title": "Simplicity Bias via Global Convergence of Sharpness Minimization",
    "volume": "main",
    "abstract": "The remarkable generalization ability of neural networks is usually attributed to the implicit bias of SGD, which often yields models with lower complexity using simpler (e.g. linear) and low-rank features. Recent works have provided empirical and theoretical evidence for the bias of particular variants of SGD (such as label noise SGD) toward flatter regions of the loss landscape. Despite the folklore intuition that flat solutions are 'simple', the connection with the simplicity of the final trained model (e.g. low-rank) is not well understood. In this work, we take a step toward bridging this gap by studying the simplicity structure that arises from minimizers of the sharpness for a class of two-layer neural networks. We show that, for any high dimensional training data and certain activations, with small enough step size, label noise SGD always converges to a network that replicates a single linear feature across all neurons; thereby implying a simple rank one feature matrix. To obtain this result, our main technical contribution is to show that label noise SGD always minimizes the sharpness on the manifold of models with zero loss for two-layer networks. Along the way, we discover a novel property — a local geodesic convexity — of the trace of Hessian of the loss at approximate stationary points on the manifold of zero loss, which links sharpness to the geometry of the manifold. This tool may be of independent interest",
    "checked": true,
    "id": "0ab20995ed9d1c02dec42ca0cf4fd11774a8bf7d",
    "semantic_title": "simplicity bias via global convergence of sharpness minimization",
    "citation_count": 0,
    "authors": [
      "Khashayar Gatmiry",
      "Zhiyuan Li",
      "Sashank J. Reddi",
      "Stefanie Jegelka"
    ]
  },
  "https://proceedings.mlr.press/v235/gatmiry24b.html": {
    "title": "Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?",
    "volume": "main",
    "abstract": "Transformers to do reasoning and few-shot learning, without any fine-tuning, is widely conjectured to stem from their ability to implicitly simulate a multi-step algorithms – such as gradient descent – with their weights in a single forward pass. Recently, there has been progress in understanding this complex phenomenon from an expressivity point of view, by demonstrating that Transformers can express such multi-step algorithms. However, our knowledge about the more fundamental aspect of its learnability, beyond single layer models, is very limited. In particular, can training Transformers enable convergence to algorithmic solutions? In this work we resolve this for in context linear regression with linear looped Transformers – a multi-layer model with weight sharing that is conjectured to have an inductive bias to learn fix-point iterative algorithms. More specifically, for this setting we show that the global minimizer of the population training loss implements multi-step preconditioned gradient descent, with a preconditioner that adapts to the data distribution. Furthermore, we show a fast convergence for gradient flow on the regression loss, despite the non-convexity of the landscape, by proving a novel gradient dominance condition. To our knowledge, this is the first theoretical analysis for multi-layer Transformer in this setting. We further validate our theoretical findings through synthetic experiments",
    "checked": true,
    "id": "32ca1dbc8d42d6edf4981fba9a9a83d32bec3179",
    "semantic_title": "can looped transformers learn to implement multi-step gradient descent for in-context learning?",
    "citation_count": 8,
    "authors": [
      "Khashayar Gatmiry",
      "Nikunj Saunshi",
      "Sashank J. Reddi",
      "Stefanie Jegelka",
      "Sanjiv Kumar"
    ]
  },
  "https://proceedings.mlr.press/v235/gaur24a.html": {
    "title": "Closing the Gap: Achieving Global Convergence (Last Iterate) of Actor-Critic under Markovian Sampling with Neural Network Parametrization",
    "volume": "main",
    "abstract": "The current state-of-the-art theoretical analysis of Actor-Critic (AC) algorithms significantly lags in addressing the practical aspects of AC implementations. This crucial gap needs bridging to bring the analysis in line with practical implementations of AC. To address this, we advocate for considering the MMCLG criteria: Multi-layer neural network parametrization for actor/critic, Markovian sampling, Continuous state-action spaces, the performance of the Last iterate, and Global optimality. These aspects are practically significant and have been largely overlooked in existing theoretical analyses of AC algorithms. In this work, we address these gaps by providing the first comprehensive theoretical analysis of AC algorithms that encompasses all five crucial practical aspects (covers MMCLG criteria). We establish global convergence sample complexity bounds of $\\tilde{\\mathcal{O}}\\left( \\epsilon^{-3} \\right)$. We achieve this result through our novel use of the weak gradient domination property of MDP's and our unique analysis of the error in critic estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mudit Gaur",
      "Amrit Bedi",
      "Di Wang",
      "Vaneet Aggarwal"
    ]
  },
  "https://proceedings.mlr.press/v235/gautam24a.html": {
    "title": "Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models",
    "volume": "main",
    "abstract": "Fine-tuning language models (LMs) has demonstrated success in a wide array of downstream tasks. However, as LMs are scaled up, the memory requirements for backpropagation become prohibitively high. Zeroth-order (ZO) optimization methods can leverage memory-efficient forward passes to estimate gradients. More recently, MeZO, an adaptation of ZO-SGD, has been shown to consistently outperform zero-shot and in-context learning when combined with suitable task prompts. In this work, we couple ZO methods with variance reduction techniques to enhance stability and convergence for inference-based LM fine-tuning. We introduce Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient (MeZO-SVRG) and demonstrate its efficacy across multiple LM fine-tuning tasks, eliminating the reliance on task-specific prompts. Evaluated across a range of both masked and autoregressive LMs on benchmark GLUE tasks, MeZO-SVRG outperforms MeZO with up to 20% increase in test accuracies in both full- and partial-parameter fine-tuning settings. MeZO-SVRG benefits from reduced computation time as it often surpasses MeZO's peak test accuracy with a $2\\times$ reduction in GPU-hours. MeZO-SVRG significantly reduces the required memory footprint compared to first-order SGD, i.e. by $2\\times$ for autoregressive models. Our experiments highlight that MeZO-SVRG's memory savings progressively improve compared to SGD with larger batch sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanmay Gautam",
      "Youngsuk Park",
      "Hao Zhou",
      "Parameswaran Raman",
      "Wooseok Ha"
    ]
  },
  "https://proceedings.mlr.press/v235/gavranovic24a.html": {
    "title": "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures",
    "volume": "main",
    "abstract": "We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory—precisely, the universal algebra of monads valued in a 2-category of parametric maps—as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory",
    "checked": true,
    "id": "32353446ed56d8606fa605e4087b44dc54674cc1",
    "semantic_title": "position: categorical deep learning is an algebraic theory of all architectures",
    "citation_count": 6,
    "authors": [
      "Bruno Gavranović",
      "Paul Lessard",
      "Andrew Joseph Dudzik",
      "Tamara Von Glehn",
      "João Guilherme Madeira Araújo",
      "Petar Veličković"
    ]
  },
  "https://proceedings.mlr.press/v235/ge24a.html": {
    "title": "Masked Face Recognition with Generative-to-Discriminative Representations",
    "volume": "main",
    "abstract": "Masked face recognition is important for social good but challenged by diverse occlusions that cause insufficient or inaccurate representations. In this work, we propose a unified deep network to learn generative-to-discriminative representations for facilitating masked face recognition. To this end, we split the network into three modules and learn them on synthetic masked faces in a greedy module-wise pretraining manner. First, we leverage a generative encoder pretrained for face inpainting and finetune it to represent masked faces into category-aware descriptors. Attribute to the generative encoder's ability in recovering context information, the resulting descriptors can provide occlusion-robust representations for masked faces, mitigating the effect of diverse masks. Then, we incorporate a multi-layer convolutional network as a discriminative reformer and learn it to convert the category-aware descriptors into identity-aware vectors, where the learning is effectively supervised by distilling relation knowledge from off-the-shelf face recognition model. In this way, the discriminative reformer together with the generative encoder serves as the pretrained backbone, providing general and discriminative representations towards masked faces. Finally, we cascade one fully-connected layer following by one softmax layer into a feature classifier and finetune it to identify the reformed identity-aware vectors. Extensive experiments on synthetic and realistic datasets demonstrate the effectiveness of our approach in recognizing masked faces",
    "checked": true,
    "id": "95087cb126f0745b58c3f423b1e76eae23f74354",
    "semantic_title": "masked face recognition with generative-to-discriminative representations",
    "citation_count": 1,
    "authors": [
      "Shiming Ge",
      "Weijia Guo",
      "Chenyu Li",
      "Zhang Junzheng",
      "Yong Li",
      "Dan Zeng"
    ]
  },
  "https://proceedings.mlr.press/v235/ge24b.html": {
    "title": "Safe and Robust Subgame Exploitation in Imperfect Information Games",
    "volume": "main",
    "abstract": "Opponent exploitation is an important task for players to exploit the weaknesses of others in games. Existing approaches mainly focus on balancing between exploitation and exploitability but are often vulnerable to modeling errors and deceptive adversaries. To address this problem, our paper offers a novel perspective on the safety of opponent exploitation, named Adaptation Safety. This concept leverages the insight that strategies, even those not explicitly aimed at opponent exploitation, may inherently be exploitable due to computational complexities, rendering traditional safety overly rigorous. In contrast, adaptation safety requires that the strategy should not be more exploitable than it would be in scenarios where opponent exploitation is not considered. Building on such adaptation safety, we further propose an Opponent eXploitation Search (OX-Search) framework by incorporating real-time search techniques for efficient online opponent exploitation. Moreover, we provide theoretical analyses to show the adaptation safety and robust exploitation of OX-Search, even with inaccurate opponent models. Empirical evaluations in popular poker games demonstrate OX-Search's superiority in both exploitability and exploitation compared to previous methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxing Ge",
      "Zheng Xu",
      "Tianyu Ding",
      "Linjian Meng",
      "Bo An",
      "Wenbin Li",
      "Yang Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/gedon24a.html": {
    "title": "No Double Descent in Principal Component Regression: A High-Dimensional Analysis",
    "volume": "main",
    "abstract": "Understanding the generalization properties of large-scale models necessitates incorporating realistic data assumptions into the analysis. Therefore, we consider Principal Component Regression (PCR)—combining principal component analysis and linear regression—on data from a low-dimensional manifold. We present an analysis of PCR when the data is sampled from a spiked covariance model, obtaining fundamental asymptotic guarantees for the generalization risk of this model. Our analysis is based on random matrix theory and allows us to provide guarantees for high-dimensional data. We additionally present an analysis of the distribution shift between training and test data. The results allow us to disentangle the effects of (1) the number of parameters, (2) the data-generating model and, (3) model misspecification on the generalization risk. The use of PCR effectively regularizes the model and prevents the interpolation peak of the double descent. Our theoretical findings are empirically validated in simulation, demonstrating their practical relevance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Gedon",
      "Antonio H. Ribeiro",
      "Thomas B. Schön"
    ]
  },
  "https://proceedings.mlr.press/v235/geirhos24a.html": {
    "title": "Don't trust your eyes: on the (un)reliability of feature visualizations",
    "volume": "main",
    "abstract": "How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to \"explain\" how neural networks process natural images. This can be used as a sanity check for feature visualizations. We underpin our empirical findings by theory proving that the set of functions that can be reliably understood by feature visualization is extremely small and does not include general black-box neural networks. Therefore, a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Geirhos",
      "Roland S. Zimmermann",
      "Blair Bilodeau",
      "Wieland Brendel",
      "Been Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/geist24a.html": {
    "title": "Learning with 3D rotations, a hitchhiker's guide to SO(3)",
    "volume": "main",
    "abstract": "Many settings in machine learning require the selection of a rotation representation. However, choosing a suitable representation from the many available options is challenging. This paper acts as a survey and guide through rotation representations. We walk through their properties that harm or benefit deep learning with gradient-based optimization. By consolidating insights from rotation-based learning, we provide a comprehensive overview of learning functions with rotation representations. We provide guidance on selecting representations based on whether rotations are in the model's input or output and whether the data primarily comprises small angles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas René Geist",
      "Jonas Frey",
      "Mikel Zhobro",
      "Anna Levina",
      "Georg Martius"
    ]
  },
  "https://proceedings.mlr.press/v235/genalti24a.html": {
    "title": "Graph-Triggered Rising Bandits",
    "volume": "main",
    "abstract": "In this paper, we propose a novel generalization of rested and restless bandits where the evolution of the arms' expected rewards is governed by a graph defined over the arms. An edge connecting a pair of arms $(i,j)$ represents the fact that a pull of arm $i$ triggers the evolution of arm $j$, and vice versa. Interestingly, rested and restless bandits are both special cases of our model for some suitable (degenerate) graphs. Still, the model can represent way more general and interesting scenarios. We first tackle the problem of computing the optimal policy when no specific structure is assumed on the graph, showing that it is NP-hard. Then, we focus on a specific structure forcing the graph to be composed of a set of fully connected subgraphs (i.e., cliques), and we prove that the optimal policy can be easily computed in closed form. Then, we move to the learning problem presenting regret minimization algorithms for deterministic and stochastic cases. Our regret bounds highlight the complexity of the learning problem by incorporating instance-dependent terms that encode specific properties of the underlying graph structure. Moreover, we illustrate how the knowledge of the underlying graph is not necessary for achieving the no-regret property",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianmarco Genalti",
      "Marco Mussi",
      "Nicola Gatti",
      "Marcello Restelli",
      "Matteo Castiglioni",
      "Alberto Maria Metelli"
    ]
  },
  "https://proceedings.mlr.press/v235/geng24a.html": {
    "title": "Improving Adversarial Energy-Based Model via Diffusion Process",
    "volume": "main",
    "abstract": "Generative models have shown strong generation ability while efficient likelihood estimation is less explored. Energy-based models (EBMs) define a flexible energy function to parameterize unnormalized densities efficiently but are notorious for being difficult to train. Adversarial EBMs introduce a generator to form a minimax training game to avoid expensive MCMC sampling used in traditional EBMs, but a noticeable gap between adversarial EBMs and other strong generative models still exists. Inspired by diffusion-based models, we embedded EBMs into each denoising step to split a long-generated process into several smaller steps. Besides, we employ a symmetric Jeffrey divergence and introduce a variational posterior distribution for the generator's training to address the main challenges that exist in adversarial EBMs. Our experiments show significant improvement in generation compared to existing adversarial EBMs, while also providing a useful energy function for efficient density estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Geng",
      "Tian Han",
      "Peng-Tao Jiang",
      "Hao Zhang",
      "Jinwei Chen",
      "Søren Hauberg",
      "Bo Li"
    ]
  },
  "https://proceedings.mlr.press/v235/geng24b.html": {
    "title": "Reinforcement Learning within Tree Search for Fast Macro Placement",
    "volume": "main",
    "abstract": "Macro placement is a crucial step in modern chip design, and reinforcement learning (RL) has recently emerged as a promising technique for improving the placement quality. However, existing RL-based techniques are hindered by their low sample efficiency, requiring numerous online rollouts or substantial offline expert data to achieve bootstrap, which are often impractical in industrial scenarios. To address this challenge, we propose a novel sample-efficient framework, namely EfficientPlace, for fast macro placement. EfficientPlace integrates a global tree search algorithm to strategically direct the optimization process, as well as a RL agent for local policy learning to advance the tree search. Experiments on commonly used benchmarks demonstrate that EfficientPlace achieves remarkable placement quality within a short timeframe, outperforming recent state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Geng",
      "Jie Wang",
      "Ziyan Liu",
      "Siyuan Xu",
      "Zhentao Tang",
      "Mingxuan Yuan",
      "Jianye Hao",
      "Yongdong Zhang",
      "Feng Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/georgiev24a.html": {
    "title": "Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich Differentiable Simulation",
    "volume": "main",
    "abstract": "Model-Free Reinforcement Learning (MFRL), leveraging the policy gradient theorem, has demonstrated considerable success in continuous control tasks. However, these approaches are plagued by high gradient variance due to zeroth-order gradient estimation, resulting in suboptimal policies. Conversely, First-Order Model-Based Reinforcement Learning (FO-MBRL) methods employing differentiable simulation provide gradients with reduced variance but are susceptible to sampling error in scenarios involving stiff dynamics, such as physical contact. This paper investigates the source of this error and introduces Adaptive Horizon Actor-Critic (AHAC), an FO-MBRL algorithm that reduces gradient error by adapting the model-based horizon to avoid stiff dynamics. Empirical findings reveal that AHAC outperforms MFRL baselines, attaining 40% more reward across a set of locomotion tasks and efficiently scaling to high-dimensional control environments with improved wall-clock-time efficiency. adaptive-horizon-actor-critic.github.io",
    "checked": true,
    "id": "5c97fb626bfa0ebc924eab30e588e3f4f2447b18",
    "semantic_title": "adaptive horizon actor-critic for policy learning in contact-rich differentiable simulation",
    "citation_count": 3,
    "authors": [
      "Ignat Georgiev",
      "Krishnan Srinivasan",
      "Jie Xu",
      "Eric Heiden",
      "Animesh Garg"
    ]
  },
  "https://proceedings.mlr.press/v235/gerken24a.html": {
    "title": "Emergent Equivariance in Deep Ensembles",
    "volume": "main",
    "abstract": "We show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments",
    "checked": true,
    "id": "e17447e8f1d240a7246953cf6434994d8aceff19",
    "semantic_title": "emergent equivariance in deep ensembles",
    "citation_count": 3,
    "authors": [
      "Jan E Gerken",
      "Pan Kessel"
    ]
  },
  "https://proceedings.mlr.press/v235/ghandeharioun24a.html": {
    "title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
    "volume": "main",
    "abstract": "Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction",
    "checked": true,
    "id": "f5df0667365764a970fc6abfa0a68b7d1d0ae413",
    "semantic_title": "patchscopes: a unifying framework for inspecting hidden representations of language models",
    "citation_count": 54,
    "authors": [
      "Asma Ghandeharioun",
      "Avi Caciularu",
      "Adam Pearce",
      "Lucas Dixon",
      "Mor Geva"
    ]
  },
  "https://proceedings.mlr.press/v235/ghazi24a.html": {
    "title": "Individualized Privacy Accounting via Subsampling with Applications in Combinatorial Optimization",
    "volume": "main",
    "abstract": "In this work, we give a new technique for analyzing individualized privacy accounting via the following simple observation: if an algorithm is one-sided add-DP, then its subsampled variant satisfies two-sided DP. From this, we obtain several improved algorithms for private combinatorial optimization problems, including decomposable submodular maximization and set cover. Our error guarantees are asymptotically tight and our algorithm satisfies pure-DP while previously known algorithms (Gupta et al., 2010; Chaturvedi et al., 2021) are approximate-DP. We also show an application of our technique beyond combinatorial optimization by giving a pure-DP algorithm for the shifting heavy hitter problem in a stream; previously, only an approximate-DP algorithm was known (Kaplan et al., 2021; Cohen & Lyu, 2023)",
    "checked": true,
    "id": "b6e80baad7c98d47942d6df70ac694b2bc1eef93",
    "semantic_title": "individualized privacy accounting via subsampling with applications in combinatorial optimization",
    "citation_count": 0,
    "authors": [
      "Badih Ghazi",
      "Pritish Kamath",
      "Ravi Kumar",
      "Pasin Manurangsi",
      "Adam Sealfon"
    ]
  },
  "https://proceedings.mlr.press/v235/ghimire24a.html": {
    "title": "State-Constrained Zero-Sum Differential Games with One-Sided Information",
    "volume": "main",
    "abstract": "We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2). The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to violate the state constraints if possible, or to maximize the payoff otherwise. One example of the game is a man-to-man matchup in football. Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players. Our theoretical contribution is an extension of this result to games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing behavioral strategies. Different from existing works that are concerned about the scalability of no-regret learning in games with discrete dynamics, our study reveals the underlying structure of strategies for belief manipulation resulting from information asymmetry and state constraints. This structure will be necessary for scalable learning on games with continuous actions and long time windows. We use a simplified football game to demonstrate the utility of this work, where we reveal player positions and belief states in which the attacker should (or should not) play specific random deceptive moves to take advantage of information asymmetry, and compute how the defender should respond",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mukesh Ghimire",
      "Lei Zhang",
      "Zhe Xu",
      "Yi Ren"
    ]
  },
  "https://proceedings.mlr.press/v235/ghosal24a.html": {
    "title": "Understanding Finetuning for Factual Knowledge Extraction",
    "volume": "main",
    "abstract": "In this work, we study the impact of QA fine-tuning data on downstream factuality. We show that fine-tuning on lesser-known facts that are poorly stored during pretraining yields significantly worse factuality than fine-tuning on well-known facts, even when all facts are seen during pretraining. We prove this phenomenon theoretically, showing that training on lesser-known facts can lead the model to ignore subject entity names and instead output a generic plausible response even when the relevant factual knowledge is encoded in the model. On three question answering benchmarks (PopQA, Entity Questions, and MMLU) and two language models (Llama-2-7B and Mistral-7B), we find that (i) finetuning on a completely factual but lesser-known subset of the data deteriorates downstream factuality (5-10%) and (ii) finetuning on a subset of better-known examples matches or outperforms finetuning on the entire dataset. Ultimately, our results shed light on the interaction between pretrained knowledge and finetuning data and demonstrate the importance of taking into account how facts are stored in the pretrained model when fine-tuning for knowledge-intensive tasks",
    "checked": true,
    "id": "27ecd1371eb7df1b7ea46c88822dd46d7e6a7dec",
    "semantic_title": "understanding finetuning for factual knowledge extraction",
    "citation_count": 5,
    "authors": [
      "Gaurav Rohit Ghosal",
      "Tatsunori Hashimoto",
      "Aditi Raghunathan"
    ]
  },
  "https://proceedings.mlr.press/v235/ghosh24a.html": {
    "title": "A Closer Look at the Limitations of Instruction Tuning",
    "volume": "main",
    "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. Our findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. We hope the insights and challenges revealed in this paper inspire future work in related directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sreyan Ghosh",
      "Chandra Kiran Reddy Evuru",
      "Sonal Kumar",
      "Ramaneswaran S",
      "Deepali Aneja",
      "Zeyu Jin",
      "Ramani Duraiswami",
      "Dinesh Manocha"
    ]
  },
  "https://proceedings.mlr.press/v235/ghosh24b.html": {
    "title": "Agnostic Learning of Mixed Linear Regressions with EM and AM Algorithms",
    "volume": "main",
    "abstract": "Mixed linear regression is a well-studied problem in parametric statistics and machine learning. Given a set of samples, tuples of covariates and labels, the task of mixed linear regression is to find a small list of linear relationships that best fit the samples. Usually it is assumed that the label is generated stochastically by randomly selecting one of two or more linear functions, applying this chosen function to the covariates, and potentially introducing noise to the result. In that situation, the objective is to estimate the ground-truth linear functions up to some parameter error. The popular expectation maximization (EM) and alternating minimization (AM) algorithms have been previously analyzed for this. In this paper, we consider the more general problem of agnostic learning of mixed linear regression from samples, without such generative models. In particular, we show that the AM and EM algorithms, under standard conditions of separability and good initialization, lead to agnostic learning in mixed linear regression by converging to the population loss minimizers, for suitably defined loss functions. In some sense, this shows the strength of AM and EM algorithms that converges to \"optimal solutions\" even in the absence of realizable generative models",
    "checked": true,
    "id": "abb6d0271ab87a2215218422f07841014e568d4b",
    "semantic_title": "agnostic learning of mixed linear regressions with em and am algorithms",
    "citation_count": 0,
    "authors": [
      "Avishek Ghosh",
      "Arya Mazumdar"
    ]
  },
  "https://proceedings.mlr.press/v235/ghosh24c.html": {
    "title": "Optimal Eye Surgeon: Finding image priors through sparse generators at initialization",
    "volume": "main",
    "abstract": "We introduce Optimal Eye Surgeon (OES), a framework for pruning and training deep image generator networks. Typically, untrained deep convolutional networks, which include image sampling operations, serve as effective image priors. However, they tend to overfit to noise in image restoration tasks due to being overparameterized. OES addresses this by adaptively pruning networks at random initialization to a level of underparameterization. This process effectively captures low-frequency image components even without training, by just masking. When trained to fit noisy image, these pruned subnetworks, which we term Sparse-DIP, resist overfitting to noise. This benefit arises from underparameterization and the regularization effect of masking, constraining them in the manifold of image priors. We demonstrate that subnetworks pruned through OES surpass other leading pruning methods, such as the Lottery Ticket Hypothesis, which is known to be suboptimal for image recovery tasks. Our extensive experiments demonstrate the transferability of OES-masks and the characteristics of sparse-subnetworks for image generation. Code is available at https://github.com/Avra98/Optimal-Eye-Surgeon",
    "checked": true,
    "id": "e42be85a0c68719100e170568283b09d920e6116",
    "semantic_title": "optimal eye surgeon: finding image priors through sparse generators at initialization",
    "citation_count": 2,
    "authors": [
      "Avrajit Ghosh",
      "Xitong Zhang",
      "Kenneth K. Sun",
      "Qing Qu",
      "Saiprasad Ravishankar",
      "Rongrong Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/gillman24a.html": {
    "title": "Self-Correcting Self-Consuming Loops for Generative Model Training",
    "volume": "main",
    "abstract": "As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates \"self-consuming loops\" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nate Gillman",
      "Michael Freeman",
      "Daksh Aggarwal",
      "Chia-Hong Hsu",
      "Calvin Luo",
      "Yonglong Tian",
      "Chen Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/glaser24a.html": {
    "title": "Kernel-Based Evaluation of Conditional Biological Sequence Models",
    "volume": "main",
    "abstract": "We propose a set of kernel-based tools to evaluate the designs and tune the hyperparameters of conditional sequence models, with a focus on problems in computational biology. The backbone of our tools is a new measure of discrepancy between the true conditional distribution and the model's estimate, called the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided that the model can be sampled from, the ACMMD can be estimated unbiasedly from data to quantify absolute model fit, integrated within hypothesis tests, and used to evaluate model reliability. We demonstrate the utility of our approach by analyzing a popular protein design model, ProteinMPNN. We are able to reject the hypothesis that ProteinMPNN fits its data for various protein families, and tune the model's temperature hyperparameter to achieve a better fit",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Glaser",
      "Steffanie Paul",
      "Alissa M Hummer",
      "Charlotte Deane",
      "Debora Susan Marks",
      "Alan Nawzad Amin"
    ]
  },
  "https://proceedings.mlr.press/v235/gloeckle24a.html": {
    "title": "Better & Faster Large Language Models via Multi-token Prediction",
    "volume": "main",
    "abstract": "Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following $n$ tokens using $n$ independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12% more problems on Human Eval and 17% more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to $3\\times$ faster at inference, even with large batch sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Gloeckle",
      "Badr Youbi Idrissi",
      "Baptiste Roziere",
      "David Lopez-Paz",
      "Gabriel Synnaeve"
    ]
  },
  "https://proceedings.mlr.press/v235/gloeckler24a.html": {
    "title": "All-in-one simulation-based inference",
    "volume": "main",
    "abstract": "Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method—the Simformer—which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Gloeckler",
      "Michael Deistler",
      "Christian Dietrich Weilbach",
      "Frank Wood",
      "Jakob H. Macke"
    ]
  },
  "https://proceedings.mlr.press/v235/glukhov24a.html": {
    "title": "Position: Fundamental Limitations of LLM Censorship Necessitate New Approaches",
    "volume": "main",
    "abstract": "Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship methods have proven to be fallible at ensuring that LLMs do not return semantically impermissible responses. We present fundamental limitations of verifying the semantic properties of LLM outputs and identifying compositional threats, illustrating inherent challenges of current approaches to censoring LLM outputs. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, and semantic properties of LLM outputs can become impossible to verify when the LLM is capable of providing \"encrypted\" outputs. We further show challenges of censorship can extend beyond just semantic censorship, as attackers can reconstruct impermissible outputs from a collection of permissible ones. Consequently, we call for a re-evaluation of the problem of censorship and its goals, stressing the need for new definitions and approaches to censorship. In addition, we provide an initial attempt toward achieving this goal through syntactic censorship, drawing from a security perspective to design censorship methods that can provide guarantees",
    "checked": true,
    "id": "965ffdc5b3e5f730a3c58a57516976c713f0e151",
    "semantic_title": "position: fundamental limitations of llm censorship necessitate new approaches",
    "citation_count": 2,
    "authors": [
      "David Glukhov",
      "Ilia Shumailov",
      "Yarin Gal",
      "Nicolas Papernot",
      "Vardan Papyan"
    ]
  },
  "https://proceedings.mlr.press/v235/goldblum24a.html": {
    "title": "Position: The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning",
    "volume": "main",
    "abstract": "No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models",
    "checked": false,
    "id": "0bde1c92e1786f95a1de5ddf77a34e68ec9b9414",
    "semantic_title": "the no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning",
    "citation_count": 29,
    "authors": [
      "Micah Goldblum",
      "Marc Anton Finzi",
      "Keefer Rowan",
      "Andrew Gordon Wilson"
    ]
  },
  "https://proceedings.mlr.press/v235/gong24a.html": {
    "title": "CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling",
    "volume": "main",
    "abstract": "Precipitation nowcasting based on radar data plays a crucial role in extreme weather prediction and has broad implications for disaster management. Despite progresses have been made based on deep learning, two key challenges of precipitation nowcasting are not well-solved: (i) the modeling of complex precipitation system evolutions with different scales, and (ii) accurate forecasts for extreme precipitation. In this work, we propose CasCast, a cascaded framework composed of a deterministic and a probabilistic part to decouple the predictions for mesoscale precipitation distributions and small-scale patterns. Then, we explore training the cascaded framework at the high resolution and conducting the probabilistic modeling in a low dimensional latent space with a frame-wise-guided diffusion transformer for enhancing the optimization of extreme events while reducing computational costs. Extensive experiments on three benchmark radar precipitation datasets show that CasCast achieves competitive performance. Especially, CasCast significantly surpasses the baseline (up to +91.8%) for regional extreme-precipitation nowcasting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junchao Gong",
      "Lei Bai",
      "Peng Ye",
      "Wanghan Xu",
      "Na Liu",
      "Jianhua Dai",
      "Xiaokang Yang",
      "Wanli Ouyang"
    ]
  },
  "https://proceedings.mlr.press/v235/gong24b.html": {
    "title": "Does Label Smoothing Help Deep Partial Label Learning?",
    "volume": "main",
    "abstract": "Although deep partial label learning (deep PLL) classifiers have shown their competitive performance, they are heavily influenced by the noisy false-positive labels leading to poorer performance as the training progresses. Meanwhile, existing deep PLL research lacks theoretical guarantee on the analysis of correlation between label noise (or ambiguity degree) and classification performance. This paper addresses the above limitations with label smoothing (LS) from both theoretical and empirical aspects. In theory, we prove lower and upper bounds of the expected risk to show that label smoothing can help deep PLL. We further derive the optimal smoothing rate to investigate the conditions, i.e., when label smoothing benefits deep PLL. In practice, we design a benchmark solution and a novel optimization algorithm called Label Smoothing-based Partial Label Learning (LS-PLL). Extensive experimental results on benchmark PLL datasets and various deep architectures validate that label smoothing does help deep PLL in improving classification performance and learning distinguishable representations, and the best results can be achieved when the empirical smoothing rate approximately approaches the optimal smoothing rate in theoretical findings. Code is publicly available at https://github.com/kalpiree/LS-PLL",
    "checked": true,
    "id": "8be123c48102569b5ffe76df61b310fdb60aa9bd",
    "semantic_title": "does label smoothing help deep partial label learning?",
    "citation_count": 0,
    "authors": [
      "Xiuwen Gong",
      "Nitin Bisht",
      "Guandong Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/gong24c.html": {
    "title": "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding",
    "volume": "main",
    "abstract": "Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids complex program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks including HumanEval and MBPP. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our code and model are publicly available at https://github.com/gonglinyuan/ast_t5",
    "checked": true,
    "id": "fbd1b4f09b19bd23c16b54525347c6643bb06322",
    "semantic_title": "ast-t5: structure-aware pretraining for code generation and understanding",
    "citation_count": 9,
    "authors": [
      "Linyuan Gong",
      "Mostafa Elhoushi",
      "Alvin Cheung"
    ]
  },
  "https://proceedings.mlr.press/v235/gong24d.html": {
    "title": "A Nearly Optimal Single Loop Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness",
    "volume": "main",
    "abstract": "This paper studies the problem of stochastic bilevel optimization where the upper-level function is nonconvex with potentially unbounded smoothness and the lower-level function is strongly convex. This problem is motivated by meta-learning applied to sequential data, such as text classification using recurrent neural networks, where the smoothness constant of the upper-level loss function scales linearly with the gradient norm and can be potentially unbounded. Existing algorithm crucially relies on the nested loop design, which requires significant tuning efforts and is not practical. In this paper, we address this issue by proposing a Single Loop bIlevel oPtimizer (SLIP). The proposed algorithm first updates the lower-level variable by a few steps of stochastic gradient descent, and then simultaneously updates the upper-level variable by normalized stochastic gradient descent with momentum and the lower-level variable by stochastic gradient descent. Under standard assumptions, we show that our algorithm finds an $\\epsilon$-stationary point within $\\widetilde{O}(1/\\epsilon^4)$[Here $\\widetilde{O}(\\cdot)$ compresses logarithmic factors of $1/\\epsilon$ and $1/\\delta$, where $\\delta\\in(0,1)$ denotes the failure probability.] oracle calls of stochastic gradient or Hessian-vector product, both in expectation and with high probability. This complexity result is nearly optimal up to logarithmic factors without mean-square smoothness of the stochastic gradient oracle. Our proof relies on (i) a refined characterization and control of the lower-level variable and (ii) establishing a novel connection between bilevel optimization and stochastic optimization under distributional drift. Our experiments on various tasks show that our algorithm significantly outperforms strong baselines in bilevel optimization",
    "checked": true,
    "id": "c643727bccd19c9cd8a749e0954a05ad1aa12559",
    "semantic_title": "a nearly optimal single loop algorithm for stochastic bilevel optimization under unbounded smoothness",
    "citation_count": 2,
    "authors": [
      "Xiaochuan Gong",
      "Jie Hao",
      "Mingrui Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/gong24e.html": {
    "title": "Evolution-Inspired Loss Functions for Protein Representation Learning",
    "volume": "main",
    "abstract": "AI-based frameworks for protein engineering use self-supervised learning (SSL) to obtain representations for downstream mutation effect predictions. The most common training objective for these methods is wildtype accuracy: given a sequence or structure where a wildtype residue has been masked, predict the missing amino acid. Wildtype accuracy, however, does not align with the primary goal of protein engineering, which is to suggest a mutation rather than to identify what already appears in nature. Here we present Evolutionary Ranking (EvoRank), a training objective that incorporates evolutionary information derived from multiple sequence alignments (MSAs) to learn more diverse protein representations. EvoRank corresponds to ranking amino-acid likelihoods in the probability distribution induced by an MSA. This objective forces models to learn the underlying evolutionary dynamics of a protein. Across a variety of phenotypes and datasets, we demonstrate that EvoRank leads to dramatic improvements in zero-shot performance and can compete with models fine-tuned on experimental data. This is particularly important in protein engineering, where it is expensive to obtain data for fine-tuning",
    "checked": true,
    "id": "1915b6a794c3016b181d80ca10f5ab9409267850",
    "semantic_title": "evolution-inspired loss functions for protein representation learning",
    "citation_count": 1,
    "authors": [
      "Chengyue Gong",
      "Adam Klivans",
      "James Madigan Loy",
      "Tianlong Chen",
      "Qiang Liu",
      "Daniel Jesus Diaz"
    ]
  },
  "https://proceedings.mlr.press/v235/gong24f.html": {
    "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks",
    "volume": "main",
    "abstract": "We introduce Syntax-Aware Fill-in-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com",
    "checked": true,
    "id": "8e36a8f8fd4451d55e927151aabc463830941a8e",
    "semantic_title": "evaluation of llms on syntax-aware code fill-in-the-middle tasks",
    "citation_count": 6,
    "authors": [
      "Linyuan Gong",
      "Sida Wang",
      "Mostafa Elhoushi",
      "Alvin Cheung"
    ]
  },
  "https://proceedings.mlr.press/v235/gong24g.html": {
    "title": "E$^2$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation",
    "volume": "main",
    "abstract": "One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating the need for training from scratch. Second, we identify crucial layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet effective rank search process, rather than fine-tuning the entire base model. Third, we investigate the minimal amount of data necessary for fine-tuning, further reducing the overall training time. Extensive experiments show that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkably reduced training and storage costs for each concept",
    "checked": false,
    "id": "32d380b78e040dd081ecf62f1a6cd4b04406a773",
    "semantic_title": "e2gan: efficient training of efficient gans for image-to-image translation",
    "citation_count": 0,
    "authors": [
      "Yifan Gong",
      "Zheng Zhan",
      "Qing Jin",
      "Yanyu Li",
      "Yerlan Idelbayev",
      "Xian Liu",
      "Andrey Zharkov",
      "Kfir Aberman",
      "Sergey Tulyakov",
      "Yanzhi Wang",
      "Jian Ren"
    ]
  },
  "https://proceedings.mlr.press/v235/gorbunov24a.html": {
    "title": "High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise",
    "volume": "main",
    "abstract": "High-probability analysis of stochastic first-order optimization methods under mild assumptions on the noise has been gaining a lot of attention in recent years. Typically, gradient clipping is one of the key algorithmic ingredients to derive good high-probability guarantees when the noise is heavy-tailed. However, if implemented naively, clipping can spoil the convergence of the popular methods for composite and distributed optimization (Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason, many works on high-probability analysis consider only unconstrained non-distributed problems, and the existing results for composite/distributed problems do not include some important special cases (like strongly convex problems) and are not optimal. To address this issue, we propose new stochastic methods for composite and distributed optimization based on the clipping of stochastic gradient differences and prove tight high-probability convergence results (including nearly optimal ones) for the new methods. In addition, we also develop new methods for composite and distributed variational inequalities and analyze the high-probability convergence of these methods",
    "checked": true,
    "id": "87d9f8b63202fa4b4cc03a3d61f099157f04cc44",
    "semantic_title": "high-probability convergence for composite and distributed stochastic minimization and variational inequalities with heavy-tailed noise",
    "citation_count": 0,
    "authors": [
      "Eduard Gorbunov",
      "Abdurakhmon Sadiev",
      "Marina Danilova",
      "Samuel Horváth",
      "Gauthier Gidel",
      "Pavel Dvurechensky",
      "Alexander Gasnikov",
      "Peter Richtárik"
    ]
  },
  "https://proceedings.mlr.press/v235/goring24a.html": {
    "title": "Out-of-Domain Generalization in Dynamical Systems Reconstruction",
    "volume": "main",
    "abstract": "In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model. We formally prove that black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model. We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space. Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice",
    "checked": true,
    "id": "e0405265de2ea5331d97fe373884f9fef12e0b8d",
    "semantic_title": "out-of-domain generalization in dynamical systems reconstruction",
    "citation_count": 7,
    "authors": [
      "Niclas Alexander Göring",
      "Florian Hess",
      "Manuel Brenner",
      "Zahra Monfared",
      "Daniel Durstewitz"
    ]
  },
  "https://proceedings.mlr.press/v235/goswami24a.html": {
    "title": "MOMENT: A Family of Open Time-series Foundation Models",
    "volume": "main",
    "abstract": "We introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on Huggingface",
    "checked": true,
    "id": "aabfdbd9db5ce9b1d598eae44e0d6250e7f0fc00",
    "semantic_title": "moment: a family of open time-series foundation models",
    "citation_count": 47,
    "authors": [
      "Mononito Goswami",
      "Konrad Szafer",
      "Arjun Choudhry",
      "Yifu Cai",
      "Shuo Li",
      "Artur Dubrawski"
    ]
  },
  "https://proceedings.mlr.press/v235/gottlieb24a.html": {
    "title": "Weighted distance nearest neighbor condensing",
    "volume": "main",
    "abstract": "The problem of nearest neighbor condensing has enjoyed a long history of study, both in its theoretical and practical aspects. In this paper, we introduce the problem of weighted distance nearest neighbor condensing, where one assigns weights to each point of the condensed set, and then new points are labeled based on their weighted distance nearest neighbor in the condensed set. We study the theoretical properties of this new model, and show that it can produce dramatically better condensing than the standard nearest neighbor rule, yet is characterized by generalization bounds almost identical to the latter. We then suggest a condensing heuristic for our new problem. We demonstrate Bayes consistency for this heuristic, and also show promising empirical results",
    "checked": true,
    "id": "c38acd49a5514f6d2fa5d040ce526fbbe2d844ed",
    "semantic_title": "weighted distance nearest neighbor condensing",
    "citation_count": 0,
    "authors": [
      "Lee-Ad Gottlieb",
      "Timor Sharabi",
      "Roi Weiss"
    ]
  },
  "https://proceedings.mlr.press/v235/gou24a.html": {
    "title": "Test-Time Degradation Adaptation for Open-Set Image Restoration",
    "volume": "main",
    "abstract": "In contrast to close-set scenarios that restore images from a predefined set of degradations, open-set image restoration aims to handle the unknown degradations that were unforeseen during the pretraining phase, which is less-touched as far as we know. This work study this challenging problem and reveal its essence as unidentified distribution shifts between the test and training data. Recently, test-time adaptation has emerged as a fundamental method to address this inherent disparities. Inspired by it, we propose a test-time degradation adaptation framework for open-set image restoration, which consists of three components, i.e., i) a pre-trained and degradation-agnostic diffusion model for generating clean images, ii) a test-time degradation adapter adapts the unknown degradations based on the input image during the testing phase, and iii) the adapter-guided image restoration guides the model through the adapter to produce the corresponding clean image. Through experiments on multiple degradations, we show that our method achieves comparable even better performance than those task-specific methods. The code is available at https://github.com/XLearning-SCU/2024-ICML-TAO",
    "checked": true,
    "id": "0c291c421fb11b5b27e2d3526efb2353d88c27a8",
    "semantic_title": "test-time degradation adaptation for open-set image restoration",
    "citation_count": 4,
    "authors": [
      "Yuanbiao Gou",
      "Haiyu Zhao",
      "Boyun Li",
      "Xinyan Xiao",
      "Xi Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/grau-moya24a.html": {
    "title": "Learning Universal Predictors",
    "volume": "main",
    "abstract": "Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data by pre-training them on a broad set of tasks. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging (memory-based) meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal prediction strategies",
    "checked": true,
    "id": "b9eb1f62952dfb04ce5d90b7317e409520282938",
    "semantic_title": "learning universal predictors",
    "citation_count": 6,
    "authors": [
      "Jordi Grau-Moya",
      "Tim Genewein",
      "Marcus Hutter",
      "Laurent Orseau",
      "Gregoire Deletang",
      "Elliot Catt",
      "Anian Ruoss",
      "Li Kevin Wenliang",
      "Christopher Mattern",
      "Matthew Aitchison",
      "Joel Veness"
    ]
  },
  "https://proceedings.mlr.press/v235/gravina24a.html": {
    "title": "Long Range Propagation on Continuous-Time Dynamic Graphs",
    "volume": "main",
    "abstract": "Learning Continuous-Time Dynamic Graphs (C-TDGs) requires accurately modeling spatio-temporal information on streams of irregularly sampled events. While many methods have been proposed recently, we find that most message passing-, recurrent- or self-attention-based methods perform poorly on long-range tasks. These tasks require correlating information that occurred \"far\" away from the current event, either spatially (higher-order node information) or along the time dimension (events occurred in the past). To address long-range dependencies, we introduce Continuous-Time Graph Anti-Symmetric Network (CTAN). Grounded within the ordinary differential equations framework, our method is designed for efficient propagation of information. In this paper, we show how CTAN's (i) long-range modeling capabilities are substantiated by theoretical findings and how (ii) its empirical performance on synthetic long-range benchmarks and real-world benchmarks is superior to other methods. Our results motivate CTAN's ability to propagate long-range information in C-TDGs as well as the inclusion of long-range tasks as part of temporal graph models evaluation",
    "checked": true,
    "id": "b76d3f9ec4171d8dccc868889c32023f3efd2174",
    "semantic_title": "long range propagation on continuous-time dynamic graphs",
    "citation_count": 1,
    "authors": [
      "Alessio Gravina",
      "Giulio Lovisotto",
      "Claudio Gallicchio",
      "Davide Bacciu",
      "Claas Grohnfeldt"
    ]
  },
  "https://proceedings.mlr.press/v235/graziani24a.html": {
    "title": "The Expressive Power of Path-Based Graph Neural Networks",
    "volume": "main",
    "abstract": "We systematically investigate the expressive power of path-based graph neural networks. While it has been shown that path-based graph neural networks can achieve strong empirical results, an investigation into their expressive power is lacking. Therefore, we propose PATH-WL, a general class of color refinement algorithms based on paths and shortest path distance information. We show that PATH-WL is incomparable to a wide range of expressive graph neural networks, can count cycles, and achieves strong empirical results on the notoriously difficult family of strongly regular graphs. Our theoretical results indicate that PATH-WL forms a new hierarchy of highly expressive graph neural networks",
    "checked": true,
    "id": "0d4ff749c180b305cf85ed36cb4243efbbd975f0",
    "semantic_title": "the expressive power of path-based graph neural networks",
    "citation_count": 0,
    "authors": [
      "Caterina Graziani",
      "Tamara Drucks",
      "Fabian Jogl",
      "Monica Bianchini",
      "Franco Scarselli",
      "Thomas Gärtner"
    ]
  },
  "https://proceedings.mlr.press/v235/grazzi24a.html": {
    "title": "Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates",
    "volume": "main",
    "abstract": "We study the problem of efficiently computing the derivative of the fixed-point of a parametric nondifferentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. We build upon the work by Bolte et al. (2022), who prove linear convergence of nonsmooth ITD under a piecewise Lipschitz smooth assumption. In the deterministic case, we provide a linear rate for AID and an improved linear rate for ITD which closely match the ones for the smooth setting. We further introduce NSID, a new stochastic method to compute the implicit derivative when the contraction map is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of NSID, encompassing the best available rates in the smooth setting. We also present illustrative experiments confirming our analysis",
    "checked": true,
    "id": "1b732690b1310cd7755955852ea60b86b06c98c7",
    "semantic_title": "nonsmooth implicit differentiation: deterministic and stochastic convergence rates",
    "citation_count": 1,
    "authors": [
      "Riccardo Grazzi",
      "Massimiliano Pontil",
      "Saverio Salzo"
    ]
  },
  "https://proceedings.mlr.press/v235/grcic24a.html": {
    "title": "Fine-grained Classes and How to Find Them",
    "volume": "main",
    "abstract": "In many practical applications, coarse-grained labels are readily available compared to fine-grained labels that reflect subtle differences between classes. However, existing methods cannot leverage coarse labels to infer fine-grained labels in an unsupervised manner. To bridge this gap, we propose FALCON, a method that discovers fine-grained classes from coarsely labeled data without any supervision at the fine-grained level. FALCON simultaneously infers unknown fine-grained classes and underlying relationships between coarse and fine-grained classes. Moreover, FALCON is a modular method that can effectively learn from multiple datasets labeled with different strategies. We evaluate FALCON on eight image classification tasks and a single-cell classification task. FALCON outperforms baselines by a large margin, achieving 22% improvement over the best baseline on the tieredImageNet dataset with over 600 fine-grained classes",
    "checked": true,
    "id": "03ff17a9a989a78846b2899ed796926ca5352f76",
    "semantic_title": "fine-grained classes and how to find them",
    "citation_count": 0,
    "authors": [
      "Matej Grcic",
      "Artyom Gadetsky",
      "Maria Brbic"
    ]
  },
  "https://proceedings.mlr.press/v235/greenblatt24a.html": {
    "title": "AI Control: Improving Safety Despite Intentional Subversion",
    "volume": "main",
    "abstract": "As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. To do so, safety measures either aim at making LLMs try to avoid harmful outcomes or aim at preventing LLMs from causing harmful outcomes, even if they try to cause them. In this paper, we focus on this second layer of defense. We develop and evaluate pipelines of safety techniques (protocols) that try to ensure safety despite intentional subversion - an approach we call AI control. We investigate a setting in which we want to solve a sequence of programming problems without ever submitting subtly wrong code, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate a range of protocols and red-team them by exploring strategies that the untrusted model could use to subvert them. We find that using the trusted model to edit untrusted-model code or using the untrusted model as a monitor substantially improves on simple baselines",
    "checked": true,
    "id": "fbd13beeb4706fb3b07b8bd22a16f0504683fcf3",
    "semantic_title": "ai control: improving safety despite intentional subversion",
    "citation_count": 16,
    "authors": [
      "Ryan Greenblatt",
      "Buck Shlegeris",
      "Kshitij Sachan",
      "Fabien Roger"
    ]
  },
  "https://proceedings.mlr.press/v235/grenioux24a.html": {
    "title": "Stochastic Localization via Iterative Posterior Sampling",
    "volume": "main",
    "abstract": "Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, Stochastic Localization via Iterative Posterior Sampling (SLIPS), to obtain approximate samples of these dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines. We illustrate the benefits and applicability of SLIPS on several benchmarks of multi-modal distributions, including Gaussian mixtures in increasing dimensions, Bayesian logistic regression and a high-dimensional field system from statistical-mechanics",
    "checked": true,
    "id": "58c56f1141402abc88f9e4921a6e5b33512178ad",
    "semantic_title": "stochastic localization via iterative posterior sampling",
    "citation_count": 6,
    "authors": [
      "Louis Grenioux",
      "Maxence Noble",
      "Marylou Gabrié",
      "Alain Oliviero Durmus"
    ]
  },
  "https://proceedings.mlr.press/v235/greshler24a.html": {
    "title": "A Bayesian Approach to Online Planning",
    "volume": "main",
    "abstract": "The combination of Monte Carlo tree search and neural networks has revolutionized online planning. As neural network approximations are often imperfect, we ask whether uncertainty estimates about the network outputs could be used to improve planning. We develop a Bayesian planning approach that facilitates such uncertainty quantification, inspired by classical ideas from the meta-reasoning literature. We propose a Thompson sampling based algorithm for searching the tree of possible actions, for which we prove the first (to our knowledge) finite time Bayesian regret bound, and propose an efficient implementation for a restricted family of posterior distributions. In addition we propose a variant of the Bayes-UCB method applied to trees. Empirically, we demonstrate that on the ProcGen Maze and Leaper environments, when the uncertainty estimates are accurate but the neural network output is inaccurate, our Bayesian approach searches the tree much more effectively. In addition, we investigate whether popular uncertainty estimation methods are accurate enough to yield significant gains in planning",
    "checked": true,
    "id": "ecb34133bdf740f054d86773660a17477304daed",
    "semantic_title": "a bayesian approach to online planning",
    "citation_count": 0,
    "authors": [
      "Nir Greshler",
      "David Ben Eli",
      "Carmel Rabinovitz",
      "Gabi Guetta",
      "Liran Gispan",
      "Guy Zohar",
      "Aviv Tamar"
    ]
  },
  "https://proceedings.mlr.press/v235/greydanus24a.html": {
    "title": "Scaling Down Deep Learning with MNIST-1D",
    "volume": "main",
    "abstract": "Although deep learning models have taken on commercial and political relevance, key aspects of their training and operation remain poorly understood. This has sparked interest in science of deep learning projects, many of which require large amounts of time, money, and electricity. But how much of this research really needs to occur at scale? In this paper, we introduce MNIST-1D: a minimalist, procedurally generated, low-memory, and low-compute alternative to classic deep learning benchmarks. Although the dimensionality of MNIST-1D is only 40 and its default training set size only 4000, MNIST-1D can be used to study inductive biases of different deep architectures, find lottery tickets, observe deep double descent, metalearn an activation function, and demonstrate guillotine regularization in self-supervised learning. All these experiments can be conducted on a GPU or often even on a CPU within minutes, allowing for fast prototyping, educational use cases, and cutting-edge research on a low budget",
    "checked": false,
    "id": "059d7d44cfabaf938947000891baa2fb6c002c63",
    "semantic_title": "learn & drop: fast learning of cnns based on layer dropping",
    "citation_count": 0,
    "authors": [
      "Samuel James Greydanus",
      "Dmitry Kobak"
    ]
  },
  "https://proceedings.mlr.press/v235/grillotti24a.html": {
    "title": "Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics",
    "volume": "main",
    "abstract": "A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion tasks. We also demonstrate that we can harness the learned skills to adapt better than other baselines to five perturbed environments. Finally, qualitative analyses showcase a range of remarkable behaviors: adaptive-intelligent-robotics.github.io/QDAC",
    "checked": true,
    "id": "851928d03830b47e795fe9925c8244fb4ae4fafd",
    "semantic_title": "quality-diversity actor-critic: learning high-performing and diverse behaviors via value and successor features critics",
    "citation_count": 2,
    "authors": [
      "Luca Grillotti",
      "Maxence Faldor",
      "Borja G. León",
      "Antoine Cully"
    ]
  },
  "https://proceedings.mlr.press/v235/gruber24a.html": {
    "title": "A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models",
    "volume": "main",
    "abstract": "Generative models, like large language models, are becoming increasingly relevant in our daily lives, yet a theoretical framework to assess their generalization behavior and uncertainty does not exist. Particularly, the problem of uncertainty estimation is commonly solved in an ad-hoc and task-dependent manner. For example, natural language approaches cannot be transferred to image generation. In this paper, we introduce the first bias-variance-covariance decomposition for kernel scores. This decomposition represents a theoretical framework from which we derive a kernel-based variance and entropy for uncertainty estimation. We propose unbiased and consistent estimators for each quantity which only require generated samples but not the underlying model itself. Based on the wide applicability of kernels, we demonstrate our framework via generalization and uncertainty experiments for image, audio, and language generation. Specifically, kernel entropy for uncertainty estimation is more predictive of performance on CoQA and TriviaQA question answering datasets than existing baselines and can also be applied to closed-source models",
    "checked": true,
    "id": "3599a33ede6b76e5bece74c9958b16699a316f41",
    "semantic_title": "a bias-variance-covariance decomposition of kernel scores for generative models",
    "citation_count": 1,
    "authors": [
      "Sebastian Gregor Gruber",
      "Florian Buettner"
    ]
  },
  "https://proceedings.mlr.press/v235/gruber24b.html": {
    "title": "Overcoming Saturation in Density Ratio Estimation by Iterated Regularization",
    "volume": "main",
    "abstract": "Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models",
    "checked": true,
    "id": "934f632e0fdea5f37ff9780f9cb20922e7324d24",
    "semantic_title": "overcoming saturation in density ratio estimation by iterated regularization",
    "citation_count": 1,
    "authors": [
      "Lukas Gruber",
      "Markus Holzleitner",
      "Johannes Lehner",
      "Sepp Hochreiter",
      "Werner Zellinger"
    ]
  },
  "https://proceedings.mlr.press/v235/gu24a.html": {
    "title": "On the Calibration of Human Pose Estimation",
    "volume": "main",
    "abstract": "2D human pose estimation predicts keypoint locations and the corresponding confidence. Calibration-wise, the confidence should be aligned with the pose accuracy. Yet existing pose estimation methods tend to estimate confidence with heuristics such as the maximum value of heatmaps. This work shows, through theoretical analysis and empirical verification, a calibration gap in current pose estimation frameworks. Our derivations directly lead to closed-form adjustments in the confidence based on additionally inferred instance size and visibility. Given the black-box nature of deep neural networks, however, it is not possible to close the gap with only closed-form adjustments. We go one step further and propose a Calibrated ConfidenceNet (CCNet) to explicitly learn network-specific adjustments with a confidence prediction branch. The proposed CCNet, as a lightweight post-hoc addition, improves the calibration of standard off-the-shelf pose estimation frameworks",
    "checked": true,
    "id": "497d95cc1538a126b856f1d8c6db6ae32d091e8c",
    "semantic_title": "on the calibration of human pose estimation",
    "citation_count": 0,
    "authors": [
      "Kerui Gu",
      "Rongyu Chen",
      "Xuanlong Yu",
      "Angela Yao"
    ]
  },
  "https://proceedings.mlr.press/v235/gu24b.html": {
    "title": "EDISON: Enhanced Dictionary-Induced Tensorized Incomplete Multi-View Clustering with Gaussian Error Rank Minimization",
    "volume": "main",
    "abstract": "This paper presents an efficient and scalable incomplete multi-view clustering method, referred to as Enhanced Dictionary-Induced tenSorized incomplete multi-view clustering with Gaussian errOr raNk minimization (EDISON). Specifically, EDISON employs an enhanced dictionary representation strategy as the foundation for inferring missing data and constructing anchor graphs, ensuring robustness to less-than-ideal data and maintaining high computational efficiency. Additionally, we introduce Gaussian error rank as a concise approximation of the true tensor rank, facilitating a comprehensive exploration of the diverse information encapsulated by various singular values in tensor data. Additionally, we integrate a hyper-anchor graph Laplacian manifold regularization into the tensor representation, allowing for the simultaneous utilization of inter-view high-order correlations and intra-view local correlations. Extensive experiments demonstrate the superiority of the EDISON model in both effectiveness and efficiency compared to SOTA methods",
    "checked": true,
    "id": "7e0a5ea6038a5336da0e46f4f98491702e97fa6b",
    "semantic_title": "edison: enhanced dictionary-induced tensorized incomplete multi-view clustering with gaussian error rank minimization",
    "citation_count": 1,
    "authors": [
      "Zhibin Gu",
      "Zhendong Li",
      "Songhe Feng"
    ]
  },
  "https://proceedings.mlr.press/v235/gu24c.html": {
    "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution",
    "volume": "main",
    "abstract": "We present Code Reasoning, Understanding, and eXecution Evaluation, a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a general recipe for generating our execution benchmark by sampling from a model, which can be used for more challenging versions of the benchmark if needed. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval show no improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction. When it comes to reasoning about code, GPT-4 has a huge edge over other models but still fails consistently on some surprisingly simple Python programs",
    "checked": true,
    "id": "4701914bc77dedef9e0a001687277103fb3ddfc6",
    "semantic_title": "cruxeval: a benchmark for code reasoning, understanding and execution",
    "citation_count": 37,
    "authors": [
      "Alex Gu",
      "Baptiste Roziere",
      "Hugh James Leather",
      "Armando Solar-Lezama",
      "Gabriel Synnaeve",
      "Sida Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/gu24d.html": {
    "title": "Data-free Distillation of Diffusion Models with Bootstrapping",
    "volume": "main",
    "abstract": "Diffusion models have demonstrated great potential for generating diverse images. However, their performance often suffers from slow generation due to iterative denoising. Knowledge distillation has been recently proposed as a remedy which can reduce the number of inference steps to one or a few, without significant quality degradation. However, existing distillation methods either require significant amounts of offline computation for generating synthetic training data from the teacher model, or need to perform expensive online learning with the help of real data. In this work, we present a novel technique called BOOT, that overcomes these limitations with an efficient data-free distillation algorithm. The core idea is to learn a time-conditioned model that predicts the output of a pre-trained diffusion model teacher given any time-step. Such a model can be efficiently trained based on bootstrapping from two consecutive sampled steps. Furthermore, our method can be easily adapted to large-scale text-to-image diffusion models, which are challenging for previous methods given the fact that the training sets are often large and difficult to access. We demonstrate the effectiveness of our approach on several benchmark datasets in the DDIM setting, achieving comparable generation quality while being orders of magnitude faster than the diffusion teacher. The text-to-image results show that the proposed approach is able to handle highly complex distributions, shedding light on more efficient generative modeling",
    "checked": true,
    "id": "a9fd657b71bb70da7ae918537286e2d6b234714e",
    "semantic_title": "data-free distillation of diffusion models with bootstrapping",
    "citation_count": 1,
    "authors": [
      "Jiatao Gu",
      "Chen Wang",
      "Shuangfei Zhai",
      "Yizhe Zhang",
      "Lingjie Liu",
      "Joshua M. Susskind"
    ]
  },
  "https://proceedings.mlr.press/v235/gu24e.html": {
    "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
    "volume": "main",
    "abstract": "A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate",
    "checked": true,
    "id": "b0ada492ba48e85016cbbfd95ec7180fb7e79648",
    "semantic_title": "agent smith: a single image can jailbreak one million multimodal llm agents exponentially fast",
    "citation_count": 24,
    "authors": [
      "Xiangming Gu",
      "Xiaosen Zheng",
      "Tianyu Pang",
      "Chao Du",
      "Qian Liu",
      "Ye Wang",
      "Jing Jiang",
      "Min Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/guerdan24a.html": {
    "title": "Predictive Performance Comparison of Decision Policies Under Confounding",
    "volume": "main",
    "abstract": "Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy",
    "checked": true,
    "id": "05db3d0f88615ac5edb5499f7ea5bdebe2c149bc",
    "semantic_title": "predictive performance comparison of decision policies under confounding",
    "citation_count": 0,
    "authors": [
      "Luke Guerdan",
      "Amanda Lee Coston",
      "Ken Holstein",
      "Steven Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/guha24a.html": {
    "title": "On the Diminishing Returns of Width for Continual Learning",
    "volume": "main",
    "abstract": "While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from catastrophic forgetting when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN), demonstrating that the diminishing returns of increasing widths to reduce forgetting. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory",
    "checked": true,
    "id": "3eb2c5e27b541038ff26fbd1267aaccc5e9c8d45",
    "semantic_title": "on the diminishing returns of width for continual learning",
    "citation_count": 1,
    "authors": [
      "Etash Kumar Guha",
      "Vihan Lakshman"
    ]
  },
  "https://proceedings.mlr.press/v235/gui24a.html": {
    "title": "Vector Quantization Pretraining for EEG Time Series with Random Projection and Phase Alignment",
    "volume": "main",
    "abstract": "In this paper, we propose a BERT-style self-supervised learning model, VQ-MTM (Vector Quantization Masked Time-Series Modeling), for the EEG time series data analysis. At its core, VQ-MTM comprises a theoretically grounded random-projection quantization module and a phase-aligning module guided by the Time-Phase-Shift Equivariance of Fourier Transform, the two modules can generate well-defined semantic units (akin to words in natural language) for the corrupted and periodic time series, thus offering robust and consistent learning signals for the EEG self-supervised learning. VQ-MTM also owns low model complexity and can easily adapt to large-scale datasets. We conduct experiments on five real-world datasets including two large-scale datasets to verify the efficacy of our proposed model, the experiment results show that VQ-MTM is able to consistently surpass the existing methods by large margins on both seizure detection and classification tasks. Our code is available at https://github.com/HaokunGUI/VQ_MTM",
    "checked": true,
    "id": "c7efd4c91872698a0473461419a4ec9c3984abdc",
    "semantic_title": "vector quantization pretraining for eeg time series with random projection and phase alignment",
    "citation_count": 2,
    "authors": [
      "Haokun Gui",
      "Xiucheng Li",
      "Xinyang Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/guille-escuret24a.html": {
    "title": "No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths",
    "volume": "main",
    "abstract": "Understanding the optimization dynamics of neural networks is necessary for closing the gap between theory and practice. Stochastic first-order optimization algorithms are known to efficiently locate favorable minima in deep neural networks. This efficiency, however, contrasts with the non-convex and seemingly complex structure of neural loss landscapes. In this study, we delve into the fundamental geometric properties of sampled gradients along optimization paths. We focus on two key quantities, the restricted secant inequality and error bound, as well as their ratio γ, which hold high significance for first-order optimization. Our analysis reveals that these quantities exhibit predictable, consistent behavior throughout training, despite the stochasticity induced by sampling minibatches. Our findings suggest that not only do optimization trajectories never encounter significant obstacles, but they also maintain stable dynamics during the majority of training. These observed properties are sufficiently expressive to theoretically guarantee linear convergence and prescribe learning rate schedules mirroring empirical practices. We conduct our experiments on image classification, semantic segmentation and language modeling across different batch sizes, network architectures, datasets, optimizers, and initialization seeds. We discuss the impact of each factor. Our work provides novel insights into the properties of neural network loss functions, and opens the door to theoretical frameworks more relevant to prevalent practice",
    "checked": true,
    "id": "df3d9a4acc1e65a54dd4901f78d4a26989394e6e",
    "semantic_title": "no wrong turns: the simple geometry of neural networks optimization paths",
    "citation_count": 3,
    "authors": [
      "Charles Guille-Escuret",
      "Hiroki Naganuma",
      "Kilian Fatras",
      "Ioannis Mitliagkas"
    ]
  },
  "https://proceedings.mlr.press/v235/guinet24a.html": {
    "title": "Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation",
    "volume": "main",
    "abstract": "We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model",
    "checked": true,
    "id": "55c3095681acc82780508b0e484dba0c30cf1caa",
    "semantic_title": "automated evaluation of retrieval-augmented language models with task-specific exam generation",
    "citation_count": 8,
    "authors": [
      "Gauthier Guinet",
      "Behrooz Omidvar-Tehrani",
      "Anoop Deoras",
      "Laurent Callot"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24a.html": {
    "title": "SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization",
    "volume": "main",
    "abstract": "Transformers have become foundational architectures for both natural language and computer vision tasks. However, the high computational cost makes it quite challenging to deploy on resource-constraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains $83.6%$ top-1 accuracy on ImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of Flatten-Swin with $0.1%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency. Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB",
    "checked": true,
    "id": "c1d654386de0cee2561001e2de2ad444430323d1",
    "semantic_title": "slab: efficient transformers with simplified linear attention and progressive re-parameterized batch normalization",
    "citation_count": 4,
    "authors": [
      "Jialong Guo",
      "Xinghao Chen",
      "Yehui Tang",
      "Yunhe Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24b.html": {
    "title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning",
    "volume": "main",
    "abstract": "In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing $1.60 and \\$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent",
    "checked": true,
    "id": "850dadb26cafc16539074d22745783c0e0bbd01f",
    "semantic_title": "ds-agent: automated data science by empowering large language models with case-based reasoning",
    "citation_count": 13,
    "authors": [
      "Siyuan Guo",
      "Cheng Deng",
      "Ying Wen",
      "Hechang Chen",
      "Yi Chang",
      "Jun Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24c.html": {
    "title": "Collaborative Heterogeneous Causal Inference Beyond Meta-analysis",
    "volume": "main",
    "abstract": "Collaboration between different data centers is often challenged by heterogeneity across sites. To account for the heterogeneity, the state-of-the-art method is to re-weight the covariate distributions in each site to match the distribution of the target population. Nevertheless, this method still relies on the concept of traditional meta-analysis after adjusting for the distribution shift. This work proposes a collaborative inverse propensity score weighting estimator for causal inference with heterogeneous data. Instead of adjusting the distribution shift separately, we use weighted propensity score models to collaboratively adjust for the distribution shift. Our method shows significant improvements over the methods based on meta-analysis when heterogeneity increases. By incorporating outcome regression models, we prove the asymptotic normality when the covariates have dimension $d<8$. Our methods preserve privacy at individual sites by implementing federated learning protocols",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Guo",
      "Sai Praneeth Karimireddy",
      "Michael Jordan"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24d.html": {
    "title": "ACM-MILP: Adaptive Constraint Modification via Grouping and Selection for Hardness-Preserving MILP Instance Generation",
    "volume": "main",
    "abstract": "Data plays a pivotal role in the development of both classic and learning-based methods for Mixed-Integer Linear Programming (MILP). However, the scarcity of data in real-world applications underscores the necessity for MILP instance generation methods. Currently, these methods primarily rely on iterating random single-constraint modifications, disregarding the underlying problem structure with constraint interrelations, thereby leading to compromised quality and solvability. In this paper, we propose ACM-MILP, a framework for MILP instance generation, to achieve adaptive constraint modification and constraint interrelation modeling. It employs an adaptive constraint selection mechanism based on probability estimation within the latent space to preserve instance characteristics. Meanwhile, it detects and groups strongly related constraints through community detection, enabling collective modifications that account for constraint dependencies. Experimental results show significant improvements in problem-solving hardness similarity under our framework. Additionally, in the downstream task, we showcase the efficacy of our generated instances for hyperparameter tuning. Source code is available: https://github.com/Thinklab-SJTU/ACM-MILP",
    "checked": true,
    "id": "49bd0835bb4ab37e1cbce02e53aaa146856bd3a1",
    "semantic_title": "acm-milp: adaptive constraint modification via grouping and selection for hardness-preserving milp instance generation",
    "citation_count": 2,
    "authors": [
      "Ziao Guo",
      "Yang Li",
      "Chang Liu",
      "Wenli Ouyang",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24e.html": {
    "title": "On the Embedding Collapse when Scaling up Recommendation Models",
    "volume": "main",
    "abstract": "Recent advances in foundation models have led to a promising trend of developing large recommendation models to leverage vast amounts of available data. Still, mainstream models remain embarrassingly small in size and naive enlarging does not lead to sufficient performance gain, suggesting a deficiency in the model scalability. In this paper, we identify the embedding collapse phenomenon as the inhibition of scalability, wherein the embedding matrix tends to occupy a low-dimensional subspace. Through empirical and theoretical analysis, we demonstrate a two-sided effect of feature interaction specific to recommendation models. On the one hand, interacting with collapsed embeddings restricts embedding learning and exacerbates the collapse issue. On the other hand, interaction is crucial in mitigating the fitting of spurious features as a scalability guarantee. Based on our analysis, we propose a simple yet effective multi-embedding design incorporating embedding-set-specific interaction modules to learn embedding sets with large diversity and thus reduce collapse. Extensive experiments demonstrate that this proposed design provides consistent scalability and effective collapse mitigation for various recommendation models. Code is available at this repository: https://github.com/thuml/Multi-Embedding",
    "checked": true,
    "id": "4544893b91b3bcae9110180dac425c7024eb1dd1",
    "semantic_title": "on the embedding collapse when scaling up recommendation models",
    "citation_count": 10,
    "authors": [
      "Xingzhuo Guo",
      "Junwei Pan",
      "Ximei Wang",
      "Baixu Chen",
      "Jie Jiang",
      "Mingsheng Long"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24f.html": {
    "title": "FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a machine learning paradigm that safeguards privacy by retaining client data on edge devices. However, optimizing FL in practice can be challenging due to the diverse and heterogeneous nature of the learning system. Though recent research has focused on improving the optimization of FL when distribution shifts occur among clients, ensuring global performance when multiple types of distribution shifts occur simultaneously among clients—such as feature distribution shift, label distribution shift, and concept shift—remain under-explored. In this paper, we identify the learning challenges posed by the simultaneous occurrence of diverse distribution shifts and propose a clustering principle to overcome these challenges. Through our research, we find that existing methods fail to address the clustering principle. Therefore, we propose a novel clustering algorithm framework, dubbed as FedRC, which adheres to our proposed clustering principle by incorporating a bi-level optimization problem and a novel objective function. Extensive experiments demonstrate that FedRC significantly outperforms other SOTA cluster-based FL methods. Our code will be publicly available",
    "checked": true,
    "id": "f440301de23bf62c11bd3f4865e4165bf59e85f4",
    "semantic_title": "fedrc: tackling diverse distribution shifts challenge in federated learning by robust clustering",
    "citation_count": 4,
    "authors": [
      "Yongxin Guo",
      "Xiaoying Tang",
      "Tao Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24g.html": {
    "title": "Compressing Large Language Models by Joint Sparsification and Quantization",
    "volume": "main",
    "abstract": "In this paper, we introduce a novel model compression technique named Joint Sparsification and Quantization (JSQ), explicitly tailored for large language models (LLMs). Traditional methods employ either sparsification or quantization individually to compress LLMs, leading to performance degradation at high compression ratios. In contrast, our JSQ approach integrates sparsification and quantization cohesively. As sparsification tend to preserve outliers that is harmful to quantization, we introduce a novel sparsity metric to serves as a bridge between the sparsification and quantization. Moreover, it is proven outliers in LLMs have significant impact but harmful to compression. Current solutions are highly coupled with quantization process, which is not helpful to sparsification. To this end, we also introduce a search-based activation editor to automatically eliminate relatively useless outliers. Comprehensive experiments across various datasets and architectures affirm the efficacy of our JSQ framework. Notably, our JSQ achieves 7.96$\\times$ computation reduction without crashing for the representative model LLaMA. This accomplishment stands in stark contrast to the limitations of most state-of-the-art LLM compression methods, which typically fail under such extreme compression ratios. Our code is released at https://github.com/uanu2002/JSQ",
    "checked": true,
    "id": "346714f374346f7afa82eb8ff50698b72feada6d",
    "semantic_title": "compressing large language models by joint sparsification and quantization",
    "citation_count": 8,
    "authors": [
      "Jinyang Guo",
      "Jianyu Wu",
      "Zining Wang",
      "Jiaheng Liu",
      "Ge Yang",
      "Yifu Ding",
      "Ruihao Gong",
      "Haotong Qin",
      "Xianglong Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24h.html": {
    "title": "Automated Loss function Search for Class-imbalanced Node Classification",
    "volume": "main",
    "abstract": "Class-imbalanced node classification tasks are prevalent in real-world scenarios. Due to the uneven distribution of nodes across different classes, learning high-quality node representations remains a challenging endeavor. The engineering of loss functions has shown promising potential in addressing this issue. It involves the meticulous design of loss functions, utilizing information about the quantities of nodes in different categories and the network's topology to learn unbiased node representations. However, the design of these loss functions heavily relies on human expert knowledge and exhibits limited adaptability to specific target tasks. In this paper, we introduce a high-performance, flexible, and generalizable automated loss function search framework to tackle this challenge. Across 15 combinations of graph neural networks and datasets, our framework achieves a significant improvement in performance compared to state-of-the-art methods. Additionally, we observe that homophily in graph-structured data significantly contributes to the transferability of the proposed framework",
    "checked": true,
    "id": "061bcc04e6155c3ef1c2e6c563770cf056a63104",
    "semantic_title": "automated loss function search for class-imbalanced node classification",
    "citation_count": 0,
    "authors": [
      "Xinyu Guo",
      "Kai Wu",
      "Xiaoyu Zhang",
      "Jing Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24i.html": {
    "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability",
    "volume": "main",
    "abstract": "Jailbreaks on large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent (suffix) attack with continuation constraint, but also allow us to address new controllable attack settings such as revising a user query adversarially with paraphrasing constraint, and inserting stealthy attacks in context with position constraint. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5, and GPT-4) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingang Guo",
      "Fangxu Yu",
      "Huan Zhang",
      "Lianhui Qin",
      "Bin Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/guo24j.html": {
    "title": "Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on supervised learning with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT). Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is conditioned on",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Guo",
      "Weichao Zhou",
      "Wenchao Li"
    ]
  },
  "https://proceedings.mlr.press/v235/gupta24a.html": {
    "title": "Diffusion Posterior Sampling is Computationally Intractable",
    "volume": "main",
    "abstract": "Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \\mid x)$ and a measurement $y$, and would like to sample from $p(x \\mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time. In this paper we show that posterior sampling is computationally intractable: under the most basic assumption in cryptography—that one-way functions exist—there are instances for which every algorithm takes superpolynomial time, even though unconditional sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert",
    "checked": true,
    "id": "911765f85f9ba6516fac8078ad2182abad604b89",
    "semantic_title": "diffusion posterior sampling is computationally intractable",
    "citation_count": 5,
    "authors": [
      "Shivam Gupta",
      "Ajil Jalal",
      "Aditya Parulekar",
      "Eric Price",
      "Zhiyang Xun"
    ]
  },
  "https://proceedings.mlr.press/v235/gupta24b.html": {
    "title": "xT: Nested Tokenization for Larger Context in Large Images",
    "volume": "main",
    "abstract": "Modern computer vision pipelines handle large images in one of two sub-optimal ways: down-sampling or cropping. These two methods incur significant losses in the amount of information and context present in an image. There are many downstream applications in which global context matters as much as high frequency details, such as in real-world satellite imagery; in such cases researchers have to make the uncomfortable choice of which information to discard. We introduce xT, a simple framework for vision transformers which effectively aggregates global context with local details and can model large images end-to-end on contemporary GPUs. We select a set of benchmark datasets across classic vision tasks which accurately reflect a vision model's ability to understand truly large images and incorporate fine details over large scales and assess our method's improvement on them. xT is a streaming, two-stage architecture that adapts existing vision backbones and long sequence language models to effectively model large images without quadratic memory growth. We are able to increase accuracy by up to 8.6% on challenging classification tasks and F1 score by 11.6 on context-dependent segmentation on images as large as 29,000 x 29,000 pixels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ritwik Gupta",
      "Shufan Li",
      "Tyler Zhu",
      "Jitendra Malik",
      "Trevor Darrell",
      "Karttikeya Mangalam"
    ]
  },
  "https://proceedings.mlr.press/v235/gupta24c.html": {
    "title": "GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks",
    "volume": "main",
    "abstract": "In-Context Learning (ICL) is the ability of Large Language Models (LLMs) to perform new tasks when conditioned on prompts comprising a few task examples. However, ICL performance can be critically sensitive to the choice of examples. To dynamically select the best examples for every test input, we propose Example Gisting, a novel approach for training example encoders through supervised finetuning with an attention bottleneck between the inputs and outputs. These gist models form the basis for GistScore, a novel metric for scoring and selecting informative examples. Further, we experiment with two variations: (1) finetuning gist models for each dataset and (2) multi-task training a single model on a large collection of datasets. The latter can be used for new tasks out-of-the-box, enabling a training-free ICL pipeline. Evaluations with 21 datasets spanning 9 tasks and 8 diverse LLMs show that our fine-tuned models get state-of-the-art ICL performance with over 20% absolute gain over off-the-shelf retrievers and 5% over the best prior methods. Further, our multi-task model generalizes well to new tasks, datasets, and prompt templates. Selection using this model matches or outperforms prior methods while being three orders of magnitude faster than the strongest training-free baseline",
    "checked": true,
    "id": "954b3be7d3800f9c3a78f38f5d6ae841bb1dc957",
    "semantic_title": "gistscore: learning better representations for in-context example selection with gist bottlenecks",
    "citation_count": 5,
    "authors": [
      "Shivanshu Gupta",
      "Clemens Rosenbaum",
      "Ethan R. Elenberg"
    ]
  },
  "https://proceedings.mlr.press/v235/gushchin24a.html": {
    "title": "Light and Optimal Schrödinger Bridge Matching",
    "volume": "main",
    "abstract": "Schrödinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB exploit the pervasive bridge matching procedures. Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them. In particular, given the EOT plan, these procedures can be adapted to solve SB. This fact is heavily exploited by recent works giving rives to matching-based SB solvers. The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training. We address these limitations and propose a novel procedure to learn SB which we call the optimal Schrödinger bridge matching. It exploits the optimal parameterization of the diffusion process and provably recovers the SB process (a) with a single bridge matching step and (b) with arbitrary transport plan as the input. Furthermore, we show that the optimal bridge matching objective coincides with the recently discovered energy-based modeling (EBM) objectives to learn EOT/SB. Inspired by this observation, we develop a light solver (which we call LightSB-M) to implement optimal matching in practice using the Gaussian mixture parameterization of the adjusted Schrödinger potential. We experimentally showcase the performance of our solver in a range of practical tasks",
    "checked": true,
    "id": "d3190f926126e414bfa2ea18deec0a665656d214",
    "semantic_title": "light and optimal schrödinger bridge matching",
    "citation_count": 6,
    "authors": [
      "Nikita Gushchin",
      "Sergei Kholkin",
      "Evgeny Burnaev",
      "Alexander Korotin"
    ]
  },
  "https://proceedings.mlr.press/v235/h-zargarbashi24a.html": {
    "title": "Robust Yet Efficient Conformal Prediction Sets",
    "volume": "main",
    "abstract": "Conformal prediction (CP) can convert any model's output into prediction sets guaranteed to include the true label with any user-specified probability. However, same as the model itself, CP is vulnerable to adversarial test examples (evasion) and perturbed calibration data (poisoning). We derive provably robust sets by bounding the worst-case change in conformity scores. Our tighter bounds lead to more efficient sets. We cover both continuous and discrete (sparse) data and our guarantees work both for evasion and poisoning attacks (on both features and labels)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soroush H. Zargarbashi",
      "Mohammad Sadegh Akhondzadeh",
      "Aleksandar Bojchevski"
    ]
  },
  "https://proceedings.mlr.press/v235/haddadan24a.html": {
    "title": "Optimally Improving Cooperative Learning in a Social Setting",
    "volume": "main",
    "abstract": "We consider a cooperative learning scenario where a collection of networked agents with individually owned classifiers dynamically update their predictions, for the same classification task, through communication or observations of each other's predictions. Clearly if highly influential vertices use erroneous classifiers, there will be a negative effect on the accuracy of all the agents in the network. We ask the following question: how can we optimally fix the prediction of a few classifiers so as maximize the overall accuracy in the entire network. To this end we consider an aggregate and an egalitarian objective function. We show a polynomial time algorithm for optimizing the aggregate objective function, and show that optimizing the egalitarian objective function is NP-hard. Furthermore, we develop approximation algorithms for the egalitarian improvement. The performance of all of our algorithms are guaranteed by mathematical analysis and backed by experiments on synthetic and real data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahrzad Haddadan",
      "Cheng Xin",
      "Jie Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/hagnberger24a.html": {
    "title": "Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations",
    "volume": "main",
    "abstract": "Transformer models are increasingly used for solving Partial Differential Equations (PDEs). Several adaptations have been proposed, all of which suffer from the typical problems of Transformers, such as quadratic memory and time complexity. Furthermore, all prevalent architectures for PDE solving lack at least one of several desirable properties of an ideal surrogate model, such as (i) generalization to PDE parameters not seen during training, (ii) spatial and temporal zero-shot super-resolution, (iii) continuous temporal extrapolation, (iv) support for 1D, 2D, and 3D PDEs, and (v) efficient inference for longer temporal rollouts. To address these limitations, we propose Vectorized Conditional Neural Fields (VCNeFs), which represent the solution of time-dependent PDEs as neural fields. Contrary to prior methods, however, VCNeFs compute, for a set of multiple spatio-temporal query points, their solutions in parallel and model their dependencies through attention mechanisms. Moreover, VCNeF can condition the neural field on both the initial conditions and the parameters of the PDEs. An extensive set of experiments demonstrates that VCNeFs are competitive with and often outperform existing ML-based surrogate models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Hagnberger",
      "Marimuthu Kalimuthu",
      "Daniel Musekamp",
      "Mathias Niepert"
    ]
  },
  "https://proceedings.mlr.press/v235/hahm24a.html": {
    "title": "Isometric Representation Learning for Disentangled Latent Space of Diffusion Models",
    "volume": "main",
    "abstract": "The latent space of diffusion model mostly still remains unexplored, despite its great success and potential in the field of generative modeling. In fact, the latent space of existing diffusion models are entangled, with a distorted mapping from its latent space to image space. To tackle this problem, we present Isometric Diffusion, equipping a diffusion model with a geometric regularizer to guide the model to learn a geometrically sound latent space of the training data manifold. This approach allows diffusion models to learn a more disentangled latent space, which enables smoother interpolation, more accurate inversion, and more precise control over attributes directly in the latent space. Our extensive experiments consisting of image interpolations, image inversions, and linear editing show the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehoon Hahm",
      "Junho Lee",
      "Sunghyun Kim",
      "Joonseok Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/hahn24a.html": {
    "title": "Pursuing Overall Welfare in Federated Learning through Sequential Decision Making",
    "volume": "main",
    "abstract": "In traditional federated learning, a single global model cannot perform equally well for all clients. Therefore, the need to achieve the client-level fairness in federated system has been emphasized, which can be realized by modifying the static aggregation scheme for updating the global model to an adaptive one, in response to the local signals of the participating clients. Our work reveals that existing fairness-aware aggregation strategies can be unified into an online convex optimization framework, in other words, a central server's sequential decision making process. To enhance the decision making capability, we propose simple and intuitive improvements for suboptimal designs within existing methods, presenting $\\texttt{AAggFF}$. Considering practical requirements, we further subdivide our method tailored for the cross-device and the cross-silo settings, respectively. Theoretical analyses guarantee sublinear regret upper bounds for both settings: $\\mathcal{O}(\\sqrt{T \\log{K}})$ for the cross-device setting, and $\\mathcal{O}(K \\log{T})$ for the cross-silo setting, with $K$ clients and $T$ federation rounds. Extensive experiments demonstrate that the federated system equipped with $\\texttt{AAggFF}$ achieves better degree of client-level fairness than existing methods in both practical settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seok-Ju Hahn",
      "Gi-Soo Kim",
      "Junghye Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/hajj24a.html": {
    "title": "Incorporating probabilistic domain knowledge into deep multiple instance learning",
    "volume": "main",
    "abstract": "Deep learning methods, including deep multiple instance learning methods, have been criticized for their limited ability to incorporate domain knowledge. A reason that knowledge incorporation is challenging in deep learning is that the models usually lack a mapping between their model components and the entities of the domain, making it a non-trivial task to incorporate probabilistic prior information. In this work, we show that such a mapping between domain entities and model components can be defined for a multiple instance learning setting and propose a framework DeeMILIP that encompasses multiple strategies to exploit this mapping for prior knowledge incorporation. We motivate and formalize these strategies from a probabilistic perspective. Experiments on an immune-based diagnostics case show that our proposed strategies allow to learn generalizable models even in settings with weak signals, limited dataset size, and limited compute",
    "checked": true,
    "id": "df9c16488e82aa0a174d0ec69fcb72d83d6f07fe",
    "semantic_title": "incorporating probabilistic domain knowledge into deep multiple instance learning",
    "citation_count": 0,
    "authors": [
      "Ghadi S. Al Hajj",
      "Aliaksandr Hubin",
      "Chakravarthi Kanduri",
      "Milena Pavlovic",
      "Knut Dagestad Rand",
      "Michael Widrich",
      "Anne Schistad Solberg",
      "Victor Greiff",
      "Johan Pensar",
      "Günter Klambauer",
      "Geir Kjetil Sandve"
    ]
  },
  "https://proceedings.mlr.press/v235/halawi24a.html": {
    "title": "Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation",
    "volume": "main",
    "abstract": "Black-box finetuning is an emerging interface for adapting state-of-the-art language models to user needs. However, such access may also let malicious actors undermine model safety. To demonstrate the challenge of defending finetuning interfaces, we introduce covert malicious finetuning, a method to compromise model safety via finetuning while evading detection. Our method constructs a malicious dataset where every individual datapoint appears innocuous, but finetuning on the dataset teaches the model to respond to encoded harmful requests with encoded harmful responses. Applied to GPT-4, our method produces a finetuned model that acts on harmful instructions 99% of the time and avoids detection by defense mechanisms such as dataset inspection, safety evaluations, and input/output classifiers. Our findings question whether black-box finetuning access can be secured against sophisticated adversaries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danny Halawi",
      "Alexander Wei",
      "Eric Wallace",
      "Tony Tong Wang",
      "Nika Haghtalab",
      "Jacob Steinhardt"
    ]
  },
  "https://proceedings.mlr.press/v235/hallak24a.html": {
    "title": "A Study of First-Order Methods with a Deterministic Relative-Error Gradient Oracle",
    "volume": "main",
    "abstract": "This paper studies the theoretical guarantees of the classical projected gradient and conditional gradient methods applied to constrained optimization problems with biased relative-error gradient oracles. These oracles are used in various settings, such as distributed optimization systems or derivative-free optimization, and are particularly common when gradients are compressed, quantized, or estimated via finite differences computations. Several settings are investigated: Optimization over the box with a coordinate-wise erroneous gradient oracle, optimization over a general compact convex set, and three more specific scenarios. Convergence guarantees are established with respect to the relative-error magnitude, and in particular, we show that the conditional gradient is invariant to relative-error when applied over the box with a coordinate-wise erroneous gradient oracle, and the projected gradient maintains its convergence guarantees when optimizing a nonconvex objective function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadav Hallak",
      "Kfir Yehuda Levy"
    ]
  },
  "https://proceedings.mlr.press/v235/hamed24a.html": {
    "title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
    "volume": "main",
    "abstract": "Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question whether and how an agent can \"dream better\" in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way. In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hany Hamed",
      "Subin Kim",
      "Dongyeong Kim",
      "Jaesik Yoon",
      "Sungjin Ahn"
    ]
  },
  "https://proceedings.mlr.press/v235/han24a.html": {
    "title": "UGrid: An Efficient-And-Rigorous Neural Multigrid Solver for Linear PDEs",
    "volume": "main",
    "abstract": "Numerical solvers of Partial Differential Equations (PDEs) are of fundamental significance to science and engineering. To date, the historical reliance on legacy techniques has circumscribed possible integration of big data knowledge and exhibits sub-optimal efficiency for certain PDE formulations, while data-driven neural methods typically lack mathematical guarantee of convergence and correctness. This paper articulates a mathematically rigorous neural solver for linear PDEs. The proposed UGrid solver, built upon the principled integration of U-Net and MultiGrid, manifests a mathematically rigorous proof of both convergence and correctness, and showcases high numerical accuracy, as well as strong generalization power to various input geometry/values and multiple PDE formulations. In addition, we devise a new residual loss metric, which enables unsupervised training and affords more stability and a larger solution space over the legacy losses",
    "checked": true,
    "id": "41a937613b023d03ef2f43e7b8bb5556cf7d969d",
    "semantic_title": "ugrid: an efficient-and-rigorous neural multigrid solver for linear pdes",
    "citation_count": 0,
    "authors": [
      "Xi Han",
      "Fei Hou",
      "Hong Qin"
    ]
  },
  "https://proceedings.mlr.press/v235/han24b.html": {
    "title": "Model Assessment and Selection under Temporal Distribution Shift",
    "volume": "main",
    "abstract": "We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and empirical experiments underscore the adaptivity of our proposed methods to the non-stationarity in data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elise Han",
      "Chengpiao Huang",
      "Kaizheng Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/han24c.html": {
    "title": "Riemannian coordinate descent algorithms on matrix manifolds",
    "volume": "main",
    "abstract": "Many machine learning applications are naturally formulated as optimization problems on Riemannian manifolds. The main idea behind Riemannian optimization is to maintain the feasibility of the variables while moving along a descent direction on the manifold. This results in updating all the variables at every iteration. In this work, we provide a general framework for developing computationally efficient coordinate descent (CD) algorithms on matrix manifolds that allows updating only a few variables at every iteration while adhering to the manifold constraint. In particular, we propose CD algorithms for various manifolds such as Stiefel, Grassmann, (generalized) hyperbolic, symplectic, and symmetric positive (semi)definite. While the cost per iteration of the proposed CD algorithms is low, we further develop a more efficient variant via a first-order approximation of the objective function. We analyze their convergence and complexity, and empirically illustrate their efficacy in several applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Han",
      "Pratik Jawanpuria",
      "Bamdev Mishra"
    ]
  },
  "https://proceedings.mlr.press/v235/han24d.html": {
    "title": "Prototypical Transformer As Unified Motion Learners",
    "volume": "main",
    "abstract": "In this work, we introduce the Prototypical Transformer (ProtoFormer), a general and unified framework that approaches various motion tasks from a prototype perspective. ProtoFormer seamlessly integrates prototype learning with Transformer by thoughtfully considering motion dynamics, introducing two innovative designs. First, Cross-Attention Prototyping discovers prototypes based on signature motion patterns, providing transparency in understanding motion scenes. Second, Latent Synchronization guides feature representation learning via prototypes, effectively mitigating the problem of motion uncertainty. Empirical results demonstrate that our approach achieves competitive performance on popular motion tasks such as optical flow and scene depth. Furthermore, it exhibits generality across various downstream tasks, including object tracking and video stabilization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Han",
      "Yawen Lu",
      "Guohao Sun",
      "James Chenhao Liang",
      "Zhiwen Cao",
      "Qifan Wang",
      "Qiang Guan",
      "Sohail Dianat",
      "Raghuveer Rao",
      "Tong Geng",
      "Zhiqiang Tao",
      "Dongfang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/han24e.html": {
    "title": "SIN: Selective and Interpretable Normalization for Long-Term Time Series Forecasting",
    "volume": "main",
    "abstract": "In real-world applications, time series data frequently exhibit non-stationarity, with statistics changing over time. This variability undermines the forecasting accuracy of deep learning models that are trained on historical data but deployed for future prediction. A common approach to mitigate this issue involves normalizing the data to counteract statistical drift, followed by denormalization on the prediction. However, existing methods often employ heuristic normalization techniques that do not fully account for the unique characteristics of the series. Our paper addresses the critical question in this context: which statistics should be removed and restored? We argue that the statistics selected for normalization should exhibit both local invariance and global variability to ensure their correctness and helpfulness. To this end, we propose the Selective and Interpretable Normalization methodology, dubbed SIN. This approach maximizes the covariance between a given look-back window and its subsequent future values, thereby identifying key statistics for normalization and simultaneously learning the corresponding normalization transformations. The interpretable framework can be used to explain the success and limitations of some popular normalization methods. By integrating SIN, we demonstrate improvements in the performance of several prevalent forecasting models, thereby validating the utility of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Han",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/han24f.html": {
    "title": "Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions. The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning. The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time. Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations. As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT",
    "checked": true,
    "id": "e4eaaaf1e435061bb2eed5f6dc187e453c3bb086",
    "semantic_title": "large language models can automatically engineer features for few-shot tabular learning",
    "citation_count": 6,
    "authors": [
      "Sungwon Han",
      "Jinsung Yoon",
      "Sercan O Arik",
      "Tomas Pfister"
    ]
  },
  "https://proceedings.mlr.press/v235/han24g.html": {
    "title": "Improving Group Robustness on Spurious Correlation Requires Preciser Group Inference",
    "volume": "main",
    "abstract": "Standard empirical risk minimization (ERM) models may prioritize learning spurious correlations between spurious features and true labels, leading to poor accuracy on groups where these correlations do not hold. Mitigating this issue often requires expensive spurious attribute (group) labels or relies on trained ERM models to infer group labels when group information is unavailable. However, the significant performance gap in worst-group accuracy between using pseudo group labels and using oracle group labels inspires us to consider further improving group robustness through preciser group inference. Therefore, we propose GIC, a novel method that accurately infers group labels, resulting in improved worst-group performance. GIC trains a spurious attribute classifier based on two key properties of spurious correlations: (1) high correlation between spurious attributes and true labels, and (2) variability in this correlation between datasets with different group distributions. Empirical studies on multiple datasets demonstrate the effectiveness of GIC in inferring group labels, and combining GIC with various downstream invariant learning methods improves worst-group accuracy, showcasing its powerful flexibility. Additionally, through analyzing the misclassifications in GIC, we identify an interesting phenomenon called semantic consistency, which may contribute to better decoupling the association between spurious attributes and labels, thereby mitigating spurious correlation. The code for GIC is available at https://github.com/yujinhanml/GIC9",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujin Han",
      "Difan Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/hang24a.html": {
    "title": "Binary Decomposition: A Problem Transformation Perspective for Open-Set Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Semi-supervised learning (SSL) is a classical machine learning paradigm dealing with labeled and unlabeled data. However, it often suffers performance degradation in real-world open-set scenarios, where unlabeled data contains outliers from novel categories that do not appear in labeled data. Existing studies commonly tackle this challenging open-set SSL problem with detect-and-filter strategy, which attempts to purify unlabeled data by detecting and filtering outliers. In this paper, we propose a novel binary decomposition strategy, which refrains from error-prone procedure of outlier detection by directly transforming the original open-set SSL problem into a number of standard binary SSL problems. Accordingly, a concise yet effective approach named BDMatch is presented. BDMatch confronts two attendant issues brought by binary decomposition, i.e. class-imbalance and representation-compromise, with adaptive logit adjustment and label-specific feature learning respectively. Comprehensive experiments on diversified benchmarks clearly validate the superiority of BDMatch as well as the effectiveness of our binary decomposition strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Yi Hang",
      "Min-Ling Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/hans24a.html": {
    "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
    "volume": "main",
    "abstract": "Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data. Code available at https://github.com/ahans30/Binoculars",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhimanyu Hans",
      "Avi Schwarzschild",
      "Valeriia Cherepanova",
      "Hamid Kazemi",
      "Aniruddha Saha",
      "Micah Goldblum",
      "Jonas Geiping",
      "Tom Goldstein"
    ]
  },
  "https://proceedings.mlr.press/v235/hansen24a.html": {
    "title": "Interpreting Equivariant Representations",
    "volume": "main",
    "abstract": "Latent representations are extensively used for tasks like visualization, interpolation, or feature extraction in deep learning models. This paper demonstrates the importance of considering the inductive bias imposed by an equivariant model when using latent representations as neglecting these biases can lead to decreased performance in downstream tasks. We propose principles for choosing invariant projections of latent representations and show their effectiveness in two examples: A permutation equivariant variational auto-encoder for molecular graph generation, where an invariant projection can be designed to maintain information without loss, and for a rotation-equivariant representation in image classification, where random invariant projections proves to retain a high degree of information. In both cases, the analysis of invariant latent representations proves superior to their equivariant counterparts. Finally, we illustrate that the phenomena documented here for equivariant neural networks have counterparts in standard neural networks where invariance is encouraged via augmentation",
    "checked": true,
    "id": "fa423012fc36e88854e80054484842a788f3d2b5",
    "semantic_title": "interpreting equivariant representations",
    "citation_count": 0,
    "authors": [
      "Andreas Abildtrup Hansen",
      "Anna Calissano",
      "Aasa Feragen"
    ]
  },
  "https://proceedings.mlr.press/v235/hao24a.html": {
    "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors",
    "volume": "main",
    "abstract": "Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongchang Hao",
      "Yanshuai Cao",
      "Lili Mou"
    ]
  },
  "https://proceedings.mlr.press/v235/hao24b.html": {
    "title": "Data-efficient Large Vision Models through Sequential Autoregression",
    "volume": "main",
    "abstract": "Training general-purpose vision models on purely sequential visual data, eschewing linguistic inputs, has heralded a new frontier in visual understanding. These models are intended to not only comprehend but also seamlessly transit to out-of-domain tasks. However, current endeavors are hamstrung by an over-reliance on colossal models, exemplified by models with upwards of 3B parameters, and the necessity for an extensive corpus of visual data, often comprising a staggering 400B tokens. In this paper, we delve into the development of an efficient, autoregression-based vision model, innovatively architected to operate on a limited dataset. We meticulously demonstrate how this model achieves proficiency in a spectrum of visual tasks spanning both high-level and low-level semantic understanding during the testing phase. Our empirical evaluations underscore the model's agility in adapting to various tasks, heralding a significant reduction in the parameter footprint, and a marked decrease in training data requirements, thereby paving the way for more sustainable and accessible advancements in the field of generalist vision models. The code is available at https://github.com/ggjy/DeLVM",
    "checked": true,
    "id": "baf1a6b0ee20422ccd9b76eb0a8287ee2ee48082",
    "semantic_title": "data-efficient large vision models through sequential autoregression",
    "citation_count": 6,
    "authors": [
      "Zhiwei Hao",
      "Jianyuan Guo",
      "Chengcheng Wang",
      "Yehui Tang",
      "Han Wu",
      "Han Hu",
      "Kai Han",
      "Chang Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/hao24c.html": {
    "title": "MGit: A Model Versioning and Management System",
    "volume": "main",
    "abstract": "New ML models are often derived from existing ones (e.g., through fine-tuning, quantization or distillation), forming an ecosystem where models are related to each other and can share structure or even parameter values. Managing such a large and evolving ecosystem of model derivatives is challenging. For instance, the overhead of storing all such models is high, and models may inherit bugs from related models, complicating error attribution and debugging. In this paper, we propose a model versioning and management system called MGit that makes it easier to store, test, update, and collaborate on related models. MGit introduces a lineage graph that records the relationships between models, optimizations to efficiently store model parameters, and abstractions over this lineage graph that facilitate model testing, updating and collaboration. We find that MGit works well in practice: MGit is able to reduce model storage footprint by up to 7$\\times$. Additionally, in a user study with 20 ML practitioners, users complete a model updating task 3$\\times$ faster on average with MGit",
    "checked": true,
    "id": "e80fdf6055749517397eaf2c07ddc5b3615c9410",
    "semantic_title": "mgit: a model versioning and management system",
    "citation_count": 1,
    "authors": [
      "Wei Hao",
      "Daniel Mendoza",
      "Rafael Mendes",
      "Deepak Narayanan",
      "Amar Phanishayee",
      "Asaf Cidon",
      "Junfeng Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/hao24d.html": {
    "title": "DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training",
    "volume": "main",
    "abstract": "Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongkai Hao",
      "Chang Su",
      "Songming Liu",
      "Julius Berner",
      "Chengyang Ying",
      "Hang Su",
      "Anima Anandkumar",
      "Jian Song",
      "Jun Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/harker24a.html": {
    "title": "Convergence Guarantees for the DeepWalk Embedding on Block Models",
    "volume": "main",
    "abstract": "Graph embeddings have emerged as a powerful tool for understanding the structure of graphs. Unlike classical spectral methods, recent methods such as DeepWalk, Node2Vec, etc. are based on solving nonlinear optimization problems on the graph, using local information obtained by performing random walks. These techniques have empirically been shown to produce \"better\" embeddings than their classical counterparts. However, due to their reliance on solving a nonconvex optimization problem, obtaining theoretical guarantees on the properties of the solution has remained a challenge, even for simple classes of graphs. In this work, we show convergence properties for the DeepWalk algorithm on graphs obtained from the Stochastic Block Model (SBM). Despite being simplistic, the SBM has proved to be a classic model for analyzing the behavior of algorithms on large graphs. Our results mirror the existing ones for spectral embeddings on SBMs, showing that even in the case of one-dimensional embeddings, the output of the DeepWalk algorithm provably recovers the cluster structure with high probability",
    "checked": true,
    "id": "aa11fc0ac4022a23df16519c180cc85d5244deef",
    "semantic_title": "convergence guarantees for the deepwalk embedding on block models",
    "citation_count": 0,
    "authors": [
      "Christopher Harker",
      "Aditya Bhaskara"
    ]
  },
  "https://proceedings.mlr.press/v235/harviainen24a.html": {
    "title": "Estimating the Permanent by Nesting Importance Sampling",
    "volume": "main",
    "abstract": "Sequential importance sampling (SIS) is one of the prominent methods for estimating high-dimensional integrals. For example, it is empirically the most efficient method known for estimating the permanent of nonnegative matrices, a notorious problem with numerous applications in computer science, statistics, and other fields. Unfortunately, SIS typically fails to provide accuracy guarantees due to difficulties in bounding the variance of the importance weights; for estimating the permanent with accuracy guarantees, the most efficient practical methods known are based on rejection sampling. Taking the best of both worlds, we give a variant of SIS, in which sampling is proportional to the upper bound used in rejection sampling. We show that this method is provably more efficient than its rejection sampling counterpart, particularly in high accuracy regimes. On estimating the permanent, we empirically obtain up to two orders-of-magnitude speedups over a state-of-the-art rejection sampling method",
    "checked": true,
    "id": "1d039e2fb4aa73c6955ab8d1d2168580bcf0fa86",
    "semantic_title": "estimating the permanent by nesting importance sampling",
    "citation_count": 0,
    "authors": [
      "Juha Harviainen",
      "Mikko Koivisto"
    ]
  },
  "https://proceedings.mlr.press/v235/hashimoto24a.html": {
    "title": "Position: $C^*$-Algebraic Machine Learning $-$ Moving in a New Direction",
    "volume": "main",
    "abstract": "Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra. We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning. The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers. It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models. We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks. Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications",
    "checked": false,
    "id": "04a9efc1e5bd07e9f2bf7e9f58aa4dbb63ff9ccb",
    "semantic_title": "position: c∗-algebraic machine learning - moving in a new direction",
    "citation_count": 0,
    "authors": [
      "Yuka Hashimoto",
      "Masahiro Ikeda",
      "Hachem Kadri"
    ]
  },
  "https://proceedings.mlr.press/v235/havens24a.html": {
    "title": "Fine-grained Local Sensitivity Analysis of Standard Dot-Product Self-Attention",
    "volume": "main",
    "abstract": "Self-attention has been widely used in various machine learning models, such as vision transformers. The standard dot-product self-attention is arguably the most popular structure, and there is a growing interest in understanding the mathematical properties of such attention mechanisms. This paper presents a fine-grained local sensitivity analysis of the standard dot-product self-attention, leading to new non-vacuous certified robustness results for vision transformers. Despite the well-known fact that dot-product self-attention is not (globally) Lipschitz, we develop new theoretical analysis of Local Fine-grained Attention Sensitivity (LoFAST) quantifying the effect of input feature perturbations on the attention output. Our analysis reveals that the local sensitivity of dot-product self-attention to $\\ell_2$ perturbations can actually be controlled by several key quantities associated with the attention weight matrices and the unperturbed input. We empirically validate our theoretical findings by computing non-vacuous certified $\\ell_2$-robustness for vision transformers on CIFAR-10 and SVHN datasets. The code for LoFAST is available at https://github.com/AaronHavens/LoFAST",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron J Havens",
      "Alexandre Araujo",
      "Huan Zhang",
      "Bin Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/haviv24a.html": {
    "title": "Wasserstein Wormhole: Scalable Optimal Transport Distance with Transformer",
    "volume": "main",
    "abstract": "Optimal transport (OT) and the related Wasserstein metric ($W$) are powerful and ubiquitous tools for comparing distributions. However, computing pairwise Wasserstein distances rapidly becomes intractable as cohort size grows. An attractive alternative would be to find an embedding space in which pairwise Euclidean distances map to OT distances, akin to standard multidimensional scaling (MDS). We present Wasserstein Wormhole, a transformer-based autoencoder that embeds empirical distributions into a latent space wherein Euclidean distances approximate OT distances. Extending MDS theory, we show that our objective function implies a bound on the error incurred when embedding non-Euclidean distances. Empirically, distances between Wormhole embeddings closely match Wasserstein distances, enabling linear time computation of OT distances. Along with an encoder that maps distributions to embeddings, Wasserstein Wormhole includes a decoder that maps embeddings back to distributions, allowing for operations in the embedding space to generalize to OT spaces, such as Wasserstein barycenter estimation and OT interpolation. By lending scalability and interpretability to OT approaches, Wasserstein Wormhole unlocks new avenues for data analysis in the fields of computational geometry and single-cell biology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doron Haviv",
      "Russell Zhang Kunes",
      "Thomas Dougherty",
      "Cassandra Burdziak",
      "Tal Nawy",
      "Anna Gilbert",
      "Dana Pe’Er"
    ]
  },
  "https://proceedings.mlr.press/v235/havrilla24a.html": {
    "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",
    "volume": "main",
    "abstract": "State-of-the-art language models can exhibit reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify when and where to refine without access to external feedback. In this paper, we propose Stepwise ORMs (SORMs) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\\star}$ as a form of Process-based reward modeling. Our experiments show that SORMs can more accurately detect incorrect reasoning steps compared to ORMs, thus enabling them to give precise step-level feedback to refinement models. We then train global refinement models, which take only the question and a draft solution as input and predict a corrected solution, and local refinement models which also take as input a critique indicating the location of the first reasoning error. We generate training data for both models synthetically by reusing data used to train the SORM. We find combining global and local refinements, using the ORM as a reranker, significantly outperforms either one individually, as well as a best of three sample baseline. With this strategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned with RL) on GSM8K from 53% to 65% when greedily sampled",
    "checked": true,
    "id": "46a5ec31987a12d60ade20c6471db64c46f90106",
    "semantic_title": "glore: when, where, and how to improve llm reasoning via global and local refinements",
    "citation_count": 18,
    "authors": [
      "Alexander Havrilla",
      "Sharath Chandra Raparthy",
      "Christoforos Nalmpantis",
      "Jane Dwivedi-Yu",
      "Maksym Zhuravinskyi",
      "Eric Hambro",
      "Roberta Raileanu"
    ]
  },
  "https://proceedings.mlr.press/v235/hayase24a.html": {
    "title": "Understanding MLP-Mixer as a wide and sparse MLP",
    "volume": "main",
    "abstract": "Multi-layer perceptron (MLP) is a fundamental component of deep learning, and recent MLP-based architectures, especially the MLP-Mixer, have achieved significant empirical success. Nevertheless, our understanding of why and how the MLP-Mixer outperforms conventional MLPs remains largely unexplored. In this work, we reveal that sparseness is a key mechanism underlying the MLP-Mixers. First, the Mixers have an effective expression as a wider MLP with Kronecker-product weights, clarifying that the Mixers efficiently embody several sparseness properties explored in deep learning. In the case of linear layers, the effective expression elucidates an implicit sparse regularization caused by the model architecture and a hidden relation to Monarch matrices, which is also known as another form of sparse parameterization. Next, for general cases, we empirically demonstrate quantitative similarities between the Mixer and the unstructured sparse-weight MLPs. Following a guiding principle proposed by Golubeva, Neyshabur and Gur-Ari (2021), which fixes the number of connections and increases the width and sparsity, the Mixers can demonstrate improved performance",
    "checked": true,
    "id": "674479ef3ec1bebe8e3bf4ecb801e54c8d21e0da",
    "semantic_title": "understanding mlp-mixer as a wide and sparse mlp",
    "citation_count": 3,
    "authors": [
      "Tomohiro Hayase",
      "Ryo Karakida"
    ]
  },
  "https://proceedings.mlr.press/v235/hayderi24a.html": {
    "title": "MAGNOLIA: Matching Algorithms via GNNs for Online Value-to-go Approximation",
    "volume": "main",
    "abstract": "Online Bayesian bipartite matching is a central problem in digital marketplaces and exchanges, including advertising, crowdsourcing, ridesharing, and kidney exchange. We introduce a graph neural network (GNN) approach that emulates the problem's combinatorially-complex optimal online algorithm, which selects actions (e.g., which nodes to match) by computing each action's value-to-go (VTG)—the expected weight of the final matching if the algorithm takes that action, then acts optimally in the future. We train a GNN to estimate VTG and show empirically that this GNN returns high-weight matchings across a variety of tasks. Moreover, we identify a common family of graph distributions in spatial crowdsourcing applications, such as rideshare, under which VTG can be efficiently approximated by aggregating information within local neighborhoods in the graphs. This structure matches the local behavior of GNNs, providing theoretical justification for our approach",
    "checked": true,
    "id": "9c03f8593bbf61e3f443724232b7a1ba65450745",
    "semantic_title": "magnolia: matching algorithms via gnns for online value-to-go approximation",
    "citation_count": 0,
    "authors": [
      "Alexandre Hayderi",
      "Amin Saberi",
      "Ellen Vitercik",
      "Anders Wikum"
    ]
  },
  "https://proceedings.mlr.press/v235/hayou24a.html": {
    "title": "LoRA+: Efficient Low Rank Adaptation of Large Models",
    "volume": "main",
    "abstract": "In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in (Hu et al., 2021) leads to suboptimal finetuning of models with large width. This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate in ADAM. Using scaling arguments for large width networks, we demonstrate that the same learning rate does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen fixed ratio. We call this proposed algorithm LoRA+. In our extensive experiments, LoRA+ improves finetuning speed (up to ∼ 2X SpeedUp) and performance (1% − 2% improvements), at the same computational cost as LoRA. The code is available at https://github.com/nikhil-ghosh-berkeley/loraplus",
    "checked": true,
    "id": "241eefc1bb11e693e0fef6977a65a0a822fb8f5e",
    "semantic_title": "lora+: efficient low rank adaptation of large models",
    "citation_count": 78,
    "authors": [
      "Soufiane Hayou",
      "Nikhil Ghosh",
      "Bin Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/he24a.html": {
    "title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems",
    "volume": "main",
    "abstract": "In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world. To this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively. Under this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting. Under proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning. Additionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret. As a remedy, we introduce an $\\epsilon$-greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small. Finally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors",
    "checked": true,
    "id": "cf5fb966719eec6e75ad5f75f4ecf8d7e8723bec",
    "semantic_title": "from words to actions: unveiling the theoretical underpinnings of llm-driven autonomous systems",
    "citation_count": 1,
    "authors": [
      "Jianliang He",
      "Siyu Chen",
      "Fengzhuo Zhang",
      "Zhuoran Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/he24b.html": {
    "title": "Deep Neural Room Acoustics Primitive",
    "volume": "main",
    "abstract": "The primary objective of room acoustics is to model the intricate sound propagation dynamics from any source to receiver position within enclosed 3D spaces. These dynamics are encapsulated in the form of a 1D room impulse response (RIR). Precisely measuring RIR is difficult due to the complexity of sound propagation encompassing reflection, diffraction, and absorption. In this work, we propose to learn a continuous neural room acoustics field that implicitly encodes all essential sound propagation primitives for each enclosed 3D space, so that we can infer the RIR corresponding to arbitrary source-receiver positions unseen in the training dataset. Our framework, dubbed DeepNeRAP, is trained in a self-supervised manner without requiring direct access to RIR ground truth that is often needed in prior methods. The key idea is to design two cooperative acoustic agents to actively probe a 3D space, one emitting and the other receiving sound at various locations. Analyzing this sound helps to inversely characterize the acoustic primitives. Our framework is well-grounded in the fundamental physical principles of sound propagation, including reciprocity and globality, and thus is acoustically interpretable and meaningful. We present experiments on both synthetic and real-world datasets, demonstrating superior quality in RIR estimation against closely related methods",
    "checked": true,
    "id": "19aa38948f870b1e229d2697756ddf32614d3786",
    "semantic_title": "deep neural room acoustics primitive",
    "citation_count": 0,
    "authors": [
      "Yuhang He",
      "Anoop Cherian",
      "Gordon Wichern",
      "Andrew Markham"
    ]
  },
  "https://proceedings.mlr.press/v235/he24c.html": {
    "title": "Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation",
    "volume": "main",
    "abstract": "In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE). For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding. The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding. The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding. Theoretical analysis shows this disentanglement of positional information makes learning more effective. The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities",
    "checked": true,
    "id": "9d932de1d2f51067b6481745f28a2db345293d48",
    "semantic_title": "two stones hit one bird: bilevel positional encoding for better length extrapolation",
    "citation_count": 7,
    "authors": [
      "Zhenyu He",
      "Guhao Feng",
      "Shengjie Luo",
      "Kai Yang",
      "Liwei Wang",
      "Jingjing Xu",
      "Zhi Zhang",
      "Hongxia Yang",
      "Di He"
    ]
  },
  "https://proceedings.mlr.press/v235/he24d.html": {
    "title": "Distributed Bilevel Optimization with Communication Compression",
    "volume": "main",
    "abstract": "Stochastic bilevel optimization tackles challenges involving nested optimization structures. Its fast-growing scale nowadays necessitates efficient distributed algorithms. In conventional distributed bilevel methods, each worker must transmit full-dimensional stochastic gradients to the server every iteration, leading to significant communication overhead and thus hindering efficiency and scalability. To resolve this issue, we introduce the first family of distributed bilevel algorithms with communication compression. The primary challenge in algorithmic development is mitigating bias in hypergradient estimation caused by the nested structure. We first propose C-SOBA, a simple yet effective approach with unbiased compression and provable linear speedup convergence. However, it relies on strong assumptions on bounded gradients. To address this limitation, we explore the use of moving average, error feedback, and multi-step compression in bilevel optimization, resulting in a series of advanced algorithms with relaxed assumptions and improved convergence properties. Numerical experiments show that our compressed bilevel algorithms can achieve $10\\times$ reduction in communication overhead without severe performance degradation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong He",
      "Jie Hu",
      "Xinmeng Huang",
      "Songtao Lu",
      "Bin Wang",
      "Kun Yuan"
    ]
  },
  "https://proceedings.mlr.press/v235/he24e.html": {
    "title": "ReDiffuser: Reliable Decision-Making Using a Diffuser with Confidence Estimation",
    "volume": "main",
    "abstract": "The diffusion model has demonstrated impressive performance in offline reinforcement learning. However, non-deterministic sampling in diffusion models can lead to unstable performance. Furthermore, the lack of confidence measurements makes it difficult to evaluate the reliability and trustworthiness of the sampled decisions. To address these issues, we present ReDiffuser, which utilizes confidence estimation to ensure reliable decision-making. We achieve this by learning a confidence function based on Random Network Distillation. The confidence function measures the reliability of sampled decisions and contributes to quantitative recognition of reliable decisions. Additionally, we integrate the confidence function into task-specific sampling procedures to realize adaptive-horizon planning and value-embedded planning. Experiments show that the proposed ReDiffuser achieves state-of-the-art performance on standard offline RL datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nantian He",
      "Shaohui Li",
      "Zhi Li",
      "Yu Liu",
      "You He"
    ]
  },
  "https://proceedings.mlr.press/v235/he24f.html": {
    "title": "Domain-wise Data Acquisition to Improve Performance under Distribution Shift",
    "volume": "main",
    "abstract": "Despite notable progress in enhancing the capability of machine learning against distribution shifts, training data quality remains a bottleneck for cross-distribution generalization. Recently, from a data-centric perspective, there have been considerable efforts to improve model performance through refining the preparation of training data. Inspired by realistic scenarios, this paper addresses a practical requirement of acquiring training samples from various domains on a limited budget to facilitate model generalization to target test domain with distribution shift. Our empirical evidence indicates that the advance in data acquisition can significantly benefit the model performance on shifted data. Additionally, by leveraging unlabeled test domain data, we introduce a Domain-wise Active Acquisition framework. This framework iteratively optimizes the data acquisition strategy as training samples are accumulated, theoretically ensuring the effective approximation of test distribution. Extensive real-world experiments demonstrate our proposal's advantages in machine learning applications. The code is available at https://github.com/dongbaili/DAA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue He",
      "Dongbai Li",
      "Pengfei Tian",
      "Han Yu",
      "Jiashuo Liu",
      "Hao Zou",
      "Peng Cui"
    ]
  },
  "https://proceedings.mlr.press/v235/he24g.html": {
    "title": "Quantum Algorithm for Online Exp-concave Optimization",
    "volume": "main",
    "abstract": "We explore whether quantum advantages can be found for the zeroth-order feedback online exp-concave optimization problem, which is also known as bandit exp-concave optimization with multi-point feedback. We present quantum online quasi-Newton methods to tackle the problem and show that there exists quantum advantages for such problems. Our method approximates the Hessian by quantum estimated inexact gradient and can achieve $O(n\\log T)$ regret with $O(1)$ queries at each round, where $n$ is the dimension of the decision set and $T$ is the total decision rounds. Such regret improves the optimal classical algorithm by a factor of $T^{2/3}$",
    "checked": true,
    "id": "639eb0cbc3a6b5d5ec3e6314449b4777c7b76183",
    "semantic_title": "quantum algorithm for online exp-concave optimization",
    "citation_count": 0,
    "authors": [
      "Jianhao He",
      "Chengchang Liu",
      "Xutong Liu",
      "Lvzhou Li",
      "John C.S. Lui"
    ]
  },
  "https://proceedings.mlr.press/v235/he24h.html": {
    "title": "Riemannian Accelerated Zeroth-order Algorithm: Improved Robustness and Lower Query Complexity",
    "volume": "main",
    "abstract": "Optimization problems with access to only zeroth-order information of the objective function on Riemannian manifolds arise in various applications, spanning from statistical learning to robot learning. While various zeroth-order algorithms have been proposed in Euclidean space, they are not inherently designed to handle the challenging constraints imposed by Riemannian manifolds. The proper adaptation of zeroth-order techniques to Riemannian manifolds remained unknown until the pioneering work of (Li et al., 2023a). However, zeroth-order algorithms are widely observed to converge slowly and be unstable in practice. To alleviate these issues, we propose a Riemannian accelerated zeroth-order algorithm with improved robustness. Regarding efficiency, our accelerated algorithm has the function query complexity of $\\mathcal{O}(\\epsilon^{-7/4}d)$ for finding an $\\epsilon$-approximate first-order stationary point. By introducing a small perturbation, it exhibits a function query complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-7/4}d)$ for seeking a second-order stationary point with a high probability, matching state-of-the-art result in Euclidean space. Moreover, we further establish the almost sure convergence in the asymptotic sense through the Stable Manifold Theorem. Regarding robustness, our algorithm requires larger smoothing parameters in the order of $\\tilde{\\mathcal{O}}(\\epsilon^{7/8}d^{-1/2})$, improving the existing result by a factor of $\\tilde{\\mathcal{O}}(\\epsilon^{3/4})$",
    "checked": true,
    "id": "3ac411d2605a6ec1f4acf6cbc702a09d7f326330",
    "semantic_title": "riemannian accelerated zeroth-order algorithm: improved robustness and lower query complexity",
    "citation_count": 0,
    "authors": [
      "Chang He",
      "Zhaoye Pan",
      "Xiao Wang",
      "Bo Jiang"
    ]
  },
  "https://proceedings.mlr.press/v235/he24i.html": {
    "title": "The Effect of Weight Precision on the Neuron Count in Deep ReLU Networks",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have become pivotal in machine learning, but the impact of weight precision, such as in networks with rectified linear units (ReLU), remains underexplored. We analytically investigate the interplay of three key factors: the precision of ReLU network weights, the number of neurons, and the time of the preprocessing algorithm that generates the network description. Our study, which, to the best of our knowledge, is the first formal work on weight precision, yields three main results. (1) We present an exponential time preprocessing algorithm that showcases the possibility of trading ReLU nodes for weight precision. Specifically, our method achieves an exponential reduction in neuron count when computing any function of high complexity with boolean input encoding. What is the implication of the above result in theoretical and practical works? (2) In theory of computing, in general, there is no free lunch. In our case, if you significantly reduce the number of neurons then you should pay the cost in weight precision. To address this, we introduce a notion of network size that considers weight precision in addition to the network's number of neurons. We establish that under this redefined notion of network size, it is generally impossible to exchange neurons for weight precision in ReLU networks of the same (redefined) size. (3) In practice, we show that high weight precision alone cannot help in reducing the neuron count. If instead of our exponential time preprocessing algorithm one uses any polynomial time algorithm, then it is impossible to non-trivially reduce the neuron count, regardless of the high weight precision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhua He",
      "Periklis A. Papakonstantinou"
    ]
  },
  "https://proceedings.mlr.press/v235/he24j.html": {
    "title": "Ambiguity-Aware Abductive Learning",
    "volume": "main",
    "abstract": "Abductive Learning (ABL) is a promising framework for integrating sub-symbolic perception and logical reasoning through abduction. In this case, the abduction process provides supervision for the perception model from the background knowledge. Nevertheless, this process naturally contains uncertainty, since the knowledge base may be satisfied by numerous potential candidates. This implies that the result of the abduction process, i.e., a set of candidates, is ambiguous; both correct and incorrect candidates are mixed in this set. The prior art of abductive learning selects the candidate that has the minimal inconsistency of the knowledge base. However, this method overlooks the ambiguity in the abduction process and is prone to error when it fails to identify the correct candidates. To address this, we propose Ambiguity-Aware Abductive Learning ($\\textrm{A}^3\\textrm{BL}$), which evaluates all potential candidates and their probabilities, thus preventing the model from falling into sub-optimal solutions. Both experimental results and theoretical analyses prove that $\\textrm{A}^3\\textrm{BL}$ markedly enhances ABL by efficiently exploiting the ambiguous abduced supervision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao-Yuan He",
      "Hui Sun",
      "Zheng Xie",
      "Ming Li"
    ]
  },
  "https://proceedings.mlr.press/v235/he24k.html": {
    "title": "Instruction Tuning for Secure Code Generation",
    "volume": "main",
    "abstract": "Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility",
    "checked": true,
    "id": "3cc4f14f4a174f524115eba6f228409db2856fdf",
    "semantic_title": "instruction tuning for secure code generation",
    "citation_count": 9,
    "authors": [
      "Jingxuan He",
      "Mark Vero",
      "Gabriela Krasnopolska",
      "Martin Vechev"
    ]
  },
  "https://proceedings.mlr.press/v235/he24l.html": {
    "title": "Be Your Own Neighborhood: Detecting Adversarial Examples by the Neighborhood Relations Built on Self-Supervised Learning",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) are vulnerable to Adversarial Examples (AEs), hindering their use in safety-critical systems. In this paper, we present BEYOND, an innovative AE detection framework designed for reliable predictions. BEYOND identifies AEs by distinguishing the AE's abnormal relation with its augmented versions, i.e. neighbors, from two prospects: representation similarity and label consistency. An off-the-shelf Self-Supervised Learning (SSL) model is used to extract the representation and predict the label for its highly informative representation capacity compared to supervised learning models. We found clean samples maintain a high degree of representation similarity and label consistency relative to their neighbors, in contrast to AEs which exhibit significant discrepancies. We explain this observation and show that leveraging this discrepancy BEYOND can accurately detect AEs. Additionally, we develop a rigorous justification for the effectiveness of BEYOND. Furthermore, as a plug-and-play model, BEYOND can easily cooperate with the Adversarial Trained Classifier (ATC), achieving state-of-the-art (SOTA) robustness accuracy. Experimental results show that BEYOND outperforms baselines by a large margin, especially under adaptive attacks. Empowered by the robust relationship built on SSL, we found that BEYOND outperforms baselines in terms of both detection ability and speed. Project page: https://huggingface.co/spaces/allenhzy/Be-Your-Own-Neighborhood",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan He",
      "Yijun Yang",
      "Pin-Yu Chen",
      "Qiang Xu",
      "Tsung-Yi Ho"
    ]
  },
  "https://proceedings.mlr.press/v235/he24m.html": {
    "title": "SFC: Achieve Accurate Fast Convolution under Low-precision Arithmetic",
    "volume": "main",
    "abstract": "Fast convolution algorithms, including Winograd and FFT, can efficiently accelerate convolution operations in deep models. However, these algorithms depend on high-precision arithmetic to maintain inference accuracy, which conflicts with the model quantization. To resolve this conflict and further improve the efficiency of quantized convolution, we proposes SFC, a new algebra transform for fast convolution by extending the Discrete Fourier Transform (DFT) with symbolic computing, in which only additions are required to perform the transformation at specific transform points, avoiding the calculation of irrational number and reducing the requirement for precision. Additionally, we enhance convolution efficiency by introducing correction terms to convert invalid circular convolution outputs of the Fourier method into effective ones. The numerical error analysis is presented for the first time in this type of work and proves that our algorithms can provide a 3.68× multiplication reduction for 3×3 convolution, while the Winograd algorithm only achieves a 2.25× reduction with similarly low numerical errors. Experiments carried out on benchmarks and FPGA show that our new algorithms can further improve the computation efficiency of quantized models while maintaining accuracy, surpassing both the quantization-alone method and existing works on fast convolution quantization",
    "checked": true,
    "id": "69f8a564a75c1b5844df33834a7b848dc32306ac",
    "semantic_title": "sfc: achieve accurate fast convolution under low-precision arithmetic",
    "citation_count": 0,
    "authors": [
      "Liulu He",
      "Yufei Zhao",
      "Rui Gao",
      "Yuan Du",
      "Li Du"
    ]
  },
  "https://proceedings.mlr.press/v235/he24n.html": {
    "title": "Robust Multi-Task Learning with Excess Risks",
    "volume": "main",
    "abstract": "Multi-task learning (MTL) considers learning a joint model for multiple tasks by optimizing a convex combination of all task losses. To solve the optimization problem, existing methods use an adaptive weight updating scheme, where task weights are dynamically adjusted based on their respective losses to prioritize difficult tasks. However, these algorithms face a great challenge whenever label noise is present, in which case excessive weights tend to be assigned to noisy tasks that have relatively large Bayes optimal errors, thereby overshadowing other tasks and causing performance to drop across the board. To overcome this limitation, we propose Multi-Task Learning with Excess Risks (ExcessMTL), an excess risk-based task balancing method that updates the task weights by their distances to convergence instead. Intuitively, ExcessMTL assigns higher weights to worse-trained tasks that are further from convergence. To estimate the excess risks, we develop an efficient and accurate method with Taylor approximation. Theoretically, we show that our proposed algorithm achieves convergence guarantees and Pareto stationarity. Empirically, we evaluate our algorithm on various MTL benchmarks and demonstrate its superior performance over existing methods in the presence of label noise. Our code is available at https://github.com/yifei-he/ExcessMTL",
    "checked": true,
    "id": "c5274ab0368de03859793840945dc865bcef1044",
    "semantic_title": "robust multi-task learning with excess risks",
    "citation_count": 4,
    "authors": [
      "Yifei He",
      "Shiji Zhou",
      "Guojun Zhang",
      "Hyokun Yun",
      "Yi Xu",
      "Belinda Zeng",
      "Trishul Chilimbi",
      "Han Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/he24o.html": {
    "title": "DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems",
    "volume": "main",
    "abstract": "Learning an effective policy to control high-dimensional, overactuated systems is a significant challenge for deep reinforcement learning algorithms. Such control scenarios are often observed in the neural control of vertebrate musculoskeletal systems. The study of these control mechanisms will provide insights into the control of high-dimensional, overactuated systems. The coordination of actuators, known as muscle synergies in neuromechanics, is considered a presumptive mechanism that simplifies the generation of motor commands. The dynamical structure of a system is the basis of its function, allowing us to derive a synergistic representation of actuators. Motivated by this theory, we propose the Dynamical Synergistic Representation (DynSyn) algorithm. DynSyn aims to generate synergistic representations from dynamical structures and perform task-specific, state-dependent adaptation to the representations to improve motor control. We demonstrate DynSyn's efficiency across various tasks involving different musculoskeletal models, achieving state-of-the-art sample efficiency and robustness compared to baseline algorithms. DynSyn generates interpretable synergistic representations that capture the essential features of dynamical structures and demonstrates generalizability across diverse motor tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaibo He",
      "Chenhui Zuo",
      "Chengtian Ma",
      "Yanan Sui"
    ]
  },
  "https://proceedings.mlr.press/v235/hebbar24a.html": {
    "title": "DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep Learning",
    "volume": "main",
    "abstract": "Progress in designing channel codes has been driven by human ingenuity and, fittingly, has been sporadic. Polar codes, developed on the foundation of Arikan's polarization kernel, represent the latest breakthrough in coding theory and have emerged as the state-of-the-art error-correction code for short-to-medium block length regimes. In an effort to automate the invention of good channel codes, especially in this regime, we explore a novel, non-linear generalization of Polar codes, which we call DeepPolar codes. DeepPolar codes extend the conventional Polar coding framework by utilizing a larger kernel size and parameterizing these kernels and matched decoders through neural networks. Our results demonstrate that these data-driven codes effectively leverage the benefits of a larger kernel size, resulting in enhanced reliability when compared to both existing neural codes and conventional Polar codes",
    "checked": true,
    "id": "51e757fef5f840982194f2fcb152fe2d20b4145e",
    "semantic_title": "deeppolar: inventing nonlinear large-kernel polar codes via deep learning",
    "citation_count": 2,
    "authors": [
      "S Ashwin Hebbar",
      "Sravan Kumar Ankireddy",
      "Hyeji Kim",
      "Sewoong Oh",
      "Pramod Viswanath"
    ]
  },
  "https://proceedings.mlr.press/v235/heilmann24a.html": {
    "title": "Differentially Private Sum-Product Networks",
    "volume": "main",
    "abstract": "Differentially private ML approaches seek to learn models which may be publicly released while guaranteeing that the input data is kept private. One issue with this construction is that further model releases based on the same training data (e.g. for a new task) incur a further privacy budget cost. Privacy-preserving synthetic data generation is one possible solution to this conundrum. However, models trained on synthetic private data struggle to approach the performance of private, ad-hoc models. In this paper, we present a novel method based on sum-product networks that is able to perform both privacy-preserving classification and privacy-preserving data generation with a single model. To the best of our knowledge, ours is the first approach that provides both discriminative and generative capabilities to differentially private ML. We show that our approach outperforms the state of the art in terms of stability (i.e. number of training runs required for convergence) and utility of the generated data",
    "checked": true,
    "id": "8c4d6198ea5ec2342a4e95f80860b5af44ab007c",
    "semantic_title": "differentially private sum-product networks",
    "citation_count": 0,
    "authors": [
      "Xenia Heilmann",
      "Mattia Cerrato",
      "Ernst Althaus"
    ]
  },
  "https://proceedings.mlr.press/v235/hemmer24a.html": {
    "title": "Optimal Recurrent Network Topologies for Dynamical Systems Reconstruction",
    "volume": "main",
    "abstract": "In dynamical systems reconstruction (DSR) we seek to infer from time series measurements a generative model of the underlying dynamical process. This is a prime objective in any scientific discipline, where we are particularly interested in parsimonious models with a low parameter load. A common strategy here is parameter pruning, removing all parameters with small weights. However, here we find this strategy does not work for DSR, where even low magnitude parameters can contribute considerably to the system dynamics. On the other hand, it is well known that many natural systems which generate complex dynamics, like the brain or ecological networks, have a sparse topology with comparatively few links. Inspired by this, we show that geometric pruning, where in contrast to magnitude-based pruning weights with a low contribution to an attractor's geometrical structure are removed, indeed manages to reduce parameter load substantially without significantly hampering DSR quality. We further find that the networks resulting from geometric pruning have a specific type of topology, and that this topology, and not the magnitude of weights, is what is most crucial to performance. We provide an algorithm that automatically generates such topologies which can be used as priors for generative modeling of dynamical systems by RNNs, and compare it to other well studied topologies like small-world or scale-free networks",
    "checked": true,
    "id": "f7262300a7dc61b5b9020753dc9d2bbd63a06cc2",
    "semantic_title": "optimal recurrent network topologies for dynamical systems reconstruction",
    "citation_count": 2,
    "authors": [
      "Christoph Jürgen Hemmer",
      "Manuel Brenner",
      "Florian Hess",
      "Daniel Durstewitz"
    ]
  },
  "https://proceedings.mlr.press/v235/herrmann24a.html": {
    "title": "Learning Useful Representations of Recurrent Neural Network Weight Matrices",
    "volume": "main",
    "abstract": "Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality–specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation learning. One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed MNIST digits. With the help of an emulation-based self-supervised learning technique we compare and evaluate the different RNN weight encoding techniques on multiple downstream applications. On the most challenging one, namely predicting which exact task the RNN was trained on, functionalist approaches show clear superiority",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Herrmann",
      "Francesco Faccio",
      "Jürgen Schmidhuber"
    ]
  },
  "https://proceedings.mlr.press/v235/herrmann24b.html": {
    "title": "Position: Why We Must Rethink Empirical Research in Machine Learning",
    "volume": "main",
    "abstract": "We warn against a common but incomplete understanding of empirical research in machine learning that leads to non-replicable results, makes findings unreliable, and threatens to undermine progress in the field. To overcome this alarming situation, we call for more awareness of the plurality of ways of gaining knowledge experimentally but also of some epistemic limitations. In particular, we argue most current empirical machine learning research is fashioned as confirmatory research while it should rather be considered exploratory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Herrmann",
      "F. Julian D. Lange",
      "Katharina Eggensperger",
      "Giuseppe Casalicchio",
      "Marcel Wever",
      "Matthias Feurer",
      "David Rügamer",
      "Eyke Hüllermeier",
      "Anne-Laure Boulesteix",
      "Bernd Bischl"
    ]
  },
  "https://proceedings.mlr.press/v235/heuillet24a.html": {
    "title": "Randomized Confidence Bounds for Stochastic Partial Monitoring",
    "volume": "main",
    "abstract": "The partial monitoring (PM) framework provides a theoretical formulation of sequential learning problems with incomplete feedback. At each round, a learning agent plays an action while the environment simultaneously chooses an outcome. The agent then observes a feedback signal that is only partially informative about the (unobserved) outcome. The agent leverages the received feedback signals to select actions that minimize the (unobserved) cumulative loss. In contextual PM, the outcomes depend on some side information that is observable by the agent before selecting the action. In this paper, we consider the contextual and non-contextual PM settings with stochastic outcomes. We introduce a new class of PM strategies based on the randomization of deterministic confidence bounds. We also extend regret guarantees to settings where existing stochastic strategies are not applicable. Our experiments show that the proposed RandCBP and RandCBPside* strategies have competitive performance against state-of-the-art baselines in multiple PM games. To illustrate how the PM framework can benefit real world applications, we design a use case on the real-world problem of monitoring the error rate of any deployed classification system",
    "checked": true,
    "id": "fcbe16dc5cf6b7837330f3aef4f57e7e8d7ac2b3",
    "semantic_title": "randomized confidence bounds for stochastic partial monitoring",
    "citation_count": 1,
    "authors": [
      "Maxime Heuillet",
      "Ola Ahmad",
      "Audrey Durand"
    ]
  },
  "https://proceedings.mlr.press/v235/heurtel-depeiges24a.html": {
    "title": "Listening to the noise: Blind Denoising with Gibbs Diffusion",
    "volume": "main",
    "abstract": "In recent years, denoising problems have become intertwined with the development of deep generative models. In particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture. However, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising. We overcome this limitation by introducing Gibbs Diffusion (GDiff), a general methodology addressing posterior sampling of both the signal and the noise parameters. Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the class of noise distributions, and a Monte Carlo sampler to infer the noise parameters. Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs stationary distribution caused by the diffusion model. We showcase our method for 1) blind denoising of natural images involving colored noises with unknown amplitude and exponent, and 2) a cosmology problem, namely the analysis of cosmic microwave background data, where Bayesian inference of \"noise\" parameters means constraining models of the evolution of the Universe",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Heurtel-Depeiges",
      "Charles Margossian",
      "Ruben Ohana",
      "Bruno Régaldo-Saint Blancard"
    ]
  },
  "https://proceedings.mlr.press/v235/higuchi24a.html": {
    "title": "Balanced Resonate-and-Fire Neurons",
    "volume": "main",
    "abstract": "The resonate-and-fire (RF) neuron, introduced over two decades ago, is a simple, efficient, yet biologically plausible spiking neuron model, which can extract frequency patterns within the time domain due to its resonating membrane dynamics. However, previous RF formulations suffer from intrinsic shortcomings that limit effective learning and prevent exploiting the principled advantage of RF neurons. Here, we introduce the balanced RF (BRF) neuron, which alleviates some of the intrinsic limitations of vanilla RF neurons and demonstrates its effectiveness within recurrent spiking neural networks (RSNNs) on various sequence learning tasks. We show that networks of BRF neurons achieve overall higher task performance, produce only a fraction of the spikes, and require significantly fewer parameters as compared to modern RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable training convergence, even when bridging many hundreds of time steps during backpropagation through time (BPTT). These results underscore that our BRF-RSNN is a strong candidate for future large-scale RSNN architectures, further lines of research in SNN methodology, and more efficient hardware implementations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saya Higuchi",
      "Sebastian Kairat",
      "Sander Bohte",
      "Sebastian Otte"
    ]
  },
  "https://proceedings.mlr.press/v235/hirono24a.html": {
    "title": "Understanding Diffusion Models by Feynman's Path Integral",
    "volume": "main",
    "abstract": "Score-based diffusion models have proven effective in image generation and have gained widespread usage; however, the underlying factors contributing to the performance disparity between stochastic and deterministic (i.e., the probability flow ODEs) sampling schemes remain unclear. We introduce a novel formulation of diffusion models using Feynman's path integral, which is a formulation originally developed for quantum physics. We find this formulation providing comprehensive descriptions of score-based generative models, and demonstrate the derivation of backward stochastic differential equations and loss functions. The formulation accommodates an interpolating parameter connecting stochastic and deterministic sampling schemes, and we identify this parameter as a counterpart of Planck's constant in quantum physics. This analogy enables us to apply the Wentzel–Kramers–Brillouin (WKB) expansion, a well-established technique in quantum physics, for evaluating the negative log-likelihood to assess the performance disparity between stochastic and deterministic sampling schemes",
    "checked": true,
    "id": "90852fe5da2d8ba691859f0be8626eaad6954adb",
    "semantic_title": "understanding diffusion models by feynman's path integral",
    "citation_count": 3,
    "authors": [
      "Yuji Hirono",
      "Akinori Tanaka",
      "Kenji Fukushima"
    ]
  },
  "https://proceedings.mlr.press/v235/hisaki24a.html": {
    "title": "RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "In this paper, we propose an off-policy deep reinforcement learning (DRL) method utilizing the average reward criterion. While most existing DRL methods employ the discounted reward criterion, this can potentially lead to a discrepancy between the training objective and performance metrics in continuing tasks, making the average reward criterion a recommended alternative. We introduce RVI-SAC, an extension of the state-of-the-art off-policy DRL method, Soft Actor-Critic (SAC), to the average reward criterion. Our proposal consists of (1) Critic updates based on RVI Q-learning, (2) Actor updates introduced by the average reward soft policy improvement theorem, and (3) automatic adjustment of Reset Cost enabling the average reward reinforcement learning to be applied to tasks with termination. We apply our method to the Gymnasium's Mujoco tasks, a subset of locomotion tasks, and demonstrate that RVI-SAC shows competitive performance compared to existing methods",
    "checked": true,
    "id": "5fdc3ce48995828abeaacf3aac34353b245dc375",
    "semantic_title": "rvi-sac: average reward off-policy deep reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Yukinari Hisaki",
      "Isao Ono"
    ]
  },
  "https://proceedings.mlr.press/v235/hoang24a.html": {
    "title": "Learning Surrogates for Offline Black-Box Optimization via Gradient Matching",
    "volume": "main",
    "abstract": "Offline design optimization problem arises in numerous science and engineering applications including material and chemical design, where expensive online experimentation necessitates the use of in silico surrogate functions to predict and maximize the target objective over candidate designs. Although these surrogates can be learned from offline data, their predictions are often inaccurate outside the offline data regime. This challenge raises a fundamental question about the impact of imperfect surrogate model on the performance gap between its optima and the true optima, and to what extent the performance loss can be mitigated. Although prior work developed methods to improve the robustness of surrogate models and their associated optimization processes, a provably quantifiable relationship between an imperfect surrogate and the corresponding performance gap, as well as whether prior methods directly address it, remain elusive. To shed light on this important question, we present a theoretical framework to understand offline black-box optimization, by explicitly bounding the optimization quality based on how well the surrogate matches the latent gradient field that underlines the offline data. Inspired by our theoretical analysis, we propose a principled black-box gradient matching algorithm to create effective surrogate models for offline optimization, improving over prior approaches on various real-world benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Hoang",
      "Azza Fadhel",
      "Aryan Deshwal",
      "Jana Doppa",
      "Trong Nghia Hoang"
    ]
  },
  "https://proceedings.mlr.press/v235/hodgson24a.html": {
    "title": "Estimating Unknown Population Sizes Using the Hypergeometric Distribution",
    "volume": "main",
    "abstract": "The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the category sizes are unknown. Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation problem, even in the presence of severe under-sampling. Our approach accounts for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, as seen in collaborative filtering, using the variational autoencoder framework. Empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and learning an informative latent space. We showcase our method's versatility through applications in NLP, by inferring and estimating the complexity of latent vocabularies in reading passage excerpts, and in biology, by accurately recovering the true number of gene transcripts from sparse single-cell genomics data",
    "checked": true,
    "id": "dd558553e0d5ee1146332bcbcc83575134680894",
    "semantic_title": "estimating unknown population sizes using the hypergeometric distribution",
    "citation_count": 0,
    "authors": [
      "Liam Hodgson",
      "Danilo Bzdok"
    ]
  },
  "https://proceedings.mlr.press/v235/hoffmann24a.html": {
    "title": "Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems",
    "volume": "main",
    "abstract": "In this work, we study rapid improvements of the training loss in transformers when being confronted with multi-step decision tasks. We found that transformers struggle to learn the intermediate task and both training and validation loss saturate for hundreds of epochs. When transformers finally learn the intermediate task, they do this rapidly and unexpectedly. We call these abrupt improvements Eureka-moments, since the transformer appears to suddenly learn a previously incomprehensible concept. We designed synthetic tasks to study the problem in detail, but the leaps in performance can be observed also for language modeling and in-context learning (ICL). We suspect that these abrupt transitions are caused by the multi-step nature of these tasks. Indeed, we find connections and show that ways to improve on the synthetic multi-step tasks can be used to improve the training of language modeling and ICL. Using the synthetic data we trace the problem back to the Softmax function in the self-attention block of transformers and show ways to alleviate the problem. These fixes reduce the required number of training steps, lead to higher likelihood to learn the intermediate task, to higher final accuracy and training becomes more robust to hyper-parameters",
    "checked": true,
    "id": "5e2cada56906540d31a73026a1e4d78cb8faf971",
    "semantic_title": "eureka-moments in transformers: multi-step tasks reveal softmax induced optimization problems",
    "citation_count": 4,
    "authors": [
      "David T Hoffmann",
      "Simon Schrodi",
      "Jelena Bratulić",
      "Nadine Behrmann",
      "Volker Fischer",
      "Thomas Brox"
    ]
  },
  "https://proceedings.mlr.press/v235/hogg24a.html": {
    "title": "Position: Is machine learning good or bad for the natural sciences?",
    "volume": "main",
    "abstract": "Machine learning (ML) methods are having a huge impact across all of the sciences. However, ML has a strong ontology — in which only the data exist — and a strong epistemology — in which a model is considered good if it performs well on held-out training data. These philosophies are in strong conflict with both standard practices and key philosophies in the natural sciences. Here we identify some locations for ML in the natural sciences at which the ontology and epistemology are valuable. For example, when an expressive machine learning model is used in a causal inference to represent the effects of confounders, such as foregrounds, backgrounds, or instrument calibration parameters, the model capacity and loose philosophy of ML can make the results more trustworthy. We also show that there are contexts in which the introduction of ML introduces strong, unwanted statistical biases. For one, when ML models are used to emulate physical (or first-principles) simulations, they amplify confirmation biases. For another, when expressive regressions are used to label datasets, those labels cannot be used in downstream joint or ensemble analyses without taking on uncontrolled biases. The question in the title is being asked of all of the natural sciences; that is, we are calling on the scientific communities to take a step back and consider the role and value of ML in their fields; the (partial) answers we give here come from the particular perspective of physics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David W Hogg",
      "Soledad Villar"
    ]
  },
  "https://proceedings.mlr.press/v235/hogsgaard24a.html": {
    "title": "Sparse Dimensionality Reduction Revisited",
    "volume": "main",
    "abstract": "The sparse Johnson-Lindenstrauss transform is one of the central techniques in dimensionality reduction. It supports embedding a set of $n$ points in $\\mathbb{R}^d$ into $m=O(\\varepsilon^{-2} \\ln n)$ dimensions while preserving all pairwise distances to within $1 \\pm \\varepsilon$. Each input point $x$ is embedded to $Ax$, where $A$ is an $m \\times d$ matrix having $s$ non-zeros per column, allowing for an embedding time of $O(s \\|x\\|_0)$. Since the sparsity of $A$ governs the embedding time, much work has gone into improving the sparsity $s$. The current state-of-the-art by Kane and Nelson (2014) shows that $s = O(\\varepsilon^{-1} \\ln n)$ suffices. This is almost matched by a lower bound of $s = \\Omega(\\varepsilon^{-1} \\ln n/\\ln(1/\\varepsilon))$ by Nelson and Nguyen (2013) for $d=\\Omega(n)$. Previous work thus suggests that we have near-optimal embeddings. In this work, we revisit sparse embeddings and present a sparser embedding for instances in which $d = n^{o(1)}$, which in many applications is realistic. Formally, our embedding achieves $s = O(\\varepsilon^{-1}(\\ln n/\\ln(1/\\varepsilon)+\\ln^{2/3}n \\ln^{1/3} d))$. We also complement our analysis by strengthening the lower bound of Nelson and Nguyen to hold also when $d \\ll n$, thereby matching the first term in our new sparsity upper bound. Finally, we also improve the sparsity of the best oblivious subspace embeddings for optimal embedding dimensionality",
    "checked": true,
    "id": "59d79e8e64d34283f99e896d04cad1942dcc64e9",
    "semantic_title": "sparse dimensionality reduction revisited",
    "citation_count": 2,
    "authors": [
      "Mikael Møller Høgsgaard",
      "Lior Kamma",
      "Kasper Green Larsen",
      "Jelani Nelson",
      "Chris Schwiegelshohn"
    ]
  },
  "https://proceedings.mlr.press/v235/hoier24a.html": {
    "title": "Two Tales of Single-Phase Contrastive Hebbian Learning",
    "volume": "main",
    "abstract": "The search for \"biologically plausible\" learning algorithms has converged on the idea of representing gradients as activity differences. However, most approaches require a high degree of synchronization (distinct phases during learning) and introduce substantial computational overhead, which raises doubts regarding their biological plausibility as well as their potential utility for neuromorphic computing. Furthermore, they commonly rely on applying infinitesimal perturbations (nudges) to output units, which is impractical in noisy environments. Recently it has been shown that by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named \"dual propagation\" to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging. However, the algorithm has the drawback that its numerical stability relies on symmetric nudging, which may be restrictive in biological and analog implementations. In this work we first provide a solid foundation for the objective underlying the dual propagation method, which also reveals a surpising connection with adversarial robustness. Second, we demonstrate how dual propagation is related to a particular adjoint state method, which is stable regardless of asymmetric nudging",
    "checked": true,
    "id": "35da93feb72477a6ca8c71ae78a476d71031595c",
    "semantic_title": "two tales of single-phase contrastive hebbian learning",
    "citation_count": 0,
    "authors": [
      "Rasmus Høier",
      "Christopher Zach"
    ]
  },
  "https://proceedings.mlr.press/v235/hojny24a.html": {
    "title": "Verifying message-passing neural networks via topology-based bounds tightening",
    "volume": "main",
    "abstract": "Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them. We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications. Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds. We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds. To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP. We test on both node and graph classification problems and consider topological attacks that both add and remove edges",
    "checked": true,
    "id": "cb6c0fcb6f19ed49f2b12c075850997ac09374b7",
    "semantic_title": "verifying message-passing neural networks via topology-based bounds tightening",
    "citation_count": 1,
    "authors": [
      "Christopher Hojny",
      "Shiqiang Zhang",
      "Juan S Campos",
      "Ruth Misener"
    ]
  },
  "https://proceedings.mlr.press/v235/holl24a.html": {
    "title": "$\\bfΦ_\\textrmFlow$: Differentiable Simulations for PyTorch, TensorFlow and Jax",
    "volume": "main",
    "abstract": "Differentiable processes have proven an invaluable tool for machine learning (ML) in scientific and engineering settings, but most ML libraries are not primarily designed for such applications. We present $\\Phi_\\textrm{Flow}$, a Python toolkit that seamlessly integrates with PyTorch, TensorFlow, Jax and NumPy, simplifying the process of writing differentiable simulation code at every step. $\\Phi_\\textrm{Flow}$ provides many essential features that go beyond the capabilities of the base libraries, such as differential operators, boundary conditions, the ability to write dimensionality-agnostic code, floating-point precision management, fully differentiable preconditioned (sparse) linear solves, automatic matrix generation via function tracing, integration of SciPy optimizers, simulation vectorization, and visualization tools. At the same time, $\\Phi_\\textrm{Flow}$ inherits all important traits of the base ML libraries, such as GPU / TPU support, just-in-time compilation, and automatic differentiation. Put together, these features drastically simplify scientific code like PDE or ODE solvers on grids or unstructured meshes, and $\\Phi_\\textrm{Flow}$ even includes out-of-the-box support for fluid simulations. $\\Phi_\\textrm{Flow}$ has been used in various publications and as a ground-truth solver in multiple scientific data sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Holl",
      "Nils Thuerey"
    ]
  },
  "https://proceedings.mlr.press/v235/holland24a.html": {
    "title": "Criterion Collapse and Loss Distribution Control",
    "volume": "main",
    "abstract": "In this work, we consider the notion of \"criterion collapse,\" in which optimization of one metric implies optimality in another, with a particular focus on conditions for collapse into error probability minimizers under a wide variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM) to non-monotonic criteria underlying recent ascent-descent algorithms explored in the literature (Flooding, SoftAD). We show how collapse in the context of losses with a Bernoulli distribution goes far beyond existing results for CVaR and DRO, then expand our scope to include surrogate losses, showing conditions where monotonic criteria such as tilted ERM cannot avoid collapse, whereas non-monotonic alternatives can",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew J. Holland"
    ]
  },
  "https://proceedings.mlr.press/v235/holstege24a.html": {
    "title": "Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation",
    "volume": "main",
    "abstract": "An important challenge in the field of interpretable machine learning is to ensure that deep neural networks (DNNs) use the correct or desirable input features in performing their tasks. Concept-removal methods aim to do this by eliminating concepts that are spuriously correlated with the main task from the neural network representation of the data. However, existing methods tend to be overzealous by inadvertently removing part of the correct or desirable features as well, leading to wrong interpretations and hurting model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly estimating two low-dimensional orthogonal subspaces of the neural network representation. By evaluating the algorithm on benchmark datasets from computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), we show it outperforms existing concept-removal methods in terms of identifying the main-task and spurious concepts, and removing only the latter",
    "checked": true,
    "id": "f9b306ed87a4cb3944f65f41c6e33ce302ee9cd1",
    "semantic_title": "removing spurious concepts from neural network representations via joint subspace estimation",
    "citation_count": 1,
    "authors": [
      "Floris Holstege",
      "Bram Wouters",
      "Noud Van Giersbergen",
      "Cees Diks"
    ]
  },
  "https://proceedings.mlr.press/v235/hong24a.html": {
    "title": "Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression",
    "volume": "main",
    "abstract": "Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% sparsity. Moreover, employing quantization within a moderate bit range could unexpectedly improve certain trustworthiness dimensions such as ethics and fairness. Conversely, extreme quantization to very low bit levels (3 bits) tends to reduce trustworthiness significantly. This increased risk cannot be uncovered by looking at benign performance alone, in turn, mandating comprehensive trustworthiness evaluation in practice. These findings culminate in practical recommendations for simultaneously achieving high utility, efficiency, and trustworthiness in LLMs. Code and models are available at https://decoding-comp-trust.github.io",
    "checked": true,
    "id": "b0a890c4726b98139e51669f39dafbad568c352f",
    "semantic_title": "decoding compressed trust: scrutinizing the trustworthiness of efficient llms under compression",
    "citation_count": 14,
    "authors": [
      "Junyuan Hong",
      "Jinhao Duan",
      "Chenhui Zhang",
      "Zhangheng Li",
      "Chulin Xie",
      "Kelsey Lieberman",
      "James Diffenderfer",
      "Brian R. Bartoldson",
      "Ajay Kumar Jaiswal",
      "Kaidi Xu",
      "Bhavya Kailkhura",
      "Dan Hendrycks",
      "Dawn Song",
      "Zhangyang Wang",
      "Bo Li"
    ]
  },
  "https://proceedings.mlr.press/v235/hong24b.html": {
    "title": "Enhancing Sufficient Dimension Reduction via Hellinger Correlation",
    "volume": "main",
    "abstract": "In this work, we develop a new theory and method for sufficient dimension reduction (SDR) in single-index models, where SDR is a sub-field of supervised dimension reduction based on conditional independence. Our work is primarily motivated by the recent introduction of the Hellinger correlation as a dependency measure. Utilizing this measure, we have developed a method capable of effectively detecting the dimension reduction subspace, complete with theoretical justification. Through extensive numerical experiments, we demonstrate that our proposed method significantly enhances and outperforms existing SDR methods. This improvement is largely attributed to our proposed method's deeper understanding of data dependencies and the refinement of existing SDR techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungbeom Hong",
      "Ilmun Kim",
      "Jun Song"
    ]
  },
  "https://proceedings.mlr.press/v235/hong24c.html": {
    "title": "Diversified Batch Selection for Training Acceleration",
    "volume": "main",
    "abstract": "The remarkable success of modern machine learning models on large datasets often demands extensive training time and resource consumption. To save cost, a prevalent research line, known as online batch selection, explores selecting informative subsets during the training process. Although recent efforts achieve advancements by measuring the impact of each sample on generalization, their reliance on additional reference models inherently limits their practical applications, when there are no such ideal models available. On the other hand, the vanilla reference-model-free methods involve independently scoring and selecting data in a sample-wise manner, which sacrifices the diversity and induces the redundancy. To tackle this dilemma, we propose Diversified Batch Selection (DivBS), which is reference-model-free and can efficiently select diverse and representative samples. Specifically, we define a novel selection objective that measures the group-wise orthogonalized representativeness to combat the redundancy issue of previous sample-wise criteria, and provide a principled selection-efficient realization. Extensive experiments across various tasks demonstrate the significant superiority of DivBS in the performance-speedup trade-off. The code is publicly available",
    "checked": true,
    "id": "a281cc773c1608d204d4ce47643b869dc8ab4019",
    "semantic_title": "diversified batch selection for training acceleration",
    "citation_count": 1,
    "authors": [
      "Feng Hong",
      "Yueming Lyu",
      "Jiangchao Yao",
      "Ya Zhang",
      "Ivor Tsang",
      "Yanfeng Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/hong24d.html": {
    "title": "Model-based Reinforcement Learning for Confounded POMDPs",
    "volume": "main",
    "abstract": "We propose a model-based offline reinforcement learning (RL) algorithm for confounded partially observable Markov decision processes (POMDPs) under general function approximations and show it is provably efficient under some technical conditions such as the partial coverage imposed on the offline data distribution. Specifically, we first establish a novel model-based identification result for learning the effect of any action on the reward and future transitions in the confounded POMDP. Using this identification result, we then design a nonparametric two-stage estimation procedure to construct an estimator for off-policy evaluation (OPE), which permits general function approximations. Finally, we learn the optimal policy by performing a conservative policy optimization within the confidence regions based on the proposed estimation procedure for OPE. Under some mild conditions, we establish a finite-sample upper bound on the suboptimality of the learned policy in finding the optimal one, which depends on the sample size and the length of horizons polynomially",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mao Hong",
      "Zhengling Qi",
      "Yanxun Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/hong24e.html": {
    "title": "A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Linear MDPs",
    "volume": "main",
    "abstract": "We study offline reinforcement learning (RL) with linear MDPs under the infinite-horizon discounted setting which aims to learn a policy that maximizes the expected discounted cumulative reward using a pre-collected dataset. Existing algorithms for this setting either require a uniform data coverage assumptions or are computationally inefficient for finding an $\\epsilon$-optimal policy with $\\mathcal{O}(\\epsilon^{-2})$ sample complexity. In this paper, we propose a primal dual algorithm for offline RL with linear MDPs in the infinite-horizon discounted setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $\\mathcal{O}(\\epsilon^{-2})$ with partial data coverage assumption. Our work is an improvement upon a recent work that requires $\\mathcal{O}(\\epsilon^{-4})$ samples. Moreover, we extend our algorithm to work in the offline constrained RL setting that enforces constraints on additional reward signals",
    "checked": true,
    "id": "78b95826b4a6bf9e21904548d7aadc277f7e8384",
    "semantic_title": "a primal-dual algorithm for offline constrained reinforcement learning with linear mdps",
    "citation_count": 1,
    "authors": [
      "Kihyuk Hong",
      "Ambuj Tewari"
    ]
  },
  "https://proceedings.mlr.press/v235/hooda24a.html": {
    "title": "Do Large Code Models Understand Programming Concepts? Counterfactual Analysis for Code Predicates",
    "volume": "main",
    "abstract": "Large Language Models' success in text generation has also made them better at code generation and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as code completion and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual testing framework to evaluate whether Large Code Models understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular Large Code Models for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow",
    "checked": true,
    "id": "b53ff078e06728e2009cd3b2494083c85f122696",
    "semantic_title": "do large code models understand programming concepts? counterfactual analysis for code predicates",
    "citation_count": 0,
    "authors": [
      "Ashish Hooda",
      "Mihai Christodorescu",
      "Miltiadis Allamanis",
      "Aaron Wilson",
      "Kassem Fawaz",
      "Somesh Jha"
    ]
  },
  "https://proceedings.mlr.press/v235/hordan24a.html": {
    "title": "Weisfeiler Leman for Euclidean Equivariant Machine Learning",
    "volume": "main",
    "abstract": "The $k$-Weisfeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs). Recently, GNNs whose expressive power is equivalent to the $2$-WL test were proven to be universal on weighted graphs which encode $3\\mathrm{D}$ point cloud data, yet this result is limited to invariant continuous functions on point clouds. In this paper, we extend this result in three ways: Firstly, we show that PPGN can simulate $2$-WL uniformly on all point clouds with low complexity. Secondly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocities, a scenario often encountered in applications. Finally, we provide a general framework for proving equivariant universality and leverage it to prove that a simple modification of this invariant PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly. Building on our results, we develop our WeLNet architecture, which sets new state-of-the-art results on the N-Body dynamics task and the GEOM-QM9 molecular conformation generation task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Snir Hordan",
      "Tal Amir",
      "Nadav Dym"
    ]
  },
  "https://proceedings.mlr.press/v235/horie24a.html": {
    "title": "Graph Neural PDE Solvers with Conservation and Similarity-Equivariance",
    "volume": "main",
    "abstract": "Utilizing machine learning to address partial differential equations (PDEs) presents significant challenges due to the diversity of spatial domains and their corresponding state configurations, which complicates the task of encompassing all potential scenarios through data-driven methodologies alone. Moreover, there are legitimate concerns regarding the generalization and reliability of such approaches, as they often overlook inherent physical constraints. In response to these challenges, this study introduces a novel machine-learning architecture that is highly generalizable and adheres to conservation laws and physical symmetries, thereby ensuring greater reliability. The foundation of this architecture is graph neural networks (GNNs), which are adept at accommodating a variety of shapes and forms. Additionally, we explore the parallels between GNNs and traditional numerical solvers, facilitating a seamless integration of conservative principles and symmetries into machine learning models. Our findings from experiments demonstrate that the model's inclusion of physical laws significantly enhances its generalizability, i.e., no significant accuracy degradation for unseen spatial domains while other models degrade. The code is available at https://github.com/yellowshippo/fluxgnn-icml2024",
    "checked": true,
    "id": "2365a6f71e27ef80ac9cf64aa64c1efeb392360a",
    "semantic_title": "graph neural pde solvers with conservation and similarity-equivariance",
    "citation_count": 1,
    "authors": [
      "Masanobu Horie",
      "Naoto Mitsume"
    ]
  },
  "https://proceedings.mlr.press/v235/horoi24a.html": {
    "title": "Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "Combining the predictions of multiple trained models through ensembling is generally a good way to improve accuracy by leveraging the different learned features of the models, however it comes with high computational and storage costs. Model fusion, the act of merging multiple models into one by combining their parameters reduces these costs but doesn't work as well in practice. Indeed, neural network loss landscapes are high-dimensional and non-convex and the minima found through learning are typically separated by high loss barriers. Numerous recent works have been focused on finding permutations matching one network features to the features of a second one, lowering the loss barrier on the linear path between them in parameter space. However, permutations are restrictive since they assume a one-to-one mapping between the different models' neurons exists. We propose a new model merging algorithm, CCA Merge, which is based on Canonical Correlation Analysis and aims to maximize the correlations between linear combinations of the model features. We show that our alignment method leads to better performances than past methods when averaging models trained on the same, or differing data splits. We also extend this analysis into the harder setting where more than 2 models are merged, and we find that CCA Merge works significantly better than past methods. Our code is publicly available at https://github.com/shoroi/align-n-merge",
    "checked": true,
    "id": "32eb03c411272a50f2ebddca2df036aab325ed79",
    "semantic_title": "harmony in diversity: merging neural networks with canonical correlation analysis",
    "citation_count": 3,
    "authors": [
      "Stefan Horoi",
      "Albert Manuel Orozco Camacho",
      "Eugene Belilovsky",
      "Guy Wolf"
    ]
  },
  "https://proceedings.mlr.press/v235/horowitz24a.html": {
    "title": "Classification Under Strategic Self-Selection",
    "volume": "main",
    "abstract": "When users stand to gain from certain predictive outcomes, they are prone to act strategically to obtain predictions that are favorable. Most current works consider strategic behavior that manifests as users modifying their features; instead, we study a novel setting in which users decide whether to even participate (or not), this in response to the learned classifier. Considering learning approaches of increasing strategic awareness, we investigate the effects of user self-selection on learning, and the implications of learning on the composition of the self-selected population. Building on this, we propose a differentiable framework for learning under self-selective behavior, which can be optimized effectively. We conclude with experiments on real data and simulated behavior that complement our analysis and demonstrate the utility of our approach",
    "checked": true,
    "id": "8a0d3d5e4e953be3a1af831c718cf7374001f52e",
    "semantic_title": "classification under strategic self-selection",
    "citation_count": 2,
    "authors": [
      "Guy Horowitz",
      "Yonatan Sommer",
      "Moran Koren",
      "Nir Rosenfeld"
    ]
  },
  "https://proceedings.mlr.press/v235/horvath24a.html": {
    "title": "Maestro: Uncovering Low-Rank Structures via Trainable Decomposition",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) have been a large driver for AI breakthroughs in recent years, ranging from self-driving cars to intelligent assistants. However, these models have been getting increasingly large as they become more accurate and safe. This means that their training becomes increasingly costly and time-consuming, and typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification or quantization of the model weights and updates. While achieving high compression rates, they often incur significant computational overheads at training or lead to non-negligible accuracy penalty. Alternatively, factorization methods have been leveraged for low-rank compression of DNNs. Similarly, such techniques (e.g., SVD) frequently rely on heavy iterative decompositions of layers and are potentially sub-optimal for non-linear models, such as DNNs. We take a further step in designing efficient low-rank models and propose Maestro, a framework for trainable low-rank layers. Instead of iteratively applying a priori decompositions, the low-rank structure is baked into the training process through LoD, a low-rank ordered decomposition. Not only is this the first time importance ordering via sampling is applied on the decomposed DNN structure, but it also allows selecting ranks at a layer granularity. Our theoretical analysis demonstrates that LoD recovers the SVD decomposition of linear mapping on uniformly distributed data and PCA for linear autoencoders. Applied to DNNs, Maestro enables the extraction of lower footprint models that preserve performance. Simultaneously, it enables the graceful tradeoff between accuracy-latency for deployment to even more constrained devices, without retraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Horváth",
      "Stefanos Laskaridis",
      "Shashank Rajput",
      "Hongyi Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/horwitz24a.html": {
    "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models",
    "volume": "main",
    "abstract": "The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral. The code is available at https://vision.huji.ac.il/spectral_detuning/",
    "checked": true,
    "id": "8e624e215908934a38044500f8434a0f88c69059",
    "semantic_title": "recovering the pre-fine-tuning weights of generative models",
    "citation_count": 6,
    "authors": [
      "Eliahu Horwitz",
      "Jonathan Kahana",
      "Yedid Hoshen"
    ]
  },
  "https://proceedings.mlr.press/v235/hossain24a.html": {
    "title": "Equilibrium of Data Markets with Externality",
    "volume": "main",
    "abstract": "We model real-world data markets, where sellers post fixed prices and buyers are free to purchase from any set of sellers, as a simultaneous game. A key component here is the negative externality buyers induce on one another due to data purchases. Starting with a simple setting where buyers know their valuations a priori, we characterize both the existence and welfare properties of the pure Nash equilibrium in the presence of such externality. While the outcomes are bleak without any intervention, mirroring the limitations of current data markets, we prove that for a standard class of externality functions, platforms intervening through a transaction cost can lead to a pure equilibrium with strong welfare guarantees. We next consider a more realistic setting where buyers learn their valuations over time through market interactions. Our intervention is feasible here as well, and we consider learning algorithms to achieve low regret concerning both individual and cumulative utility metrics. Lastly, we analyze the promises of this intervention under a much richer externality model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Safwan Hossain",
      "Yiling Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/hossain24b.html": {
    "title": "A Persuasive Approach to Combating Misinformation",
    "volume": "main",
    "abstract": "Bayesian Persuasion is proposed as a tool for social media platforms to combat the spread of misinformation. Since platforms can use machine learning to predict the popularity and misinformation features of to-be-shared posts, and users are largely motivated to share popular content, platforms can strategically signal this informational advantage to change user beliefs and persuade them not to share misinformation. We characterize the optimal signaling scheme with imperfect predictions as a linear program and give sufficient and necessary conditions on the classifier to ensure optimal platform utility is non-decreasing and continuous. Next, this interaction is considered under a performative model, wherein platform intervention affects the user's future behaviour. The convergence and stability of optimal signaling under this performative process are fully characterized. Lastly, we experimentally validate that our approach significantly reduces misinformation in both the single round and performative setting",
    "checked": true,
    "id": "2fc628e941d44a1cb190342e5d33fb9e642b8507",
    "semantic_title": "a persuasive approach to combating misinformation",
    "citation_count": 0,
    "authors": [
      "Safwan Hossain",
      "Andjela Mladenovic",
      "Yiling Chen",
      "Gauthier Gidel"
    ]
  },
  "https://proceedings.mlr.press/v235/hossain24c.html": {
    "title": "Multi-Sender Persuasion: A Computational Perspective",
    "volume": "main",
    "abstract": "We consider multiple senders with informational advantage signaling to convince a single self-interested actor to take certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class of economic problems",
    "checked": true,
    "id": "1f50b236dbc717fb40b48e6987f9ce152baf1811",
    "semantic_title": "multi-sender persuasion: a computational perspective",
    "citation_count": 4,
    "authors": [
      "Safwan Hossain",
      "Tonghan Wang",
      "Tao Lin",
      "Yiling Chen",
      "David C. Parkes",
      "Haifeng Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/hotti24a.html": {
    "title": "Efficient Mixture Learning in Black-Box Variational Inference",
    "volume": "main",
    "abstract": "Mixture variational distributions in black box variational inference (BBVI) have demonstrated impressive results in challenging density estimation tasks. However, currently scaling the number of mixture components can lead to a linear increase in the number of learnable parameters and a quadratic increase in inference time due to the evaluation of the evidence lower bound (ELBO). Our two key contributions address these limitations. First, we introduce the novel Multiple Importance Sampling Variational Autoencoder (MISVAE), which amortizes the mapping from input to mixture-parameter space using one-hot encodings. Fortunately, with MISVAE, each additional mixture component incurs a negligible increase in network parameters. Second, we construct two new estimators of the ELBO for mixtures in BBVI, enabling a tremendous reduction in inference time with marginal or even improved impact on performance. Collectively, our contributions enable scalability to hundreds of mixture components and provide superior estimation performance in shorter time, with fewer network parameters compared to previous Mixture VAEs. Experimenting with MISVAE, we achieve astonishing, SOTA results on MNIST. Furthermore, we empirically validate our estimators in other BBVI settings, including Bayesian phylogenetic inference, where we improve inference times for the SOTA mixture model on eight data sets",
    "checked": true,
    "id": "7e983989e5895313123b550c4b62e0a82caedd72",
    "semantic_title": "efficient mixture learning in black-box variational inference",
    "citation_count": 0,
    "authors": [
      "Alexandra Hotti",
      "Oskar Kviman",
      "Ricky Molén",
      "Vı́ctor Elvira",
      "Jens Lagergren"
    ]
  },
  "https://proceedings.mlr.press/v235/hou24a.html": {
    "title": "IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries can maliciously trigger model misclassifications by implanting a hidden backdoor during model training. This paper proposes a simple yet effective input-level backdoor detection (dubbed IBD-PSC) as a ‘firewall' to filter out malicious testing images. Our method is motivated by an intriguing phenomenon, i.e., parameter-oriented scaling consistency (PSC), where the prediction confidences of poisoned samples are significantly more consistent than those of benign ones when amplifying model parameters. In particular, we provide theoretical analysis to safeguard the foundations of the PSC phenomenon. We also design an adaptive method to select BN layers to scale up for effective detection. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our IBD-PSC method and its resistance to adaptive attacks. Codes are available at https://github.com/THUYimingLi/BackdoorBox",
    "checked": true,
    "id": "4696ec65c7f9067d638ab2209214ba732307a8aa",
    "semantic_title": "ibd-psc: input-level backdoor detection via parameter-oriented scaling consistency",
    "citation_count": 4,
    "authors": [
      "Linshan Hou",
      "Ruili Feng",
      "Zhongyun Hua",
      "Wei Luo",
      "Leo Yu Zhang",
      "Yiming Li"
    ]
  },
  "https://proceedings.mlr.press/v235/hou24b.html": {
    "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
    "volume": "main",
    "abstract": "Uncertainty decomposition refers to the task of decomposing the total uncertainty of a predictive model into aleatoric (data) uncertainty, resulting from inherent randomness in the data-generating process, and epistemic (model) uncertainty, resulting from missing information in the model's training data. In large language models (LLMs) specifically, identifying sources of uncertainty is an important step toward improving reliability, trustworthiness, and interpretability, but remains an important open research question. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarification ensembling, which can be applied to any pre-trained LLM. Our approach generates a set of clarifications for the input, feeds them into an LLM, and ensembles the corresponding predictions. We show that, when aleatoric uncertainty arises from ambiguity or under-specification in LLM inputs, this approach makes it possible to factor an (un-clarified) LLM's predictions into separate aleatoric and epistemic terms, using a decomposition similar to the one employed by Bayesian neural networks. Empirical evaluations demonstrate that input clarification ensembling provides accurate and reliable uncertainty quantification on several language processing tasks. Code and data are available at https://github.com/UCSB-NLP-Chang/llm_uncertainty",
    "checked": true,
    "id": "67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3",
    "semantic_title": "decomposing uncertainty for large language models through input clarification ensembling",
    "citation_count": 30,
    "authors": [
      "Bairu Hou",
      "Yujian Liu",
      "Kaizhi Qian",
      "Jacob Andreas",
      "Shiyu Chang",
      "Yang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/hou24c.html": {
    "title": "PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs",
    "volume": "main",
    "abstract": "On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using 9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and 100$\\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text",
    "checked": true,
    "id": "97f7a0c4be425f0019f7fca603d3dfd522da025a",
    "semantic_title": "pre-text: training language models on private federated data in the age of llms",
    "citation_count": 3,
    "authors": [
      "Charlie Hou",
      "Akshat Shrivastava",
      "Hongyuan Zhan",
      "Rylan Conway",
      "Trang Le",
      "Adithya Sagar",
      "Giulia Fanti",
      "Daniel Lazar"
    ]
  },
  "https://proceedings.mlr.press/v235/hounie24a.html": {
    "title": "Loss Shaping Constraints for Long-Term Time Series Forecasting",
    "volume": "main",
    "abstract": "Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show that despite its non-convexity, the resulting problem has a bounded duality gap. We propose a practical primal-dual algorithm to tackle it, and demonstrate that the proposed approach exhibits competitive average performance in time series forecasting benchmarks, while shaping the distribution of errors across the predicted window",
    "checked": true,
    "id": "8dbc20029d404d08d81cf5ad84466e623931bab9",
    "semantic_title": "loss shaping constraints for long-term time series forecasting",
    "citation_count": 0,
    "authors": [
      "Ignacio Hounie",
      "Javier Porras-Valenzuela",
      "Alejandro Ribeiro"
    ]
  },
  "https://proceedings.mlr.press/v235/hsieh24a.html": {
    "title": "Careful with that Scalpel: Improving Gradient Surgery with an EMA",
    "volume": "main",
    "abstract": "Beyond minimizing a single training loss, many deep learning estimation pipelines rely on an auxiliary objective to quantify and encourage desirable properties of the model (e.g. performance on another dataset, robustness, agreement with a prior). Although the simplest approach to incorporating an auxiliary loss is to sum it with the training loss as a regularizer, recent works have shown that one can improve performance by blending the gradients beyond a simple sum; this is known as gradient surgery. We cast the problem as a constrained minimization problem where the auxiliary objective is minimized among the set of minimizers of the training loss. To solve this bilevel problem, we follow a parameter update direction that combines the training loss gradient and the orthogonal projection of the auxiliary gradient to the training gradient. In a setting where gradients come from mini-batches, we explain how, using a moving average of the training loss gradients, we can carefully maintain this critical orthogonality property. We demonstrate that our method, Bloop, can lead to much better performances on NLP and vision experiments than other gradient surgery methods without EMA",
    "checked": true,
    "id": "da828611b9654af1b87de9cbc09ce6fe1b463d45",
    "semantic_title": "careful with that scalpel: improving gradient surgery with an ema",
    "citation_count": 0,
    "authors": [
      "Yu-Guan Hsieh",
      "James Thornton",
      "Eugene Ndiaye",
      "Michal Klein",
      "Marco Cuturi",
      "Pierre Ablin"
    ]
  },
  "https://proceedings.mlr.press/v235/hsu24a.html": {
    "title": "Tripod: Three Complementary Inductive Biases for Disentangled Representation Learning",
    "volume": "main",
    "abstract": "Inductive biases are crucial in disentangled representation learning for narrowing down an underspecified solution set. In this work, we consider endowing a neural network autoencoder with three select inductive biases from the literature: data compression into a grid-like latent space via quantization, collective independence amongst latents, and minimal functional influence of any latent on how other latents determine data generation. In principle, these inductive biases are deeply complementary: they most directly specify properties of the latent space, encoder, and decoder, respectively. In practice, however, naively combining existing techniques instantiating these inductive biases fails to yield significant benefits. To address this, we propose adaptations to the three techniques that simplify the learning problem, equip key regularization terms with stabilizing invariances, and quash degenerate incentives. The resulting model, Tripod, achieves state-of-the-art results on a suite of four image disentanglement benchmarks. We also verify that Tripod significantly improves upon its naive incarnation and that all three of its \"legs\" are necessary for best performance",
    "checked": true,
    "id": "9a4feda6c6b13b29febefed6bd29d547ee143c38",
    "semantic_title": "tripod: three complementary inductive biases for disentangled representation learning",
    "citation_count": 3,
    "authors": [
      "Kyle Hsu",
      "Jubayer Ibn Hamid",
      "Kaylee Burns",
      "Chelsea Finn",
      "Jiajun Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24a.html": {
    "title": "Outlier-Efficient Hopfield Layers for Large Transformer-Based Models",
    "volume": "main",
    "abstract": "We introduce an Outlier-Efficient Modern Hopfield Model (termed OutEffHop) and use it to address the outlier inefficiency problem of training gigantic transformer-based models. Our main contribution is a novel associative memory model facilitating outlier-efficient associative memory retrievals. Interestingly, this memory model manifests a model-based interpretation of an outlier-efficient attention mechanism (Softmax_1): it is an approximation of the memory retrieval process of OutEffHop. Methodologically, this allows us to introduce novel outlier-efficient Hopfield layers as powerful alternatives to traditional attention mechanisms, with superior post-quantization performance. Theoretically, the Outlier-Efficient Modern Hopfield Model retains and improves the desirable properties of standard modern Hopfield models, including fixed point convergence and exponential storage capacity. Empirically, we demonstrate the efficacy of the proposed model across large-scale transformer-based and Hopfield-based models (including BERT, OPT, ViT, and STanHop-Net), benchmarking against state-of-the-art methods like Clipped_Softmax and Gated_Attention. Notably, OutEffHop achieves an average reduction of 22+% in average kurtosis and 26+% in the maximum infinity norm of model outputs across four models. Code is available at GitHub; future updates are on arXiv",
    "checked": true,
    "id": "faf7488da821133dbf34fe006676adfda4a764a0",
    "semantic_title": "outlier-efficient hopfield layers for large transformer-based models",
    "citation_count": 20,
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Pei-Hsuan Chang",
      "Haozheng Luo",
      "Hong-Yu Chen",
      "Weijian Li",
      "Wei-Po Wang",
      "Han Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24b.html": {
    "title": "Task-aware Orthogonal Sparse Network for Exploring Shared Knowledge in Continual Learning",
    "volume": "main",
    "abstract": "Continual learning (CL) aims to learn from sequentially arriving tasks without catastrophic forgetting (CF). By partitioning the network into two parts based on the Lottery Ticket Hypothesis—one for holding the knowledge of the old tasks while the other for learning the knowledge of the new task—the recent progress has achieved forget-free CL. Although addressing the CF issue well, such methods would encounter serious under-fitting in long-term CL, in which the learning process will continue for a long time and the number of new tasks involved will be much higher. To solve this problem, this paper partitions the network into three parts—with a new part for exploring the knowledge sharing between the old and new tasks. With the shared knowledge, this part of network can be learnt to simultaneously consolidate the old tasks and fit to the new task. To achieve this goal, we propose a task-aware Orthogonal Sparse Network (OSN), which contains shared knowledge induced network partition and sharpness-aware orthogonal sparse network learning. The former partitions the network to select shared parameters, while the latter guides the exploration of shared knowledge through shared parameters. Qualitative and quantitative analyses, show that the proposed OSN induces minimum to no interference with past tasks, i.e., approximately no forgetting, while greatly improves the model plasticity and capacity, and finally achieves the state-of-the-art performances",
    "checked": true,
    "id": "b45e830c92a9be878b8838e4e4387ec8b9d08310",
    "semantic_title": "task-aware orthogonal sparse network for exploring shared knowledge in continual learning",
    "citation_count": 0,
    "authors": [
      "Yusong Hu",
      "De Cheng",
      "Dingwen Zhang",
      "Nannan Wang",
      "Tongliang Liu",
      "Xinbo Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24c.html": {
    "title": "Q-value Regularized Transformer for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Recent advancements in offline reinforcement learning (RL) have underscored the capabilities of Conditional Sequence Modeling (CSM), a paradigm that learns the action distribution based on history trajectory and target returns for each state. However, these methods often struggle with stitching together optimal trajectories from sub-optimal ones due to the inconsistency between the sampled returns within individual trajectories and the optimal returns across multiple trajectories. Fortunately, Dynamic Programming (DP) methods offer a solution by leveraging a value function to approximate optimal future returns for each state, while these techniques are prone to unstable learning behaviors, particularly in long-horizon and sparse-reward scenarios. Building upon these insights, we propose the Q-value regularized Transformer (QT), which combines the trajectory modeling ability of the Transformer with the predictability of optimal future returns from DP methods. QT learns an action-value function and integrates a term maximizing action-values into the training loss of CSM, which aims to seek optimal actions that align closely with the behavior policy. Empirical evaluations on D4RL benchmark datasets demonstrate the superiority of QT over traditional DP and CSM methods, highlighting the potential of QT to enhance the state-of-the-art in offline RL",
    "checked": true,
    "id": "b0bc8cbb3c8b5a7b2c325588c2a6f228d59efc0c",
    "semantic_title": "q-value regularized transformer for offline reinforcement learning",
    "citation_count": 5,
    "authors": [
      "Shengchao Hu",
      "Ziqing Fan",
      "Chaoqin Huang",
      "Li Shen",
      "Ya Zhang",
      "Yanfeng Wang",
      "Dacheng Tao"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24d.html": {
    "title": "HarmoDT: Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "The purpose of offline multi-task reinforcement learning (MTRL) is to develop a unified policy applicable to diverse tasks without the need for online environmental interaction. Recent advancements approach this through sequence modeling, leveraging the Transformer architecture's scalability and the benefits of parameter sharing to exploit task similarities. However, variations in task content and complexity pose significant challenges in policy formulation, necessitating judicious parameter sharing and management of conflicting gradients for optimal policy performance. In this work, we introduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel solution designed to identify an optimal harmony subspace of parameters for each task. We approach this as a bi-level optimization problem, employing a meta-learning framework that leverages gradient-based techniques. The upper level of this framework is dedicated to learning a task-specific mask that delineates the harmony subspace, while the inner level focuses on updating parameters to enhance the overall performance of the unified policy. Empirical evaluations on a series of benchmarks demonstrate the superiority of HarmoDT, verifying the effectiveness of our approach",
    "checked": true,
    "id": "e60842b9969e269d73571a4a483b127b027ce16f",
    "semantic_title": "harmodt: harmony multi-task decision transformer for offline reinforcement learning",
    "citation_count": 5,
    "authors": [
      "Shengchao Hu",
      "Ziqing Fan",
      "Li Shen",
      "Ya Zhang",
      "Yanfeng Wang",
      "Dacheng Tao"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24e.html": {
    "title": "An Information Theoretic Approach to Interaction-Grounded Learning",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) problems where the learner attempts to infer an unobserved reward from some feedback variables have been studied in several recent papers. The setting of Interaction-Grounded Learning (IGL) is an example of such feedback-based reinforcement learning tasks where the learner optimizes the return by inferring latent binary rewards from the interaction with the environment. In the IGL setting, a relevant assumption used in the RL literature is that the feedback variable $Y$ is conditionally independent of the context-action $(X,A)$ given the latent reward $R$. In this work, we propose Variational Information-based IGL (VI-IGL) as an information-theoretic method to enforce the conditional independence assumption in the IGL-based RL problem. The VI-IGL framework learns a reward decoder using an information-based objective based on the conditional mutual information (MI) between the context-action $(X,A)$ and the feedback variable $Y$ observed from the environment. To estimate and optimize the information-based terms for the continuous random variables in the RL problem, VI-IGL leverages the variational representation of mutual information and results in a min-max optimization problem. Theoretical analysis shows that the optimization problem can be sample-efficiently solved. Furthermore, we extend the VI-IGL framework to general $f$-Information measures in the information theory literature, leading to the generalized $f$-VI-IGL framework to address the RL problem under the IGL condition. Finally, the empirical results on several reinforcement learning settings indicate an improved performance in comparison to the previous IGL-based RL algorithm",
    "checked": true,
    "id": "4e22616355e35524adc63adee01a386872cf83d8",
    "semantic_title": "an information theoretic approach to interaction-grounded learning",
    "citation_count": 1,
    "authors": [
      "Xiaoyan Hu",
      "Farzan Farnia",
      "Ho-Fung Leung"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24f.html": {
    "title": "Accelerated Speculative Sampling Based on Tree Monte Carlo",
    "volume": "main",
    "abstract": "Speculative Sampling (SpS) has been introduced to speed up inference of large language models (LLMs) by generating multiple tokens in a single forward pass under the guidance of a reference model, while preserving the original distribution. We observe that SpS can be derived through maximum coupling on the token distribution. However, we find that this approach is not optimal as it applies maximum coupling incrementally for each new token, rather than seeking a global maximum coupling that yields a faster algorithm, given the tree-space nature of LLM generative distributions. In this paper, we shift our focus from distributions on a token space to those on a tree space. We propose a novel class of Tree Monte Carlo (TMC) methods, demonstrating their unbiasedness and convergence. As a particular instance of TMC, our new algorithm, Accelerated Speculative Sampling (ASpS), outperforms traditional SpS by generating more tokens per step on average, achieving faster inference, while maintaining the original distribution",
    "checked": true,
    "id": "1ef23ac36e05ca85e64938e5161b951fbd9d9cf7",
    "semantic_title": "accelerated speculative sampling based on tree monte carlo",
    "citation_count": 1,
    "authors": [
      "Zhengmian Hu",
      "Heng Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24g.html": {
    "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code",
    "volume": "main",
    "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal",
    "checked": false,
    "id": "ecd3091debcd2f393379508df70bceb94db0be3b",
    "semantic_title": "scenecraft: an llm agent for synthesizing 3d scene as blender code",
    "citation_count": 10,
    "authors": [
      "Ziniu Hu",
      "Ahmet Iscen",
      "Aashi Jain",
      "Thomas Kipf",
      "Yisong Yue",
      "David A Ross",
      "Cordelia Schmid",
      "Alireza Fathi"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24h.html": {
    "title": "InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization",
    "volume": "main",
    "abstract": "Estimating mutual correlations between random variables or data streams is essential for intelligent behavior and decision-making. As a fundamental quantity for measuring statistical relationships, mutual information has been extensively studied and utilized for its generality and equitability. However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms. We introduce a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures. By maximizing a dual formulation of mutual information through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability. We evaluate the effectiveness and generalization of our proposed mutual information estimation scheme on various families of distributions and applications. Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties. We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time mutual information estimation",
    "checked": true,
    "id": "05cb90b9e3eee5185655a7b42e8b7c2851b5fc44",
    "semantic_title": "infonet: neural estimation of mutual information without test-time optimization",
    "citation_count": 1,
    "authors": [
      "Zhengyang Hu",
      "Song Kang",
      "Qunsong Zeng",
      "Kaibin Huang",
      "Yanchao Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24i.html": {
    "title": "Pseudo-Calibration: Improving Predictive Uncertainty Estimation in Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation (UDA) has seen substantial efforts to improve model accuracy for an unlabeled target domain with the help of a labeled source domain. However, UDA models often exhibit poorly calibrated predictive uncertainty on target data, a problem that remains under-explored and poses risks in safety-critical UDA applications. The calibration problem in UDA is particularly challenging due to the absence of labeled target data and severe distribution shifts between domains. In this paper, we approach UDA calibration as a target-domain-specific unsupervised problem, different from mainstream solutions based on covariate shift. We introduce Pseudo-Calibration (PseudoCal), a novel post-hoc calibration framework. Our innovative use of inference-stage mixup synthesizes a labeled pseudo-target set capturing the structure of the real unlabeled target data. This turns the unsupervised calibration problem into a supervised one, easily solvable with temperature scaling. Extensive empirical evaluations across 5 diverse UDA scenarios involving 10 UDA methods consistently demonstrate the superior performance and versatility of PseudoCal over existing solutions",
    "checked": true,
    "id": "63ec7c542e571bc3d22e3457e7f67409c0495cda",
    "semantic_title": "pseudo-calibration: improving predictive uncertainty estimation in unsupervised domain adaptation",
    "citation_count": 0,
    "authors": [
      "Dapeng Hu",
      "Jian Liang",
      "Xinchao Wang",
      "Chuan-Sheng Foo"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24j.html": {
    "title": "On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis",
    "volume": "main",
    "abstract": "We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\\max$$\\{$ # of stored memory patterns, length of input query sequence$\\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity",
    "checked": true,
    "id": "22910f92c164971ff6ae886ece9c586c703c7153",
    "semantic_title": "on computational limits of modern hopfield models: a fine-grained complexity analysis",
    "citation_count": 19,
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Thomas Lin",
      "Zhao Song",
      "Han Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24k.html": {
    "title": "Improving Interpretation Faithfulness for Vision Transformers",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance for various vision tasks. One reason behind the success lies in their ability to provide plausible innate explanations for the behavior of neural architectures. However, ViTs suffer from issues with explanation faithfulness, as their focal points are fragile to adversarial attacks and can be easily changed with even slight perturbations on the input image. In this paper, we propose a rigorous approach to mitigate these issues by introducing Faithful ViTs (FViTs). Briefly speaking, an FViT should have the following two properties: (1) The top-$k$ indices of its self-attention vector should remain mostly unchanged under input perturbation, indicating stable explanations; (2) The prediction distribution should be robust to perturbations. To achieve this, we propose a new method called Denoised Diffusion Smoothing (DDS), which adopts randomized smoothing and diffusion-based denoising. We theoretically prove that processing ViTs directly with DDS can turn them into FViTs. We also show that Gaussian noise is nearly optimal for both $\\ell_2$ and $\\ell_\\infty$-norm cases. Finally, we demonstrate the effectiveness of our approach through comprehensive experiments and evaluations. Results show that FViTs are more robust against adversarial attacks while maintaining the explainability of attention, indicating higher faithfulness",
    "checked": true,
    "id": "baefc63fc1fa873776696ef13e5a938a8ae6a20a",
    "semantic_title": "improving interpretation faithfulness for vision transformers",
    "citation_count": 2,
    "authors": [
      "Lijie Hu",
      "Yixin Liu",
      "Ninghao Liu",
      "Mengdi Huai",
      "Lichao Sun",
      "Di Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24l.html": {
    "title": "Multigroup Robustness",
    "volume": "main",
    "abstract": "To address the shortcomings of real-world datasets, robust learning algorithms have been designed to overcome arbitrary and indiscriminate data corruption. However, practical processes of gathering data may lead to patterns of data corruption that are localized to specific partitions of the training dataset. Motivated by critical applications where the learned model is deployed to make predictions about people from a rich collection of overlapping subpopulations, we initiate the study of multigroup robust algorithms whose robustness guarantees for each subpopulation only degrade with the amount of data corruption inside that subpopulation. When the data corruption is not distributed uniformly over subpopulations, our algorithms provide more meaningful robustness guarantees than standard guarantees that are oblivious to how the data corruption and the affected subpopulations are related. Our techniques establish a new connection between multigroup fairness and robustness",
    "checked": true,
    "id": "4236ba683c43bc7d8a127b7c1f4b911e5fb23239",
    "semantic_title": "multigroup robustness",
    "citation_count": 1,
    "authors": [
      "Lunjia Hu",
      "Charlotte Peale",
      "Judy Hanwen Shen"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24m.html": {
    "title": "Provable Privacy with Non-Private Pre-Processing",
    "volume": "main",
    "abstract": "When analyzing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication, standard scaling and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines",
    "checked": true,
    "id": "09c1219ce152774c2f372dd5d8e8d17870537825",
    "semantic_title": "provable privacy with non-private pre-processing",
    "citation_count": 1,
    "authors": [
      "Yaxi Hu",
      "Amartya Sanyal",
      "Bernhard Schölkopf"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24n.html": {
    "title": "Case-Based or Rule-Based: How Do Transformers Do the Math?",
    "volume": "main",
    "abstract": "Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar cases seen in the training corpus for help. We define these two different reasoning mechanisms as \"rule-based reasoning\" and \"case-based reasoning\". Since rule-based reasoning is essential for acquiring systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to use rules explicitly helps them learn rule-based reasoning and generalize better in length. Code is available at https://github.com/GraphPKU/Case_or_Rule",
    "checked": true,
    "id": "fc45b0c7249c9e48de2cd35fc3d9984490229392",
    "semantic_title": "case-based or rule-based: how do transformers do the math?",
    "citation_count": 11,
    "authors": [
      "Yi Hu",
      "Xiaojuan Tang",
      "Haotong Yang",
      "Muhan Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24o.html": {
    "title": "Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications",
    "volume": "main",
    "abstract": "Model inversion, which aims to reconstruct the original training data from pre-trained discriminative models, is especially useful when the original training data is unavailable due to privacy, usage rights, or size constraints. However, existing dense inversion methods attempt to reconstruct the entire image area, making them extremely inefficient when inverting high-resolution images from large-scale Vision Transformers (ViTs). We further identify two underlying causes of this inefficiency: the redundant inversion of noisy backgrounds and the unintended inversion of spurious correlations—a phenomenon we term \"hallucination\" in model inversion. To address these limitations, we propose a novel sparse model inversion strategy, as a plug-and-play extension to speed up existing dense inversion methods with no need for modifying their original loss functions. Specifically, we selectively invert semantic foregrounds while stopping the inversion of noisy backgrounds and potential spurious correlations. Through both theoretical and empirical studies, we validate the efficacy of our approach in achieving significant inversion acceleration (up to $\\times$3.79) while maintaining comparable or even enhanced downstream performance in data-free model quantization and data-free knowledge transfer. Code is available at https://github.com/Egg-Hu/SMI",
    "checked": true,
    "id": "cca0498bc0a2ea3b13a6298f67a93478fd962c7d",
    "semantic_title": "sparse model inversion: efficient inversion of vision transformers for data-free applications",
    "citation_count": 0,
    "authors": [
      "Zixuan Hu",
      "Yongxian Wei",
      "Li Shen",
      "Zhenyi Wang",
      "Lei Li",
      "Chun Yuan",
      "Dacheng Tao"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24p.html": {
    "title": "Bayesian Design Principles for Offline-to-Online Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe. However, offline learned policies are often suboptimal, and further online fine-tuning is required. In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop. We show that Bayesian design principles are crucial in solving such a dilemma. Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies. Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy. Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach. Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data",
    "checked": true,
    "id": "e825e6325dfb5b93066817e7cc6b226ba7d3b799",
    "semantic_title": "bayesian design principles for offline-to-online reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Hao Hu",
      "Yiqin Yang",
      "Jianing Ye",
      "Chengjie Wu",
      "Ziqing Mai",
      "Yujing Hu",
      "Tangjie Lv",
      "Changjie Fan",
      "Qianchuan Zhao",
      "Chongjie Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24q.html": {
    "title": "High-Performance Temporal Reversible Spiking Neural Networks with $\\mathcalO(L)$ Training Memory and $\\mathcalO(1)$ Inference Cost",
    "volume": "main",
    "abstract": "Multi-timestep simulation of brain-inspired Spiking Neural Networks (SNNs) boost memory requirements during training and increase inference energy cost. Current training methods cannot simultaneously solve both training and inference dilemmas. This work proposes a novel Temporal Reversible architecture for SNNs (T-RevSNN) to jointly address the training and inference challenges by altering the forward propagation of SNNs. We turn off the temporal dynamics of most spiking neurons and design multi-level temporal reversible interactions at temporal turn-on spiking neurons, resulting in a $\\mathcal{O}(L)$ training memory. Combined with the temporal reversible nature, we redesign the input encoding and network organization of SNNs to achieve $\\mathcal{O}(1)$ inference energy cost. Then, we finely adjust the internal units and residual connections of the basic SNN block to ensure the effectiveness of sparse temporal information interaction. T-RevSNN achieves excellent accuracy on ImageNet, while the memory efficiency, training time acceleration and inference energy efficiency can be significantly improved by $8.6 \\times$, $2.0 \\times$ and $1.6 \\times$, respectively. This work is expected to break the technical bottleneck of significantly increasing memory cost and training time for large-scale SNNs while maintaining both high performance and low inference energy cost",
    "checked": false,
    "id": "a656622ac14a33d8b77b16afd51e1502bda58475",
    "semantic_title": "high-performance temporal reversible spiking neural networks with o(l) training memory and o(1) inference cost",
    "citation_count": 2,
    "authors": [
      "Jiakui Hu",
      "Man Yao",
      "Xuerui Qiu",
      "Yuhong Chou",
      "Yuxuan Cai",
      "Ning Qiao",
      "Yonghong Tian",
      "Bo Xu",
      "Guoqi Li"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24r.html": {
    "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
    "volume": "main",
    "abstract": "Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a \"flip rate\" to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain",
    "checked": false,
    "id": "b00d3c32f25da4dd0bb97a87a7054bacd3bc3034",
    "semantic_title": "accelerating transformer pre-training with 2: 4 sparsity",
    "citation_count": 4,
    "authors": [
      "Yuezhou Hu",
      "Kang Zhao",
      "Weiyu Huang",
      "Jianfei Chen",
      "Jun Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/hu24s.html": {
    "title": "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks",
    "volume": "main",
    "abstract": "In this paper, we introduce InfiAgent-DABench, the first benchmark specifically designed to evaluate LLM-based agents on data analysis tasks. Agents need to solve these tasks end-to-end by interacting with an execution environment. This benchmark contains DAEval, a dataset consisting of 603 data analysis questions derived from 124 CSV files, and an agent framework which incorporates LLMs to serve as data analysis agents for both serving and evaluating. Since data analysis questions are often open-ended and hard to evaluate without human supervision, we adopt a format-prompting technique to convert each question into a closed-form format so that they can be automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the current challenges encountered in data analysis tasks. In addition, building upon our agent framework, we develop a specialized agent, DAAgent, which surpasses GPT-3.5 by 3.9% on DABench. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent",
    "checked": true,
    "id": "ecd5d38b8590f6e6a504b29110d16ff717cd56ef",
    "semantic_title": "infiagent-dabench: evaluating agents on data analysis tasks",
    "citation_count": 17,
    "authors": [
      "Xueyu Hu",
      "Ziyu Zhao",
      "Shuang Wei",
      "Ziwei Chai",
      "Qianli Ma",
      "Guoyin Wang",
      "Xuwu Wang",
      "Jing Su",
      "Jingjing Xu",
      "Ming Zhu",
      "Yao Cheng",
      "Jianbo Yuan",
      "Jiwei Li",
      "Kun Kuang",
      "Yang Yang",
      "Hongxia Yang",
      "Fei Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/hua24a.html": {
    "title": "ReconBoost: Boosting Can Achieve Modality Reconcilement",
    "volume": "main",
    "abstract": "This paper explores a novel multi-modal alternating learning paradigm pursuing a reconciliation between the exploitation of uni-modal features and the exploration of cross-modal interactions. This is motivated by the fact that current paradigms of multi-modal learning tend to explore multi-modal features simultaneously. The resulting gradient prohibits further exploitation of the features in the weak modality, leading to modality competition, where the dominant modality overpowers the learning process. To address this issue, we study the modality-alternating learning paradigm to achieve reconcilement. Specifically, we propose a new method called ReconBoost to update a fixed modality each time. Herein, the learning objective is dynamically adjusted with a reconcilement regularization against competition with the historical models. By choosing a KL-based reconcilement, we show that the proposed method resembles Friedman's Gradient-Boosting (GB) algorithm, where the updated learner can correct errors made by others and help enhance the overall performance. The major difference with the classic GB is that we only preserve the newest model for each modality to avoid overfitting caused by ensembling strong learners. Furthermore, we propose a memory consolidation scheme and a global rectification scheme to make this strategy more effective. Experiments over six multi-modal benchmarks speak to the efficacy of the proposed method",
    "checked": true,
    "id": "dd08e27ec1014e6ed2d8a300fd3835e4882530c6",
    "semantic_title": "reconboost: boosting can achieve modality reconcilement",
    "citation_count": 2,
    "authors": [
      "Cong Hua",
      "Qianqian Xu",
      "Shilong Bao",
      "Zhiyong Yang",
      "Qingming Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24a.html": {
    "title": "Optimal Hessian/Jacobian-Free Nonconvex-PL Bilevel Optimization",
    "volume": "main",
    "abstract": "Bilevel optimization is widely applied in many machine learning tasks such as hyper-parameter learning, meta learning and reinforcement learning. Although many algorithms recently have been developed to solve the bilevel optimization problems, they generally rely on the (strongly) convex lower-level problems. More recently, some methods have been proposed to solve the nonconvex-PL bilevel optimization problems, where their upper-level problems are possibly nonconvex, and their lower-level problems are also possibly nonconvex while satisfying Polyak-Łojasiewicz (PL) condition. However, these methods still have a high convergence complexity or a high computation complexity such as requiring compute expensive Hessian/Jacobian matrices and its inverses. In the paper, thus, we propose an efficient Hessian/Jacobian-free method (i.e., HJFBiO) with the optimal convergence complexity to solve the nonconvex-PL bilevel problems. Theoretically, under some mild conditions, we prove that our HJFBiO method obtains an optimal convergence rate of $O(\\frac{1}{T})$, where $T$ denotes the number of iterations, and has an optimal gradient complexity of $O(\\epsilon^{-1})$ in finding an $\\epsilon$-stationary solution. We conduct some numerical experiments on the bilevel PL game and hyper-representation learning task to demonstrate efficiency of our proposed method",
    "checked": true,
    "id": "2d475f16fada22c36a1d8d64bd70dc9c3da1f63f",
    "semantic_title": "optimal hessian/jacobian-free nonconvex-pl bilevel optimization",
    "citation_count": 0,
    "authors": [
      "Feihu Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24b.html": {
    "title": "NeuralIndicator: Implicit Surface Reconstruction from Neural Indicator Priors",
    "volume": "main",
    "abstract": "The neural implicit surface reconstruction from unorganized points is still challenging, especially when the point clouds are incomplete and/or noisy with complex topology structure. Unlike previous approaches performing neural implicit surface learning relying on local shape priors, this paper proposes to utilize global shape priors to regularize the neural implicit function learning for more reliable surface reconstruction. To this end, we first introduce a differentiable module to generate a smooth indicator function, which globally encodes both the indicative prior and local SDFs of the entire input point cloud. Benefit from this, we propose a new framework, called NeuralIndicator, to jointly learn both the smooth indicator function and neural implicit function simultaneously, using the global shape prior encoded by smooth indicator function to effectively regularize the neural implicit function learning, towards reliable and high-fidelity surface reconstruction from unorganized points without any normal information. Extensive evaluations on synthetic and real-scan datasets show that our approach consistently outperforms previous approaches, especially when point clouds are incomplete and/or noisy with complex topology structure",
    "checked": true,
    "id": "bcf4da3ea178cfd2fcde336c8100ebdc45406b9d",
    "semantic_title": "neuralindicator: implicit surface reconstruction from neural indicator priors",
    "citation_count": 0,
    "authors": [
      "Shi-Sheng Huang",
      "Guo Chen",
      "Chen Li Heng",
      "Hua Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24c.html": {
    "title": "Auctionformer: A Unified Deep Learning Algorithm for Solving Equilibrium Strategies in Auction Games",
    "volume": "main",
    "abstract": "Auction games have been widely used in plenty of trading environments such as online advertising and real estate. The complexity of real-world scenarios, characterized by diverse auction mechanisms and bidder asymmetries, poses significant challenges in efficiently solving for equilibria. Traditional learning approaches often face limitations due to their specificity to certain settings and high resource demands. Addressing this, we introduce Auctionformer, an efficient transformer-based method to solve equilibria of diverse auctions in a unified framework. Leveraging the flexible tokenization schemes, Auctionformer translates varying auction games into a standard token series, making use of renowned Transformer architectures. Moreover, we employ Nash error as the loss term, sidestepping the need for underlying equilibrium solutions and enabling efficient training and inference. Furthermore, a few-shot framework supports adaptability to new mechanisms, reinforced by a self-supervised fine-tuning approach. Extensive experimental results affirm the superior performance of Auctionformer over contemporary methods, heralding its potential for broad real-world applications",
    "checked": true,
    "id": "a80653baa6ac86aa4e8a894825770df9dd7b98df",
    "semantic_title": "auctionformer: a unified deep learning algorithm for solving equilibrium strategies in auction games",
    "citation_count": 0,
    "authors": [
      "Kexin Huang",
      "Ziqian Chen",
      "Xue Wang",
      "Chongming Gao",
      "Jinyang Gao",
      "Bolin Ding",
      "Xiang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24d.html": {
    "title": "In-context Convergence of Transformers",
    "volume": "main",
    "abstract": "Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map. More notably, for data with imbalanced features, we show that the learning dynamics take a stage-wise convergence process, where the transformer first converges to a near-zero prediction error for the query tokens of dominant features, and then converges later to a near-zero error for query tokens of under-represented features, via one and four training phases. Our proof features new techniques for analyzing the competing strengths of two types of attention weights, the change of which determines different training phases",
    "checked": true,
    "id": "ab643f5b02786fc4772b662adcdb558c557d3bf6",
    "semantic_title": "in-context convergence of transformers",
    "citation_count": 42,
    "authors": [
      "Yu Huang",
      "Yuan Cheng",
      "Yingbin Liang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24e.html": {
    "title": "Near-Linear Time Approximation Algorithms for k-means with Outliers",
    "volume": "main",
    "abstract": "The k-means with outliers problem is one of the most extensively studied clustering problems in the field of machine learning, where the goal is to discard up to z outliers and identify a minimum k-means clustering on the remaining data points. Most previous results for this problem have running time dependent on the aspect ratio Δ (the ratio between the maximum and the minimum pairwise distances) to achieve fast approximations. To address the issue of aspect ratio dependency on the running time, we propose sampling-based algorithms with almost linear running time in the data size, where a crucial component of our approach is an algorithm called Fast-Sampling. Fast-Sampling algorithm can find inliers that well approximate the optimal clustering centers without relying on a guess for the optimal clustering costs, where a 4-approximate solution can be obtained in time $O(\\frac{ndk\\log\\log n}{\\epsilon^2})$ with O(k/ϵ) centers opened and (1+ϵ)z outliers discarded. To reduce the number of centers opened, we propose a center reduction algorithm, where an O(1/ϵ)-approximate solution can be obtained in time $O(\\frac{ndk\\log \\log n}{\\epsilon^2} + dpoly(k, \\frac{1}{\\epsilon})\\log(n\\Delta))$ with (1+ϵ)z outliers discarded and exactly k centers opened. Empirical experiments suggest that our proposed sampling-based algorithms outperform state-of-the-art algorithms for the k-means with outliers problem",
    "checked": true,
    "id": "14b4b90ebbef9b62fbcfac438a31fb70487af098",
    "semantic_title": "near-linear time approximation algorithms for k-means with outliers",
    "citation_count": 0,
    "authors": [
      "Junyu Huang",
      "Qilong Feng",
      "Ziyun Huang",
      "Jinhui Xu",
      "Jianxin Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24f.html": {
    "title": "Contrastive Predict-and-Search for Mixed Integer Linear Programs",
    "volume": "main",
    "abstract": "Mixed integer linear programs (MILP) are flexible and powerful tools for modeling and solving many difficult real-world combinatorial optimization problems. In this paper, we propose a novel machine learning (ML)-based framework ConPaS that learns to predict solutions to MILPs with contrastive learning. For training, we collect high-quality solutions as positive samples. We also collect low-quality or infeasible solutions as negative samples using novel optimization-based or sampling approaches. We then learn to make discriminative predictions by contrasting the positive and negative samples. During testing, we predict and fix the assignments for a subset of integer variables and then solve the resulting reduced MILP to find high-quality solutions. Empirically, ConPaS achieves state-of-the-art results compared to other ML-based approaches in terms of the quality of and the speed at which solutions are found",
    "checked": true,
    "id": "f36cde4f4ea8b8c365ebc81501e7571d0a933aef",
    "semantic_title": "contrastive predict-and-search for mixed integer linear programs",
    "citation_count": 2,
    "authors": [
      "Taoan Huang",
      "Aaron M Ferber",
      "Arman Zharmagambetov",
      "Yuandong Tian",
      "Bistra Dilkina"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24g.html": {
    "title": "Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion",
    "volume": "main",
    "abstract": "We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quality and rule-based controllability, outperforming current state-of-the-art generators in a variety of settings. For detailed demonstrations, code and model checkpoints, please visit our project website",
    "checked": true,
    "id": "ee9ed60f41a0fe1c515c2e4d348ae4840f9f7e3e",
    "semantic_title": "symbolic music generation with non-differentiable rule guided diffusion",
    "citation_count": 9,
    "authors": [
      "Yujia Huang",
      "Adishree Ghatare",
      "Yuanzhe Liu",
      "Ziniu Hu",
      "Qinsheng Zhang",
      "Chandramouli Shama Sastry",
      "Siddharth Gururani",
      "Sageev Oore",
      "Yisong Yue"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24h.html": {
    "title": "DiffDA: a Diffusion model for weather-scale Data Assimilation",
    "volume": "main",
    "abstract": "The generation of initial conditions via accurate data assimilation is crucial for weather forecasting and climate modeling. We propose DiffDA as a denoising diffusion model capable of assimilating atmospheric variables using predicted states and sparse observations. Acknowledging the similarity between a weather forecast model and a denoising diffusion model dedicated to weather applications, we adapt the pretrained GraphCast neural network as the backbone of the diffusion model. Through experiments based on simulated observations from the ERA5 reanalysis dataset, our method can produce assimilated global atmospheric data consistent with observations at 0.25$^\\circ$ ($\\approx$30km) resolution globally. This marks the highest resolution achieved by ML data assimilation models. The experiments also show that the initial conditions assimilated from sparse observations (less than 0.96% of gridded data) and 48-hour forecast can be used for forecast models with a loss of lead time of at most 24 hours compared to initial conditions from state-of-the-art data assimilation in ERA5. This enables the application of the method to real-world applications, such as creating reanalysis datasets with autoregressive data assimilation",
    "checked": true,
    "id": "244d8dc93c0af56a4f9db9b474d5b3ed1793a995",
    "semantic_title": "diffda: a diffusion model for weather-scale data assimilation",
    "citation_count": 19,
    "authors": [
      "Langwen Huang",
      "Lukas Gianinazzi",
      "Yuejiang Yu",
      "Peter Dominik Dueben",
      "Torsten Hoefler"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24i.html": {
    "title": "Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL",
    "volume": "main",
    "abstract": "We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by Huang et al. (2024). We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t. P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems. We further extend our results to Multi-Type MFGs, generalizing from conventional MFGs and involving multiple types of agents. This extension implies statistical tractability of a broader class of Markov Games through the efficacy of mean-field approximation. Finally, inspired by our theoretical algorithm, we present a heuristic approach with improved computational efficiency and empirically demonstrate its effectiveness",
    "checked": true,
    "id": "03bccba9349f257ebc90a3d717e0a14f7703a12b",
    "semantic_title": "model-based rl for mean-field games is not statistically harder than single-agent rl",
    "citation_count": 4,
    "authors": [
      "Jiawei Huang",
      "Niao He",
      "Andreas Krause"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24j.html": {
    "title": "In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought",
    "volume": "main",
    "abstract": "In-context learning is a promising approach for offline reinforcement learning (RL) to handle online tasks, which can be achieved by providing task prompts. Recent works demonstrated that in-context RL could emerge with self-improvement in a trial-and-error manner when treating RL tasks as an across-episodic sequential prediction problem. Despite the self-improvement not requiring gradient updates, current works still suffer from high computational costs when the across-episodic sequence increases with task horizons. To this end, we propose an In-context Decision Transformer (IDT) to achieve self-improvement in a high-level trial-and-error manner. Specifically, IDT is inspired by the efficient hierarchical structure of human decision-making and thus reconstructs the sequence to consist of high-level decisions instead of low-level actions that interact with environments. As one high-level decision can guide multi-step low-level actions, IDT naturally avoids excessively long sequences and solves online tasks more efficiently. Experimental results show that IDT achieves state-of-the-art in long-horizon tasks over current in-context RL methods. In particular, the online evaluation time of our IDT is 36$\\times$ times faster than baselines in the D4RL benchmark and 27$\\times$ times faster in the Grid World benchmark",
    "checked": true,
    "id": "b04c769c92f2bc58c5f9e6cf220480bae26405d6",
    "semantic_title": "in-context decision transformer: reinforcement learning via hierarchical chain-of-thought",
    "citation_count": 2,
    "authors": [
      "Sili Huang",
      "Jifeng Hu",
      "Hechang Chen",
      "Lichao Sun",
      "Bo Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24k.html": {
    "title": "InstructSpeech: Following Speech Editing Instructions via Large Language Models",
    "volume": "main",
    "abstract": "Instruction-guided speech editing aims to follow the user's natural language instruction to manipulate the semantic and acoustic attributes of a speech. In this work, we construct triplet paired data (instruction, input speech, output speech) to alleviate data scarcity and train a multi-task large language model named InstructSpeech. To mitigate the challenges of accurately executing user's instructions, we 1) introduce the learned task embeddings with a fine-tuned Flan-T5-XL to guide the generation process towards the correct generative task; 2) include an extensive and diverse set of speech editing and processing tasks to enhance model capabilities; 3) investigate chain-of-thought reasoning for free-form semantic content editing; and 4) propose a hierarchical adapter that effectively updates a small portion of parameters for generalization to new tasks. To assess instruction speech editing in greater depth, we introduce a benchmark evaluation with contrastive instruction-speech pre-training (CISP) to test the speech quality and instruction-speech alignment faithfulness. Experimental results demonstrate that InstructSpeech achieves state-of-the-art results in eleven tasks, for the first time unlocking the ability to edit speech's acoustic and semantic attributes following a user's instruction. Audio samples are available at https://InstructSpeech.github.io",
    "checked": true,
    "id": "b4aa236384ddfd53d5c2616746c829a65efd9163",
    "semantic_title": "instructspeech: following speech editing instructions via large language models",
    "citation_count": 1,
    "authors": [
      "Rongjie Huang",
      "Ruofan Hu",
      "Yongqi Wang",
      "Zehan Wang",
      "Xize Cheng",
      "Ziyue Jiang",
      "Zhenhui Ye",
      "Dongchao Yang",
      "Luping Liu",
      "Peng Gao",
      "Zhou Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24l.html": {
    "title": "Bayesian Power Steering: An Effective Approach for Domain Adaptation of Diffusion Models",
    "volume": "main",
    "abstract": "We propose a Bayesian framework for fine-tuning large diffusion models with a novel network structure called Bayesian Power Steering (BPS). We clarify the meaning behind adaptation from a large probability space to a small probability space and explore the task of fine-tuning pre-trained models using learnable modules from a Bayesian perspective. BPS extracts task-specific knowledge from a pre-trained model's learned prior distribution. It efficiently leverages large diffusion models, differentially intervening different hidden features with a head-heavy and foot-light configuration. Experiments highlight the superiority of BPS over contemporary methods across a range of tasks even with limited amount of data. Notably, BPS attains an FID score of 10.49 under the sketch condition on the COCO17 dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Huang",
      "Ting Li",
      "Jian Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24m.html": {
    "title": "AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios",
    "volume": "main",
    "abstract": "We propose the attention-inspired numerical solver (AttNS), a concise method that helps the generalization and robustness issues faced by the AI-Hybrid numerical solver in solving differential equations due to limited data. AttNS is inspired by the effectiveness of attention modules in Residual Neural Networks (ResNet) in enhancing model generalization and robustness for conventional deep learning tasks. Drawing from the dynamical system perspective of ResNet, We seamlessly incorporate attention mechanisms into the design of numerical methods tailored for the characteristics of solving differential equations. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, showcase AttNS consistently enhancing various numerical solvers without any intricate model crafting. Finally, we analyze AttNS experimentally and theoretically, demonstrating its ability to achieve strong generalization and robustness while ensuring the convergence of the solver. This includes requiring less data compared to other advanced methods to achieve comparable generalization errors and better prevention of numerical explosion issues when solving differential equations",
    "checked": true,
    "id": "0f24f543b65fb0c5424b03f8f985b5993519764a",
    "semantic_title": "attns: attention-inspired numerical solving for limited data scenarios",
    "citation_count": 1,
    "authors": [
      "Zhongzhan Huang",
      "Mingfu Liang",
      "Shanshan Zhong",
      "Liang Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24n.html": {
    "title": "CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Compared to conventional deep Artificial Neural Networks (ANNs), SNNs exhibit superior efficiency and capability to process temporal information. However, it remains a challenge to train SNNs due to their undifferentiable spiking mechanism. The surrogate gradients method is commonly used to train SNNs, but often comes with an accuracy disadvantage over ANNs counterpart. We link the degraded accuracy to the vanishing of gradient on the temporal dimension through the analytical and experimental study of the training process of Leaky Integrate-and-Fire (LIF) Neuron-based SNNs. Moreover, we propose the Complementary Leaky Integrate-and-Fire (CLIF) Neuron. CLIF creates extra paths to facilitate the backpropagation in computing temporal gradient while keeping binary output. CLIF is hyperparameter-free and features broad applicability. Extensive experiments on a variety of datasets demonstrate CLIF's clear performance advantage over other neuron models. Furthermore, the CLIF's performance even slightly surpasses superior ANNs with identical network structure and training conditions. The code is available at https://github.com/HuuYuLong/Complementary-LIF",
    "checked": true,
    "id": "a59bce51ffc91f03f990859af7ece7a166155329",
    "semantic_title": "clif: complementary leaky integrate-and-fire neuron for spiking neural networks",
    "citation_count": 1,
    "authors": [
      "Yulong Huang",
      "Xiaopeng Lin",
      "Hongwei Ren",
      "Haotian Fu",
      "Yue Zhou",
      "Zunchang Liu",
      "Biao Pan",
      "Bojun Cheng"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24o.html": {
    "title": "Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning",
    "volume": "main",
    "abstract": "Although pre-trained models such as Contrastive Language-Image Pre-Training (CLIP) show impressive generalization results, their robustness is still limited under Out-of-Distribution (OOD) scenarios. Instead of undesirably leveraging human annotation as commonly done, it is possible to leverage the visual understanding power of Multi-modal Large Language Models (MLLMs). However, MLLMs struggle with vision problems due to task incompatibility, thus hindering their effectiveness. In this paper, we propose to effectively leverage MLLMs via Machine Vision Therapy which aims to rectify erroneous predictions of specific vision models. By supervising vision models using MLLM predictions, visual robustness can be boosted in a nearly unsupervised manner. Moreover, we propose a Denoising In-Context Learning (DICL) strategy to solve the incompatibility issue. Concretely, by examining the noise probability of each example through a transition matrix, we construct an instruction containing a correct exemplar and a probable erroneous one, which enables MLLMs to detect and rectify the incorrect predictions of vision models. Under mild assumptions, we theoretically show that our DICL method is guaranteed to find the ground truth. Through extensive experiments on various OOD datasets, our method demonstrates powerful capabilities for enhancing visual robustness under many OOD scenarios",
    "checked": true,
    "id": "ad5d29eea82fd929adb0c5cfc3a11c9e5004c97f",
    "semantic_title": "machine vision therapy: multimodal large language models can enhance visual robustness via denoising in-context learning",
    "citation_count": 0,
    "authors": [
      "Zhuo Huang",
      "Chang Liu",
      "Yinpeng Dong",
      "Hang Su",
      "Shibao Zheng",
      "Tongliang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24p.html": {
    "title": "Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning",
    "volume": "main",
    "abstract": "Despite the recent successes of multi-agent reinforcement learning (MARL) algorithms, efficiently adapting to co-players in mixed-motive environments remains a significant challenge. One feasible approach is to hierarchically model co-players' behavior based on inferring their characteristics. However, these methods often encounter difficulties in efficient reasoning and utilization of inferred information. To address these issues, we propose Hierarchical Opponent modeling and Planning (HOP), a novel multi-agent decision-making algorithm that enables few-shot adaptation to unseen policies in mixed-motive environments. HOP is hierarchically composed of two modules: an opponent modeling module that infers others' goals and learns corresponding goal-conditioned policies, and a planning module that employs Monte Carlo Tree Search (MCTS) to identify the best response. Our approach improves efficiency by updating beliefs about others' goals both across and within episodes and by using information from the opponent modeling module to guide planning. Experimental results demonstrate that in mixed-motive environments, HOP exhibits superior few-shot adaptation capabilities when interacting with various unseen agents, and excels in self-play scenarios. Furthermore, the emergence of social intelligence during our experiments underscores the potential of our approach in complex multi-agent environments",
    "checked": true,
    "id": "90ef2bbe472d969d21545502abbee504c701730d",
    "semantic_title": "efficient adaptation in mixed-motive environments via hierarchical opponent modeling and planning",
    "citation_count": 2,
    "authors": [
      "Yizhe Huang",
      "Anji Liu",
      "Fanqi Kong",
      "Yaodong Yang",
      "Song-Chun Zhu",
      "Xue Feng"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24q.html": {
    "title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs",
    "volume": "main",
    "abstract": "Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM, for the first time, achieves high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across various LLM families and evaluation metrics, outperforms SOTA quantization methods of LLM by significant margins. Moreover, BiLLM enables the binarization process of a 7-billion LLM within 0.5 hours on a single GPU, demonstrating satisfactory time efficiency. Our code is available at https://github.com/Aaronhuang-778/BiLLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Yangdong Liu",
      "Haotong Qin",
      "Ying Li",
      "Shiming Zhang",
      "Xianglong Liu",
      "Michele Magno",
      "Xiaojuan Qi"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24r.html": {
    "title": "An Empirical Examination of Balancing Strategy for Counterfactual Estimation on Time Series",
    "volume": "main",
    "abstract": "Counterfactual estimation from observations represents a critical endeavor in numerous application fields, such as healthcare and finance, with the primary challenge being the mitigation of treatment bias. The balancing strategy aimed at reducing covariate disparities between different treatment groups serves as a universal solution. However, when it comes to the time series data, the effectiveness of balancing strategies remains an open question, with a thorough analysis of the robustness and applicability of balancing strategies still lacking. This paper revisits counterfactual estimation in the temporal setting and provides a brief overview of recent advancements in balancing strategies. More importantly, we conduct a critical empirical examination for the effectiveness of the balancing strategies within the realm of temporal counterfactual estimation in various settings on multiple datasets. Our findings could be of significant interest to researchers and practitioners and call for a reexamination of the balancing strategy in time series settings",
    "checked": true,
    "id": "c11575b69424770336add6ee1def3c63c8724411",
    "semantic_title": "an empirical examination of balancing strategy for counterfactual estimation on time series",
    "citation_count": 0,
    "authors": [
      "Qiang Huang",
      "Chuizheng Meng",
      "Defu Cao",
      "Biwei Huang",
      "Yi Chang",
      "Yan Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24s.html": {
    "title": "MFTN: A Multi-scale Feature Transfer Network Based on IMatchFormer for Hyperspectral Image Super-Resolution",
    "volume": "main",
    "abstract": "Hyperspectral image super-resolution (HISR) aims to fuse a low-resolution hyperspectral image (LR-HSI) with a high-resolution multispectral image (HR-MSI) to obtain a high-resolution hyperspectral image (HR-HSI). Due to some existing HISR methods ignoring the significant feature difference between LR-HSI and HR-MSI, the reconstructed HR-HSI typically exhibits spectral distortion and blurring of spatial texture. To solve this issue, we propose a multi-scale feature transfer network (MFTN) for HISR. Firstly, three multi-scale feature extractors are constructed to extract features of different scales from the input images. Then, a multi-scale feature transfer module (MFTM) consisting of three improved feature matching Transformers (IMatchFormers) is designed to learn the detail features of different scales from HR-MSI by establishing the cross-model feature correlation between LR-HSI and degraded HR-MSI. Finally, a multiscale dynamic aggregation module (MDAM) containing three spectral aware aggregation modules (SAAMs) is constructed to reconstruct the final HR-HSI by gradually aggregating features of different scales. Extensive experimental results on three commonly used datasets demonstrate that the proposed model achieves better performance compared to state- of-the-art (SOTA) methods",
    "checked": true,
    "id": "17ea1cf4f4d2a502a34736a30cb3c3e57658dadc",
    "semantic_title": "mftn: a multi-scale feature transfer network based on imatchformer for hyperspectral image super-resolution",
    "citation_count": 0,
    "authors": [
      "Shuying Huang",
      "Mingyang Ren",
      "Yong Yang",
      "Xiaozheng Wang",
      "Yingzhi Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24t.html": {
    "title": "On Which Nodes Does GCN Fail? Enhancing GCN From the Node Perspective",
    "volume": "main",
    "abstract": "The label smoothness assumption is at the core of Graph Convolutional Networks (GCNs): nodes in a local region have similar labels. Thus, GCN performs local feature smoothing operation to adhere to this assumption. However, there exist some nodes whose labels obtained by feature smoothing conflict with the label smoothness assumption. We find that the label smoothness assumption and the process of feature smoothing are both problematic on these nodes, and call these nodes out of GCN's control (OOC nodes). In this paper, first, we design the corresponding algorithm to locate the OOC nodes, then we summarize the characteristics of OOC nodes that affect their representation learning, and based on their characteristics, we present DaGCN, an efficient framework that can facilitate the OOC nodes. Extensive experiments verify the superiority of the proposed method and demonstrate that current advanced GCNs are improvements specifically on OOC nodes; the remaining nodes under GCN's control (UC nodes) are already optimally represented by vanilla GCN on most datasets",
    "checked": true,
    "id": "53a4db63cfff3a00917c65fd88c2ca4fad549555",
    "semantic_title": "on which nodes does gcn fail? enhancing gcn from the node perspective",
    "citation_count": 3,
    "authors": [
      "Jincheng Huang",
      "Jialie Shen",
      "Xiaoshuang Shi",
      "Xiaofeng Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24u.html": {
    "title": "Self-Driven Entropy Aggregation for Byzantine-Robust Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "Federated learning presents massive potential for privacy-friendly collaboration. However, the performance of federated learning is deeply affected by byzantine attacks, where malicious clients deliberately upload crafted vicious updates. While various robust aggregations have been proposed to defend against such attacks, they are subject to certain assumptions: homogeneous private data and related proxy datasets. To address these limitations, we propose Self-Driven Entropy Aggregation (SDEA), which leverages the random public dataset to conduct Byzantine-robust aggregation in heterogeneous federated learning. For Byzantine attackers, we observe that benign ones typically present more confident (sharper) predictions than evils on the public dataset. Thus, we highlight benign clients by introducing learnable aggregation weight to minimize the instance-prediction entropy of the global model on the random public dataset. Besides, with inherent data heterogeneity in federated learning, we reveal that it brings heterogeneous sharpness. Specifically, clients are optimized under distinct distribution and thus present fruitful predictive preferences. The learnable aggregation weight blindly allocates high attention to limited ones for sharper predictions, resulting in a biased global model. To alleviate this problem, we encourage the global model to offer diverse predictions via batch-prediction entropy maximization and conduct clustering to equally divide honest weights to accommodate different tendencies. This endows SDEA to detect Byzantine attackers in heterogeneous federated learning. Empirical results demonstrate the effectiveness",
    "checked": true,
    "id": "ec35adf666b1b6c3bea1c22395dd97b18f3471a2",
    "semantic_title": "self-driven entropy aggregation for byzantine-robust heterogeneous federated learning",
    "citation_count": 1,
    "authors": [
      "Wenke Huang",
      "Zekun Shi",
      "Mang Ye",
      "He Li",
      "Bo Du"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24v.html": {
    "title": "Overcoming Data and Model heterogeneities in Decentralized Federated Learning via Synthetic Anchors",
    "volume": "main",
    "abstract": "Conventional Federated Learning (FL) involves collaborative training of a global model while maintaining user data privacy. One of its branches, decentralized FL, is a serverless network that allows clients to own and optimize different local models separately, which results in saving management and communication resources. Despite the promising advancements in decentralized FL, it may reduce model generalizability due to lacking a global model. In this scenario, managing data and model heterogeneity among clients becomes a crucial problem, which poses a unique challenge that must be overcome: How can every client's local model learn generalizable representation in a decentralized manner? To address this challenge, we propose a novel Decentralized FL technique by introducing Synthetic Anchors, dubbed as DeSA. Based on the theory of domain adaptation and Knowledge Distillation (KD), we theoretically and empirically show that synthesizing global anchors based on raw data distribution facilitates mutual knowledge transfer. We further design two effective regularization terms for local training: 1) REG loss that regularizes the distribution of the client's latent embedding with the anchors and 2) KD loss that enables clients to learn from others. Through extensive experiments on diverse client data distributions, we showcase the effectiveness of DeSA in enhancing both inter- and intra-domain accuracy of each client. The implementation of DeSA can be found at: https://github.com/ubc-tea/DESA",
    "checked": true,
    "id": "dcfa0ddddb232a68bab27e505a3344ca86eb1f73",
    "semantic_title": "overcoming data and model heterogeneities in decentralized federated learning via synthetic anchors",
    "citation_count": 0,
    "authors": [
      "Chun-Yin Huang",
      "Kartik Srinivas",
      "Xin Zhang",
      "Xiaoxiao Li"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24w.html": {
    "title": "Quasi-Monte Carlo Features for Kernel Approximation",
    "volume": "main",
    "abstract": "Random features (Rahimi & Recht, 2007), based on Monte Carlo (MC) method, is one of the most popular approximation techniques to accelerate kernel methods. We show for a class of kernels, including Gaussian kernels, quasi-Monte Carlo (QMC) methods can be used in place of MC to improve the approximation error from $O_P(1/\\sqrt{M})$ to $O(1/M)$ (up to logarithmic factors), for estimating both the kernel function itself and the associated integral operator, where $M$ is the number of features being used. Furthermore, we demonstrate the advantage of QMC features in the case of kernel ridge regression, where theoretically, fewer random features suffice to guarantee the same convergence rate of the excess risk. In practice, the QMC kernel approximation approach is easily implementable and shows superior performance, as supported by the empirical evidence provided in the paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Huang",
      "Jiajin Sun",
      "Yian Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24x.html": {
    "title": "Position: TrustLLM: Trustworthiness in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and capability (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones, suggesting that open-source models can achieve high levels of trustworthiness without additional mechanisms like moderator, offering valuable insights for developers in this field. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Besides these observations, we've uncovered key insights into the multifaceted trustworthiness in LLMs. We emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. We advocate that the establishment of an AI alliance between industry, academia, the open-source community to foster collaboration is imperative to advance the trustworthiness of LLMs",
    "checked": true,
    "id": "d8aeef8fcaad91af51b4a2b40b4852de6a0c1198",
    "semantic_title": "position: trustllm: trustworthiness in large language models",
    "citation_count": 5,
    "authors": [
      "Yue Huang",
      "Lichao Sun",
      "Haoran Wang",
      "Siyuan Wu",
      "Qihui Zhang",
      "Yuan Li",
      "Chujie Gao",
      "Yixin Huang",
      "Wenhan Lyu",
      "Yixuan Zhang",
      "Xiner Li",
      "Hanchi Sun",
      "Zhengliang Liu",
      "Yixin Liu",
      "Yijue Wang",
      "Zhikun Zhang",
      "Bertie Vidgen",
      "Bhavya Kailkhura",
      "Caiming Xiong",
      "Chaowei Xiao",
      "Chunyuan Li",
      "Eric P. Xing",
      "Furong Huang",
      "Hao Liu",
      "Heng Ji",
      "Hongyi Wang",
      "Huan Zhang",
      "Huaxiu Yao",
      "Manolis Kellis",
      "Marinka Zitnik",
      "Meng Jiang",
      "Mohit Bansal",
      "James Zou",
      "Jian Pei",
      "Jian Liu",
      "Jianfeng Gao",
      "Jiawei Han",
      "Jieyu Zhao",
      "Jiliang Tang",
      "Jindong Wang",
      "Joaquin Vanschoren",
      "John Mitchell",
      "Kai Shu",
      "Kaidi Xu",
      "Kai-Wei Chang",
      "Lifang He",
      "Lifu Huang",
      "Michael Backes",
      "Neil Zhenqiang Gong",
      "Philip S. Yu",
      "Pin-Yu Chen",
      "Quanquan Gu",
      "Ran Xu",
      "Rex Ying",
      "Shuiwang Ji",
      "Suman Jana",
      "Tianlong Chen",
      "Tianming Liu",
      "Tianyi Zhou",
      "William Yang Wang",
      "Xiang Li",
      "Xiangliang Zhang",
      "Xiao Wang",
      "Xing Xie",
      "Xun Chen",
      "Xuyu Wang",
      "Yan Liu",
      "Yanfang Ye",
      "Yinzhi Cao",
      "Yong Chen",
      "Yue Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24y.html": {
    "title": "MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation",
    "volume": "main",
    "abstract": "A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with 37.5% average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from 100% on well-established older datasets to as low as 0% on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Huang",
      "Jian Vora",
      "Percy Liang",
      "Jure Leskovec"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24z.html": {
    "title": "How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing",
    "volume": "main",
    "abstract": "Spectral Graph Neural Networks (GNNs), alternatively known as graph filters, have gained increasing prevalence for heterophily graphs. Optimal graph filters rely on Laplacian eigendecomposition for Fourier transform. In an attempt to avert prohibitive computations, numerous polynomial filters have been proposed. However, polynomials in the majority of these filters are predefined and remain fixed across different graphs, failing to accommodate the varying degrees of heterophily. Addressing this gap, we demystify the intrinsic correlation between the spectral property of desired polynomial bases and the heterophily degrees via thorough theoretical analyses. Subsequently, we develop a novel adaptive heterophily basis wherein the basis vectors mutually form angles reflecting the heterophily degree of the graph. We integrate this heterophily basis with the homophily basis to construct a universal polynomial basis UniBasis, which devises a polynomial filter based graph neural network – UniFilter. It optimizes the convolution and propagation in GNN, thus effectively limiting over-smoothing and alleviating over-squashing. Our extensive experiments, conducted on datasets with a diverse range of heterophily, support the superiority of UniBasis in the universality but also its proficiency in graph explanation",
    "checked": true,
    "id": "f5d41be213ca353367e6c46b693cb5a8c4af8e8e",
    "semantic_title": "how universal polynomial bases enhance spectral graph neural networks: heterophily, over-smoothing, and over-squashing",
    "citation_count": 4,
    "authors": [
      "Keke Huang",
      "Yu Guang Wang",
      "Ming Li",
      "Pietro Lio"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24aa.html": {
    "title": "Conformal Prediction for Deep Classifier via Label Ranking",
    "volume": "main",
    "abstract": "Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. To address this issue, we propose a novel algorithm named $\\textit{Sorted Adaptive Prediction Sets}$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce compact prediction sets and communicate instance-wise uncertainty. Extensive experiments validate that SAPS not only lessens the prediction sets but also broadly enhances the conditional coverage rate of prediction sets",
    "checked": true,
    "id": "bcc0b29dee28b36162910121934952025f0dc6ef",
    "semantic_title": "conformal prediction for deep classifier via label ranking",
    "citation_count": 16,
    "authors": [
      "Jianguo Huang",
      "Huajun Xi",
      "Linjun Zhang",
      "Huaxiu Yao",
      "Yue Qiu",
      "Hongxin Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24ab.html": {
    "title": "Interaction-based Retrieval-augmented Diffusion Models for Protein-specific 3D Molecule Generation",
    "volume": "main",
    "abstract": "Generating ligand molecules that bind to specific protein targets via generative models holds substantial promise for advancing structure-based drug design. Existing methods generate molecules from scratch without reference or template ligands, which poses challenges in model optimization and may yield suboptimal outcomes. To address this problem, we propose an innovative interaction-based retrieval-augmented diffusion model named IRDiff to facilitate target-aware molecule generation. IRDiff leverages a curated set of ligand references, i.e., those with desired properties such as high binding affinity, to steer the diffusion model towards synthesizing ligands that satisfy design criteria. Specifically, we utilize a protein-molecule interaction network (PMINet), which is pretrained with binding affinity signals to: (i) retrieve target-aware ligand molecules with high binding affinity to serve as references, and (ii) incorporate essential protein-ligand binding structures for steering molecular diffusion generation with two effective augmentation mechanisms, i.e., retrieval augmentation and self augmentation. Empirical studies on CrossDocked2020 dataset show IRDiff can generate molecules with more realistic 3D structures and achieve state-of-the-art binding affinities towards the protein targets, while maintaining proper molecular properties. The codes and models are available at https://github.com/YangLing0818/IRDiff",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilin Huang",
      "Ling Yang",
      "Xiangxin Zhou",
      "Chujun Qin",
      "Yijie Yu",
      "Xiawu Zheng",
      "Zikun Zhou",
      "Wentao Zhang",
      "Yu Wang",
      "Wenming Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24ac.html": {
    "title": "Enhancing Size Generalization in Graph Neural Networks through Disentangled Representation Learning",
    "volume": "main",
    "abstract": "Although most graph neural networks (GNNs) can operate on graphs of any size, their classification performance often declines on graphs larger than those encountered during training. Existing methods insufficiently address the removal of size information from graph representations, resulting in sub-optimal performance and reliance on backbone models. In response, we propose DISGEN, a novel and model-agnostic framework designed to disentangle size factors from graph representations. DISGEN employs size- and task-invariant augmentations and introduces a decoupling loss that minimizes shared information in hidden representations, with theoretical guarantees for its effectiveness. Our empirical results show that DISGEN outperforms the state-of-the-art models by up to 6% on real-world datasets, underscoring its effectiveness in enhancing the size generalizability of GNNs. Our codes are available at: https://github.com/GraphmindDartmouth/DISGEN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Huang",
      "Qihui Yang",
      "Dawei Zhou",
      "Yujun Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24ad.html": {
    "title": "Triadic-OCD: Asynchronous Online Change Detection with Provable Robustness, Optimality, and Convergence",
    "volume": "main",
    "abstract": "The primary goal of online change detection (OCD) is to promptly identify changes in the data stream. OCD problem find a wide variety of applications in diverse areas, e.g., security detection in smart grids and intrusion detection in communication networks. Prior research usually assumes precise knowledge of the system parameters. Nevertheless, this presumption often proves unattainable in practical scenarios due to factors such as estimation errors, system updates, etc. This paper aims to take the first attempt to develop a triadic-OCD framework with certifiable robustness, provable optimality, and guaranteed convergence. In addition, the proposed triadic-OCD algorithm can be realized in a fully asynchronous distributed manner, easing the necessity of transmitting the data to a single server. This asynchronous mechanism could also mitigate the straggler issue that faced by traditional synchronous algorithm. Moreover, the non-asymptotic convergence property of Triadic-OCD is theoretically analyzed, and its iteration complexity to achieve an $\\epsilon$-optimal point is derived. Extensive experiments have been conducted to elucidate the effectiveness of the proposed method",
    "checked": true,
    "id": "dd92c005ae7625ab179e51183341b231282ae9b3",
    "semantic_title": "triadic-ocd: asynchronous online change detection with provable robustness, optimality, and convergence",
    "citation_count": 0,
    "authors": [
      "Yancheng Huang",
      "Kai Yang",
      "Zelin Zhu",
      "Leian Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24ae.html": {
    "title": "An Embodied Generalist Agent in 3D World",
    "volume": "main",
    "abstract": "Leveraging massive knowledge from large language models (LLMs), recent machine learning models show notable successes in general-purpose task solving in diverse domains such as computer vision and robotics. However, several significant challenges remain: (i) most of these models rely on 2D images yet exhibit a limited capacity for 3D input; (ii) these models rarely explore the tasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning and acting. We argue these limitations significantly hinder current models from performing real-world tasks and approaching general intelligence. To this end, we introduce LEO, an embodied multi-modal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is trained with a unified task interface, model architecture, and objective in two stages: (i) 3D vision-language (VL) alignment and (ii) 3D vision-language-action (VLA) instruction tuning. We collect large-scale datasets comprising diverse object-level and scene-level tasks, which require considerable understanding of and interaction with the 3D world. Moreover, we meticulously design an LLM-assisted pipeline to produce high-quality 3D VL data. Through extensive experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation. Our ablative studies and scaling analyses further provide valuable insights for developing future embodied generalist agents. Code and data are available on project page",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangyong Huang",
      "Silong Yong",
      "Xiaojian Ma",
      "Xiongkun Linghu",
      "Puhao Li",
      "Yan Wang",
      "Qing Li",
      "Song-Chun Zhu",
      "Baoxiong Jia",
      "Siyuan Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24af.html": {
    "title": "InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Semi-supervised learning (SSL) seeks to enhance task performance by training on both labeled and unlabeled data. Mainstream SSL image classification methods mostly optimize a loss that additively combines a supervised classification objective with a regularization term derived solely from unlabeled data. This formulation often neglects the potential for interaction between labeled and unlabeled images. In this paper, we introduce InterLUDE, a new approach to enhance SSL made of two parts that each benefit from labeled-unlabeled interaction. The first part, embedding fusion, interpolates between labeled and unlabeled embeddings to improve representation learning. The second part is a new loss, grounded in the principle of consistency regularization, that aims to minimize discrepancies in the model's predictions between labeled versus unlabeled inputs. Experiments on standard closed-set SSL benchmarks and a medical SSL task with an uncurated unlabeled set show clear benefits to our approach. On the STL-10 dataset with only 40 labels, InterLUDE achieves 3.2% error rate, while the best previous method reports 6.3%",
    "checked": false,
    "id": "932602df44b8274ad1ed9be8c2a1e226e35c91c3",
    "semantic_title": "mutual- and self- prototype alignment for semi-supervised medical image segmentation",
    "citation_count": 2,
    "authors": [
      "Zhe Huang",
      "Xiaowei Yu",
      "Dajiang Zhu",
      "Michael C Hughes"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24ag.html": {
    "title": "Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge",
    "volume": "main",
    "abstract": "Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock demonstrate our model's superior effectiveness and efficiency over current methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Huang",
      "Odin Zhang",
      "Lirong Wu",
      "Cheng Tan",
      "Haitao Lin",
      "Zhangyang Gao",
      "Siyuan Li",
      "Stan Z. Li"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24ah.html": {
    "title": "Faster Adaptive Decentralized Learning Algorithms",
    "volume": "main",
    "abstract": "Decentralized learning recently has received increasing attention in machine learning due to its advantages in implementation simplicity and system robustness, data privacy. Meanwhile, the adaptive gradient methods show superior performances in many machine learning tasks such as training neural networks. Although some works focus on studying decentralized optimization algorithms with adaptive learning rates, these adaptive decentralized algorithms still suffer from high sample complexity. To fill these gaps, we propose a class of faster adaptive decentralized algorithms (i.e., AdaMDOS and AdaMDOF) for distributed nonconvex stochastic and finite-sum optimization, respectively. Moreover, we provide a solid convergence analysis framework for our methods. In particular, we prove that our AdaMDOS obtains a near-optimal sample complexity of $\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary solution of nonconvex stochastic optimization. Meanwhile, our AdaMDOF obtains a near-optimal sample complexity of $O(\\sqrt{n}\\epsilon^{-2})$ for finding an $\\epsilon$-stationary solution of for nonconvex finite-sum optimization, where $n$ denotes the sample size. To the best of our knowledge, our AdaMDOF algorithm is the first adaptive decentralized algorithm for nonconvex finite-sum optimization. Some experimental results demonstrate efficiency of our algorithms",
    "checked": true,
    "id": "29cc62c0fd916317d2087b8c41da066f70d0f26e",
    "semantic_title": "faster adaptive decentralized learning algorithms",
    "citation_count": 0,
    "authors": [
      "Feihu Huang",
      "Jianyu Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24ai.html": {
    "title": "Adversarially Robust Deep Multi-View Clustering: A Novel Attack and Defense Framework",
    "volume": "main",
    "abstract": "Deep Multi-view Clustering (DMVC) stands out as a widely adopted technique aiming at enhanced clustering performance by leveraging diverse data sources. However, the critical issue of vulnerability to adversarial attacks is unexplored due to the lack of well-defined attack objectives. To fill this crucial gap, this paper is the first work to investigate the possibility of adversarial attacks on DMVC models. Specifically, we introduce an adversarial attack with Generative Adversarial Networks (GANs) with the aim to maximally change the complementarity and consistency of multiple views, thus leading to wrong clustering. Building upon this adversarial context, in the realm of defense, we propose a novel Adversarially Robust Deep Multi-View Clustering by leveraging adversarial training. Based on the analysis from an information-theoretic perspective, we design an Attack Mitigator that provides a foundation to guarantee the adversarial robustness of our DMVC models. Experiments conducted on multi-view datasets confirmed that our attack framework effectively reduces the clustering performance of the target model. Furthermore, our proposed adversarially robust method is also demonstrated to be an effective defense against such attacks. This work is a pioneer in exploring adversarial threats and advancing both theoretical understanding and practical strategies for robust multi-view clustering. Code is available at https://github.com/libertyhhn/AR-DMVC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Huang",
      "Guoxu Zhou",
      "Yanghang Zheng",
      "Yuning Qiu",
      "Andong Wang",
      "Qibin Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/huang24aj.html": {
    "title": "Faster Sampling via Stochastic Gradient Proximal Sampler",
    "volume": "main",
    "abstract": "Stochastic gradients have been widely integrated into Langevin-based methods to improve their scalability and efficiency in solving large-scale sampling problems. However, the proximal sampler, which exhibits much faster convergence than Langevin-based algorithms in the deterministic setting (Lee et al., 2021), has yet to be explored in its stochastic variants. In this paper, we study the Stochastic Proximal Samplers (SPS) for sampling from non-log-concave distributions. We first establish a general framework for implementing stochastic proximal samplers and establish the convergence theory accordingly. We show that the convergence to the target distribution can be guaranteed as long as the second moment of the algorithm trajectory is bounded and restricted Gaussian oracles can be well approximated. We then provide two implementable variants based on Stochastic gradient Langevin dynamics (SGLD) and Metropolis-adjusted Langevin algorithm (MALA), giving rise to SPS-SGLD and SPS-MALA. We further show that SPS-SGLD and SPS-MALA can achieve $\\epsilon$-sampling error in total variation (TV) distance within $\\tilde{\\mathcal{O}}(d\\epsilon^{-2})$ and $\\tilde{\\mathcal{O}}(d^{1/2}\\epsilon^{-2})$ gradient complexities, which outperform the best-known result by at least an $\\tilde{\\mathcal{O}}(d^{1/3})$ factor. This enhancement in performance is corroborated by our empirical studies on synthetic data with various dimensions, demonstrating the efficiency of our proposed algorithm",
    "checked": true,
    "id": "89755a94948ec7e320ba8bd0ca3056c1f0534742",
    "semantic_title": "faster sampling via stochastic gradient proximal sampler",
    "citation_count": 0,
    "authors": [
      "Xunpeng Huang",
      "Difan Zou",
      "Hanze Dong",
      "Yian Ma",
      "Tong Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/hughes24a.html": {
    "title": "Position: Open-Endedness is Essential for Artificial Superhuman Intelligence",
    "volume": "main",
    "abstract": "In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internet-scale data. Nevertheless, the creation of open-ended, ever self-improving AI remains elusive. In this position paper, we argue that the ingredients are now in place to achieve open-endedness in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI). We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, human-relevant discoveries. We conclude by examining the safety implications of generally-capable open-ended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future",
    "checked": false,
    "id": "e76b4e3dcd69220ee11e40ebcb6357e5088f04a8",
    "semantic_title": "open-endedness is essential for artificial superhuman intelligence",
    "citation_count": 3,
    "authors": [
      "Edward Hughes",
      "Michael D Dennis",
      "Jack Parker-Holder",
      "Feryal Behbahani",
      "Aditi Mavalankar",
      "Yuge Shi",
      "Tom Schaul",
      "Tim Rocktäschel"
    ]
  },
  "https://proceedings.mlr.press/v235/huh24a.html": {
    "title": "Position: The Platonic Representation Hypothesis",
    "volume": "main",
    "abstract": "We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minyoung Huh",
      "Brian Cheung",
      "Tongzhou Wang",
      "Phillip Isola"
    ]
  },
  "https://proceedings.mlr.press/v235/huh24b.html": {
    "title": "Nash Incentive-compatible Online Mechanism Learning via Weakly Differentially Private Online Learning",
    "volume": "main",
    "abstract": "We study a multi-round mechanism design problem, where we interact with a set of agents over a sequence of rounds. We wish to design an incentive-compatible (IC) online learning scheme to maximize an application-specific objective within a given class of mechanisms, without prior knowledge of the agents' type distributions. Even if each mechanism in this class is IC in a single round, if an algorithm naively chooses from this class on each round, the entire learning process may not be IC against non-myopic buyers who appear over multiple rounds. On each round, our method randomly chooses between the recommendation of a weakly differentially private online learning algorithm (e.g., Hedge), and a commitment mechanism which penalizes non-truthful behavior. Our method is IC and achieves $O(T^{\\frac{1+h}{2}})$ regret for the application-specific objective in an adversarial setting, where $h$ quantifies the long-sightedness of the agents. When compared to prior work, our approach is conceptually simpler, it applies to general mechanism design problems (beyond auctions), and its regret scales gracefully with the size of the mechanism class",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joon Suk Huh",
      "Kirthevasan Kandasamy"
    ]
  },
  "https://proceedings.mlr.press/v235/hui24a.html": {
    "title": "Make-A-Shape: a Ten-Million-scale 3D Shape Model",
    "volume": "main",
    "abstract": "The progression in large-scale 3D generative models has been impeded by significant resource requirements for training and challenges like inefficient representations. This paper introduces Make-A-Shape, a novel 3D generative model trained on a vast scale, using 10 million publicly-available shapes. We first innovate the wavelet-tree representation to encode high-resolution SDF shapes with minimal loss, leveraging our newly-proposed subband coefficient filtering scheme. We then design a subband coefficient packing scheme to facilitate diffusion-based generation and a subband adaptive training strategy for effective training on the large-scale dataset. Our generative framework is versatile, capable of conditioning on various input modalities such as images, point clouds, and voxels, enabling a variety of downstream applications, e.g., unconditional generation, completion, and conditional generation. Our approach clearly surpasses the existing baselines in delivering high-quality results and can efficiently generate shapes within two seconds for most conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ka-Hei Hui",
      "Aditya Sanghi",
      "Arianna Rampini",
      "Kamal Rahimi Malekshan",
      "Zhengzhe Liu",
      "Hooman Shayani",
      "Chi-Wing Fu"
    ]
  },
  "https://proceedings.mlr.press/v235/huijben24a.html": {
    "title": "Residual Quantization with Implicit Neural Codebooks",
    "volume": "main",
    "abstract": "Vector quantization is a fundamental operation for data compression and vector search. To obtain high accuracy, multi-codebook methods represent each vector using codewords across several codebooks. Residual quantization (RQ) is one such method, which iteratively quantizes the error of the previous step. While the error distribution is dependent on previously-selected codewords, this dependency is not accounted for in conventional RQ as it uses a fixed codebook per quantization step. In this paper, we propose QINCo, a neural RQ variant that constructs specialized codebooks per step that depend on the approximation of the vector from previous steps. Experiments show that QINCo outperforms state-of-the-art methods by a large margin on several datasets and code sizes. For example, QINCo achieves better nearest-neighbor search accuracy using 12-byte codes than the state-of-the-art UNQ using 16 bytes on the BigANN1M and Deep1M datasets",
    "checked": true,
    "id": "0c08725a79cb5dbb1354793a287810646dbefe52",
    "semantic_title": "residual quantization with implicit neural codebooks",
    "citation_count": 4,
    "authors": [
      "Iris A.M. Huijben",
      "Matthijs Douze",
      "Matthew J. Muckley",
      "Ruud Van Sloun",
      "Jakob Verbeek"
    ]
  },
  "https://proceedings.mlr.press/v235/huix24a.html": {
    "title": "Theoretical Guarantees for Variational Inference with Fixed-Variance Mixture of Gaussians",
    "volume": "main",
    "abstract": "Variational inference (VI) is a popular approach in Bayesian inference, that looks for the best approximation of the posterior distribution within a parametric family, minimizing a loss that is (typically) the reverse Kullback-Leibler (KL) divergence. Despite its empirical success, the theoretical properties of VI have only recently received attention, and is restricted to the Gaussian case. This research paper aims to contribute to the theoretical study of VI in the non-Gaussian case by investigating the setting of Mixture of Gaussians with fixed covariance. In this view, VI over this specific family can be casted as the minimization of a Mollified relative entropy, i.e. the KL between the convolution (with respect to a Gaussian kernel) of an atomic measure supported on Diracs, where the support of the atomic measure correspond to the localization of the Gaussian components, and the target distribution. Hence, solving variational inference is equivalent to optimizing the positions of the Diracs (the particles), which can be done through gradient descent and takes the form of an interacting particle system. We study two sources of error in variational inference in this context. The first is an optimization result that is a descent lemma establishing that the algorithm decreases the objective at each iteration. The second is an approximation error that upper bounds the mollified relative entropy between an optimal finite mixture and the target distribution",
    "checked": true,
    "id": "375d53a1848ebc27a885328205277dfc3167d814",
    "semantic_title": "theoretical guarantees for variational inference with fixed-variance mixture of gaussians",
    "citation_count": 5,
    "authors": [
      "Tom Huix",
      "Anna Korba",
      "Alain Oliviero Durmus",
      "Eric Moulines"
    ]
  },
  "https://proceedings.mlr.press/v235/humayun24a.html": {
    "title": "Deep Networks Always Grok and Here is Why",
    "volume": "main",
    "abstract": "Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on the local complexity of a DNN's input-output mapping. Our local complexity measures the density of so-called \"linear regions'' (aka, spline partition regions) that tile the DNN input space and serves as a utile progress measure for training. We provide the first evidence that, for classification problems, the linear regions undergo a phase transition during training whereafter they migrate away from the training samples (making the DNN mapping smoother there) and towards the decision boundary (making the DNN mapping less smooth there). Grokking occurs post phase transition as a robust partition of the input space thanks to the linearization of the DNN mapping around the training points. Web: https://bit.ly/grok-adversarial",
    "checked": true,
    "id": "69d15a3ec038a9396181d2f813ec6da2018e4d40",
    "semantic_title": "deep networks always grok and here is why",
    "citation_count": 9,
    "authors": [
      "Ahmed Imtiaz Humayun",
      "Randall Balestriero",
      "Richard Baraniuk"
    ]
  },
  "https://proceedings.mlr.press/v235/huo24a.html": {
    "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
    "volume": "main",
    "abstract": "Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark",
    "checked": true,
    "id": "88ee6398a970acbb1cab592cbd07c944d8b59b4e",
    "semantic_title": "token-specific watermarking with enhanced detectability and semantic coherence for large language models",
    "citation_count": 8,
    "authors": [
      "Mingjia Huo",
      "Sai Ashish Somayajula",
      "Youwei Liang",
      "Ruisi Zhang",
      "Farinaz Koushanfar",
      "Pengtao Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/hussain24a.html": {
    "title": "Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers",
    "volume": "main",
    "abstract": "Graph transformers typically lack third-order interactions, limiting their geometric understanding which is crucial for tasks like molecular geometry prediction. We propose the Triplet Graph Transformer (TGT) that enables direct communication between pairs within a 3-tuple of nodes via novel triplet attention and aggregation mechanisms. TGT is applied to molecular property prediction by first predicting interatomic distances from 2D graphs and then using these distances for downstream tasks. A novel three-stage training procedure and stochastic inference further improve training efficiency and model performance. Our model achieves new state-of-the-art (SOTA) results on open challenge benchmarks PCQM4Mv2 and OC20 IS2RE. We also obtain SOTA results on QM9, MOLPCBA, and LIT-PCBA molecular property prediction benchmarks via transfer learning. We also demonstrate the generality of TGT with SOTA results on the traveling salesman problem (TSP)",
    "checked": true,
    "id": "d85316b6eb3bff2f47f3d0382de7aee3929a55c9",
    "semantic_title": "triplet interaction improves graph transformers: accurate molecular graph learning with triplet graph transformers",
    "citation_count": 0,
    "authors": [
      "Md Shamim Hussain",
      "Mohammed J Zaki",
      "Dharmashankar Subramanian"
    ]
  },
  "https://proceedings.mlr.press/v235/hvarfner24a.html": {
    "title": "Vanilla Bayesian Optimization Performs Great in High Dimensions",
    "volume": "main",
    "abstract": "High-dimensional optimization problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make BO more performant in this setting, commonly by imposing various simplifying assumptions on the objective, thereby decreasing its presumed complexity. In this paper, we identify the degeneracies that make vanilla BO poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of model complexity. Motivated by the model complexity measure, we derive an enhancement to the prior assumptions that are typical of the vanilla BO algorithm, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior in the dimensionality - reveals that standard BO works drastically better than previously thought in high dimensions. Our insights are supplemented by substantial out-performance of existing state-of-the-art on multiple commonly considered real-world high-dimensional tasks",
    "checked": true,
    "id": "7dd070c7138053f52e41536b3d6ad2b3bd7a3533",
    "semantic_title": "vanilla bayesian optimization performs great in high dimensions",
    "citation_count": 6,
    "authors": [
      "Carl Hvarfner",
      "Erik Orm Hellsten",
      "Luigi Nardi"
    ]
  },
  "https://proceedings.mlr.press/v235/hwang24a.html": {
    "title": "On Positivity Condition for Causal Inference",
    "volume": "main",
    "abstract": "Identifying and estimating a causal effect is a fundamental task when researchers want to infer a causal effect using an observational study without experiments. A conventional assumption is the strict positivity of the given distribution, or so called positivity (or overlap) under the unconfounded assumption that the probabilities of treatments are positive. However, there exist many environments where neither observational data exhibits strict positivity nor unconfounded assumption holds. Against this background, we examine the graphical counterpart of the conventional positivity condition so as to license the use of identification formula without strict positivity. In particular, we explore various approaches, including analysis in a post-hoc manner, do-calculus, $Q$-decomposition, and algorithmic, to yielding a positivity condition for an identification formula, where we relate them, providing a comprehensive view. We further discuss the design of a positivity-aware identification algorithm based on the theoretical characterization of identification formulas",
    "checked": true,
    "id": "7fdf4249440440a55ebeaf3f97df6b2477f9521c",
    "semantic_title": "on positivity condition for causal inference",
    "citation_count": 1,
    "authors": [
      "Inwoo Hwang",
      "Yesong Choe",
      "Yeahoon Kwon",
      "Sanghack Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/hwang24b.html": {
    "title": "Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning",
    "volume": "main",
    "abstract": "Causal dynamics learning has recently emerged as a promising approach to enhancing robustness in reinforcement learning (RL). Typically, the goal is to build a dynamics model that makes predictions based on the causal relationships among the entities. Despite the fact that causal connections often manifest only under certain contexts, existing approaches overlook such fine-grained relationships and lack a detailed understanding of the dynamics. In this work, we propose a novel dynamics model that infers fine-grained causal structures and employs them for prediction, leading to improved robustness in RL. The key idea is to jointly learn the dynamics model with a discrete latent variable that quantizes the state-action space into subgroups. This leads to recognizing meaningful context that displays sparse dependencies, where causal structures are learned for each subgroup throughout the training. Experimental results demonstrate the robustness of our method to unseen states and locally spurious correlations in downstream tasks where fine-grained causal reasoning is crucial. We further illustrate the effectiveness of our subgroup-based approach with quantization in discovering fine-grained causal relationships compared to prior methods",
    "checked": true,
    "id": "bebadf2e75bb9d79d488bbf2fb75364e4b8c0016",
    "semantic_title": "fine-grained causal dynamics learning with quantization for improving robustness in reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Inwoo Hwang",
      "Yunhyeok Kwak",
      "Suhyung Choi",
      "Byoung-Tak Zhang",
      "Sanghack Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/hwang24c.html": {
    "title": "Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control",
    "volume": "main",
    "abstract": "Vision Transformers (ViT), when paired with large-scale pretraining, have shown remarkable performance across various computer vision tasks, primarily due to their weak inductive bias. However, while such weak inductive bias aids in pretraining scalability, this may hinder the effective adaptation of ViTs for visuo-motor control tasks as a result of the absence of control-centric inductive biases. Such absent inductive biases include spatial locality and translation equivariance bias which convolutions naturally offer. To this end, we introduce Convolution Injector (CoIn), an add-on module that injects convolutions which are rich in locality and equivariance biases into a pretrained ViT for effective adaptation in visuo-motor control. We evaluate CoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12 varied control tasks within three separate domains (Adroit, MetaWorld, DMC), and demonstrate that CoIn consistently enhances control task performance across all experimented environments and models, validating the effectiveness of providing pretrained ViTs with control-centric biases",
    "checked": true,
    "id": "1381b776a6a99fd9c13141d4116d44c40e919e64",
    "semantic_title": "adapting pretrained vits with convolution injector for visuo-motor control",
    "citation_count": 0,
    "authors": [
      "Dongyoon Hwang",
      "Byungkun Lee",
      "Hojoon Lee",
      "Hyunseung Kim",
      "Jaegul Choo"
    ]
  },
  "https://proceedings.mlr.press/v235/hwang24d.html": {
    "title": "EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens",
    "volume": "main",
    "abstract": "Masked Video Autoencoder (MVA) approaches have demonstrated their potential by significantly outperforming previous video representation learning methods. However, they waste an excessive amount of computations and memory in predicting uninformative tokens/frames due to random masking strategies. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose EVEREST, a surprisingly efficient MVA approach for video representation learning that finds tokens containing rich motion features and discards uninformative ones during both pre-training and fine-tuning. We further present an information-intensive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces the computation and memory requirements of MVA, enabling the pre-training and fine-tuning on a single machine with 8 GPUs while achieving comparable performance to computation- and memory-heavy baselines on multiple benchmarks and the uncurated Ego4D dataset. We hope that our work contributes to reducing the barrier to further research on video understanding",
    "checked": true,
    "id": "8258c3ea1755893bce347e8d06983c4f24dd48fd",
    "semantic_title": "everest: efficient masked video autoencoder by removing redundant spatiotemporal tokens",
    "citation_count": 1,
    "authors": [
      "Sunil Hwang",
      "Jaehong Yoon",
      "Youngwan Lee",
      "Sung Ju Hwang"
    ]
  },
  "https://proceedings.mlr.press/v235/igel24a.html": {
    "title": "Smooth Min-Max Monotonic Networks",
    "volume": "main",
    "abstract": "Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer-aided decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of partial derivatives being zero when computing extrema. We propose a simple modification of the MM network using strictly-increasing smooth minimum and maximum functions that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is conceptually simple and computationally less demanding than state-of-the-art neural networks for monotonic modelling. Our experiments show that this does not come with a loss in generalization performance compared to alternative neural and non-neural approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Igel"
    ]
  },
  "https://proceedings.mlr.press/v235/ilbert24a.html": {
    "title": "SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention",
    "volume": "main",
    "abstract": "Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses current state-of-the-art methods and is on par with the biggest foundation model MOIRAI while having significantly fewer parameters. The code is available at https://github.com/romilbert/samformer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Ilbert",
      "Ambroise Odonnat",
      "Vasilii Feofanov",
      "Aladin Virmaux",
      "Giuseppe Paolo",
      "Themis Palpanas",
      "Ievgen Redko"
    ]
  },
  "https://proceedings.mlr.press/v235/ildiz24a.html": {
    "title": "From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers",
    "volume": "main",
    "abstract": "Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and the associated outputs sampled from the model. We first establish a formal link between the self-attention mechanism and Markov models under suitable conditions: Inputting a prompt to the self-attention model samples the output token according to a context-conditioned Markov chain (CCMC). CCMC is obtained by weighing the transition matrix of a standard Markov chain according to the sufficient statistics of the prompt/context. Building on this formalism, we develop identifiability/coverage conditions for the data distribution that guarantee consistent estimation of the latent model under a teacher-student setting and establish sample complexity guarantees under IID data. Finally, we study the problem of learning from a single output trajectory generated in response to an initial prompt. We characterize a winner-takes-all phenomenon where the generative process of self-attention evolves to sampling from a small set of winner tokens that dominate the context window. This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text",
    "checked": true,
    "id": "04a75094f7acbd1243b69d78af8fcc8608011b2d",
    "semantic_title": "from self-attention to markov models: unveiling the dynamics of generative transformers",
    "citation_count": 10,
    "authors": [
      "Muhammed Emrullah Ildiz",
      "Yixiao Huang",
      "Yingcong Li",
      "Ankit Singh Rawat",
      "Samet Oymak"
    ]
  },
  "https://proceedings.mlr.press/v235/im24a.html": {
    "title": "Understanding the Learning Dynamics of Alignment with Human Feedback",
    "volume": "main",
    "abstract": "Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potentially offensive text; reader discretion is advised",
    "checked": true,
    "id": "279dbec2194f05ee0e9d49818eff5fbe5f61b2ee",
    "semantic_title": "understanding the learning dynamics of alignment with human feedback",
    "citation_count": 9,
    "authors": [
      "Shawn Im",
      "Yixuan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/ingebrand24a.html": {
    "title": "Zero-Shot Reinforcement Learning via Function Encoders",
    "volume": "main",
    "abstract": "Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encoder task representation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Ingebrand",
      "Amy Zhang",
      "Ufuk Topcu"
    ]
  },
  "https://proceedings.mlr.press/v235/iollo24a.html": {
    "title": "PASOA- PArticle baSed Bayesian Optimal Adaptive design",
    "volume": "main",
    "abstract": "We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG). As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance. To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance. This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical experiments confirm the potential of the approach, which outperforms other recent existing procedures",
    "checked": true,
    "id": "eafb8b847917cfe10e6ed54c4151b33d27a57f27",
    "semantic_title": "pasoa- particle based bayesian optimal adaptive design",
    "citation_count": 1,
    "authors": [
      "Jacopo Iollo",
      "Christophe Heinkelé",
      "Pierre Alliez",
      "Florence Forbes"
    ]
  },
  "https://proceedings.mlr.press/v235/iqbal24a.html": {
    "title": "Nesting Particle Filters for Experimental Design in Dynamical Systems",
    "volume": "main",
    "abstract": "In this paper, we propose a novel approach to Bayesian experimental design for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential Monte Carlo technique to infer optimal designs, and embed it into a particle Markov chain Monte Carlo framework to perform gradient-based policy amortization. Our approach is distinct from other amortized experimental design techniques, as it does not rely on contrastive estimators. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies",
    "checked": true,
    "id": "d298b22971695dcd64c551e13cfd7e0bad4dfde1",
    "semantic_title": "nesting particle filters for experimental design in dynamical systems",
    "citation_count": 2,
    "authors": [
      "Sahel Iqbal",
      "Adrien Corenflos",
      "Simo Särkkä",
      "Hany Abdulsamad"
    ]
  },
  "https://proceedings.mlr.press/v235/j-thiagarajan24a.html": {
    "title": "PAGER: Accurate Failure Characterization in Deep Regression Models",
    "volume": "main",
    "abstract": "Safe deployment of AI models requires proactive detection of failures to prevent costly errors. To this end, we study the important problem of detecting failures in deep regression models. Existing approaches rely on epistemic uncertainty estimates or inconsistency w.r.t the training data to identify failure. Interestingly, we find that while uncertainties are necessary they are insufficient to accurately characterize failure in practice. Hence, we introduce PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regressors. Built upon the principle of anchored training in deep models, PAGER unifies both epistemic uncertainty and complementary manifold non-conformity scores to accurately organize samples into different risk regimes",
    "checked": true,
    "id": "67a562700c734259e6e8f34ec36620ef29fbc0f8",
    "semantic_title": "pager: accurate failure characterization in deep regression models",
    "citation_count": 2,
    "authors": [
      "Jayaraman J. Thiagarajan",
      "Vivek Narayanaswamy",
      "Puja Trivedi",
      "Rushil Anirudh"
    ]
  },
  "https://proceedings.mlr.press/v235/jacobsen24a.html": {
    "title": "Online Linear Regression in Dynamic Environments via Discounting",
    "volume": "main",
    "abstract": "We develop algorithms for online linear regression which achieve optimal static and dynamic regret guarantees even in the complete absence of prior knowledge. We present a novel analysis showing that a discounted variant of the Vovk-Azoury-Warmuth forecaster achieves dynamic regret of the form $R_{T}(\\vec{u})\\le O\\Big(d\\log(T)\\vee \\sqrt{dP_{T}^{\\gamma}(\\vec{u})T}\\Big)$, where $P_{T}^{\\gamma}(\\vec{u})$ is a measure of variability of the comparator sequence, and show that the discount factor achieving this result can be learned on-the-fly. We show that this result is optimal by providing a matching lower bound. We also extend our results to strongly-adaptive guarantees which hold over every sub-interval $[a,b]\\subseteq[1,T]$ simultaneously",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Jacobsen",
      "Ashok Cutkosky"
    ]
  },
  "https://proceedings.mlr.press/v235/jagadish24a.html": {
    "title": "Human-like Category Learning by Injecting Ecological Priors from Large Language Models into Neural Networks",
    "volume": "main",
    "abstract": "Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy for assigning categories with learning, and (3) it generalizes to unseen stimuli in a human-like way. Furthermore, we show that ERMI's ecologically valid priors allow it to achieve state-of-the-art performance on the OpenML-CC18 classification benchmark",
    "checked": true,
    "id": "c6f7a3fa3a336d638b0ca921e8c95dab0101d3c7",
    "semantic_title": "human-like category learning by injecting ecological priors from large language models into neural networks",
    "citation_count": 1,
    "authors": [
      "Akshay Kumar Jagadish",
      "Julian Coda-Forno",
      "Mirko Thalmann",
      "Eric Schulz",
      "Marcel Binz"
    ]
  },
  "https://proceedings.mlr.press/v235/jain24a.html": {
    "title": "Position: Scarce Resource Allocations That Rely On Machine Learning Should Be Randomized",
    "volume": "main",
    "abstract": "Contrary to traditional deterministic notions of algorithmic fairness, this paper argues that fairly allocating scarce resources using machine learning often requires randomness. We address why, when, and how to randomize by offering a set of stochastic procedures that more adequately account for all of the claims individuals have to allocations of social goods or opportunities and effectively balances their interests",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shomik Jain",
      "Kathleen Creel",
      "Ashia Camage Wilson"
    ]
  },
  "https://proceedings.mlr.press/v235/jain24b.html": {
    "title": "Learning to Reach Goals via Diffusion",
    "volume": "main",
    "abstract": "We present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of denoising diffusion models. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy to reverse these deviations, analogous to the score function. This approach, which we call Merlin, can reach specified goals from arbitrary initial states without learning a separate value function. In contrast to recent works utilizing diffusion models in offline RL, Merlin stands out as the first method to perform diffusion in the state space, requiring only one \"denoising\" iteration per environment step. We experimentally validate our approach in various offline goal-reaching tasks, demonstrating substantial performance enhancements compared to state-of-the-art methods while improving computational efficiency over other diffusion-based RL methods by an order of magnitude. Our results suggest that this perspective on diffusion for RL is a simple and scalable approach for sequential decision making",
    "checked": true,
    "id": "a0de92d6dc8e1dd457e2b30d2680a49d90b1dcb5",
    "semantic_title": "learning to reach goals via diffusion",
    "citation_count": 3,
    "authors": [
      "Vineet Jain",
      "Siamak Ravanbakhsh"
    ]
  },
  "https://proceedings.mlr.press/v235/jain24c.html": {
    "title": "R2E: Turning any Github Repository into a Programming Agent Environment",
    "volume": "main",
    "abstract": "While Large Language Models' (LLMs) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI coding agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R2E code is available at https://r2e.dev/",
    "checked": true,
    "id": "2860bfaa9fa1d249c24aaf0981ad61bdbcd9c544",
    "semantic_title": "r2e: turning any github repository into a programming agent environment",
    "citation_count": 9,
    "authors": [
      "Naman Jain",
      "Manish Shetty",
      "Tianjun Zhang",
      "King Han",
      "Koushik Sen",
      "Ion Stoica"
    ]
  },
  "https://proceedings.mlr.press/v235/jamshidi24a.html": {
    "title": "On the sample complexity of conditional independence testing with Von Mises estimator with application to causal discovery",
    "volume": "main",
    "abstract": "Motivated by conditional independence testing, an essential step in constraint-based causal discovery algorithms, we study the nonparametric Von Mises estimator for the entropy of multivariate distributions built on a kernel density estimator. We establish an exponential concentration inequality for this estimator. We design a test for conditional independence (CI) based on our estimator, called VM-CI, which achieves optimal parametric rates under smoothness assumptions. Leveraging the exponential concentration, we prove a tight upper bound for the overall error of VM-CI. This, in turn, allows us to characterize the sample complexity of any constraint-based causal discovery algorithm that uses VM-CI for CI tests. To the best of our knowledge, this is the first sample complexity guarantee for causal discovery for non-linear models and non-Gaussian continuous variables. Furthermore, we empirically show that VM-CI outperforms other popular CI tests in terms of either time, sample complexity, or both. This enhancement significantly improves the performance in structure learning as well",
    "checked": true,
    "id": "55dea92796b700210ad01e14a317329f67dc250e",
    "semantic_title": "on the sample complexity of conditional independence testing with von mises estimator with application to causal discovery",
    "citation_count": 0,
    "authors": [
      "Fateme Jamshidi",
      "Luca Ganassali",
      "Negar Kiyavash"
    ]
  },
  "https://proceedings.mlr.press/v235/jang24a.html": {
    "title": "Rethinking DP-SGD in Discrete Domain: Exploring Logistic Distribution in the Realm of signSGD",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have a risk of remembering sensitive data from their training datasets, inadvertently leading to substantial information leakage through privacy attacks like membership inference attacks. DP-SGD is a simple but effective defense method, incorporating Gaussian noise into gradient updates to safeguard sensitive information. With the prevalence of large neural networks, DP-signSGD, a variant of DP-SGD, has emerged, aiming to curtail memory usage while maintaining security. However, it is noteworthy that most DP-signSGD algorithms default to Gaussian noise, suitable only for DP-SGD, without scant discussion of its appropriateness for signSGD. Our study delves into an intriguing question: \"Can we find a more efficient substitute for Gaussian noise to secure privacy in DP-signSGD?\" We propose an answer with a Logistic mechanism, which conforms to signSGD principles and is interestingly evolved from an exponential mechanism. In this paper, we provide both theoretical and experimental evidence showing that our method surpasses DP-signSGD",
    "checked": true,
    "id": "b66e63b8a632029c237e726c51b7f2ec432c6d07",
    "semantic_title": "rethinking dp-sgd in discrete domain: exploring logistic distribution in the realm of signsgd",
    "citation_count": 0,
    "authors": [
      "Jonggyu Jang",
      "Seongjin Hwang",
      "Hyun Jong Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/jang24b.html": {
    "title": "Degeneration-free Policy Optimization: RL Fine-Tuning for Language Models without Degeneration",
    "volume": "main",
    "abstract": "As the pre-training objectives (e.g., next token prediction) of language models (LMs) are inherently not aligned with task scores, optimizing LMs to achieve higher downstream task scores is essential. One of the promising approaches is to fine-tune LMs through reinforcement learning (RL). However, conventional RL methods based on PPO and a penalty of KL divergence are vulnerable to text degeneration where LMs do not generate natural texts anymore after RL fine-tuning. To address this problem, we provide Degeneration-free Policy Optimization (DfPO) that can fine-tune LMs to generate texts that achieve improved downstream task scores, while preserving the ability to generate natural texts. To achieve this, we introduce KL-masking which masks out the actions that potentially cause deviation from the reference policy when its likelihood is increased or decreased. Then, we devise truncated advantage functions for separately performing likelihood maximization and minimization to improve the task performance. In the experiments, we provide the results of DfPO and baseline algorithms on various generative NLP tasks including text continuation, text detoxification, and commonsense generation. Our experiments demonstrate that DfPO successfully improves the downstream task scores while preserving the ability to generate natural texts, without requiring additional hyperparameter search",
    "checked": true,
    "id": "5ac7b57e45abae7e44d8dd7455d19b0875b33d99",
    "semantic_title": "degeneration-free policy optimization: rl fine-tuning for language models without degeneration",
    "citation_count": 0,
    "authors": [
      "Youngsoo Jang",
      "Geon-Hyeong Kim",
      "Byoungjip Kim",
      "Yu Jin Kim",
      "Honglak Lee",
      "Moontae Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/jang24c.html": {
    "title": "Visual Representation Learning with Stochastic Frame Prediction",
    "volume": "main",
    "abstract": "Self-supervised learning of image representations by predicting future frames is a promising direction but still remains a challenge. This is because of the under-determined nature of frame prediction; multiple potential futures can arise from a single current frame. To tackle this challenge, in this paper, we revisit the idea of stochastic video generation that learns to capture uncertainty in frame prediction and explore its effectiveness for representation learning. Specifically, we design a framework that trains a stochastic frame prediction model to learn temporal information between frames. Moreover, to learn dense information within each frame, we introduce an auxiliary masked image modeling objective along with a shared decoder architecture. We find this architecture allows for combining both objectives in a synergistic and compute-efficient manner. We demonstrate the effectiveness of our framework on a variety of tasks from video label propagation and vision-based robot learning domains, such as video segmentation, pose tracking, vision-based robotic locomotion, and manipulation tasks. Code is available on the project webpage: https://sites.google.com/view/2024rsp",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiwon Jang",
      "Dongyoung Kim",
      "Junsu Kim",
      "Jinwoo Shin",
      "Pieter Abbeel",
      "Younggyo Seo"
    ]
  },
  "https://proceedings.mlr.press/v235/jang24d.html": {
    "title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
    "volume": "main",
    "abstract": "Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with rank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uijeong Jang",
      "Jason D. Lee",
      "Ernest K. Ryu"
    ]
  },
  "https://proceedings.mlr.press/v235/jang24e.html": {
    "title": "Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits",
    "volume": "main",
    "abstract": "We study low-rank matrix trace regression and the related problem of low-rank matrix bandits. Assuming access to the distribution of the covariates, we propose a novel low-rank matrix estimation method called LowPopArt and provide its recovery guarantee that depends on a novel quantity denoted by $B(Q)$ that characterizes the hardness of the problem, where $Q$ is the covariance matrix of the measurement distribution. We show that our method can provide tighter recovery guarantees than classical nuclear norm penalized least squares (Koltchinskii et al., 2011) in several problems. To perform an efficient estimation with a limited number of measurements from an arbitrarily given measurement set $\\mathcal{A}$, we also propose a novel experimental design criterion that minimizes $B(Q)$ with computational efficiency. We leverage our novel estimator and design of experiments to derive two low-rank linear bandit algorithms for general arm sets that enjoy improved regret upper bounds. This improves over previous works on low-rank bandits, which make somewhat restrictive assumptions that the arm set is the unit ball or that an efficient exploration distribution is given. To our knowledge, our experimental design criterion is the first one tailored to low-rank matrix estimation beyond the naive reduction to linear regression, which can be of independent interest",
    "checked": true,
    "id": "e78ddab4520cb33ab7f6c130ded0fd560bcc6b48",
    "semantic_title": "efficient low-rank matrix estimation, experimental design, and arm-set-dependent low-rank bandits",
    "citation_count": 1,
    "authors": [
      "Kyoungseok Jang",
      "Chicheng Zhang",
      "Kwang-Sung Jun"
    ]
  },
  "https://proceedings.mlr.press/v235/jaquier24a.html": {
    "title": "Bringing Motion Taxonomies to Continuous Domains via GPLVM on Hyperbolic manifolds",
    "volume": "main",
    "abstract": "Human motion taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different human motion taxonomies to learn hyperbolic embeddings that faithfully preserve the original graph structure. We show that our model properly encodes unseen data from existing or new taxonomy categories, and outperforms its Euclidean and VAE-based counterparts. Finally, through proof-of-concept experiments, we show that our model may be used to generate realistic trajectories between the learned embeddings",
    "checked": true,
    "id": "567f7e626365d70b829467b86d30251927601768",
    "semantic_title": "bringing motion taxonomies to continuous domains via gplvm on hyperbolic manifolds",
    "citation_count": 5,
    "authors": [
      "Noémie Jaquier",
      "Leonel Rozo",
      "Miguel González-Duque",
      "Viacheslav Borovitskiy",
      "Tamim Asfour"
    ]
  },
  "https://proceedings.mlr.press/v235/javanmard24a.html": {
    "title": "PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses",
    "volume": "main",
    "abstract": "This work studies algorithms for learning from aggregate responses. We focus on the construction of aggregation sets (called bags in the literature) for event-level loss functions. We prove for linear regression and generalized linear models (GLMs) that the optimal bagging problem reduces to one-dimensional size-constrained $k$-means clustering. Further, we theoretically quantify the advantage of using curated bags over random bags. We then propose the $\\texttt{PriorBoost}$ algorithm, which adaptively forms bags of samples that are increasingly homogeneous with respect to (unobserved) individual responses to improve model quality. We study label differential privacy for aggregate learning, and we also provide extensive experiments showing that $\\texttt{PriorBoost}$ regularly achieves optimal model quality for event-level predictions, in stark contrast to non-adaptive algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adel Javanmard",
      "Matthew Fahrbach",
      "Vahab Mirrokni"
    ]
  },
  "https://proceedings.mlr.press/v235/jedra24a.html": {
    "title": "Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery",
    "volume": "main",
    "abstract": "We study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\\in [m]\\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\\varepsilon$-optimal policy with probability at least $1-\\delta$ typically scales as $\\frac{m+n}{\\varepsilon^2}\\log(1/\\delta)$. Our regret minimization algorithm enjoys minimax guarantees typically scaling as $r^{5/4}(m+n)^{3/4}\\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they first leverage spectral methods to estimate the left and right singular subspaces of the low-rank reward matrix. We show that these estimates enjoy tight error guarantees in the two-to-infinity norm. This in turn allows us to reformulate our problems as a misspecified linear bandit problem with dimension roughly $r(m+n)$ and misspecification controlled by the subspace recovery error, as well as to design the second phase of our algorithms efficiently",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassir Jedra",
      "William Réveillard",
      "Stefan Stojanovic",
      "Alexandre Proutiere"
    ]
  },
  "https://proceedings.mlr.press/v235/jeeveswaran24a.html": {
    "title": "Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method",
    "volume": "main",
    "abstract": "Domain incremental learning (DIL) poses a significant challenge in real-world scenarios, as models need to be sequentially trained on diverse domains over time, all the while avoiding catastrophic forgetting. Mitigating representation drift, which refers to the phenomenon of learned representations undergoing changes as the model adapts to new tasks, can help alleviate catastrophic forgetting. In this study, we propose a novel DIL method named DARE, featuring a three-stage training process: Divergence, Adaptation, and REfinement. This process gradually adapts the representations associated with new tasks into the feature space spanned by samples from previous tasks, simultaneously integrating task-specific decision boundaries. Additionally, we introduce a novel strategy for buffer sampling and demonstrate the effectiveness of our proposed method, combined with this sampling strategy, in reducing representation drift within the feature encoder. This contribution effectively alleviates catastrophic forgetting across multiple DIL benchmarks. Furthermore, our approach prevents sudden representation drift at task boundaries, resulting in a well-calibrated DIL model that maintains the performance on previous tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kishaan Jeeveswaran",
      "Elahe Arani",
      "Bahram Zonooz"
    ]
  },
  "https://proceedings.mlr.press/v235/jelassi24a.html": {
    "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
    "volume": "main",
    "abstract": "Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as \"generalized state space models\" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samy Jelassi",
      "David Brandfonbrener",
      "Sham M. Kakade",
      "Eran Malach"
    ]
  },
  "https://proceedings.mlr.press/v235/jeon24a.html": {
    "title": "An Information-Theoretic Analysis of In-Context Learning",
    "volume": "main",
    "abstract": "Previous theoretical results pertaining to meta-learning on sequences build on contrived and convoluted mixing time assumptions. We introduce new information-theoretic tools that lead to a concise yet general decomposition of error for a Bayes optimal predictor into two components: meta-learning error and intra-task error. These tools unify analyses across many meta-learning challenges. To illustrate, we apply them to establish new results about in-context learning with transformers and corroborate existing results a simple linear setting. Our theoretical results characterize how error decays in both the number of training sequences and sequence lengths. Our results are very general; for example, they avoid contrived mixing time assumptions made by all prior results that establish decay of error with sequence length",
    "checked": true,
    "id": "d03d34a404676709d183bd71dc5da96f05a74cc4",
    "semantic_title": "an information-theoretic analysis of in-context learning",
    "citation_count": 16,
    "authors": [
      "Hong Jun Jeon",
      "Jason D. Lee",
      "Qi Lei",
      "Benjamin Van Roy"
    ]
  },
  "https://proceedings.mlr.press/v235/jessica24a.html": {
    "title": "Finite Volume Features, Global Geometry Representations, and Residual Training for Deep Learning-based CFD Simulation",
    "volume": "main",
    "abstract": "Computational fluid dynamics (CFD) simulation is an irreplaceable modelling step in many engineering designs, but it is often computationally expensive. Some graph neural network (GNN)-based CFD methods have been proposed. However, the current methods inherit the weakness of traditional numerical simulators, as well as ignore the cell characteristics in the mesh used in the finite volume method, a common method in practical CFD applications. Specifically, the input nodes in these GNN methods have very limited information about any object immersed in the simulation domain and its surrounding environment. Also, the cell characteristics of the mesh such as cell volume, face surface area, and face centroid are not included in the message-passing operations in the GNN methods. To address these weaknesses, this work proposes two novel geometric representations: Shortest Vector (SV) and Directional Integrated Distance (DID). Extracted from the mesh, the SV and DID provide global geometry perspective to each input node, thus removing the need to collect this information through message-passing. This work also introduces the use of Finite Volume Features (FVF) in the graph convolutions as node and edge attributes, enabling its message-passing operations to adjust to different nodes. Finally, this work is the first to demonstrate how residual training, with the availability of low-resolution data, can be adopted to improve the flow field prediction accuracy. Experimental results on two datasets with five different state-of-the-art GNN methods for CFD indicate that SV, DID, FVF and residual training can effectively reduce the predictive error of current GNN-based methods by as much as 41%. Our codes and datasets are available at https://github.com/toggled/FvFGeo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Loh Sher En Jessica",
      "Naheed Anjum Arafat",
      "Wei Xian Lim",
      "Wai Lee Chan",
      "Adams Wai-Kin Kong"
    ]
  },
  "https://proceedings.mlr.press/v235/jesson24a.html": {
    "title": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages",
    "volume": "main",
    "abstract": "This paper proposes a step toward approximate Bayesian inference in on-policy actor-critic deep reinforcement learning. It is implemented through three changes to the Asynchronous Advantage Actor-Critic (A3C) algorithm: (1) applying a ReLU function to advantage estimates, (2) spectral normalization of actor-critic weights, and (3) incorporating dropout as a Bayesian approximation. We prove under standard assumptions that restricting policy updates to positive advantages optimizes for value by maximizing a lower bound on the value function plus an additive term. We show that the additive term is bounded proportional to the Lipschitz constant of the value function, which offers theoretical grounding for spectral normalization of critic weights. Finally, our application of dropout corresponds to approximate Bayesian inference over both the actor and critic parameters, which enables adaptive state-aware exploration around the modes of the actor via Thompson sampling. We demonstrate significant improvements for median and interquartile mean metrics over A3C, PPO, SAC, and TD3 on the MuJoCo continuous control benchmark and improvement over PPO in the challenging ProcGen generalization benchmark",
    "checked": true,
    "id": "c99db0656d62e185486da6bcba6485f182307811",
    "semantic_title": "relu to the rescue: improve your on-policy actor-critic with positive advantages",
    "citation_count": 5,
    "authors": [
      "Andrew Jesson",
      "Chris Lu",
      "Gunshi Gupta",
      "Nicolas Beltran-Velez",
      "Angelos Filos",
      "Jakob Nicolaus Foerster",
      "Yarin Gal"
    ]
  },
  "https://proceedings.mlr.press/v235/ji24a.html": {
    "title": "Advancing Dynamic Sparse Training by Exploring Optimization Opportunities",
    "volume": "main",
    "abstract": "Dynamic Sparse Training (DST) is an effective approach for addressing the substantial training resource requirements posed by the ever-increasing size of the Deep Neural Networks (DNNs). Characterized by its dynamic \"train-prune-grow\" schedule during training, DST implicitly develops a bi-level structure for training the weights while discovering a subnetwork topology. However, such a structure is consistently overlooked by the current DST algorithms for further optimization opportunities, and these algorithms, on the other hand, solely optimize the weights while determining masks heuristically. In this paper, we extensively study DST algorithms and argue that the training scheme of DST naturally forms a bi-level problem in which the updating of weight and mask is interdependent. Based on this observation, we introduce a novel efficient training framework called BiDST, which for the first time, introduces bi-level optimization methodology into dynamic sparse training domain. Unlike traditional partial-heuristic DST schemes, which suffer from sub-optimal search efficiency for masks and miss the opportunity to fully explore the topological space of neural networks, BiDST excels at discovering excellent sparse patterns by optimizing mask and weight simultaneously, resulting in maximum 2.62% higher accuracy, 2.1$\\times$ faster execution speed, and 25$\\times$ reduced overhead. Code available at https://github.com/jjsrf/BiDST-ICML2024",
    "checked": true,
    "id": "83d1decf4b462a7af82bcf6e73b425ae46fb00c3",
    "semantic_title": "advancing dynamic sparse training by exploring optimization opportunities",
    "citation_count": 1,
    "authors": [
      "Jie Ji",
      "Gen Li",
      "Lu Yin",
      "Minghai Qin",
      "Geng Yuan",
      "Linke Guo",
      "Shiwei Liu",
      "Xiaolong Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/ji24b.html": {
    "title": "ACE: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization",
    "volume": "main",
    "abstract": "The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which underscores the effectiveness, versatility, and efficient sample efficiency of our approach. Benchmark results and videos are available at https://ace-rl.github.io/",
    "checked": false,
    "id": "06df2005b1244940c711eb4819be41cc26e7eed7",
    "semantic_title": "ace : off-policy actor-critic with causality-aware entropy regularization",
    "citation_count": 5,
    "authors": [
      "Tianying Ji",
      "Yongyuan Liang",
      "Yan Zeng",
      "Yu Luo",
      "Guowei Xu",
      "Jiawei Guo",
      "Ruijie Zheng",
      "Furong Huang",
      "Fuchun Sun",
      "Huazhe Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/ji24c.html": {
    "title": "Towards Efficient Exact Optimization of Language Model Alignment",
    "volume": "main",
    "abstract": "The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. However, we show that DPO derived based on the optimal solution of the problem leads to a compromised mean-seeking approximation of the optimal solution in practice. In this paper, we propose efficient exact optimization (EXO) of the alignment objective. EXO is guaranteed to optimize in the same direction as RL algorithms asymptotically for arbitrary policy parametrization. This leads to the same mode-seeking solution, while enables efficient optimization by circumventing the complexities of RL. We also compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data. Code is available at https://github.com/haozheji/exact-optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Ji",
      "Cheng Lu",
      "Yilin Niu",
      "Pei Ke",
      "Hongning Wang",
      "Jun Zhu",
      "Jie Tang",
      "Minlie Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/ji24d.html": {
    "title": "Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic",
    "volume": "main",
    "abstract": "Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works primarily focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that $Q$-values are often underestimated in the latter stage of the RL training process, potentially hindering policy learning and reducing sample efficiency. We find that such a long-neglected phenomenon is often related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. Based on BEE, the resulting practical algorithm BAC outperforms state-of-the-art methods in over 50 continuous control tasks and achieves strong performance in failure-prone scenarios and real-world robot tasks. Benchmark results and videos are available at https://jity16.github.io/BEE/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianying Ji",
      "Yu Luo",
      "Fuchun Sun",
      "Xianyuan Zhan",
      "Jianwei Zhang",
      "Huazhe Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/ji24e.html": {
    "title": "Discrete Latent Perspective Learning for Segmentation and Detection",
    "volume": "main",
    "abstract": "In this paper, we address the challenge of Perspective-Invariant Learning in machine learning and computer vision, which involves enabling a network to understand images from varying perspectives to achieve consistent semantic interpretation. While standard approaches rely on the labor-intensive collection of multi-view images or limited data augmentation techniques, we propose a novel framework, Discrete Latent Perspective Learning (DLPL), for latent multi-perspective fusion learning using conventional single-view images. DLPL comprises three main modules: Perspective Discrete Decomposition (PDD), Perspective Homography Transformation (PHT), and Perspective Invariant Attention (PIA), which work together to discretize visual features, transform perspectives, and fuse multi-perspective semantic information, respectively. DLPL is a universal perspective learning framework applicable to a variety of scenarios and vision tasks. Extensive experiments demonstrate that DLPL significantly enhances the network's capacity to depict images across diverse scenarios (daily photos, UAV, auto-driving) and tasks (detection, segmentation)",
    "checked": true,
    "id": "d82eab566a8e74484c7379f7adcc075af1cec310",
    "semantic_title": "discrete latent perspective learning for segmentation and detection",
    "citation_count": 4,
    "authors": [
      "Deyi Ji",
      "Feng Zhao",
      "Lanyun Zhu",
      "Wenwei Jin",
      "Hongtao Lu",
      "Jieping Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/jia24a.html": {
    "title": "Simulation-Based Inference with Quantile Regression",
    "volume": "main",
    "abstract": "We present Neural Quantile Estimation (NQE), a novel Simulation-Based Inference (SBI) method based on conditional quantile regression. NQE autoregressively learns individual one dimensional quantiles for each posterior dimension, conditioned on the data and previous posterior dimensions. Posterior samples are obtained by interpolating the predicted quantiles using monotonic cubic Hermite spline, with specific treatment for the tail behavior and multi-modal distributions. We introduce an alternative definition for the Bayesian credible region using the local Cumulative Density Function (CDF), offering substantially faster evaluation than the traditional Highest Posterior Density Region (HPDR). In case of limited simulation budget and/or known model misspecification, a post-processing calibration step can be integrated into NQE to ensure the unbiasedness of the posterior estimation with negligible additional computational cost. We demonstrate that NQE achieves state-of-the-art performance on a variety of benchmark problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He Jia"
    ]
  },
  "https://proceedings.mlr.press/v235/jia24b.html": {
    "title": "GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer",
    "volume": "main",
    "abstract": "Cross-modal transformers have demonstrated superiority in various vision tasks by effectively integrating different modalities. This paper first critiques prior token exchange methods which replace less informative tokens with inter-modal features, and demonstrate exchange based methods underperform cross-attention mechanisms, while the computational demand of the latter inevitably restricts its use with longer sequences. To surmount the computational challenges, we propose GeminiFusion, a pixel-wise fusion approach that capitalizes on aligned cross-modal representations. GeminiFusion elegantly combines intra-modal and inter-modal attentions, dynamically integrating complementary information across modalities. We employ a layer-adaptive noise to adaptively control their interplay on a per-layer basis, thereby achieving a harmonized fusion process. Notably, GeminiFusion maintains linear complexity with respect to the number of input tokens, ensuring this multimodal framework operates with efficiency comparable to unimodal networks. Comprehensive evaluations across multimodal image-to-image translation, $3$D object detection and arbitrary-modal semantic segmentation tasks, including RGB, depth, LiDAR, event data, etc. demonstrate the superior performance of our GeminiFusion against leading-edge techniques. The PyTorch code is available here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Jia",
      "Jianyuan Guo",
      "Kai Han",
      "Han Wu",
      "Chao Zhang",
      "Chang Xu",
      "Xinghao Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/jia24c.html": {
    "title": "Chain-of-Thought Predictive Control",
    "volume": "main",
    "abstract": "We study generalizable policy learning from demonstrations for complex low-level control (e.g., contact-rich object manipulations). We propose a novel hierarchical imitation learning method that utilizes sub-optimal demos. Firstly, we propose an observation space-agnostic approach that efficiently discovers the multi-step subskill decomposition of the demos in an unsupervised manner. By grouping temporarily close and functionally similar actions into subskill-level demo segments, the observations at the segment boundaries constitute a chain of planning steps for the task, which we refer to as the chain-of-thought (CoT). Next, we propose a Transformer-based design that effectively learns to predict the CoT as the subskill-level guidance. We couple action and subskill predictions via learnable prompt tokens and a hybrid masking strategy, which enable dynamically updated guidance at test time and improve feature representation of the trajectory for generalizable policy learning. Our method, Chain-of-Thought Predictive Control (CoTPC), consistently surpasses existing strong baselines on various challenging low-level manipulation tasks with sub-optimal demos. See project page at https://sites.google.com/view/cotpc",
    "checked": true,
    "id": "e1bd151a3f670fd0f77580702fe7a85dc78a41cb",
    "semantic_title": "chain-of-thought predictive control",
    "citation_count": 15,
    "authors": [
      "Zhiwei Jia",
      "Vineet Thumuluri",
      "Fangchen Liu",
      "Linghao Chen",
      "Zhiao Huang",
      "Hao Su"
    ]
  },
  "https://proceedings.mlr.press/v235/jiale24a.html": {
    "title": "Pre-Training Protein Bi-level Representation Through Span Mask Strategy On 3D Protein Chains",
    "volume": "main",
    "abstract": "In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein chains to learn meaningful representations of both residues and atoms. This leads to a simple yet effective approach to learning protein representation suitable for diverse downstream tasks. Extensive experimental results on binding site prediction and function prediction tasks demonstrate our proposed pre-training approach significantly outperforms other methods. Our code will be made public",
    "checked": true,
    "id": "19fd02fff8f69732a6b2d688f2074a8f4b6a7b72",
    "semantic_title": "pre-training protein bi-level representation through span mask strategy on 3d protein chains",
    "citation_count": 0,
    "authors": [
      "Zhao Jiale",
      "Wanru Zhuang",
      "Jia Song",
      "Yaqi Li",
      "Shuqi Lu"
    ]
  },
  "https://proceedings.mlr.press/v235/jiang24a.html": {
    "title": "NDOT: Neuronal Dynamics-based Online Training for Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNNs) are attracting great attention for their energy-efficient and fast-inference properties in neuromorphic computing. However, the efficient training of deep SNNs poses challenges in gradient calculation due to the non-differentiability of their binary spike-generating activation functions. The widely used surrogate gradient (SG) method, combined with the back-propagation through time (BPTT), has shown considerable effectiveness. Yet, BPTT's process of unfolding and back-propagating along the computation graph requires storing intermediate information at all time-steps, resulting in huge memory consumption and failing to meet online requirements. In this work, we propose Neuronal Dynamics-based Online Training (NDOT) for SNNs, which uses the neuronal dynamics-based temporal dependency/sensitivity in gradient computation. NDOT enables forward-in-time learning by decomposing the full gradient into temporal and spatial gradients. To illustrate the intuition behind NDOT, we employ the Follow-the-Regularized-Leader (FTRL) algorithm. FTRL explicitly utilizes historical information and addresses limitations in instantaneous loss. Our proposed NDOT method accurately captures temporal dependencies through neuronal dynamics, functioning similarly to FTRL's explicit utilizing historical information. Experiments on CIFAR-10, CIFAR-100, and CIFAR10-DVS demonstrate the superior performance of our NDOT method on large-scale static and neuromorphic datasets within a small number of time steps. The codes are available at https://github.com/HaiyanJiang/SNN-NDOT",
    "checked": true,
    "id": "4529df5895a431eda3ef1e51594151469788b584",
    "semantic_title": "ndot: neuronal dynamics-based online training for spiking neural networks",
    "citation_count": 2,
    "authors": [
      "Haiyan Jiang",
      "Giulia De Masi",
      "Huan Xiong",
      "Bin Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/jiang24b.html": {
    "title": "Conditional Common Entropy for Instrumental Variable Testing and Partial Identification",
    "volume": "main",
    "abstract": "Instrumental variables (IVs) are widely used for estimating causal effects. There are two main challenges when using instrumental variables. First of all, using IV without additional assumptions such as linearity, the causal effect may still not be identifiable. Second, when selecting an IV, the validity of the selected IV is typically not testable since the causal graph is not identifiable from observational data. In this paper, we propose a method for bounding the causal effect with instrumental variables under weak confounding. In addition, we present a novel criterion to falsify the IV with side information about the confounder. We demonstrate the utility of the proposed method with simulated and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Jiang",
      "Murat Kocaoglu"
    ]
  },
  "https://proceedings.mlr.press/v235/jiang24c.html": {
    "title": "ProtoGate: Prototype-based Neural Networks with Global-to-local Feature Selection for Tabular Biomedical Data",
    "volume": "main",
    "abstract": "Tabular biomedical data poses challenges in machine learning because it is often high-dimensional and typically low-sample-size (HDLSS). Previous research has attempted to address these challenges via local feature selection, but existing approaches often fail to achieve optimal performance due to their limitation in identifying globally important features and their susceptibility to the co-adaptation problem. In this paper, we propose ProtoGate, a prototype-based neural model for feature selection on HDLSS data. ProtoGate first selects instance-wise features via adaptively balancing global and local feature selection. Furthermore, ProtoGate employs a non-parametric prototype-based prediction mechanism to tackle the co-adaptation problem, ensuring the feature selection results and predictions are consistent with underlying data clusters. We conduct comprehensive experiments to evaluate the performance and interpretability of ProtoGate on synthetic and real-world datasets. The results show that ProtoGate generally outperforms state-of-the-art methods in prediction accuracy by a clear margin while providing high-fidelity feature selection and explainable predictions. Code is available at https://github.com/SilenceX12138/ProtoGate",
    "checked": true,
    "id": "82cfe0390daf75a13f4cd4ab4dff0a329ae17178",
    "semantic_title": "protogate: prototype-based neural networks with global-to-local feature selection for tabular biomedical data",
    "citation_count": 2,
    "authors": [
      "Xiangjian Jiang",
      "Andrei Margeloiu",
      "Nikola Simidjievski",
      "Mateja Jamnik"
    ]
  },
  "https://proceedings.mlr.press/v235/jiang24d.html": {
    "title": "On the Origins of Linear Representations in Large Language Models",
    "volume": "main",
    "abstract": "An array of recent works have argued that high-level semantic concepts are encoded \"linearly\" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to prove that linearity arises as a consequence of the loss function and the implicit bias of gradient descent. The theory is further substantiated empirically via experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Jiang",
      "Goutham Rajendran",
      "Pradeep Kumar Ravikumar",
      "Bryon Aragam",
      "Victor Veitch"
    ]
  },
  "https://proceedings.mlr.press/v235/jiang24e.html": {
    "title": "Federated Optimization with Doubly Regularized Drift Correction",
    "volume": "main",
    "abstract": "Federated learning is a distributed optimization paradigm that allows training machine learning models across decentralized devices while keeping the data localized. The standard method, FedAvg, suffers from client drift which can hamper performance and increase communication costs over centralized methods. Previous works proposed various strategies to mitigate drift, yet none have shown consistently improved communication-computation trade-offs over vanilla gradient descent across all standard function classes. In this work, we revisit DANE, an established method in distributed optimization. We show that (i) DANE can achieve the desired communication reduction under Hessian similarity constraints. Furthermore, (ii) we present an extension, DANE+, which supports arbitrary inexact local solvers and has more freedom to choose how to aggregate the local updates. We propose (iii) a novel method, FedRed, which has improved local computational complexity and retains the same communication complexity compared to DANE/DANE+. This is achieved by doubly regularized drift correction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaowen Jiang",
      "Anton Rodomanov",
      "Sebastian U Stich"
    ]
  },
  "https://proceedings.mlr.press/v235/jiang24f.html": {
    "title": "HexGen: Generative Inference of Large Language Model over Heterogeneous Environment",
    "volume": "main",
    "abstract": "Serving generative inference of the large language model is a crucial component of contemporary AI applications. In this paper, our focus lies in deploying such services in a heterogeneous and cross-datacenter setting to mitigate the substantial inference costs typically associated with a single centralized datacenter. Towards this end, we propose HexGen, a flexible distributed inference engine that uniquely supports the asymmetric partition of generative inference computations over both tensor model parallelism and pipeline parallelism, which allows for effective deployment across diverse GPUs interconnected by a fully heterogeneous network. We further propose a sophisticated scheduling algorithm grounded in constrained optimization that can adaptively assign asymmetric inference computation across the GPUs to fulfill inference requests while maintaining acceptable latency levels. We conduct an extensive empirical study to evaluate the efficiency of HexGen by serving the state-of-the-art Llama-2 (70B) model. The experimental results suggest that HexGen can choose to achieve up to $2.3\\times$ lower latency deadlines or tolerate up to $4\\times$ more traffic request rates compared with the homogeneous baseline given the same budget",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youhe Jiang",
      "Ran Yan",
      "Xiaozhe Yao",
      "Yang Zhou",
      "Beidi Chen",
      "Binhang Yuan"
    ]
  },
  "https://proceedings.mlr.press/v235/jiang24g.html": {
    "title": "Projection-Free Variance Reduction Methods for Stochastic Constrained Multi-Level Compositional Optimization",
    "volume": "main",
    "abstract": "This paper investigates projection-free algorithms for stochastic constrained multi-level optimization. In this context, the objective function is a nested composition of several smooth functions, and the decision set is closed and convex. Existing projection-free algorithms for solving this problem suffer from two limitations: 1) they solely focus on the gradient mapping criterion and fail to match the optimal sample complexities in unconstrained settings; 2) their analysis is exclusively applicable to non-convex functions, without considering convex and strongly convex objectives. To address these issues, we introduce novel projection-free variance reduction algorithms and analyze their complexities under different criteria. For gradient mapping, our complexities improve existing results and match the optimal rates for unconstrained problems. For the widely-used Frank-Wolfe gap criterion, we provide theoretical guarantees that align with those for single-level problems. Additionally, by using a stage-wise adaptation, we further obtain complexities for convex and strongly convex functions. Finally, numerical experiments on different tasks demonstrate the effectiveness of our methods",
    "checked": true,
    "id": "4fad1eaf1aa7dbb58f896c668035c0c462e71b39",
    "semantic_title": "projection-free variance reduction methods for stochastic constrained multi-level compositional optimization",
    "citation_count": 2,
    "authors": [
      "Wei Jiang",
      "Sifan Yang",
      "Wenhao Yang",
      "Yibo Wang",
      "Yuanyu Wan",
      "Lijun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/jiang24h.html": {
    "title": "Tabular Insights, Visual Impacts: Transferring Expertise from Tables to Images",
    "volume": "main",
    "abstract": "Transferring knowledge across diverse data modalities is receiving increasing attention in machine learning. This paper tackles the task of leveraging expert-derived, yet expensive, tabular data to enhance image-based predictions when tabular data is unavailable during inference. The primary challenges stem from the inherent complexity of accurately mapping diverse tabular data to visual contexts, coupled with the necessity to devise distinct strategies for numerical and categorical tabular attributes. We propose CHannel tAbulaR alignment with optiMal tranSport (Charms), which establishes an alignment between image channels and tabular attributes, enabling selective knowledge transfer that is pertinent to visual features. Specifically, Charms measures similarity distributions across modalities to effectively differentiate and transfer relevant tabular features, with a focus on morphological characteristics, enhancing the capabilities of visual classifiers. By maximizing the mutual information between image channels and tabular features, knowledge from both numerical and categorical tabular attributes are extracted. Experimental results demonstrate that Charms not only enhances the performance of image classifiers but also improves their interpretability by effectively utilizing tabular knowledge",
    "checked": true,
    "id": "79303ccec7582e463905ca3a25c6fb4e624fcb74",
    "semantic_title": "tabular insights, visual impacts: transferring expertise from tables to images",
    "citation_count": 0,
    "authors": [
      "Jun-Peng Jiang",
      "Han-Jia Ye",
      "Leye Wang",
      "Yang Yang",
      "Yuan Jiang",
      "De-Chuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/jiang24i.html": {
    "title": "Generalized Neural Collapse for a Large Number of Classes",
    "volume": "main",
    "abstract": "Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized. We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show that the generalized neural collapse provably occurs under unconstrained feature model with spherical constraint, under certain technical conditions on feature dimension and number of classes",
    "checked": true,
    "id": "732329e1af9b1822a3d0ea9fea473e9e22dd44fa",
    "semantic_title": "generalized neural collapse for a large number of classes",
    "citation_count": 18,
    "authors": [
      "Jiachen Jiang",
      "Jinxin Zhou",
      "Peng Wang",
      "Qing Qu",
      "Dustin G. Mixon",
      "Chong You",
      "Zhihui Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/jiawei24a.html": {
    "title": "SuDA: Support-based Domain Adaptation for Sim2Real Hinge Joint Tracking with Flexible Sensors",
    "volume": "main",
    "abstract": "Flexible sensors hold promise for human motion capture (MoCap), offering advantages such as wearability, privacy preservation, and minimal constraints on natural movement. However, existing flexible sensor-based MoCap methods rely on deep learning and necessitate large and diverse labeled datasets for training. These data typically need to be collected in MoCap studios with specialized equipment and substantial manual labor, making them difficult and expensive to obtain at scale. Thanks to the high-linearity of flexible sensors, we address this challenge by proposing a novel Sim2Real solution for hinge joint tracking based on domain adaptation, eliminating the need for labeled data yet achieving comparable accuracy to supervised learning. Our solution relies on a novel Support-based Domain Adaptation method, namely SuDA, which aligns the supports of the predictive functions rather than the instance-dependent distributions between the source and target domains. Extensive experimental results demonstrate the effectiveness of our method and its superiority overstate-of-the-art distribution-based domain adaptation methods in our task",
    "checked": true,
    "id": "ae705fe4c8de8741615a2872dfff52ab018799b1",
    "semantic_title": "suda: support-based domain adaptation for sim2real hinge joint tracking with flexible sensors",
    "citation_count": 0,
    "authors": [
      "Fang Jiawei",
      "Haishan Song",
      "Chengxu Zuo",
      "Xiaoxia Gao",
      "Xiaowei Chen",
      "Shihui Guo",
      "Yipeng Qin"
    ]
  },
  "https://proceedings.mlr.press/v235/jie24a.html": {
    "title": "Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning",
    "volume": "main",
    "abstract": "Current solutions for efficiently constructing large vision-language (VL) models follow a two-step paradigm: projecting the output of pre-trained vision encoders to the input space of pre-trained language models as visual prompts; and then transferring the models to downstream VL tasks via end-to-end parameter-efficient fine-tuning (PEFT). However, this paradigm still exhibits inefficiency since it significantly increases the input length of the language models. In this paper, in contrast to integrating visual prompts into inputs, we regard visual prompts as additional knowledge that facilitates language models in addressing tasks associated with visual information. Motivated by the finding that Feed-Forward Network (FFN) of language models acts as \"key-value memory\", we introduce a novel approach termed memory-space visual prompting (MemVP), wherein visual prompts are concatenated with the weights of FFN for visual knowledge injection. Experimental results across various VL tasks and language models reveal that MemVP significantly reduces the training time and inference latency of the finetuned VL models and surpasses the performance of previous PEFT methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibo Jie",
      "Yehui Tang",
      "Ning Ding",
      "Zhi-Hong Deng",
      "Kai Han",
      "Yunhe Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/jin24a.html": {
    "title": "Homomorphism Counts for Graph Neural Networks: All About That Basis",
    "volume": "main",
    "abstract": "A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the \"basis\" of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical results on node-level and graph-level motif parameters and empirically validate them on standard benchmark datasets",
    "checked": true,
    "id": "0ed53dfee261f5d74f6981cb207534966e61c8c9",
    "semantic_title": "homomorphism counts for graph neural networks: all about that basis",
    "citation_count": 5,
    "authors": [
      "Emily Jin",
      "Michael M. Bronstein",
      "Ismail Ilkan Ceylan",
      "Matthias Lanzinger"
    ]
  },
  "https://proceedings.mlr.press/v235/jin24b.html": {
    "title": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning",
    "volume": "main",
    "abstract": "It is well known that LLMs cannot generalize well to long contexts whose lengths are larger than the training sequence length. This poses challenges when employing LLMs for processing long input sequences during inference. In this work, we argue that LLMs themselves have inherent capabilities to handles s long contexts without fine-tuning. To achieve this goal, we propose SelfExtend to extend the context window of LLMs by constructing bi-level attention information: the grouped attention and the neighbor attention. The grouped attention captures the dependencies among tokens that are far apart, while neighbor attention captures dependencies among adjacent tokens within a specified range. The two-level attentions are computed based on the original model's self-attention mechanism during inference. With minor code modification, our SelfExtend can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments on multiple benchmarks and the results show that our SelfExtend can effectively extend existing LLMs' context window length",
    "checked": false,
    "id": "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b",
    "semantic_title": "llm maybe longlm: self-extend llm context window without tuning",
    "citation_count": 58,
    "authors": [
      "Hongye Jin",
      "Xiaotian Han",
      "Jingfeng Yang",
      "Zhimeng Jiang",
      "Zirui Liu",
      "Chia-Yuan Chang",
      "Huiyuan Chen",
      "Xia Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/jin24c.html": {
    "title": "On the Maximal Local Disparity of Fairness-Aware Classifiers",
    "volume": "main",
    "abstract": "Fairness has become a crucial aspect in the development of trustworthy machine learning algorithms. Current fairness metrics to measure the violation of demographic parity have the following drawbacks: (i) the average difference of model predictions on two groups cannot reflect their distribution disparity, and (ii) the overall calculation along all possible predictions conceals the extreme local disparity at or around certain predictions. In this work, we propose a novel fairness metric called Maximal Cumulative ratio Disparity along varying Predictions' neighborhood (MCDP), for measuring the maximal local disparity of the fairness-aware classifiers. To accurately and efficiently calculate the MCDP, we develop a provably exact and an approximate calculation algorithm that greatly reduces the computational complexity with low estimation error. We further propose a bi-level optimization algorithm using a differentiable approximation of the MCDP for improving the algorithmic fairness. Extensive experiments on both tabular and image datasets validate that our fair training algorithm can achieve superior fairness-accuracy trade-offs",
    "checked": true,
    "id": "7d98e03231653906f10e3ae86d902e39f7e87384",
    "semantic_title": "on the maximal local disparity of fairness-aware classifiers",
    "citation_count": 0,
    "authors": [
      "Jinqiu Jin",
      "Haoxuan Li",
      "Fuli Feng"
    ]
  },
  "https://proceedings.mlr.press/v235/jin24d.html": {
    "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
    "volume": "main",
    "abstract": "Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting—the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products of example representations achieves better forecasting performance over a series of setups. Finally, we show that we reduce forgetting of upstream pretraining examples by replaying examples that are forecasted to be forgotten, demonstrating the practical utility of forecasting example forgetting",
    "checked": true,
    "id": "50c9dc9907d19afcafc082332e53b5a0ac58956d",
    "semantic_title": "what will my model forget? forecasting forgotten examples in language model refinement",
    "citation_count": 5,
    "authors": [
      "Xisen Jin",
      "Xiang Ren"
    ]
  },
  "https://proceedings.mlr.press/v235/jin24e.html": {
    "title": "Emergent Representations of Program Semantics in Language Models Trained on Programs",
    "volume": "main",
    "abstract": "We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to interpret programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We anticipate that this technique may be generally applicable to a broad range of semantic probing experiments. In summary, this paper does not propose any new techniques for training LMs of code, but develops an experimental framework for and provides insights into the acquisition and representation of formal semantics in statistical models of code",
    "checked": true,
    "id": "1b36bd7672cbdf296645276303f4596bc7ed5468",
    "semantic_title": "emergent representations of program semantics in language models trained on programs",
    "citation_count": 15,
    "authors": [
      "Charles Jin",
      "Martin Rinard"
    ]
  },
  "https://proceedings.mlr.press/v235/jin24f.html": {
    "title": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization",
    "volume": "main",
    "abstract": "In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models are available at https://video-lavit.github.io",
    "checked": true,
    "id": "c1b5195bc09a2232ec2b69e5a2a6bd39b3162c62",
    "semantic_title": "video-lavit: unified video-language pre-training with decoupled visual-motional tokenization",
    "citation_count": 17,
    "authors": [
      "Yang Jin",
      "Zhicheng Sun",
      "Kun Xu",
      "Kun Xu",
      "Liwei Chen",
      "Hao Jiang",
      "Quzhe Huang",
      "Chengru Song",
      "Yuliang Liu",
      "Di Zhang",
      "Yang Song",
      "Kun Gai",
      "Yadong Mu"
    ]
  },
  "https://proceedings.mlr.press/v235/jin24g.html": {
    "title": "An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning",
    "volume": "main",
    "abstract": "Textural Inversion, a prompt learning method, learns a singular text embedding for a new \"word\" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying multiple unknown object-level concepts within one scene remains a complex challenge. While recent methods have resorted to cropping or masking individual images to learn multiple concepts, these techniques often require prior knowledge of new concepts and are labour-intensive. To address this challenge, we introduce Multi-Concept Prompt Learning (MCPL), where multiple unknown \"words\" are simultaneously learned from a single sentence-image pair, without any imagery annotations. To enhance the accuracy of word-concept correlation and refine attention mask boundaries, we propose three regularisation techniques: Attention Masking, Prompts Contrastive Loss, and Bind Adjective. Extensive quantitative comparisons with both real-world categories and biomedical images demonstrate that our method can learn new semantically disentangled concepts. Our approach emphasises learning solely from textual embeddings, using less than 10% of the storage space compared to others. The project page, code, and data are available at https://astrazeneca.github.io/mcpl.github.io",
    "checked": true,
    "id": "82aa114890203efc46d573f1903c9f2527f782f3",
    "semantic_title": "an image is worth multiple words: discovering object level concepts using multi-concept prompt learning",
    "citation_count": 0,
    "authors": [
      "Chen Jin",
      "Ryutaro Tanno",
      "Amrutha Saseendran",
      "Tom Diethe",
      "Philip Alexander Teare"
    ]
  },
  "https://proceedings.mlr.press/v235/jin24h.html": {
    "title": "Language Models as Semantic Indexers",
    "volume": "main",
    "abstract": "Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss, and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. It is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMIndexer, a self-supervised framework to learn semantic IDs with a generative language model. We tackle the challenge of sequential discrete ID by introducing a semantic indexer capable of generating neural sequential discrete representations with progressive training and contrastive learning. In response to the semantic supervision deficiency, we propose to train the model with a self-supervised document reconstruction objective. We show the high quality of the learned IDs and demonstrate their effectiveness on three tasks including recommendation, product search, and document retrieval on five datasets from various domains. Code is available at https://github.com/PeterGriffinJin/LMIndexer",
    "checked": true,
    "id": "c06b76673ccca18e3123421e62458da4c3a3edcc",
    "semantic_title": "language models as semantic indexers",
    "citation_count": 9,
    "authors": [
      "Bowen Jin",
      "Hansi Zeng",
      "Guoyin Wang",
      "Xiusi Chen",
      "Tianxin Wei",
      "Ruirui Li",
      "Zhengyang Wang",
      "Zheng Li",
      "Yang Li",
      "Hanqing Lu",
      "Suhang Wang",
      "Jiawei Han",
      "Xianfeng Tang"
    ]
  },
  "https://proceedings.mlr.press/v235/jin24i.html": {
    "title": "Position: What Can Large Language Models Tell Us about Time Series Analysis",
    "volume": "main",
    "abstract": "Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research",
    "checked": true,
    "id": "d622d8b2d5adc4b638a73b105686840e264dfb8f",
    "semantic_title": "position: what can large language models tell us about time series analysis",
    "citation_count": 6,
    "authors": [
      "Ming Jin",
      "Yifan Zhang",
      "Wei Chen",
      "Kexin Zhang",
      "Yuxuan Liang",
      "Bin Yang",
      "Jindong Wang",
      "Shirui Pan",
      "Qingsong Wen"
    ]
  },
  "https://proceedings.mlr.press/v235/jing24a.html": {
    "title": "AlphaFold Meets Flow Matching for Generating Protein Ensembles",
    "volume": "main",
    "abstract": "The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditioned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at https://github.com/bjing2016/alphaflow",
    "checked": true,
    "id": "8136c9a5915cee9bf332e0969719dd4884f7c673",
    "semantic_title": "alphafold meets flow matching for generating protein ensembles",
    "citation_count": 46,
    "authors": [
      "Bowen Jing",
      "Bonnie Berger",
      "Tommi Jaakkola"
    ]
  },
  "https://proceedings.mlr.press/v235/jing24b.html": {
    "title": "FedSC: Provable Federated Self-supervised Learning with Spectral Contrastive Objective over Non-i.i.d. Data",
    "volume": "main",
    "abstract": "Recent efforts have been made to integrate self-supervised learning (SSL) with the framework of federated learning (FL). One unique challenge of federated self-supervised learning (FedSSL) is that the global objective of FedSSL usually does not equal the weighted sum of local SSL objectives. Consequently, conventional approaches, such as federated averaging (FedAvg), fail to precisely minimize the FedSSL global objective, often resulting in suboptimal performance, especially when data is non-i.i.d.. To fill this gap, we propose a provable FedSSL algorithm, named FedSC, based on the spectral contrastive objective. In FedSC, clients share correlation matrices of data representations in addition to model weights periodically, which enables inter-client contrast of data samples in addition to intra-client contrast and contraction, resulting in improved quality of data representations. Differential privacy (DP) protection is deployed to control the additional privacy leakage on local datasets when correlation matrices are shared. We provide theoretical analysis on convergence and extra privacy leakage, and conduct numerical experiments to justify the effectiveness of our proposed algorithm",
    "checked": true,
    "id": "c565883787ddf73d3718cbd70938fb47b62b4669",
    "semantic_title": "fedsc: provable federated self-supervised learning with spectral contrastive objective over non-i.i.d. data",
    "citation_count": 0,
    "authors": [
      "Shusen Jing",
      "Anlan Yu",
      "Shuai Zhang",
      "Songyang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/jinnai24a.html": {
    "title": "Model-Based Minimum Bayes Risk Decoding for Text Generation",
    "volume": "main",
    "abstract": "Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative to beam search decoding in a variety of text generation tasks. MBR decoding selects a hypothesis from a pool of hypotheses that has the least expected risk under a probability model according to a given utility function. Since it is impractical to compute the expected risk exactly over all possible hypotheses, two approximations are commonly used in MBR. First, it integrates over a sampled set of hypotheses rather than over all possible hypotheses. Second, it estimates the probability of each hypothesis using a Monte Carlo estimator. While the first approximation is necessary to make it computationally feasible, the second is not essential since we typically have access to the model probability at inference time. We propose model-based MBR (MBMBR), a variant of MBR that uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. We show analytically and empirically that the model-based estimate is more promising than the Monte Carlo estimate in text generation tasks. Our experiments show that MBMBR outperforms MBR in several text generation tasks, both with encoder-decoder models and with language models",
    "checked": true,
    "id": "39eb1ff665e5c7c2acf4ab6061b646096919b5b3",
    "semantic_title": "model-based minimum bayes risk decoding for text generation",
    "citation_count": 6,
    "authors": [
      "Yuu Jinnai",
      "Tetsuro Morimura",
      "Ukyo Honda",
      "Kaito Ariu",
      "Kenshi Abe"
    ]
  },
  "https://proceedings.mlr.press/v235/jo24a.html": {
    "title": "Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes",
    "volume": "main",
    "abstract": "Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications in diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative diffusion process on manifolds. Instead of following the denoising approach of previous diffusion models, we construct a diffusion process using a mixture of bridge processes derived on general manifolds without requiring heat kernel estimations. We develop a geometric understanding of the mixture process, deriving the drift as a weighted mean of tangent directions to the data points that guides the process toward the data distribution. We further propose a scalable training objective for learning the mixture process that readily applies to general manifolds. Our method achieves superior performance on diverse manifolds with dramatically reduced number of in-training simulation steps for general manifolds",
    "checked": true,
    "id": "da110a60e022517b68006dc9ece71a564276144f",
    "semantic_title": "generative modeling on manifolds through mixture of riemannian diffusion processes",
    "citation_count": 2,
    "authors": [
      "Jaehyeong Jo",
      "Sung Ju Hwang"
    ]
  },
  "https://proceedings.mlr.press/v235/jo24b.html": {
    "title": "Graph Generation with Diffusion Mixture",
    "volume": "main",
    "abstract": "Generation of graphs is a major challenge for real-world tasks that require understanding the complex nature of their non-Euclidean structures. Although diffusion models have achieved notable success in graph generation recently, they are ill-suited for modeling the topological properties of graphs since learning to denoise the noisy samples does not explicitly learn the graph structures to be generated. To tackle this limitation, we propose a generative framework that models the topology of graphs by explicitly learning the final graph structures of the diffusion process. Specifically, we design the generative process as a mixture of endpoint-conditioned diffusion processes which is driven toward the predicted graph that results in rapid convergence. We further introduce a simple parameterization of the mixture process and develop an objective for learning the final graph structure, which enables maximum likelihood training. Through extensive experimental validation on general graph and 2D/3D molecule generation tasks, we show that our method outperforms previous generative models, generating graphs with correct topology with both continuous (e.g. 3D coordinates) and discrete (e.g. atom types) features. Our code is available at https://github.com/harryjo97/GruM",
    "checked": true,
    "id": "55c821de4ae6db8def78b619889669c83120a5ea",
    "semantic_title": "graph generation with diffusion mixture",
    "citation_count": 10,
    "authors": [
      "Jaehyeong Jo",
      "Dongki Kim",
      "Sung Ju Hwang"
    ]
  },
  "https://proceedings.mlr.press/v235/johnson24a.html": {
    "title": "Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs",
    "volume": "main",
    "abstract": "Identifying how much a model $\\hat{p}_{Y|X}^{\\theta}$ knows about the stochastic real-world process $p_{Y|X}$ it was trained on is important to ensure it avoids producing incorrect or \"hallucinated\" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p_{Y|X}$ and also estimate the remaining gaps between $\\hat{p}_{Y|X}^{\\theta}$ and $p_{Y|X}$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to \"cheat\" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we prove that being good at cheating (i.e. cheating whenever it improves your prediction) is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for $p_{Y|X}$ and detect incorrect responses with high probability. We demonstrate empirically that our approach accurately estimates how much models don't know across ambiguous image classification, (synthetic) language modeling, and partially-observable navigation tasks, outperforming existing techniques",
    "checked": true,
    "id": "741b4e0dca58cb136b123ce04887605ad0613d2a",
    "semantic_title": "experts don't cheat: learning what you don't know by predicting pairs",
    "citation_count": 5,
    "authors": [
      "Daniel D. Johnson",
      "Daniel Tarlow",
      "David Duvenaud",
      "Chris J. Maddison"
    ]
  },
  "https://proceedings.mlr.press/v235/jones24a.html": {
    "title": "Learning to Infer Generative Template Programs for Visual Concepts",
    "volume": "main",
    "abstract": "People grasp flexible visual concepts from a few examples. We explore a neurosymbolic system that learns how to infer programs that capture visual concepts in a domain-general fashion. We introduce Template Programs: programmatic expressions from a domain-specific language that specify structural and parametric patterns common to an input concept. Our framework supports multiple concept-related tasks, including few-shot generation and co-segmentation through parsing. We develop a learning paradigm that allows us to train networks that infer Template Programs directly from visual datasets that contain concept groupings. We run experiments across multiple visual domains: 2D layouts, Omniglot characters, and 3D shapes. We find that our method outperforms task-specific alternatives, and performs competitively against domain-specific approaches for the limited domains where they exist",
    "checked": true,
    "id": "cff9ee7bb4c9fb361811cc04c9d8c8b9afbe7e98",
    "semantic_title": "learning to infer generative template programs for visual concepts",
    "citation_count": 1,
    "authors": [
      "R. Kenny Jones",
      "Siddhartha Chaudhuri",
      "Daniel Ritchie"
    ]
  },
  "https://proceedings.mlr.press/v235/jonnarth24a.html": {
    "title": "Learning Coverage Paths in Unknown Environments with Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Coverage path planning (CPP) is the problem of finding a path that covers the entire free space of a confined area, with applications ranging from robotic lawn mowing to search-and-rescue. When the environment is unknown, the path needs to be planned online while mapping the environment, which cannot be addressed by offline planning methods that do not allow for a flexible path space. We investigate how suitable reinforcement learning is for this challenging problem, and analyze the involved components required to efficiently learn coverage paths, such as action space, input feature representation, neural network architecture, and reward function. We propose a computationally feasible egocentric map representation based on frontiers, and a novel reward term based on total variation to promote complete coverage. Through extensive experiments, we show that our approach surpasses the performance of both previous RL-based approaches and highly specialized methods across multiple CPP variations",
    "checked": true,
    "id": "330a0efe6863cc2d5e7431d1656bd48c0b04dc38",
    "semantic_title": "learning coverage paths in unknown environments with deep reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Arvi Jonnarth",
      "Jie Zhao",
      "Michael Felsberg"
    ]
  },
  "https://proceedings.mlr.press/v235/joo24a.html": {
    "title": "IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation",
    "volume": "main",
    "abstract": "Distribution shifts pose significant challenges for model calibration and model selection tasks in the unsupervised domain adaptation problem—a scenario where the goal is to perform well in a distribution shifted domain without labels. In this work, we tackle difficulties coming from distribution shifts by developing a novel importance weighted group accuracy estimator. Specifically, we present a new perspective of addressing the model calibration and model selection tasks by estimating the group accuracy. Then, we formulate an optimization problem for finding an importance weight that leads to an accurate group accuracy estimation with theoretical analyses. Our extensive experiments show that our approach improves state-of-the-art performances by 22% in the model calibration task and 14% in the model selection task",
    "checked": true,
    "id": "fc51f7342e78c55078adaf173539c6e70d55cff5",
    "semantic_title": "iw-gae: importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation",
    "citation_count": 1,
    "authors": [
      "Taejong Joo",
      "Diego Klabjan"
    ]
  },
  "https://proceedings.mlr.press/v235/jordahn24a.html": {
    "title": "Decoupling Feature Extraction and Classification Layers for Calibrated Neural Networks",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNN) have shown great promise in many classification applications, yet are widely known to have poorly calibrated predictions when they are over-parametrized. Improving DNN calibration without comprising on model accuracy is of extreme importance and interest in safety critical applications such as in the health-care sector. In this work, we show that decoupling the training of feature extraction layers and classification layers in over-parametrized DNN architectures such as Wide Residual Networks (WRN) and Vision Transformers (ViT) significantly improves model calibration whilst retaining accuracy, and at a low training cost. In addition, we show that placing a Gaussian prior on the last hidden layer outputs of a DNN, and training the model variationally in the classification training stage, even further improves calibration. We illustrate these methods improve calibration across ViT and WRN architectures for several image classification benchmark datasets",
    "checked": true,
    "id": "967cfb464fea6811520068f29ab80de76c698251",
    "semantic_title": "decoupling feature extraction and classification layers for calibrated neural networks",
    "citation_count": 0,
    "authors": [
      "Mikkel Jordahn",
      "Pablo M. Olmos"
    ]
  },
  "https://proceedings.mlr.press/v235/jordan24a.html": {
    "title": "Position: Benchmarking is Limited in Reinforcement Learning Research",
    "volume": "main",
    "abstract": "Novel reinforcement learning algorithms, or improvements on existing ones, are commonly justified by evaluating their performance on benchmark environments and are compared to an ever-changing set of standard algorithms. However, despite numerous calls for improvements, experimental practices continue to produce misleading or unsupported claims. One reason for the ongoing substandard practices is that conducting rigorous benchmarking experiments requires substantial computational time. This work investigates the sources of increased computation costs in rigorous experiment designs. We show that conducting rigorous performance benchmarks will likely have computational costs that are often prohibitive. As a result, we argue for using an additional experimentation paradigm to overcome the limitations of benchmarking",
    "checked": true,
    "id": "a82b6aa2e045051f8d5c63c0ab0bd8df1f8c6242",
    "semantic_title": "position: benchmarking is limited in reinforcement learning research",
    "citation_count": 0,
    "authors": [
      "Scott M. Jordan",
      "Adam White",
      "Bruno Castro Da Silva",
      "Martha White",
      "Philip S. Thomas"
    ]
  },
  "https://proceedings.mlr.press/v235/jovanovic24a.html": {
    "title": "Watermark Stealing in Large Language Models",
    "volume": "main",
    "abstract": "LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as hypothesized in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org",
    "checked": true,
    "id": "c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
    "semantic_title": "watermark stealing in large language models",
    "citation_count": 23,
    "authors": [
      "Nikola Jovanović",
      "Robin Staab",
      "Martin Vechev"
    ]
  },
  "https://proceedings.mlr.press/v235/ju24a.html": {
    "title": "Hypergraph-enhanced Dual Semi-supervised Graph Classification",
    "volume": "main",
    "abstract": "In this paper, we study semi-supervised graph classification, which aims at accurately predicting the categories of graphs in scenarios with limited labeled graphs and abundant unlabeled graphs. Despite the promising capability of graph neural networks (GNNs), they typically require a large number of costly labeled graphs, while a wealth of unlabeled graphs fail to be effectively utilized. Moreover, GNNs are inherently limited to encoding local neighborhood information using message-passing mechanisms, thus lacking the ability to model higher-order dependencies among nodes. To tackle these challenges, we propose a Hypergraph-Enhanced DuAL framework named HEAL for semi-supervised graph classification, which captures graph semantics from the perspective of the hypergraph and the line graph, respectively. Specifically, to better explore the higher-order relationships among nodes, we design a hypergraph structure learning to adaptively learn complex node dependencies beyond pairwise relations. Meanwhile, based on the learned hypergraph, we introduce a line graph to capture the interaction between hyperedges, thereby better mining the underlying semantic structures. Finally, we develop a relational consistency learning to facilitate knowledge transfer between the two branches and provide better mutual guidance. Extensive experiments on real-world graph datasets verify the effectiveness of the proposed method against existing state-of-the-art methods",
    "checked": true,
    "id": "5adde0ac3e8470ab02e9a2a5b5be78380db98d98",
    "semantic_title": "hypergraph-enhanced dual semi-supervised graph classification",
    "citation_count": 2,
    "authors": [
      "Wei Ju",
      "Zhengyang Mao",
      "Siyu Yi",
      "Yifang Qin",
      "Yiyang Gu",
      "Zhiping Xiao",
      "Yifan Wang",
      "Xiao Luo",
      "Ming Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/ju24b.html": {
    "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
    "volume": "main",
    "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall shorts in speech quality, similarity, and prosody. Considering that speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model, which generates attributes in each subspace following its corresponding prompt. With this factorization design, our method can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way. Experimental results show that our method outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility",
    "checked": true,
    "id": "575bf1f23a19620a8f696a965528e15d4833179a",
    "semantic_title": "naturalspeech 3: zero-shot speech synthesis with factorized codec and diffusion models",
    "citation_count": 86,
    "authors": [
      "Zeqian Ju",
      "Yuancheng Wang",
      "Kai Shen",
      "Xu Tan",
      "Detai Xin",
      "Dongchao Yang",
      "Eric Liu",
      "Yichong Leng",
      "Kaitao Song",
      "Siliang Tang",
      "Zhizheng Wu",
      "Tao Qin",
      "Xiangyang Li",
      "Wei Ye",
      "Shikun Zhang",
      "Jiang Bian",
      "Lei He",
      "Jinyu Li",
      "Sheng Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/juergens24a.html": {
    "title": "Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?",
    "volume": "main",
    "abstract": "Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-order loss minimization, and the relative (rather than absolute) nature of epistemic uncertainty measures",
    "checked": true,
    "id": "a3d1eec0988634cf6486165e36cf74ead3048a1e",
    "semantic_title": "is epistemic uncertainty faithfully represented by evidential deep learning methods?",
    "citation_count": 3,
    "authors": [
      "Mira Juergens",
      "Nis Meinert",
      "Viktor Bengs",
      "Eyke Hüllermeier",
      "Willem Waegeman"
    ]
  },
  "https://proceedings.mlr.press/v235/jun24a.html": {
    "title": "Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization",
    "volume": "main",
    "abstract": "Adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified. We report significant progress in addressing this issue in linear bandits in two respects. First, we propose a novel confidence set that is 'semi-adaptive' to the unknown sub-Gaussian parameter $\\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\\sqrt{d\\sigma_*^2 + \\sigma_0^2}$ where $d$ is the dimension and $\\sigma_0^2$ is the specified sub-Gaussian parameter (known) that can be much larger than $\\sigma_*^2$. This is a significant improvement over $\\sqrt{d\\sigma_0^2}$ of the standard confidence set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that this leads to an improved regret bound in linear bandits. Second, for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numerical performance upon prior art. We then apply this confidence set to develop, as we claim, the first practical variance-adaptive linear bandit algorithm via an optimistic approach, which is enabled by our novel regret analysis technique. Both of our confidence sets rely critically on ‘regret equality' from online learning. Our empirical evaluation in Bayesian optimization tasks shows that our algorithms demonstrate better or comparable performance compared to existing methods",
    "checked": true,
    "id": "c37461033e104c0e2a863fe8bf5d0e0913ceb1e5",
    "semantic_title": "noise-adaptive confidence sets for linear bandits and application to bayesian optimization",
    "citation_count": 1,
    "authors": [
      "Kwang-Sung Jun",
      "Jungtaek Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/jung24a.html": {
    "title": "Unsupervised Episode Generation for Graph Meta-learning",
    "volume": "main",
    "abstract": "We propose Unsupervised Episode Generation method called Neighbors as Queries (NaQ) to solve the Few-Shot Node-Classification (FSNC) task by unsupervised Graph Meta-learning. Doing so enables full utilization of the information of all nodes in a graph, which is not possible in current supervised meta-learning methods for FSNC due to the label-scarcity problem. In addition, unlike unsupervised Graph Contrastive Learning (GCL) methods that overlook the downstream task to be solved at the training phase resulting in vulnerability to class imbalance of a graph, we adopt the episodic learning framework that allows the model to be aware of the downstream task format, i.e., FSNC. The proposed NaQ is a simple but effective unsupervised episode generation method that randomly samples nodes from a graph to make a support set, followed by similarity-based sampling of nodes to make the corresponding query set. Since NaQ is model-agnostic, any existing supervised graph meta-learning methods can be trained in an unsupervised manner, while not sacrificing much of their performance or sometimes even improving them. Extensive experimental results demonstrate the effectiveness of our proposed unsupervised episode generation method for graph meta-learning towards the FSNC task. Our code is available at: https://github.com/JhngJng/NaQ-PyTorch",
    "checked": true,
    "id": "a7430dae143c96af570e4b80e74d9fbe003cf98b",
    "semantic_title": "unsupervised episode generation for graph meta-learning",
    "citation_count": 0,
    "authors": [
      "Jihyeong Jung",
      "Sangwoo Seo",
      "Sungwon Kim",
      "Chanyoung Park"
    ]
  },
  "https://proceedings.mlr.press/v235/jung24b.html": {
    "title": "PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRF) have shown remarkable performance in learning 3D scenes. However, NeRF exhibits vulnerability when confronted with distractors in the training images – unexpected objects are present only within specific views, such as moving entities like pedestrians or birds. Excluding distractors during dataset construction is a straightforward solution, but without prior knowledge of their types and quantities, it becomes prohibitively expensive. In this paper, we propose PruNeRF, a segment-centric dataset pruning framework via 3D spatial consistency, that effectively identifies and prunes the distractors. We first examine existing metrics for measuring pixel-wise distraction and introduce Influence Functions for more accurate measurements. Then, we assess 3D spatial consistency using a depth-based reprojection technique to obtain 3D-aware distraction. Furthermore, we incorporate segmentation for pixel-to-segment refinement, enabling more precise identification. Our experiments on benchmark datasets demonstrate that PruNeRF consistently outperforms state-of-the-art methods in robustness against distractors",
    "checked": true,
    "id": "26264c4274dd9566e61a9538da40b4392be7d7b6",
    "semantic_title": "prunerf: segment-centric dataset pruning via 3d spatial consistency",
    "citation_count": 1,
    "authors": [
      "Yeonsung Jung",
      "Heecheol Yun",
      "Joonhyung Park",
      "Jin-Hwa Kim",
      "Eunho Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/kabra24a.html": {
    "title": "Leveraging VLM-Based Pipelines to Annotate 3D Objects",
    "volume": "main",
    "abstract": "Pretrained vision language models (VLMs) present an opportunity to caption unlabeled 3D objects at scale. The leading approach to summarize VLM descriptions from different views of an object (Luo et al., 2023) relies on a language model (GPT4) to produce the final output. This text-based aggregation is susceptible to hallucinations as it merges potentially contradictory descriptions. We propose an alternative algorithm to marginalize over factors such as the viewpoint that affect the VLM's response. Instead of merging text-only responses, we utilize the VLM's joint image-text likelihoods. We show our probabilistic aggregation is not only more reliable and efficient, but sets the SoTA on inferring object types with respect to human-verified labels. The aggregated annotations are also useful for conditional inference; they improve downstream predictions (e.g., of object material) when the object's type is specified as an auxiliary text-based input. Such auxiliary inputs allow ablating the contribution of visual reasoning over visionless reasoning in an unsupervised setting. With these supervised and unsupervised evaluations, we show how a VLM-based pipeline can be leveraged to produce reliable annotations for 764K objects from the Objaverse dataset",
    "checked": true,
    "id": "854432bf4cda193f5dec27952b09013ef289c85d",
    "semantic_title": "leveraging vlm-based pipelines to annotate 3d objects",
    "citation_count": 1,
    "authors": [
      "Rishabh Kabra",
      "Loic Matthey",
      "Alexander Lerchner",
      "Niloy Mitra"
    ]
  },
  "https://proceedings.mlr.press/v235/kacham24a.html": {
    "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels",
    "volume": "main",
    "abstract": "The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models. Recent theoretical results indicate the intractability of sub-quadratic softmax attention approximation under reasonable complexity assumptions. This paper addresses this challenge by first demonstrating that polynomial attention with high degree can effectively replace softmax without sacrificing model quality. Next, we develop polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees. Crucially, our approach achieves this speedup without requiring the sparsification of attention matrices. We also present a block-based algorithm to apply causal masking efficiently. Combining these techniques, we provide PolySketchFormer, a practical linear-time Transformer architecture for language modeling that offers provable guarantees. We validate PolySketchFormer empirically by training language models capable of handling long contexts. These experiments utilize both synthetic and real-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32k and GPT-2 style models, our model achieves 2x speedup in training compared to FlashAttention of the fastest configuration, with no observed degradation in quality across our experiments",
    "checked": true,
    "id": "d9a2f89dc65b2cb7cd03cf2d37e2f67c6f72359c",
    "semantic_title": "polysketchformer: fast transformers via sketching polynomial kernels",
    "citation_count": 1,
    "authors": [
      "Praneeth Kacham",
      "Vahab Mirrokni",
      "Peilin Zhong"
    ]
  },
  "https://proceedings.mlr.press/v235/kadlecova24a.html": {
    "title": "Surprisingly Strong Performance Prediction with Neural Graph Features",
    "volume": "main",
    "abstract": "Performance prediction has been a key part of the neural architecture search (NAS) process, allowing to speed up NAS algorithms by avoiding resource-consuming network training. Although many performance predictors correlate well with ground truth performance, they require training data in the form of trained networks. Recently, zero-cost proxies have been proposed as an efficient method to estimate network performance without any training. However, they are still poorly understood, exhibit biases with network properties, and their performance is limited. Inspired by the drawbacks of zero-cost proxies, we propose neural graph features (GRAF), simple to compute properties of architectural graphs. GRAF offers fast and interpretable performance prediction while outperforming zero-cost proxies and other common encodings. In combination with other zero-cost proxies, GRAF outperforms most existing performance predictors at a fraction of the cost",
    "checked": true,
    "id": "22918eeafa3ebd6b838a2a102993540cfe8bf9a2",
    "semantic_title": "surprisingly strong performance prediction with neural graph features",
    "citation_count": 1,
    "authors": [
      "Gabriela Kadlecová",
      "Jovita Lukasik",
      "Martin Pilát",
      "Petra Vidnerová",
      "Mahmoud Safari",
      "Roman Neruda",
      "Frank Hutter"
    ]
  },
  "https://proceedings.mlr.press/v235/kai24a.html": {
    "title": "EvTexture: Event-driven Texture Enhancement for Video Super-Resolution",
    "volume": "main",
    "abstract": "Event-based vision has drawn increasing attention due to its unique characteristics, such as high temporal resolution and high dynamic range. It has been used in video super-resolution (VSR) recently to enhance the flow estimation and temporal alignment. Rather than for motion learning, we propose in this paper the first VSR method that utilizes event signals for texture enhancement. Our method, called EvTexture, leverages high-frequency details of events to better recover texture regions in VSR. In our EvTexture, a new texture enhancement branch is presented. We further introduce an iterative texture enhancement module to progressively explore the high-temporal-resolution event information for texture restoration. This allows for gradual refinement of texture regions across multiple iterations, leading to more accurate and rich high-resolution details. Experimental results show that our EvTexture achieves state-of-the-art performance on four datasets. For the Vid4 dataset with rich textures, our method can get up to 4.67dB gain compared with recent event-based methods. Code: https://github.com/DachunKai/EvTexture",
    "checked": true,
    "id": "df23a8c33c33cbff570097947f29da1c565d26fd",
    "semantic_title": "evtexture: event-driven texture enhancement for video super-resolution",
    "citation_count": 1,
    "authors": [
      "Dachun Kai",
      "Jiayao Lu",
      "Yueyi Zhang",
      "Xiaoyan Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/kaissis24a.html": {
    "title": "Beyond the Calibration Point: Mechanism Comparison in Differential Privacy",
    "volume": "main",
    "abstract": "In differentially private (DP) machine learning, the privacy guarantees of DP mechanisms are often reported and compared on the basis of a single $(\\varepsilon, \\delta)$-pair. This practice overlooks that DP guarantees can vary substantially even between mechanisms sharing a given $(\\varepsilon, \\delta)$, and potentially introduces privacy vulnerabilities which can remain undetected. This motivates the need for robust, rigorous methods for comparing DP guarantees in such cases. Here, we introduce the $\\Delta$-divergence between mechanisms which quantifies the worst-case excess privacy vulnerability of choosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP and in terms of a newly presented Bayesian interpretation. Moreover, as a generalisation of the Blackwell theorem, it is endowed with strong decision-theoretic foundations. Through application examples, we show that our techniques can facilitate informed decision-making and reveal gaps in the current understanding of privacy risks, as current practices in DP-SGD often result in choosing mechanisms with high excess privacy vulnerabilities",
    "checked": true,
    "id": "1a6b3e92a0918bae49ae49cf0f607fcbd974c695",
    "semantic_title": "beyond the calibration point: mechanism comparison in differential privacy",
    "citation_count": 1,
    "authors": [
      "Georgios Kaissis",
      "Stefan Kolek",
      "Borja Balle",
      "Jamie Hayes",
      "Daniel Rueckert"
    ]
  },
  "https://proceedings.mlr.press/v235/kalavasis24a.html": {
    "title": "Replicable Learning of Large-Margin Halfspaces",
    "volume": "main",
    "abstract": "We provide an efficient replicable algorithm for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell (STOC, 2022). We design the first dimension-independent replicable algorithm for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. (STOC, 2022) with respect to all the relevant parameters. Moreover, our algorithm has sample complexity that is optimal with respect to the accuracy parameter $\\epsilon$. Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun et al. (STOC 2023), we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\\tau$, but running time doubly exponential in $1/\\tau^2$ and worse sample complexity dependence on $\\epsilon$ than our previous algorithm. We then design an improved algorithm with better sample complexity than both of our previous algorithms and running time exponential in $1/\\tau^{2}.$",
    "checked": true,
    "id": "6c55cb3e0a0d390093aa11327f9a8ceb93f0ed91",
    "semantic_title": "replicable learning of large-margin halfspaces",
    "citation_count": 3,
    "authors": [
      "Alkis Kalavasis",
      "Amin Karbasi",
      "Kasper Green Larsen",
      "Grigoris Velegkas",
      "Felix Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/kalluri24a.html": {
    "title": "Tell, Don't Show: Language Guidance Eases Transfer Across Domains in Images and Videos",
    "volume": "main",
    "abstract": "We introduce LaGTran, a novel framework that utilizes text supervision to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain gaps. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiveness. To further extend the scope of our study beyond images, we introduce a new benchmark called Ego2Exo to study ego-exo transfer in videos and find that our language-aided approach LaGTran yields significant gains in this highly challenging and non-trivial transfer setting. Code, models, and proposed datasets are publicly available at https://tarun005.github.io/lagtran/",
    "checked": false,
    "id": "79cd5ce9ffbeb4b0575127272274bbbf27ecfadc",
    "semantic_title": "tell, don't show!: language guidance eases transfer across domains in images and videos",
    "citation_count": 2,
    "authors": [
      "Tarun Kalluri",
      "Bodhisattwa Prasad Majumder",
      "Manmohan Chandraker"
    ]
  },
  "https://proceedings.mlr.press/v235/kambhampati24a.html": {
    "title": "Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
    "volume": "main",
    "abstract": "We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of LLM-Modulo Frameworks that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications",
    "checked": true,
    "id": "6aeeedf23ba3a903325dc175a020bd7776194ca2",
    "semantic_title": "position: llms can't plan, but can help planning in llm-modulo frameworks",
    "citation_count": 15,
    "authors": [
      "Subbarao Kambhampati",
      "Karthik Valmeekam",
      "Lin Guan",
      "Mudit Verma",
      "Kaya Stechly",
      "Siddhant Bhambri",
      "Lucas Paul Saldyt",
      "Anil B Murthy"
    ]
  },
  "https://proceedings.mlr.press/v235/kamkari24a.html": {
    "title": "A Geometric Explanation of the Likelihood OOD Detection Paradox",
    "volume": "main",
    "abstract": "Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can be applied to normalizing flows and score-based diffusion models, and obtains results which match or surpass state-of-the-art OOD detection benchmarks using the same DGM backbones. Our code is available at our GitHub repository",
    "checked": true,
    "id": "f119a2370955d0087b9db32555671f05d6557e1a",
    "semantic_title": "a geometric explanation of the likelihood ood detection paradox",
    "citation_count": 6,
    "authors": [
      "Hamidreza Kamkari",
      "Brendan Leigh Ross",
      "Jesse C. Cresswell",
      "Anthony L. Caterini",
      "Rahul Krishnan",
      "Gabriel Loaiza-Ganem"
    ]
  },
  "https://proceedings.mlr.press/v235/kanamori24a.html": {
    "title": "Learning Decision Trees and Forests with Algorithmic Recourse",
    "volume": "main",
    "abstract": "This paper proposes a new algorithm for learning accurate tree-based models while ensuring the existence of recourse actions. Algorithmic Recourse (AR) aims to provide a recourse action for altering the undesired prediction result given by a model. Typical AR methods provide a reasonable action by solving an optimization task of minimizing the required effort among executable actions. In practice, however, such actions do not always exist for models optimized only for predictive performance. To alleviate this issue, we formulate the task of learning an accurate classification tree under the constraint of ensuring the existence of reasonable actions for as many instances as possible. Then, we propose an efficient top-down greedy algorithm by leveraging the adversarial training techniques. We also show that our proposed algorithm can be applied to the random forest, which is known as a popular framework for learning tree ensembles. Experimental results demonstrated that our method successfully provided reasonable actions to more instances than the baselines without significantly degrading accuracy and computational efficiency",
    "checked": true,
    "id": "806d87528300c74a0efc6b8d39c575489c4afc20",
    "semantic_title": "learning decision trees and forests with algorithmic recourse",
    "citation_count": 1,
    "authors": [
      "Kentaro Kanamori",
      "Takuya Takagi",
      "Ken Kobayashi",
      "Yuichi Ike"
    ]
  },
  "https://proceedings.mlr.press/v235/kang24a.html": {
    "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
    "volume": "main",
    "abstract": "Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models",
    "checked": true,
    "id": "32c260ddd7e2d0b05c58f2e67d244b8036699db4",
    "semantic_title": "c-rag: certified generation risks for retrieval-augmented language models",
    "citation_count": 19,
    "authors": [
      "Mintong Kang",
      "Nezihe Merve Gürel",
      "Ning Yu",
      "Dawn Song",
      "Bo Li"
    ]
  },
  "https://proceedings.mlr.press/v235/kang24b.html": {
    "title": "Think Before You Act: Decision Transformers with Working Memory",
    "volume": "main",
    "abstract": "Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture",
    "checked": true,
    "id": "26794d92b563088f18f52ecdbe08d3309bdf6dd5",
    "semantic_title": "think before you act: decision transformers with working memory",
    "citation_count": 0,
    "authors": [
      "Jikun Kang",
      "Romain Laroche",
      "Xingdi Yuan",
      "Adam Trischler",
      "Xue Liu",
      "Jie Fu"
    ]
  },
  "https://proceedings.mlr.press/v235/kang24c.html": {
    "title": "Certifiably Byzantine-Robust Federated Conformal Prediction",
    "volume": "main",
    "abstract": "Conformal prediction has shown impressive capacity in constructing statistically rigorous prediction sets for machine learning models with exchangeable data samples. The siloed datasets, coupled with the escalating privacy concerns related to local data sharing, have inspired recent innovations extending conformal prediction into federated environments with distributed data samples. However, this framework for distributed uncertainty quantification is susceptible to Byzantine failures. A minor subset of malicious clients can significantly compromise the practicality of coverage guarantees. To address this vulnerability, we introduce a novel framework Rob-FCP, which executes robust federated conformal prediction, effectively countering malicious clients capable of reporting arbitrary statistics with the conformal calibration process. We theoretically provide the conformal coverage bound of Rob-FCP in the Byzantine setting and show that the coverage of Rob-FCP is asymptotically close to the desired coverage level. We also propose a malicious client number estimator to tackle a more challenging setting where the number of malicious clients is unknown to the defender and theoretically shows its effectiveness. We empirically demonstrate the robustness of Rob-FCP against diverse proportions of malicious clients under a variety of Byzantine attacks on five standard benchmark and real-world healthcare datasets",
    "checked": true,
    "id": "7e6ea5b28f82f1dedfaaa2eba48a81d9d68b300f",
    "semantic_title": "certifiably byzantine-robust federated conformal prediction",
    "citation_count": 3,
    "authors": [
      "Mintong Kang",
      "Zhen Lin",
      "Jimeng Sun",
      "Cao Xiao",
      "Bo Li"
    ]
  },
  "https://proceedings.mlr.press/v235/kanoh24a.html": {
    "title": "Neural Tangent Kernels for Axis-Aligned Tree Ensembles",
    "volume": "main",
    "abstract": "While axis-aligned rules are known to induce an important inductive bias in machine learning models such as typical hard decision tree ensembles, theoretical understanding of the learning behavior is largely unrevealed due to the discrete nature of rules. To address this issue, we impose the axis-aligned constraint on soft trees, which relax the splitting process of decision trees and are trained using a gradient method, and present their Neural Tangent Kernel (NTK), which enables us to analytically describe the training behavior. We study two cases: imposing the axis-aligned constraint throughout the entire training process, and only at the initial state. Moreover, we extend the NTK framework to handle various tree architectures simultaneously, and prove that any axis-aligned non-oblivious tree ensemble can be transformed into axis-aligned oblivious tree ensembles with the same NTK. One can search for suitable tree architecture via Multiple Kernel Learning (MKL), and our numerical experiments show a variety of suitable features depending on the type of constraints. Our NTK analysis highlights both the theoretical and practical impacts of the axis-aligned constraint in tree ensemble learning",
    "checked": true,
    "id": "4a4c4545ac763d42d43cae5e2ba2ce6de9abebc7",
    "semantic_title": "neural tangent kernels for axis-aligned tree ensembles",
    "citation_count": 0,
    "authors": [
      "Ryuichi Kanoh",
      "Mahito Sugiyama"
    ]
  },
  "https://proceedings.mlr.press/v235/kapoor24a.html": {
    "title": "Position: On the Societal Impact of Open Foundation Models",
    "volume": "main",
    "abstract": "Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on open foundation models, defined here as those with broadly available model weights (e.g., Llama 3, Stable Diffusion XL). We identify five distinctive properties (e.g., greater customizability, poor monitoring) that mediate their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their marginal risk. Across several misuse vectors (e.g., cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work helps support a more grounded assessment of the societal impact of open foundation models by outlining what research is needed to empirically validate their theoretical benefits and risks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayash Kapoor",
      "Rishi Bommasani",
      "Kevin Klyman",
      "Shayne Longpre",
      "Ashwin Ramaswami",
      "Peter Cihon",
      "Aspen K Hopkins",
      "Kevin Bankston",
      "Stella Biderman",
      "Miranda Bogen",
      "Rumman Chowdhury",
      "Alex Engler",
      "Peter Henderson",
      "Yacine Jernite",
      "Seth Lazar",
      "Stefano Maffulli",
      "Alondra Nelson",
      "Joelle Pineau",
      "Aviya Skowron",
      "Dawn Song",
      "Victor Storchan",
      "Daniel Zhang",
      "Daniel E. Ho",
      "Percy Liang",
      "Arvind Narayanan"
    ]
  },
  "https://proceedings.mlr.press/v235/karakulev24a.html": {
    "title": "Adaptive Robust Learning using Latent Bernoulli Variables",
    "volume": "main",
    "abstract": "We present an adaptive approach for robust learning from corrupted training sets. We identify corrupted and non-corrupted samples with latent Bernoulli variables and thus formulate the learning problem as maximization of the likelihood where latent variables are marginalized. The resulting problem is solved via variational inference, using an efficient Expectation-Maximization based method. The proposed approach improves over the state-of-the-art by automatically inferring the corruption level, while adding minimal computational overhead. We demonstrate our robust learning method and its parameter-free nature on a wide variety of machine learning tasks including online learning and deep learning where it adapts to different levels of noise and maintains high prediction accuracy",
    "checked": true,
    "id": "89d8420fbb4c503dc7a09230fcc1b7cf0b717474",
    "semantic_title": "adaptive robust learning using latent bernoulli variables",
    "citation_count": 0,
    "authors": [
      "Aleksandr Karakulev",
      "Dave Zachariah",
      "Prashant Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/karamcheti24a.html": {
    "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
    "volume": "main",
    "abstract": "Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance – a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization, and challenge sets that probe properties such as hallucination; evaluations that provide fine-grained insight VLM capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and training from base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible training code, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open VLMs",
    "checked": true,
    "id": "f86f71cfd2e9682a56d7334736a7b8a0b1c70b45",
    "semantic_title": "prismatic vlms: investigating the design space of visually-conditioned language models",
    "citation_count": 55,
    "authors": [
      "Siddharth Karamcheti",
      "Suraj Nair",
      "Ashwin Balakrishna",
      "Percy Liang",
      "Thomas Kollar",
      "Dorsa Sadigh"
    ]
  },
  "https://proceedings.mlr.press/v235/karchmer24a.html": {
    "title": "On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning",
    "volume": "main",
    "abstract": "Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4). Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible separations between theoretical models of multimodal and unimodal learning. In particular, Lu (ALT '24) shows a computational separation, which is relevant to worst-case instances of the learning task. In this paper, we give a stronger average-case computational separation, where for \"typical\" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy. We then question how \"natural\" the average-case separation is. Would it be encountered in practice? To this end, we prove that under basic conditions, any given computational separation between average-case unimodal and multimodal learning tasks implies a corresponding cryptographic key agreement protocol. We suggest to interpret this as evidence that very strong computational advantages of multimodal learning may arise infrequently in practice, since they exist only for the \"pathological\" case of inherently cryptographic distributions. However, this does not apply to possible (super-polynomial) statistical advantages",
    "checked": true,
    "id": "5ed9f16b928b9f1f38cf3aa597a855a2bdc9fa49",
    "semantic_title": "on stronger computational separations between multimodal and unimodal machine learning",
    "citation_count": 2,
    "authors": [
      "Ari Karchmer"
    ]
  },
  "https://proceedings.mlr.press/v235/karczewski24a.html": {
    "title": "On the Generalization of Equivariant Graph Neural Networks",
    "volume": "main",
    "abstract": "$E(n)$-Equivariant Graph Neural Networks (EGNNs) are among the most widely used and successful models for representation learning on geometric graphs (e.g., 3D molecules). However, while the expressivity of EGNNs has been explored in terms of geometric variants of the Weisfeiler-Leman isomorphism test, characterizing their generalization capability remains open. In this work, we establish the first generalization bound for EGNNs. Our bound depicts a dependence on the weighted sum of logarithms of the spectral norms of the weight matrices (EGNN parameters). In addition, our main result reveals interesting novel insights: $i$) the spectral norms of the initial layers may impact generalization more than the final ones; $ii$) $\\varepsilon$-normalization is beneficial to generalization — confirming prior empirical evidence. We leverage these insights to introduce a spectral norm regularizer tailored to EGNNs. Experiments on real-world datasets substantiate our analysis, demonstrating a high correlation between theoretical and empirical generalization gaps and the effectiveness of the proposed regularization scheme",
    "checked": true,
    "id": "f8552938af06d3ed7b29f46aaa2d985fa50a5e2c",
    "semantic_title": "on the generalization of equivariant graph neural networks",
    "citation_count": 2,
    "authors": [
      "Rafal Karczewski",
      "Amauri H Souza",
      "Vikas Garg"
    ]
  },
  "https://proceedings.mlr.press/v235/kargin24a.html": {
    "title": "Infinite-Horizon Distributionally Robust Regret-Optimal Control",
    "volume": "main",
    "abstract": "We study the infinite-horizon distributionally robust (DR) control of linear systems with quadratic costs, where disturbances have unknown, possibly time-correlated distribution within a Wasserstein-2 ambiguity set. We aim to minimize the worst-case expected regret—the excess cost of a causal policy compared to a non-causal one with access to future disturbance. Though the optimal policy lacks a finite-order state-space realization (i.e., it is non-rational), it can be characterized by a finite-dimensional parameter. Leveraging this, we develop an efficient frequency-domain algorithm to compute this optimal control policy and present a convex optimization method to construct a near-optimal state-space controller that approximates the optimal non-rational controller in the $\\mathit{H}_\\infty$-norm. This approach avoids solving a computationally expensive semi-definite program (SDP) that scales with the time horizon in the finite-horizon setting",
    "checked": true,
    "id": "89d9cbe5dffa3de1d85711d0d02d8914bfa59ab0",
    "semantic_title": "infinite-horizon distributionally robust regret-optimal control",
    "citation_count": 2,
    "authors": [
      "Taylan Kargin",
      "Joudi Hajar",
      "Vikrant Malik",
      "Babak Hassibi"
    ]
  },
  "https://proceedings.mlr.press/v235/karimi-mamaghan24a.html": {
    "title": "Challenges and Considerations in the Evaluation of Bayesian Causal Discovery",
    "volume": "main",
    "abstract": "Representing uncertainty in causal discovery is a crucial component for experimental design, and more broadly, for safe and reliable causal decision making. Bayesian Causal Discovery (BCD) offers a principled approach to encapsulating this uncertainty. Unlike non-Bayesian causal discovery, which relies on a single estimated causal graph and model parameters for assessment, evaluating BCD presents challenges due to the nature of its inferred quantity – the posterior distribution. As a result, the research community has proposed various metrics to assess the quality of the approximate posterior. However, there is, to date, no consensus on the most suitable metric(s) for evaluation. In this work, we reexamine this question by dissecting various metrics and understanding their limitations. Through extensive empirical evaluation, we find that many existing metrics fail to exhibit a strong correlation with the quality of approximation to the true posterior, especially in scenarios with low sample sizes where BCD is most desirable. We highlight the suitability (or lack thereof) of these metrics under two distinct factors: the identifiability of the underlying causal model and the quantity of available data. Both factors affect the entropy of the true posterior, indicating that the current metrics are less fitting in settings of higher entropy. Our findings underline the importance of a more nuanced evaluation of new methods by taking into account the nature of the true posterior, as well as guide and motivate the development of new evaluation procedures for this challenge",
    "checked": true,
    "id": "db392d13e6a9487979449bf6609a34d36b6f06c6",
    "semantic_title": "challenges and considerations in the evaluation of bayesian causal discovery",
    "citation_count": 0,
    "authors": [
      "Amir Mohammad Karimi Mamaghan",
      "Panagiotis Tigas",
      "Karl Henrik Johansson",
      "Yarin Gal",
      "Yashas Annadani",
      "Stefan Bauer"
    ]
  },
  "https://proceedings.mlr.press/v235/kariyappa24a.html": {
    "title": "Progressive Inference: Explaining Decoder-Only Sequence Classification Models Using Intermediate Predictions",
    "volume": "main",
    "abstract": "This paper proposes Progressive inference–a framework to explain the predictions of decoder-only transformer models trained to perform sequence classification tasks. Our work is based on the insight that the classification head of a decoder-only model can be used to make intermediate predictions by evaluating them at different points in the input sequence. Due to the masked attention mechanism used in decoder-only models, these intermediate predictions only depend on the tokens seen before the inference point, allowing us to obtain the model's prediction on a masked input sub-sequence, with negligible computational overheads. We develop two methods to provide sub-sequence level attributions using this core insight. First, we propose Single Pass-Progressive Inference (SP-PI) to compute attributions by simply taking the difference between intermediate predictions. Second, we exploit a connection with Kernel SHAP to develop Multi Pass-Progressive Inference (MP-PI); this uses intermediate predictions from multiple masked versions of the input to compute higher-quality attributions that approximate SHAP values. We perform studies on several text classification datasets to demonstrate that our proposal provides better explanations compared to prior work, both in the single-pass and multi-pass settings",
    "checked": true,
    "id": "4a05b76187ee07c82edf2d4c2961586ebe854714",
    "semantic_title": "progressive inference: explaining decoder-only sequence classification models using intermediate predictions",
    "citation_count": 0,
    "authors": [
      "Sanjay Kariyappa",
      "Freddy Lecue",
      "Saumitra Mishra",
      "Christopher Pond",
      "Daniele Magazzeni",
      "Manuela Veloso"
    ]
  },
  "https://proceedings.mlr.press/v235/karl24a.html": {
    "title": "Position: Embracing Negative Results in Machine Learning",
    "volume": "main",
    "abstract": "Publications proposing novel machine learning methods are often primarily rated by exhibited predictive performance on selected problems. In this position paper we argue that predictive performance alone is not a good indicator for the worth of a publication. Using it as such even fosters problems like inefficiencies of the machine learning research community as a whole and setting wrong incentives for researchers. We therefore put out a call for the publication of \"negative\" results, which can help alleviate some of these problems and improve the scientific output of the machine learning research community. To substantiate our position, we present the advantages of publishing negative results and provide concrete measures for the community to move towards a paradigm where their publication is normalized",
    "checked": true,
    "id": "5ba24e06a8ddcce6bccac5f79058d7e62668e9b3",
    "semantic_title": "position: embracing negative results in machine learning",
    "citation_count": 0,
    "authors": [
      "Florian Karl",
      "Malte Kemeter",
      "Gabriel Dax",
      "Paulina Sierak"
    ]
  },
  "https://proceedings.mlr.press/v235/karuvally24a.html": {
    "title": "Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Traveling waves are a fundamental phenomenon in the brain, playing a crucial role in short-term information storage. In this study, we leverage the concept of traveling wave dynamics within a neural lattice to formulate a theoretical model of neural working memory in Recurrent Neural Networks (RNNs), study its properties, and its real world implications in AI. The proposed model diverges from traditional approaches, which assume information storage in static, register-like locations updated by interference. Instead, the model stores data as waves that is updated by the wave's boundary conditions. We rigorously examine the model's capabilities in representing and learning state histories, which are vital for learning history-dependent dynamical systems. The findings reveal that the model reliably stores external information and enhances the learning process by addressing the diminishing gradient problem of RNNs. To understand the model's real-world applicability, we explore two cases: linear boundary condition and non-linear, self-attention-driven boundary condition. The experiments reveal that the linear scenario is effectively learned by RNNs through backpropagation when modeling history-dependent dynamical systems. Conversely, the non-linear scenario parallels an attention-only transformer. Collectively, our findings suggest the broader relevance of traveling waves in AI and its potential in advancing neural network architectures",
    "checked": true,
    "id": "e253d32de875c4bcde644255c77adf476dcdf91f",
    "semantic_title": "hidden traveling waves bind working memory variables in recurrent neural networks",
    "citation_count": 3,
    "authors": [
      "Arjun Karuvally",
      "Terrence Sejnowski",
      "Hava T Siegelmann"
    ]
  },
  "https://proceedings.mlr.press/v235/kato24a.html": {
    "title": "Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choice",
    "volume": "main",
    "abstract": "This study designs an adaptive experiment for efficiently estimating average treatment effects (ATEs). In each round of our adaptive experiment, an experimenter sequentially samples an experimental unit, assigns a treatment, and observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using the gathered samples. The objective is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose optimizing the covariate density as well as the propensity score. First, we derive the efficient covariate density and propensity score that minimize the semiparametric efficiency bound and find that optimizing both covariate density and propensity score minimizes the semiparametric efficiency bound more effectively than optimizing only the propensity score. Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment. Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound",
    "checked": false,
    "id": "ab7dc1dc7ff64e93afcf7af56d4e1bd8ef07a0f6",
    "semantic_title": "active adaptive experimental design for treatment effect estimation with covariate choices",
    "citation_count": 0,
    "authors": [
      "Masahiro Kato",
      "Akihiro Oga",
      "Wataru Komatsubara",
      "Ryo Inokuchi"
    ]
  },
  "https://proceedings.mlr.press/v235/kaufman24a.html": {
    "title": "First-Order Manifold Data Augmentation for Regression Learning",
    "volume": "main",
    "abstract": "Data augmentation (DA) methods tailored to specific domains generate synthetic samples by applying transformations that are appropriate for the characteristics of the underlying data domain, such as rotations on images and time warping on time series data. In contrast, domain-independent approaches, e.g. mixup, are applicable to various data modalities, and as such they are general and versatile. While regularizing classification tasks via DA is a well-explored research topic, the effect of DA on regression problems received less attention. To bridge this gap, we study the problem of domain-independent augmentation for regression, and we introduce FOMA: a new data-driven domain-independent data augmentation method. Essentially, our approach samples new examples from the tangent planes of the train distribution. Augmenting data in this way aligns with the network tendency towards capturing the dominant features of its input signals. We evaluate FOMA on in-distribution generalization and out-of-distribution robustness benchmarks, and we show that it improves the generalization of several neural architectures. We also find that strong baselines based on mixup are less effective in comparison to our approach. Our code is publicly available at https://github.com/azencot-group/FOMA",
    "checked": true,
    "id": "6869cca20e125ffe74c5e280b2d895235f7c9095",
    "semantic_title": "first-order manifold data augmentation for regression learning",
    "citation_count": 2,
    "authors": [
      "Ilya Kaufman",
      "Omri Azencot"
    ]
  },
  "https://proceedings.mlr.press/v235/kaushik24a.html": {
    "title": "Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance",
    "volume": "main",
    "abstract": "Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pre-trained encoders, and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation strategies to mitigate the issue. Our work sheds light on the class-dependent effects of learning, and provides new insights into how state-of-the-art pre-trained features may have unknown biases that can be diagnosed through their spectra",
    "checked": true,
    "id": "f42cfe01de6d26c9f2d860c073eeb97392a98b2a",
    "semantic_title": "balanced data, imbalanced spectra: unveiling class disparities with spectral imbalance",
    "citation_count": 1,
    "authors": [
      "Chiraag Kaushik",
      "Ran Liu",
      "Chi-Heng Lin",
      "Amrit Khera",
      "Matthew Y Jin",
      "Wenrui Ma",
      "Vidya Muthukumar",
      "Eva L Dyer"
    ]
  },
  "https://proceedings.mlr.press/v235/kazadi24a.html": {
    "title": "Pluvial Flood Emulation with Hydraulics-informed Message Passing",
    "volume": "main",
    "abstract": "Machine Learning (ML) has emerged as a promising alternative to numerical methods for physics-based simulation due to its flexibility and efficiency. Flood modeling is a key case study for ML-based simulation due to its relevance as a tool for supporting preventive and emergency measures to mitigate flood risks. However, the complexity of the topography or domain (ground elevation) and the sparsity of the time-evolving precipitations (external forcing) can be challenging for most existing ML approaches for simulating flooding processes in space and time. Another critical challenge is incorporating physics domain knowledge (hydraulics) into these data-driven models. This paper addresses these challenges by introducing a hydraulics-informed graph neural network for flood simulation. Given a (geographical) region and precipitation data, our model predicts water depths in an auto-regressive fashion. We propose a message-passing framework inspired by the conservation of momentum and mass expressed in the shallow-water equations, which describe the physical process of a flooding event. Empirical results on a dataset covering 9 regions and 7 historical precipitation events demonstrate that our model outperforms the best baseline, and can capture the propagation of water flow more effectively, especially at the very early stage of the flooding event when the amount of water in the domain is scarce. Differently from some of the most recent methods for ML-based simulation, which tend to work well only when the domain is a smooth surface (e.g., flat terrain), we show that our solution achieves accurate results for real ground elevation data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnold Kazadi",
      "James Doss-Gollin",
      "Arlei Lopes Da Silva"
    ]
  },
  "https://proceedings.mlr.press/v235/ke24a.html": {
    "title": "Accelerating Convergence in Bayesian Few-Shot Classification",
    "volume": "main",
    "abstract": "Bayesian few-shot classification has been a focal point in the field of few-shot learning. This paper seamlessly integrates mirror descent-based variational inference into Gaussian process-based few-shot classification, addressing the challenge of non-conjugate inference. By leveraging non-Euclidean geometry, mirror descent achieves accelerated convergence by providing the steepest descent direction along the corresponding manifold. It also exhibits the parameterization invariance property concerning the variational distribution. Experimental results demonstrate competitive classification accuracy, improved uncertainty quantification, and faster convergence compared to baseline models. Additionally, we investigate the impact of hyperparameters and components. Code is publicly available at https://github.com/keanson/MD-BSFC",
    "checked": true,
    "id": "817d1f230278d23603abf752b58f3d4cff978e0a",
    "semantic_title": "accelerating convergence in bayesian few-shot classification",
    "citation_count": 0,
    "authors": [
      "Tianjun Ke",
      "Haoqun Cao",
      "Feng Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/ke24b.html": {
    "title": "An Improved Finite-time Analysis of Temporal Difference Learning with Deep Neural Networks",
    "volume": "main",
    "abstract": "Temporal difference (TD) learning algorithms with neural network function parameterization have well-established empirical success in many practical large-scale reinforcement learning tasks. However, theoretical understanding of these algorithms remains challenging due to the nonlinearity of the action-value approximation. In this paper, we develop an improved non-asymptotic analysis of the neural TD method with a general $L$-layer neural network. New proof techniques are developed and an improved new $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ sample complexity is derived. To our best knowledge, this is the first finite-time analysis of neural TD that achieves an $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ complexity under the Markovian sampling, as opposed to the best known $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ complexity in the existing literature",
    "checked": true,
    "id": "98dc2f0fd77d7d3c28eda7a36f1fd150958c71a3",
    "semantic_title": "an improved finite-time analysis of temporal difference learning with deep neural networks",
    "citation_count": 0,
    "authors": [
      "Zhifa Ke",
      "Zaiwen Wen",
      "Junyu Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/ke24c.html": {
    "title": "DUPLEX: Dual GAT for Complex Embedding of Directed Graphs",
    "volume": "main",
    "abstract": "Current directed graph embedding methods build upon undirected techniques but often inadequately capture directed edge information, leading to challenges such as: (1) Suboptimal representations for nodes with low in/out-degrees, due to the insufficient neighbor interactions; (2) Limited inductive ability for representing new nodes post-training; (3) Narrow generalizability, as training is overly coupled with specific tasks. In response, we propose DUPLEX, an inductive framework for complex embeddings of directed graphs. It (1) leverages Hermitian adjacency matrix decomposition for comprehensive neighbor integration, (2) employs a dual GAT encoder for directional neighbor modeling, and (3) features two parameter-free decoders to decouple training from particular tasks. DUPLEX outperforms state-of-the-art models, especially for nodes with sparse connectivity, and demonstrates robust inductive capability and adaptability across various tasks. The code will be available upon publication",
    "checked": true,
    "id": "82b5653a5ddbdb2c4958ae147e85adf458ba43eb",
    "semantic_title": "duplex: dual gat for complex embedding of directed graphs",
    "citation_count": 2,
    "authors": [
      "Zhaoru Ke",
      "Hang Yu",
      "Jianguo Li",
      "Haipeng Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/kedia24a.html": {
    "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
    "volume": "main",
    "abstract": "In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 1000 layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across encoder-only, decoder-only and encoder-decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for Image Classification",
    "checked": true,
    "id": "587bf8566bebe2a1b4950dff1a10a762d524c5d3",
    "semantic_title": "transformers get stable: an end-to-end signal propagation theory for language models",
    "citation_count": 0,
    "authors": [
      "Akhil Kedia",
      "Mohd Abbas Zaidi",
      "Sushil Khyalia",
      "Jungho Jung",
      "Harshith Goka",
      "Haejun Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/kerger24a.html": {
    "title": "A Universal Transfer Theorem for Convex Optimization Algorithms Using Inexact First-order Oracles",
    "volume": "main",
    "abstract": "Given any algorithm for convex optimization that uses exact first-order information (i.e., function values and subgradients), we show how to use such an algorithm to solve the problem with access to inexact first-order information. This is done in a \"black-box\" manner without knowledge of the internal workings of the algorithm. This complements previous work that considers the performance of specific algorithms like (accelerated) gradient descent with inexact information. In particular, our results apply to a wider range of algorithms beyond variants of gradient descent, e.g., projection-free methods, cutting-plane methods, or any other first-order methods formulated in the future. Further, they also apply to algorithms that handle structured nonconvexities like mixed-integer decision variables",
    "checked": true,
    "id": "0bc374b25a88ff1305b3f1c50f2ae03d67965d4b",
    "semantic_title": "a universal transfer theorem for convex optimization algorithms using inexact first-order oracles",
    "citation_count": 0,
    "authors": [
      "Phillip Kerger",
      "Marco Molinaro",
      "Hongyi Jiang",
      "Amitabh Basu"
    ]
  },
  "https://proceedings.mlr.press/v235/keswani24a.html": {
    "title": "Fair Classification with Partial Feedback: An Exploration-Based Data Collection Approach",
    "volume": "main",
    "abstract": "In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a \"desired\" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees and encode context-specific group fairness properties. Evaluation on real-world datasets shows that this approach consistently boosts the quality of collected outcome data and improves the fraction of true positives for all groups, with only a small reduction in predictive utility",
    "checked": false,
    "id": "a02dbf0c1d2debc1a825e55f17958853eb3d40aa",
    "semantic_title": "fair classification with partial feedback: an exploration-based data-collection approach",
    "citation_count": 0,
    "authors": [
      "Vijay Keswani",
      "Anay Mehrotra",
      "L. Elisa Celis"
    ]
  },
  "https://proceedings.mlr.press/v235/khalafi24a.html": {
    "title": "Neural Tangent Kernels Motivate Cross-Covariance Graphs in Neural Networks",
    "volume": "main",
    "abstract": "Neural tangent kernels (NTKs) provide a theoretical regime to analyze the learning and generalization behavior of over-parametrized neural networks. For a supervised learning task, the association between the eigenvectors of the NTK and given data (a concept referred to as alignment in this paper) can govern the rate of convergence of gradient descent, as well as generalization to unseen data. Building upon this concept and leveraging the structure of NTKs for graph neural networks (GNNs), we theoretically investigate NTKs and alignment, where our analysis reveals that optimizing the alignment translates to optimizing the graph representation or the graph shift operator (GSO) in a GNN. Our results further establish theoretical guarantees on the optimality of the alignment for a two-layer GNN and these guarantees are characterized by the graph shift operator being a function of the cross-covariance between the input and the output data. The theoretical insights drawn from the analysis of NTKs are validated by our experiments focused on a multi-variate time series prediction task for a publicly available dataset. Specifically, they demonstrate that GNN-based learning models that operate on the cross-covariance matrix indeed outperform those that operate on the covariance matrix estimated from only the input data",
    "checked": true,
    "id": "7675d261c388fa2a1da23cb20de0c12c182a71be",
    "semantic_title": "neural tangent kernels motivate cross-covariance graphs in neural networks",
    "citation_count": 0,
    "authors": [
      "Shervin Khalafi",
      "Saurabh Sihag",
      "Alejandro Ribeiro"
    ]
  },
  "https://proceedings.mlr.press/v235/khaled24a.html": {
    "title": "Tuning-Free Stochastic Optimization",
    "volume": "main",
    "abstract": "Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of \"tuning-free\" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved. For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of SGD that matches the best-known high-probability convergence rate for tuned SGD at only an additional polylogarithmic cost. However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned SGD with high probability",
    "checked": true,
    "id": "bc14e62aaf5c2dfe1259d67c1a03d19063e7fa9d",
    "semantic_title": "tuning-free stochastic optimization",
    "citation_count": 6,
    "authors": [
      "Ahmed Khaled",
      "Chi Jin"
    ]
  },
  "https://proceedings.mlr.press/v235/khan24a.html": {
    "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
    "volume": "main",
    "abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth",
    "checked": true,
    "id": "e5d297461c53153b9b7aae099c146741e97322b8",
    "semantic_title": "debating with more persuasive llms leads to more truthful answers",
    "citation_count": 39,
    "authors": [
      "Akbir Khan",
      "John Hughes",
      "Dan Valentine",
      "Laura Ruis",
      "Kshitij Sachan",
      "Ansh Radhakrishnan",
      "Edward Grefenstette",
      "Samuel R. Bowman",
      "Tim Rocktäschel",
      "Ethan Perez"
    ]
  },
  "https://proceedings.mlr.press/v235/khan24b.html": {
    "title": "Off-policy Evaluation Beyond Overlap: Sharp Partial Identification Under Smoothness",
    "volume": "main",
    "abstract": "Off-policy evaluation, and the complementary problem of policy learning, use historical data collected under a logging policy to estimate and/or optimize the value of a target policy. Methods for these tasks typically assume overlap between the target and logging policy, enabling solutions based on importance weighting and/or imputation. Absent such an overlap assumption, existing work either relies on a well-specified model or optimizes needlessly conservative bounds. In this work, we develop methods for no-overlap policy evaluation without a well-specified model, relying instead on non-parametric assumptions on the expected outcome, with a particular focus on Lipschitz smoothness. Under such assumptions we are able to provide sharp bounds on the off-policy value, along with optimal estimators of those bounds. For Lipschitz smoothness, we construct a pair of linear programs that upper and lower bound the contribution of the no-overlap region to the off-policy value. We show that these programs have a concise closed form solution, and that their solutions converge under the Lipschitz assumption to the sharp partial identification bounds at a minimax optimal rate, up to log factors. We demonstrate the effectiveness our methods on two semi-synthetic examples, and obtain informative and valid bounds that are tighter than those possible without smoothness assumptions",
    "checked": true,
    "id": "39bb5c329330cafcbbe5921e8d58b2e45673c287",
    "semantic_title": "off-policy evaluation beyond overlap: sharp partial identification under smoothness",
    "citation_count": 0,
    "authors": [
      "Samir Khan",
      "Martin Saveski",
      "Johan Ugander"
    ]
  },
  "https://proceedings.mlr.press/v235/khona24a.html": {
    "title": "Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
    "volume": "main",
    "abstract": "Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. To unravel the underlying mechanisms of stepwise inference we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. We find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy trade-off in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon",
    "checked": true,
    "id": "5c4c0b687ae98e3292a1cdf3772b105213dad7af",
    "semantic_title": "towards an understanding of stepwise inference in transformers: a synthetic graph navigation model",
    "citation_count": 4,
    "authors": [
      "Mikail Khona",
      "Maya Okawa",
      "Jan Hula",
      "Rahul Ramesh",
      "Kento Nishi",
      "Robert P. Dick",
      "Ekdeep Singh Lubana",
      "Hidenori Tanaka"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24a.html": {
    "title": "Can Machines Learn the True Probabilities?",
    "volume": "main",
    "abstract": "When there exists uncertainty, AI machines are designed to make decisions so as to reach the best expected outcomes. Expectations are based on true facts about the objective environment the machines interact with, and those facts can be encoded into AI models in the form of true objective probability functions. Accordingly, AI models involve probabilistic machine learning in which the probabilities should be objectively interpreted. We prove under some basic assumptions when machines can learn the true objective probabilities, if any, and when machines cannot learn them",
    "checked": true,
    "id": "4e066f3dac10ca49aff4983439b5b945eedf823e",
    "semantic_title": "can machines learn the true probabilities?",
    "citation_count": 1,
    "authors": [
      "Jinsook Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24b.html": {
    "title": "Gaussian Plane-Wave Neural Operator for Electron Density Estimation",
    "volume": "main",
    "abstract": "This work studies machine learning for electron density prediction, which is fundamental for understanding chemical systems and density functional theory (DFT) simulations. To this end, we introduce the Gaussian plane-wave neural operator (GPWNO), which operates in the infinite-dimensional functional space using the plane-wave and Gaussian-type orbital bases, widely recognized in the context of DFT. In particular, both high- and low-frequency components of the density can be effectively represented due to the complementary nature of the two bases. Extensive experiments on QM9, MD, and material project datasets demonstrate GPWNO's superior performance over ten baselines",
    "checked": true,
    "id": "7b931191278c80989f01185a9e52dd880565819b",
    "semantic_title": "gaussian plane-wave neural operator for electron density estimation",
    "citation_count": 2,
    "authors": [
      "Seongsu Kim",
      "Sungsoo Ahn"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24c.html": {
    "title": "LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging",
    "volume": "main",
    "abstract": "Recent works show that reducing the number of layers in a convolutional neural network can enhance efficiency while maintaining the performance of the network. Existing depth compression methods remove redundant non-linear activation functions and merge the consecutive convolution layers into a single layer. However, these methods suffer from a critical drawback; the kernel size of the merged layers becomes larger, significantly undermining the latency reduction gained from reducing the depth of the network. We show that this problem can be addressed by jointly pruning convolution layers and activation functions. To this end, we propose LayerMerge, a novel depth compression method that selects which activation layers and convolution layers to remove, to achieve a desired inference speed-up while minimizing performance loss. Since the corresponding selection problem involves an exponential search space, we formulate a novel surrogate optimization problem and efficiently solve it via dynamic programming. Empirical results demonstrate that our method consistently outperforms existing depth compression and layer pruning methods on various network architectures, both on image classification and generation tasks. We release the code at https://github.com/snu-mllab/LayerMerge",
    "checked": true,
    "id": "6a7dc1c5f60f6a4af44cab50a01e1a828c46b64a",
    "semantic_title": "layermerge: neural network depth compression through layer pruning and merging",
    "citation_count": 0,
    "authors": [
      "Jinuk Kim",
      "Marwa El Halabi",
      "Mingi Ji",
      "Hyun Oh Song"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24d.html": {
    "title": "CARTE: Pretraining and Transfer for Tabular Learning",
    "volume": "main",
    "abstract": "Pretrained deep-learning models are the go-to solution for images or text. However, for tabular data the standard is still to train tree-based models. Indeed, transfer learning on tables hits the challenge of data integration: finding correspondences, correspondences in the entries (entity matching) where different words may denote the same entity, correspondences across columns (schema matching), which may come in different orders, names... We propose a neural architecture that does not need such correspondences. As a result, we can pretrain it on background data that has not been matched. The architecture –CARTE for Context Aware Representation of Table Entries– uses a graph representation of tabular (or relational) data to process tables with different columns, string embedding of entries and columns names to model an open vocabulary, and a graph-attentional network to contextualize entries with column names and neighboring entries. An extensive benchmark shows that CARTE facilitates learning, outperforming a solid set of baselines including the best tree-based models. CARTE also enables joint learning across tables with unmatched columns, enhancing a small table with bigger ones. CARTE opens the door to large pretrained models for tabular data",
    "checked": true,
    "id": "659fe890e963c574c083f1b60754a071d945b5b2",
    "semantic_title": "carte: pretraining and transfer for tabular learning",
    "citation_count": 5,
    "authors": [
      "Myung Jun Kim",
      "Leo Grinsztajn",
      "Gael Varoquaux"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24e.html": {
    "title": "Achieving Lossless Gradient Sparsification via Mapping to Alternative Space in Federated Learning",
    "volume": "main",
    "abstract": "Handling the substantial communication burden in federated learning (FL) still remains a significant challenge. Although recent studies have attempted to compress the local gradients to address this issue, they typically perform compression only within the original parameter space, which may potentially limit the fundamental compression rate of the gradient. In this paper, instead of restricting our scope to a fixed traditional space, we consider an alternative space that provides an improved compressibility of the gradient. To this end, we utilize the structures of input activation and output gradient in designing our mapping function to a new space, which enables lossless gradient sparsification, i.e., mapping the gradient to our new space induces a greater number of near-zero elements without any loss of information. In light of this attribute, employing sparsification-based compressors in our new space allows for more aggressive compression with minimal information loss than the baselines. More surprisingly, our model even reaches higher accuracies than the full gradient uploading strategy in some cases, an extra benefit for utilizing the new space. We also theoretically confirm that our approach does not alter the existing, best known convergence rate of FL thanks to the orthogonal transformation properties of our mapping",
    "checked": true,
    "id": "9c5315a5f99a4c1c620bf7127bc41d03725353fc",
    "semantic_title": "achieving lossless gradient sparsification via mapping to alternative space in federated learning",
    "citation_count": 0,
    "authors": [
      "Do-Yeon Kim",
      "Dong-Jun Han",
      "Jun Seo",
      "Jaekyun Moon"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24f.html": {
    "title": "SqueezeLLM: Dense-and-Sparse Quantization",
    "volume": "main",
    "abstract": "Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sehoon Kim",
      "Coleman Richard Charles Hooper",
      "Amir Gholami",
      "Zhen Dong",
      "Xiuyu Li",
      "Sheng Shen",
      "Michael W. Mahoney",
      "Kurt Keutzer"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24g.html": {
    "title": "Active Label Correction for Semantic Segmentation with Foundation Models",
    "volume": "main",
    "abstract": "Training and validating models for semantic segmentation require datasets with pixel-wise annotations, which are notoriously labor-intensive. Although useful priors such as foundation models or crowdsourced datasets are available, they are error-prone. We hence propose an effective framework of active label correction (ALC) based on a design of correction query to rectify pseudo labels of pixels, which in turn is more annotator-friendly than the standard one inquiring to classify a pixel directly according to our theoretical analysis and user study. Specifically, leveraging foundation models providing useful zero-shot predictions on pseudo labels and superpixels, our method comprises two key techniques: (i) an annotator-friendly design of correction query with the pseudo labels, and (ii) an acquisition function looking ahead label expansions based on the superpixels. Experimental results on PASCAL, Cityscapes, and Kvasir-SEG datasets demonstrate the effectiveness of our ALC framework, outperforming prior methods for active semantic segmentation and label correction. Notably, utilizing our method, we obtained a revised dataset of PASCAL by rectifying errors in 2.6 million pixels in PASCAL dataset",
    "checked": true,
    "id": "c07de46ee1f89a2b1d842df95c38d1d6147523f2",
    "semantic_title": "active label correction for semantic segmentation with foundation models",
    "citation_count": 0,
    "authors": [
      "Hoyoung Kim",
      "Sehyun Hwang",
      "Suha Kwak",
      "Jungseul Ok"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24h.html": {
    "title": "ODIM: Outlier Detection via Likelihood of Under-Fitted Generative Models",
    "volume": "main",
    "abstract": "The unsupervised outlier detection (UOD) problem refers to a task to identify inliers given training data which contain outliers as well as inliers, without any labeled information about inliers and outliers. It has been widely recognized that using fully-trained likelihood-based deep generative models (DGMs) often results in poor performance in distinguishing inliers from outliers. In this study, we claim that the likelihood itself could serve as powerful evidence for identifying inliers in UOD tasks, provided that DGMs are carefully under-fitted. Our approach begins with a novel observation called the inlier-memorization (IM) effect–when training a deep generative model with data including outliers, the model initially memorizes inliers before outliers. Based on this finding, we develop a new method called the outlier detection via the IM effect (ODIM). Remarkably, the ODIM requires only a few updates, making it computationally efficient–at least tens of times faster than other deep-learning-based algorithms. Also, the ODIM filters out outliers excellently, regardless of the data type, including tabular, image, and text data. To validate the superiority and efficiency of our method, we provide extensive empirical analyses on close to 60 datasets",
    "checked": true,
    "id": "82cd319ebd1cacd64b9a6d310923863c713a8e89",
    "semantic_title": "odim: outlier detection via likelihood of under-fitted generative models",
    "citation_count": 0,
    "authors": [
      "Dongha Kim",
      "Jaesung Hwang",
      "Jongjin Lee",
      "Kunwoong Kim",
      "Yongdai Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24i.html": {
    "title": "Hybrid Neural Representations for Spherical Data",
    "volume": "main",
    "abstract": "In this paper, we study hybrid neural representations for spherical data, a domain of increasing relevance in scientific research. In particular, our work focuses on weather and climate data as well as cosmic microwave background (CMB) data. Although previous studies have delved into coordinate-based neural representations for spherical signals, they often fail to capture the intricate details of highly nonlinear signals. To address this limitation, we introduce a novel approach named Hybrid Neural Representations for Spherical data (HNeR-S). Our main idea is to use spherical feature-grids to obtain positional features which are combined with a multi-layer perceptron to predict the target signal. We consider feature-grids with equirectangular and hierarchical equal area isolatitude pixelization structures that align with weather data and CMB data, respectively. We extensively verify the effectiveness of our HNeR-S for regression, super-resolution, temporal interpolation, and compression tasks",
    "checked": true,
    "id": "f8936c47d1ad17d0b3cb12695d6e2f6831a2a61f",
    "semantic_title": "hybrid neural representations for spherical data",
    "citation_count": 3,
    "authors": [
      "Hyomin Kim",
      "Yunhui Jang",
      "Jaeho Lee",
      "Sungsoo Ahn"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24j.html": {
    "title": "Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs",
    "volume": "main",
    "abstract": "The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields. This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices. However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept. This phenomenon leads to instability and inefficiency when training poses are sparse. In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based MLP network known for strong bias toward low-frequency signals. The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. We demonstrate that using residual connections between them seamlessly preserves their own inherent properties. Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. We demonstrate empirically that our proposed method not only outperforms baseline models for both static and dynamic NeRFs with sparse inputs, but also achieves comparable results with fewer parameters",
    "checked": true,
    "id": "7f47c8cf6965ed60abcebd2f9b8e2cf52ab92a05",
    "semantic_title": "synergistic integration of coordinate network and tensorial feature for improving neural radiance fields from sparse inputs",
    "citation_count": 0,
    "authors": [
      "Mingyu Kim",
      "Kim Jun-Seong",
      "Se-Young Yun",
      "Jin-Hwa Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24k.html": {
    "title": "Learning to Explore for Stochastic Gradient MCMC",
    "volume": "main",
    "abstract": "Bayesian Neural Networks(BNNs) with high-dimensional parameters pose a challenge for posterior inference due to the multi-modality of the posterior distributions. Stochastic Gradient Markov Chain Monte Carlo(SGMCMC) with cyclical learning rate scheduling is a promising solution, but it requires a large number of sampling steps to explore high-dimensional multi-modal posteriors, making it computationally expensive. In this paper, we propose a meta-learning strategy to build SGMCMC which can efficiently explore the multi-modal target distributions. Our algorithm allows the learned SGMCMC to quickly explore the high-density region of the posterior landscape. Also, we show that this exploration property is transferrable to various tasks, even for the ones unseen during a meta-training stage. Using popular image classification benchmarks and a variety of downstream tasks, we demonstrate that our method significantly improves the sampling efficiency, achieving better performance than vanilla SGMCMC without incurring significant computational overhead",
    "checked": true,
    "id": "a7e5c2024520f2bc8fe01abf5872652b4e02c6ed",
    "semantic_title": "learning to explore for stochastic gradient mcmc",
    "citation_count": 0,
    "authors": [
      "Seunghyun Kim",
      "Seohyeon Jung",
      "Seonghyeon Kim",
      "Juho Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24l.html": {
    "title": "Improving Robustness to Multiple Spurious Correlations by Multi-Objective Optimization",
    "volume": "main",
    "abstract": "We study the problem of training an unbiased and accurate model given a dataset with multiple biases. This problem is challenging since the multiple biases cause multiple undesirable shortcuts during training, and even worse, mitigating one may exacerbate the other. We propose a novel training method to tackle this challenge. Our method first groups training data so that different groups induce different shortcuts, and then optimizes a linear combination of group-wise losses while adjusting their weights dynamically to alleviate conflicts between the groups in performance; this approach, rooted in the multi-objective optimization theory, encourages to achieve the minimax Pareto solution. We also present a new benchmark with multiple biases, dubbed MultiCelebA, for evaluating debiased training methods under realistic and challenging scenarios. Our method achieved the best on three datasets with multiple biases, and also showed superior performance on conventional single-bias datasets",
    "checked": true,
    "id": "9523f74a5b48787dfa0dfa1f6fae9d267a8f1a7d",
    "semantic_title": "improving robustness to multiple spurious correlations by multi-objective optimization",
    "citation_count": 0,
    "authors": [
      "Nayeong Kim",
      "Juwon Kang",
      "Sungsoo Ahn",
      "Jungseul Ok",
      "Suha Kwak"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24m.html": {
    "title": "Double-Step Alternating Extragradient with Increasing Timescale Separation for Finding Local Minimax Points: Provable Improvements",
    "volume": "main",
    "abstract": "In nonconvex-nonconcave minimax optimization, two-timescale gradient methods have shown their potential to find local minimax (optimal) points, provided that the timescale separation between the min and the max player is sufficiently large. However, existing two-timescale variants of gradient descent ascent and extragradient methods face two shortcomings, especially when we search for non-strict local minimax points that are prevalent in modern overparameterized setting. In specific, (1) these methods can be unstable at some non-strict local minimax points even with sufficiently large timescale separation, and even (2) computing a proper amount of timescale separation is infeasible in practice. To remedy these two issues, we propose to incorporate two simple but provably effective schemes, double-step alternating update and increasing timescale separation, into the two-timescale extragradient method, respectively. Under mild conditions, we show that the proposed methods converge to non-strict local minimax points that all existing two-timescale methods fail to converge",
    "checked": true,
    "id": "3b33b961d5b2819f7200d117baf7ccc62f9b79c2",
    "semantic_title": "double-step alternating extragradient with increasing timescale separation for finding local minimax points: provable improvements",
    "citation_count": 0,
    "authors": [
      "Kyuwon Kim",
      "Donghwan Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24n.html": {
    "title": "Scene Graph Generation Strategy with Co-occurrence Knowledge and Learnable Term Frequency",
    "volume": "main",
    "abstract": "Scene graph generation (SGG) is an important task in image understanding because it represents the relationships between objects in an image as a graph structure, making it possible to understand the semantic relationships between objects intuitively. Previous SGG studies used a message-passing neural networks (MPNN) to update features, which can effectively reflect information about surrounding objects. However, these studies have failed to reflect the co-occurrence of objects during SGG generation. In addition, they only addressed the long-tail problem of the training dataset from the perspectives of sampling and learning methods. To address these two problems, we propose CooK, which reflects the Co-occurrence Knowledge between objects, and the learnable term frequency-inverse document frequency (TF-$l$-IDF) to solve the long-tail problem. We applied the proposed model to the SGG benchmark dataset, and the results showed a performance improvement of up to 3.8% compared with existing state-of-the-art models in SGGen subtask. The proposed method exhibits generalization ability from the results obtained, showing uniform performance improvement for all MPNN models",
    "checked": true,
    "id": "7254f40bb6dfc1067da03bf1296414bd46289fb8",
    "semantic_title": "scene graph generation strategy with co-occurrence knowledge and learnable term frequency",
    "citation_count": 1,
    "authors": [
      "Hyeongjin Kim",
      "Sangwon Kim",
      "Dasom Ahn",
      "Jong Taek Lee",
      "Byoung Chul Ko"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24o.html": {
    "title": "Symmetric Replay Training: Enhancing Sample Efficiency in Deep Reinforcement Learning for Combinatorial Optimization",
    "volume": "main",
    "abstract": "Deep reinforcement learning (DRL) has significantly advanced the field of combinatorial optimization (CO). However, its practicality is hindered by the necessity for a large number of reward evaluations, especially in scenarios involving computationally intensive function assessments. To enhance the sample efficiency, we propose a simple but effective method, called symmetric replay training (SRT), which can be easily integrated into various DRL methods. Our method leverages high-reward samples to encourage exploration of the under-explored symmetric regions without additional online interactions - free. Through replay training, the policy is trained to maximize the likelihood of the symmetric trajectories of discovered high-rewarded samples. Experimental results demonstrate the consistent improvement of our method in sample efficiency across diverse DRL methods applied to real-world tasks, such as molecular optimization and hardware design",
    "checked": true,
    "id": "c649007297c9f0a77e14eb94384a11b994c66530",
    "semantic_title": "symmetric replay training: enhancing sample efficiency in deep reinforcement learning for combinatorial optimization",
    "citation_count": 3,
    "authors": [
      "Hyeonah Kim",
      "Minsu Kim",
      "Sungsoo Ahn",
      "Jinkyoo Park"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24p.html": {
    "title": "Clustered Federated Learning via Gradient-based Partitioning",
    "volume": "main",
    "abstract": "Clustered Federated Learning (CFL) is a promising distributed learning framework that addresses data heterogeneity issues across multiple clients by grouping clients and providing a shared generalized model for each group. However, under privacy-preserving federated learning protocols where there is no direct sharing of clients' local datasets, existing approaches often fail to find optimal client groupings resulting in sub-optimal performance. In this paper, we propose a novel CFL algorithm that achieves robust clustering and learning performance. Conceptually, our algorithm groups clients that exhibit similarity in their model updates by periodically accumulating and clustering the gradients that clients compute for various models. The proposed algorithm is shown to achieve a near-optimal error rate for stochastic convergence to optimal models under mild conditions. We present a detailed analysis of the algorithm along with an evaluation on several CFL benchmarks demonstrating that it outperforms existing approaches in terms of convergence speed, clustering accuracy, and task performance",
    "checked": true,
    "id": "40434b07196cb734a547a5e2061ec18d5e8d8b71",
    "semantic_title": "clustered federated learning via gradient-based partitioning",
    "citation_count": 0,
    "authors": [
      "Heasung Kim",
      "Hyeji Kim",
      "Gustavo De Veciana"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24q.html": {
    "title": "Variational Partial Group Convolutions for Input-Aware Partial Equivariance of Rotations and Color-Shifts",
    "volume": "main",
    "abstract": "Group Equivariant CNNs (G-CNNs) have shown promising efficacy in various tasks, owing to their ability to capture hierarchical features in an equivariant manner. However, their equivariance is fixed to the symmetry of the whole group, limiting adaptability to diverse partial symmetries in real-world datasets, such as limited rotation symmetry of handwritten digit images and limited color-shift symmetry of flower images. Recent efforts address this limitation, one example being Partial G-CNN which restricts the output group space of convolution layers to break full equivariance. However, such an approach still fails to adjust equivariance levels across data. In this paper, we propose a novel approach, Variational Partial G-CNN (VP G-CNN), to capture varying levels of partial equivariance specific to each data instance. VP G-CNN redesigns the distribution of the output group elements to be conditioned on input data, leveraging variational inference to avoid overfitting. This enables the model to adjust its equivariance levels according to the needs of individual data points. Additionally, we address training instability inherent in discrete group equivariance models by redesigning the reparametrizable distribution. We demonstrate the effectiveness of VP G-CNN on both toy and real-world datasets, including MNIST67-180, CIFAR10, ColorMNIST, and Flowers102. Our results show robust performance, even in uncertainty metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsu Kim",
      "Yegon Kim",
      "Hongseok Yang",
      "Juho Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24r.html": {
    "title": "Demystifying SGD with Doubly Stochastic Gradients",
    "volume": "main",
    "abstract": "Optimization objectives in the form of a sum of intractable expectations are rising in importance (e.g.,, diffusion models, variational autoencoders, and many more), a setting also known as \"finite sum with infinite data.\" For these problems, a popular strategy is to employ SGD with doubly stochastic gradients (doubly SGD): the expectations are estimated using the gradient estimator of each component, while the sum is estimated by subsampling over these estimators. Despite its popularity, little is known about the convergence properties of doubly SGD, except under strong assumptions such as bounded variance. In this work, we establish the convergence of doubly SGD with independent minibatching and random reshuffling under general conditions, which encompasses dependent component gradient estimators. In particular, for dependent estimators, our analysis allows fined-grained analysis of the effect correlations. As a result, under a per-iteration computational budget of $b \\times m$, where $b$ is the minibatch size and $m$ is the number of Monte Carlo samples, our analysis suggests where one should invest most of the budget in general. Furthermore, we prove that random reshuffling (RR) improves the complexity dependence on the subsampling noise",
    "checked": true,
    "id": "b4a7565dd5dbe837d3a27570fc1b19ebdd67da09",
    "semantic_title": "demystifying sgd with doubly stochastic gradients",
    "citation_count": 0,
    "authors": [
      "Kyurae Kim",
      "Joohwan Ko",
      "Yian Ma",
      "Jacob R. Gardner"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24s.html": {
    "title": "Learning to Scale Logits for Temperature-Conditional GFlowNets",
    "volume": "main",
    "abstract": "GFlowNets are probabilistic models that sequentially generate compositional structures through a stochastic policy. Among GFlowNets, temperature-conditional GFlowNets can introduce temperature-based controllability for exploration and exploitation. We propose Logit-scaling GFlowNets (Logit-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets. It is based on the idea that previously proposed approaches introduced numerical challenges in the deep network training, since different temperatures may give rise to very different gradient profiles as well as magnitudes of the policy's logits. We find that the challenge is greatly reduced if a learned function of the temperature is used to scale the policy's logits directly. Also, using Logit-GFN, GFlowNets can be improved by having better generalization capabilities in offline learning and mode discovery capabilities in online learning, which is empirically verified in various biological and chemical tasks. Our code is available at https://github.com/dbsxodud-11/logit-gfn",
    "checked": true,
    "id": "81055d18f5906cc1d1e269335e20233b0103c4d3",
    "semantic_title": "learning to scale logits for temperature-conditional gflownets",
    "citation_count": 17,
    "authors": [
      "Minsu Kim",
      "Joohwan Ko",
      "Taeyoung Yun",
      "Dinghuai Zhang",
      "Ling Pan",
      "Woo Chang Kim",
      "Jinkyoo Park",
      "Emmanuel Bengio",
      "Yoshua Bengio"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24t.html": {
    "title": "Attribute Based Interpretable Evaluation Metrics for Generative Models",
    "volume": "main",
    "abstract": "When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond \"diversity\". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Singleattribute Divergence (SaD) reveals the attributes that are generated excessively or insufficiently by measuring the divergence of PDFs of individual attributes. Paired-attribute Divergence (PaD) reveals such pairs of attributes by measuring the divergence of joint PDFs of pairs of attributes. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous initial points. With SaD and PaD, we reveal the following about existing generative models. ProjectedGAN generates implausible attribute relationships such as baby with beard even though it has competitive scores of existing metrics. Diffusion models struggle to capture diverse colors in the datasets. The larger sampling timesteps of the latent diffusion model generate the more minor objects including earrings and necklace. Stable Diffusion v1.5 better captures the attributes than v2.1. Our metrics lay a foundation for explainable evaluations of generative models",
    "checked": true,
    "id": "a245d5417204ad3b553199c30700de41506bbdb6",
    "semantic_title": "attribute based interpretable evaluation metrics for generative models",
    "citation_count": 2,
    "authors": [
      "Dongkyun Kim",
      "Mingi Kwon",
      "Youngjung Uh"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24u.html": {
    "title": "Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "Recently, various pre-training methods have been introduced in vision-based Reinforcement Learning (RL). However, their generalization ability remains unclear due to evaluations being limited to in-distribution environments and non-unified experimental setups. To address this, we introduce the Atari Pre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10 million transitions from 50 Atari games and evaluates it across diverse environment distributions. Our experiments show that pre-training objectives focused on learning task-agnostic features (e.g., identifying objects and understanding temporal dynamics) enhance generalization across different environments. In contrast, objectives focused on learning task-specific knowledge (e.g., identifying agents and fitting reward functions) improve performance in environments similar to the pre-training dataset but not in varied ones. We publicize our codes, datasets, and model checkpoints at https://github.com/dojeon-ai/Atari-PB",
    "checked": true,
    "id": "7f1b7dff5330f57eb9587fd0c7f0699acb12d5a5",
    "semantic_title": "investigating pre-training objectives for generalization in vision-based reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Donghu Kim",
      "Hojoon Lee",
      "Kyungmin Lee",
      "Dongyoon Hwang",
      "Jaegul Choo"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24v.html": {
    "title": "EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning",
    "volume": "main",
    "abstract": "Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. EquiAV outperforms previous works across various audio-visual benchmarks. The code is available on https://github.com/JongSuk1/EquiAV",
    "checked": true,
    "id": "e4d7730f2992cea4dcca43082ae3f950ab3b0d36",
    "semantic_title": "equiav: leveraging equivariance for audio-visual contrastive learning",
    "citation_count": 0,
    "authors": [
      "Jongsuk Kim",
      "Hyeongkeun Lee",
      "Kyeongha Rho",
      "Junmo Kim",
      "Joon Son Chung"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24w.html": {
    "title": "Scaling Beyond the GPU Memory Limit for Large Mixture-of-Experts Model Training",
    "volume": "main",
    "abstract": "Mixture-of-Experts (MoE) is a powerful technique for enhancing the performance of neural networks while decoupling computational complexity from the number of parameters. However, despite this, scaling the number of experts requires adding more GPUs. In addition, the load imbalance in token load across experts causes unnecessary computation or straggler problems. We present ES-MoE, a novel method for efficient scaling MoE training. It offloads expert parameters to host memory and leverages pipelined expert processing to overlap GPU-CPU communication with GPU computation. It dynamically balances token loads across GPUs, improving computational efficiency. ES-MoE accelerates MoE training on a limited number of GPUs without degradation in model performance. We validate our approach on GPT-based MoE models, demonstrating 67$\\times$ better scalability and up to 17.5$\\times$ better throughput over existing frameworks",
    "checked": true,
    "id": "00989817f243f7e42f9eec11bf2334038d158839",
    "semantic_title": "scaling beyond the gpu memory limit for large mixture-of-experts model training",
    "citation_count": 1,
    "authors": [
      "Yechan Kim",
      "Hwijoon Lim",
      "Dongsu Han"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24x.html": {
    "title": "Risk-Sensitive Policy Optimization via Predictive CVaR Policy Gradient",
    "volume": "main",
    "abstract": "This paper addresses a policy optimization task with the conditional value-at-risk (CVaR) objective. We introduce the predictive CVaR policy gradient, a novel approach that seamlessly integrates risk-neutral policy gradient algorithms with minimal modifications. Our method incorporates a reweighting strategy in gradient calculation – individual cost terms are reweighted in proportion to their predicted contribution to the objective. These weights can be easily estimated through a separate learning procedure. We provide theoretical and empirical analyses, demonstrating the validity and effectiveness of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju-Hyun Kim",
      "Seungki Min"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24y.html": {
    "title": "An LLM Compiler for Parallel Function Calling",
    "volume": "main",
    "abstract": "The reasoning capabilities of the recent LLMs enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed LLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multiple function calls. Drawing inspiration from the principles of classical compilers, LLMCompiler enables parallel function calling with three components: (i) a Function Calling Planner, formulating execution plans for function calling; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically generates an optimized orchestration for the function calls and can be used with both open-source and closed-source models. We have benchmarked LLMCompiler on a range of tasks with different patterns of function calling. We observe consistent latency speedup of up to $3.7 \\times$, cost savings of up to $6.7 \\times$, and accuracy improvement of up to $\\sim 9 %$ compared to ReAct.Our code is available at https://github.com/SqueezeAILab/LLMCompiler",
    "checked": true,
    "id": "36f71673d9337b432babc51da77ef38b2070b5ed",
    "semantic_title": "an llm compiler for parallel function calling",
    "citation_count": 33,
    "authors": [
      "Sehoon Kim",
      "Suhong Moon",
      "Ryan Tabrizi",
      "Nicholas Lee",
      "Michael W. Mahoney",
      "Kurt Keutzer",
      "Amir Gholami"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24z.html": {
    "title": "Data-Efficient Molecular Generation with Hierarchical Textual Inversion",
    "volume": "main",
    "abstract": "Developing an effective molecular generation framework even with a limited number of molecules is often important for its practical deployment, e.g., drug discovery, since acquiring task-related molecular data requires expensive and time-consuming experimental costs. To tackle this issue, we introduce Hierarchical Textual Inversion for Molecular Generation (HI-Mol), a novel data-efficient molecular generation method. HI-Mol is inspired by the importance of hierarchical information, e.g., both coarse- and fine-grained features, in understanding the molecule distribution. We propose to use multi-level embeddings to reflect such hierarchical features based on the adoption of the recent textual inversion technique in the visual domain, which achieves data-efficient image generation. Compared to the conventional textual inversion method in the image domain using a single-level token embedding, our multi-level token embeddings allow the model to effectively learn the underlying low-shot molecule distribution. We then generate molecules based on the interpolation of the multi-level token embeddings. Extensive experiments demonstrate the superiority of HI-Mol with notable data-efficiency. For instance, on QM9, HI-Mol outperforms the prior state-of-the-art method with 50x less training data. We also show the effectiveness of molecules generated by HI-Mol in low-shot molecular property prediction. Code is available at https://github.com/Seojin-Kim/HI-Mol",
    "checked": true,
    "id": "4ffd4b3b22c471c628bba8e3b69e11a392976190",
    "semantic_title": "data-efficient molecular generation with hierarchical textual inversion",
    "citation_count": 2,
    "authors": [
      "Seojin Kim",
      "Jaehyun Nam",
      "Sihyun Yu",
      "Younghoon Shin",
      "Jinwoo Shin"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24aa.html": {
    "title": "Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning",
    "volume": "main",
    "abstract": "Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models with S2N translation can process 183 – 711 times more subgraph samples than state-of-the-art models at a better or similar performance level",
    "checked": true,
    "id": "f87e4bdbd4285557a32cf3aec002af2b143b37ab",
    "semantic_title": "translating subgraphs to nodes makes simple gnns strong and efficient for subgraph representation learning",
    "citation_count": 0,
    "authors": [
      "Dongkwan Kim",
      "Alice Oh"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24ab.html": {
    "title": "Privacy-Preserving Embedding via Look-up Table Evaluation with Fully Homomorphic Encryption",
    "volume": "main",
    "abstract": "In privacy-preserving machine learning (PPML), homomorphic encryption (HE) has emerged as a significant primitive, allowing the use of machine learning (ML) models while protecting the confidentiality of input data. Although extensive research has been conducted on implementing PPML with HE by developing the efficient construction of private counterparts to ML models, the efficient HE implementation of embedding layers for token inputs such as words remains inadequately addressed. Thus, our study proposes an efficient algorithm for privacy-preserving embedding via look-up table evaluation with HE(HELUT) by developing an encrypted indicator function (EIF) that assures high precision with the use of the approximate HE scheme(CKKS). Based on the proposed EIF, we propose the CodedHELUT algorithm to facilitate an encrypted embedding layer for the first time. CodedHELUT leverages coded inputs to improve overall efficiency and optimize memory usage. Our comprehensive empirical analysis encompasses both synthetic tables and real-world largescale word embedding models. CodedHELUT algorithm achieves amortized evaluation time of 0.018-0.242s for GloVe6B50d, 0.104-01.298s for GloVe42300d, 0.262-3.283s for GPT-2 and BERT embedding layers while maintaining high precision (16 bits)",
    "checked": true,
    "id": "5ed1079810cff9144173f808d87defd7e0188248",
    "semantic_title": "privacy-preserving embedding via look-up table evaluation with fully homomorphic encryption",
    "citation_count": 0,
    "authors": [
      "Jae-Yun Kim",
      "Saerom Park",
      "Joohee Lee",
      "Jung Hee Cheon"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24ac.html": {
    "title": "Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time",
    "volume": "main",
    "abstract": "In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of O(√log n), where n is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that local gradient methods converge to a point with low training loss with high probability. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well",
    "checked": true,
    "id": "6738b7280d3b94f7365982197a51b32f0bbb10c8",
    "semantic_title": "convex relaxations of relu neural networks approximate global optima in polynomial time",
    "citation_count": 3,
    "authors": [
      "Sungyoon Kim",
      "Mert Pilanci"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24ad.html": {
    "title": "USTAD: Unified Single-model Training Achieving Diverse Scores for Information Retrieval",
    "volume": "main",
    "abstract": "Modern information retrieval (IR) systems consists of multiple stages like retrieval and ranking, with Transformer-based models achieving state-of-the-art performance at each stage. In this paper, we challenge the tradition of using separate models for different stages and ask if a single Transformer encoder can provide relevance score needed in each stage. We present USTAD – a new unified approach to train a single network that can provide powerful ranking scores as a cross-encoder (CE) model as well as factorized embeddings for large-scale retrieval as a dual-encoder (DE) model. Empirically, we find a single USTAD model to be competitive to separate ranking CE and retrieval DE models. Furthermore, USTAD combines well with a novel embedding matching-based distillation, significantly improving CE to DE distillation. It further motivates novel asymmetric architectures for student models to ensure a better embedding alignment between the student and the teacher while ensuring small online inference cost. On standard benchmarks like MSMARCO, we demonstrate that USTAD with our proposed distillation method leads to asymmetric students with only 1/10th trainable parameter but retaining 95-97% of the teacher performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungyeon Kim",
      "Ankit Singh Rawat",
      "Manzil Zaheer",
      "Wittawat Jitkrittum",
      "Veeranjaneyulu Sadhanala",
      "Sadeep Jayasumana",
      "Aditya Krishna Menon",
      "Rob Fergus",
      "Sanjiv Kumar"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24ae.html": {
    "title": "Polynomial-based Self-Attention for Table Representation Learning",
    "volume": "main",
    "abstract": "Structured data, which constitutes a significant portion of existing data types, has been a long-standing research topic in the field of machine learning. Various representation learning methods for tabular data have been proposed, ranging from encoder-decoder structures to Transformers. Among these, Transformer-based methods have achieved state-of-the-art performance not only in tabular data but also in various other fields, including computer vision and natural language processing. However, recent studies have revealed that self-attention, a key component of Transformers, can lead to an oversmoothing issue. We show that Transformers for tabular data also face this problem. To tackle the problem, we suggest a novel self-attention layer for tabular data, leveraging matrix polynomials. This proposed layer serves as a replacement for the original self-attention layer, contributing to the improvement of model scalability. In our experiments with three representative table learning models equipped with our proposed layer, we illustrate that the layer effectively mitigates the oversmoothing problem and enhances the representation performance of the existing methods, outperforming the state-of-the-art table representation methods",
    "checked": true,
    "id": "f86ea57bb32217d15671d1bee975ea4c7ca67f17",
    "semantic_title": "polynomial-based self-attention for table representation learning",
    "citation_count": 1,
    "authors": [
      "Jayoung Kim",
      "Yehjin Shin",
      "Jeongwhan Choi",
      "Hyowon Wi",
      "Noseong Park"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24af.html": {
    "title": "Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape",
    "volume": "main",
    "abstract": "Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juno Kim",
      "Taiji Suzuki"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24ag.html": {
    "title": "Discovering Features with Synergistic Interactions in Multiple Views",
    "volume": "main",
    "abstract": "Discovering features with synergistic interactions in multi-view data, that provide more information gain when considered together than when considered separately, is particularly valuable. This fosters a more comprehensive understanding of the target outcome from diverse perspectives (views). However, despite the increasing opportunities presented by multi-view data, surprisingly little attention has been paid to uncovering these crucial interactions. To address this gap, we formally define the problem of selecting synergistic and non-synergistic feature subsets in multi-view data, leveraging an information-theoretic concept known as interaction information. To this end, we introduce a novel deep learning-based feature selection method that identifies different interactions across multiple views, employing a Bernoulli relaxation technique to solve this intractable subset searching problem. Experiments on synthetic, semi-synthetic, and real-world multi-view datasets demonstrate that our model discovers relevant feature subsets with synergistic and non-synergistic interactions, achieving remarkable similarity to the ground truth. Furthermore, we corroborate the discovered features with supporting medical and scientific literature, underscoring its utility in elucidating complex dependencies and interactions in multi-view data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chohee Kim",
      "Mihaela Van Der Schaar",
      "Changhee Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24ah.html": {
    "title": "An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural Network",
    "volume": "main",
    "abstract": "The recent theoretical analysis of deep neural networks in their infinite-width limits has deepened our understanding of initialisation, feature learning, and training of those networks, and brought new practical techniques for finding appropriate hyperparameters, learning network weights, and performing inference. In this paper, we broaden this line of research by showing that this infinite-width analysis can be extended to the Jacobian of a deep neural network. We show that a multilayer perceptron (MLP) and its Jacobian at initialisation jointly converge to a Gaussian process (GP) as the widths of the MLP's hidden layers go to infinity and characterise this GP. We also prove that in the infinite-width limit, the evolution of the MLP under the so-called robust training (i.e., training with a regulariser on the Jacobian) is described by a linear first-order ordinary differential equation that is determined by a variant of the Neural Tangent Kernel. We experimentally show the relevance of our theoretical claims to wide finite networks, and empirically analyse the properties of kernel regression solution to obtain an insight into Jacobian regularisation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeyoung Kim",
      "Hongseok Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24ai.html": {
    "title": "One Size Fits All for Semantic Shifts: Adaptive Prompt Tuning for Continual Learning",
    "volume": "main",
    "abstract": "In real-world continual learning (CL) scenarios, tasks often exhibit intricate and unpredictable semantic shifts, posing challenges for fixed prompt management strategies which are tailored to only handle semantic shifts of uniform degree (i.e., uniformly mild or uniformly abrupt). To address this limitation, we propose an adaptive prompting approach that effectively accommodates semantic shifts of varying degree where mild and abrupt shifts are mixed. AdaPromptCL employs the assign-and-refine semantic grouping mechanism that dynamically manages prompt groups in accordance with the semantic similarity between tasks, enhancing the quality of grouping through continuous refinement. Our experiment results demonstrate that AdaPromptCL outperforms existing prompting methods by up to 21.3%, especially in the benchmark datasets with diverse semantic shifts between tasks",
    "checked": true,
    "id": "10fd2f9b47da6c769dedd81125564ca21acc01c3",
    "semantic_title": "one size fits all for semantic shifts: adaptive prompt tuning for continual learning",
    "citation_count": 2,
    "authors": [
      "Doyoung Kim",
      "Susik Yoon",
      "Dongmin Park",
      "Youngjun Lee",
      "Hwanjun Song",
      "Jihwan Bang",
      "Jae-Gil Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24aj.html": {
    "title": "Do Topological Characteristics Help in Knowledge Distillation?",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) aims to transfer knowledge from larger (teacher) to smaller (student) networks. Previous studies focus on point-to-point or pairwise relationships in embedding features as knowledge and struggle to efficiently transfer relationships of complex latent spaces. To tackle this issue, we propose a novel KD method called TopKD, which considers the global topology of the latent spaces. We define global topology knowledge using the persistence diagram (PD) that captures comprehensive geometric structures such as shape of distribution, multiscale structure and connectivity, and the topology distillation loss for teaching this knowledge. To make the PD transferable within reasonable computational time, we employ approximated persistence images of PDs. Through experiments, we support the benefits of using global topology as knowledge and demonstrate the potential of TopKD. Code is available at https://github.com/jekim5418/TopKD",
    "checked": true,
    "id": "1a015200fb24d385f09b47afbf5aad8e5d187766",
    "semantic_title": "do topological characteristics help in knowledge distillation?",
    "citation_count": 0,
    "authors": [
      "Jungeun Kim",
      "Junwon You",
      "Dongjin Lee",
      "Ha Young Kim",
      "Jae-Hun Jung"
    ]
  },
  "https://proceedings.mlr.press/v235/kim24ak.html": {
    "title": "A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback",
    "volume": "main",
    "abstract": "Inverse Reinforcement Learning (IRL) and Reinforcement Learning from Human Feedback (RLHF) are pivotal methodologies in reward learning, which involve inferring and shaping the underlying reward function of sequential decision-making problems based on observed human demonstrations and feedback. Most prior work in reward learning has relied on prior knowledge or assumptions about decision or preference models, potentially leading to robustness issues. In response, this paper introduces a novel linear programming (LP) framework tailored for offline reward learning. Utilizing pre-collected trajectories without online exploration, this framework estimates a feasible reward set from the primal-dual optimality conditions of a suitably designed LP, and offers an optimality guarantee with provable sample efficiency. Our LP framework also enables aligning the reward functions with human feedback, such as pairwise trajectory comparison data, while maintaining computational tractability and sample efficiency. We demonstrate that our framework potentially achieves better performance compared to the conventional maximum likelihood estimation (MLE) approach through analytical examples and numerical experiments",
    "checked": true,
    "id": "1ccd32a6988c405a4c6a93593ca9aeba1a2bcb13",
    "semantic_title": "a unified linear programming framework for offline reward learning from human demonstrations and feedback",
    "citation_count": 1,
    "authors": [
      "Kihyun Kim",
      "Jiawei Zhang",
      "Asuman E. Ozdaglar",
      "Pablo Parrilo"
    ]
  },
  "https://proceedings.mlr.press/v235/kirschstein24a.html": {
    "title": "The Merit of River Network Topology for Neural Flood Forecasting",
    "volume": "main",
    "abstract": "Climate change exacerbates riverine floods, which occur with higher frequency and intensity than ever. The much-needed forecasting systems typically rely on accurate river discharge predictions. To this end, the SOTA data-driven approaches treat forecasting at spatially distributed gauge stations as isolated problems, even within the same river network. However, incorporating the known topology of the river network into the prediction model has the potential to leverage the adjacency relationship between gauges. Thus, we model river discharge for a network of gauging stations with GNNs and compare the forecasting performance achieved by different adjacency definitions. Our results show that the model fails to benefit from the river network topology information, both on the entire network and small subgraphs. The learned edge weights correlate with neither of the static definitions and exhibit no regular pattern. Furthermore, the GNNs struggle to predict sudden, narrow discharge spikes. Our work hints at a more general underlying phenomenon of neural prediction not always benefitting from graphical structure and may inspire a systematic study of the conditions under which this happens",
    "checked": true,
    "id": "d51826bf92e38b2353c2975305f92cd32952b073",
    "semantic_title": "the merit of river network topology for neural flood forecasting",
    "citation_count": 0,
    "authors": [
      "Nikolas Kirschstein",
      "Yixuan Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/kitouni24a.html": {
    "title": "From Neurons to Neutrons: A Case Study in Interpretability",
    "volume": "main",
    "abstract": "Mechanistic Interpretability (MI) proposes a path toward fully understanding how neural networks make their predictions. Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters. Does this mean neuron-level interpretability techniques have limited applicability? Here, we argue that high-dimensional neural networks can learn useful low-dimensional representations of the data they were trained on, going beyond simply making good predictions: Such representations can be understood with the MI lens and provide insights that are surprisingly faithful to human-derived domain knowledge. This indicates that such approaches to interpretability can be useful for deriving a new understanding of a problem from models trained to solve it. As a case study, we extract nuclear physics concepts by studying models trained to reproduce nuclear data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ouail Kitouni",
      "Niklas Nolte",
      "Vı́ctor Samuel Pérez-Dı́az",
      "Sokratis Trifinopoulos",
      "Mike Williams"
    ]
  },
  "https://proceedings.mlr.press/v235/kiyani24a.html": {
    "title": "Conformal Prediction with Learned Features",
    "volume": "main",
    "abstract": "In this paper, we focus on the problem of conformal prediction with conditional guarantees. Prior work has shown that it is impossible to construct nontrivial prediction sets with full conditional coverage guarantees. A wealth of research has considered relaxations of full conditional guarantees, relying on some predefined uncertainty structures. Departing from this line of thinking, we propose Partition Learning Conformal Prediction (PLCP), a framework to improve conditional validity of prediction sets through learning uncertainty-guided features from the calibration data. We implement PLCP efficiently with alternating gradient descent, utilizing off-the-shelf machine learning models. We further analyze PLCP theoretically and provide conditional guarantees for infinite and finite sample sizes. Finally, our experimental results over four real-world and synthetic datasets show the superior performance of PLCP compared to state-of-the-art methods in terms of coverage and length in both classification and regression scenarios",
    "checked": true,
    "id": "06fa24b27f719dd92e45b6afc467e7f377323c0b",
    "semantic_title": "conformal prediction with learned features",
    "citation_count": 2,
    "authors": [
      "Shayan Kiyani",
      "George J. Pappas",
      "Hamed Hassani"
    ]
  },
  "https://proceedings.mlr.press/v235/klarner24a.html": {
    "title": "Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design",
    "volume": "main",
    "abstract": "Generative models have the potential to accelerate key steps in the discovery of novel molecular therapeutics and materials. Diffusion models have recently emerged as a powerful approach, excelling at unconditional sample generation and, with data-driven guidance, conditional generation within their training domain. Reliably sampling from high-value regions beyond the training data, however, remains an open challenge—with current methods predominantly focusing on modifying the diffusion process itself. In this paper, we develop context-guided diffusion (CGD), a simple plug-and-play method that leverages unlabeled data and smoothness constraints to improve the out-of-distribution generalization of guided diffusion models. We demonstrate that this approach leads to substantial performance gains across various settings, including continuous, discrete, and graph-structured diffusion processes with applications across drug discovery, materials science, and protein design",
    "checked": true,
    "id": "0ea6a96e7049fed2a3d9023e02d55400ebfdec5c",
    "semantic_title": "context-guided diffusion for out-of-distribution molecular and protein design",
    "citation_count": 4,
    "authors": [
      "Leo Klarner",
      "Tim G. J. Rudner",
      "Garrett M Morris",
      "Charlotte Deane",
      "Yee Whye Teh"
    ]
  },
  "https://proceedings.mlr.press/v235/kleine-buening24a.html": {
    "title": "Environment Design for Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "Learning a reward function from demonstrations suffers from low sample-efficiency. Even with abundant data, current inverse reinforcement learning methods that focus on learning from a single environment can fail to handle slight changes in the environment dynamics. We tackle these challenges through adaptive environment design. In our framework, the learner repeatedly interacts with the expert, with the former selecting environments to identify the reward function as quickly as possible from the expert's demonstrations in said environments. This results in improvements in both sample-efficiency and robustness, as we show experimentally, for both exact and approximate inference",
    "checked": true,
    "id": "32971d45062ce0df1830af60d5c6d75a990ec53a",
    "semantic_title": "environment design for inverse reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Thomas Kleine Buening",
      "Victor Villin",
      "Christos Dimitrakakis"
    ]
  },
  "https://proceedings.mlr.press/v235/ko24a.html": {
    "title": "What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks",
    "volume": "main",
    "abstract": "Recent years have witnessed a paradigm shift in deep learning from task-centric model design to task-agnostic representation learning and task-specific fine-tuning. Pretrained model representations are commonly evaluated extensively across various real-world tasks and used as a foundation for different downstream tasks. This paper proposes a solution for assessing the quality of representations in a task-agnostic way. To circumvent the need for real-world data in evaluation, we explore the use of synthetic binary classification tasks with Gaussian mixtures to probe pretrained models and compare the robustness-accuracy performance on pretrained representations with an idealized reference. Our approach offers a holistic evaluation, revealing intrinsic model capabilities and reducing the dependency on real-life data for model evaluation. Evaluated with various pretrained image models, the experimental results confirm that our task-agnostic evaluation correlates with actual linear probing performance on downstream tasks and can also guide parameter choice in robust linear probing to achieve a better robustness-accuracy trade-off",
    "checked": true,
    "id": "3ee2e9b64d44c7a8a787adf7139f792c497d265d",
    "semantic_title": "what would gauss say about representations? probing pretrained image models using synthetic gaussian benchmarks",
    "citation_count": 0,
    "authors": [
      "Ching-Yun Ko",
      "Pin-Yu Chen",
      "Payel Das",
      "Jeet Mohapatra",
      "Luca Daniel"
    ]
  },
  "https://proceedings.mlr.press/v235/ko24b.html": {
    "title": "Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes",
    "volume": "main",
    "abstract": "In this paper, we prove the universal consistency of wide and deep ReLU neural network classifiers. We also give sufficient conditions for a class of probability measures for which classifiers based on neural networks achieve minimax optimal rates of convergence. The result applies to a wide range of known function classes. In particular, while most previous works impose explicit smoothness assumptions on the regression function, our framework encompasses more general settings. The proposed neural networks are either the minimizers of the $0$-$1$ loss that exhibit a benign overfitting behavior",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunouk Ko",
      "Xiaoming Huo"
    ]
  },
  "https://proceedings.mlr.press/v235/ko24c.html": {
    "title": "DistiLLM: Towards Streamlined Distillation for Large Language Models",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\\times$ speedup compared to recent KD methods",
    "checked": true,
    "id": "49b7baceecd32f81a08aa8e84e2fe71c2b879ee6",
    "semantic_title": "distillm: towards streamlined distillation for large language models",
    "citation_count": 11,
    "authors": [
      "Jongwoo Ko",
      "Sungnyun Kim",
      "Tianyi Chen",
      "Se-Young Yun"
    ]
  },
  "https://proceedings.mlr.press/v235/ko24d.html": {
    "title": "Provably Scalable Black-Box Variational Inference with Structured Variational Families",
    "volume": "main",
    "abstract": "Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean-field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit $\\mathcal{O}(N^2)$ dependence on the dataset size $N$. In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\\mathcal{O}\\left(N\\right)$, implying better scaling with respect to $N$. We empirically verify our theoretical results on large-scale hierarchical models",
    "checked": true,
    "id": "066fc36b0353bd77c50ea91eb1dc498175e32a61",
    "semantic_title": "provably scalable black-box variational inference with structured variational families",
    "citation_count": 1,
    "authors": [
      "Joohwan Ko",
      "Kyurae Kim",
      "Woo Chang Kim",
      "Jacob R. Gardner"
    ]
  },
  "https://proceedings.mlr.press/v235/ko24e.html": {
    "title": "Stochastic Conditional Diffusion Models for Robust Semantic Image Synthesis",
    "volume": "main",
    "abstract": "Semantic image synthesis (SIS) is a task to generate realistic images corresponding to semantic maps (labels). However, in real-world applications, SIS often encounters noisy user inputs. To address this, we propose Stochastic Conditional Diffusion Model (SCDM), which is a robust conditional diffusion model that features novel forward and generation processes tailored for SIS with noisy labels. It enhances robustness by stochastically perturbing the semantic label maps through Label Diffusion, which diffuses the labels with discrete diffusion. Through the diffusion of labels, the noisy and clean semantic maps become similar as the timestep increases, eventually becoming identical at $t=T$. This facilitates the generation of an image close to a clean image, enabling robust generation. Furthermore, we propose a class-wise noise schedule to differentially diffuse the labels depending on the class. We demonstrate that the proposed method generates high-quality samples through extensive experiments and analyses on benchmark datasets, including a novel experimental setup simulating human errors during real-world applications. Code is available at https://github.com/mlvlab/SCDM",
    "checked": true,
    "id": "2dca8ba2656838e2a10ba4c7d6cb491d4caa1341",
    "semantic_title": "stochastic conditional diffusion models for robust semantic image synthesis",
    "citation_count": 3,
    "authors": [
      "Juyeon Ko",
      "Inho Kong",
      "Dogyun Park",
      "Hyunwoo J. Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/kogler24a.html": {
    "title": "Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth",
    "volume": "main",
    "abstract": "Autoencoders are a prominent model in many empirical branches of machine learning and lossy data compression. However, basic theoretical questions remain unanswered even in a shallow two-layer setting. In particular, to what degree does a shallow autoencoder capture the structure of the underlying data distribution? For the prototypical case of the 1-bit compression of sparse Gaussian data, we prove that gradient descent converges to a solution that completely disregards the sparse structure of the input. Namely, the performance of the algorithm is the same as if it was compressing a Gaussian source – with no sparsity. For general data distributions, we give evidence of a phase transition phenomenon in the shape of the gradient descent minimizer, as a function of the data sparsity: below the critical sparsity level, the minimizer is a rotation taken uniformly at random (just like in the compression of non-sparse data); above the critical sparsity, the minimizer is the identity (up to a permutation). Finally, by exploiting a connection with approximate message passing algorithms, we show how to improve upon Gaussian performance for the compression of sparse data: adding a denoising function to a shallow architecture already reduces the loss provably, and a suitable multi-layer decoder leads to a further improvement. We validate our findings on image datasets, such as CIFAR-10 and MNIST",
    "checked": true,
    "id": "b6686c05f554de8cbaa0a9d5e758f2f6038daa52",
    "semantic_title": "compression of structured data with autoencoders: provable benefit of nonlinearities and depth",
    "citation_count": 0,
    "authors": [
      "Kevin Kögler",
      "Aleksandr Shevchenko",
      "Hamed Hassani",
      "Marco Mondelli"
    ]
  },
  "https://proceedings.mlr.press/v235/kolesov24a.html": {
    "title": "Estimating Barycenters of Distributions with Neural Optimal Transport",
    "volume": "main",
    "abstract": "Given a collection of probability measures, a practitioner sometimes needs to find an \"average\" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness in illustrative scenarios and image data setups. Our source code is available at https://github.com/justkolesov/NOTBarycenters",
    "checked": true,
    "id": "f1eacab912d55cd99a30d20da973c9b5d2b27ec4",
    "semantic_title": "estimating barycenters of distributions with neural optimal transport",
    "citation_count": 4,
    "authors": [
      "Alexander Kolesov",
      "Petr Mokrov",
      "Igor Udovichenko",
      "Milena Gazdieva",
      "Gudmund Pammer",
      "Evgeny Burnaev",
      "Alexander Korotin"
    ]
  },
  "https://proceedings.mlr.press/v235/kolluru24a.html": {
    "title": "AdsorbDiff: Adsorbate Placement via Conditional Denoising Diffusion",
    "volume": "main",
    "abstract": "Determining the optimal configuration of adsorbates on a slab (adslab) is pivotal in the exploration of novel catalysts across diverse applications. Traditionally, the quest for the lowest energy adslab configuration involves placing the adsorbate onto the slab followed by an optimization process. Prior methodologies have relied on heuristics, problem-specific intuitions, or brute-force approaches to guide adsorbate placement. In this work, we propose a novel framework for adsorbate placement using denoising diffusion. The model is designed to predict the optimal adsorbate site and orientation corresponding to the lowest energy configuration. Further, we have an end-to-end evaluation framework where diffusion-predicted adslab configuration is optimized with a pretrained machine learning force field and finally evaluated with Density Functional Theory (DFT). Our findings demonstrate an acceleration of up to 5x or 3.5x improvement in accuracy compared to the previous best approach. Given the novelty of this framework and application, we provide insights into the impact of pretraining, model architectures, and conduct extensive experiments to underscore the significance of this approach",
    "checked": true,
    "id": "4fb0c8b83382ffa249721f9183f6f3d23cb7cbe1",
    "semantic_title": "adsorbdiff: adsorbate placement via conditional denoising diffusion",
    "citation_count": 0,
    "authors": [
      "Adeesh Kolluru",
      "John R. Kitchin"
    ]
  },
  "https://proceedings.mlr.press/v235/koloskova24a.html": {
    "title": "On Convergence of Incremental Gradient for Non-convex Smooth Functions",
    "volume": "main",
    "abstract": "In machine learning and neural network optimization, algorithms like incremental gradient, single shuffle SGD, and random reshuffle SGD are popular due to their cache-mismatch efficiency and good practical convergence behavior. However, their optimization properties in theory, especially for non-convex smooth functions, remain incompletely explored. This paper delves into the convergence properties of SGD algorithms with arbitrary data ordering, within a broad framework for non-convex smooth functions. Our findings show enhanced convergence guarantees for incremental gradient and single shuffle SGD. Particularly if $n$ is the training set size, we improve $n$ times the optimization term of convergence guarantee to reach accuracy $\\epsilon$ from $O \\left( \\frac{n}{\\epsilon} \\right)$ to $O \\left( \\frac{1}{\\epsilon}\\right)$",
    "checked": true,
    "id": "13c82153ac69aaae73fb8f9152b45672cd7f6d19",
    "semantic_title": "on convergence of incremental gradient for non-convex smooth functions",
    "citation_count": 1,
    "authors": [
      "Anastasia Koloskova",
      "Nikita Doikov",
      "Sebastian U Stich",
      "Martin Jaggi"
    ]
  },
  "https://proceedings.mlr.press/v235/komodromos24a.html": {
    "title": "Logistic Variational Bayes Revisited",
    "volume": "main",
    "abstract": "Variational logistic regression is a popular method for approximate Bayesian inference seeing wide-spread use in many areas of machine learning including: Bayesian optimization, reinforcement learning and multi-instance learning to name a few. However, due to the intractability of the Evidence Lower Bound, authors have turned to the use of Monte Carlo, quadrature or bounds to perform inference, methods which are costly or give poor approximations to the true posterior. In this paper we introduce a new bound for the expectation of softplus function and subsequently show how this can be applied to variational logistic regression and Gaussian process classification. Unlike other bounds, our proposal does not rely on extending the variational family, or introducing additional parameters to ensure the bound is tight. In fact, we show that this bound is tighter than the state-of-the-art, and that the resulting variational posterior achieves state-of-the-art performance, whilst being significantly faster to compute than Monte-Carlo methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Komodromos",
      "Marina Evangelou",
      "Sarah Lucie Filippi"
    ]
  },
  "https://proceedings.mlr.press/v235/kondratyuk24a.html": {
    "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
    "volume": "main",
    "abstract": "We present VideoPoet, a language model capable of synthesizing high-quality video from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs – including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting the ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/",
    "checked": true,
    "id": "0c4f46e4dcae5527018e6432fb60cfe8c3354e97",
    "semantic_title": "videopoet: a large language model for zero-shot video generation",
    "citation_count": 124,
    "authors": [
      "Dan Kondratyuk",
      "Lijun Yu",
      "Xiuye Gu",
      "Jose Lezama",
      "Jonathan Huang",
      "Grant Schindler",
      "Rachel Hornung",
      "Vighnesh Birodkar",
      "Jimmy Yan",
      "Ming-Chang Chiu",
      "Krishna Somandepalli",
      "Hassan Akbari",
      "Yair Alon",
      "Yong Cheng",
      "Joshua V. Dillon",
      "Agrim Gupta",
      "Meera Hahn",
      "Anja Hauth",
      "David Hendon",
      "Alonso Martinez",
      "David Minnen",
      "Mikhail Sirotenko",
      "Kihyuk Sohn",
      "Xuan Yang",
      "Hartwig Adam",
      "Ming-Hsuan Yang",
      "Irfan Essa",
      "Huisheng Wang",
      "David A Ross",
      "Bryan Seybold",
      "Lu Jiang"
    ]
  },
  "https://proceedings.mlr.press/v235/kong24a.html": {
    "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
    "volume": "main",
    "abstract": "Augmenting large language models (LLMs) to understand audio – including non-speech sounds and non-verbal speech – is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo",
    "checked": true,
    "id": "69e6dd39bf13d290fb9d885da90cc037f0dd2975",
    "semantic_title": "audio flamingo: a novel audio language model with few-shot learning and dialogue abilities",
    "citation_count": 47,
    "authors": [
      "Zhifeng Kong",
      "Arushi Goel",
      "Rohan Badlani",
      "Wei Ping",
      "Rafael Valle",
      "Bryan Catanzaro"
    ]
  },
  "https://proceedings.mlr.press/v235/kong24b.html": {
    "title": "Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning",
    "volume": "main",
    "abstract": "Many processes in biology and drug discovery involve various 3D interactions between molecules, such as protein and protein, protein and small molecule, etc. Given that different molecules are usually represented in different granularity, existing methods usually encode each type of molecules independently with different models, leaving it defective to learn the various underlying interaction physics. In this paper, we first propose to universally represent an arbitrary 3D complex as a geometric graph of sets, shedding light on encoding all types of molecules with one model. We then propose a Generalist Equivariant Transformer (GET) to effectively capture both domain-specific hierarchies and domain-agnostic interaction physics. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where each module is E(3) equivariant and specialized for handling sets of variable sizes. Notably, in contrast to conventional pooling-based hierarchical models, our GET is able to retain fine-grained information of all levels. Extensive experiments on the interactions between proteins, small molecules and RNA/DNAs verify the effectiveness and generalization capability of our proposed method across different domains",
    "checked": true,
    "id": "3187b8fe34086d71135787ad12afe6eb8993ce1f",
    "semantic_title": "generalist equivariant transformer towards 3d molecular interaction learning",
    "citation_count": 11,
    "authors": [
      "Xiangzhe Kong",
      "Wenbing Huang",
      "Yang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/kong24c.html": {
    "title": "ELF: Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis",
    "volume": "main",
    "abstract": "In this work, we propose a novel method for modeling numerous speakers, which enables expressing the overall characteristics of speakers in detail like a trained multi-speaker model without additional training on the target speaker's dataset. Although various works with similar purposes have been actively studied, their performance has not yet reached that of trained multi-speaker models due to their fundamental limitations. To overcome previous limitations, we propose effective methods for feature learning and representing target speakers' speech characteristics by discretizing the features and conditioning them to a speech synthesis model. Our method obtained a significantly higher similarity mean opinion score (SMOS) in subjective similarity evaluation than seen speakers of a high-performance multi-speaker model, even with unseen speakers. The proposed method also outperforms a zero-shot method by significant margins. Furthermore, our method shows remarkable performance in generating new artificial speakers. In addition, we demonstrate that the encoded latent features are sufficiently informative to reconstruct an original speaker's speech completely. It implies that our method can be used as a general methodology to encode and reconstruct speakers' characteristics in various tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungil Kong",
      "Junmo Lee",
      "Jeongmin Kim",
      "Beomjeong Kim",
      "Jihoon Park",
      "Dohee Kong",
      "Changheon Lee",
      "Sangjin Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/kontogiannis24a.html": {
    "title": "The Computational Complexity of Finding Second-Order Stationary Points",
    "volume": "main",
    "abstract": "Non-convex minimization problems are universally considered hard, and even guaranteeing that a computed solution is locally minimizing is known to be NP-hard. In this general context, our paper focuses on the problem of finding stationary points that satisfy an approximate second-order optimality condition, which serves to exclude strict saddles and other non-minimizing stationary points. Our main result is that the problem of finding approximate second-order stationary points (SOSPs) is PLS-complete, i.e., of the same complexity as the problem of finding first-order stationary points (FOSPs), thus resolving an open question in the field. In particular, our results imply that, under the widely believed complexity conjecture that PLS $\\neq$ FNP, finding approximate SOSPs in unconstrained domains is easier than in constrained domains, which is known to be NP-hard. This comes in stark contrast with earlier results which implied that, unless PLS = CLS, finding approximate FOSPs in unconstrained domains is harder than in constrained domains",
    "checked": true,
    "id": "0d377332f57113f8d99c1859ed79186d68d5aa95",
    "semantic_title": "the computational complexity of finding second-order stationary points",
    "citation_count": 0,
    "authors": [
      "Andreas Kontogiannis",
      "Vasilis Pollatos",
      "Sotiris Kanellopoulos",
      "Panayotis Mertikopoulos",
      "Aris Pagourtzis",
      "Ioannis Panageas"
    ]
  },
  "https://proceedings.mlr.press/v235/koppel24a.html": {
    "title": "Information-Directed Pessimism for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Policy optimization from batch data, i.e., offline reinforcement learning (RL) is important when collecting data from a current policy is not possible. This setting incurs distribution mismatch between batch training data and trajectories from the current policy. Pessimistic offsets estimate mismatch using concentration bounds, which possess strong theoretical guarantees and simplicity of implementation. Mismatch may be conservative in sparse data regions and less so otherwise, which can result in under-performing their no-penalty variants in practice. We derive a new pessimistic penalty as the distance between the data and the true distribution using an evaluable one-sample test known as Stein Discrepancy that requires minimal smoothness conditions, and noticeably, allows a mixture family representation of distribution over next states. This entity forms a quantifier of information in offline data, which justifies calling this approach information-directed pessimism (IDP) for offline RL. We further establish that this new penalty based on discrete Stein discrepancy yields practical gains in performance while generalizing the regret of prior art to multimodal distributions",
    "checked": true,
    "id": "b8f3ef5753c06ac70f65b26388fda033b9fe7888",
    "semantic_title": "information-directed pessimism for offline reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Alec Koppel",
      "Sujay Bhatt",
      "Jiacheng Guo",
      "Joe Eappen",
      "Mengdi Wang",
      "Sumitra Ganesh"
    ]
  },
  "https://proceedings.mlr.press/v235/korkmaz24a.html": {
    "title": "Understanding and Diagnosing Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Deep neural policies have recently been installed in a diverse range of settings, from biotechnology to automated financial systems. However, the utilization of deep neural networks to approximate the value function leads to concerns on the decision boundary stability, in particular, with regard to the sensitivity of policy decision making to indiscernible, non-robust features due to highly non-convex and complex deep neural manifolds. These concerns constitute an obstruction to understanding the reasoning made by deep neural policies, and their foundational limitations. Hence, it is crucial to develop techniques that aim to understand the sensitivities in the learnt representations of neural network policies. To achieve this we introduce a theoretically founded method that provides a systematic analysis of the unstable directions in the deep neural policy decision boundary across both time and space. Through experiments in the Arcade Learning Environment (ALE), we demonstrate the effectiveness of our technique for identifying correlated directions of instability, and for measuring how sample shifts remold the set of sensitive directions in the neural policy landscape. Most importantly, we demonstrate that state-of-the-art robust training techniques yield learning of disjoint unstable directions, with dramatically larger oscillations over time, when compared to standard training. We believe our results reveal the fundamental properties of the decision process made by reinforcement learning policies, and can help in constructing reliable and robust deep neural policies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ezgi Korkmaz"
    ]
  },
  "https://proceedings.mlr.press/v235/koromilas24a.html": {
    "title": "Bridging Mini-Batch and Asymptotic Analysis in Contrastive Learning: From InfoNCE to Kernel-Based Losses",
    "volume": "main",
    "abstract": "What do different contrastive learning (CL) losses actually optimize for? Although multiple CL methods have demonstrated remarkable representation learning capabilities, the differences in their inner workings remain largely opaque. In this work, we analyse several CL families and prove that, under certain conditions, they admit the same minimisers when optimizing either their batch-level objectives or their expectations asymptotically. In both cases, an intimate connection with the hyperspherical energy minimisation (HEM) problem resurfaces. Drawing inspiration from this, we introduce a novel CL objective, coined Decoupled Hyperspherical Energy Loss (DHEL). DHEL simplifies the problem by decoupling the target hyperspherical energy from the alignment of positive examples while preserving the same theoretical guarantees. Going one step further, we show the same results hold for another relevant CL family, namely kernel contrastive learning (KCL), with the additional advantage of the expected loss being independent of batch size, thus identifying the minimisers in the non-asymptotic regime. Empirical results demonstrate improved downstream performance and robustness across combinations of different batch sizes and hyperparameters and reduced dimensionality collapse, on several computer vision datasets",
    "checked": true,
    "id": "4bfa39a3c2ad157c25d51470bf62ab22118a44db",
    "semantic_title": "bridging mini-batch and asymptotic analysis in contrastive learning: from infonce to kernel-based losses",
    "citation_count": 0,
    "authors": [
      "Panagiotis Koromilas",
      "Giorgos Bouritsas",
      "Theodoros Giannakopoulos",
      "Mihalis Nicolaou",
      "Yannis Panagakis"
    ]
  },
  "https://proceedings.mlr.press/v235/koshkin24a.html": {
    "title": "convSeq: Fast and Scalable Method for Detecting Patterns in Spike Data",
    "volume": "main",
    "abstract": "Spontaneous neural activity, crucial in memory, learning, and spatial navigation, often manifests itself as repetitive spatiotemporal patterns. Despite their importance, analyzing these patterns in large neural recordings remains challenging due to a lack of efficient and scalable detection methods. Addressing this gap, we introduce convSeq, an unsupervised method that employs backpropagation for optimizing spatiotemporal filters that effectively identify these neural patterns. Our method's performance is validated on various synthetic data and real neural recordings, revealing spike sequences with unprecedented scalability and efficiency. Significantly surpassing existing methods in speed, convSeq sets a new standard for analyzing spontaneous neural activity, potentially advancing our understanding of information processing in neural circuits",
    "checked": true,
    "id": "85dd371ccddb2c65c6200de15885633532205fab",
    "semantic_title": "convseq: fast and scalable method for detecting patterns in spike data",
    "citation_count": 0,
    "authors": [
      "Roman Koshkin",
      "Tomoki Fukai"
    ]
  },
  "https://proceedings.mlr.press/v235/koskela24a.html": {
    "title": "Privacy Profiles for Private Selection",
    "volume": "main",
    "abstract": "Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are fundamental primitives of differentially private (DP) data analysis with wide applications to private query release, voting, and hyperparameter tuning. Recent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made significant progress in both generalizing private selection mechanisms and tightening their privacy analysis using modern numerical privacy accounting tools, e.g., Rényi DP. But Rényi DP is known to be lossy when $(\\epsilon,\\delta)$-DP is ultimately needed, and there is a trend to close the gap by directly handling privacy profiles, i.e., $\\delta$ as a function of $\\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work out an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax and PrivateTuning using the privacy profiles of the base algorithms they corral. Numerically, our approach improves over the RDP-based accounting in all regimes of interest and leads to substantial benefits in end-to-end private learning experiments. Our analysis also suggests new distributions, e.g., binomial distribution for randomizing the number of rounds that leads to more substantial improvements in certain regimes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antti Koskela",
      "Rachel Emily Redberg",
      "Yu-Xiang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/kosson24a.html": {
    "title": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks",
    "volume": "main",
    "abstract": "This study investigates how weight decay affects the update behavior of individual neurons in deep neural networks through a combination of applied analysis and experimentation. Weight decay can cause the expected magnitude and angular updates of a neuron's weight vector to converge to a steady state we call rotational equilibrium. These states can be highly homogeneous, effectively balancing the average rotation—a proxy for the effective learning rate—across different layers and neurons. Our work analyzes these dynamics across optimizers like Adam, Lion, and SGD with momentum, offering a new simple perspective on training that elucidates the efficacy of widely used but poorly understood methods in deep learning. We demonstrate how balanced rotation plays a key role in the effectiveness of normalization like Weight Standardization, as well as that of AdamW over Adam with L2-regularization. Finally, we show that explicitly controlling the rotation provides the benefits of weight decay while substantially reducing the need for learning rate warmup",
    "checked": true,
    "id": "8df063d355e17d2f958d9d4a83068f16447156b7",
    "semantic_title": "rotational equilibrium: how weight decay balances learning across neural networks",
    "citation_count": 7,
    "authors": [
      "Atli Kosson",
      "Bettina Messmer",
      "Martin Jaggi"
    ]
  },
  "https://proceedings.mlr.press/v235/kostic24a.html": {
    "title": "Consistent Long-Term Forecasting of Ergodic Dynamical Systems",
    "volume": "main",
    "abstract": "We study the problem of forecasting the evolution of a function of the state (observable) of a discrete ergodic dynamical system over multiple time steps. The elegant theory of Koopman and transfer operators can be used to evolve any such function forward in time. However, their estimators are usually unreliable in long-term forecasting. We show how classical techniques of eigenvalue deflation from operator theory and feature centering from statistics can be exploited to enhance standard estimators. We develop a novel technique to derive high probability bounds on powers of empirical estimators. Our approach, rooted in the stability theory of non-normal operators, allows us to establish uniform in time bounds for the forecasting error, which hold even on infinite time horizons. We further show that our approach can be seamlessly employed to forecast future state distributions from an initial one, with provably uniform error bounds. Numerical experiments illustrate the advantages of our approach in practice",
    "checked": true,
    "id": "6bd0baa7c1b4d851b8654eade805ce3184dedd58",
    "semantic_title": "consistent long-term forecasting of ergodic dynamical systems",
    "citation_count": 2,
    "authors": [
      "Vladimir R Kostic",
      "Karim Lounici",
      "Prune Inzerilli",
      "Pietro Novelli",
      "Massimiliano Pontil"
    ]
  },
  "https://proceedings.mlr.press/v235/kotlowski24a.html": {
    "title": "A General Online Algorithm for Optimizing Complex Performance Metrics",
    "volume": "main",
    "abstract": "We consider sequential maximization of performance metrics that are general functions of a confusion matrix of a classifier (such as precision, F-measure, or G-mean). Such metrics are, in general, non-decomposable over individual instances, making their optimization very challenging. While they have been extensively studied under different frameworks in the batch setting, their analysis in the online learning regime is very limited, with only a few distinguished exceptions. In this paper, we introduce and analyze a general online algorithm that can be used in a straightforward way with a variety of complex performance metrics in binary, multi-class, and multi-label classification problems. The algorithm's update and prediction rules are appealingly simple and computationally efficient without the need to store any past data. We show the algorithm attains $\\mathcal{O}(\\frac{\\ln n}{n})$ regret for concave and smooth metrics and verify the efficiency of the proposed algorithm in empirical studies",
    "checked": true,
    "id": "8267a8f9668b83931d28ce83afbd65ab0df0a7fa",
    "semantic_title": "a general online algorithm for optimizing complex performance metrics",
    "citation_count": 0,
    "authors": [
      "Wojciech Kotlowski",
      "Marek Wydmuch",
      "Erik Schultheis",
      "Rohit Babbar",
      "Krzysztof Dembczynski"
    ]
  },
  "https://proceedings.mlr.press/v235/kou24a.html": {
    "title": "CLLMs: Consistency Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Kou",
      "Lanxiang Hu",
      "Zhezhi He",
      "Zhijie Deng",
      "Hao Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/kou24b.html": {
    "title": "KISA: A Unified Keyframe Identifier and Skill Annotator for Long-Horizon Robotics Demonstrations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longxin Kou",
      "Fei Ni",
      "Yan Zheng",
      "Jinyi Liu",
      "Yifu Yuan",
      "Zibin Dong",
      "Jianye Hao"
    ]
  },
  "https://proceedings.mlr.press/v235/koul24a.html": {
    "title": "PcLast: Discovering Plannable Continuous Latent States",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anurag Koul",
      "Shivakanth Sujit",
      "Shaoru Chen",
      "Ben Evans",
      "Lili Wu",
      "Byron Xu",
      "Rajan Chari",
      "Riashat Islam",
      "Raihan Seraj",
      "Yonathan Efroni",
      "Lekan P Molu",
      "Miroslav Dudı́k",
      "John Langford",
      "Alex Lamb"
    ]
  },
  "https://proceedings.mlr.press/v235/kozdoba24a.html": {
    "title": "Sobolev Space Regularised Pre Density Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Kozdoba",
      "Binyamin Perets",
      "Shie Mannor"
    ]
  },
  "https://proceedings.mlr.press/v235/krasheninnikov24a.html": {
    "title": "Implicit meta-learning may lead language models to trust more reliable sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitrii Krasheninnikov",
      "Egor Krasheninnikov",
      "Bruno Kacper Mlodozeniec",
      "Tegan Maharaj",
      "David Krueger"
    ]
  },
  "https://proceedings.mlr.press/v235/kremer24a.html": {
    "title": "Geometry-Aware Instrumental Variable Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heiner Kremer",
      "Bernhard Schölkopf"
    ]
  },
  "https://proceedings.mlr.press/v235/krishna24a.html": {
    "title": "Understanding the Effects of Iterative Prompting on Truthfulness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satyapriya Krishna",
      "Chirag Agarwal",
      "Himabindu Lakkaraju"
    ]
  },
  "https://proceedings.mlr.press/v235/kristiadi24a.html": {
    "title": "A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agustinus Kristiadi",
      "Felix Strieth-Kalthoff",
      "Marta Skreta",
      "Pascal Poupart",
      "Alan Aspuru-Guzik",
      "Geoff Pleiss"
    ]
  },
  "https://proceedings.mlr.press/v235/kuang24a.html": {
    "title": "Towards General Algorithm Discovery for Combinatorial Optimization: Learning Symbolic Branching Policy from Bipartite Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Kuang",
      "Jie Wang",
      "Yuyan Zhou",
      "Xijun Li",
      "Fangzhou Zhu",
      "Jianye Hao",
      "Feng Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/kulesza24a.html": {
    "title": "Mean Estimation in the Add-Remove Model of Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Kulesza",
      "Ananda Theertha Suresh",
      "Yuyan Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/kumar24a.html": {
    "title": "No Free Prune: Information-Theoretic Barriers to Pruning at Initialization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanishq Kumar",
      "Kevin Luo",
      "Mark Sellke"
    ]
  },
  "https://proceedings.mlr.press/v235/kumar24b.html": {
    "title": "Efficient Value Iteration for s-rectangular Robust Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navdeep Kumar",
      "Kaixin Wang",
      "Kfir Yehuda Levy",
      "Shie Mannor"
    ]
  },
  "https://proceedings.mlr.press/v235/kur24a.html": {
    "title": "Minimum Norm Interpolation Meets The Local Theory of Banach Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gil Kur",
      "Pedro Abdalla",
      "Pierre Bizeul",
      "Fanny Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/kwon24a.html": {
    "title": "Prospective Side Information for Latent MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongyeol Kwon",
      "Yonathan Efroni",
      "Shie Mannor",
      "Constantine Caramanis"
    ]
  },
  "https://proceedings.mlr.press/v235/kwon24b.html": {
    "title": "On The Complexity of First-Order Methods in Stochastic Bilevel Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongyeol Kwon",
      "Dohyun Kwon",
      "Hanbaek Lyu"
    ]
  },
  "https://proceedings.mlr.press/v235/kwon24c.html": {
    "title": "TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Young D. Kwon",
      "Rui Li",
      "Stylianos Venieris",
      "Jagmohan Chauhan",
      "Nicholas Donald Lane",
      "Cecilia Mascolo"
    ]
  },
  "https://proceedings.mlr.press/v235/laenen24a.html": {
    "title": "Dynamic Spectral Clustering with Provable Approximation Guarantee",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steinar Laenen",
      "He Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/lai24a.html": {
    "title": "Collective Certified Robustness against Graph Injection Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuni Lai",
      "Bailin Pan",
      "Kaihuang Chen",
      "Yancheng Yuan",
      "Kai Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/lai24b.html": {
    "title": "Position: Evolving AI Collectives Enhance Human Diversity and Enable Self-Regulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyang Lai",
      "Yujin Potter",
      "Junsol Kim",
      "Richard Zhuang",
      "Dawn Song",
      "James Evans"
    ]
  },
  "https://proceedings.mlr.press/v235/lai24c.html": {
    "title": "Invariant Risk Minimization Is A Total Variation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao-Rong Lai",
      "Weiwen Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/lalanne24a.html": {
    "title": "Privately Learning Smooth Distributions on the Hypercube by Projections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément Lalanne",
      "Sébastien Gadat"
    ]
  },
  "https://proceedings.mlr.press/v235/lan24a.html": {
    "title": "A Neural-Preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Weixian Lan",
      "Elias Gueidon",
      "Ayano Kaneda",
      "Julian Panetta",
      "Joseph Teran"
    ]
  },
  "https://proceedings.mlr.press/v235/lao24a.html": {
    "title": "Sub-token ViT Embedding via Stochastic Resonance Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Lao",
      "Yangchao Wu",
      "Tian Yu Liu",
      "Alex Wong",
      "Stefano Soatto"
    ]
  },
  "https://proceedings.mlr.press/v235/laszkiewicz24a.html": {
    "title": "Single-Model Attribution of Generative Models Through Final-Layer Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mike Laszkiewicz",
      "Jonas Ricker",
      "Johannes Lederer",
      "Asja Fischer"
    ]
  },
  "https://proceedings.mlr.press/v235/lavie24a.html": {
    "title": "Towards Understanding Inductive Bias in Transformers: A View From Infinity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itay Lavie",
      "Guy Gur-Ari",
      "Zohar Ringel"
    ]
  },
  "https://proceedings.mlr.press/v235/lavoie24a.html": {
    "title": "Modeling Caption Diversity in Contrastive Vision-Language Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Lavoie",
      "Polina Kirichenko",
      "Mark Ibrahim",
      "Mido Assran",
      "Andrew Gordon Wilson",
      "Aaron Courville",
      "Nicolas Ballas"
    ]
  },
  "https://proceedings.mlr.press/v235/lazzati24a.html": {
    "title": "Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippo Lazzati",
      "Mirco Mutti",
      "Alberto Maria Metelli"
    ]
  },
  "https://proceedings.mlr.press/v235/le24a.html": {
    "title": "Generalized Sobolev Transport for Probability Measures on a Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tam Le",
      "Truyen Nguyen",
      "Kenji Fukumizu"
    ]
  },
  "https://proceedings.mlr.press/v235/le24b.html": {
    "title": "Robust Inverse Graphics via Probabilistic Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan Anh Le",
      "Pavel Sountsov",
      "Matthew Douglas Hoffman",
      "Ben Lee",
      "Brian Patton",
      "Rif A. Saurous"
    ]
  },
  "https://proceedings.mlr.press/v235/le24c.html": {
    "title": "Knowledge Graphs Can be Learned with Just Intersection Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duy Le",
      "Shaochen Zhong",
      "Zirui Liu",
      "Shuai Xu",
      "Vipin Chaudhary",
      "Kaixiong Zhou",
      "Zhaozhuo Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/le-bars24a.html": {
    "title": "Improved Stability and Generalization Guarantees of the Decentralized SGD Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Batiste Le Bars",
      "Aurélien Bellet",
      "Marc Tommasi",
      "Kevin Scaman",
      "Giovanni Neglia"
    ]
  },
  "https://proceedings.mlr.press/v235/leahy24a.html": {
    "title": "Run-Time Task Composition with Safety Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Leahy",
      "Makai Mann",
      "Zachary Serlin"
    ]
  },
  "https://proceedings.mlr.press/v235/lechowicz24a.html": {
    "title": "Chasing Convex Functions with Long-term Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Lechowicz",
      "Nicolas Christianson",
      "Bo Sun",
      "Noman Bashir",
      "Mohammad Hajiesmaili",
      "Adam Wierman",
      "Prashant Shenoy"
    ]
  },
  "https://proceedings.mlr.press/v235/ledent24a.html": {
    "title": "Generalization Analysis of Deep Non-linear Matrix Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Ledent",
      "Rodrigo Alves"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24a.html": {
    "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Lee",
      "Xiaoyan Bai",
      "Itamar Pres",
      "Martin Wattenberg",
      "Jonathan K. Kummerfeld",
      "Rada Mihalcea"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24b.html": {
    "title": "Stationary Latent Weight Inference for Unreliable Observations from Online Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae-Hong Lee",
      "Joon-Hyuk Chang"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24c.html": {
    "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuang-Huei Lee",
      "Xinyun Chen",
      "Hiroki Furuta",
      "John Canny",
      "Ian Fischer"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24d.html": {
    "title": "Slow and Steady Wins the Race: Maintaining Plasticity with Hare and Tortoise Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hojoon Lee",
      "Hyeonseo Cho",
      "Hyunseung Kim",
      "Donghu Kim",
      "Dugki Min",
      "Jaegul Choo",
      "Clare Lyle"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24e.html": {
    "title": "Fundamental Benefit of Alternating Updates in Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewook Lee",
      "Hanseul Cho",
      "Chulhee Yun"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24f.html": {
    "title": "DataFreeShield: Defending Adversarial Attacks without Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeyoon Lee",
      "Kanghyun Choi",
      "Dain Kwon",
      "Sunjong Park",
      "Mayoore Selvarasa Jaiswal",
      "Noseong Park",
      "Jonghyun Choi",
      "Jinho Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24g.html": {
    "title": "SelMatch: Effectively Scaling Up Dataset Distillation via Selection-Based Initialization and Partial Updates by Trajectory Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongmin Lee",
      "Hye Won Chung"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24h.html": {
    "title": "Recurrent Early Exits for Federated Learning with Heterogeneous Clients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Royson Lee",
      "Javier Fernandez-Marques",
      "Shell Xu Hu",
      "Da Li",
      "Stefanos Laskaridis",
      "Łukasz Dudziak",
      "Timothy Hospedales",
      "Ferenc Huszár",
      "Nicholas Donald Lane"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24i.html": {
    "title": "PAC-Bayesian Generalization Bounds for Knowledge Graph Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaejun Lee",
      "Minsung Hwang",
      "Joyce Jiyoung Whang"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24j.html": {
    "title": "Learning to Continually Learn with the Bayesian Principle",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soochan Lee",
      "Hyeonseong Jeon",
      "Jaehyeon Son",
      "Gunhee Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24k.html": {
    "title": "Graph Neural Networks with a Distribution of Parametrized Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "See Hian Lee",
      "Feng Ji",
      "Kelin Xia",
      "Wee Peng Tay"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24l.html": {
    "title": "Pausing Policy Learning in Non-stationary Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunin Lee",
      "Ming Jin",
      "Javad Lavaei",
      "Somayeh Sojoudi"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24m.html": {
    "title": "Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soo Yong Lee",
      "Sunwoo Kim",
      "Fanchen Bu",
      "Jaemin Yoo",
      "Jiliang Tang",
      "Kijung Shin"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24n.html": {
    "title": "Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hagyeong Lee",
      "Minkyu Kim",
      "Jun-Hyuk Kim",
      "Seungeon Kim",
      "Dokwan Oh",
      "Jaeho Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24o.html": {
    "title": "Drug Discovery with Dynamic Goal-aware Fragments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seul Lee",
      "Seanie Lee",
      "Kenji Kawaguchi",
      "Sung Ju Hwang"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24p.html": {
    "title": "Supervised Matrix Factorization: Local Landscape Analysis and Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joowon Lee",
      "Hanbaek Lyu",
      "Weixin Yao"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24q.html": {
    "title": "Defining Neural Network Architecture through Polytope Structures of Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangmin Lee",
      "Abbas Mammadov",
      "Jong Chul Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24r.html": {
    "title": "Why Do Animals Need Shaping? A Theory of Task Composition and Curriculum Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Hwa Lee",
      "Stefano Sarao Mannelli",
      "Andrew M Saxe"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24s.html": {
    "title": "3D Geometric Shape Assembly via Efficient Point Cloud Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nahyuk Lee",
      "Juhong Min",
      "Junha Lee",
      "Seungwook Kim",
      "Kanghee Lee",
      "Jaesik Park",
      "Minsu Cho"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24t.html": {
    "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harrison Lee",
      "Samrat Phatale",
      "Hassan Mansoor",
      "Thomas Mesnard",
      "Johan Ferret",
      "Kellie Ren Lu",
      "Colton Bishop",
      "Ethan Hall",
      "Victor Carbune",
      "Abhinav Rastogi",
      "Sushant Prakash"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24u.html": {
    "title": "StrWAEs to Invariant Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunjong Lee",
      "Yedarm Seong",
      "Sungdong Lee",
      "Joong-Ho Won"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24v.html": {
    "title": "Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungeun Lee",
      "Ye Seul Sim",
      "Hyeseung Cho",
      "Moonjung Eo",
      "Suhee Yoon",
      "Sanghyu Yoon",
      "Woohyung Lim"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24w.html": {
    "title": "Training Greedy Policy for Proposal Batch Selection in Expensive Multi-Objective Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deokjae Lee",
      "Hyun Oh Song",
      "Kyunghyun Cho"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24x.html": {
    "title": "Robust Optimization in Protein Fitness Landscapes Using Reinforcement Learning in Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minji Lee",
      "Luiz Felipe Vecchietti",
      "Hyunkyu Jung",
      "Hyun Joo Ro",
      "Meeyoung Cha",
      "Ho Min Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24y.html": {
    "title": "Behavior Generation with Latent Actions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjae Lee",
      "Yibin Wang",
      "Haritheja Etukuru",
      "H. Jin Kim",
      "Nur Muhammad Mahi Shafiullah",
      "Lerrel Pinto"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24z.html": {
    "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonho Lee",
      "Jae Oh Woo",
      "Juree Seok",
      "Parisa Hassanzadeh",
      "Wooseok Jang",
      "Juyoun Son",
      "Sima Didari",
      "Baruch Gutow",
      "Heng Hao",
      "Hankyu Moon",
      "Wenjun Hu",
      "Yeong-Dae Kwon",
      "Taehee Lee",
      "Seungjai Min"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24aa.html": {
    "title": "Rethinking the Flat Minima Searching in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehwan Lee",
      "Sung Whan Yoon"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24ab.html": {
    "title": "BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daeun Lee",
      "Jaehong Yoon",
      "Sung Ju Hwang"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24ac.html": {
    "title": "STELLA: Continual Audio-Video Pre-training with SpatioTemporal Localized Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoo Lee",
      "Jaehong Yoon",
      "Wonjae Kim",
      "Yunji Kim",
      "Sung Ju Hwang"
    ]
  },
  "https://proceedings.mlr.press/v235/lee24ad.html": {
    "title": "Sign Rank Limitations for Inner Product Graph Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Su Hyeong Lee",
      "Qingqi Zhang",
      "Risi Kondor"
    ]
  },
  "https://proceedings.mlr.press/v235/legacci24a.html": {
    "title": "A Geometric Decomposition of Finite Games: Convergence vs. Recurrence under Exponential Weights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Legacci",
      "Panayotis Mertikopoulos",
      "Bary Pradelski"
    ]
  },
  "https://proceedings.mlr.press/v235/lei24a.html": {
    "title": "Langevin Policy for Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fenghao Lei",
      "Long Yang",
      "Shiting Wen",
      "Zhixiong Huang",
      "Zhiwang Zhang",
      "Chaoyi Pang"
    ]
  },
  "https://proceedings.mlr.press/v235/leluc24a.html": {
    "title": "Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rémi Leluc",
      "Aymeric Dieuleveut",
      "François Portier",
      "Johan Segers",
      "Aigerim Zhuman"
    ]
  },
  "https://proceedings.mlr.press/v235/lemercier24a.html": {
    "title": "An Independence-promoting Loss for Music Generation with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean-Marie Lemercier",
      "Simon Rouard",
      "Jade Copet",
      "Yossi Adi",
      "Alexandre Défossez"
    ]
  },
  "https://proceedings.mlr.press/v235/lemos24a.html": {
    "title": "Improving Gradient-Guided Nested Sampling for Posterior Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Lemos",
      "Nikolay Malkin",
      "Will Handley",
      "Yoshua Bengio",
      "Yashar Hezaveh",
      "Laurence Perreault-Levasseur"
    ]
  },
  "https://proceedings.mlr.press/v235/letzelter24a.html": {
    "title": "Winner-takes-all learners are geometry-aware conditional density estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Letzelter",
      "David Perera",
      "Cédric Rommel",
      "Mathieu Fontaine",
      "Slim Essid",
      "Gaël Richard",
      "Patrick Perez"
    ]
  },
  "https://proceedings.mlr.press/v235/leveni24a.html": {
    "title": "Online Isolation Forest",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippo Leveni",
      "Guilherme Weigert Cassales",
      "Bernhard Pfahringer",
      "Albert Bifet",
      "Giacomo Boracchi"
    ]
  },
  "https://proceedings.mlr.press/v235/levine24a.html": {
    "title": "Cell2Sentence: Teaching Large Language Models the Language of Biology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Levine",
      "Syed A Rizvi",
      "Sacha Lévy",
      "Nazreen Pallikkavaliyaveetil",
      "David Zhang",
      "Xingyu Chen",
      "Sina Ghadermarzi",
      "Ruiming Wu",
      "Zihe Zheng",
      "Ivan Vrkic",
      "Anna Zhong",
      "Daphne Raskin",
      "Insu Han",
      "Antonio Henrique De Oliveira Fonseca",
      "Josue Ortega Caro",
      "Amin Karbasi",
      "Rahul Madhav Dhodapkar",
      "David Van Dijk"
    ]
  },
  "https://proceedings.mlr.press/v235/levy24a.html": {
    "title": "Eluder-based Regret for Stochastic Contextual MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orin Levy",
      "Asaf Cassel",
      "Alon Cohen",
      "Yishay Mansour"
    ]
  },
  "https://proceedings.mlr.press/v235/li24a.html": {
    "title": "Feature Reuse and Scaling: Understanding Transfer Learning with Protein Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesca-Zhoufan Li",
      "Ava P Amini",
      "Yisong Yue",
      "Kevin K Yang",
      "Alex Xijie Lu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24b.html": {
    "title": "Convergence and Complexity Guarantee for Inexact First-order Riemannian Optimization Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Li",
      "Laura Balzano",
      "Deanna Needell",
      "Hanbaek Lyu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24c.html": {
    "title": "DetKDS: Knowledge Distillation Search for Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujun Li",
      "Yufan Bao",
      "Peijie Dong",
      "Chuanguang Yang",
      "Anggeng Li",
      "Wenhan Luo",
      "Qifeng Liu",
      "Wei Xue",
      "Yike Guo"
    ]
  },
  "https://proceedings.mlr.press/v235/li24d.html": {
    "title": "Denoising Autoregressive Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yazhe Li",
      "Jorg Bornschein",
      "Ting Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/li24e.html": {
    "title": "Purifying Quantization-conditioned Backdoors via Layer-wise Activation Correction with Distribution Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boheng Li",
      "Yishuo Cai",
      "Jisong Cai",
      "Yiming Li",
      "Han Qiu",
      "Run Wang",
      "Tianwei Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24f.html": {
    "title": "Improving Neural Logic Machines via Failure Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiming Li",
      "Yushi Cao",
      "Yan Zheng",
      "Xu Liu",
      "Bozhi Wu",
      "Tianlin Li",
      "Xiufeng Xu",
      "Junzhe Jiang",
      "Yon Shin Teo",
      "Shang-Wei Lin",
      "Yang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24g.html": {
    "title": "Critical windows: non-asymptotic theory for feature emergence in diffusion models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marvin Li",
      "Sitan Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/li24h.html": {
    "title": "Learning Causal Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuke Li",
      "Guangyi Chen",
      "Ben Abramowitz",
      "Stefano Anzellotti",
      "Donglai Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/li24i.html": {
    "title": "GiLOT: Interpreting Generative Language Models via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuhong Li",
      "Jiamin Chen",
      "Yekun Chai",
      "Haoyi Xiong"
    ]
  },
  "https://proceedings.mlr.press/v235/li24j.html": {
    "title": "Completing Visual Objects via Bridging Generation and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Yinpeng Chen",
      "Chung-Ching Lin",
      "Hao Chen",
      "Kai Hu",
      "Rita Singh",
      "Bhiksha Raj",
      "Lijuan Wang",
      "Zicheng Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24k.html": {
    "title": "Evolving Subnetwork Training for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqi Li",
      "Lu Chen",
      "Da Ma",
      "Zijian Wu",
      "Su Zhu",
      "Kai Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24l.html": {
    "title": "Data Poisoning Attacks against Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyi Li",
      "Aobo Chen",
      "Wei Qian",
      "Chenxu Zhao",
      "Divya Lidder",
      "Mengdi Huai"
    ]
  },
  "https://proceedings.mlr.press/v235/li24m.html": {
    "title": "ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshuo Li",
      "Xinghao Chen",
      "Han Shu",
      "Yehui Tang",
      "Yunhe Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24n.html": {
    "title": "Two-sided Competing Matching Recommendation Markets With Quota and Complementary Preferences Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuantong Li",
      "Guang Cheng",
      "Xiaowu Dai"
    ]
  },
  "https://proceedings.mlr.press/v235/li24o.html": {
    "title": "Full-Atom Peptide Design based on Multi-modal Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahan Li",
      "Chaoran Cheng",
      "Zuofan Wu",
      "Ruihan Guo",
      "Shitong Luo",
      "Zhizhou Ren",
      "Jian Peng",
      "Jianzhu Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/li24p.html": {
    "title": "Positive and Unlabeled Learning with Controlled Probability Boundary Fence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changchun Li",
      "Yuanchao Dai",
      "Lei Feng",
      "Ximing Li",
      "Bing Wang",
      "Jihong Ouyang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24q.html": {
    "title": "FightLadder: A Benchmark for Competitive Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhe Li",
      "Zihan Ding",
      "Seth Karten",
      "Chi Jin"
    ]
  },
  "https://proceedings.mlr.press/v235/li24r.html": {
    "title": "Debiased Distribution Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingxiao Li",
      "Raaz Dwivedi",
      "Lester Mackey"
    ]
  },
  "https://proceedings.mlr.press/v235/li24s.html": {
    "title": "Improving Context Understanding in Multimodal Large Language Models via Multimodal Composition Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Hehe Fan",
      "Yongkang Wong",
      "Yi Yang",
      "Mohan Kankanhalli"
    ]
  },
  "https://proceedings.mlr.press/v235/li24t.html": {
    "title": "RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boning Li",
      "Zhixuan Fang",
      "Longbo Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24u.html": {
    "title": "Vague Prototype-Oriented Diffusion Model for Multi-Class Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Li",
      "Yaoxuan Feng",
      "Bo Chen",
      "Wenchao Chen",
      "Yubiao Wang",
      "Xinyue Hu",
      "Baolin Sun",
      "Chunhui Qu",
      "Mingyuan Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/li24v.html": {
    "title": "Automated Statistical Model Discovery with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Y. Li",
      "Emily Fox",
      "Noah Goodman"
    ]
  },
  "https://proceedings.mlr.press/v235/li24w.html": {
    "title": "Adaptive Feature Selection for No-Reference Image Quality Assessment by Mitigating Semantic Noise Sensitivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Li",
      "Timin Gao",
      "Runze Hu",
      "Yan Zhang",
      "Shengchuan Zhang",
      "Xiawu Zheng",
      "Jingyuan Zheng",
      "Yunhang Shen",
      "Ke Li",
      "Yutao Liu",
      "Pingyang Dai",
      "Rongrong Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/li24x.html": {
    "title": "Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Li",
      "Qiaozi Gao",
      "Michael Johnston",
      "Xiaofeng Gao",
      "Xuehai He",
      "Hangjie Shi",
      "Suhaila Shakiah",
      "Reza Ghanadan",
      "William Yang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24y.html": {
    "title": "Graph Structure Extrapolation for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiner Li",
      "Shurui Gui",
      "Youzhi Luo",
      "Shuiwang Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/li24z.html": {
    "title": "Value-Evolutionary-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyi Li",
      "Jianye Hao",
      "Hongyao Tang",
      "Yan Zheng",
      "Fazl Barez"
    ]
  },
  "https://proceedings.mlr.press/v235/li24aa.html": {
    "title": "Image Clustering with External Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfan Li",
      "Peng Hu",
      "Dezhong Peng",
      "Jiancheng Lv",
      "Jianping Fan",
      "Xi Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ab.html": {
    "title": "VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxin Li",
      "Baotian Hu",
      "Haoyuan Shi",
      "Wei Wang",
      "Longyue Wang",
      "Min Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ac.html": {
    "title": "Integrating Global Context Contrast and Local Sensitivity for Blind Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Li",
      "Runze Hu",
      "Jingyuan Zheng",
      "Yan Zhang",
      "Shengchuan Zhang",
      "Xiawu Zheng",
      "Ke Li",
      "Yunhang Shen",
      "Yutao Liu",
      "Pingyang Dai",
      "Rongrong Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ad.html": {
    "title": "Accelerating Convergence of Score-Based Diffusion Models, Provably",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gen Li",
      "Yu Huang",
      "Timofey Efimov",
      "Yuting Wei",
      "Yuejie Chi",
      "Yuxin Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ae.html": {
    "title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenneth Li",
      "Samy Jelassi",
      "Hugh Zhang",
      "Sham M. Kakade",
      "Martin Wattenberg",
      "David Brandfonbrener"
    ]
  },
  "https://proceedings.mlr.press/v235/li24af.html": {
    "title": "Promises and Pitfalls of Generative Masked Language Modeling: Theoretical Framework and Practical Guidelines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Li",
      "Alexandre Kirchmeyer",
      "Aashay Mehta",
      "Yilong Qin",
      "Boris Dadachev",
      "Kishore Papineni",
      "Sanjiv Kumar",
      "Andrej Risteski"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ag.html": {
    "title": "Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhao Li",
      "Haopeng Li",
      "Sarah Monazam Erfani",
      "Lei Feng",
      "James Bailey",
      "Feng Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ah.html": {
    "title": "Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Chaozhuo Li",
      "Yanming Shen",
      "Zeyu Zhang",
      "Xu Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ai.html": {
    "title": "Neural Collapse in Multi-label Learning with Pick-all-label Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Li",
      "Xiao Li",
      "Yutong Wang",
      "Qing Qu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24aj.html": {
    "title": "A Differentiable Partially Observable Generalized Linear Model with Forward-Backward Message Passing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengrui Li",
      "Weihan Li",
      "Yule Wang",
      "Anqi Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ak.html": {
    "title": "Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihan Li",
      "Chengrui Li",
      "Yule Wang",
      "Anqi Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24al.html": {
    "title": "A Generative Approach for Treatment Effect Estimation under Collider Bias: From an Out-of-Distribution Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baohong Li",
      "Haoxuan Li",
      "Anpeng Wu",
      "Minqin Zhu",
      "Shiyuan Peng",
      "Qingyu Cao",
      "Kun Kuang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24am.html": {
    "title": "Learning Shadow Variable Representation for Treatment Effect Estimation under Collider Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baohong Li",
      "Haoxuan Li",
      "Ruoxuan Xiong",
      "Anpeng Wu",
      "Fei Wu",
      "Kun Kuang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24an.html": {
    "title": "Configurable Mirror Descent: Towards a Unification of Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengdeng Li",
      "Shuxin Li",
      "Chang Yang",
      "Xinrun Wang",
      "Shuyue Hu",
      "Xiao Huang",
      "Hau Chan",
      "Bo An"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ao.html": {
    "title": "Enhancing Class-Imbalanced Learning with Pre-Trained Guidance through Class-Conditional Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Li",
      "Xin-Chun Li",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ap.html": {
    "title": "A Neural-Guided Dynamic Symbolic Network for Exploring Mathematical Expressions from Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqiang Li",
      "Weijun Li",
      "Lina Yu",
      "Min Wu",
      "Linjun Sun",
      "Jingyi Liu",
      "Yanjie Li",
      "Shu Wei",
      "Deng Yusong",
      "Meilan Hao"
    ]
  },
  "https://proceedings.mlr.press/v235/li24aq.html": {
    "title": "Cascade-CLIP: Cascaded Vision-Language Embeddings Alignment for Zero-Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunheng Li",
      "Zhong-Yu Li",
      "Quan-Sheng Zeng",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ar.html": {
    "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengshu Li",
      "Jacky Liang",
      "Andy Zeng",
      "Xinyun Chen",
      "Karol Hausman",
      "Dorsa Sadigh",
      "Sergey Levine",
      "Li Fei-Fei",
      "Fei Xia",
      "Brian Ichter"
    ]
  },
  "https://proceedings.mlr.press/v235/li24as.html": {
    "title": "Preventing Model Collapse in Gaussian Process Latent Variable Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Li",
      "Zhidi Lin",
      "Feng Yin",
      "Michael Minyi Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24at.html": {
    "title": "A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boqi Li",
      "Weiwei Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24au.html": {
    "title": "Concentration Inequalities for General Functions of Heavy-Tailed Random Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaojie Li",
      "Yong Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24av.html": {
    "title": "Sparse Cocktail: Every Sparse Pattern Every Sparse Ratio All At Once",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangheng Li",
      "Shiwei Liu",
      "Tianlong Chen",
      "Ajay Kumar Jaiswal",
      "Zhenyu Zhang",
      "Dilin Wang",
      "Raghuraman Krishnamoorthi",
      "Shiyu Chang",
      "Zhangyang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24aw.html": {
    "title": "Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Li",
      "Zifan Liu",
      "Shibo Chen",
      "Jun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ax.html": {
    "title": "Learning Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongxin Li",
      "Mengyuan Liu",
      "You Wu",
      "Xucheng Wang",
      "Xiangyang Yang",
      "Shuiwang Li"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ay.html": {
    "title": "PID: Prompt-Independent Data Protection Against Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ang Li",
      "Yichuan Mo",
      "Mingjie Li",
      "Yisen Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24az.html": {
    "title": "A Contextual Combinatorial Bandit Approach to Negotiation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexin Li",
      "Zhancun Mu",
      "Siyuan Qi"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ba.html": {
    "title": "Improving Prototypical Visual Explanations with Reward Reweighing, Reselection, and Retraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron Jiaxun Li",
      "Robin Netzorg",
      "Zhihan Cheng",
      "Zhuoqin Zhang",
      "Bin Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bb.html": {
    "title": "Evaluating Quantized Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyao Li",
      "Xuefei Ning",
      "Luning Wang",
      "Tengxuan Liu",
      "Xiangsheng Shi",
      "Shengen Yan",
      "Guohao Dai",
      "Huazhong Yang",
      "Yu Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bc.html": {
    "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathaniel Li",
      "Alexander Pan",
      "Anjali Gopal",
      "Summer Yue",
      "Daniel Berrios",
      "Alice Gatti",
      "Justin D. Li",
      "Ann-Kathrin Dombrowski",
      "Shashwat Goel",
      "Gabriel Mukobi",
      "Nathan Helm-Burger",
      "Rassin Lababidi",
      "Lennart Justen",
      "Andrew Bo Liu",
      "Michael Chen",
      "Isabelle Barrass",
      "Oliver Zhang",
      "Xiaoyuan Zhu",
      "Rishub Tamirisa",
      "Bhrugu Bharathi",
      "Ariel Herbert-Voss",
      "Cort B Breuer",
      "Andy Zou",
      "Mantas Mazeika",
      "Zifan Wang",
      "Palash Oswal",
      "Weiran Lin",
      "Adam Alfred Hunt",
      "Justin Tienken-Harder",
      "Kevin Y. Shih",
      "Kemper Talley",
      "John Guan",
      "Ian Steneker",
      "David Campbell",
      "Brad Jokubaitis",
      "Steven Basart",
      "Stephen Fitz",
      "Ponnurangam Kumaraguru",
      "Kallol Krishna Karmakar",
      "Uday Tupakula",
      "Vijay Varadharajan",
      "Yan Shoshitaishvili",
      "Jimmy Ba",
      "Kevin M. Esvelt",
      "Alexandr Wang",
      "Dan Hendrycks"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bd.html": {
    "title": "Graph Neural Network Explanations are Fragile",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiate Li",
      "Meng Pang",
      "Yun Dong",
      "Jinyuan Jia",
      "Binghui Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24be.html": {
    "title": "When Do Skills Help Reinforcement Learning? A Theoretical Analysis of Temporal Abstractions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhening Li",
      "Gabriel Poesia",
      "Armando Solar-Lezama"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bf.html": {
    "title": "DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanghe Li",
      "Yixiang Shan",
      "Zhengbang Zhu",
      "Ting Long",
      "Weinan Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bg.html": {
    "title": "Privacy Preserving Adaptive Experiment Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachun Li",
      "Kaining Shi",
      "David Simchi-Levi"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bh.html": {
    "title": "Combining Experimental and Historical Data for Policy Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Li",
      "Chengchun Shi",
      "Qianglin Wen",
      "Yang Sui",
      "Yongli Qin",
      "Chunbo Lai",
      "Hongtu Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bi.html": {
    "title": "LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyan Li",
      "Yongqiang Tang",
      "Wensheng Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bj.html": {
    "title": "DiffFPR: Diffusion Prior for Oversampled Fourier Phase Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Li",
      "Chao Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bk.html": {
    "title": "Harnessing Neural Unit Dynamics for Effective and Scalable Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Depeng Li",
      "Tianqi Wang",
      "Junwei Chen",
      "Wei Dai",
      "Zhigang Zeng"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bl.html": {
    "title": "Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Li",
      "Yufei Wang",
      "Alex Kot",
      "Bihan Wen"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bm.html": {
    "title": "VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Li",
      "Zedong Wang",
      "Zicheng Liu",
      "Di Wu",
      "Cheng Tan",
      "Jiangbin Zheng",
      "Yufei Huang",
      "Stan Z. Li"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bn.html": {
    "title": "How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongkang Li",
      "Meng Wang",
      "Songtao Lu",
      "Xiaodong Cui",
      "Pin-Yu Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bo.html": {
    "title": "What Improves the Generalization of Graph Transformers? A Theoretical Dive into the Self-attention and Positional Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongkang Li",
      "Meng Wang",
      "Tengfei Ma",
      "Sijia Liu",
      "Zaixi Zhang",
      "Pin-Yu Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bp.html": {
    "title": "OODRobustBench: a Benchmark and Large-Scale Analysis of Adversarial Robustness under Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Li",
      "Yifei Wang",
      "Chawin Sitawarin",
      "Michael W. Spratling"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bq.html": {
    "title": "Improved Bounds for Pure Private Agnostic Learning: Item-Level and User-Level Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Wei Wang",
      "Peng Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/li24br.html": {
    "title": "Disentangled Graph Self-supervised Learning for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Li",
      "Xin Wang",
      "Zeyang Zhang",
      "Haibo Chen",
      "Ziwei Zhang",
      "Wenwu Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bs.html": {
    "title": "The Good, The Bad, and Why: Unveiling Emotions in Generative AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Li",
      "Jindong Wang",
      "Yixuan Zhang",
      "Kaijie Zhu",
      "Xinyi Wang",
      "Wenxin Hou",
      "Jianxun Lian",
      "Fang Luo",
      "Qiang Yang",
      "Xing Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bt.html": {
    "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Li",
      "Fangyun Wei",
      "Chao Zhang",
      "Hongyang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bu.html": {
    "title": "Two-Stage Shadow Inclusion Estimation: An IV Approach for Causal Inference under Latent Confounding and Collider Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baohong Li",
      "Anpeng Wu",
      "Ruoxuan Xiong",
      "Kun Kuang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bv.html": {
    "title": "Towards Realistic Model Selection for Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muyang Li",
      "Xiaobo Xia",
      "Runze Wu",
      "Fengming Huang",
      "Jun Yu",
      "Bo Han",
      "Tongliang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bw.html": {
    "title": "FlashST: A Simple and Universal Prompt-Tuning Framework for Traffic Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghang Li",
      "Lianghao Xia",
      "Yong Xu",
      "Chao Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bx.html": {
    "title": "Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiran Li",
      "Qianqian Xu",
      "Shilong Bao",
      "Zhiyong Yang",
      "Runmin Cong",
      "Xiaochun Cao",
      "Qingming Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24by.html": {
    "title": "Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice via HyperAgent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingru Li",
      "Jiawei Xu",
      "Lei Han",
      "Zhi-Quan Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/li24bz.html": {
    "title": "Towards efficient deep spiking neural networks construction with spiking activity based pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaxin Li",
      "Qi Xu",
      "Jiangrong Shen",
      "Hongming Xu",
      "Long Chen",
      "Gang Pan"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ca.html": {
    "title": "FedBAT: Communication-Efficient Federated Learning via Learnable Binarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwei Li",
      "Wenchao Xu",
      "Haozhao Wang",
      "Xing Tang",
      "Yining Qi",
      "Shijie Xu",
      "Weihong Luo",
      "Yuhua Li",
      "Xiuqiang He",
      "Ruixuan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cb.html": {
    "title": "Beyond Point Prediction: Score Matching-based Pseudolikelihood Estimation of Neural Marked Spatio-Temporal Point Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichong Li",
      "Qunzhi Xu",
      "Zhenghao Xu",
      "Yajun Mei",
      "Tuo Zhao",
      "Hongyuan Zha"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cc.html": {
    "title": "Statistical Properties of Robust Satisficing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyi Li",
      "Yunbei Xu",
      "Ruohan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cd.html": {
    "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziniu Li",
      "Tian Xu",
      "Yushun Zhang",
      "Zhihang Lin",
      "Yang Yu",
      "Ruoyu Sun",
      "Zhi-Quan Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ce.html": {
    "title": "PDHG-Unrolled Learning-to-Optimize Method for Large-Scale Linear Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingheng Li",
      "Linxin Yang",
      "Yupeng Chen",
      "Senmiao Wang",
      "Haitao Mao",
      "Qian Chen",
      "Yao Ma",
      "Akang Wang",
      "Tian Ding",
      "Jiliang Tang",
      "Ruoyu Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cf.html": {
    "title": "IIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Li",
      "Runxuan Yang",
      "Fuchun Sun",
      "Xiaolin Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cg.html": {
    "title": "KernelWarehouse: Rethinking the Design of Dynamic Convolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Li",
      "Anbang Yao"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ch.html": {
    "title": "GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Li",
      "Yu Ye",
      "Bingchuan Jiang",
      "Wei Zeng"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ci.html": {
    "title": "Learning the Uncertainty Sets of Linear Control Systems via Set Membership: A Non-asymptotic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingying Li",
      "Jing Yu",
      "Lauren Conger",
      "Taylan Kargin",
      "Adam Wierman"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cj.html": {
    "title": "Seesaw: Compensating for Nonlinear Reduction with Linear Computations for Private Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabing Li",
      "Yuanhao Zhai",
      "Shuangyu Cai",
      "Mingyu Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/li24ck.html": {
    "title": "Agnostic Interactive Imitation Learning: New Theory and Practical Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Li",
      "Chicheng Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cl.html": {
    "title": "Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Li",
      "Zicheng Zhang",
      "Wang Luo",
      "Congying Han",
      "Yudong Hu",
      "Tiande Guo",
      "Shichen Liao"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cm.html": {
    "title": "Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Li",
      "Shichang Zhang",
      "Longwen Tang",
      "Mathieu Bauchy",
      "Yizhou Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cn.html": {
    "title": "From Fourier to Neural ODEs: Flow Matching for Modeling Complex Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Li",
      "Jingdong Zhang",
      "Qunxi Zhu",
      "Chengli Zhao",
      "Xue Zhang",
      "Xiaojun Duan",
      "Wei Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/li24co.html": {
    "title": "Feel-Good Thompson Sampling for Contextual Dueling Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuheng Li",
      "Heyang Zhao",
      "Quanquan Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cp.html": {
    "title": "EvoRainbow: Combining Improvements in Evolutionary Reinforcement Learning for Policy Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyi Li",
      "Yan Zheng",
      "Hongyao Tang",
      "Xian Fu",
      "Jianye Hao"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cq.html": {
    "title": "Relaxing the Accurate Imputation Assumption in Doubly Robust Learning for Debiased Collaborative Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Li",
      "Chunyuan Zheng",
      "Shuyi Wang",
      "Kunhan Wu",
      "Eric Wang",
      "Peng Wu",
      "Zhi Geng",
      "Xu Chen",
      "Xiao-Hua Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cr.html": {
    "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxiong Li",
      "Jinliang Zheng",
      "Yinan Zheng",
      "Liyuan Mao",
      "Xiao Hu",
      "Sijie Cheng",
      "Haoyi Niu",
      "Jihao Liu",
      "Yu Liu",
      "Jingjing Liu",
      "Ya-Qin Zhang",
      "Xianyuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/li24cs.html": {
    "title": "Algorithmic Stability Unleashed: Generalization Bounds with Unbounded Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaojie Li",
      "Bowei Zhu",
      "Yong Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/lian24a.html": {
    "title": "Kepler codebook",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junrong Lian",
      "Ziyue Dong",
      "Pengxu Wei",
      "Wei Ke",
      "Chang Liu",
      "Qixiang Ye",
      "Xiangyang Ji",
      "Liang Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/lian24b.html": {
    "title": "Receptive Fields As Experts in Convolutional Neural Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongze Lian",
      "Weihao Yu",
      "Xinchao Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/lian24c.html": {
    "title": "Diving into Underwater: Segment Anything Model Guided Underwater Salient Instance Segmentation and A Large-scale Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Lian",
      "Ziyi Zhang",
      "Hua Li",
      "Wenjie Li",
      "Laurence Tianruo Yang",
      "Sam Kwong",
      "Runmin Cong"
    ]
  },
  "https://proceedings.mlr.press/v235/liang24a.html": {
    "title": "Graph External Attention Enhanced Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianqing Liang",
      "Min Chen",
      "Jiye Liang"
    ]
  },
  "https://proceedings.mlr.press/v235/liang24b.html": {
    "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixin Liang",
      "Zachary Izzo",
      "Yaohui Zhang",
      "Haley Lepp",
      "Hancheng Cao",
      "Xuandong Zhao",
      "Lingjiao Chen",
      "Haotian Ye",
      "Sheng Liu",
      "Zhi Huang",
      "Daniel Mcfarland",
      "James Y. Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/liang24c.html": {
    "title": "Sign is Not a Remedy: Multiset-to-Multiset Message Passing for Learning on Heterophilic Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Langzhang Liang",
      "Sunwoo Kim",
      "Kijung Shin",
      "Zenglin Xu",
      "Shirui Pan",
      "Yuan Qi"
    ]
  },
  "https://proceedings.mlr.press/v235/liang24d.html": {
    "title": "Single-Trajectory Distributionally Robust Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Liang",
      "Xiaoteng Ma",
      "Jose Blanchet",
      "Jun Yang",
      "Jiheng Zhang",
      "Zhengyuan Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/liang24e.html": {
    "title": "Realistic Unsupervised CLIP Fine-tuning with Universal Entropy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Liang",
      "Lijun Sheng",
      "Zhengbo Wang",
      "Ran He",
      "Tieniu Tan"
    ]
  },
  "https://proceedings.mlr.press/v235/liang24f.html": {
    "title": "Efficient Precision and Recall Metrics for Assessing Generative Models using Hubness-aware Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanbang Liang",
      "Jing Wu",
      "Yu-Kun Lai",
      "Yipeng Qin"
    ]
  },
  "https://proceedings.mlr.press/v235/liang24g.html": {
    "title": "Scalable Multiple Kernel Clustering: Learning Clustering Structure from Expectation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixuan Liang",
      "En Zhu",
      "Shengju Yu",
      "Huiying Xu",
      "Xinzhong Zhu",
      "Xinwang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/liao24a.html": {
    "title": "On the Error-Propagation of Inexact Hotelling's Deflation for Principal Component Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangshuo Liao",
      "Junhyung Lyle Kim",
      "Cruz Barnum",
      "Anastasios Kyrillidis"
    ]
  },
  "https://proceedings.mlr.press/v235/liao24b.html": {
    "title": "Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luofeng Liao",
      "Christian Kroer"
    ]
  },
  "https://proceedings.mlr.press/v235/lien24a.html": {
    "title": "Enhancing Value Function Estimation through First-Order State-Action Dynamics in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun-Hsuan Lien",
      "Ping-Chun Hsieh",
      "Tzu-Mao Li",
      "Yu-Shuen Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/lim24a.html": {
    "title": "Graph Geometry-Preserving Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungbin Lim",
      "Jihwan Kim",
      "Yonghyeon Lee",
      "Cheongjae Jang",
      "Frank C. Park"
    ]
  },
  "https://proceedings.mlr.press/v235/lim24b.html": {
    "title": "Momentum Particle Maximum Likelihood",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jen Ning Lim",
      "Juan Kuntz",
      "Samuel Power",
      "Adam Michael Johansen"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24a.html": {
    "title": "An Effective Dynamic Gradient Calibration Method for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichen Lin",
      "Jiaxiang Chen",
      "Ruomin Huang",
      "Hu Ding"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24b.html": {
    "title": "Equivariant Diffusion for Crystal Structure Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijia Lin",
      "Pin Chen",
      "Rui Jiao",
      "Qing Mo",
      "Cen Jianhuan",
      "Wenbing Huang",
      "Yang Liu",
      "Dan Huang",
      "Yutong Lu"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24c.html": {
    "title": "Revisiting the Role of Language Priors in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiu Lin",
      "Xinyue Chen",
      "Deepak Pathak",
      "Pengchuan Zhang",
      "Deva Ramanan"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24d.html": {
    "title": "Non-confusing Generation of Customized Concepts in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Lin",
      "Jingyuan Chen",
      "Jiaxin Shi",
      "Yichen Zhu",
      "Chen Liang",
      "Junzhong Miao",
      "Tao Jin",
      "Zhou Zhao",
      "Fei Wu",
      "Shuicheng Yan",
      "Hanwang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24e.html": {
    "title": "Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wu Lin",
      "Felix Dangel",
      "Runa Eschenhagen",
      "Juhan Bae",
      "Richard E. Turner",
      "Alireza Makhzani"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24f.html": {
    "title": "Structured Inverse-Free Natural Gradient Descent: Memory-Efficient & Numerically-Stable KFAC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wu Lin",
      "Felix Dangel",
      "Runa Eschenhagen",
      "Kirill Neklyudov",
      "Agustinus Kristiadi",
      "Richard E. Turner",
      "Alireza Makhzani"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24g.html": {
    "title": "Learning to Model the World With Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jessy Lin",
      "Yuqing Du",
      "Olivia Watkins",
      "Danijar Hafner",
      "Pieter Abbeel",
      "Dan Klein",
      "Anca Dragan"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24h.html": {
    "title": "Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Lin",
      "Reinhard Heckel"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24i.html": {
    "title": "Equivariance via Minimal Frame Averaging for More Symmetries and Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchao Lin",
      "Jacob Helwig",
      "Shurui Gui",
      "Shuiwang Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24j.html": {
    "title": "Selecting Large Language Model to Fine-tune via Rectified Scaling Law",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Lin",
      "Baizhou Huang",
      "Haotian Ye",
      "Qinyu Chen",
      "Zihao Wang",
      "Sujian Li",
      "Jianzhu Ma",
      "Xiaojun Wan",
      "James Zou",
      "Yitao Liang"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24k.html": {
    "title": "Graph-enhanced Large Language Models in Asynchronous Plan Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangru Lin",
      "Emanuele La Malfa",
      "Valentin Hofmann",
      "Elle Michelle Yang",
      "Anthony G. Cohn",
      "Janet B. Pierrehumbert"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24l.html": {
    "title": "Dual Operating Modes of In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqian Lin",
      "Kangwook Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24m.html": {
    "title": "HGAP: Boosting Permutation Invariant and Permutation Equivariant in Multi-Agent Reinforcement Learning via Graph Attention Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bor-Jiun Lin",
      "Chun-Yi Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24n.html": {
    "title": "SparseTSF: Modeling Long-term Time Series Forecasting with *1k* Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengsheng Lin",
      "Weiwei Lin",
      "Wentai Wu",
      "Haojun Chen",
      "Junjie Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24o.html": {
    "title": "Fast and Sample Efficient Multi-Task Representation Learning in Stochastic Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiabin Lin",
      "Shana Moothedath",
      "Namrata Vaswani"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24p.html": {
    "title": "On Hypothesis Transfer Learning of Functional Linear Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Lin",
      "Matthew Reimherr"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24q.html": {
    "title": "Smoothness Adaptive Hypothesis Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Lin",
      "Matthew Reimherr"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24r.html": {
    "title": "Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqiang Lin",
      "Zhaoxuan Wu",
      "Zhongxiang Dai",
      "Wenyang Hu",
      "Yao Shu",
      "See-Kiong Ng",
      "Patrick Jaillet",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24s.html": {
    "title": "GeoAB: Towards Realistic Antibody Design and Reliable Affinity Maturation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitao Lin",
      "Lirong Wu",
      "Yufei Huang",
      "Yunfan Liu",
      "Odin Zhang",
      "Yuanqing Zhou",
      "Rui Sun",
      "Stan Z. Li"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24t.html": {
    "title": "Distributionally Robust Data Valuation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqiang Lin",
      "Xinyi Xu",
      "Zhaoxuan Wu",
      "See-Kiong Ng",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24u.html": {
    "title": "A Single-Loop Robust Policy Gradient Method for Robust Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenwei Lin",
      "Chenyu Xue",
      "Qi Deng",
      "Yinyu Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24v.html": {
    "title": "Layer-Aware Analysis of Catastrophic Overfitting: Revealing the Pseudo-Robust Shortcut Dependency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runqi Lin",
      "Chaojian Yu",
      "Bo Han",
      "Hang Su",
      "Tongliang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24w.html": {
    "title": "Autonomous Sparse Mean-CVaR Portfolio Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizun Lin",
      "Yangyu Zhang",
      "Zhao-Rong Lai",
      "Cheng Li"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24x.html": {
    "title": "Graph Neural Stochastic Diffusion for Estimating Uncertainty in Node Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixun Lin",
      "Wenxiao Zhang",
      "Fengzhao Shi",
      "Chuan Zhou",
      "Lixin Zou",
      "Xiangyu Zhao",
      "Dawei Yin",
      "Shirui Pan",
      "Yanan Cao"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24y.html": {
    "title": "Smooth Tchebycheff Scalarization for Multi-Objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Lin",
      "Xiaoyuan Zhang",
      "Zhiyuan Yang",
      "Fei Liu",
      "Zhenkun Wang",
      "Qingfu Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24z.html": {
    "title": "PPFLOW: Target-Aware Peptide Design with Torsional Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitao Lin",
      "Odin Zhang",
      "Huifeng Zhao",
      "Dejun Jiang",
      "Lirong Wu",
      "Zicheng Liu",
      "Yufei Huang",
      "Stan Z. Li"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24aa.html": {
    "title": "Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tzu-Yuan Lin",
      "Minghan Zhu",
      "Maani Ghaffari"
    ]
  },
  "https://proceedings.mlr.press/v235/lin24ab.html": {
    "title": "Plug-in Performative Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Licong Lin",
      "Tijana Zrnic"
    ]
  },
  "https://proceedings.mlr.press/v235/lindauer24a.html": {
    "title": "Position: A Call to Action for a Human-Centered AutoML Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marius Lindauer",
      "Florian Karl",
      "Anne Klier",
      "Julia Moosbauer",
      "Alexander Tornede",
      "Andreas C Mueller",
      "Frank Hutter",
      "Matthias Feurer",
      "Bernd Bischl"
    ]
  },
  "https://proceedings.mlr.press/v235/ling24a.html": {
    "title": "Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zenan Ling",
      "Longbo Li",
      "Zhanbo Feng",
      "Yixuan Zhang",
      "Feng Zhou",
      "Robert C Qiu",
      "Zhenyu Liao"
    ]
  },
  "https://proceedings.mlr.press/v235/lingsch24a.html": {
    "title": "Beyond Regular Grids: Fourier-Based Neural Operators on Arbitrary Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Levi E. Lingsch",
      "Mike Yan Michelis",
      "Emmanuel De Bezenac",
      "Sirani M. Perera",
      "Robert K. Katzschmann",
      "Siddhartha Mishra"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24a.html": {
    "title": "Scaling Tractable Probabilistic Circuits: A Systems Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anji Liu",
      "Kareem Ahmed",
      "Guy Van Den Broeck"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24b.html": {
    "title": "Causal Bandits: The Pareto Optimal Frontier of Adaptivity, a Reduction to Linear Bandits, and Limitations around Unknown Marginals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Liu",
      "Idan Attias",
      "Daniel M. Roy"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24c.html": {
    "title": "Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaizhao Liu",
      "Jose Blanchet",
      "Lexing Ying",
      "Yiping Lu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24d.html": {
    "title": "Graph Distillation with Eigenbasis Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Deyu Bo",
      "Chuan Shi"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24e.html": {
    "title": "Adaptive Text Watermark for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yepeng Liu",
      "Yuheng Bu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24f.html": {
    "title": "Enhancing Adversarial Robustness in SNNs with Sparse Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujia Liu",
      "Tong Bu",
      "Jianhao Ding",
      "Zecheng Hao",
      "Tiejun Huang",
      "Zhaofei Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24g.html": {
    "title": "ReLU Network with Width $d+\\mathcalO(1)$ Can Achieve Optimal Approximation Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghao Liu",
      "Minghua Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24h.html": {
    "title": "Graph Adversarial Diffusion Convolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songtao Liu",
      "Jinghui Chen",
      "Tianfan Fu",
      "Lu Lin",
      "Marinka Zitnik",
      "Dinghao Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24i.html": {
    "title": "Decentralized Convex Finite-Sum Optimization with Better Dependence on Condition Numbers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxing Liu",
      "Lesi Chen",
      "Luo Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24j.html": {
    "title": "Zeroth-Order Methods for Constrained Nonconvex Nonsmooth Stochastic Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuanghua Liu",
      "Cheng Chen",
      "Luo Luo",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24k.html": {
    "title": "Unifying Image Processing as Visual Prompting Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Liu",
      "Xiangyu Chen",
      "Xianzheng Ma",
      "Xintao Wang",
      "Jiantao Zhou",
      "Yu Qiao",
      "Chao Dong"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24l.html": {
    "title": "ESNet: Evolution and Succession Network for High-Resolution Salient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Liu",
      "Runmin Cong",
      "Hua Li",
      "Qianqian Xu",
      "Qingming Huang",
      "Wei Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24m.html": {
    "title": "The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziquan Liu",
      "Yufei Cui",
      "Yan Yan",
      "Yi Xu",
      "Xiangyang Ji",
      "Xue Liu",
      "Antoni B. Chan"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24n.html": {
    "title": "Preference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songtao Liu",
      "Hanjun Dai",
      "Yue Zhao",
      "Peng Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24o.html": {
    "title": "PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runze Liu",
      "Yali Du",
      "Fengshuo Bai",
      "Jiafei Lyu",
      "Xiu Li"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24p.html": {
    "title": "Harnessing the Power of Neural Operators with Automatically Encoded Conservation Laws",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Liu",
      "Yiming Fan",
      "Xianyi Zeng",
      "Milan Klöwer",
      "Lu Zhang",
      "Yue Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24q.html": {
    "title": "Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenlong Liu",
      "Lei Feng",
      "Huiping Zhuang",
      "Xiaofeng Cao",
      "Hongxin Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24r.html": {
    "title": "Decoding-time Realignment of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlin Liu",
      "Shangmin Guo",
      "Leonardo Bianco",
      "Daniele Calandriello",
      "Quentin Berthet",
      "Felipe Llinares-López",
      "Jessica Hoffmann",
      "Lucas Dixon",
      "Michal Valko",
      "Mathieu Blondel"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24s.html": {
    "title": "DIDI: Diffusion-Guided Diversity for Offline Behavioral Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxin Liu",
      "Xinghong Guo",
      "Zifeng Zhuang",
      "Donglin Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24t.html": {
    "title": "Bidirectional Reciprocative Information Communication for Few-Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanwei Liu",
      "Junwei Han",
      "Xiwen Yao",
      "Salman Khan",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Nian Liu",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24u.html": {
    "title": "Unlock the Cognitive Generalization of Deep Reinforcement Learning via Granular Ball Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashun Liu",
      "Jianye Hao",
      "Yi Ma",
      "Shuyin Xia"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24v.html": {
    "title": "PAPM: A Physics-aware Proxy Model for Process Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengwei Liu",
      "Zhongkai Hao",
      "Xingyu Ren",
      "Hangjie Yuan",
      "Jiayang Ren",
      "Dong Ni"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24w.html": {
    "title": "ELTA: An Enhancer against Long-Tail for Aesthetics-oriented Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Limin Liu",
      "Shuai He",
      "Anlong Ming",
      "Rui Xie",
      "Huadong Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24x.html": {
    "title": "On the Feasibility of Single-Pass Full-Capacity Learning in Linear Threshold Neurons with Binary Input Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruipeng Liu",
      "Borui He",
      "Naveed Tahir",
      "Garrett Ethan Katz"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24y.html": {
    "title": "Online Speculative Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxuan Liu",
      "Lanxiang Hu",
      "Peter Bailis",
      "Alvin Cheung",
      "Zhijie Deng",
      "Ion Stoica",
      "Hao Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24z.html": {
    "title": "Tuning-free Estimation and Inference of Cumulative Distribution Function under Local Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Liu",
      "Qirui Hu",
      "Linglong Kong"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24aa.html": {
    "title": "Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuantong Liu",
      "Tianyang Hu",
      "Wenjia Wang",
      "Kenji Kawaguchi",
      "Yuan Yao"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ab.html": {
    "title": "Reason for Future, Act for Now: A Principled Architecture for Autonomous LLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihan Liu",
      "Hao Hu",
      "Shenao Zhang",
      "Hongyi Guo",
      "Shuqi Ke",
      "Boyi Liu",
      "Zhaoran Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ac.html": {
    "title": "Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Liu",
      "Luping Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ad.html": {
    "title": "StyDeSty: Min-Max Stylization and Destylization for Single Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhua Liu",
      "Xin Jin",
      "Xingyi Yang",
      "Jingwen Ye",
      "Xinchao Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ae.html": {
    "title": "Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxin Liu",
      "Harshavardhan Kamarthi",
      "Lingkai Kong",
      "Zhiyuan Zhao",
      "Chao Zhang",
      "B. Aditya Prakash"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24af.html": {
    "title": "Stereo Risk: A Continuous Modeling Approach to Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ce Liu",
      "Suryansh Kumar",
      "Shuhang Gu",
      "Radu Timofte",
      "Yao Yao",
      "Luc Van Gool"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ag.html": {
    "title": "Multi-Source Conformal Inference Under Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Liu",
      "Alexander Levis",
      "Sharon-Lise Normand",
      "Larry Han"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ah.html": {
    "title": "From Generalization Analysis to Optimization Designs for State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fusheng Liu",
      "Qianxiao Li"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ai.html": {
    "title": "Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tenglong Liu",
      "Yang Li",
      "Yixing Lan",
      "Hao Gao",
      "Wei Pan",
      "Xin Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24aj.html": {
    "title": "Two-timescale Derivative Free Optimization for Performative Prediction with Markovian Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitong Liu",
      "Qiang Li",
      "Hoi To Wai"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ak.html": {
    "title": "Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Liu",
      "Siyuan Li",
      "Li Wang",
      "Zedong Wang",
      "Yunfan Liu",
      "Stan Z. Li"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24al.html": {
    "title": "Differentiable Combinatorial Scheduling at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingju Liu",
      "Yingjie Li",
      "Jiaqi Yin",
      "Zhiru Zhang",
      "Cunxi Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24am.html": {
    "title": "Characterizing ResNet's Universal Approximation Capability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghao Liu",
      "Enming Liang",
      "Minghua Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24an.html": {
    "title": "Convergence of Online Learning Algorithm for a Mixture of Multiple Linear Regressions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujing Liu",
      "Zhixin Liu",
      "Lei Guo"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ao.html": {
    "title": "Energy-Guided Diffusion Sampling for Offline-to-Online Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu-Hui Liu",
      "Tian-Shuo Liu",
      "Shengyi Jiang",
      "Ruifeng Chen",
      "Zhilong Zhang",
      "Xinwei Chen",
      "Yang Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ap.html": {
    "title": "Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-Loop and Hessian-Free Solution Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Risheng Liu",
      "Zhu Liu",
      "Wei Yao",
      "Shangzhi Zeng",
      "Jin Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24aq.html": {
    "title": "Position: Foundation Agents as the Paradigm Shift for Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqian Liu",
      "Xingzhou Lou",
      "Jianbin Jiao",
      "Junge Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ar.html": {
    "title": "Learning with Partial-Label and Unlabeled Data: A Uniform Treatment for Supervision Redundancy and Insufficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangfan Liu",
      "Jiaqi Lv",
      "Xin Geng",
      "Ning Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24as.html": {
    "title": "Revisiting Context Aggregation for Image Matting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinglin Liu",
      "Xiaoqian Lv",
      "Quanling Meng",
      "Zonglin Li",
      "Xiangyuan Lan",
      "Shuo Yang",
      "Shengping Zhang",
      "Liqiang Nie"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24at.html": {
    "title": "Amortized Equation Discovery in Hybrid Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongtuo Liu",
      "Sara Magliacane",
      "Miltiadis Kofinas",
      "Stratis Gavves"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24au.html": {
    "title": "KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junnan Liu",
      "Qianren Mao",
      "Weifeng Jiang",
      "Jianxin Li"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24av.html": {
    "title": "Floating Anchor Diffusion Model for Multi-motif Scaffolding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Liu",
      "Weian Mao",
      "Shuaike Shen",
      "Xiaoran Jiao",
      "Zheng Sun",
      "Hao Chen",
      "Chunhua Shen"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24aw.html": {
    "title": "Deep Functional Factor Models: Forecasting High-Dimensional Functional Time Series via Bayesian Nonparametric Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yirui Liu",
      "Xinghao Qiao",
      "Yulong Pei",
      "Liying Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ax.html": {
    "title": "Fast Decision Boundary based Out-of-Distribution Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Litian Liu",
      "Yao Qin"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ay.html": {
    "title": "Class-Imbalanced Graph Learning without Class Rebalancing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhining Liu",
      "Ruizhong Qiu",
      "Zhichen Zeng",
      "Hyunsik Yoo",
      "David Zhou",
      "Zhe Xu",
      "Yada Zhu",
      "Kommy Weldemariam",
      "Jingrui He",
      "Hanghang Tong"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24az.html": {
    "title": "Generative Marginalization Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sulin Liu",
      "Peter Ramadge",
      "Ryan P Adams"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ba.html": {
    "title": "Federated Representation Learning in the Under-Parameterized Regime",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renpu Liu",
      "Cong Shen",
      "Jing Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bb.html": {
    "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Liu",
      "Theodore Sumers",
      "Ishita Dasgupta",
      "Thomas L. Griffiths"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bc.html": {
    "title": "Causal Discovery via Conditional Independence Testing with Proxy Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhou Liu",
      "Xinwei Sun",
      "Yu Qiao",
      "Yizhou Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bd.html": {
    "title": "Perfect Alignment May be Poisonous to Graph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyu Liu",
      "Huayi Tang",
      "Yong Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24be.html": {
    "title": "Entropy-Reinforced Planning with Large Language Models for Drug Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuefeng Liu",
      "Chih-Chan Tien",
      "Peng Ding",
      "Songhao Jiang",
      "Rick L. Stevens"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bf.html": {
    "title": "New Sample Complexity Bounds for Sample Average Approximation in Heavy-Tailed Stochastic Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongcheng Liu",
      "Jindong Tong"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bg.html": {
    "title": "Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Che Liu",
      "Zhongwei Wan",
      "Cheng Ouyang",
      "Anand Shah",
      "Wenjia Bai",
      "Rossella Arcucci"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bh.html": {
    "title": "Unified Generation, Reconstruction, and Representation: Generalized Diffusion with Adaptive Latent Encoding-Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyi Liu",
      "Yu Wang",
      "Zeyu Feng",
      "Qiyu Wu",
      "Liping Tang",
      "Yuan Gao",
      "Zhen Li",
      "Shuguang Cui",
      "Julian Mcauley",
      "Zichao Yang",
      "Eric P. Xing",
      "Zhiting Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bi.html": {
    "title": "Differentiable Model Scaling using Differentiable Topk",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Liu",
      "Ruohui Wang",
      "Jianfei Gao",
      "Kai Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bj.html": {
    "title": "Symmetric Matrix Completion with ReLU Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huikang Liu",
      "Peng Wang",
      "Longxiu Huang",
      "Qing Qu",
      "Laura Balzano"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bk.html": {
    "title": "DNA-SE: Towards Deep Neural-Nets Assisted Semiparametric Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinshuo Liu",
      "Zixin Wang",
      "Xi’An Li",
      "Xinyao Ji",
      "Lei Zhang",
      "Lin Liu",
      "Zhonghua Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bl.html": {
    "title": "TimeX++: Learning Time-Series Explanations with Information Bottleneck",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichuan Liu",
      "Tianchun Wang",
      "Jimeng Shi",
      "Xu Zheng",
      "Zhuomin Chen",
      "Lei Song",
      "Wenqian Dong",
      "Jayantha Obeysekera",
      "Farhad Shirani",
      "Dongsheng Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bm.html": {
    "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianci Liu",
      "Haoyu Wang",
      "Shiyang Wang",
      "Yu Cheng",
      "Jing Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bn.html": {
    "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shih-Yang Liu",
      "Chien-Yi Wang",
      "Hongxu Yin",
      "Pavlo Molchanov",
      "Yu-Chiang Frank Wang",
      "Kwang-Ting Cheng",
      "Min-Hung Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bo.html": {
    "title": "High-Probability Bound for Non-Smooth Non-Convex Stochastic Optimization with Heavy Tails",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Langqi Liu",
      "Yibo Wang",
      "Lijun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bp.html": {
    "title": "Combinatorial Multivariant Multi-Armed Bandits with Applications to Episodic Reinforcement Learning and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xutong Liu",
      "Siwei Wang",
      "Jinhang Zuo",
      "Han Zhong",
      "Xuchuang Wang",
      "Zhiyong Wang",
      "Shuai Li",
      "Mohammad Hajiesmaili",
      "John C.S. Lui",
      "Wei Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bq.html": {
    "title": "Language-Driven Cross-Modal Classifier for Zero-Shot Multi-Label Image Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Liu",
      "Jie Wen",
      "Chengliang Liu",
      "Xiaozhao Fang",
      "Zuoyong Li",
      "Yong Xu",
      "Zheng Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24br.html": {
    "title": "Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo Liu",
      "Jiayun Wu",
      "Tianyu Wang",
      "Hao Zou",
      "Bo Li",
      "Peng Cui"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bs.html": {
    "title": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Liu",
      "Tong Xialiang",
      "Mingxuan Yuan",
      "Xi Lin",
      "Fu Luo",
      "Zhenkun Wang",
      "Zhichao Lu",
      "Qingfu Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bt.html": {
    "title": "Correlation-Induced Label Prior for Semi-Supervised Multi-Label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biao Liu",
      "Ning Xu",
      "Xiangyu Fang",
      "Xin Geng"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bu.html": {
    "title": "Causality Based Front-door Defense Against Backdoor Attack on Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Liu",
      "Xiaoang Xu",
      "Zhiyi Hou",
      "Yang Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bv.html": {
    "title": "Partial Multi-View Multi-Label Classification via Semantic Invariance Learning and Prototype Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengliang Liu",
      "Gehui Xu",
      "Jie Wen",
      "Yabo Liu",
      "Chao Huang",
      "Yong Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bw.html": {
    "title": "Building Socially-Equitable Public Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejia Liu",
      "Jianyi Yang",
      "Pengfei Li",
      "Tongxin Li",
      "Shaolei Ren"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bx.html": {
    "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Liu",
      "Haotian Ye",
      "Lei Xing",
      "James Y. Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24by.html": {
    "title": "Minimizing $f$-Divergences by Interpolating Velocity Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Liu",
      "Jiahao Yu",
      "Jack Simons",
      "Mingxuan Yi",
      "Mark Beaumont"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24bz.html": {
    "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Liu",
      "Jiayi Yuan",
      "Hongye Jin",
      "Shaochen Zhong",
      "Zhaozhuo Xu",
      "Vladimir Braverman",
      "Beidi Chen",
      "Xia Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ca.html": {
    "title": "Efficient Policy Evaluation with Offline Data Informed Behavior Policy Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuze Liu",
      "Shangtong Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24cb.html": {
    "title": "Timer: Generative Pre-trained Transformers Are Large Time Series Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Liu",
      "Haoran Zhang",
      "Chenyu Li",
      "Xiangdong Huang",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24cc.html": {
    "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyang Liu",
      "Renrui Zhang",
      "Longtian Qiu",
      "Siyuan Huang",
      "Weifeng Lin",
      "Shitian Zhao",
      "Shijie Geng",
      "Ziyi Lin",
      "Peng Jin",
      "Kaipeng Zhang",
      "Wenqi Shao",
      "Chao Xu",
      "Conghui He",
      "Junjun He",
      "Hao Shao",
      "Pan Lu",
      "Yu Qiao",
      "Hongsheng Li",
      "Peng Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24cd.html": {
    "title": "DFD: Distilling the Feature Disparity Differently for Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Liu",
      "Yingyi Zhang",
      "Jingyun Zhang",
      "Jinmin Li",
      "Jun Wang",
      "Shaoming Wang",
      "Chun Yuan",
      "Rizen Guo"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ce.html": {
    "title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zechun Liu",
      "Changsheng Zhao",
      "Forrest Iandola",
      "Chen Lai",
      "Yuandong Tian",
      "Igor Fedorov",
      "Yunyang Xiong",
      "Ernie Chang",
      "Yangyang Shi",
      "Raghuraman Krishnamoorthi",
      "Liangzhen Lai",
      "Vikas Chandra"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24cf.html": {
    "title": "Reducing Item Discrepancy via Differentially Private Robust Embedding Alignment for Privacy-Preserving Cross Domain Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Liu",
      "Xiaolin Zheng",
      "Chaochao Chen",
      "Jiahe Xu",
      "Xinting Liao",
      "Fan Wang",
      "Yanchao Tan",
      "Yew-Soon Ong"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24cg.html": {
    "title": "On the Last-Iterate Convergence of Shuffling Gradient Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Liu",
      "Zhengyuan Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ch.html": {
    "title": "FedLMT: Tackling System Heterogeneity of Federated Learning via Low-Rank Model Training with Theoretical Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Liu",
      "Yipeng Zhou",
      "Di Wu",
      "Miao Hu",
      "Mohsen Guizani",
      "Quan Z. Sheng"
    ]
  },
  "https://proceedings.mlr.press/v235/liu24ci.html": {
    "title": "Pairwise Alignment Improves Graph Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikun Liu",
      "Deyu Zou",
      "Han Zhao",
      "Pan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/liu-schiaffini24a.html": {
    "title": "Neural Operators with Localized Integral and Differential Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miguel Liu-Schiaffini",
      "Julius Berner",
      "Boris Bonev",
      "Thorsten Kurth",
      "Kamyar Azizzadenesheli",
      "Anima Anandkumar"
    ]
  },
  "https://proceedings.mlr.press/v235/lizaire24a.html": {
    "title": "A Tensor Decomposition Perspective on Second-order RNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maude Lizaire",
      "Michael Rizvi-Martel",
      "Marawan Gamal",
      "Guillaume Rabusseau"
    ]
  },
  "https://proceedings.mlr.press/v235/loeschcke24a.html": {
    "title": "Coarse-To-Fine Tensor Trains for Compact Visual Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Bugge Loeschcke",
      "Dan Wang",
      "Christian Munklinde Leth-Espensen",
      "Serge Belongie",
      "Michael Kastoryano",
      "Sagie Benaim"
    ]
  },
  "https://proceedings.mlr.press/v235/loffredo24a.html": {
    "title": "Restoring balance: principled under/oversampling of data for optimal classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Loffredo",
      "Mauro Pastore",
      "Simona Cocco",
      "Remi Monasson"
    ]
  },
  "https://proceedings.mlr.press/v235/loftus24a.html": {
    "title": "Position: The Causal Revolution Needs Scientific Pragmatism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua R. Loftus"
    ]
  },
  "https://proceedings.mlr.press/v235/long24a.html": {
    "title": "Reparameterized Importance Sampling for Robust Variational Bayesian Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfei Long",
      "Zilin Tian",
      "Liguo Zhang",
      "Huosheng Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/longpre24a.html": {
    "title": "Position: A Safe Harbor for AI Evaluation and Red Teaming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayne Longpre",
      "Sayash Kapoor",
      "Kevin Klyman",
      "Ashwin Ramaswami",
      "Rishi Bommasani",
      "Borhane Blili-Hamelin",
      "Yangsibo Huang",
      "Aviya Skowron",
      "Zheng Xin Yong",
      "Suhas Kotha",
      "Yi Zeng",
      "Weiyan Shi",
      "Xianjun Yang",
      "Reid Southen",
      "Alexander Robey",
      "Patrick Chao",
      "Diyi Yang",
      "Ruoxi Jia",
      "Daniel Kang",
      "Alex Pentland",
      "Arvind Narayanan",
      "Percy Liang",
      "Peter Henderson"
    ]
  },
  "https://proceedings.mlr.press/v235/longpre24b.html": {
    "title": "Position: Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayne Longpre",
      "Robert Mahari",
      "Naana Obeng-Marnu",
      "William Brannon",
      "Tobin South",
      "Katy Ilonka Gero",
      "Alex Pentland",
      "Jad Kabbara"
    ]
  },
  "https://proceedings.mlr.press/v235/lonnqvist24a.html": {
    "title": "Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Lonnqvist",
      "Zhengqing Wu",
      "Michael Herzog"
    ]
  },
  "https://proceedings.mlr.press/v235/loo24a.html": {
    "title": "Large Scale Dataset Distillation with Domain Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noel Loo",
      "Alaa Maalouf",
      "Ramin Hasani",
      "Mathias Lechner",
      "Alexander Amini",
      "Daniela Rus"
    ]
  },
  "https://proceedings.mlr.press/v235/lopardo24a.html": {
    "title": "Attention Meets Post-hoc Interpretability: A Mathematical Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianluigi Lopardo",
      "Frederic Precioso",
      "Damien Garreau"
    ]
  },
  "https://proceedings.mlr.press/v235/lotfi24a.html": {
    "title": "Non-Vacuous Generalization Bounds for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanae Lotfi",
      "Marc Anton Finzi",
      "Yilun Kuang",
      "Tim G. J. Rudner",
      "Micah Goldblum",
      "Andrew Gordon Wilson"
    ]
  },
  "https://proceedings.mlr.press/v235/lou24a.html": {
    "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron Lou",
      "Chenlin Meng",
      "Stefano Ermon"
    ]
  },
  "https://proceedings.mlr.press/v235/lowy24a.html": {
    "title": "Optimal Differentially Private Model Training with Public Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Lowy",
      "Zeman Li",
      "Tianjian Huang",
      "Meisam Razaviyayn"
    ]
  },
  "https://proceedings.mlr.press/v235/lowy24b.html": {
    "title": "How to Make the Gradients Small Privately: Improved Rates for Differentially Private Non-Convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Lowy",
      "Jonathan Ullman",
      "Stephen Wright"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24a.html": {
    "title": "Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihe Lu",
      "Jiawang Bai",
      "Xin Li",
      "Zeyu Xiao",
      "Xinchao Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24b.html": {
    "title": "HumanTOMATO: Text-aligned Whole-body Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunlin Lu",
      "Ling-Hao Chen",
      "Ailing Zeng",
      "Jing Lin",
      "Ruimao Zhang",
      "Lei Zhang",
      "Heung-Yeung Shum"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24c.html": {
    "title": "Position: Exploring the Robustness of Pipeline-Parallelism-Based Decentralized Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Lu",
      "Chenxi Dai",
      "Wangcheng Tao",
      "Binhang Yuan",
      "Yanan Sun",
      "Pan Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24d.html": {
    "title": "CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiecheng Lu",
      "Xu Han",
      "Yan Sun",
      "Shihao Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24e.html": {
    "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Han Lu",
      "Zdeněk Kasner",
      "Siva Reddy"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24f.html": {
    "title": "Open-Domain Text Evaluation via Contrastive Distribution Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sidi Lu",
      "Hongyi Liu",
      "Asli Celikyilmaz",
      "Tianlu Wang",
      "Nanyun Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24g.html": {
    "title": "EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in Linear Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyao Lu",
      "Bang Liu",
      "Keith G Mills",
      "Jiao He",
      "Di Niu"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24h.html": {
    "title": "Rethinking Transformers in Solving POMDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Lu",
      "Ruizhe Shi",
      "Yuyao Liu",
      "Kaizhe Hu",
      "Simon Shaolei Du",
      "Huazhe Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24i.html": {
    "title": "CauDiTS: Causal Disentangled Domain Adaptation of Multivariate Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxin Lu",
      "Shiliang Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24j.html": {
    "title": "NeWRF: A Deep Learning Framework for Wireless Radiation Field Reconstruction and Channel Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haofan Lu",
      "Christopher Vattheuer",
      "Baharan Mirzasoleiman",
      "Omid Abari"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24k.html": {
    "title": "FiT: Flexible Vision Transformer for Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Lu",
      "Zidong Wang",
      "Di Huang",
      "Chengyue Wu",
      "Xihui Liu",
      "Wanli Ouyang",
      "Lei Bai"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24l.html": {
    "title": "Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kejing Lu",
      "Chuan Xiao",
      "Yoshiharu Ishikawa"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24m.html": {
    "title": "Disguised Copyright Infringement of Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Lu",
      "Matthew Y. R. Yang",
      "Zuoqiu Liu",
      "Gautam Kamath",
      "Yaoliang Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24n.html": {
    "title": "OxyGenerator: Reconstructing Global Ocean Deoxygenation Over a Century with Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Lu",
      "Ze Zhao",
      "Luyu Han",
      "Xiaoying Gan",
      "Yuntao Zhou",
      "Lei Zhou",
      "Luoyi Fu",
      "Xinbing Wang",
      "Chenghu Zhou",
      "Jing Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24o.html": {
    "title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sidi Lu",
      "Wenbo Zhao",
      "Chenyang Tao",
      "Arpit Gupta",
      "Shanchan Wu",
      "Tagyoung Chung",
      "Nanyun Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/lu24p.html": {
    "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Lu",
      "Aojun Zhou",
      "Yuhui Xu",
      "Renrui Zhang",
      "Peng Gao",
      "Hongsheng Li"
    ]
  },
  "https://proceedings.mlr.press/v235/ludziejewski24a.html": {
    "title": "Scaling Laws for Fine-Grained Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Ludziejewski",
      "Jakub Krajewski",
      "Kamil Adamczewski",
      "Maciej Pióro",
      "Michał Krutul",
      "Szymon Antoniak",
      "Kamil Ciebiera",
      "Krystian Król",
      "Tomasz Odrzygóźdź",
      "Piotr Sankowski",
      "Marek Cygan",
      "Sebastian Jaszczur"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24a.html": {
    "title": "Projecting Molecules into Synthesizable Chemical Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitong Luo",
      "Wenhao Gao",
      "Zuofan Wu",
      "Jian Peng",
      "Connor W. Coley",
      "Jianzhu Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24b.html": {
    "title": "PGODE: Towards High-quality System Dynamics Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Luo",
      "Yiyang Gu",
      "Huiyu Jiang",
      "Hang Zhou",
      "Jinsheng Huang",
      "Wei Ju",
      "Zhiping Xiao",
      "Ming Zhang",
      "Yizhou Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24c.html": {
    "title": "Unveiling the Cycloid Trajectory of EM Iterations in Mixed Linear Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhankun Luo",
      "Abolfazl Hashemi"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24d.html": {
    "title": "OMPO: A Unified Framework for RL under Policy and Dynamics Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Luo",
      "Tianying Ji",
      "Fuchun Sun",
      "Jianwei Zhang",
      "Huazhe Xu",
      "Xianyuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24e.html": {
    "title": "Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Luo",
      "Tianying Ji",
      "Fuchun Sun",
      "Jianwei Zhang",
      "Huazhe Xu",
      "Xianyuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24f.html": {
    "title": "Position: Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyao Luo",
      "Yangchen Pan",
      "Peter Watkinson",
      "Tingting Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24g.html": {
    "title": "Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xihaier Luo",
      "Xiaoning Qian",
      "Byung-Jun Yoon"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24h.html": {
    "title": "Potential Based Diffusion Motion Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Luo",
      "Chen Sun",
      "Joshua B. Tenenbaum",
      "Yilun Du"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24i.html": {
    "title": "Cluster-Aware Similarity Diffusion for Instance Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jifei Luo",
      "Hantao Yao",
      "Changsheng Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/luo24j.html": {
    "title": "End-to-End Neuro-Symbolic Reinforcement Learning with Textual Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lirui Luo",
      "Guoxi Zhang",
      "Hongming Xu",
      "Yaodong Yang",
      "Cong Fang",
      "Qing Li"
    ]
  },
  "https://proceedings.mlr.press/v235/lv24a.html": {
    "title": "RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Lv",
      "Hao Li",
      "Xiang Deng",
      "Rui Shao",
      "Michael Y Wang",
      "Liqiang Nie"
    ]
  },
  "https://proceedings.mlr.press/v235/lv24b.html": {
    "title": "Contamination-Resilient Anomaly Detection via Adversarial Learning on Partially-Observed Normal and Anomalous Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxi Lv",
      "Qinliang Su",
      "Hai Wan",
      "Hongteng Xu",
      "Wenchao Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/lv24c.html": {
    "title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitan Lv",
      "Jie Wang",
      "Hanzhu Chen",
      "Bin Li",
      "Yongdong Zhang",
      "Feng Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/lv24d.html": {
    "title": "Efficient and Effective Time-Series Forecasting with Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changze Lv",
      "Yansen Wang",
      "Dongqi Han",
      "Xiaoqing Zheng",
      "Xuanjing Huang",
      "Dongsheng Li"
    ]
  },
  "https://proceedings.mlr.press/v235/lyu24a.html": {
    "title": "Cross-Domain Policy Adaptation by Capturing Representation Mismatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiafei Lyu",
      "Chenjia Bai",
      "Jing-Wen Yang",
      "Zongqing Lu",
      "Xiu Li"
    ]
  },
  "https://proceedings.mlr.press/v235/lyu24b.html": {
    "title": "Sampling is as easy as keeping the consistency: convergence guarantee for Consistency Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlong Lyu",
      "Zhitang Chen",
      "Shoubo Feng"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24a.html": {
    "title": "Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Ma",
      "Xu Chu",
      "Zhibang Yang",
      "Yang Lin",
      "Xin Gao",
      "Junfeng Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24b.html": {
    "title": "Rethinking Decision Transformer via Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Ma",
      "Jianye Hao",
      "Hebin Liang",
      "Chenjun Xiao"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24c.html": {
    "title": "Better Locally Private Sparse Estimation Given Multiple Samples Per User",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Ma",
      "Ke Jia",
      "Hanfang Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24d.html": {
    "title": "Learning Modality Knowledge Alignment for Cross-Modality Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Ma",
      "Shuang Li",
      "Lincan Cai",
      "Jingxuan Kang"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24e.html": {
    "title": "Beyond the Federation: Topology-aware Federated Learning for Generalization to Unseen Clients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengmeng Ma",
      "Tang Li",
      "Xi Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24f.html": {
    "title": "Outlier-aware Slicing for Post-Training Quantization in Vision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuexiao Ma",
      "Huixia Li",
      "Xiawu Zheng",
      "Feng Ling",
      "Xuefeng Xiao",
      "Rui Wang",
      "Shilei Wen",
      "Fei Chao",
      "Rongrong Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24g.html": {
    "title": "X-Oscar: A Progressive Framework for High-quality Text-guided 3D Animatable Avatar Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Ma",
      "Zhekai Lin",
      "Jiayi Ji",
      "Yijun Fan",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24h.html": {
    "title": "Neighboring Perturbations of Knowledge Editing on Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Yu Ma",
      "Zhen-Hua Ling",
      "Ningyu Zhang",
      "Jia-Chen Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24i.html": {
    "title": "Do Transformer World Models Give Better Policy Gradients?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michel Ma",
      "Tianwei Ni",
      "Clement Gehring",
      "Pierluca D’Oro",
      "Pierre-Luc Bacon"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24j.html": {
    "title": "Differentiable Distributionally Robust Optimization Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xutao Ma",
      "Chao Ning",
      "Wenli Du"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24k.html": {
    "title": "CKGConv: General Graph Convolution with Continuous Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liheng Ma",
      "Soumyasundar Pal",
      "Yitian Zhang",
      "Jiaming Zhou",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24l.html": {
    "title": "Reward Shaping for Reinforcement Learning with An Assistant Reward Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Ma",
      "Kuankuan Sima",
      "Thanh Vinh Vo",
      "Di Fu",
      "Tze-Yun Leong"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24m.html": {
    "title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingchuan Ma",
      "Tsun-Hsuan Wang",
      "Minghao Guo",
      "Zhiqing Sun",
      "Joshua B. Tenenbaum",
      "Daniela Rus",
      "Chuang Gan",
      "Wojciech Matusik"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24n.html": {
    "title": "Fast Peer Adaptation with Context-aware Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Ma",
      "Yuanfei Wang",
      "Fangwei Zhong",
      "Song-Chun Zhu",
      "Yizhou Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24o.html": {
    "title": "HarmonyDream: Task Harmonization Inside World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Ma",
      "Jialong Wu",
      "Ningya Feng",
      "Chenjun Xiao",
      "Dong Li",
      "Jianye Hao",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24p.html": {
    "title": "High-dimensional Linear Bandits with Knapsacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanteng Ma",
      "Dong Xia",
      "Jiashuo Jiang"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24q.html": {
    "title": "SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziping Ma",
      "Furong Xu",
      "Jian Liu",
      "Ming Yang",
      "Qingpei Guo"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24r.html": {
    "title": "The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Ma",
      "Shuchen Xue",
      "Tianyang Hu",
      "Wenjia Wang",
      "Zhaoqiang Liu",
      "Zhenguo Li",
      "Zhi-Ming Ma",
      "Kenji Kawaguchi"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24s.html": {
    "title": "Correcting Diffusion-Based Perceptual Image Compression with Privileged End-to-End Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Ma",
      "Wenhan Yang",
      "Jiaying Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/ma24t.html": {
    "title": "A Provable Decision Rule for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinsong Ma",
      "Xin Zou",
      "Weiwei Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/machiraju24a.html": {
    "title": "Prospector Heads: Generalized Feature Attribution for Large Models & Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gautam Machiraju",
      "Alexander Derry",
      "Arjun D Desai",
      "Neel Guha",
      "Amir-Hossein Karimi",
      "James Zou",
      "Russ B Altman",
      "Christopher Re",
      "Parag Mallick"
    ]
  },
  "https://proceedings.mlr.press/v235/madhu24a.html": {
    "title": "Unsupervised Parameter-free Simplicial Representation Learning with Scattering Transforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiren Madhu",
      "Sravanthi Gurugubelli",
      "Sundeep Prabhakar Chepuri"
    ]
  },
  "https://proceedings.mlr.press/v235/madsen24a.html": {
    "title": "Faithfulness Measurable Masked Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Madsen",
      "Siva Reddy",
      "Sarath Chandar"
    ]
  },
  "https://proceedings.mlr.press/v235/maene24a.html": {
    "title": "On the Hardness of Probabilistic Neurosymbolic Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaron Maene",
      "Vincent Derkinderen",
      "Luc De Raedt"
    ]
  },
  "https://proceedings.mlr.press/v235/mahankali24a.html": {
    "title": "Random Latent Exploration for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srinath V. Mahankali",
      "Zhang-Wei Hong",
      "Ayush Sekhari",
      "Alexander Rakhlin",
      "Pulkit Agrawal"
    ]
  },
  "https://proceedings.mlr.press/v235/mahlau24a.html": {
    "title": "Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannik Mahlau",
      "Frederik Schubert",
      "Bodo Rosenhahn"
    ]
  },
  "https://proceedings.mlr.press/v235/mai24a.html": {
    "title": "Split-and-Denoise: Protect large language model inference with local differential privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peihua Mai",
      "Ran Yan",
      "Zhe Huang",
      "Youjia Yang",
      "Yan Pang"
    ]
  },
  "https://proceedings.mlr.press/v235/maia-polo24a.html": {
    "title": "tinyBenchmarks: evaluating LLMs with fewer examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Maia Polo",
      "Lucas Weber",
      "Leshem Choshen",
      "Yuekai Sun",
      "Gongjun Xu",
      "Mikhail Yurochkin"
    ]
  },
  "https://proceedings.mlr.press/v235/majee24a.html": {
    "title": "SCoRe: Submodular Combinatorial Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anay Majee",
      "Suraj Nandkishor Kothawade",
      "Krishnateja Killamsetty",
      "Rishabh K Iyer"
    ]
  },
  "https://proceedings.mlr.press/v235/majumder24a.html": {
    "title": "Position: Data-driven Discovery with Large Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bodhisattwa Prasad Majumder",
      "Harshit Surana",
      "Dhruv Agarwal",
      "Sanchaita Hazra",
      "Ashish Sabharwal",
      "Peter Clark"
    ]
  },
  "https://proceedings.mlr.press/v235/makkuva24a.html": {
    "title": "LASER: Linear Compression in Wireless Distributed Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashok Vardhan Makkuva",
      "Marco Bondaschi",
      "Thijs Vogels",
      "Martin Jaggi",
      "Hyeji Kim",
      "Michael Gastpar"
    ]
  },
  "https://proceedings.mlr.press/v235/malach24a.html": {
    "title": "Auto-Regressive Next-Token Predictors are Universal Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eran Malach"
    ]
  },
  "https://proceedings.mlr.press/v235/malagon24a.html": {
    "title": "Self-Composing Policies for Scalable Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikel Malagon",
      "Josu Ceberio",
      "Jose A. Lozano"
    ]
  },
  "https://proceedings.mlr.press/v235/malekmohammadi24a.html": {
    "title": "Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saber Malekmohammadi",
      "Yaoliang Yu",
      "Yang Cao"
    ]
  },
  "https://proceedings.mlr.press/v235/malherbe24a.html": {
    "title": "Measures of diversity and space-filling designs for categorical data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cedric Malherbe",
      "Emilio Domı́nguez-Sánchez",
      "Merwan Barlier",
      "Igor Colin",
      "Haitham Bou Ammar",
      "Tom Diethe"
    ]
  },
  "https://proceedings.mlr.press/v235/malla24a.html": {
    "title": "COPAL: Continual Pruning in Large Language Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srikanth Malla",
      "Joon Hee Choi",
      "Chiho Choi"
    ]
  },
  "https://proceedings.mlr.press/v235/mallinar24a.html": {
    "title": "Minimum-Norm Interpolation Under Covariate Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neil Rohit Mallinar",
      "Austin Zane",
      "Spencer Frei",
      "Bin Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/mannelli24a.html": {
    "title": "Tilting the Odds at the Lottery: the Interplay of Overparameterisation and Curricula in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Sarao Mannelli",
      "Yaraslau Ivashynka",
      "Andrew M Saxe",
      "Luca Saglietti"
    ]
  },
  "https://proceedings.mlr.press/v235/manor24a.html": {
    "title": "Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hila Manor",
      "Tomer Michaeli"
    ]
  },
  "https://proceedings.mlr.press/v235/manupriya24a.html": {
    "title": "Submodular framework for structured-sparse optimal transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piyushi Manupriya",
      "Pratik Jawanpuria",
      "Karthik S. Gurumoorthy",
      "Sakethanath Jagarlapudi",
      "Bamdev Mishra"
    ]
  },
  "https://proceedings.mlr.press/v235/manvi24a.html": {
    "title": "Large Language Models are Geographically Biased",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohin Manvi",
      "Samar Khanna",
      "Marshall Burke",
      "David B. Lobell",
      "Stefano Ermon"
    ]
  },
  "https://proceedings.mlr.press/v235/mao24a.html": {
    "title": "Position: Graph Foundation Models Are Already Here",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitao Mao",
      "Zhikai Chen",
      "Wenzhuo Tang",
      "Jianan Zhao",
      "Yao Ma",
      "Tong Zhao",
      "Neil Shah",
      "Mikhail Galkin",
      "Jiliang Tang"
    ]
  },
  "https://proceedings.mlr.press/v235/mao24b.html": {
    "title": "Towards General Neural Surrogate Solvers with Specialized Neural Accelerators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenkai Mao",
      "Robert Lupoiu",
      "Tianxiang Dai",
      "Mingkun Chen",
      "Jonathan Fan"
    ]
  },
  "https://proceedings.mlr.press/v235/mao24c.html": {
    "title": "$H$-Consistency Guarantees for Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Mao",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://proceedings.mlr.press/v235/mao24d.html": {
    "title": "Regression with Multi-Expert Deferral",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Mao",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://proceedings.mlr.press/v235/maran24a.html": {
    "title": "No-Regret Reinforcement Learning in Smooth MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Maran",
      "Alberto Maria Metelli",
      "Matteo Papini",
      "Marcello Restelli"
    ]
  },
  "https://proceedings.mlr.press/v235/marcotte24a.html": {
    "title": "Keep the Momentum: Conservation Laws beyond Euclidean Gradient Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sibylle Marcotte",
      "Rémi Gribonval",
      "Gabriel Peyré"
    ]
  },
  "https://proceedings.mlr.press/v235/mariella24a.html": {
    "title": "Quantum Theory and Application of Contextual Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Mariella",
      "Albert Akhriev",
      "Francesco Tacchino",
      "Christa Zoufal",
      "Juan Carlos Gonzalez-Espitia",
      "Benedek Harsanyi",
      "Eugene Koskin",
      "Ivano Tavernelli",
      "Stefan Woerner",
      "Marianna Rapsomaniki",
      "Sergiy Zhuk",
      "Jannis Born"
    ]
  },
  "https://proceedings.mlr.press/v235/marisca24a.html": {
    "title": "Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Marisca",
      "Cesare Alippi",
      "Filippo Maria Bianchi"
    ]
  },
  "https://proceedings.mlr.press/v235/marnissi24a.html": {
    "title": "A Unified View of FANOVA: A Comprehensive Bayesian Framework for Component Selection and Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yosra Marnissi",
      "Maxime Leiber"
    ]
  },
  "https://proceedings.mlr.press/v235/martinelli24a.html": {
    "title": "Expand-and-Cluster: Parameter Recovery of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Flavio Martinelli",
      "Berfin Simsek",
      "Wulfram Gerstner",
      "Johanni Brea"
    ]
  },
  "https://proceedings.mlr.press/v235/marti-nez-rubio24a.html": {
    "title": "Convergence and Trade-Offs in Riemannian Gradient Descent and Riemannian Proximal Point",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Martı́nez-Rubio",
      "Christophe Roux",
      "Sebastian Pokutta"
    ]
  },
  "https://proceedings.mlr.press/v235/marusich24a.html": {
    "title": "Using AI Uncertainty Quantification to Improve Human Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura Marusich",
      "Jonathan Bakdash",
      "Yan Zhou",
      "Murat Kantarcioglu"
    ]
  },
  "https://proceedings.mlr.press/v235/marzouk24a.html": {
    "title": "On the Tractability of SHAP Explanations under Markovian Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reda Marzouk",
      "Colin De La Higuera"
    ]
  },
  "https://proceedings.mlr.press/v235/masserano24a.html": {
    "title": "Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Masserano",
      "Alexander Shen",
      "Michele Doro",
      "Tommaso Dorigo",
      "Rafael Izbicki",
      "Ann B. Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/massiani24a.html": {
    "title": "On the Consistency of Kernel Methods with Dependent Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre-François Massiani",
      "Sebastian Trimpe",
      "Friedrich Solowjow"
    ]
  },
  "https://proceedings.mlr.press/v235/mastrototaro24a.html": {
    "title": "Online Variational Sequential Monte Carlo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Mastrototaro",
      "Jimmy Olsson"
    ]
  },
  "https://proceedings.mlr.press/v235/matias24a.html": {
    "title": "Amortized Variational Deep Kernel Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alan L. S. Matias",
      "César Lincoln Mattos",
      "João Paulo Pordeus Gomes",
      "Diego Mesquita"
    ]
  },
  "https://proceedings.mlr.press/v235/mattes24a.html": {
    "title": "Hieros: Hierarchical Imagination on Structured State Space Sequence World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Mattes",
      "Rainer Schlosser",
      "Ralf Herbrich"
    ]
  },
  "https://proceedings.mlr.press/v235/matthews24a.html": {
    "title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Matthews",
      "Michael Beukman",
      "Benjamin Ellis",
      "Mikayel Samvelyan",
      "Matthew Thomas Jackson",
      "Samuel Coward",
      "Jakob Nicolaus Foerster"
    ]
  },
  "https://proceedings.mlr.press/v235/maurais24a.html": {
    "title": "Sampling in Unit Time with Kernel Fisher-Rao Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aimee Maurais",
      "Youssef Marzouk"
    ]
  },
  "https://proceedings.mlr.press/v235/maus24a.html": {
    "title": "Joint Composite Latent Space Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natalie Maus",
      "Zhiyuan Jerry Lin",
      "Maximilian Balandat",
      "Eytan Bakshy"
    ]
  },
  "https://proceedings.mlr.press/v235/mazeika24a.html": {
    "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mantas Mazeika",
      "Long Phan",
      "Xuwang Yin",
      "Andy Zou",
      "Zifan Wang",
      "Norman Mu",
      "Elham Sakhaee",
      "Nathaniel Li",
      "Steven Basart",
      "Bo Li",
      "David Forsyth",
      "Dan Hendrycks"
    ]
  },
  "https://proceedings.mlr.press/v235/mazzawi24a.html": {
    "title": "Deep Fusion: Efficient Network Training via Pre-trained Initializations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanna Mazzawi",
      "Javier Gonzalvo",
      "Michael Wunder",
      "Sammy Jerome",
      "Benoit Dherin"
    ]
  },
  "https://proceedings.mlr.press/v235/mccauley24a.html": {
    "title": "Incremental Topological Ordering and Cycle Detection with Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Mccauley",
      "Benjamin Moseley",
      "Aidin Niaparast",
      "Shikha Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/mcduff24a.html": {
    "title": "Position: Standardization of Behavioral Use Clauses is Necessary for the Adoption of Responsible Licensing of AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Mcduff",
      "Tim Korjakow",
      "Scott Cambo",
      "Jesse Josua Benjamin",
      "Jenny Lee",
      "Yacine Jernite",
      "Carlos Muñoz Ferrandis",
      "Aaron Gokaslan",
      "Alek Tarkowski",
      "Joseph Lindley",
      "A. Feder Cooper",
      "Danish Contractor"
    ]
  },
  "https://proceedings.mlr.press/v235/mcmahan24a.html": {
    "title": "Roping in Uncertainty: Robustness and Regularization in Markov Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Mcmahan",
      "Giovanni Artiglio",
      "Qiaomin Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/meeus24a.html": {
    "title": "Copyright Traps for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthieu Meeus",
      "Igor Shilov",
      "Manuel Faysse",
      "Yves-Alexandre De Montjoye"
    ]
  },
  "https://proceedings.mlr.press/v235/melas-kyriazi24a.html": {
    "title": "IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Melas-Kyriazi",
      "Iro Laina",
      "Christian Rupprecht",
      "Natalia Neverova",
      "Andrea Vedaldi",
      "Oran Gafni",
      "Filippos Kokkinos"
    ]
  },
  "https://proceedings.mlr.press/v235/melnyk24a.html": {
    "title": "O$n$ Learning Deep O($n$)-Equivariant Hyperspheres",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavlo Melnyk",
      "Michael Felsberg",
      "Mårten Wadenbäck",
      "Andreas Robinson",
      "Cuong Le"
    ]
  },
  "https://proceedings.mlr.press/v235/memmel24a.html": {
    "title": "Position: Tensor Networks are a Valuable Asset for Green AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eva Memmel",
      "Clara Menzen",
      "Jetze Schuurmans",
      "Frederiek Wesel",
      "Kim Batselier"
    ]
  },
  "https://proceedings.mlr.press/v235/meng24a.html": {
    "title": "OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Meng",
      "Shibal Ibrahim",
      "Kayhan Behdin",
      "Hussein Hazimeh",
      "Natalia Ponomareva",
      "Rahul Mazumder"
    ]
  },
  "https://proceedings.mlr.press/v235/meng24b.html": {
    "title": "Physics-Informed Neural Network Policy Iteration: Algorithms, Convergence, and Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Meng",
      "Ruikun Zhou",
      "Amartya Mukherjee",
      "Maxwell Fitzsimmons",
      "Christopher Song",
      "Jun Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/meng24c.html": {
    "title": "Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuran Meng",
      "Difan Zou",
      "Yuan Cao"
    ]
  },
  "https://proceedings.mlr.press/v235/mergny24a.html": {
    "title": "Spectral Phase Transition and Optimal PCA in Block-Structured Spiked Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Mergny",
      "Justin Ko",
      "Florent Krzakala"
    ]
  },
  "https://proceedings.mlr.press/v235/merrill24a.html": {
    "title": "The Illusion of State in State-Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Merrill",
      "Jackson Petty",
      "Ashish Sabharwal"
    ]
  },
  "https://proceedings.mlr.press/v235/merth24a.html": {
    "title": "Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Merth",
      "Qichen Fu",
      "Mohammad Rastegari",
      "Mahyar Najibi"
    ]
  },
  "https://proceedings.mlr.press/v235/miao24a.html": {
    "title": "How Deep Do We Need: Accelerating Training and Inference of Neural ODEs via Control Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyan Miao",
      "Konstantinos Gatsis"
    ]
  },
  "https://proceedings.mlr.press/v235/miao24b.html": {
    "title": "Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Miao",
      "Zhiyuan Lu",
      "Mia Liu",
      "Javier Duarte",
      "Pan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/miao24c.html": {
    "title": "Rethinking Independent Cross-Entropy Loss For Graph-Structured Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Miao",
      "Kaixiong Zhou",
      "Yili Wang",
      "Ninghao Liu",
      "Ying Wang",
      "Xin Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/miao24d.html": {
    "title": "DFlow: A Generative Model Combining Denoising AutoEncoder and Normalizing Flow for High Fidelity Waveform Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenfeng Miao",
      "Qingying Zhu",
      "Minchuan Chen",
      "Wei Hu",
      "Zijian Li",
      "Shaojun Wang",
      "Jing Xiao"
    ]
  },
  "https://proceedings.mlr.press/v235/michel24a.html": {
    "title": "Rethinking Momentum Knowledge Distillation in Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Michel",
      "Maorong Wang",
      "Ling Xiao",
      "Toshihiko Yamasaki"
    ]
  },
  "https://proceedings.mlr.press/v235/micheli24a.html": {
    "title": "Efficient World Models with Context-Aware Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Micheli",
      "Eloi Alonso",
      "François Fleuret"
    ]
  },
  "https://proceedings.mlr.press/v235/mihelich24a.html": {
    "title": "Interplay of ROC and Precision-Recall AUCs: Theoretical Limits and Practical Implications in Binary Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Mihelich",
      "François Castagnos",
      "Charles Dognin"
    ]
  },
  "https://proceedings.mlr.press/v235/mikhael24a.html": {
    "title": "CLIPZyme: Reaction-Conditioned Virtual Screening of Enzymes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Mikhael",
      "Itamar Chinn",
      "Regina Barzilay"
    ]
  },
  "https://proceedings.mlr.press/v235/miller24a.html": {
    "title": "FlowMM: Generating Materials with Riemannian Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Kurt Miller",
      "Ricky T. Q. Chen",
      "Anuroop Sriram",
      "Brandon M Wood"
    ]
  },
  "https://proceedings.mlr.press/v235/min24a.html": {
    "title": "Can Implicit Bias Imply Adversarial Robustness?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hancheng Min",
      "Rene Vidal"
    ]
  },
  "https://proceedings.mlr.press/v235/ming24a.html": {
    "title": "Understanding Retrieval-Augmented Task Adaptation for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Ming",
      "Yixuan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/mirzaei24a.html": {
    "title": "RODEO: Robust Outlier Detection via Exposing Adaptive Out-of-Distribution Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Mirzaei",
      "Mohammad Jafari",
      "Hamid Reza Dehbashi",
      "Ali Ansari",
      "Sepehr Ghobadi",
      "Masoud Hadi",
      "Arshia Soltani Moakhar",
      "Mohammad Azizmalayeri",
      "Mahdieh Soleymani Baghshah",
      "Mohammad Hossein Rohban"
    ]
  },
  "https://proceedings.mlr.press/v235/mishchenko24a.html": {
    "title": "Prodigy: An Expeditiously Adaptive Parameter-Free Learner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Mishchenko",
      "Aaron Defazio"
    ]
  },
  "https://proceedings.mlr.press/v235/mishra24a.html": {
    "title": "From Inverse Optimization to Feasibility to ERM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurabh Kumar Mishra",
      "Anant Raj",
      "Sharan Vaswani"
    ]
  },
  "https://proceedings.mlr.press/v235/misra24a.html": {
    "title": "Provable Interactive Learning with Hindsight Instruction Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dipendra Misra",
      "Aldo Pacchiano",
      "Robert E. Schapire"
    ]
  },
  "https://proceedings.mlr.press/v235/mitelut24a.html": {
    "title": "Position: Intent-aligned AI Systems Must Optimize for Agency Preservation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Catalin Mitelut",
      "Benjamin Smith",
      "Peter Vamplew"
    ]
  },
  "https://proceedings.mlr.press/v235/mitrovic24a.html": {
    "title": "Faster Streaming and Scalable Algorithms for Finding Directed Dense Subgraphs in Large Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Slobodan Mitrovic",
      "Theodore Pan"
    ]
  },
  "https://proceedings.mlr.press/v235/mo24a.html": {
    "title": "TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichuan Mo",
      "Hui Huang",
      "Mingjie Li",
      "Ang Li",
      "Yisen Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/modoranu24a.html": {
    "title": "Error Feedback Can Accurately Compress Preconditioners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ionut-Vlad Modoranu",
      "Aleksei Kalinov",
      "Eldar Kurtic",
      "Elias Frantar",
      "Dan Alistarh"
    ]
  },
  "https://proceedings.mlr.press/v235/mohamadi24a.html": {
    "title": "Why Do You Grok? A Theoretical Analysis on Grokking Modular Addition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamad Amin Mohamadi",
      "Zhiyuan Li",
      "Lei Wu",
      "Danica J. Sutherland"
    ]
  },
  "https://proceedings.mlr.press/v235/mohamed24a.html": {
    "title": "Straight-Through Meets Sparse Recovery: the Support Exploration Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mimoun Mohamed",
      "Francois Malgouyres",
      "Valentin Emiya",
      "Caroline Chaux"
    ]
  },
  "https://proceedings.mlr.press/v235/mohan24a.html": {
    "title": "OAK: Enriching Document Representations using Auxiliary Knowledge for Extreme Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikhar Mohan",
      "Deepak Saini",
      "Anshul Mittal",
      "Sayak Ray Chowdhury",
      "Bhawna Paliwal",
      "Jian Jiao",
      "Manish Gupta",
      "Manik Varma"
    ]
  },
  "https://proceedings.mlr.press/v235/mohri24a.html": {
    "title": "Language Models with Conformal Factuality Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Mohri",
      "Tatsunori Hashimoto"
    ]
  },
  "https://proceedings.mlr.press/v235/moller24a.html": {
    "title": "Finding NEM-U: Explaining unsupervised representation learning through neural network generated explanation masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bjørn Leth Møller",
      "Christian Igel",
      "Kristoffer Knutsen Wickstrøm",
      "Jon Sporring",
      "Robert Jenssen",
      "Bulat Ibragimov"
    ]
  },
  "https://proceedings.mlr.press/v235/monath24a.html": {
    "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Monath",
      "Will Sussman Grathwohl",
      "Michael Boratko",
      "Rob Fergus",
      "Andrew Mccallum",
      "Manzil Zaheer"
    ]
  },
  "https://proceedings.mlr.press/v235/mondal24a.html": {
    "title": "Slot Abstractors: Toward Scalable Abstract Visual Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanka Subhra Mondal",
      "Jonathan D. Cohen",
      "Taylor Whittington Webb"
    ]
  },
  "https://proceedings.mlr.press/v235/moniri24a.html": {
    "title": "A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Behrad Moniri",
      "Donghwan Lee",
      "Hamed Hassani",
      "Edgar Dobriban"
    ]
  },
  "https://proceedings.mlr.press/v235/montenegro24a.html": {
    "title": "Learning Optimal Deterministic Policies with Stochastic Policy Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Montenegro",
      "Marco Mussi",
      "Alberto Maria Metelli",
      "Matteo Papini"
    ]
  },
  "https://proceedings.mlr.press/v235/moon24a.html": {
    "title": "A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehong Moon",
      "Moonseok Choi",
      "Eunggu Yun",
      "Jongmin Yoon",
      "Gayoung Lee",
      "Jaewoong Cho",
      "Juho Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/mordido24a.html": {
    "title": "Lookbehind-SAM: k steps back, 1 step forward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Goncalo Mordido",
      "Pranshu Malviya",
      "Aristide Baratin",
      "Sarath Chandar"
    ]
  },
  "https://proceedings.mlr.press/v235/morioka24a.html": {
    "title": "Causal Representation Learning Made Identifiable by Grouping of Observational Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroshi Morioka",
      "Aapo Hyvarinen"
    ]
  },
  "https://proceedings.mlr.press/v235/morris24a.html": {
    "title": "Position: Future Directions in the Theory of Graph Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Morris",
      "Fabrizio Frasca",
      "Nadav Dym",
      "Haggai Maron",
      "Ismail Ilkan Ceylan",
      "Ron Levie",
      "Derek Lim",
      "Michael M. Bronstein",
      "Martin Grohe",
      "Stefanie Jegelka"
    ]
  },
  "https://proceedings.mlr.press/v235/morris24b.html": {
    "title": "Position: Levels of AGI for Operationalizing Progress on the Path to AGI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meredith Ringel Morris",
      "Jascha Sohl-Dickstein",
      "Noah Fiedel",
      "Tris Warkentin",
      "Allan Dafoe",
      "Aleksandra Faust",
      "Clement Farabet",
      "Shane Legg"
    ]
  },
  "https://proceedings.mlr.press/v235/motamedi24a.html": {
    "title": "Gibbs Sampling of Continuous Potentials on a Quantum Computer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arsalan Motamedi",
      "Pooya Ronagh"
    ]
  },
  "https://proceedings.mlr.press/v235/mouli24a.html": {
    "title": "Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "S Chandra Mouli",
      "Danielle C. Maddix",
      "Shima Alizadeh",
      "Gaurav Gupta",
      "Andrew Stuart",
      "Michael W. Mahoney",
      "Bernie Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/mrini24a.html": {
    "title": "Privacy Attacks in Decentralized Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdellah El Mrini",
      "Edwige Cyffers",
      "Aurélien Bellet"
    ]
  },
  "https://proceedings.mlr.press/v235/mu24a.html": {
    "title": "RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Mu",
      "Junting Chen",
      "Qing-Long Zhang",
      "Shoufa Chen",
      "Qiaojun Yu",
      "Chongjian Ge",
      "Runjian Chen",
      "Zhixuan Liang",
      "Mengkang Hu",
      "Chaofan Tao",
      "Peize Sun",
      "Haibao Yu",
      "Chao Yang",
      "Wenqi Shao",
      "Wenhai Wang",
      "Jifeng Dai",
      "Yu Qiao",
      "Mingyu Ding",
      "Ping Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/mu24b.html": {
    "title": "On the Second-Order Convergence of Biased Policy Gradient Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqiao Mu",
      "Diego Klabjan"
    ]
  },
  "https://proceedings.mlr.press/v235/mudgal24a.html": {
    "title": "Controlled Decoding from Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sidharth Mudgal",
      "Jong Lee",
      "Harish Ganapathy",
      "Yaguang Li",
      "Tao Wang",
      "Yanping Huang",
      "Zhifeng Chen",
      "Heng-Tze Cheng",
      "Michael Collins",
      "Trevor Strohman",
      "Jilin Chen",
      "Alex Beutel",
      "Ahmad Beirami"
    ]
  },
  "https://proceedings.mlr.press/v235/mudrik24a.html": {
    "title": "SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noga Mudrik",
      "Gal Mishne",
      "Adam Shabti Charles"
    ]
  },
  "https://proceedings.mlr.press/v235/mukherjee24a.html": {
    "title": "SaVeR: Optimal Data Collection Strategy for Safe Policy Evaluation in Tabular MDP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhojyoti Mukherjee",
      "Josiah P. Hanna",
      "Robert D Nowak"
    ]
  },
  "https://proceedings.mlr.press/v235/muldrew24a.html": {
    "title": "Active Preference Learning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Muldrew",
      "Peter Hayes",
      "Mingtian Zhang",
      "David Barber"
    ]
  },
  "https://proceedings.mlr.press/v235/muller24a.html": {
    "title": "Is Kernel Prediction More Powerful than Gating in Convolutional Neural Networks?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenz K Muller"
    ]
  },
  "https://proceedings.mlr.press/v235/muller24b.html": {
    "title": "Truly No-Regret Learning in Constrained MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Müller",
      "Pragnya Alatur",
      "Volkan Cevher",
      "Giorgia Ramponi",
      "Niao He"
    ]
  },
  "https://proceedings.mlr.press/v235/muller24c.html": {
    "title": "Aligning Transformers with Weisfeiler-Leman",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luis Müller",
      "Christopher Morris"
    ]
  },
  "https://proceedings.mlr.press/v235/muller24d.html": {
    "title": "Position: Optimization in SciML Should Employ the Function Space Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Müller",
      "Marius Zeinhofer"
    ]
  },
  "https://proceedings.mlr.press/v235/munagala24a.html": {
    "title": "Individual Fairness in Graph Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamesh Munagala",
      "Govind S. Sankar"
    ]
  },
  "https://proceedings.mlr.press/v235/munos24a.html": {
    "title": "Nash Learning from Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Remi Munos",
      "Michal Valko",
      "Daniele Calandriello",
      "Mohammad Gheshlaghi Azar",
      "Mark Rowland",
      "Zhaohan Daniel Guo",
      "Yunhao Tang",
      "Matthieu Geist",
      "Thomas Mesnard",
      "Côme Fiegel",
      "Andrea Michi",
      "Marco Selvi",
      "Sertan Girgin",
      "Nikola Momchev",
      "Olivier Bachem",
      "Daniel J Mankowitz",
      "Doina Precup",
      "Bilal Piot"
    ]
  },
  "https://proceedings.mlr.press/v235/munteanu24a.html": {
    "title": "Optimal bounds for $\\ell_p$ sensitivity sampling via $\\ell_2$ augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Munteanu",
      "Simon Omlor"
    ]
  },
  "https://proceedings.mlr.press/v235/munteanu24b.html": {
    "title": "Turnstile $\\ell_p$ leverage score sampling with applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Munteanu",
      "Simon Omlor"
    ]
  },
  "https://proceedings.mlr.press/v235/muqeeth24a.html": {
    "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Muqeeth",
      "Haokun Liu",
      "Yufan Liu",
      "Colin Raffel"
    ]
  },
  "https://proceedings.mlr.press/v235/murphy24a.html": {
    "title": "Autoformalizing Euclidean Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Logan Murphy",
      "Kaiyu Yang",
      "Jialiang Sun",
      "Zhaoyu Li",
      "Anima Anandkumar",
      "Xujie Si"
    ]
  },
  "https://proceedings.mlr.press/v235/murty24a.html": {
    "title": "BAGEL: Bootstrapping Agents by Guiding Exploration with Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikhar Murty",
      "Christopher D Manning",
      "Peter Shaw",
      "Mandar Joshi",
      "Kenton Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/mussi24a.html": {
    "title": "Factored-Reward Bandits with Intermediate Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Mussi",
      "Simone Drago",
      "Marcello Restelli",
      "Alberto Maria Metelli"
    ]
  },
  "https://proceedings.mlr.press/v235/mussi24b.html": {
    "title": "Best Arm Identification for Stochastic Rising Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Mussi",
      "Alessandro Montenegro",
      "Francesco Trovò",
      "Marcello Restelli",
      "Alberto Maria Metelli"
    ]
  },
  "https://proceedings.mlr.press/v235/mustafa24a.html": {
    "title": "GATE: How to Keep Out Intrusive Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nimrah Mustafa",
      "Rebekka Burkholz"
    ]
  },
  "https://proceedings.mlr.press/v235/mutti24a.html": {
    "title": "Test-Time Regret Minimization in Meta Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mirco Mutti",
      "Aviv Tamar"
    ]
  },
  "https://proceedings.mlr.press/v235/muzellec24a.html": {
    "title": "Saliency strikes back: How filtering out high frequencies improves white-box explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabine Muzellec",
      "Thomas Fel",
      "Victor Boutin",
      "Léo Andéol",
      "Rufin Vanrullen",
      "Thomas Serre"
    ]
  },
  "https://proceedings.mlr.press/v235/myers24a.html": {
    "title": "Learning Temporal Distances: Contrastive Successor Features Can Provide a Metric Structure for Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Myers",
      "Chongyi Zheng",
      "Anca Dragan",
      "Sergey Levine",
      "Benjamin Eysenbach"
    ]
  },
  "https://proceedings.mlr.press/v235/na24a.html": {
    "title": "Diffusion Rejection Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeonghu Na",
      "Yeongmin Kim",
      "Minsang Park",
      "Donghyeok Shin",
      "Wanmo Kang",
      "Il-Chul Moon"
    ]
  },
  "https://proceedings.mlr.press/v235/na24b.html": {
    "title": "LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungho Na",
      "Il-Chul Moon"
    ]
  },
  "https://proceedings.mlr.press/v235/nabarro24a.html": {
    "title": "Learning in Deep Factor Graphs with Gaussian Belief Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seth Nabarro",
      "Mark Van Der Wilk",
      "Andrew Davison"
    ]
  },
  "https://proceedings.mlr.press/v235/naderiparizi24a.html": {
    "title": "Don't be so Negative! Score-based Generative Modeling with Oracle-assisted Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeid Naderiparizi",
      "Xiaoxuan Liang",
      "Setareh Cohan",
      "Berend Zwartsenberg",
      "Frank Wood"
    ]
  },
  "https://proceedings.mlr.press/v235/nadew24a.html": {
    "title": "Conditionally-Conjugate Gaussian Process Factor Analysis for Spike Count Data via Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yididiya Y. Nadew",
      "Xuhui Fan",
      "Christopher John Quinn"
    ]
  },
  "https://proceedings.mlr.press/v235/nadjahi24a.html": {
    "title": "Slicing Mutual Information Generalization Bounds for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kimia Nadjahi",
      "Kristjan Greenewald",
      "Rickard Brüel Gabrielsson",
      "Justin Solomon"
    ]
  },
  "https://proceedings.mlr.press/v235/nagalapatti24a.html": {
    "title": "PairNet: Training with Observed Pairs to Estimate Individual Treatment Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lokesh Nagalapatti",
      "Pranava Singhal",
      "Avishek Ghosh",
      "Sunita Sarawagi"
    ]
  },
  "https://proceedings.mlr.press/v235/nagumo24a.html": {
    "title": "Density Ratio Estimation with Doubly Strong Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryosuke Nagumo",
      "Hironori Fujisawa"
    ]
  },
  "https://proceedings.mlr.press/v235/nam24a.html": {
    "title": "Solving Poisson Equations using Neural Walk-on-Spheres",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Chul Nam",
      "Julius Berner",
      "Anima Anandkumar"
    ]
  },
  "https://proceedings.mlr.press/v235/narasimhan24a.html": {
    "title": "Time Weaver: A Conditional Time Series Generation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Shankar Narasimhan",
      "Shubhankar Agarwal",
      "Oguzhan Akcin",
      "Sujay Sanghavi",
      "Sandeep P. Chinchali"
    ]
  },
  "https://proceedings.mlr.press/v235/nasiriany24a.html": {
    "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soroush Nasiriany",
      "Fei Xia",
      "Wenhao Yu",
      "Ted Xiao",
      "Jacky Liang",
      "Ishita Dasgupta",
      "Annie Xie",
      "Danny Driess",
      "Ayzaan Wahid",
      "Zhuo Xu",
      "Quan Vuong",
      "Tingnan Zhang",
      "Tsang-Wei Edward Lee",
      "Kuang-Huei Lee",
      "Peng Xu",
      "Sean Kirmani",
      "Yuke Zhu",
      "Andy Zeng",
      "Karol Hausman",
      "Nicolas Heess",
      "Chelsea Finn",
      "Sergey Levine",
      "Brian Ichter"
    ]
  },
  "https://proceedings.mlr.press/v235/nauman24a.html": {
    "title": "Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Nauman",
      "Michał Bortkiewicz",
      "Piotr Miłoś",
      "Tomasz Trzcinski",
      "Mateusz Ostaszewski",
      "Marek Cygan"
    ]
  },
  "https://proceedings.mlr.press/v235/naumann24a.html": {
    "title": "Box Facets and Cut Facets of Lifted Multicut Polytopes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Fabian Naumann",
      "Jannik Irmai",
      "Shengxian Zhao",
      "Bjoern Andres"
    ]
  },
  "https://proceedings.mlr.press/v235/navon24a.html": {
    "title": "Equivariant Deep Weight Space Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aviv Navon",
      "Aviv Shamsian",
      "Ethan Fetaya",
      "Gal Chechik",
      "Nadav Dym",
      "Haggai Maron"
    ]
  },
  "https://proceedings.mlr.press/v235/nawrot24a.html": {
    "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Nawrot",
      "Adrian Łańcucki",
      "Marcin Chochowski",
      "David Tarjan",
      "Edoardo Ponti"
    ]
  },
  "https://proceedings.mlr.press/v235/nazaret24a.html": {
    "title": "Stable Differentiable Causal Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Achille Nazaret",
      "Justin Hong",
      "Elham Azizi",
      "David Blei"
    ]
  },
  "https://proceedings.mlr.press/v235/neekhara24a.html": {
    "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paarth Neekhara",
      "Shehzeen Samarah Hussain",
      "Rafael Valle",
      "Boris Ginsburg",
      "Rishabh Ranjan",
      "Shlomo Dubnov",
      "Farinaz Koushanfar",
      "Julian Mcauley"
    ]
  },
  "https://proceedings.mlr.press/v235/neklyudov24a.html": {
    "title": "A Computational Framework for Solving Wasserstein Lagrangian Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirill Neklyudov",
      "Rob Brekelmans",
      "Alexander Tong",
      "Lazar Atanackovic",
      "Qiang Liu",
      "Alireza Makhzani"
    ]
  },
  "https://proceedings.mlr.press/v235/nelaturu24a.html": {
    "title": "On The Fairness Impacts of Hardware Selection in Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sree Harsha Nelaturu",
      "Nishaanth Kanna Ravichandran",
      "Cuong Tran",
      "Sara Hooker",
      "Ferdinando Fioretto"
    ]
  },
  "https://proceedings.mlr.press/v235/neu24a.html": {
    "title": "Dealing With Unbounded Gradients in Stochastic Saddle-point Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gergely Neu",
      "Nneka Okolo"
    ]
  },
  "https://proceedings.mlr.press/v235/ng24a.html": {
    "title": "Score-Based Causal Discovery of Latent Variable Causal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ignavier Ng",
      "Xinshuai Dong",
      "Haoyue Dai",
      "Biwei Huang",
      "Peter Spirtes",
      "Kun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/ng24b.html": {
    "title": "Measuring Stochastic Data Complexity with Boltzmann Influence Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Hoyen Ng",
      "Roger Baker Grosse",
      "Marzyeh Ghassemi"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24a.html": {
    "title": "Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nguyen",
      "Pedram Akbarian",
      "Nhat Ho"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24b.html": {
    "title": "A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nguyen",
      "Pedram Akbarian",
      "Trungtin Nguyen",
      "Nhat Ho"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24c.html": {
    "title": "PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phong C.H. Nguyen",
      "Xinlun Cheng",
      "Shahab Azarfar",
      "Pradeep Seshadri",
      "Yen T. Nguyen",
      "Munho Kim",
      "Sanghun Choi",
      "H.S. Udaykumar",
      "Stephen Baek"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24d.html": {
    "title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Nguyen",
      "Adji Bousso Dieng"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24e.html": {
    "title": "Multiplicative Weights Update, Area Convexity and Random Coordinate Descent for Densest Subgraph Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ta Duy Nguyen",
      "Alina Ene"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24f.html": {
    "title": "On Least Square Estimation in Softmax Gating Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nguyen",
      "Nhat Ho",
      "Alessandro Rinaldo"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24g.html": {
    "title": "Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duy Minh Ho Nguyen",
      "Nina Lukashina",
      "Tai Nguyen",
      "An Thai Le",
      "Trungtin Nguyen",
      "Nhat Ho",
      "Jan Peters",
      "Daniel Sonntag",
      "Viktor Zaverkin",
      "Mathias Niepert"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24h.html": {
    "title": "Generative Conditional Distributions by Neural (Entropic) Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bao Nguyen",
      "Binh Nguyen",
      "Hieu Trung Nguyen",
      "Viet Anh Nguyen"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24i.html": {
    "title": "PIDformer: Transformer Meets Control Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tam Minh Nguyen",
      "Cesar A Uribe",
      "Tan Minh Nguyen",
      "Richard Baraniuk"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24j.html": {
    "title": "Differentially private exact recovery for stochastic block models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dung Nguyen",
      "Anil Kumar Vullikanti"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24k.html": {
    "title": "Novel Spectral Algorithms for the Partial Credit Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duc Nguyen",
      "Anderson Ye Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen24l.html": {
    "title": "Sliced Wasserstein with Random-Path Projecting Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khai Nguyen",
      "Shujian Zhang",
      "Tam Le",
      "Nhat Ho"
    ]
  },
  "https://proceedings.mlr.press/v235/nguyen-tang24a.html": {
    "title": "On The Statistical Complexity of Offline Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh Nguyen-Tang",
      "Raman Arora"
    ]
  },
  "https://proceedings.mlr.press/v235/ni24a.html": {
    "title": "NExT: Teaching Large Language Models to Reason about Code Execution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ansong Ni",
      "Miltiadis Allamanis",
      "Arman Cohan",
      "Yinlin Deng",
      "Kensen Shi",
      "Charles Sutton",
      "Pengcheng Yin"
    ]
  },
  "https://proceedings.mlr.press/v235/ni24b.html": {
    "title": "On the Nonlinearity of Layer Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Ni",
      "Yuxin Guo",
      "Junlong Jia",
      "Lei Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/ni24c.html": {
    "title": "Risk-Sensitive Reward-Free Reinforcement Learning with CVaR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Ni",
      "Guanlin Liu",
      "Lifeng Lai"
    ]
  },
  "https://proceedings.mlr.press/v235/nichani24a.html": {
    "title": "How Transformers Learn Causal Structure with Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eshaan Nichani",
      "Alex Damian",
      "Jason D. Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/nie24a.html": {
    "title": "Online Cascade Learning for Efficient Inference over Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lunyiu Nie",
      "Zhimin Ding",
      "Erdong Hu",
      "Christopher Jermaine",
      "Swarat Chaudhuri"
    ]
  },
  "https://proceedings.mlr.press/v235/nie24b.html": {
    "title": "Compositional Text-to-Image Generation with Dense Blob Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weili Nie",
      "Sifei Liu",
      "Morteza Mardani",
      "Chao Liu",
      "Benjamin Eckart",
      "Arash Vahdat"
    ]
  },
  "https://proceedings.mlr.press/v235/niedoba24a.html": {
    "title": "Nearest Neighbour Score Estimators for Diffusion Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Niedoba",
      "Dylan Green",
      "Saeid Naderiparizi",
      "Vasileios Lioutas",
      "Jonathan Wilder Lavington",
      "Xiaoxuan Liang",
      "Yunpeng Liu",
      "Ke Zhang",
      "Setareh Dabiri",
      "Adam Scibior",
      "Berend Zwartsenberg",
      "Frank Wood"
    ]
  },
  "https://proceedings.mlr.press/v235/nika24a.html": {
    "title": "Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Nika",
      "Debmalya Mandal",
      "Parameswaran Kamalaruban",
      "Georgios Tzannetos",
      "Goran Radanovic",
      "Adish Singla"
    ]
  },
  "https://proceedings.mlr.press/v235/nikdan24a.html": {
    "title": "RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Nikdan",
      "Soroush Tabesh",
      "Elvir Crnčević",
      "Dan Alistarh"
    ]
  },
  "https://proceedings.mlr.press/v235/nilsson24a.html": {
    "title": "REMEDI: Corrective Transformations for Improved Neural Entropy Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktor Nilsson",
      "Anirban Samaddar",
      "Sandeep Madireddy",
      "Pierre Nyquist"
    ]
  },
  "https://proceedings.mlr.press/v235/nilsson24b.html": {
    "title": "Indirectly Parameterized Concrete Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alfred Nilsson",
      "Klas Wijk",
      "Sai Bharath Chandra Gutha",
      "Erik Englesson",
      "Alexandra Hotti",
      "Carlo Saccardi",
      "Oskar Kviman",
      "Jens Lagergren",
      "Ricardo Vinuesa Motilva",
      "Hossein Azizpour"
    ]
  },
  "https://proceedings.mlr.press/v235/nishino24a.html": {
    "title": "Understanding the Impact of Introducing Constraints at Inference Time on Generalization Error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masaaki Nishino",
      "Kengo Nakamura",
      "Norihito Yasuda"
    ]
  },
  "https://proceedings.mlr.press/v235/nitsure24a.html": {
    "title": "Risk Aware Benchmarking of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Apoorva Nitsure",
      "Youssef Mroueh",
      "Mattia Rigotti",
      "Kristjan Greenewald",
      "Brian Belgodere",
      "Mikhail Yurochkin",
      "Jiri Navratil",
      "Igor Melnyk",
      "Jarret Ross"
    ]
  },
  "https://proceedings.mlr.press/v235/niu24a.html": {
    "title": "Test-Time Model Adaptation with Only Forward Passes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaicheng Niu",
      "Chunyan Miao",
      "Guohao Chen",
      "Pengcheng Wu",
      "Peilin Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/niu24b.html": {
    "title": "Latent Optimal Paths by Gumbel Propagation for Variational Bayesian Dynamic Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinlei Niu",
      "Christian Walder",
      "Jing Zhang",
      "Charles Patrick Martin"
    ]
  },
  "https://proceedings.mlr.press/v235/niu24c.html": {
    "title": "GFlowNet Training by Policy Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puhua Niu",
      "Shili Wu",
      "Mingzhou Fan",
      "Xiaoning Qian"
    ]
  },
  "https://proceedings.mlr.press/v235/niu24d.html": {
    "title": "Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijia Niu",
      "Dongxia Wu",
      "Kai Kim",
      "Yian Ma",
      "Duncan Watson-Parris",
      "Rose Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/nori24a.html": {
    "title": "RNAFlow: RNA Structure & Sequence Design via Inverse Folding-Based Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divya Nori",
      "Wengong Jin"
    ]
  },
  "https://proceedings.mlr.press/v235/nottingham24a.html": {
    "title": "Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kolby Nottingham",
      "Bodhisattwa Prasad Majumder",
      "Bhavana Dalvi Mishra",
      "Sameer Singh",
      "Peter Clark",
      "Roy Fox"
    ]
  },
  "https://proceedings.mlr.press/v235/novack24a.html": {
    "title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Novack",
      "Julian Mcauley",
      "Taylor Berg-Kirkpatrick",
      "Nicholas J. Bryan"
    ]
  },
  "https://proceedings.mlr.press/v235/novello24a.html": {
    "title": "$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Novello",
      "Andrea M Tonello"
    ]
  },
  "https://proceedings.mlr.press/v235/nowak24a.html": {
    "title": "Sparser, Better, Deeper, Stronger: Improving Static Sparse Training with Exact Orthogonal Initialization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandra Nowak",
      "Łukasz Gniecki",
      "Filip Szatkowski",
      "Jacek Tabor"
    ]
  },
  "https://proceedings.mlr.press/v235/obando-ceron24a.html": {
    "title": "In value-based deep reinforcement learning, a pruned network is a good network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johan Samir Obando Ceron",
      "Aaron Courville",
      "Pablo Samuel Castro"
    ]
  },
  "https://proceedings.mlr.press/v235/obando-ceron24b.html": {
    "title": "Mixtures of Experts Unlock Parameter Scaling for Deep RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johan Samir Obando Ceron",
      "Ghada Sokar",
      "Timon Willi",
      "Clare Lyle",
      "Jesse Farebrother",
      "Jakob Nicolaus Foerster",
      "Gintare Karolina Dziugaite",
      "Doina Precup",
      "Pablo Samuel Castro"
    ]
  },
  "https://proceedings.mlr.press/v235/oh24a.html": {
    "title": "On the Effectiveness of Supervision in Asymmetric Non-Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongheon Oh",
      "Kibok Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/oh24b.html": {
    "title": "Sign Gradient Descent-based Neuronal Dynamics: ANN-to-SNN Conversion Beyond ReLU Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunseok Oh",
      "Youngki Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/ohayon24a.html": {
    "title": "The Perception-Robustness Tradeoff in Deterministic Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Ohayon",
      "Tomer Michaeli",
      "Michael Elad"
    ]
  },
  "https://proceedings.mlr.press/v235/oikarinen24a.html": {
    "title": "Linear Explanations for Individual Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuomas Oikarinen",
      "Tsui-Wei Weng"
    ]
  },
  "https://proceedings.mlr.press/v235/oikonomidis24a.html": {
    "title": "Adaptive Proximal Gradient Methods Are Universal Without Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Oikonomidis",
      "Emanuel Laude",
      "Puya Latafat",
      "Andreas Themelis",
      "Panagiotis Patrinos"
    ]
  },
  "https://proceedings.mlr.press/v235/oko24a.html": {
    "title": "SILVER: Single-loop variance reduction and application to federated learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazusato Oko",
      "Shunta Akiyama",
      "Denny Wu",
      "Tomoya Murata",
      "Taiji Suzuki"
    ]
  },
  "https://proceedings.mlr.press/v235/oosterhuis24a.html": {
    "title": "Local Feature Selection without Label or Feature Leakage for Interpretable Machine Learning Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harrie Oosterhuis",
      "Lijun Lyu",
      "Avishek Anand"
    ]
  },
  "https://proceedings.mlr.press/v235/opedal24a.html": {
    "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Opedal",
      "Alessandro Stolfo",
      "Haruki Shirakami",
      "Ying Jiao",
      "Ryan Cotterell",
      "Bernhard Schölkopf",
      "Abulhair Saparov",
      "Mrinmaya Sachan"
    ]
  },
  "https://proceedings.mlr.press/v235/orlova24a.html": {
    "title": "Deep Stochastic Mechanics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Orlova",
      "Aleksei Ustimenko",
      "Ruoxi Jiang",
      "Peter Y. Lu",
      "Rebecca Willett"
    ]
  },
  "https://proceedings.mlr.press/v235/ortega24a.html": {
    "title": "Variational Linearized Laplace Approximation for Bayesian Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luis A. Ortega",
      "Simon Rodriguez Santana",
      "Daniel Hernández-Lobato"
    ]
  },
  "https://proceedings.mlr.press/v235/orvieto24a.html": {
    "title": "Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Orvieto",
      "Soham De",
      "Caglar Gulcehre",
      "Razvan Pascanu",
      "Samuel L Smith"
    ]
  },
  "https://proceedings.mlr.press/v235/osa24a.html": {
    "title": "Discovering Multiple Solutions from a Single Task in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takayuki Osa",
      "Tatsuya Harada"
    ]
  },
  "https://proceedings.mlr.press/v235/ostapenko24a.html": {
    "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oleksiy Ostapenko",
      "Zhan Su",
      "Edoardo Ponti",
      "Laurent Charlin",
      "Nicolas Le Roux",
      "Lucas Caccia",
      "Alessandro Sordoni"
    ]
  },
  "https://proceedings.mlr.press/v235/ouasfi24a.html": {
    "title": "Few-Shot Unsupervised Implicit Neural Shape Representation Learning with Spatial Adversaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amine Ouasfi",
      "Adnane Boukhayma"
    ]
  },
  "https://proceedings.mlr.press/v235/oulhaj24a.html": {
    "title": "Differentiable Mapper for Topological Optimization of Data Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyad Oulhaj",
      "Mathieu Carrière",
      "Bertrand Michel"
    ]
  },
  "https://proceedings.mlr.press/v235/ouyang24a.html": {
    "title": "Structured Chemistry Reasoning with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siru Ouyang",
      "Zhuosheng Zhang",
      "Bing Yan",
      "Xuan Liu",
      "Yejin Choi",
      "Jiawei Han",
      "Lianhui Qin"
    ]
  },
  "https://proceedings.mlr.press/v235/ozgul24a.html": {
    "title": "Stochastic Quantum Sampling for Non-Logconcave Distributions and Estimating Partition Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guneykan Ozgul",
      "Xiantao Li",
      "Mehrdad Mahdavi",
      "Chunhao Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/ozkara24a.html": {
    "title": "MADA: Meta-Adaptive Optimizers Through Hyper-Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaan Ozkara",
      "Can Karakus",
      "Parameswaran Raman",
      "Mingyi Hong",
      "Shoham Sabach",
      "Branislav Kveton",
      "Volkan Cevher"
    ]
  },
  "https://proceedings.mlr.press/v235/paissan24a.html": {
    "title": "Listenable Maps for Audio Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Paissan",
      "Mirco Ravanelli",
      "Cem Subakan"
    ]
  },
  "https://proceedings.mlr.press/v235/pal24a.html": {
    "title": "Implicit Representations via Operator Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourav Pal",
      "Harshavardhan Adepu",
      "Clinton Wang",
      "Polina Golland",
      "Vikas Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/palmarini24a.html": {
    "title": "Bayesian Program Learning by Decompiling Amortized Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro B. Palmarini",
      "Christopher G. Lucas",
      "Siddharth N"
    ]
  },
  "https://proceedings.mlr.press/v235/palumbo24a.html": {
    "title": "Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nils Palumbo",
      "Yang Guo",
      "Xi Wu",
      "Jiefeng Chen",
      "Yingyu Liang",
      "Somesh Jha"
    ]
  },
  "https://proceedings.mlr.press/v235/pan24a.html": {
    "title": "Counterfactual Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushu Pan",
      "Elias Bareinboim"
    ]
  },
  "https://proceedings.mlr.press/v235/pan24b.html": {
    "title": "MALIBO: Meta-learning for Likelihood-free Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarong Pan",
      "Stefan Falkner",
      "Felix Berkenkamp",
      "Joaquin Vanschoren"
    ]
  },
  "https://proceedings.mlr.press/v235/pan24c.html": {
    "title": "$S^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Pan",
      "Yushan Jiang",
      "Sahil Garg",
      "Anderson Schneider",
      "Yuriy Nevmyvaka",
      "Dongjin Song"
    ]
  },
  "https://proceedings.mlr.press/v235/pan24d.html": {
    "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Pan",
      "Erik Jones",
      "Meena Jagadeesan",
      "Jacob Steinhardt"
    ]
  },
  "https://proceedings.mlr.press/v235/pan24e.html": {
    "title": "Stability and Generalization for Stochastic Recursive Momentum-based Algorithms for (Strongly-)Convex One to $K$-Level Stochastic Optimizations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaokang Pan",
      "Xingyu Li",
      "Jin Liu",
      "Tao Sun",
      "Kai Sun",
      "Lixing Chen",
      "Zhe Qu"
    ]
  },
  "https://proceedings.mlr.press/v235/pan24f.html": {
    "title": "RMIB: Representation Matching Information Bottleneck for Matching Text Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haihui Pan",
      "Zhifang Liao",
      "Wenrui Xie",
      "Kun Han"
    ]
  },
  "https://proceedings.mlr.press/v235/pan24g.html": {
    "title": "Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michelle Pan",
      "Mariah L Schrum",
      "Vivek Myers",
      "Erdem Biyik",
      "Anca Dragan"
    ]
  },
  "https://proceedings.mlr.press/v235/pan24h.html": {
    "title": "Auto-Encoding Morph-Tokens for Multimodal LLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaihang Pan",
      "Siliang Tang",
      "Juncheng Li",
      "Zhaoyu Fan",
      "Wei Chow",
      "Shuicheng Yan",
      "Tat-Seng Chua",
      "Yueting Zhuang",
      "Hanwang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/panaganti24a.html": {
    "title": "Model-Free Robust $φ$-Divergence Reinforcement Learning Using Both Offline and Online Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kishan Panaganti",
      "Adam Wierman",
      "Eric Mazumdar"
    ]
  },
  "https://proceedings.mlr.press/v235/panda24a.html": {
    "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashwinee Panda",
      "Xinyu Tang",
      "Saeed Mahloujifar",
      "Vikash Sehwag",
      "Prateek Mittal"
    ]
  },
  "https://proceedings.mlr.press/v235/pandey24a.html": {
    "title": "BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Pandey",
      "Yatin Nandwani",
      "Tahira Naseem",
      "Mayank Mishra",
      "Guangxuan Xu",
      "Dinesh Raghu",
      "Sachindra Joshi",
      "Asim Munawar",
      "Ramón Fernandez Astudillo"
    ]
  },
  "https://proceedings.mlr.press/v235/pang24a.html": {
    "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianghe Pang",
      "Shuo Tang",
      "Rui Ye",
      "Yuxin Xiong",
      "Bolun Zhang",
      "Yanfeng Wang",
      "Siheng Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/panigrahi24a.html": {
    "title": "Trainable Transformer in Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Panigrahi",
      "Sadhika Malladi",
      "Mengzhou Xia",
      "Sanjeev Arora"
    ]
  },
  "https://proceedings.mlr.press/v235/paolo24a.html": {
    "title": "Position: A Call for Embodied AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Paolo",
      "Jonas Gonzalez-Billandon",
      "Balázs Kégl"
    ]
  },
  "https://proceedings.mlr.press/v235/papadopoulos24a.html": {
    "title": "Arrows of Time for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vassilis Papadopoulos",
      "Jérémie Wenger",
      "Clément Hongler"
    ]
  },
  "https://proceedings.mlr.press/v235/papamarkou24a.html": {
    "title": "Position: Topological Deep Learning is the New Frontier for Relational Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theodore Papamarkou",
      "Tolga Birdal",
      "Michael M. Bronstein",
      "Gunnar E. Carlsson",
      "Justin Curry",
      "Yue Gao",
      "Mustafa Hajij",
      "Roland Kwitt",
      "Pietro Lio",
      "Paolo Di Lorenzo",
      "Vasileios Maroulas",
      "Nina Miolane",
      "Farzana Nasrin",
      "Karthikeyan Natesan Ramamurthy",
      "Bastian Rieck",
      "Simone Scardapane",
      "Michael T Schaub",
      "Petar Veličković",
      "Bei Wang",
      "Yusu Wang",
      "Guowei Wei",
      "Ghada Zamzmi"
    ]
  },
  "https://proceedings.mlr.press/v235/papamarkou24b.html": {
    "title": "Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theodore Papamarkou",
      "Maria Skoularidou",
      "Konstantina Palla",
      "Laurence Aitchison",
      "Julyan Arbel",
      "David Dunson",
      "Maurizio Filippone",
      "Vincent Fortuin",
      "Philipp Hennig",
      "José Miguel Hernández-Lobato",
      "Aliaksandr Hubin",
      "Alexander Immer",
      "Theofanis Karaletsos",
      "Mohammad Emtiyaz Khan",
      "Agustinus Kristiadi",
      "Yingzhen Li",
      "Stephan Mandt",
      "Christopher Nemeth",
      "Michael A Osborne",
      "Tim G. J. Rudner",
      "David Rügamer",
      "Yee Whye Teh",
      "Max Welling",
      "Andrew Gordon Wilson",
      "Ruqi Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/park24a.html": {
    "title": "Memoria: Resolving Fateful Forgetting Problem through Human-Inspired Memory Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangjun Park",
      "Jinyeong Bak"
    ]
  },
  "https://proceedings.mlr.press/v235/park24b.html": {
    "title": "The Max-Min Formulation of Multi-Objective Reinforcement Learning: From Theory to a Model-Free Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giseung Park",
      "Woohyeon Byeon",
      "Seongmin Kim",
      "Elad Havakuk",
      "Amir Leshem",
      "Youngchul Sung"
    ]
  },
  "https://proceedings.mlr.press/v235/park24c.html": {
    "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiho Park",
      "Yo Joong Choe",
      "Victor Veitch"
    ]
  },
  "https://proceedings.mlr.press/v235/park24d.html": {
    "title": "Mitigating Oversmoothing Through Reverse Process of GNNs for Heterophilic Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moonjeong Park",
      "Jaeseung Heo",
      "Dongwoo Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/park24e.html": {
    "title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeonhong Park",
      "Jake Hyun",
      "Sanglyul Cho",
      "Bonggeun Sim",
      "Jae W. Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/park24f.html": {
    "title": "Mean-field Chaos Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungwoo Park",
      "Dongjun Kim",
      "Ahmed Alaa"
    ]
  },
  "https://proceedings.mlr.press/v235/park24g.html": {
    "title": "Foundation Policies with Hilbert Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seohong Park",
      "Tobias Kreiman",
      "Sergey Levine"
    ]
  },
  "https://proceedings.mlr.press/v235/park24h.html": {
    "title": "SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chanho Park",
      "Namyoon Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/park24i.html": {
    "title": "Position: Automatic Environment Shaping is the Next Frontier in RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Younghyo Park",
      "Gabriel B. Margolis",
      "Pulkit Agrawal"
    ]
  },
  "https://proceedings.mlr.press/v235/park24j.html": {
    "title": "Can Mamba Learn How To Learn? A Comparative Study on In-Context Learning Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongho Park",
      "Jaeseung Park",
      "Zheyang Xiong",
      "Nayoung Lee",
      "Jaewoong Cho",
      "Samet Oymak",
      "Kangwook Lee",
      "Dimitris Papailiopoulos"
    ]
  },
  "https://proceedings.mlr.press/v235/park24k.html": {
    "title": "BOtied: Multi-objective Bayesian optimization with tied multivariate ranks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Won Park",
      "Natasa Tagasovska",
      "Michael Maser",
      "Stephen Ra",
      "Kyunghyun Cho"
    ]
  },
  "https://proceedings.mlr.press/v235/parnichkun24a.html": {
    "title": "State-Free Inference of State-Space Models: The *Transfer Function* Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rom Parnichkun",
      "Stefano Massaroli",
      "Alessandro Moro",
      "Jimmy T.H. Smith",
      "Ramin Hasani",
      "Mathias Lechner",
      "Qi An",
      "Christopher Re",
      "Hajime Asama",
      "Stefano Ermon",
      "Taiji Suzuki",
      "Michael Poli",
      "Atsushi Yamashita"
    ]
  },
  "https://proceedings.mlr.press/v235/patel24a.html": {
    "title": "Variational Inference with Coverage Guarantees in Simulation-Based Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Patel",
      "Declan Mcnamara",
      "Jackson Loper",
      "Jeffrey Regier",
      "Ambuj Tewari"
    ]
  },
  "https://proceedings.mlr.press/v235/patel24b.html": {
    "title": "Towards Global Optimality for Practical Average Reward Reinforcement Learning without Mixing Time Oracles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhrij Patel",
      "Wesley A Suttle",
      "Alec Koppel",
      "Vaneet Aggarwal",
      "Brian M. Sadler",
      "Dinesh Manocha",
      "Amrit Bedi"
    ]
  },
  "https://proceedings.mlr.press/v235/patil24a.html": {
    "title": "Optimal Ridge Regularization for Out-of-Distribution Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratik Patil",
      "Jin-Hong Du",
      "Ryan Tibshirani"
    ]
  },
  "https://proceedings.mlr.press/v235/patwari24a.html": {
    "title": "PerceptAnon: Exploring the Human Perception of Image Anonymization Beyond Pseudonymization for GDPR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Patwari",
      "Chen-Nee Chuah",
      "Lingjuan Lyu",
      "Vivek Sharma"
    ]
  },
  "https://proceedings.mlr.press/v235/pauls24a.html": {
    "title": "Estimating Canopy Height at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Pauls",
      "Max Zimmer",
      "Una M. Kelly",
      "Martin Schwartz",
      "Sassan Saatchi",
      "Philippe Ciais",
      "Sebastian Pokutta",
      "Martin Brandt",
      "Fabian Gieseke"
    ]
  },
  "https://proceedings.mlr.press/v235/paulus24a.html": {
    "title": "LPGD: A General Framework for Backpropagation through Embedded Optimization Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anselm Paulus",
      "Georg Martius",
      "Vı́t Musil"
    ]
  },
  "https://proceedings.mlr.press/v235/pavse24a.html": {
    "title": "Learning to Stabilize Online Reinforcement Learning in Unbounded State Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brahma S Pavse",
      "Matthew Zurek",
      "Yudong Chen",
      "Qiaomin Xie",
      "Josiah P. Hanna"
    ]
  },
  "https://proceedings.mlr.press/v235/pawelczyk24a.html": {
    "title": "In-Context Unlearning: Language Models as Few-Shot Unlearners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Pawelczyk",
      "Seth Neel",
      "Himabindu Lakkaraju"
    ]
  },
  "https://proceedings.mlr.press/v235/pearce-crump24a.html": {
    "title": "Graph Automorphism Group Equivariant Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edward Pearce-Crump",
      "William Knottenbelt"
    ]
  },
  "https://proceedings.mlr.press/v235/pei24a.html": {
    "title": "Multi-Track Message Passing: Tackling Oversmoothing and Oversquashing in Graph Learning via Preventing Heterophily Mixing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbin Pei",
      "Yu Li",
      "Huiqi Deng",
      "Jingxin Hai",
      "Pinghui Wang",
      "Jie Ma",
      "Jing Tao",
      "Yuheng Xiong",
      "Xiaohong Guan"
    ]
  },
  "https://proceedings.mlr.press/v235/pei24b.html": {
    "title": "Exploiting Code Symmetries for Learning Program Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kexin Pei",
      "Weichen Li",
      "Qirui Jin",
      "Shuyang Liu",
      "Scott Geng",
      "Lorenzo Cavallaro",
      "Junfeng Yang",
      "Suman Jana"
    ]
  },
  "https://proceedings.mlr.press/v235/pei24c.html": {
    "title": "Modeling Language Tokens as Functionals of Semantic Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengqi Pei",
      "Anran Zhang",
      "Shuhui Wang",
      "Qingming Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/pei24d.html": {
    "title": "Data-free Neural Representation Compression with Riemannian Neural Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengqi Pei",
      "Anran Zhang",
      "Shuhui Wang",
      "Xiangyang Ji",
      "Qingming Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/pei24e.html": {
    "title": "BetterV: Controlled Verilog Generation with Discriminative Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehua Pei",
      "Huiling Zhen",
      "Mingxuan Yuan",
      "Yu Huang",
      "Bei Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/peleg24a.html": {
    "title": "Bias of Stochastic Gradient Descent or the Architecture: Disentangling the Effects of Overparameterization of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Peleg",
      "Matthias Hein"
    ]
  },
  "https://proceedings.mlr.press/v235/peng24a.html": {
    "title": "Knowledge Distillation with Auxiliary Variable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Peng",
      "Zhen Fang",
      "Guangquan Zhang",
      "Jie Lu"
    ]
  },
  "https://proceedings.mlr.press/v235/peng24b.html": {
    "title": "UPAM: Unified Prompt Attack in Text-to-Image Generation Models Against Both Textual Filters and Visual Checkers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Peng",
      "Qiuhong Ke",
      "Jun Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/peng24c.html": {
    "title": "eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Peng",
      "Xinyi Ling",
      "Ziru Chen",
      "Huan Sun",
      "Xia Ning"
    ]
  },
  "https://proceedings.mlr.press/v235/peng24d.html": {
    "title": "Pragmatic Feature Preferences: Learning Reward-Relevant Preferences from Human Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Peng",
      "Yuying Sun",
      "Tianmin Shu",
      "David Abel"
    ]
  },
  "https://proceedings.mlr.press/v235/peng24e.html": {
    "title": "UPOCR: Towards Unified Pixel-Level OCR Interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dezhi Peng",
      "Zhenhua Yang",
      "Jiaxin Zhang",
      "Chongyu Liu",
      "Yongxin Shi",
      "Kai Ding",
      "Fengjun Guo",
      "Lianwen Jin"
    ]
  },
  "https://proceedings.mlr.press/v235/peng24f.html": {
    "title": "Block Acceleration Without Momentum: On Optimal Stepsizes of Block Gradient Descent for Least-Squares",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangzu Peng",
      "Wotao Yin"
    ]
  },
  "https://proceedings.mlr.press/v235/peng24g.html": {
    "title": "FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Peng",
      "Han Yu",
      "Xiaoli Tang",
      "Xiaoxiao Li"
    ]
  },
  "https://proceedings.mlr.press/v235/peng24h.html": {
    "title": "Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Peng",
      "Ziyang Zheng",
      "Wenrui Dai",
      "Nuoqian Xiao",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://proceedings.mlr.press/v235/pensia24a.html": {
    "title": "A Subquadratic Time Algorithm for Robust Sparse Mean Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Pensia"
    ]
  },
  "https://proceedings.mlr.press/v235/pentyala24a.html": {
    "title": "CaPS: Collaborative and Private Synthetic Data Generation from Distributed Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sikha Pentyala",
      "Mayana Pereira",
      "Martine De Cock"
    ]
  },
  "https://proceedings.mlr.press/v235/peralez24a.html": {
    "title": "Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johan Peralez",
      "Aurélien Delage",
      "Olivier Buffet",
      "Jilles Steeve Dibangoye"
    ]
  },
  "https://proceedings.mlr.press/v235/perdomo24a.html": {
    "title": "The Relative Value of Prediction in Algorithmic Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Carlos Perdomo"
    ]
  },
  "https://proceedings.mlr.press/v235/permenter24a.html": {
    "title": "Interpreting and Improving Diffusion Models from an Optimization Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frank Permenter",
      "Chenyang Yuan"
    ]
  },
  "https://proceedings.mlr.press/v235/pervez24a.html": {
    "title": "Mechanistic Neural Networks for Scientific Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adeel Pervez",
      "Francesco Locatello",
      "Stratis Gavves"
    ]
  },
  "https://proceedings.mlr.press/v235/petrik24a.html": {
    "title": "Bayesian Regret Minimization in Offline Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marek Petrik",
      "Guy Tennenholtz",
      "Mohammad Ghavamzadeh"
    ]
  },
  "https://proceedings.mlr.press/v235/petrov24a.html": {
    "title": "Prompting a Pretrained Transformer Can Be a Universal Approximator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Petrov",
      "Philip Torr",
      "Adel Bibi"
    ]
  },
  "https://proceedings.mlr.press/v235/pezeshki24a.html": {
    "title": "Discovering Environments with XRM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Pezeshki",
      "Diane Bouchacourt",
      "Mark Ibrahim",
      "Nicolas Ballas",
      "Pascal Vincent",
      "David Lopez-Paz"
    ]
  },
  "https://proceedings.mlr.press/v235/pfrommer24a.html": {
    "title": "Transport of Algebraic Structure to Latent Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Pfrommer",
      "Brendon G. Anderson",
      "Somayeh Sojoudi"
    ]
  },
  "https://proceedings.mlr.press/v235/pham24a.html": {
    "title": "Neural NeRF Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan Pham",
      "Stephan Mandt"
    ]
  },
  "https://proceedings.mlr.press/v235/pham24b.html": {
    "title": "Cross-view Masked Diffusion Transformers for Person Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trung X. Pham",
      "Kang Zhang",
      "Chang D. Yoo"
    ]
  },
  "https://proceedings.mlr.press/v235/phan24a.html": {
    "title": "When is Transfer Learning Possible?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "My Phan",
      "Kianté Brantley",
      "Stephanie Milani",
      "Soroush Mehri",
      "Gokul Swamy",
      "Geoffrey J. Gordon"
    ]
  },
  "https://proceedings.mlr.press/v235/phan24b.html": {
    "title": "Controllable Prompt Tuning For Balancing Group Distributional Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Phan",
      "Andrew Gordon Wilson",
      "Qi Lei"
    ]
  },
  "https://proceedings.mlr.press/v235/phillips24a.html": {
    "title": "Particle Denoising Diffusion Sampler",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angus Phillips",
      "Hai-Dang Dau",
      "Michael John Hutchinson",
      "Valentin De Bortoli",
      "George Deligiannidis",
      "Arnaud Doucet"
    ]
  },
  "https://proceedings.mlr.press/v235/piao24a.html": {
    "title": "Federated Continual Learning via Prompt-based Dual Knowledge Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongming Piao",
      "Yichen Wu",
      "Dapeng Wu",
      "Ying Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/pieroth24a.html": {
    "title": "Detecting Influence Structures in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Raoul Pieroth",
      "Katherine Fitch",
      "Lenz Belzner"
    ]
  },
  "https://proceedings.mlr.press/v235/pierquin24a.html": {
    "title": "Rényi Pufferfish Privacy: General Additive Noise Mechanisms and Privacy Amplification by Iteration via Shift Reduction Lemmas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément Pierquin",
      "Aurélien Bellet",
      "Marc Tommasi",
      "Matthieu Boussard"
    ]
  },
  "https://proceedings.mlr.press/v235/pinheiro24a.html": {
    "title": "Structure-based drug design by denoising voxel grids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro O. Pinheiro",
      "Arian Rokkum Jamasb",
      "Omar Mahmood",
      "Vishnu Sresht",
      "Saeed Saremi"
    ]
  },
  "https://proceedings.mlr.press/v235/pinto24a.html": {
    "title": "Extracting Training Data From Document-Based VQA Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Pinto",
      "Nathalie Rauschmayr",
      "Florian Tramèr",
      "Philip Torr",
      "Federico Tombari"
    ]
  },
  "https://proceedings.mlr.press/v235/piran24a.html": {
    "title": "Contrasting Multiple Representations with the Multi-Marginal Matching Gap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zoe Piran",
      "Michal Klein",
      "James Thornton",
      "Marco Cuturi"
    ]
  },
  "https://proceedings.mlr.press/v235/piterbarg24a.html": {
    "title": "diff History for Neural Language Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ulyana Piterbarg",
      "Lerrel Pinto",
      "Rob Fergus"
    ]
  },
  "https://proceedings.mlr.press/v235/plaksin24a.html": {
    "title": "Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Plaksin",
      "Vitaly Kalev"
    ]
  },
  "https://proceedings.mlr.press/v235/podkopaev24a.html": {
    "title": "Adaptive Conformal Inference by Betting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandr Podkopaev",
      "Dong Xu",
      "Kuang-Chih Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/poli24a.html": {
    "title": "Mechanistic Design and Scaling of Hybrid Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Poli",
      "Armin W Thomas",
      "Eric Nguyen",
      "Pragaash Ponnusamy",
      "Björn Deiseroth",
      "Kristian Kersting",
      "Taiji Suzuki",
      "Brian Hie",
      "Stefano Ermon",
      "Christopher Re",
      "Ce Zhang",
      "Stefano Massaroli"
    ]
  },
  "https://proceedings.mlr.press/v235/pouplin24a.html": {
    "title": "Relaxed Quantile Regression: Prediction Intervals for Asymmetric Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Pouplin",
      "Alan Jeffares",
      "Nabeel Seedat",
      "Mihaela Van Der Schaar"
    ]
  },
  "https://proceedings.mlr.press/v235/poursoltani24a.html": {
    "title": "Robust Data-driven Prescriptiveness Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehran Poursoltani",
      "Erick Delage",
      "Angelos Georghiou"
    ]
  },
  "https://proceedings.mlr.press/v235/powell24a.html": {
    "title": "Stochastic Optimization with Arbitrary Recurrent Data Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Powell",
      "Hanbaek Lyu"
    ]
  },
  "https://proceedings.mlr.press/v235/prabhu24a.html": {
    "title": "Learning Multiple Secrets in Mastermind",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milind Prabhu",
      "David Woodruff"
    ]
  },
  "https://proceedings.mlr.press/v235/prajwal24a.html": {
    "title": "MusicFlow: Cascaded Flow Matching for Text Guided Music Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "K R Prajwal",
      "Bowen Shi",
      "Matthew Le",
      "Apoorv Vyas",
      "Andros Tjandra",
      "Mahi Luthra",
      "Baishan Guo",
      "Huiyu Wang",
      "Triantafyllos Afouras",
      "David Kant",
      "Wei-Ning Hsu"
    ]
  },
  "https://proceedings.mlr.press/v235/press24a.html": {
    "title": "The Entropy Enigma: Success and Failure of Entropy Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ori Press",
      "Ravid Shwartz-Ziv",
      "Yann Lecun",
      "Matthias Bethge"
    ]
  },
  "https://proceedings.mlr.press/v235/prinster24a.html": {
    "title": "Conformal Validity Guarantees Exist for Any Data Distribution (and How to Find Them)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Drew Prinster",
      "Samuel Don Stanton",
      "Anqi Liu",
      "Suchi Saria"
    ]
  },
  "https://proceedings.mlr.press/v235/prokhorov24a.html": {
    "title": "Autoencoding Conditional Neural Processes for Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Prokhorov",
      "Ivan Titov",
      "Siddharth N"
    ]
  },
  "https://proceedings.mlr.press/v235/provodin24a.html": {
    "title": "Efficient Exploration in Average-Reward Constrained Reinforcement Learning: Achieving Near-Optimal Regret With Posterior Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danil Provodin",
      "Maurits Clemens Kaptein",
      "Mykola Pechenizkiy"
    ]
  },
  "https://proceedings.mlr.press/v235/psenka24a.html": {
    "title": "Learning a Diffusion Model Policy from Rewards via Q-Score Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Psenka",
      "Alejandro Escontrela",
      "Pieter Abbeel",
      "Yi Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/pu24a.html": {
    "title": "Learning-Efficient Yet Generalizable Collaborative Filtering for Item Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Pu",
      "Xiaolong Chen",
      "Xu Huang",
      "Jin Chen",
      "Defu Lian",
      "Enhong Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/pu24b.html": {
    "title": "Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Pu",
      "Xingguo Lv",
      "Jiewen Yang",
      "He Guannan",
      "Xingbo Dong",
      "Yiqun Lin",
      "Li Shengli",
      "Tan Ying",
      "Liu Fei",
      "Ming Chen",
      "Zhe Jin",
      "Kenli Li",
      "Xiaomeng Li"
    ]
  },
  "https://proceedings.mlr.press/v235/pu24c.html": {
    "title": "Amortizing Pragmatic Program Synthesis with Rankings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yewen Pu",
      "Saujas Vaduguru",
      "Priyan Vaithilingam",
      "Elena Glassman",
      "Daniel Fried"
    ]
  },
  "https://proceedings.mlr.press/v235/puigdemont24a.html": {
    "title": "Learning to Remove Cuts in Integer Linear Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pol Puigdemont",
      "Stratis Skoulakis",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://proceedings.mlr.press/v235/pushkin24a.html": {
    "title": "On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denys Pushkin",
      "Raphaël Berthier",
      "Emmanuel Abbe"
    ]
  },
  "https://proceedings.mlr.press/v235/pyakurel24a.html": {
    "title": "Hierarchical Novelty Detection via Fine-Grained Evidence Allocation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spandan Pyakurel",
      "Qi Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/qi24a.html": {
    "title": "Conformalized Survival Distributions: A Generic Post-Process to Increase Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi-Ang Qi",
      "Yakun Yu",
      "Russell Greiner"
    ]
  },
  "https://proceedings.mlr.press/v235/qian24a.html": {
    "title": "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Qian",
      "Juncheng Li",
      "Yu Wu",
      "Yaobo Ye",
      "Hao Fei",
      "Tat-Seng Chua",
      "Yueting Zhuang",
      "Siliang Tang"
    ]
  },
  "https://proceedings.mlr.press/v235/qian24b.html": {
    "title": "ByMI: Byzantine Machine Identification with False Discovery Rate Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengde Qian",
      "Mengyuan Wang",
      "Haojie Ren",
      "Changliang Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/qian24c.html": {
    "title": "Efficient Non-stationary Online Learning by Wavelets with Applications to Online Distribution Shift Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Yang Qian",
      "Peng Zhao",
      "Yu-Jie Zhang",
      "Masashi Sugiyama",
      "Zhi-Hua Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/qiao24a.html": {
    "title": "Ensemble Pruning for Out-of-distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengchun Qiao",
      "Xi Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/qiao24b.html": {
    "title": "Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Qiao",
      "Yu-Xiang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/qiao24c.html": {
    "title": "ULAREF: A Unified Label Refinement Framework for Learning with Inaccurate Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Congyu Qiao",
      "Ning Xu",
      "Yihao Hu",
      "Xin Geng"
    ]
  },
  "https://proceedings.mlr.press/v235/qin24a.html": {
    "title": "Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qin",
      "Daoyuan Chen",
      "Bingchen Qian",
      "Bolin Ding",
      "Yaliang Li",
      "Shuiguang Deng"
    ]
  },
  "https://proceedings.mlr.press/v235/qin24b.html": {
    "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotong Qin",
      "Xudong Ma",
      "Xingyu Zheng",
      "Xiaoyang Li",
      "Yang Zhang",
      "Shouda Liu",
      "Jie Luo",
      "Xianglong Liu",
      "Michele Magno"
    ]
  },
  "https://proceedings.mlr.press/v235/qin24c.html": {
    "title": "Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qin",
      "Weigao Sun",
      "Dong Li",
      "Xuyang Shen",
      "Weixuan Sun",
      "Yiran Zhong"
    ]
  },
  "https://proceedings.mlr.press/v235/qin24d.html": {
    "title": "Feasible Reachable Policy Iteration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentao Qin",
      "Yujie Yang",
      "Yao Mu",
      "Jie Li",
      "Wenjun Zou",
      "Jingliang Duan",
      "Shengbo Eben Li"
    ]
  },
  "https://proceedings.mlr.press/v235/qiu24a.html": {
    "title": "Efficient PAC Learnability of Dynamical Systems Over Multilayer Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirou Qiu",
      "Abhijin Adiga",
      "Madhav Marathe",
      "S. S. Ravi",
      "Daniel Rosenkrantz",
      "Richard Stearns",
      "Anil Kumar Vullikanti"
    ]
  },
  "https://proceedings.mlr.press/v235/qiu24b.html": {
    "title": "Learning High-Order Relationships of Brain Regions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weikang Qiu",
      "Huangrui Chu",
      "Selena Wang",
      "Haolan Zuo",
      "Xiaoxiao Li",
      "Yize Zhao",
      "Rex Ying"
    ]
  },
  "https://proceedings.mlr.press/v235/qiu24c.html": {
    "title": "To Cool or not to Cool? Temperature Network Meets Large Foundation Models via DRO",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi-Hao Qiu",
      "Siqi Guo",
      "Mao Xu",
      "Tuo Zhao",
      "Lijun Zhang",
      "Tianbao Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/qiu24d.html": {
    "title": "Transferring Knowledge From Large Foundation Models to Small Downstream Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikai Qiu",
      "Boran Han",
      "Danielle C. Maddix",
      "Shuai Zhang",
      "Bernie Wang",
      "Andrew Gordon Wilson"
    ]
  },
  "https://proceedings.mlr.press/v235/qiu24e.html": {
    "title": "Complexity Matters: Feature Learning in the Presence of Spurious Correlations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanwen Qiu",
      "Da Kuang",
      "Surbhi Goel"
    ]
  },
  "https://proceedings.mlr.press/v235/qiu24f.html": {
    "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikai Qiu",
      "Andres Potapczynski",
      "Marc Anton Finzi",
      "Micah Goldblum",
      "Andrew Gordon Wilson"
    ]
  },
  "https://proceedings.mlr.press/v235/qiu24g.html": {
    "title": "Gradient Compressed Sensing: A Query-Efficient Gradient Estimator for High-Dimensional Zeroth-Order Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizhong Qiu",
      "Hanghang Tong"
    ]
  },
  "https://proceedings.mlr.press/v235/qu24a.html": {
    "title": "MolCRAFT: Structure-Based Drug Design in Continuous Parameter Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanru Qu",
      "Keyue Qiu",
      "Yuxuan Song",
      "Jingjing Gong",
      "Jiawei Han",
      "Mingyue Zheng",
      "Hao Zhou",
      "Wei-Ying Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/qu24b.html": {
    "title": "Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Helen Qu",
      "Sang Michael Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/quan24a.html": {
    "title": "Learning Constraints from Offline Demonstrations via Superior Distribution Correction Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guorui Quan",
      "Zhiqiang Xu",
      "Guiliang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/quang24a.html": {
    "title": "Augmenting Decision with Hypothesis in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nguyen Minh Quang",
      "Hady W. Lauw"
    ]
  },
  "https://proceedings.mlr.press/v235/quintas-martinez24a.html": {
    "title": "Multiply-Robust Causal Change Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Quintas-Martinez",
      "Mohammad Taha Bahadori",
      "Eduardo Santiago",
      "Jeff Mu",
      "David Heckerman"
    ]
  },
  "https://proceedings.mlr.press/v235/rafiey24a.html": {
    "title": "Decomposable Submodular Maximization in Federated Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akbar Rafiey"
    ]
  },
  "https://proceedings.mlr.press/v235/raghvendra24a.html": {
    "title": "A New Robust Partial p-Wasserstein-Based Metric for Comparing Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharath Raghvendra",
      "Pouyan Shirzadian",
      "Kaiyi Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/rahman24a.html": {
    "title": "Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Musfiqur Rahman",
      "Murat Kocaoglu"
    ]
  },
  "https://proceedings.mlr.press/v235/rahmani24a.html": {
    "title": "Fundamental Limits of Distributed Covariance Matrix Estimation Under Communication Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Reza Rahmani",
      "Mohammad Hossein Yassaee",
      "Mohammad Ali Maddah-Ali",
      "Mohammad Reza Aref"
    ]
  },
  "https://proceedings.mlr.press/v235/raisa24a.html": {
    "title": "Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ossi Räisä",
      "Joonas Jälkö",
      "Antti Honkela"
    ]
  },
  "https://proceedings.mlr.press/v235/rakotoarison24a.html": {
    "title": "In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Herilalaina Rakotoarison",
      "Steven Adriaensen",
      "Neeratyoy Mallik",
      "Samir Garibov",
      "Eddie Bergman",
      "Frank Hutter"
    ]
  },
  "https://proceedings.mlr.press/v235/raman24a.html": {
    "title": "Understanding Inter-Concept Relationships in Concept-Based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naveen Janaki Raman",
      "Mateo Espinosa Zarlenga",
      "Mateja Jamnik"
    ]
  },
  "https://proceedings.mlr.press/v235/raman24b.html": {
    "title": "STEER: Assessing the Economic Rationality of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Narun Krishnamurthi Raman",
      "Taylor Lundy",
      "Samuel Joseph Amouyal",
      "Yoav Levine",
      "Kevin Leyton-Brown",
      "Moshe Tennenholtz"
    ]
  },
  "https://proceedings.mlr.press/v235/rame24a.html": {
    "title": "WARM: On the Benefits of Weight Averaged Reward Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Rame",
      "Nino Vieillard",
      "Leonard Hussenot",
      "Robert Dadashi-Tazehozi",
      "Geoffrey Cideron",
      "Olivier Bachem",
      "Johan Ferret"
    ]
  },
  "https://proceedings.mlr.press/v235/ramesh24a.html": {
    "title": "Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Ramesh",
      "Ekdeep Singh Lubana",
      "Mikail Khona",
      "Robert P. Dick",
      "Hidenori Tanaka"
    ]
  },
  "https://proceedings.mlr.press/v235/ramesh24b.html": {
    "title": "Sequence Compression Speeds Up Credit Assignment in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Ramesh",
      "Kenny John Young",
      "Louis Kirsch",
      "Jürgen Schmidhuber"
    ]
  },
  "https://proceedings.mlr.press/v235/rane24a.html": {
    "title": "Position: The Reasonable Person Standard for AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunayana Rane"
    ]
  },
  "https://proceedings.mlr.press/v235/raparthy24a.html": {
    "title": "Generalization to New Sequential Decision Making Tasks with In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharath Chandra Raparthy",
      "Eric Hambro",
      "Robert Kirk",
      "Mikael Henaff",
      "Roberta Raileanu"
    ]
  },
  "https://proceedings.mlr.press/v235/rathore24a.html": {
    "title": "Challenges in Training PINNs: A Loss Landscape Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratik Rathore",
      "Weimu Lei",
      "Zachary Frangella",
      "Lu Lu",
      "Madeleine Udell"
    ]
  },
  "https://proceedings.mlr.press/v235/ravikumar24a.html": {
    "title": "Unveiling Privacy, Memorization, and Input Curvature Links",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepak Ravikumar",
      "Efstathia Soufleri",
      "Abolfazl Hashemi",
      "Kaushik Roy"
    ]
  },
  "https://proceedings.mlr.press/v235/rawal24a.html": {
    "title": "Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ishaan Singh Rawal",
      "Alexander Matyasko",
      "Shantanu Jaiswal",
      "Basura Fernando",
      "Cheston Tan"
    ]
  },
  "https://proceedings.mlr.press/v235/ray-chaudhury24a.html": {
    "title": "Fair Federated Learning via the Proportional Veto Core",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhaskar Ray Chaudhury",
      "Aniket Murhekar",
      "Zhuowen Yuan",
      "Bo Li",
      "Ruta Mehta",
      "Ariel D. Procaccia"
    ]
  },
  "https://proceedings.mlr.press/v235/ray-chowdhury24a.html": {
    "title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayak Ray Chowdhury",
      "Anush Kini",
      "Nagarajan Natarajan"
    ]
  },
  "https://proceedings.mlr.press/v235/razin24a.html": {
    "title": "Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noam Razin",
      "Yotam Alexander",
      "Edo Cohen-Karlik",
      "Raja Giryes",
      "Amir Globerson",
      "Nadav Cohen"
    ]
  },
  "https://proceedings.mlr.press/v235/reifenstein24a.html": {
    "title": "Dynamic Anisotropic Smoothing for Noisy Derivative-Free Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sam Reifenstein",
      "Timothee Leleu",
      "Yoshihisa Yamamoto"
    ]
  },
  "https://proceedings.mlr.press/v235/reizinger24a.html": {
    "title": "Position: Understanding LLMs Requires More Than Statistical Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrik Reizinger",
      "Szilvia Ujváry",
      "Anna Mészáros",
      "Anna Kerekes",
      "Wieland Brendel",
      "Ferenc Huszár"
    ]
  },
  "https://proceedings.mlr.press/v235/ren24a.html": {
    "title": "Optimal Batched Linear Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanfei Ren",
      "Tianyuan Jin",
      "Pan Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/ren24b.html": {
    "title": "TabLog: Test-Time Adaptation for Tabular Data Using Logic Rules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijieying Ren",
      "Xiaoting Li",
      "Huiyuan Chen",
      "Vineeth Rakesh",
      "Zhuoyi Wang",
      "Mahashweta Das",
      "Vasant G Honavar"
    ]
  },
  "https://proceedings.mlr.press/v235/ren24c.html": {
    "title": "Hybrid Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntao Ren",
      "Gokul Swamy",
      "Steven Wu",
      "Drew Bagnell",
      "Sanjiban Choudhury"
    ]
  },
  "https://proceedings.mlr.press/v235/ren24d.html": {
    "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sucheng Ren",
      "Zeyu Wang",
      "Hongru Zhu",
      "Junfei Xiao",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/ren24e.html": {
    "title": "CarbonNovo: Joint Design of Protein Structure and Sequence Using a Unified Energy-based Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milong Ren",
      "Tian Zhu",
      "Haicang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/renaud24a.html": {
    "title": "Plug-and-Play image restoration with Stochastic deNOising REgularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marien Renaud",
      "Jean Prost",
      "Arthur Leclaire",
      "Nicolas Papadakis"
    ]
  },
  "https://proceedings.mlr.press/v235/reshef24a.html": {
    "title": "Private and Federated Stochastic Convex Optimization: Efficient Strategies for Centralized Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roie Reshef",
      "Kfir Yehuda Levy"
    ]
  },
  "https://proceedings.mlr.press/v235/reuel24a.html": {
    "title": "Position: Technical Research and Talent is Needed for Effective AI Governance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anka Reuel",
      "Lisa Soder",
      "Benjamin Bucknall",
      "Trond Arne Undheim"
    ]
  },
  "https://proceedings.mlr.press/v235/ribar24a.html": {
    "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luka Ribar",
      "Ivan Chelombiev",
      "Luke Hudlass-Galley",
      "Charlie Blake",
      "Carlo Luschi",
      "Douglas Orr"
    ]
  },
  "https://proceedings.mlr.press/v235/ringel24a.html": {
    "title": "Early Time Classification with Accumulated Accuracy Gap Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liran Ringel",
      "Regev Cohen",
      "Daniel Freedman",
      "Michael Elad",
      "Yaniv Romano"
    ]
  },
  "https://proceedings.mlr.press/v235/robertson24a.html": {
    "title": "Implicit Regularization in Feedback Alignment Learning Mechanisms for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Robertson",
      "Sanmi Koyejo"
    ]
  },
  "https://proceedings.mlr.press/v235/rodomanov24a.html": {
    "title": "Universal Gradient Methods for Stochastic Convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Rodomanov",
      "Ali Kavis",
      "Yongtao Wu",
      "Kimon Antonakopoulos",
      "Volkan Cevher"
    ]
  },
  "https://proceedings.mlr.press/v235/rogers24a.html": {
    "title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Rogers",
      "Sasha Luccioni"
    ]
  },
  "https://proceedings.mlr.press/v235/roh24a.html": {
    "title": "LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuji Roh",
      "Qingyun Liu",
      "Huan Gui",
      "Zhe Yuan",
      "Yujin Tang",
      "Steven Euijong Whang",
      "Liang Liu",
      "Shuchao Bi",
      "Lichan Hong",
      "Ed H. Chi",
      "Zhe Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/rolf24a.html": {
    "title": "Position: Mission Critical – Satellite Data is a Distinct Modality in Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esther Rolf",
      "Konstantin Klemmer",
      "Caleb Robinson",
      "Hannah Kerner"
    ]
  },
  "https://proceedings.mlr.press/v235/rolnick24a.html": {
    "title": "Position: Application-Driven Innovation in Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Rolnick",
      "Alan Aspuru-Guzik",
      "Sara Beery",
      "Bistra Dilkina",
      "Priya L. Donti",
      "Marzyeh Ghassemi",
      "Hannah Kerner",
      "Claire Monteleoni",
      "Esther Rolf",
      "Milind Tambe",
      "Adam White"
    ]
  },
  "https://proceedings.mlr.press/v235/rosenfeld24a.html": {
    "title": "One-Shot Strategic Classification Under Unknown Costs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elan Rosenfeld",
      "Nir Rosenfeld"
    ]
  },
  "https://proceedings.mlr.press/v235/ruaud24a.html": {
    "title": "Modelling Microbial Communities with Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albane Ruaud",
      "Cansu Sancaktar",
      "Marco Bagatella",
      "Christoph Ratzke",
      "Georg Martius"
    ]
  },
  "https://proceedings.mlr.press/v235/rudikov24a.html": {
    "title": "Neural operators meet conjugate gradients: The FCG-NO method for efficient PDE solving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Rudikov",
      "Vladimir Fanaskov",
      "Ekaterina Muravleva",
      "Yuri M. Laevsky",
      "Ivan Oseledets"
    ]
  },
  "https://proceedings.mlr.press/v235/rudin24a.html": {
    "title": "Position: Amazing Things Come From Having Many Good Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cynthia Rudin",
      "Chudi Zhong",
      "Lesia Semenova",
      "Margo Seltzer",
      "Ronald Parr",
      "Jiachang Liu",
      "Srikar Katta",
      "Jon Donnelly",
      "Harry Chen",
      "Zachery Boner"
    ]
  },
  "https://proceedings.mlr.press/v235/rugamer24a.html": {
    "title": "Generalizing Orthogonalization for Models with Non-Linearities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Rügamer",
      "Chris Kolb",
      "Tobias Weber",
      "Lucas Kook",
      "Thomas Nagler"
    ]
  },
  "https://proceedings.mlr.press/v235/ruhe24a.html": {
    "title": "Rolling Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Ruhe",
      "Jonathan Heek",
      "Tim Salimans",
      "Emiel Hoogeboom"
    ]
  },
  "https://proceedings.mlr.press/v235/rushing24a.html": {
    "title": "Explorations of Self-Repair in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cody Rushing",
      "Neel Nanda"
    ]
  },
  "https://proceedings.mlr.press/v235/ryu24a.html": {
    "title": "Gambling-Based Confidence Sequences for Bounded Random Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongha Jon Ryu",
      "Gregory W. Wornell"
    ]
  },
  "https://proceedings.mlr.press/v235/ryu24b.html": {
    "title": "Operator SVD with Neural Networks via Nested Low-Rank Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongha Jon Ryu",
      "Xiangxiang Xu",
      "Hasan Sabri Melihcan Erol",
      "Yuheng Bu",
      "Lizhong Zheng",
      "Gregory W. Wornell"
    ]
  },
  "https://proceedings.mlr.press/v235/s24a.html": {
    "title": "Tandem Transformers for Inference Efficient LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishwarya P S",
      "Pranav Ajit Nair",
      "Yashas Samaga B L",
      "Toby James Boyd",
      "Sanjiv Kumar",
      "Prateek Jain",
      "Praneeth Netrapalli"
    ]
  },
  "https://proceedings.mlr.press/v235/saad-falcon24a.html": {
    "title": "Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jon Saad-Falcon",
      "Daniel Y Fu",
      "Simran Arora",
      "Neel Guha",
      "Christopher Re"
    ]
  },
  "https://proceedings.mlr.press/v235/sabour24a.html": {
    "title": "Align Your Steps: Optimizing Sampling Schedules in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirmojtaba Sabour",
      "Sanja Fidler",
      "Karsten Kreis"
    ]
  },
  "https://proceedings.mlr.press/v235/sadasivan24a.html": {
    "title": "Fast Adversarial Attacks on Language Models In One GPU Minute",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinu Sankar Sadasivan",
      "Shoumik Saha",
      "Gaurang Sriramanan",
      "Priyatham Kattakinda",
      "Atoosa Chegini",
      "Soheil Feizi"
    ]
  },
  "https://proceedings.mlr.press/v235/sagar24a.html": {
    "title": "Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Som Sagar",
      "Aditya Taparia",
      "Ransalu Senanayake"
    ]
  },
  "https://proceedings.mlr.press/v235/saha24a.html": {
    "title": "I/O Complexity of Attention, or How Optimal is FlashAttention?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barna Saha",
      "Christopher Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/sahinoglu24a.html": {
    "title": "An Online Optimization Perspective on First-Order and Zero-Order Decentralized Nonsmooth Nonconvex Stochastic Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emre Sahinoglu",
      "Shahin Shahrampour"
    ]
  },
  "https://proceedings.mlr.press/v235/sale24a.html": {
    "title": "Second-Order Uncertainty Quantification: A Distance-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuf Sale",
      "Viktor Bengs",
      "Michele Caprio",
      "Eyke Hüllermeier"
    ]
  },
  "https://proceedings.mlr.press/v235/saleh24a.html": {
    "title": "Learning from Integral Losses in Physics Informed Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ehsan Saleh",
      "Saba Ghaffari",
      "Tim Bretl",
      "Luke Olson",
      "Matthew West"
    ]
  },
  "https://proceedings.mlr.press/v235/salgia24a.html": {
    "title": "Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudeep Salgia",
      "Sattar Vakili",
      "Qing Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/salvatori24a.html": {
    "title": "Predictive Coding beyond Correlations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tommaso Salvatori",
      "Luca Pinchetti",
      "Amine M’Charrak",
      "Beren Millidge",
      "Thomas Lukasiewicz"
    ]
  },
  "https://proceedings.mlr.press/v235/san-roman24a.html": {
    "title": "Proactive Detection of Voice Cloning with Localized Watermarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin San Roman",
      "Pierre Fernandez",
      "Hady Elsahar",
      "Alexandre Défossez",
      "Teddy Furon",
      "Tuan Tran"
    ]
  },
  "https://proceedings.mlr.press/v235/sanchez24a.html": {
    "title": "Stay on Topic with Classifier-Free Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Sanchez",
      "Alexander Spangher",
      "Honglu Fan",
      "Elad Levi",
      "Stella Biderman"
    ]
  },
  "https://proceedings.mlr.press/v235/sander24a.html": {
    "title": "How do Transformers Perform In-Context Autoregressive Learning ?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Eli Sander",
      "Raja Giryes",
      "Taiji Suzuki",
      "Mathieu Blondel",
      "Gabriel Peyré"
    ]
  },
  "https://proceedings.mlr.press/v235/sander24b.html": {
    "title": "Differentially Private Representation Learning via Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Sander",
      "Yaodong Yu",
      "Maziar Sanjabi",
      "Alain Oliviero Durmus",
      "Yi Ma",
      "Kamalika Chaudhuri",
      "Chuan Guo"
    ]
  },
  "https://proceedings.mlr.press/v235/sanford24a.html": {
    "title": "Transformers, parallel computation, and logarithmic depth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clayton Sanford",
      "Daniel Hsu",
      "Matus Telgarsky"
    ]
  },
  "https://proceedings.mlr.press/v235/sankararaman24a.html": {
    "title": "Promoting External and Internal Equities Under Ex-Ante/Ex-Post Metrics in Online Resource Allocation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Abinav Sankararaman",
      "Aravind Srinivasan",
      "Pan Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/sanokowski24a.html": {
    "title": "A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Sanokowski",
      "Sepp Hochreiter",
      "Sebastian Lehner"
    ]
  },
  "https://proceedings.mlr.press/v235/santos24a.html": {
    "title": "Sparse and Structured Hopfield Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saul José Rodrigues Dos Santos",
      "Vlad Niculae",
      "Daniel C Mcnamee",
      "Andre Martins"
    ]
  },
  "https://proceedings.mlr.press/v235/sapkota24a.html": {
    "title": "Meta Evidential Transformer for Few-Shot Open-Set Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hitesh Sapkota",
      "Krishna Prasad Neupane",
      "Qi Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/sapora24a.html": {
    "title": "EvIL: Evolution Strategies for Generalisable Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silvia Sapora",
      "Gokul Swamy",
      "Chris Lu",
      "Yee Whye Teh",
      "Jakob Nicolaus Foerster"
    ]
  },
  "https://proceedings.mlr.press/v235/saratchandran24a.html": {
    "title": "A sampling theory perspective on activations for implicit neural representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hemanth Saratchandran",
      "Sameera Ramasinghe",
      "Violetta Shevchenko",
      "Alexander Long",
      "Simon Lucey"
    ]
  },
  "https://proceedings.mlr.press/v235/sardana24a.html": {
    "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Sardana",
      "Jacob Portes",
      "Sasha Doubov",
      "Jonathan Frankle"
    ]
  },
  "https://proceedings.mlr.press/v235/sarfraz24a.html": {
    "title": "Position: Quo Vadis, Unsupervised Time Series Anomaly Detection?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "M. Saquib Sarfraz",
      "Mei-Yen Chen",
      "Lukas Layer",
      "Kunyu Peng",
      "Marios Koulakis"
    ]
  },
  "https://proceedings.mlr.press/v235/scellier24a.html": {
    "title": "A fast algorithm to simulate nonlinear resistive networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Scellier"
    ]
  },
  "https://proceedings.mlr.press/v235/scetbon24a.html": {
    "title": "A Fixed-Point Approach for Causal Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meyer Scetbon",
      "Joel Jennings",
      "Agrin Hilmkil",
      "Cheng Zhang",
      "Chao Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/schaipp24a.html": {
    "title": "MoMo: Momentum Models for Adaptive Learning Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Schaipp",
      "Ruben Ohana",
      "Michael Eickenberg",
      "Aaron Defazio",
      "Robert M. Gower"
    ]
  },
  "https://proceedings.mlr.press/v235/schar24a.html": {
    "title": "Parallel Affine Transformation Tuning of Markov Chain Monte Carlo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Schär",
      "Michael Habeck",
      "Daniel Rudolf"
    ]
  },
  "https://proceedings.mlr.press/v235/scheid24a.html": {
    "title": "Incentivized Learning in Principal-Agent Bandit Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Scheid",
      "Daniil Tiapkin",
      "Etienne Boursier",
      "Aymeric Capitaine",
      "Eric Moulines",
      "Michael Jordan",
      "El-Mahdi El-Mhamdi",
      "Alain Oliviero Durmus"
    ]
  },
  "https://proceedings.mlr.press/v235/schiff24a.html": {
    "title": "Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yair Schiff",
      "Chia Hsiang Kao",
      "Aaron Gokaslan",
      "Tri Dao",
      "Albert Gu",
      "Volodymyr Kuleshov"
    ]
  },
  "https://proceedings.mlr.press/v235/schiff24b.html": {
    "title": "DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yair Schiff",
      "Zhong Yi Wan",
      "Jeffrey B. Parker",
      "Stephan Hoyer",
      "Volodymyr Kuleshov",
      "Fei Sha",
      "Leonardo Zepeda-Núñez"
    ]
  },
  "https://proceedings.mlr.press/v235/schlarmann24a.html": {
    "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Schlarmann",
      "Naman Deep Singh",
      "Francesco Croce",
      "Matthias Hein"
    ]
  },
  "https://proceedings.mlr.press/v235/schmidt24a.html": {
    "title": "Tilt your Head: Activating the Hidden Spatial-Invariance of Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johann Schmidt",
      "Sebastian Stober"
    ]
  },
  "https://proceedings.mlr.press/v235/schmitt24a.html": {
    "title": "Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marvin Schmitt",
      "Desi R. Ivanova",
      "Daniel Habermann",
      "Ullrich Koethe",
      "Paul-Christian Bürkner",
      "Stefan T. Radev"
    ]
  },
  "https://proceedings.mlr.press/v235/schmitt-forster24a.html": {
    "title": "Regularized Q-learning through Robust Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Schmitt-Förster",
      "Tobias Sutter"
    ]
  },
  "https://proceedings.mlr.press/v235/schneider24a.html": {
    "title": "Implicit Representations for Constrained Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Philipp Schneider",
      "Mishal Fatima",
      "Jovita Lukasik",
      "Andreas Kolb",
      "Margret Keuper",
      "Michael Moeller"
    ]
  },
  "https://proceedings.mlr.press/v235/schneider24b.html": {
    "title": "Online Learning with Bounded Recall",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jon Schneider",
      "Kiran Vodrahalli"
    ]
  },
  "https://proceedings.mlr.press/v235/schopmans24a.html": {
    "title": "Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henrik Schopmans",
      "Pascal Friederich"
    ]
  },
  "https://proceedings.mlr.press/v235/schramm24a.html": {
    "title": "Provably Efficient Long-Horizon Exploration in Monte Carlo Tree Search through State Occupancy Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liam Schramm",
      "Abdeslam Boularias"
    ]
  },
  "https://proceedings.mlr.press/v235/schroder24a.html": {
    "title": "Asymptotics of Learning with Deep Structured (Random) Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Schröder",
      "Daniil Dmitriev",
      "Hugo Cui",
      "Bruno Loureiro"
    ]
  },
  "https://proceedings.mlr.press/v235/schroder24b.html": {
    "title": "Simultaneous identification of models and parameters of scientific simulators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cornelius Schröder",
      "Jakob H. Macke"
    ]
  },
  "https://proceedings.mlr.press/v235/schubert24a.html": {
    "title": "In-Context Learning Agents Are Asymmetric Belief Updaters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes A. Schubert",
      "Akshay Kumar Jagadish",
      "Marcel Binz",
      "Eric Schulz"
    ]
  },
  "https://proceedings.mlr.press/v235/schurholt24a.html": {
    "title": "Towards Scalable and Versatile Weight Space Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Schürholt",
      "Michael W. Mahoney",
      "Damian Borth"
    ]
  },
  "https://proceedings.mlr.press/v235/schweisthal24a.html": {
    "title": "Meta-Learners for Partially-Identified Treatment Effects Across Multiple Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Schweisthal",
      "Dennis Frauen",
      "Mihaela Van Der Schaar",
      "Stefan Feuerriegel"
    ]
  },
  "https://proceedings.mlr.press/v235/scoccola24a.html": {
    "title": "Differentiability and Optimization of Multiparameter Persistent Homology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luis Scoccola",
      "Siddharth Setlur",
      "David Loiseaux",
      "Mathieu Carrière",
      "Steve Oudot"
    ]
  },
  "https://proceedings.mlr.press/v235/scott24a.html": {
    "title": "Improved Modelling of Federated Datasets using Mixtures-of-Dirichlet-Multinomials",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Scott",
      "Áine Cahill"
    ]
  },
  "https://proceedings.mlr.press/v235/scotti24a.html": {
    "title": "MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Steven Scotti",
      "Mihir Tripathy",
      "Cesar Torrico",
      "Reese Kneeland",
      "Tong Chen",
      "Ashutosh Narang",
      "Charan Santhirasegaran",
      "Jonathan Xu",
      "Thomas Naselaris",
      "Kenneth A. Norman",
      "Tanishq Mathew Abraham"
    ]
  },
  "https://proceedings.mlr.press/v235/seedat24a.html": {
    "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in low-data regimes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nabeel Seedat",
      "Nicolas Huynh",
      "Boris van Breugel",
      "Mihaela van der Schaar"
    ]
  },
  "https://proceedings.mlr.press/v235/sefidgaran24a.html": {
    "title": "Lessons from Generalization Error Analysis of Federated Learning: You May Communicate Less Often!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milad Sefidgaran",
      "Romain Chor",
      "Abdellatif Zaidi",
      "Yijun Wan"
    ]
  },
  "https://proceedings.mlr.press/v235/sel24a.html": {
    "title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bilgehan Sel",
      "Ahmad Tawaha",
      "Vanshaj Khattar",
      "Ruoxi Jia",
      "Ming Jin"
    ]
  },
  "https://proceedings.mlr.press/v235/seo24a.html": {
    "title": "Retrieval-Augmented Score Distillation for Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyoung Seo",
      "Susung Hong",
      "Wooseok Jang",
      "Inès Hyeonsu Kim",
      "Min-Seop Kwak",
      "Doyup Lee",
      "Seungryong Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/seong24a.html": {
    "title": "Self-Supervised Interpretable End-to-End Learning via Latent Functional Modularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunki Seong",
      "Hyunchul Shim"
    ]
  },
  "https://proceedings.mlr.press/v235/setlur24a.html": {
    "title": "Prompting is a Double-Edged Sword: Improving Worst-Group Robustness of Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrith Setlur",
      "Saurabh Garg",
      "Virginia Smith",
      "Sergey Levine"
    ]
  },
  "https://proceedings.mlr.press/v235/shah24a.html": {
    "title": "Decomposing and Editing Predictions by Modeling Model Computation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harshay Shah",
      "Andrew Ilyas",
      "Aleksander Madry"
    ]
  },
  "https://proceedings.mlr.press/v235/shaham24a.html": {
    "title": "A Multimodal Automated Interpretability Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tamar Rott Shaham",
      "Sarah Schwettmann",
      "Franklin Wang",
      "Achyuta Rajaram",
      "Evan Hernandez",
      "Jacob Andreas",
      "Antonio Torralba"
    ]
  },
  "https://proceedings.mlr.press/v235/shahroudi24a.html": {
    "title": "Evaluation of Trajectory Distribution Predictions with Energy Score",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Novin Shahroudi",
      "Mihkel Lepson",
      "Meelis Kull"
    ]
  },
  "https://proceedings.mlr.press/v235/shalam24a.html": {
    "title": "The Balanced-Pairwise-Affinities Feature Transform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Shalam",
      "Simon Korman"
    ]
  },
  "https://proceedings.mlr.press/v235/shamshoum24a.html": {
    "title": "DNCs Require More Planning Steps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yara Shamshoum",
      "Nitzan Hodos",
      "Yuval Sieradzki",
      "Assaf Schuster"
    ]
  },
  "https://proceedings.mlr.press/v235/shamsian24a.html": {
    "title": "Improved Generalization of Weight Space Networks via Augmentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aviv Shamsian",
      "Aviv Navon",
      "David W. Zhang",
      "Yan Zhang",
      "Ethan Fetaya",
      "Gal Chechik",
      "Haggai Maron"
    ]
  },
  "https://proceedings.mlr.press/v235/shankar24a.html": {
    "title": "On Online Experimentation without Device Identifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiv Shankar",
      "Ritwik Sinha",
      "Madalina Fiterau"
    ]
  },
  "https://proceedings.mlr.press/v235/shao24a.html": {
    "title": "Improved Dimensionality Dependence for Zeroth-Order Optimisation over Cross-Polytopes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Shao"
    ]
  },
  "https://proceedings.mlr.press/v235/shao24b.html": {
    "title": "On Multi-Armed Bandit with Impatient Arms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuming Shao",
      "Zhixuan Fang"
    ]
  },
  "https://proceedings.mlr.press/v235/shao24c.html": {
    "title": "Language Generation with Strictly Proper Scoring Rules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenze Shao",
      "Fandong Meng",
      "Yijin Liu",
      "Jie Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/shao24d.html": {
    "title": "Learning Decision Policies with Instrumental Variables through Double Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daqian Shao",
      "Ashkan Soleymani",
      "Francesco Quinzan",
      "Marta Kwiatkowska"
    ]
  },
  "https://proceedings.mlr.press/v235/sharma24a.html": {
    "title": "How Far Can Fairness Constraints Help Recover From Biased Data?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohit Sharma",
      "Amit Deshpande"
    ]
  },
  "https://proceedings.mlr.press/v235/sharma24b.html": {
    "title": "Diffuse, Sample, Project: Plug-And-Play Controllable Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Sharma",
      "Srijan Kumar",
      "Rakshit Trivedi"
    ]
  },
  "https://proceedings.mlr.press/v235/sharrock24a.html": {
    "title": "Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Sharrock",
      "Jack Simons",
      "Song Liu",
      "Mark Beaumont"
    ]
  },
  "https://proceedings.mlr.press/v235/shaul24a.html": {
    "title": "Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neta Shaul",
      "Uriel Singer",
      "Ricky T. Q. Chen",
      "Matthew Le",
      "Ali Thabet",
      "Albert Pumarola",
      "Yaron Lipman"
    ]
  },
  "https://proceedings.mlr.press/v235/shekhar24a.html": {
    "title": "Reducing sequential change detection to sequential estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhanshu Shekhar",
      "Aaditya Ramdas"
    ]
  },
  "https://proceedings.mlr.press/v235/shen24a.html": {
    "title": "Exploring the Complexity of Deep Neural Networks through Functional Equivalence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guohao Shen"
    ]
  },
  "https://proceedings.mlr.press/v235/shen24b.html": {
    "title": "Variational Learning is Effective for Large Deep Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuesong Shen",
      "Nico Daheim",
      "Bai Cong",
      "Peter Nickl",
      "Gian Maria Marconi",
      "Bazan Clement Emile Marcel Raoul",
      "Rio Yokota",
      "Iryna Gurevych",
      "Daniel Cremers",
      "Mohammad Emtiyaz Khan",
      "Thomas Möllenhoff"
    ]
  },
  "https://proceedings.mlr.press/v235/shen24c.html": {
    "title": "Thermometer: Towards Universal Calibration for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maohao Shen",
      "Subhro Das",
      "Kristjan Greenewald",
      "Prasanna Sattigeri",
      "Gregory W. Wornell",
      "Soumya Ghosh"
    ]
  },
  "https://proceedings.mlr.press/v235/shen24d.html": {
    "title": "Position: Do pretrained Transformers Learn In-Context by Gradient Descent?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingfeng Shen",
      "Aayush Mishra",
      "Daniel Khashabi"
    ]
  },
  "https://proceedings.mlr.press/v235/shen24e.html": {
    "title": "Adaptive Stabilization Based on Machine Learning for Column Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhuang Shen",
      "Yuan Sun",
      "Xiaodong Li",
      "Zhiguang Cao",
      "Andrew Eberhard",
      "Guangquan Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/shen24f.html": {
    "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhong Shen",
      "Neil Tenenholtz",
      "James Brian Hall",
      "David Alvarez-Melis",
      "Nicolo Fusi"
    ]
  },
  "https://proceedings.mlr.press/v235/shen24g.html": {
    "title": "Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Shen",
      "Zhuoran Yang",
      "Tianyi Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/shenouda24a.html": {
    "title": "ReLUs Are Sufficient for Learning Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Shenouda",
      "Yamin Zhou",
      "Robert D Nowak"
    ]
  },
  "https://proceedings.mlr.press/v235/sherman24a.html": {
    "title": "Rate-Optimal Policy Optimization for Linear Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uri Sherman",
      "Alon Cohen",
      "Tomer Koren",
      "Yishay Mansour"
    ]
  },
  "https://proceedings.mlr.press/v235/shi24a.html": {
    "title": "Double Momentum Method for Lower-Level Constrained Bilevel Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanli Shi",
      "Yi Chang",
      "Bin Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/shi24b.html": {
    "title": "OT-CLIP: Understanding and Generalizing CLIP via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangliang Shi",
      "Jack Fan",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/shi24c.html": {
    "title": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Shi",
      "Gautam Rajendrakumar Gare",
      "Jinjin Tian",
      "Siqi Chai",
      "Zhiqiu Lin",
      "Arun Balajee Vasudevan",
      "Di Feng",
      "Francesco Ferroni",
      "Shu Kong"
    ]
  },
  "https://proceedings.mlr.press/v235/shi24d.html": {
    "title": "Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laixi Shi",
      "Eric Mazumdar",
      "Yuejie Chi",
      "Adam Wierman"
    ]
  },
  "https://proceedings.mlr.press/v235/shi24e.html": {
    "title": "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dachuan Shi",
      "Chaofan Tao",
      "Anyi Rao",
      "Zhendong Yang",
      "Chun Yuan",
      "Jiaqi Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/shi24f.html": {
    "title": "Why Larger Language Models Do In-context Learning Differently?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenmei Shi",
      "Junyi Wei",
      "Zhuoyan Xu",
      "Yingyu Liang"
    ]
  },
  "https://proceedings.mlr.press/v235/shi24g.html": {
    "title": "Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang-Xin Shi",
      "Tong Wei",
      "Zhi Zhou",
      "Jie-Jing Shao",
      "Xin-Yan Han",
      "Yu-Feng Li"
    ]
  },
  "https://proceedings.mlr.press/v235/shimizu24a.html": {
    "title": "Neural-Kernel Conditional Mean Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eiki Shimizu",
      "Kenji Fukumizu",
      "Dino Sejdinovic"
    ]
  },
  "https://proceedings.mlr.press/v235/shiragur24a.html": {
    "title": "Causal Discovery with Fewer Conditional Independence Tests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirankumar Shiragur",
      "Jiaqi Zhang",
      "Caroline Uhler"
    ]
  },
  "https://proceedings.mlr.press/v235/shiraishi24a.html": {
    "title": "Statistical Test for Attention Maps in Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomohiro Shiraishi",
      "Daiki Miwa",
      "Teruyuki Katsuoka",
      "Vo Nguyen Le Duy",
      "Kouichi Taji",
      "Ichiro Takeuchi"
    ]
  },
  "https://proceedings.mlr.press/v235/shirakawa24a.html": {
    "title": "Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toru Shirakawa",
      "Yi Li",
      "Yulun Wu",
      "Sky Qiu",
      "Yuxuan Li",
      "Mingduo Zhao",
      "Hiroyasu Iso",
      "Mark J. Van Der Laan"
    ]
  },
  "https://proceedings.mlr.press/v235/shirali24a.html": {
    "title": "Allocation Requires Prediction Only if Inequality Is Low",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Shirali",
      "Rediet Abebe",
      "Moritz Hardt"
    ]
  },
  "https://proceedings.mlr.press/v235/shoushtari24a.html": {
    "title": "Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shirin Shoushtari",
      "Jiaming Liu",
      "Edward P. Chandler",
      "M. Salman Asif",
      "Ulugbek S. Kamilov"
    ]
  },
  "https://proceedings.mlr.press/v235/shu24a.html": {
    "title": "Effects of Exponential Gaussian Distribution on (Double Sampling) Randomized Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youwei Shu",
      "Xi Xiao",
      "Derui Wang",
      "Yuxin Cao",
      "Siji Chen",
      "Jason Xue",
      "Linyi Li",
      "Bo Li"
    ]
  },
  "https://proceedings.mlr.press/v235/shubham24a.html": {
    "title": "WISER: Weak Supervision and Supervised Representation Learning to Improve Drug Response Prediction in Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumar Shubham",
      "Aishwarya Jayagopal",
      "Syed Mohammed Danish",
      "Prathosh Ap",
      "Vaibhav Rajan"
    ]
  },
  "https://proceedings.mlr.press/v235/shukla24a.html": {
    "title": "TIC-TAC: A Framework For Improved Covariance Estimation In Deep Heteroscedastic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Megh Shukla",
      "Mathieu Salzmann",
      "Alexandre Alahi"
    ]
  },
  "https://proceedings.mlr.press/v235/shulgin24a.html": {
    "title": "Towards a Better Theoretical Understanding of Independent Subnetwork Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Egor Shulgin",
      "Peter Richtárik"
    ]
  },
  "https://proceedings.mlr.press/v235/shumaylov24a.html": {
    "title": "Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zakhar Shumaylov",
      "Jeremy Budd",
      "Subhadip Mukherjee",
      "Carola-Bibiane Schönlieb"
    ]
  },
  "https://proceedings.mlr.press/v235/shumilin24a.html": {
    "title": "Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergei Shumilin",
      "Alexander Ryabov",
      "Nikolay Yavich",
      "Evgeny Burnaev",
      "Vladimir Vanovskiy"
    ]
  },
  "https://proceedings.mlr.press/v235/shumitskaya24a.html": {
    "title": "IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image- and Video-Quality Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekaterina Shumitskaya",
      "Anastasia Antsiferova",
      "Dmitriy S. Vatolin"
    ]
  },
  "https://proceedings.mlr.press/v235/si24a.html": {
    "title": "InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Yoke Hong Si",
      "Wendy Yusi Cheng",
      "Michael Cooper",
      "Rahul Krishnan"
    ]
  },
  "https://proceedings.mlr.press/v235/silva24a.html": {
    "title": "Embarrassingly Parallel GFlowNets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiago Silva",
      "Luiz Max Carvalho",
      "Amauri H Souza",
      "Samuel Kaski",
      "Diego Mesquita"
    ]
  },
  "https://proceedings.mlr.press/v235/silva24b.html": {
    "title": "On the Unexpected Effectiveness of Reinforcement Learning for Sequential Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Álvaro Labarca Silva",
      "Denis Parra",
      "Rodrigo Toro Icarte"
    ]
  },
  "https://proceedings.mlr.press/v235/silva24c.html": {
    "title": "Learning from Memory: Non-Parametric Memory Augmented Self-Supervised Learning of Visual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thalles Silva",
      "Helio Pedrini",
      "Adı́n Ramı́rez Rivera"
    ]
  },
  "https://proceedings.mlr.press/v235/sim24a.html": {
    "title": "Deletion-Anticipative Data Selection with a Limited Budget",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rachael Hwee Ling Sim",
      "Jue Fan",
      "Xiao Tian",
      "Patrick Jaillet",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://proceedings.mlr.press/v235/simmons-edler24a.html": {
    "title": "Position: AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riley Simmons-Edler",
      "Ryan Paul Badman",
      "Shayne Longpre",
      "Kanaka Rajan"
    ]
  },
  "https://proceedings.mlr.press/v235/sinelnikov24a.html": {
    "title": "Latent variable model for high-dimensional point process with structured missingness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksim Sinelnikov",
      "Manuel Haussmann",
      "Harri Lähdesmäki"
    ]
  },
  "https://proceedings.mlr.press/v235/singh24a.html": {
    "title": "Domain Generalisation via Imprecise Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anurag Singh",
      "Siu Lun Chau",
      "Shahine Bouabid",
      "Krikamol Muandet"
    ]
  },
  "https://proceedings.mlr.press/v235/singh24b.html": {
    "title": "Finite Time Logarithmic Regret Bounds for Self-Tuning Regulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Singh",
      "Akshay Mete",
      "Avik Kar",
      "Panganamala Kumar"
    ]
  },
  "https://proceedings.mlr.press/v235/singh24c.html": {
    "title": "What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaditya K Singh",
      "Ted Moskovitz",
      "Felix Hill",
      "Stephanie C.Y. Chan",
      "Andrew M Saxe"
    ]
  },
  "https://proceedings.mlr.press/v235/singh24d.html": {
    "title": "Representation Surgery: Theory and Practice of Affine Steering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashwat Singh",
      "Shauli Ravfogel",
      "Jonathan Herzig",
      "Roee Aharoni",
      "Ryan Cotterell",
      "Ponnurangam Kumaraguru"
    ]
  },
  "https://proceedings.mlr.press/v235/singh24e.html": {
    "title": "PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning via Hindsight Relabeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Utsav Singh",
      "Wesley A Suttle",
      "Brian M. Sadler",
      "Vinay P. Namboodiri",
      "Amrit Bedi"
    ]
  },
  "https://proceedings.mlr.press/v235/singh24f.html": {
    "title": "Byzantine Resilient and Fast Federated Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Pratap Singh",
      "Namrata Vaswani"
    ]
  },
  "https://proceedings.mlr.press/v235/singh24g.html": {
    "title": "Parallelized Spatiotemporal Slot Binding for Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gautam Singh",
      "Yue Wang",
      "Jiawei Yang",
      "Boris Ivanovic",
      "Sungjin Ahn",
      "Marco Pavone",
      "Tong Che"
    ]
  },
  "https://proceedings.mlr.press/v235/singhal24a.html": {
    "title": "What's the score? Automated Denoising Score Matching for Nonlinear Diffusions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghav Singhal",
      "Mark Goldstein",
      "Rajesh Ranganath"
    ]
  },
  "https://proceedings.mlr.press/v235/singla24a.html": {
    "title": "SAPG: Split and Aggregate Policy Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jayesh Singla",
      "Ananye Agarwal",
      "Deepak Pathak"
    ]
  },
  "https://proceedings.mlr.press/v235/sinii24a.html": {
    "title": "In-Context Reinforcement Learning for Variable Action Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viacheslav Sinii",
      "Alexander Nikulin",
      "Vladislav Kurenkov",
      "Ilya Zisman",
      "Sergey Kolesnikov"
    ]
  },
  "https://proceedings.mlr.press/v235/sittoni24a.html": {
    "title": "Subhomogeneous Deep Equilibrium Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pietro Sittoni",
      "Francesco Tudisco"
    ]
  },
  "https://proceedings.mlr.press/v235/sivagnanam24a.html": {
    "title": "Multi-Agent Reinforcement Learning with Hierarchical Coordination for Emergency Responder Stationing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amutheezan Sivagnanam",
      "Ava Pettet",
      "Hunter Lee",
      "Ayan Mukhopadhyay",
      "Abhishek Dubey",
      "Aron Laszka"
    ]
  },
  "https://proceedings.mlr.press/v235/smee24a.html": {
    "title": "Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Smee",
      "Fred Roosta"
    ]
  },
  "https://proceedings.mlr.press/v235/smit24a.html": {
    "title": "Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andries Petrus Smit",
      "Nathan Grinsztajn",
      "Paul Duckworth",
      "Thomas D Barrett",
      "Arnu Pretorius"
    ]
  },
  "https://proceedings.mlr.press/v235/soares24a.html": {
    "title": "Probabilistic Modeling of Interpersonal Coordination Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paulo Soares",
      "Adarsh Pyarelal",
      "Meghavarshini Krishnaswamy",
      "Emily Butler",
      "Kobus Barnard"
    ]
  },
  "https://proceedings.mlr.press/v235/sohrabi24a.html": {
    "title": "On PI Controllers for Updating Lagrange Multipliers in Constrained Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Motahareh Sohrabi",
      "Juan Ramirez",
      "Tianyue H. Zhang",
      "Simon Lacoste-Julien",
      "Jose Gallego-Posada"
    ]
  },
  "https://proceedings.mlr.press/v235/soltani-moakhar24a.html": {
    "title": "SPADE: Sparsity-Guided Debugging for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arshia Soltani Moakhar",
      "Eugenia Iofinova",
      "Elias Frantar",
      "Dan Alistarh"
    ]
  },
  "https://proceedings.mlr.press/v235/sommer24a.html": {
    "title": "Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuel Sommer",
      "Lisa Wimmer",
      "Theodore Papamarkou",
      "Ludwig Bothmann",
      "Bernd Bischl",
      "David Rügamer"
    ]
  },
  "https://proceedings.mlr.press/v235/song24a.html": {
    "title": "Hybrid Reinforcement Learning from Offline Observation Alone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuda Song",
      "Drew Bagnell",
      "Aarti Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/song24b.html": {
    "title": "Multimodal Prototyping for cancer survival prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew H. Song",
      "Richard J. Chen",
      "Guillaume Jaume",
      "Anurag Jayant Vaidya",
      "Alexander Baras",
      "Faisal Mahmood"
    ]
  },
  "https://proceedings.mlr.press/v235/song24c.html": {
    "title": "SurfPro: Functional Protein Design Based on Continuous Surface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenqiao Song",
      "Tinglin Huang",
      "Lei Li",
      "Wengong Jin"
    ]
  },
  "https://proceedings.mlr.press/v235/song24d.html": {
    "title": "OSN: Infinite Representations of Dynamic 3D Scenes from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Song",
      "Jinxi Li",
      "Bo Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/song24e.html": {
    "title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixi Song",
      "Zuchao Li",
      "Lefei Zhang",
      "Hai Zhao",
      "Bo Du"
    ]
  },
  "https://proceedings.mlr.press/v235/song24f.html": {
    "title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwon Song",
      "Kyungseok Oh",
      "Taesu Kim",
      "Hyungjun Kim",
      "Yulhwa Kim",
      "Jae-Joon Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/song24g.html": {
    "title": "Unveiling the Dynamics of Information Interplay in Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Song",
      "Zhiquan Tan",
      "Bochao Zou",
      "Huimin Ma",
      "Weiran Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/song24h.html": {
    "title": "Position: Leverage Foundational Models for Black-Box Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyou Song",
      "Yingtao Tian",
      "Robert Tjarko Lange",
      "Chansoo Lee",
      "Yujin Tang",
      "Yutian Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/song24i.html": {
    "title": "Rich-Observation Reinforcement Learning with Continuous Latent Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuda Song",
      "Lili Wu",
      "Dylan J Foster",
      "Akshay Krishnamurthy"
    ]
  },
  "https://proceedings.mlr.press/v235/song24j.html": {
    "title": "Latent Logic Tree Extraction for Event Sequence Explanation from LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitao Song",
      "Chao Yang",
      "Chaojie Wang",
      "Bo An",
      "Shuang Li"
    ]
  },
  "https://proceedings.mlr.press/v235/song24k.html": {
    "title": "Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenqiao Song",
      "Yunlong Zhao",
      "Wenxian Shi",
      "Wengong Jin",
      "Yang Yang",
      "Lei Li"
    ]
  },
  "https://proceedings.mlr.press/v235/sorensen24a.html": {
    "title": "Position: A Roadmap to Pluralistic Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taylor Sorensen",
      "Jared Moore",
      "Jillian Fisher",
      "Mitchell L Gordon",
      "Niloofar Mireshghallah",
      "Christopher Michael Rytting",
      "Andre Ye",
      "Liwei Jiang",
      "Ximing Lu",
      "Nouha Dziri",
      "Tim Althoff",
      "Yejin Choi"
    ]
  },
  "https://proceedings.mlr.press/v235/spangher24a.html": {
    "title": "Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Spangher",
      "Allen M. Wang",
      "Andrew Maris",
      "Myles Stapelberg",
      "Viraj Mehta",
      "Alex Saperstein",
      "Stephen Lane-Walsh",
      "Akshata Kishore Moharir",
      "Alessandro Pau",
      "Cristina Rea"
    ]
  },
  "https://proceedings.mlr.press/v235/springenberg24a.html": {
    "title": "Offline Actor-Critic Reinforcement Learning Scales to Large Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jost Tobias Springenberg",
      "Abbas Abdolmaleki",
      "Jingwei Zhang",
      "Oliver Groth",
      "Michael Bloesch",
      "Thomas Lampe",
      "Philemon Brakel",
      "Sarah Maria Elisabeth Bechtle",
      "Steven Kapturowski",
      "Roland Hafner",
      "Nicolas Heess",
      "Martin Riedmiller"
    ]
  },
  "https://proceedings.mlr.press/v235/sprueill24a.html": {
    "title": "CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henry W. Sprueill",
      "Carl Edwards",
      "Khushbu Agarwal",
      "Mariefel V Olarte",
      "Udishnu Sanyal",
      "Conrad Johnston",
      "Hongbin Liu",
      "Heng Ji",
      "Sutanay Choudhury"
    ]
  },
  "https://proceedings.mlr.press/v235/sristi24a.html": {
    "title": "Contextual Feature Selection with Conditional Stochastic Gates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ram Dyuthi Sristi",
      "Ofir Lindenbaum",
      "Shira Lifshitz",
      "Maria Lavzin",
      "Jackie Schiller",
      "Gal Mishne",
      "Hadas Benisty"
    ]
  },
  "https://proceedings.mlr.press/v235/stadler24a.html": {
    "title": "The Fundamental Limits of Least-Privilege Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theresa Stadler",
      "Bogdan Kulynych",
      "Michael Gastpar",
      "Nicolas Papernot",
      "Carmela Troncoso"
    ]
  },
  "https://proceedings.mlr.press/v235/stanczuk24a.html": {
    "title": "Diffusion Models Encode the Intrinsic Dimension of Data Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Pawel Stanczuk",
      "Georgios Batzolis",
      "Teo Deveney",
      "Carola-Bibiane Schönlieb"
    ]
  },
  "https://proceedings.mlr.press/v235/stander24a.html": {
    "title": "Grokking Group Multiplication with Cosets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dashiell Stander",
      "Qinan Yu",
      "Honglu Fan",
      "Stella Biderman"
    ]
  },
  "https://proceedings.mlr.press/v235/stark24a.html": {
    "title": "Harmonic Self-Conditioned Flow Matching for joint Multi-Ligand Docking and Binding Site Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hannes Stark",
      "Bowen Jing",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ]
  },
  "https://proceedings.mlr.press/v235/stark24b.html": {
    "title": "Dirichlet Flow Matching with Applications to DNA Sequence Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hannes Stark",
      "Bowen Jing",
      "Chenyu Wang",
      "Gabriele Corso",
      "Bonnie Berger",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ]
  },
  "https://proceedings.mlr.press/v235/stein24a.html": {
    "title": "Partial Optimality in the Linear Ordering Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Stein",
      "Bjoern Andres"
    ]
  },
  "https://proceedings.mlr.press/v235/stein24b.html": {
    "title": "Towards Compositionality in Concept Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Stein",
      "Aaditya Naik",
      "Yinjun Wu",
      "Mayur Naik",
      "Eric Wong"
    ]
  },
  "https://proceedings.mlr.press/v235/steinmann24a.html": {
    "title": "Learning to Intervene on Concept Bottlenecks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Steinmann",
      "Wolfgang Stammer",
      "Felix Friedrich",
      "Kristian Kersting"
    ]
  },
  "https://proceedings.mlr.press/v235/stella24a.html": {
    "title": "QORA: Zero-Shot Transfer via Interpretable Object-Relational Model Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Stella",
      "Dmitri Loguinov"
    ]
  },
  "https://proceedings.mlr.press/v235/stemmer24a.html": {
    "title": "Private Truly-Everlasting Robust-Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uri Stemmer"
    ]
  },
  "https://proceedings.mlr.press/v235/stengel-eskin24a.html": {
    "title": "ReGAL: Refactoring Programs to Discover Generalizable Abstractions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elias Stengel-Eskin",
      "Archiki Prasad",
      "Mohit Bansal"
    ]
  },
  "https://proceedings.mlr.press/v235/stephan24a.html": {
    "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Pascal Stephan",
      "Alexander Khazatsky",
      "Eric Mitchell",
      "Annie S Chen",
      "Sheryl Hsu",
      "Archit Sharma",
      "Chelsea Finn"
    ]
  },
  "https://proceedings.mlr.press/v235/stoica24a.html": {
    "title": "Causal Inference from Competing Treatments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ana-Andreea Stoica",
      "Vivian Yvonne Nastl",
      "Moritz Hardt"
    ]
  },
  "https://proceedings.mlr.press/v235/stradi24a.html": {
    "title": "Online Learning in CMDPs: Handling Stochastic and Adversarial Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Emanuele Stradi",
      "Jacopo Germano",
      "Gianmarco Genalti",
      "Matteo Castiglioni",
      "Alberto Marchesi",
      "Nicola Gatti"
    ]
  },
  "https://proceedings.mlr.press/v235/straitouri24a.html": {
    "title": "Designing Decision Support Systems using Counterfactual Prediction Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleni Straitouri",
      "Manuel Gomez Rodriguez"
    ]
  },
  "https://proceedings.mlr.press/v235/strati24a.html": {
    "title": "DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Foteini Strati",
      "Sara Mcallister",
      "Amar Phanishayee",
      "Jakub Tarnawski",
      "Ana Klimovic"
    ]
  },
  "https://proceedings.mlr.press/v235/su24a.html": {
    "title": "Learning from Streaming Data when Users Choose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyan Su",
      "Sarah Dean"
    ]
  },
  "https://proceedings.mlr.press/v235/su24b.html": {
    "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Su",
      "Xiulong Liu",
      "Eli Shlizerman"
    ]
  },
  "https://proceedings.mlr.press/v235/su24c.html": {
    "title": "Compositional Image Decomposition with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jocelin Su",
      "Nan Liu",
      "Yanbo Wang",
      "Joshua B. Tenenbaum",
      "Yilun Du"
    ]
  },
  "https://proceedings.mlr.press/v235/suau24a.html": {
    "title": "Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xavier Suau",
      "Pieter Delobelle",
      "Katherine Metcalf",
      "Armand Joulin",
      "Nicholas Apostoloff",
      "Luca Zappella",
      "Pau Rodriguez"
    ]
  },
  "https://proceedings.mlr.press/v235/subramaniam24a.html": {
    "title": "Revealing Vision-Language Integration in the Brain with Multimodal Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vighnesh Subramaniam",
      "Colin Conwell",
      "Christopher Wang",
      "Gabriel Kreiman",
      "Boris Katz",
      "Ignacio Cases",
      "Andrei Barbu"
    ]
  },
  "https://proceedings.mlr.press/v235/subramonian24a.html": {
    "title": "Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arjun Subramonian",
      "Levent Sagun",
      "Yizhou Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/suh24a.html": {
    "title": "Enforcing Constraints in RNA Secondary Structure Predictions: A Post-Processing Framework Based on the Assignment Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geewon Suh",
      "Gyeongjo Hwang",
      "Seokjun Kang",
      "Doojin Baek",
      "Mingeun Kang"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24a.html": {
    "title": "ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwen Sun",
      "Abhineet Agarwal",
      "Aaron Kornblith",
      "Bin Yu",
      "Chenyan Xiong"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24b.html": {
    "title": "Breaking the Barrier: Enhanced Utility and Robustness in Smoothed DRL Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chung-En Sun",
      "Sicun Gao",
      "Tsui-Wei Weng"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24c.html": {
    "title": "Representing Molecules as Random Walks Over Interpretable Grammars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Sun",
      "Minghao Guo",
      "Weize Yuan",
      "Veronika Thost",
      "Crystal Elaine Owens",
      "Aristotle Franklin Grosz",
      "Sharvaa Selvan",
      "Katelyn Zhou",
      "Hassan Mohiuddin",
      "Benjamin J Pedretti",
      "Zachary P Smith",
      "Jie Chen",
      "Wojciech Matusik"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24d.html": {
    "title": "Constrained Reinforcement Learning Under Model Mismatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongchang Sun",
      "Sihong He",
      "Fei Miao",
      "Shaofeng Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24e.html": {
    "title": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyou Sun",
      "Junjie Hu",
      "Wei Cheng",
      "Haifeng Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24f.html": {
    "title": "Online Algorithms with Uncertainty-Quantified Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Sun",
      "Jerry Huang",
      "Nicolas Christianson",
      "Mohammad Hajiesmaili",
      "Adam Wierman",
      "Raouf Boutaba"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24g.html": {
    "title": "LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Sun",
      "Zhenhao Huang",
      "Hao Peng",
      "Yujie Wang",
      "Chunyang Liu",
      "Philip S. Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24h.html": {
    "title": "Online Adaptive Anomaly Thresholding with Confidence Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophia Huiwen Sun",
      "Abishek Sankararaman",
      "Balakrishnan Murali Narayanaswamy"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24i.html": {
    "title": "Learning Graph Representation via Graph Entropy Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Sun",
      "Xudong Wang",
      "Chris Ding",
      "Jicong Fan"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24j.html": {
    "title": "FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Sun",
      "Ziyue Xu",
      "Hongxu Yin",
      "Dong Yang",
      "Daguang Xu",
      "Yudong Liu",
      "Zhixu Du",
      "Yiran Chen",
      "Holger R Roth"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24k.html": {
    "title": "Regression Learning with Limited Observations of Multivariate Outcomes and Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Sun",
      "Grace Yi"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24l.html": {
    "title": "video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangzhi Sun",
      "Wenyi Yu",
      "Changli Tang",
      "Xianzhao Chen",
      "Tian Tan",
      "Wei Li",
      "Lu Lu",
      "Zejun Ma",
      "Yuxuan Wang",
      "Chao Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24m.html": {
    "title": "Learning High-Frequency Functions Made Easy with Sinusoidal Positional Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhao Sun",
      "Zhihang Yuan",
      "Kai Xu",
      "Luo Mai",
      "Siddharth N",
      "Shuo Chen",
      "Mahesh K. Marina"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24n.html": {
    "title": "Learning Latent Dynamic Robust Representations for World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruixiang Sun",
      "Hongyu Zang",
      "Xin Li",
      "Riashat Islam"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24o.html": {
    "title": "Self-cognitive Denoising in the Presence of Multiple Noisy Label Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Xuan Sun",
      "Ya-Lin Zhang",
      "Bin Han",
      "Longfei Li",
      "Jun Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/sun24p.html": {
    "title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Sun",
      "Yuchen Zhuang",
      "Wei Wei",
      "Chao Zhang",
      "Bo Dai"
    ]
  },
  "https://proceedings.mlr.press/v235/sunde24a.html": {
    "title": "On a Combinatorial Problem Arising in Machine Teaching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joakim Sunde",
      "Brigt Håvardstun",
      "Jan Kratochvı́l",
      "Jan Arne Telle"
    ]
  },
  "https://proceedings.mlr.press/v235/svirsky24a.html": {
    "title": "Interpretable Deep Clustering for Tabular Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Svirsky",
      "Ofir Lindenbaum"
    ]
  },
  "https://proceedings.mlr.press/v235/svoboda24a.html": {
    "title": "Reinforcement Learning from Reachability Specifications: PAC Guarantees with Expected Conditional Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Svoboda",
      "Suguman Bansal",
      "Krishnendu Chatterjee"
    ]
  },
  "https://proceedings.mlr.press/v235/swamy24a.html": {
    "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gokul Swamy",
      "Christoph Dann",
      "Rahul Kidambi",
      "Steven Wu",
      "Alekh Agarwal"
    ]
  },
  "https://proceedings.mlr.press/v235/swartworth24a.html": {
    "title": "Fast Sampling-Based Sketches for Tensors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Joseph Swartworth",
      "David Woodruff"
    ]
  },
  "https://proceedings.mlr.press/v235/tahmasebi24a.html": {
    "title": "Sample Complexity Bounds for Estimating Probability Divergences under Invariances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Behrooz Tahmasebi",
      "Stefanie Jegelka"
    ]
  },
  "https://proceedings.mlr.press/v235/tahmasebi24b.html": {
    "title": "A Universal Class of Sharpness-Aware Minimization Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Behrooz Tahmasebi",
      "Ashkan Soleymani",
      "Dara Bahri",
      "Stefanie Jegelka",
      "Patrick Jaillet"
    ]
  },
  "https://proceedings.mlr.press/v235/tajwar24a.html": {
    "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fahim Tajwar",
      "Anikait Singh",
      "Archit Sharma",
      "Rafael Rafailov",
      "Jeff Schneider",
      "Tengyang Xie",
      "Stefano Ermon",
      "Chelsea Finn",
      "Aviral Kumar"
    ]
  },
  "https://proceedings.mlr.press/v235/takakura24a.html": {
    "title": "Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shokichi Takakura",
      "Taiji Suzuki"
    ]
  },
  "https://proceedings.mlr.press/v235/takeno24a.html": {
    "title": "Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shion Takeno",
      "Yu Inatsu",
      "Masayuki Karasuyama",
      "Ichiro Takeuchi"
    ]
  },
  "https://proceedings.mlr.press/v235/tamkin24a.html": {
    "title": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Tamkin",
      "Mohammad Taufeeque",
      "Noah Goodman"
    ]
  },
  "https://proceedings.mlr.press/v235/tan24a.html": {
    "title": "Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Tan",
      "Zhangyang Gao",
      "Hanqun Cao",
      "Xingran Chen",
      "Ge Wang",
      "Lirong Wu",
      "Jun Xia",
      "Jiangbin Zheng",
      "Stan Z. Li"
    ]
  },
  "https://proceedings.mlr.press/v235/tan24b.html": {
    "title": "Community-Invariant Graph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyin Tan",
      "Dongyuan Li",
      "Renhe Jiang",
      "Ying Zhang",
      "Manabu Okumura"
    ]
  },
  "https://proceedings.mlr.press/v235/tan24c.html": {
    "title": "Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengkai Tan",
      "Songming Liu",
      "Kai Ma",
      "Chengyang Ying",
      "Xingxing Zhang",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/tan24d.html": {
    "title": "Learning Solution-Aware Transformers for Efficiently Solving Quadratic Assignment Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhentao Tan",
      "Yadong Mu"
    ]
  },
  "https://proceedings.mlr.press/v235/tan24e.html": {
    "title": "Information Flow in Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiquan Tan",
      "Jingqin Yang",
      "Weiran Huang",
      "Yang Yuan",
      "Yifan Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/tan24f.html": {
    "title": "OTMatch: Improving Semi-Supervised Learning with Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiquan Tan",
      "Kaipeng Zheng",
      "Weiran Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/tan24g.html": {
    "title": "Post-hoc Part-Prototype Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Tan",
      "Fengtao Zhou",
      "Hao Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24a.html": {
    "title": "Causally Motivated Personalized Federated Invariant Learning with Shortcut-Averse Information-Theoretic Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyang Tang",
      "Song Guo",
      "Jingcai Guo",
      "Jie Zhang",
      "Yue Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24b.html": {
    "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Tang",
      "Zhaohan Daniel Guo",
      "Zeyu Zheng",
      "Daniele Calandriello",
      "Remi Munos",
      "Mark Rowland",
      "Pierre Harvey Richemond",
      "Michal Valko",
      "Bernardo Avila Pires",
      "Bilal Piot"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24c.html": {
    "title": "Rethinking Optimization and Architecture for Tiny Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yehui Tang",
      "Kai Han",
      "Fangcheng Liu",
      "Yunsheng Ni",
      "Yuchuan Tian",
      "Zheyuan Bai",
      "Yi-Qi Hu",
      "Sichao Liu",
      "Shangling Jui",
      "Yunhe Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24d.html": {
    "title": "Residual-Conditioned Optimal Transport: Towards Structure-Preserving Unpaired and Paired Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaole Tang",
      "Xin Hu",
      "Xiang Gu",
      "Jian Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24e.html": {
    "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anke Tang",
      "Li Shen",
      "Yong Luo",
      "Nan Yin",
      "Lefei Zhang",
      "Dacheng Tao"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24f.html": {
    "title": "Accelerating Parallel Sampling of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Tang",
      "Jiasheng Tang",
      "Hao Luo",
      "Fan Wang",
      "Tsung-Hui Chang"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24g.html": {
    "title": "Membership Inference Attacks on Diffusion Models via Quantile Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Tang",
      "Steven Wu",
      "Sergul Aydore",
      "Michael Kearns",
      "Aaron Roth"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24h.html": {
    "title": "StrokeNUWA—Tokenizing Strokes for Vector Graphic Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zecheng Tang",
      "Chenfei Wu",
      "Zekai Zhang",
      "Minheng Ni",
      "Shengming Yin",
      "Yu Liu",
      "Zhengyuan Yang",
      "Lijuan Wang",
      "Zicheng Liu",
      "Juntao Li",
      "Nan Duan"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24i.html": {
    "title": "SSL4Q: Semi-Supervised Learning of Quantum Data with Application to Quantum State Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yehui Tang",
      "Nianzu Yang",
      "Mabiao Long",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24j.html": {
    "title": "Finite Smoothing Algorithm for High-Dimensional Support Vector Machines and Quantile Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Tang",
      "Yikai Zhang",
      "Boxiang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24k.html": {
    "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyang Tang",
      "Xingxing Zhang",
      "Benyou Wang",
      "Furu Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/tang24l.html": {
    "title": "QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Tang",
      "Yilong Zhao",
      "Kan Zhu",
      "Guangxuan Xiao",
      "Baris Kasikci",
      "Song Han"
    ]
  },
  "https://proceedings.mlr.press/v235/tao24a.html": {
    "title": "MLI Formula: A Nearly Scale-Invariant Solution with Noise Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Tao",
      "Xin-Chun Li",
      "De-Chuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/tao24b.html": {
    "title": "Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel SVD and Nyström method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinghua Tao",
      "Francesco Tonin",
      "Alex Lambert",
      "Yingyi Chen",
      "Panagiotis Patrinos",
      "Johan Suykens"
    ]
  },
  "https://proceedings.mlr.press/v235/teney24a.html": {
    "title": "Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Teney",
      "Jindong Wang",
      "Ehsan Abbasnejad"
    ]
  },
  "https://proceedings.mlr.press/v235/thabet24a.html": {
    "title": "Quantum Positional Encodings for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Slimane Thabet",
      "Mehdi Djellabi",
      "Igor Olegovich Sokolov",
      "Sachin Kasture",
      "Louis-Paul Henry",
      "Loic Henriet"
    ]
  },
  "https://proceedings.mlr.press/v235/thangarasa24a.html": {
    "title": "Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vithursan Thangarasa",
      "Shreyas Saxena",
      "Abhay Gupta",
      "Sean Lie"
    ]
  },
  "https://proceedings.mlr.press/v235/thapa24a.html": {
    "title": "SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity, ECG and Respiratory Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Thapa",
      "Bryan He",
      "Magnus Ruud Kjaer",
      "Hyatt Moore Iv",
      "Gauri Ganjoo",
      "Emmanuel Mignot",
      "James Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/thapa24b.html": {
    "title": "Bayesian Adaptation of Network Depth and Width for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeevan Thapa",
      "Rui Li"
    ]
  },
  "https://proceedings.mlr.press/v235/theis24a.html": {
    "title": "Position: What makes an image realistic?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Theis"
    ]
  },
  "https://proceedings.mlr.press/v235/guyard24a.html": {
    "title": "A New Branch-and-Bound Pruning Framework for $\\ell_0$-Regularized Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theo Guyard",
      "Cédric Herzet",
      "Clément Elvira",
      "Ayse-Nur Arslan"
    ]
  },
  "https://proceedings.mlr.press/v235/thimonier24a.html": {
    "title": "Beyond Individual Input for Deep Anomaly Detection on Tabular Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugo Thimonier",
      "Fabrice Popineau",
      "Arpad Rimmel",
      "Bich-Liên Doan"
    ]
  },
  "https://proceedings.mlr.press/v235/thoppe24a.html": {
    "title": "Risk Estimation in a Markov Cost Process: Lower and Upper Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gugan Thoppe",
      "Prashanth L A",
      "Sanjay P. Bhat"
    ]
  },
  "https://proceedings.mlr.press/v235/tian24a.html": {
    "title": "Collapse-Aware Triplet Decoupling for Adversarially Robust Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiwei Tian",
      "Chenhao Lin",
      "Zhengyu Zhao",
      "Qian Li",
      "Chao Shen"
    ]
  },
  "https://proceedings.mlr.press/v235/tian24b.html": {
    "title": "MOKD: Cross-domain Finetuning for Few-shot Classification via Maximizing Optimized Kernel Dependence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongduan Tian",
      "Feng Liu",
      "Tongliang Liu",
      "Bo Du",
      "Yiu-Ming Cheung",
      "Bo Han"
    ]
  },
  "https://proceedings.mlr.press/v235/tian24c.html": {
    "title": "Liouville Flow Importance Sampler",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng Tian",
      "Nishant Panda",
      "Yen Ting Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/tian24d.html": {
    "title": "Ranking-based Client Imitation Selection for Efficient Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunlin Tian",
      "Zhan Shi",
      "Xinpeng Qin",
      "Li Li",
      "Cheng-Zhong Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/tian24e.html": {
    "title": "Towards the Theory of Unsupervised Federated Learning: Non-asymptotic Analysis of Federated EM Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Tian",
      "Haolei Weng",
      "Yang Feng"
    ]
  },
  "https://proceedings.mlr.press/v235/tian24f.html": {
    "title": "Copula-Nested Spectral Kernel Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyue Tian",
      "Hui Xue",
      "Yanfang Xue",
      "Pengfei Fang"
    ]
  },
  "https://proceedings.mlr.press/v235/tian24g.html": {
    "title": "Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsheng Tian",
      "Ane Zuniga",
      "Xinwei Zhang",
      "Johannes P. Dürholt",
      "Payel Das",
      "Jie Chen",
      "Wojciech Matusik",
      "Mina Konakovic Lukovic"
    ]
  },
  "https://proceedings.mlr.press/v235/tifrea24a.html": {
    "title": "FRAPPÉ: A Group Fairness Framework for Post-Processing Everything",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandru Tifrea",
      "Preethi Lahoti",
      "Ben Packer",
      "Yoni Halpern",
      "Ahmad Beirami",
      "Flavien Prost"
    ]
  },
  "https://proceedings.mlr.press/v235/tiwari24a.html": {
    "title": "Faster Maximum Inner Product Search in High Dimensions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mo Tiwari",
      "Ryan Kang",
      "Jaeyong Lee",
      "Donghyun Lee",
      "Christopher J Piech",
      "Sebastian Thrun",
      "Ilan Shomorony",
      "Martin Jinye Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/tkachenko24a.html": {
    "title": "Position: Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yegor Tkachenko"
    ]
  },
  "https://proceedings.mlr.press/v235/tomasini24a.html": {
    "title": "How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Umberto Maria Tomasini",
      "Matthieu Wyart"
    ]
  },
  "https://proceedings.mlr.press/v235/tomaszewska24a.html": {
    "title": "Position: Do Not Explain Vision Models Without Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paulina Tomaszewska",
      "Przemyslaw Biecek"
    ]
  },
  "https://proceedings.mlr.press/v235/toner24a.html": {
    "title": "An Analysis of Linear Time Series Forecasting Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Toner",
      "Luke Nicholas Darlow"
    ]
  },
  "https://proceedings.mlr.press/v235/toshev24a.html": {
    "title": "Neural SPH: Improved Neural Modeling of Lagrangian Fluid Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artur Toshev",
      "Jonas A. Erbesdobler",
      "Nikolaus A. Adams",
      "Johannes Brandstetter"
    ]
  },
  "https://proceedings.mlr.press/v235/tramer24a.html": {
    "title": "Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Tramèr",
      "Gautam Kamath",
      "Nicholas Carlini"
    ]
  },
  "https://proceedings.mlr.press/v235/tramontano24a.html": {
    "title": "Causal Effect Identification in LiNGAM Models with Latent Confounders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniele Tramontano",
      "Yaroslav Kivva",
      "Saber Salehkaleybar",
      "Mathias Drton",
      "Negar Kiyavash"
    ]
  },
  "https://proceedings.mlr.press/v235/tran24a.html": {
    "title": "Stereographic Spherical Sliced Wasserstein Distances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Tran",
      "Yikun Bai",
      "Abihith Kothapalli",
      "Ashkan Shahbazi",
      "Xinran Liu",
      "Rocio P Diaz Martin",
      "Soheil Kolouri"
    ]
  },
  "https://proceedings.mlr.press/v235/tran24b.html": {
    "title": "Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Allen Tran",
      "Aurelien Bibaut",
      "Nathan Kallus"
    ]
  },
  "https://proceedings.mlr.press/v235/triantafyllou24a.html": {
    "title": "Agent-Specific Effects: A Causal Effect Propagation Analysis in Multi-Agent MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stelios Triantafyllou",
      "Aleksa Sukovic",
      "Debmalya Mandal",
      "Goran Radanovic"
    ]
  },
  "https://proceedings.mlr.press/v235/trivedi24a.html": {
    "title": "Editing Partially Observable Networks via Graph Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puja Trivedi",
      "Ryan A. Rossi",
      "David Arbour",
      "Tong Yu",
      "Franck Dernoncourt",
      "Sungchul Kim",
      "Nedim Lipka",
      "Namyong Park",
      "Nesreen K. Ahmed",
      "Danai Koutra"
    ]
  },
  "https://proceedings.mlr.press/v235/tseng24a.html": {
    "title": "QuIP$#$: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert Tseng",
      "Jerry Chee",
      "Qingyao Sun",
      "Volodymyr Kuleshov",
      "Christopher De Sa"
    ]
  },
  "https://proceedings.mlr.press/v235/tsiamis24a.html": {
    "title": "Predictive Linear Online Tracking for Unknown Targets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasios Tsiamis",
      "Aren Karapetyan",
      "Yueshan Li",
      "Efe C. Balta",
      "John Lygeros"
    ]
  },
  "https://proceedings.mlr.press/v235/tsiourvas24a.html": {
    "title": "Overcoming the Optimizer's Curse: Obtaining Realistic Prescriptions from Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asterios Tsiourvas",
      "Georgia Perakis"
    ]
  },
  "https://proceedings.mlr.press/v235/tsiourvas24b.html": {
    "title": "Learning Optimal Projection for Forecast Reconciliation of Hierarchical Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asterios Tsiourvas",
      "Wei Sun",
      "Georgia Perakis",
      "Pin-Yu Chen",
      "Yada Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/tsoy24a.html": {
    "title": "Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Tsoy",
      "Nikola Konstantinov"
    ]
  },
  "https://proceedings.mlr.press/v235/tsuchiya24a.html": {
    "title": "Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taira Tsuchiya",
      "Shinji Ito",
      "Junya Honda"
    ]
  },
  "https://proceedings.mlr.press/v235/tu24a.html": {
    "title": "An Empirical Study Into What Matters for Calibrating Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Tu",
      "Weijian Deng",
      "Dylan Campbell",
      "Stephen Gould",
      "Tom Gedeon"
    ]
  },
  "https://proceedings.mlr.press/v235/tucker24a.html": {
    "title": "Coactive Learning for Large Language Models using Implicit User Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron David Tucker",
      "Kianté Brantley",
      "Adam Cahall",
      "Thorsten Joachims"
    ]
  },
  "https://proceedings.mlr.press/v235/tulsiani24a.html": {
    "title": "An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hitesh Tulsiani",
      "David Chan",
      "Shalini Ghosh",
      "Garima Lalwani",
      "Prabhat Pandey",
      "Ankish Bansal",
      "Sri Garimella",
      "Ariya Rastrow",
      "Björn Hoffmeister"
    ]
  },
  "https://proceedings.mlr.press/v235/turrero24a.html": {
    "title": "ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carmen Martin Turrero",
      "Maxence Bouvier",
      "Manuel Breitenstein",
      "Pietro Zanuttigh",
      "Vincent Parret"
    ]
  },
  "https://proceedings.mlr.press/v235/tzeng24a.html": {
    "title": "Matroid Semi-Bandits in Sublinear Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruo-Chun Tzeng",
      "Naoto Ohsaka",
      "Kaito Ariu"
    ]
  },
  "https://proceedings.mlr.press/v235/ucar24a.html": {
    "title": "Improving Antibody Humanness Prediction using Patent Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Talip Ucar",
      "Aubin Ramon",
      "Dino Oglic",
      "Rebecca Croasdale-Wood",
      "Tom Diethe",
      "Pietro Sormanni"
    ]
  },
  "https://proceedings.mlr.press/v235/uehara24a.html": {
    "title": "Feedback Efficient Online Fine-Tuning of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masatoshi Uehara",
      "Yulai Zhao",
      "Kevin Black",
      "Ehsan Hajiramezanali",
      "Gabriele Scalia",
      "Nathaniel Lee Diamant",
      "Alex M Tseng",
      "Sergey Levine",
      "Tommaso Biancalani"
    ]
  },
  "https://proceedings.mlr.press/v235/vafa24a.html": {
    "title": "Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyon Vafa",
      "Ashesh Rambachan",
      "Sendhil Mullainathan"
    ]
  },
  "https://proceedings.mlr.press/v235/vakili24a.html": {
    "title": "Reward-Free Kernel-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sattar Vakili",
      "Farhang Nabiei",
      "Da-Shan Shiu",
      "Alberto Bernacchia"
    ]
  },
  "https://proceedings.mlr.press/v235/valancius24a.html": {
    "title": "Acquisition Conditioned Oracle for Nongreedy Active Feature Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Valancius",
      "Maxwell Lennon",
      "Junier Oliva"
    ]
  },
  "https://proceedings.mlr.press/v235/van-breugel24a.html": {
    "title": "Position: Why Tabular Foundation Models Should Be a Research Priority",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boris Van Breugel",
      "Mihaela Van Der Schaar"
    ]
  },
  "https://proceedings.mlr.press/v235/van-den-bos24a.html": {
    "title": "Piecewise Constant and Linear Regression Trees: An Optimal Dynamic Programming Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mim Van Den Bos",
      "Jacobus G. M. Van Der Linden",
      "Emir Demirović"
    ]
  },
  "https://proceedings.mlr.press/v235/van-den-brand24a.html": {
    "title": "Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Van Den Brand",
      "Zhao Song",
      "Tianyi Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/van-dijk24a.html": {
    "title": "Proactive DP: A Multiple Target Optimization Framework for DP-SGD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marten Van Dijk",
      "Nhuong Van Nguyen",
      "Toan N. Nguyen",
      "Lam M. Nguyen",
      "Phuong Ha Nguyen"
    ]
  },
  "https://proceedings.mlr.press/v235/van-krieken24a.html": {
    "title": "On the Independence Assumption in Neurosymbolic Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emile Van Krieken",
      "Pasquale Minervini",
      "Edoardo Ponti",
      "Antonio Vergari"
    ]
  },
  "https://proceedings.mlr.press/v235/van-rossem24a.html": {
    "title": "When Representations Align: Universality in Representation Learning Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Loek Van Rossem",
      "Andrew M Saxe"
    ]
  },
  "https://proceedings.mlr.press/v235/vani24a.html": {
    "title": "Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Vani",
      "Frederick Tung",
      "Gabriel L. Oliveira",
      "Hossein Sharifi-Noghabi"
    ]
  },
  "https://proceedings.mlr.press/v235/vankov24a.html": {
    "title": "Generalized Smooth Variational Inequalities: Methods with Adaptive Stepsizes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Vankov",
      "Angelia Nedich",
      "Lalitha Sankar"
    ]
  },
  "https://proceedings.mlr.press/v235/varambally24a.html": {
    "title": "Discovering Mixtures of Structural Causal Models from Time Series Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumanth Varambally",
      "Yian Ma",
      "Rose Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/vardanyan24a.html": {
    "title": "Statistically Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elen Vardanyan",
      "Sona Hunanyan",
      "Tigran Galstyan",
      "Arshak Minasyan",
      "Arnak S. Dalalyan"
    ]
  },
  "https://proceedings.mlr.press/v235/vary24a.html": {
    "title": "Optimization without Retraction on the Random Generalized Stiefel Manifold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Vary",
      "Pierre Ablin",
      "Bin Gao",
      "Pierre-Antoine Absil"
    ]
  },
  "https://proceedings.mlr.press/v235/veefkind24a.html": {
    "title": "A Probabilistic Approach to Learning the Degree of Equivariance in Steerable CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lars Veefkind",
      "Gabriele Cesa"
    ]
  },
  "https://proceedings.mlr.press/v235/veiga24a.html": {
    "title": "Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rodrigo Veiga",
      "Anastasia Remizova",
      "Nicolas Macris"
    ]
  },
  "https://proceedings.mlr.press/v235/vemulapalli24a.html": {
    "title": "Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raviteja Vemulapalli",
      "Hadi Pouransari",
      "Fartash Faghri",
      "Sachin Mehta",
      "Mehrdad Farajtabar",
      "Mohammad Rastegari",
      "Oncel Tuzel"
    ]
  },
  "https://proceedings.mlr.press/v235/venuto24a.html": {
    "title": "Code as Reward: Empowering Reinforcement Learning with VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Venuto",
      "Mohammad Sami Nur Islam",
      "Martin Klissarov",
      "Doina Precup",
      "Sherry Yang",
      "Ankit Anand"
    ]
  },
  "https://proceedings.mlr.press/v235/verma24a.html": {
    "title": "Topological Neural Networks go Persistent, Equivariant, and Continuous",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yogesh Verma",
      "Amauri H Souza",
      "Vikas Garg"
    ]
  },
  "https://proceedings.mlr.press/v235/vero24a.html": {
    "title": "CuTS: Customizable Tabular Synthetic Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Vero",
      "Mislav Balunovic",
      "Martin Vechev"
    ]
  },
  "https://proceedings.mlr.press/v235/vesseron24a.html": {
    "title": "On a Neural Implementation of Brenier's Polar Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Vesseron",
      "Marco Cuturi"
    ]
  },
  "https://proceedings.mlr.press/v235/veviurko24a.html": {
    "title": "To the Max: Reinventing Reward in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grigorii Veviurko",
      "Wendelin Boehmer",
      "Mathijs De Weerdt"
    ]
  },
  "https://proceedings.mlr.press/v235/viano24a.html": {
    "title": "Imitation Learning in Discounted Linear MDPs without exploration assumptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Viano",
      "Stratis Skoulakis",
      "Volkan Cevher"
    ]
  },
  "https://proceedings.mlr.press/v235/vilas24a.html": {
    "title": "Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martina G. Vilas",
      "Federico Adolfi",
      "David Poeppel",
      "Gemma Roig"
    ]
  },
  "https://proceedings.mlr.press/v235/villalobos24a.html": {
    "title": "Position: Will we run out of data? Limits of LLM scaling based on human-generated data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Villalobos",
      "Anson Ho",
      "Jaime Sevilla",
      "Tamay Besiroglu",
      "Lennart Heim",
      "Marius Hobbhahn"
    ]
  },
  "https://proceedings.mlr.press/v235/vishniakov24a.html": {
    "title": "ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirill Vishniakov",
      "Zhiqiang Shen",
      "Zhuang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/vivier-ardisson24a.html": {
    "title": "CF-OPT: Counterfactual Explanations for Structured Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Germain Vivier-Ardisson",
      "Alexandre Forel",
      "Axel Parmentier",
      "Thibaut Vidal"
    ]
  },
  "https://proceedings.mlr.press/v235/vo24a.html": {
    "title": "Parameter Estimation in DAGs from Incomplete Data via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vy Vo",
      "Trung Le",
      "Long Tung Vuong",
      "He Zhao",
      "Edwin V. Bonilla",
      "Dinh Phung"
    ]
  },
  "https://proceedings.mlr.press/v235/vo24b.html": {
    "title": "Optimal Transport for Structure Learning Under Missing Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vy Vo",
      "He Zhao",
      "Trung Le",
      "Edwin V. Bonilla",
      "Dinh Phung"
    ]
  },
  "https://proceedings.mlr.press/v235/vodrahalli24a.html": {
    "title": "ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kailas Vodrahalli",
      "James Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/von-rutte24a.html": {
    "title": "A Language Model's Guide Through Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitri Von Rütte",
      "Sotiris Anagnostidis",
      "Gregor Bachmann",
      "Thomas Hofmann"
    ]
  },
  "https://proceedings.mlr.press/v235/voracek24a.html": {
    "title": "Convergence of Some Convex Message Passing Algorithms to a Fixed Point",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaclav Voracek",
      "Tomas Werner"
    ]
  },
  "https://proceedings.mlr.press/v235/vyas24a.html": {
    "title": "Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Vyas",
      "Depen Morwani",
      "Rosie Zhao",
      "Gal Kaplun",
      "Sham M. Kakade",
      "Boaz Barak"
    ]
  },
  "https://proceedings.mlr.press/v235/vysogorets24a.html": {
    "title": "Deconstructing the Goldilocks Zone of Neural Network Initialization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artem M Vysogorets",
      "Anna Dawid",
      "Julia Kempe"
    ]
  },
  "https://proceedings.mlr.press/v235/wadhawan24a.html": {
    "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Wadhawan",
      "Hritik Bansal",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/wagner24a.html": {
    "title": "Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Sylvius Wagner",
      "Stefan Harmeling"
    ]
  },
  "https://proceedings.mlr.press/v235/waiwitlikhit24a.html": {
    "title": "Trustless Audits without Revealing Data or Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suppakit Waiwitlikhit",
      "Ion Stoica",
      "Yi Sun",
      "Tatsunori Hashimoto",
      "Daniel Kang"
    ]
  },
  "https://proceedings.mlr.press/v235/walker24a.html": {
    "title": "Log Neural Controlled Differential Equations: The Lie Brackets Make A Difference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Walker",
      "Andrew Donald Mcleod",
      "Tiexin Qin",
      "Yichuan Cheng",
      "Haoliang Li",
      "Terry Lyons"
    ]
  },
  "https://proceedings.mlr.press/v235/wan24a.html": {
    "title": "Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Wan",
      "Melih Barsbey",
      "Abdellatif Zaidi",
      "Umut Simsekli"
    ]
  },
  "https://proceedings.mlr.press/v235/wan24b.html": {
    "title": "SeMOPO: Learning High-quality Model and Policy from Low-quality Offline Visual Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghua Wan",
      "Ziyuan Chen",
      "Le Gan",
      "Shuai Feng",
      "De-Chuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/wan24c.html": {
    "title": "AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Wan",
      "Xidong Feng",
      "Muning Wen",
      "Stephen Marcus Mcaleer",
      "Ying Wen",
      "Weinan Zhang",
      "Jun Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wan24d.html": {
    "title": "VinT-6D: A Large-Scale Object-in-hand Dataset from Vision, Touch and Proprioception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoliang Wan",
      "Yonggen Ling",
      "Senlin Yi",
      "Lu Qi",
      "Wang Wei Lee",
      "Minglei Lu",
      "Sicheng Yang",
      "Xiao Teng",
      "Peng Lu",
      "Xu Yang",
      "Ming-Hsuan Yang",
      "Hui Cheng"
    ]
  },
  "https://proceedings.mlr.press/v235/wan24e.html": {
    "title": "Decouple then Classify: A Dynamic Multi-view Labeling Strategy with Shared and Specific Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhang Wan",
      "Jiyuan Liu",
      "Xinwang Liu",
      "Yi Wen",
      "Hao Yu",
      "Siwei Wang",
      "Shengju Yu",
      "Tianjiao Wan",
      "Jun Wang",
      "En Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/wan24f.html": {
    "title": "Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diwen Wan",
      "Ruijie Lu",
      "Gang Zeng"
    ]
  },
  "https://proceedings.mlr.press/v235/wan24g.html": {
    "title": "S3GCL: Spectral, Swift, Spatial Graph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guancheng Wan",
      "Yijun Tian",
      "Wenke Huang",
      "Nitesh V Chawla",
      "Mang Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/wan24h.html": {
    "title": "Non-stationary Online Convex Optimization with Arbitrary Delays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyu Wan",
      "Chang Yao",
      "Mingli Song",
      "Lijun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/wan24i.html": {
    "title": "Towards Unified Multi-granularity Text Detection with Interactive Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Wan",
      "Chengquan Zhang",
      "Pengyuan Lyu",
      "Sen Fan",
      "Zihan Ni",
      "Kun Yao",
      "Errui Ding",
      "Jingdong Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24a.html": {
    "title": "Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Wang",
      "Alfonso Amayuelas",
      "Kexun Zhang",
      "Liangming Pan",
      "Wenhu Chen",
      "William Yang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24b.html": {
    "title": "One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruochen Wang",
      "Sohyun An",
      "Minhao Cheng",
      "Tianyi Zhou",
      "Sung Ju Hwang",
      "Cho-Jui Hsieh"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24c.html": {
    "title": "On Universally Optimal Algorithms for A/B Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Po-An Wang",
      "Kaito Ariu",
      "Alexandre Proutiere"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24d.html": {
    "title": "Adversarially Robust Hypothesis Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunjuan Wang",
      "Raman Arora"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24e.html": {
    "title": "Towards Theoretical Understanding of Learning Large-scale Dependent Data via Random Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang",
      "Xin Bing",
      "Xin He",
      "Caixing Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24f.html": {
    "title": "Non-parametric Online Change Point Detection on Riemannian Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuheng Wang",
      "Ricardo Augusto Borsoi",
      "Cédric Richard"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24g.html": {
    "title": "A Circuit Domain Generalization Framework for Efficient Logic Synthesis in Chip Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihai Wang",
      "Lei Chen",
      "Jie Wang",
      "Yinqi Bai",
      "Xing Li",
      "Xijun Li",
      "Mingxuan Yuan",
      "Jianye Hao",
      "Yongdong Zhang",
      "Feng Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24h.html": {
    "title": "Executable Code Actions Elicit Better LLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyao Wang",
      "Yangyi Chen",
      "Lifan Yuan",
      "Yizhe Zhang",
      "Yunzhu Li",
      "Hao Peng",
      "Heng Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24i.html": {
    "title": "Revisiting the Power of Prompt for Visual Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhu Wang",
      "Lechao Cheng",
      "Chaowei Fang",
      "Dingwen Zhang",
      "Manni Duan",
      "Meng Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24j.html": {
    "title": "TVE: Learning Meta-attribution for Transferable Vision Explainer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanchu Wang",
      "Yu-Neng Chuang",
      "Fan Yang",
      "Mengnan Du",
      "Chia-Yuan Chang",
      "Shaochen Zhong",
      "Zirui Liu",
      "Zhaozhuo Xu",
      "Kaixiong Zhou",
      "Xuanting Cai",
      "Xia Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24k.html": {
    "title": "Localizing Task Information for Improved Model Merging and Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Wang",
      "Nikolaos Dimitriadis",
      "Guillermo Ortiz-Jimenez",
      "François Fleuret",
      "Pascal Frossard"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24l.html": {
    "title": "Adaptively Learning to Select-Rank in Online Platforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyuan Wang",
      "Perry Dong",
      "Ying Jin",
      "Ruohan Zhan",
      "Zhengyuan Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24m.html": {
    "title": "Imitation Learning from Purified Demonstrations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunke Wang",
      "Minjing Dong",
      "Yukun Zhao",
      "Bo Du",
      "Chang Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24n.html": {
    "title": "Diagnosing the Compositional Knowledge of Vision Language Models from a Game-Theoretic View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Wang",
      "Shichao Dong",
      "Yapeng Zhu",
      "Kelu Yao",
      "Weidong Zhao",
      "Chao Li",
      "Ping Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24o.html": {
    "title": "An Efficient Maximal Ancestral Graph Listing Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian-Zuo Wang",
      "Wen-Bo Du",
      "Zhi-Hua Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24p.html": {
    "title": "Monotone, Bi-Lipschitz, and Polyak-Łojasiewicz Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruigang Wang",
      "Krishnamurthy Dj Dvijotham",
      "Ian Manchester"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24q.html": {
    "title": "Swallowing the Bitter Pill: Simplified Scalable Conformer Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Wang",
      "Ahmed A. A. Elhag",
      "Navdeep Jaitly",
      "Joshua M. Susskind",
      "Miguel Ángel Bautista"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24r.html": {
    "title": "Optimal Kernel Quantile Learning with Random Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caixing Wang",
      "Xingdong Feng"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24s.html": {
    "title": "MEMORYLLM: Towards Self-Updatable Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Yifan Gao",
      "Xiusi Chen",
      "Haoming Jiang",
      "Shiyang Li",
      "Jingfeng Yang",
      "Qingyu Yin",
      "Zheng Li",
      "Xian Li",
      "Bing Yin",
      "Jingbo Shang",
      "Julian Mcauley"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24t.html": {
    "title": "Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiao Wang",
      "Amiremad Ghassami",
      "Ilya Shpitser"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24u.html": {
    "title": "Understanding Heterophily for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfu Wang",
      "Yuanfang Guo",
      "Liang Yang",
      "Yunhong Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24v.html": {
    "title": "Momentum for the Win: Collaborative Federated Reinforcement Learning across Heterogeneous Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang",
      "Sihong He",
      "Zhili Zhang",
      "Fei Miao",
      "James Anderson"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24w.html": {
    "title": "EvGGS: A Collaborative Learning Framework for Event-based Generalizable Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxu Wang",
      "Junhao He",
      "Ziyi Zhang",
      "Mingyuan Sun",
      "Jingkai Sun",
      "Renjing Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24x.html": {
    "title": "Mollification Effects of Policy Gradient Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Wang",
      "Sylvia Lee Herbert",
      "Sicun Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24y.html": {
    "title": "Discovering Symmetry Breaking in Physical Systems with Relaxed Group Convolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Wang",
      "Elyssa Hofgard",
      "Han Gao",
      "Robin Walters",
      "Tess Smidt"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24z.html": {
    "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxuan Wang",
      "Ziniu Hu",
      "Pan Lu",
      "Yanqiao Zhu",
      "Jieyu Zhang",
      "Satyen Subramaniam",
      "Arjun R Loomba",
      "Shichang Zhang",
      "Yizhou Sun",
      "Wei Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24aa.html": {
    "title": "Optimal Kernel Choice for Score Function-based Causal Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Wang",
      "Biwei Huang",
      "Feng Liu",
      "Xinge You",
      "Tongliang Liu",
      "Kun Zhang",
      "Mingming Gong"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ab.html": {
    "title": "Rapid Learning without Catastrophic Forgetting in the Morris Water Maze",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raymond Wang",
      "Jaedong Hwang",
      "Akhilan Boopathy",
      "Ila R Fiete"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ac.html": {
    "title": "Learning with Complementary Labels Revisited: The Selected-Completely-at-Random Setting Is More Practical",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Wang",
      "Takashi Ishida",
      "Yu-Jie Zhang",
      "Gang Niu",
      "Masashi Sugiyama"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ad.html": {
    "title": "Total Variation Floodgate for Variable Importance Inference in Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshuo Wang",
      "Lucas Janson",
      "Lihua Lei",
      "Aaditya Ramdas"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ae.html": {
    "title": "In-context Learning on Function Classes Unveiled for Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijie Wang",
      "Bo Jiang",
      "Shuai Li"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24af.html": {
    "title": "MS$^3$D: A RG Flow-Based Regularization for GAN Training with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Xin Lan",
      "Yuxin Tian",
      "Jiancheng Lv"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ag.html": {
    "title": "StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shida Wang",
      "Qianxiao Li"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ah.html": {
    "title": "Bootstrap AutoEncoders With Contrastive Paradigm for Self-supervised Gaze Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoming Wang",
      "Jin Li",
      "Wenrui Dai",
      "Bowen Shi",
      "Xiaopeng Zhang",
      "Chenglin Li",
      "Hongkai Xiong"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ai.html": {
    "title": "Highway Value Iteration Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Wang",
      "Weida Li",
      "Francesco Faccio",
      "Qingyuan Wu",
      "Jürgen Schmidhuber"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24aj.html": {
    "title": "Improving Generalization in Offline Reinforcement Learning via Adversarial Data Splitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Wang",
      "Lin Li",
      "Wei Wei",
      "Qixian Yu",
      "Jianye Hao",
      "Jiye Liang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ak.html": {
    "title": "Visual Transformer with Differentiable Channel Selection: An Information Bottleneck Inspired Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancheng Wang",
      "Ping Li",
      "Yingzhen Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24al.html": {
    "title": "An Iterative Min-Min Optimization Method for Sparse Bayesian Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasen Wang",
      "Junlin Li",
      "Zuogong Yue",
      "Ye Yuan"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24am.html": {
    "title": "Graph As Point Set",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyuan Wang",
      "Pan Li",
      "Muhan Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24an.html": {
    "title": "Open Ad Hoc Teamwork with Cooperative Game Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhong Wang",
      "Yang Li",
      "Yuan Zhang",
      "Wei Pan",
      "Samuel Kaski"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ao.html": {
    "title": "Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengbo Wang",
      "Jian Liang",
      "Ran He",
      "Zilei Wang",
      "Tieniu Tan"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ap.html": {
    "title": "Bridging Data Gaps in Diffusion Models with Adversarial Noise-Based Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyu Wang",
      "Baijiong Lin",
      "Daochang Liu",
      "Ying-Cong Chen",
      "Chang Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24aq.html": {
    "title": "Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingtan Wang",
      "Xiaoqiang Lin",
      "Rui Qiao",
      "Chuan-Sheng Foo",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ar.html": {
    "title": "On Discrete Prompt Optimization for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruochen Wang",
      "Ting Liu",
      "Cho-Jui Hsieh",
      "Boqing Gong"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24as.html": {
    "title": "A Global Geometric Analysis of Maximal Coding Rate Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wang",
      "Huikang Liu",
      "Druv Pai",
      "Yaodong Yu",
      "Zhihui Zhu",
      "Qing Qu",
      "Yi Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24at.html": {
    "title": "EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Wang",
      "Shaohuai Liu",
      "Weirui Ye",
      "Jiacheng You",
      "Yang Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24au.html": {
    "title": "A Dual-module Framework for Counterfactual Estimation over Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wang",
      "Shengfei Lyu",
      "Lishan Yang",
      "Yibing Zhan",
      "Huanhuan Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24av.html": {
    "title": "MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangyu Wang",
      "Gengchen Mai",
      "Krzysztof Janowicz",
      "Ni Lao"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24aw.html": {
    "title": "EvoluNet: Advancing Dynamic Non-IID Transfer Learning on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haohui Wang",
      "Yuzhen Mao",
      "Yujun Yan",
      "Yaoqing Yang",
      "Jianhui Sun",
      "Kevin Choi",
      "Balaji Veeramani",
      "Alison Hu",
      "Edward Bowen",
      "Tyler Cody",
      "Dawei Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ax.html": {
    "title": "Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingze Wang",
      "Zeping Min",
      "Lei Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ay.html": {
    "title": "Transforming and Combining Rewards for Aligning Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Wang",
      "Chirag Nagpal",
      "Jonathan Berant",
      "Jacob Eisenstein",
      "Alexander Nicholas D’Amour",
      "Sanmi Koyejo",
      "Victor Veitch"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24az.html": {
    "title": "TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiruo Wang",
      "Graham Neubig",
      "Daniel Fried"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ba.html": {
    "title": "More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Wang",
      "Owen Oertell",
      "Alekh Agarwal",
      "Nathan Kallus",
      "Wen Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bb.html": {
    "title": "Pi-DUAL: Using privileged information to distinguish clean from noisy labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Wang",
      "Guillermo Ortiz-Jimenez",
      "Rodolphe Jenatton",
      "Mark Collier",
      "Efi Kokiopoulou",
      "Pascal Frossard"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bc.html": {
    "title": "Sample Average Approximation for Conditional Stochastic Optimization with Dependent Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yafei Wang",
      "Bo Pan",
      "Mei Li",
      "Jianya Lu",
      "Lingchen Kong",
      "Bei Jiang",
      "Linglong Kong"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bd.html": {
    "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boxin Wang",
      "Wei Ping",
      "Lawrence Mcafee",
      "Peng Xu",
      "Bo Li",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24be.html": {
    "title": "A Fine-grained Analysis of Fitted Q-evaluation: Beyond Parametric Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Wang",
      "Zhengling Qi",
      "Raymond K. W. Wong"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bf.html": {
    "title": "Probabilistic Constrained Reinforcement Learning with Formal Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanran Wang",
      "Qiuchen Qian",
      "David Boyle"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bg.html": {
    "title": "Efficient Online Set-valued Classification with Bandit Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Wang",
      "Xingye Qiao"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bh.html": {
    "title": "LLM-Empowered State Representation for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyuan Wang",
      "Yun Qu",
      "Yuhang Jiang",
      "Jianzhun Shao",
      "Chang Liu",
      "Wenming Yang",
      "Xiangyang Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bi.html": {
    "title": "Proteus: Exploring Protein Structure Generation for Enhanced Designability and Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chentong Wang",
      "Yannan Qu",
      "Zhangzhi Peng",
      "Yukai Wang",
      "Hongli Zhu",
      "Dachuan Chen",
      "Longxing Cao"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bj.html": {
    "title": "How to Trace Latent Generative Model Generated Images without Artificial Watermark?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenting Wang",
      "Vikash Sehwag",
      "Chen Chen",
      "Lingjuan Lyu",
      "Dimitris N. Metaxas",
      "Shiqing Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bk.html": {
    "title": "Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caixing Wang",
      "Ziliang Shen"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bl.html": {
    "title": "Generalization Analysis of Stochastic Weight Averaging with General Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wang",
      "Li Shen",
      "Zerui Tao",
      "Shuaida He",
      "Dacheng Tao"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bm.html": {
    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Qianli Shen",
      "Yao Tong",
      "Yang Zhang",
      "Kenji Kawaguchi"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bn.html": {
    "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Wang",
      "Zhanyi Sun",
      "Jesse Zhang",
      "Zhou Xian",
      "Erdem Biyik",
      "David Held",
      "Zackory Erickson"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bo.html": {
    "title": "Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengyi Wang",
      "Shiwei Tan",
      "Hao Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bp.html": {
    "title": "Integrated Hardware Architecture and Device Placement Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Irene Wang",
      "Jakub Tarnawski",
      "Amar Phanishayee",
      "Divya Mahajan"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bq.html": {
    "title": "AD3: Implicit Action is the Key for World Models to Distinguish the Diverse Visual Distractors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucen Wang",
      "Shenghua Wan",
      "Le Gan",
      "Shuai Feng",
      "De-Chuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24br.html": {
    "title": "Benchmarking Deletion Metrics with the Principled Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yipei Wang",
      "Xiaoqian Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bs.html": {
    "title": "CW Complex Hypothesis for Image Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wang",
      "Zhiren Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bt.html": {
    "title": "Optimal Exact Recovery in Semi-Supervised Learning: A Study of Spectral Methods and Graph Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haixiao Wang",
      "Zhichao Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bu.html": {
    "title": "A New Theoretical Perspective on Data Heterogeneity in Federated Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Wang",
      "Shiqiang Wang",
      "Rong-Rong Chen",
      "Mingyue Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bv.html": {
    "title": "FADAS: Towards Federated Adaptive Asynchronous Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujia Wang",
      "Shiqiang Wang",
      "Songtao Lu",
      "Jinghui Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bw.html": {
    "title": "Open-Vocabulary Calibration for Fine-tuned CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuoyuan Wang",
      "Jindong Wang",
      "Guoqing Wang",
      "Bob Zhang",
      "Kaiyang Zhou",
      "Hongxin Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bx.html": {
    "title": "Probabilistic Subgoal Representations for Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivienne Huiling Wang",
      "Tinghuai Wang",
      "Wenyan Yang",
      "Joni-Kristian Kamarainen",
      "Joni Pajarinen"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24by.html": {
    "title": "Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible Function Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yudan Wang",
      "Yue Wang",
      "Yi Zhou",
      "Shaofeng Zou"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24bz.html": {
    "title": "A Hierarchical Adaptive Multi-Task Reinforcement Learning Framework for Multiplier Circuit Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihai Wang",
      "Jie Wang",
      "Dongsheng Zuo",
      "Ji Yunjie",
      "Xilin Xia",
      "Yuzhe Ma",
      "Jianye Hao",
      "Mingxuan Yuan",
      "Yongdong Zhang",
      "Feng Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ca.html": {
    "title": "Transformers Provably Learn Sparse Token Selection While Fully-Connected Nets Cannot",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Wang",
      "Stanley Wei",
      "Daniel Hsu",
      "Jason D. Lee"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cb.html": {
    "title": "Defense against Model Extraction Attack by Bayesian Active Watermarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyi Wang",
      "Yihan Wu",
      "Heng Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cc.html": {
    "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Wang",
      "Zhou Xian",
      "Feng Chen",
      "Tsun-Hsuan Wang",
      "Yian Wang",
      "Katerina Fragkiadaki",
      "Zackory Erickson",
      "David Held",
      "Chuang Gan"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cd.html": {
    "title": "Quality-Diversity with Limited Resources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ren-Jian Wang",
      "Ke Xue",
      "Cong Guan",
      "Chao Qian"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ce.html": {
    "title": "Probability Distribution of Hypervolume Improvement in Bi-objective Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Kaifeng Yang",
      "Michael Affenzeller"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cf.html": {
    "title": "Vision Transformers as Probabilistic Expansion from Learngene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiufeng Wang",
      "Xu Yang",
      "Haokun Chen",
      "Xin Geng"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cg.html": {
    "title": "Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen T. Wang",
      "Tianji Yang",
      "James Zou",
      "Yongchan Kwon",
      "Ruoxi Jia"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ch.html": {
    "title": "Can Gaussian Sketching Converge Faster on a Preconditioned Landscape?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilong Wang",
      "Haishan Ye",
      "Guang Dai",
      "Ivor Tsang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ci.html": {
    "title": "Autaptic Synaptic Circuit Enhances Spatio-temporal Predictive Learning of Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihao Wang",
      "Zhaofei Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cj.html": {
    "title": "Learning with Adaptive Resource Allocation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "Miao Yu",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ck.html": {
    "title": "Taylor Videos for Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Xiuyuan Yuan",
      "Tom Gedeon",
      "Liang Zheng"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cl.html": {
    "title": "An Empirical Study of Realized GNN Expressiveness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbo Wang",
      "Muhan Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cm.html": {
    "title": "Calibration Bottleneck: Over-compressed Representations are Less Calibratable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deng-Bao Wang",
      "Min-Ling Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cn.html": {
    "title": "Benign Overfitting in Adversarial Training of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunjuan Wang",
      "Kaibo Zhang",
      "Raman Arora"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24co.html": {
    "title": "FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehan Wang",
      "Ziang Zhang",
      "Xize Cheng",
      "Rongjie Huang",
      "Luping Liu",
      "Zhenhui Ye",
      "Haifeng Huang",
      "Yang Zhao",
      "Tao Jin",
      "Peng Gao",
      "Zhou Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cp.html": {
    "title": "Exploring Intrinsic Dimension for Vision-Language Model Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhang Wang",
      "Jiawen Zhang",
      "Qingyuan Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cq.html": {
    "title": "Knowledge-aware Reinforced Language Models for Protein Directed Evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Wang",
      "Qiang Zhang",
      "Ming Qin",
      "Xiang Zhuang",
      "Xiaotong Li",
      "Zhichen Gong",
      "Zeyuan Wang",
      "Yu Zhao",
      "Jianhua Yao",
      "Keyan Ding",
      "Huajun Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cr.html": {
    "title": "Boximator: Generating Rich and Controllable Motions for Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Wang",
      "Yuchen Zhang",
      "Jiaxin Zou",
      "Yan Zeng",
      "Guoqiang Wei",
      "Liping Yuan",
      "Hang Li"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cs.html": {
    "title": "Bridging Model Heterogeneity in Federated Learning via Uncertainty-based Asymmetrical Reciprocity Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Wang",
      "Chenxu Zhao",
      "Lingjuan Lyu",
      "Quanzeng You",
      "Mengdi Huai",
      "Fenglong Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24ct.html": {
    "title": "Diffusion Language Models Are Versatile Protein Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyou Wang",
      "Zaixiang Zheng",
      "Fei Ye",
      "Dongyu Xue",
      "Shujian Huang",
      "Quanquan Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cu.html": {
    "title": "Neural Collapse meets Differential Privacy: Curious behaviors of NoisyGD with Near-Perfect Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chendi Wang",
      "Yuqing Zhu",
      "Weijie J Su",
      "Yu-Xiang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/ziqi24a.html": {
    "title": "Batch Singular Value Polarization and Weighted Semantic Augmentation for Universal Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Ziqi",
      "Wei Wang",
      "Chao Huang",
      "Jie Wen",
      "Cong Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wayland24a.html": {
    "title": "Mapping the Multiverse of Latent Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Wayland",
      "Corinna Coupette",
      "Bastian Rieck"
    ]
  },
  "https://proceedings.mlr.press/v235/weber24a.html": {
    "title": "Reinforcement Learning and Regret Bounds for Admission Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Weber",
      "Ana Busic",
      "Jiamin Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/weber24b.html": {
    "title": "Learning to Compile Programs to Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Logan Weber",
      "Jesse Michel",
      "Alex Renda",
      "Michael Carbin"
    ]
  },
  "https://proceedings.mlr.press/v235/wedenig24a.html": {
    "title": "Exact Soft Analytical Side-Channel Attacks using Tractable Circuits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Wedenig",
      "Rishub Nagpal",
      "Gaëtan Cassiers",
      "Stefan Mangard",
      "Robert Peharz"
    ]
  },
  "https://proceedings.mlr.press/v235/wei24a.html": {
    "title": "Exploiting Human-AI Dependence for Learning to Defer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixi Wei",
      "Yuzhou Cao",
      "Lei Feng"
    ]
  },
  "https://proceedings.mlr.press/v235/wei24b.html": {
    "title": "Learning Pseudo-Contractive Denoisers for Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deliang Wei",
      "Peng Chen",
      "Fang Li"
    ]
  },
  "https://proceedings.mlr.press/v235/wei24c.html": {
    "title": "Rethinking Generative Large Language Model Evaluation for Semantic Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyun Wei",
      "Xi Chen",
      "Lin Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/wei24d.html": {
    "title": "MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yake Wei",
      "Di Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/wei24e.html": {
    "title": "Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongxian Wei",
      "Zixuan Hu",
      "Li Shen",
      "Zhenyi Wang",
      "Yu Li",
      "Chun Yuan",
      "Dacheng Tao"
    ]
  },
  "https://proceedings.mlr.press/v235/wei24f.html": {
    "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyi Wei",
      "Kaixuan Huang",
      "Yangsibo Huang",
      "Tinghao Xie",
      "Xiangyu Qi",
      "Mengzhou Xia",
      "Prateek Mittal",
      "Mengdi Wang",
      "Peter Henderson"
    ]
  },
  "https://proceedings.mlr.press/v235/wei24g.html": {
    "title": "Learning Label Shift Correction for Test-Agnostic Long-Tailed Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wei",
      "Zhen Mao",
      "Zi-Hao Zhou",
      "Yuanyu Wan",
      "Min-Ling Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/wei24h.html": {
    "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Wei",
      "Zhe Wang",
      "Jiawei Liu",
      "Yifeng Ding",
      "Lingming Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/wei24i.html": {
    "title": "Extending Test-Time Augmentation with Metamorphic Relations for Combinatorial Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siwei Wei",
      "Xudong Zhang",
      "Zhiyang Zhou",
      "Yan Cai"
    ]
  },
  "https://proceedings.mlr.press/v235/weissburg24a.html": {
    "title": "Position: AI/ML Influencers Have a Place in the Academic Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iain Weissburg",
      "Mehir Arora",
      "Xinyi Wang",
      "Liangming Pan",
      "William Yang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/weissenbacher24a.html": {
    "title": "SiT: Symmetry-invariant Transformers for Generalisation in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias Weissenbacher",
      "Rishabh Agarwal",
      "Yoshinobu Kawahara"
    ]
  },
  "https://proceedings.mlr.press/v235/wen24a.html": {
    "title": "Contrastive Representation for Data Filtering in Cross-Domain Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Wen",
      "Chenjia Bai",
      "Kang Xu",
      "Xudong Yu",
      "Yang Zhang",
      "Xuelong Li",
      "Zhen Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wen24b.html": {
    "title": "Cross-domain Open-world Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Wen",
      "Maria Brbic"
    ]
  },
  "https://proceedings.mlr.press/v235/wen24c.html": {
    "title": "Diffusion-based Missing-view Generation With the Application on Incomplete Multi-view Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Wen",
      "Shijie Deng",
      "Waikeung Wong",
      "Guoqing Chao",
      "Chao Huang",
      "Lunke Fei",
      "Yong Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/wen24d.html": {
    "title": "Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiao Wen",
      "Arthur Jacot"
    ]
  },
  "https://proceedings.mlr.press/v235/wen24e.html": {
    "title": "From Coarse to Fine: Enable Comprehensive Graph Self-supervised Learning with Multi-granular Semantic Ensemble",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianlong Wen",
      "Mingxuan Ju",
      "Zhongyu Ouyang",
      "Chuxu Zhang",
      "Yanfang Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/wen24f.html": {
    "title": "Provable Contrastive Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Wen",
      "Zhiquan Tan",
      "Kaipeng Zheng",
      "Chuanlong Xie",
      "Weiran Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/wenliang24a.html": {
    "title": "Distributional Bellman Operators over Mean Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Kevin Wenliang",
      "Gregoire Deletang",
      "Matthew Aitchison",
      "Marcus Hutter",
      "Anian Ruoss",
      "Arthur Gretton",
      "Mark Rowland"
    ]
  },
  "https://proceedings.mlr.press/v235/westerhout24a.html": {
    "title": "On the Asymptotic Distribution of the Minimum Empirical Risk",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Westerhout",
      "Trungtin Nguyen",
      "Xin Guo",
      "Hien Duy Nguyen"
    ]
  },
  "https://proceedings.mlr.press/v235/westny24a.html": {
    "title": "Stability-Informed Initialization of Neural Ordinary Differential Equations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theodor Westny",
      "Arman Mohammadi",
      "Daniel Jung",
      "Erik Frisk"
    ]
  },
  "https://proceedings.mlr.press/v235/wettig24a.html": {
    "title": "QuRating: Selecting High-Quality Data for Training Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Wettig",
      "Aatmik Gupta",
      "Saumya Malik",
      "Danqi Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/wilkins-reeves24a.html": {
    "title": "Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Wilkins-Reeves",
      "Xu Chen",
      "Qi Ma",
      "Christine Agarwal",
      "Aude Hofleitner"
    ]
  },
  "https://proceedings.mlr.press/v235/wiltzer24a.html": {
    "title": "A Distributional Analogue to the Successor Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harley Wiltzer",
      "Jesse Farebrother",
      "Arthur Gretton",
      "Yunhao Tang",
      "Andre Barreto",
      "Will Dabney",
      "Marc G Bellemare",
      "Mark Rowland"
    ]
  },
  "https://proceedings.mlr.press/v235/winkler24a.html": {
    "title": "Bridging discrete and continuous state spaces: Exploring the Ehrenfest process in time-continuous diffusion models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ludwig Winkler",
      "Lorenz Richter",
      "Manfred Opper"
    ]
  },
  "https://proceedings.mlr.press/v235/wolczyk24a.html": {
    "title": "Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maciej Wolczyk",
      "Bartłomiej Cupiał",
      "Mateusz Ostaszewski",
      "Michał Bortkiewicz",
      "Michał Zając",
      "Razvan Pascanu",
      "Łukasz Kuciński",
      "Piotr Miłoś"
    ]
  },
  "https://proceedings.mlr.press/v235/wolf24a.html": {
    "title": "Fundamental Limitations of Alignment in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yotam Wolf",
      "Noam Wies",
      "Oshri Avnery",
      "Yoav Levine",
      "Amnon Shashua"
    ]
  },
  "https://proceedings.mlr.press/v235/wollschlager24a.html": {
    "title": "Expressivity and Generalization: Fragment-Biases for Molecular GNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Wollschläger",
      "Niklas Kemper",
      "Leon Hetzel",
      "Johanna Sommer",
      "Stephan Günnemann"
    ]
  },
  "https://proceedings.mlr.press/v235/woo24a.html": {
    "title": "Unified Training of Universal Time Series Forecasting Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gerald Woo",
      "Chenghao Liu",
      "Akshat Kumar",
      "Caiming Xiong",
      "Silvio Savarese",
      "Doyen Sahoo"
    ]
  },
  "https://proceedings.mlr.press/v235/woo24b.html": {
    "title": "Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiin Woo",
      "Laixi Shi",
      "Gauri Joshi",
      "Yuejie Chi"
    ]
  },
  "https://proceedings.mlr.press/v235/woodruff24a.html": {
    "title": "Coresets for Multiple $\\ell_p$ Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Woodruff",
      "Taisuke Yasuda"
    ]
  },
  "https://proceedings.mlr.press/v235/woodruff24b.html": {
    "title": "Reweighted Solutions for Weighted Low Rank Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Woodruff",
      "Taisuke Yasuda"
    ]
  },
  "https://proceedings.mlr.press/v235/wouters24a.html": {
    "title": "Optimizing Watermarks for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bram Wouters"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24a.html": {
    "title": "Repoformer: Selective Retrieval for Repository-Level Code Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Wasi Uddin Ahmad",
      "Dejiao Zhang",
      "Murali Krishna Ramanathan",
      "Xiaofei Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24b.html": {
    "title": "Theoretical insights for diffusion guidance: A case study for Gaussian mixture models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Wu",
      "Minshuo Chen",
      "Zihao Li",
      "Mengdi Wang",
      "Yuting Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24c.html": {
    "title": "Adaptive Accompaniment with ReaLchords",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusong Wu",
      "Tim Cooijmans",
      "Kyle Kastner",
      "Adam Roberts",
      "Ian Simon",
      "Alexander Scarlatos",
      "Chris Donahue",
      "Cassie Tarakajian",
      "Shayegan Omidshafiei",
      "Aaron Courville",
      "Pablo Samuel Castro",
      "Natasha Jaques",
      "Cheng-Zhi Anna Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24d.html": {
    "title": "Ditto: Quantization-aware Secure Inference of Transformers upon MPC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqi Wu",
      "Wenjing Fang",
      "Yancheng Zheng",
      "Junming Ma",
      "Jin Tan",
      "Lei Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24e.html": {
    "title": "NExT-GPT: Any-to-Any Multimodal LLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Leigang Qu",
      "Wei Ji",
      "Tat-Seng Chua"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24f.html": {
    "title": "Understanding Stochastic Natural Gradient Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Wu",
      "Jacob R. Gardner"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24g.html": {
    "title": "FAFE: Immune Complex Modeling with Geodesic Distance Loss on Noisy Group Frames",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruidong Wu",
      "Ruihan Guo",
      "Rui Wang",
      "Shitong Luo",
      "Yue Xu",
      "Jiahan Li",
      "Jianzhu Ma",
      "Qiang Liu",
      "Yunan Luo",
      "Jian Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24h.html": {
    "title": "A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Wu",
      "Zhengmian Hu",
      "Junfeng Guo",
      "Hongyang Zhang",
      "Heng Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24i.html": {
    "title": "Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dennis Wu",
      "Jerry Yao-Chieh Hu",
      "Teng-Yun Hsiao",
      "Han Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24j.html": {
    "title": "Planning, Fast and Slow: Online Reinforcement Learning with Action-Free Offline Data via Multiscale Planners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengjie Wu",
      "Hao Hu",
      "Yiqin Yang",
      "Ning Zhang",
      "Chongjie Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24k.html": {
    "title": "PointMC: Multi-instance Point Cloud Registration based on Maximal Cliques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Wu",
      "Xidao Hu",
      "Yongzhe Yuan",
      "Xiaolong Fan",
      "Maoguo Gong",
      "Hao Li",
      "Mingyang Zhang",
      "Qiguang Miao",
      "Wenping Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24l.html": {
    "title": "Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Wu",
      "Jiayi Ji",
      "Oucheng Huang",
      "Jiale Li",
      "Yuhang Wu",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24m.html": {
    "title": "Borda Regret Minimization for Generalized Linear Dueling Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Wu",
      "Tao Jin",
      "Qiwei Di",
      "Hao Lou",
      "Farzad Farnoud",
      "Quanquan Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24n.html": {
    "title": "DISCRET: Synthesizing Faithful Explanations For Treatment Effect Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinjun Wu",
      "Mayank Keoliya",
      "Kan Chen",
      "Neelay Velingker",
      "Ziyang Li",
      "Emily J Getzen",
      "Qi Long",
      "Mayur Naik",
      "Ravi B Parikh",
      "Eric Wong"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24o.html": {
    "title": "Surface-VQMAE: Vector-quantized Masked Auto-encoders on Molecular Surfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Wu",
      "Stan Z. Li"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24p.html": {
    "title": "Learning Causal Relations from Subsampled Time Series with Two Time-Slices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anpeng Wu",
      "Haoxuan Li",
      "Kun Kuang",
      "Zhang Keli",
      "Fei Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24q.html": {
    "title": "AND: Audio Network Dissection for Interpreting Deep Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tung-Yu Wu",
      "Yu-Xiang Lin",
      "Tsui-Wei Weng"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24r.html": {
    "title": "Transolver: A Fast Transformer Solver for PDEs on General Geometries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haixu Wu",
      "Huakun Luo",
      "Haowen Wang",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24s.html": {
    "title": "Confidence-aware Contrastive Learning for Selective Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Chang Wu",
      "Shen-Huan Lyu",
      "Haopu Shang",
      "Xiangyu Wang",
      "Chao Qian"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24t.html": {
    "title": "Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Young Wu",
      "Jeremy Mcmahan",
      "Yiding Chen",
      "Yudong Chen",
      "Jerry Zhu",
      "Qiaomin Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24u.html": {
    "title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengying Wu",
      "Yao Mu",
      "Bingxian Wu",
      "Yi Hou",
      "Ji Ma",
      "Shanghang Zhang",
      "Chang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24v.html": {
    "title": "Learning Divergence Fields for Shift-Robust Graph Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitian Wu",
      "Fan Nie",
      "Chenxiao Yang",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24w.html": {
    "title": "Profile Reconstruction from Private Sketches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wu",
      "Rasmus Pagh"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24x.html": {
    "title": "Policy Learning for Balancing Short-Term and Long-Term Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wu",
      "Ziyu Shen",
      "Feng Xie",
      "Wang Zhongyao",
      "Chunchen Liu",
      "Yan Zeng"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24y.html": {
    "title": "Learning to Predict Mutational Effects of Protein-Protein Interactions by Microenvironment-aware Hierarchical Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lirong Wu",
      "Yijun Tian",
      "Haitao Lin",
      "Yufei Huang",
      "Siyuan Li",
      "Nitesh V Chawla",
      "Stan Z. Li"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24z.html": {
    "title": "A Theory of Fault-Tolerant Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changlong Wu",
      "Yifan Wang",
      "Ananth Grama"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24aa.html": {
    "title": "Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wu",
      "Huiyuan Wang",
      "Kun Wang",
      "Weiyan Wang",
      "Changan Ye",
      "Yangyu Tao",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Xiao Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24ab.html": {
    "title": "Mitigating Catastrophic Forgetting in Online Continual Learning by Modeling Previous Task Interrelations via Pareto Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Wu",
      "Hong Wang",
      "Peilin Zhao",
      "Yefeng Zheng",
      "Ying Wei",
      "Long-Kai Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24ac.html": {
    "title": "Detecting Any instruction-to-answer interaction relationship:Universal Instruction-to-Answer Navigator for Med-VQA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongze Wu",
      "Hongyan Xu",
      "Yitian Long",
      "Shan You",
      "Xiu Su",
      "Jun Long",
      "Yueyi Luo",
      "Chang Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24ad.html": {
    "title": "Unraveling the Impact of Heterophilic Structures on Graph Positive-Unlabeled Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Wu",
      "Jiangchao Yao",
      "Bo Han",
      "Lina Yao",
      "Tongliang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24ae.html": {
    "title": "Mitigating Label Noise on Graphs via Topological Sample Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Wu",
      "Jiangchao Yao",
      "Xiaobo Xia",
      "Jun Yu",
      "Ruxin Wang",
      "Bo Han",
      "Tongliang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24af.html": {
    "title": "Boosting Reinforcement Learning with Strongly Delayed Feedback Through Auxiliary Short Delays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Wu",
      "Simon Sinong Zhan",
      "Yixuan Wang",
      "Yuhui Wang",
      "Chung-Wei Lin",
      "Chen Lv",
      "Qi Zhu",
      "Jürgen Schmidhuber",
      "Chao Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24ag.html": {
    "title": "HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Wu",
      "Yifan Zhang",
      "Zhenxing Liang",
      "Jian Cheng"
    ]
  },
  "https://proceedings.mlr.press/v235/wu24ah.html": {
    "title": "Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoning Wu",
      "Zicheng Zhang",
      "Weixia Zhang",
      "Chaofeng Chen",
      "Liang Liao",
      "Chunyi Li",
      "Yixuan Gao",
      "Annan Wang",
      "Erli Zhang",
      "Wenxiu Sun",
      "Qiong Yan",
      "Xiongkuo Min",
      "Guangtao Zhai",
      "Weisi Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/xi24a.html": {
    "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Xi",
      "Wenxiang Chen",
      "Boyang Hong",
      "Senjie Jin",
      "Rui Zheng",
      "Wei He",
      "Yiwen Ding",
      "Shichun Liu",
      "Xin Guo",
      "Junzhe Wang",
      "Honglin Guo",
      "Wei Shen",
      "Xiaoran Fan",
      "Yuhao Zhou",
      "Shihan Dou",
      "Xiao Wang",
      "Xinbo Zhang",
      "Peng Sun",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/xi24b.html": {
    "title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haocheng Xi",
      "Yuxiang Chen",
      "Kang Zhao",
      "Kai Jun Teh",
      "Jianfei Chen",
      "Jun Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/xia24a.html": {
    "title": "Unbiased Multi-Label Learning from Crowdsourced Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxuan Xia",
      "Zenan Huang",
      "Runze Wu",
      "Gengyu Lyu",
      "Junbo Zhao",
      "Gang Chen",
      "Haobo Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/xia24b.html": {
    "title": "Refined Coreset Selection: Towards Minimal Coreset Size under Model Performance Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobo Xia",
      "Jiale Liu",
      "Shaokun Zhang",
      "Qingyun Wu",
      "Hongxin Wei",
      "Tongliang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/xia24c.html": {
    "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengzhou Xia",
      "Sadhika Malladi",
      "Suchin Gururangan",
      "Sanjeev Arora",
      "Danqi Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/xia24d.html": {
    "title": "SMaRt: Improving GANs with Score Matching Regularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengfei Xia",
      "Yujun Shen",
      "Ceyuan Yang",
      "Ran Yi",
      "Wenping Wang",
      "Yong-Jin Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/xia24e.html": {
    "title": "Contrastive Learning for Clinical Outcome Prediction with Partial Data Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Xia",
      "Jonathan Wilson",
      "Benjamin Goldstein",
      "Ricardo Henao"
    ]
  },
  "https://proceedings.mlr.press/v235/xia24f.html": {
    "title": "Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Xia",
      "Xianliang Yang",
      "Zichuan Liu",
      "Zhihao Liu",
      "Lei Song",
      "Jiang Bian"
    ]
  },
  "https://proceedings.mlr.press/v235/xian24a.html": {
    "title": "Delving into the Convergence of Generalized Smooth Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhan Xian",
      "Ziyi Chen",
      "Heng Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/xian24b.html": {
    "title": "Differentially Private Post-Processing for Fair Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruicheng Xian",
      "Qiaobo Li",
      "Gautam Kamath",
      "Han Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/xiang24a.html": {
    "title": "Towards Neural Architecture Search through Hierarchical Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lichuan Xiang",
      "Łukasz Dudziak",
      "Mohamed S Abdelfattah",
      "Abhinav Mehrotra",
      "Nicholas Donald Lane",
      "Hongkai Wen"
    ]
  },
  "https://proceedings.mlr.press/v235/xiao24a.html": {
    "title": "Towards a Self-contained Data-driven Global Weather Forecasting Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Xiao",
      "Lei Bai",
      "Wei Xue",
      "Hao Chen",
      "Kun Chen",
      "Kang Chen",
      "Tao Han",
      "Wanli Ouyang"
    ]
  },
  "https://proceedings.mlr.press/v235/xiao24b.html": {
    "title": "Category-Aware Active Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxiao Xiao",
      "Jiuxiang Gu",
      "Hongfu Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/xiao24c.html": {
    "title": "Improved Operator Learning by Orthogonal Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Xiao",
      "Zhongkai Hao",
      "Bokai Lin",
      "Zhijie Deng",
      "Hang Su"
    ]
  },
  "https://proceedings.mlr.press/v235/xiao24d.html": {
    "title": "Improving Transformers with Dynamically Composable Multi-Head Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Xiao",
      "Qingye Meng",
      "Shengping Li",
      "Xingyuan Yuan"
    ]
  },
  "https://proceedings.mlr.press/v235/xiao24e.html": {
    "title": "Uniformly Stable Algorithms for Adversarial Training and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiancong Xiao",
      "Jiawei Zhang",
      "Zhi-Quan Luo",
      "Asuman E. Ozdaglar"
    ]
  },
  "https://proceedings.mlr.press/v235/xiao24f.html": {
    "title": "Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqing Xiao",
      "Yixin Zhu",
      "Di He",
      "Zhouchen Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/xiao24g.html": {
    "title": "Efficient Contrastive Learning for Fast and Accurate Inference on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Xiao",
      "Huaisheng Zhu",
      "Zhiwei Zhang",
      "Zhimeng Guo",
      "Charu C. Aggarwal",
      "Suhang Wang",
      "Vasant G Honavar"
    ]
  },
  "https://proceedings.mlr.press/v235/xiao24h.html": {
    "title": "CCM: Real-Time Controllable Visual Content Creation Using Text-to-Image Consistency Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Xiao",
      "Kai Zhu",
      "Han Zhang",
      "Zhiheng Liu",
      "Yujun Shen",
      "Zhantao Yang",
      "Ruili Feng",
      "Yu Liu",
      "Xueyang Fu",
      "Zheng-Jun Zha"
    ]
  },
  "https://proceedings.mlr.press/v235/xiaofan24a.html": {
    "title": "Intersecting-Boundary-Sensitive Fingerprinting for Tampering Detection of DNN Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bai Xiaofan",
      "Chaoxiang He",
      "Xiaojing Ma",
      "Bin Benjamin Zhu",
      "Hai Jin"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24a.html": {
    "title": "Learning to Explore in POMDPs with Informational Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Annie Xie",
      "Logan Mondal Bhamidipaty",
      "Evan Zheran Liu",
      "Joey Hong",
      "Sergey Levine",
      "Chelsea Finn"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24b.html": {
    "title": "Automating the Selection of Proxy Variables of Unmeasured Confounders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Xie",
      "Zhengming Chen",
      "Shanshan Luo",
      "Wang Miao",
      "Ruichu Cai",
      "Zhi Geng"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24c.html": {
    "title": "FedREDefense: Defending against Model Poisoning Attacks for Federated Learning using Model Update Reconstruction Error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueqi Xie",
      "Minghong Fang",
      "Neil Zhenqiang Gong"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24d.html": {
    "title": "Improving SAM Requires Rethinking its Optimization Formulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyun Xie",
      "Fabian Latorre",
      "Kimon Antonakopoulos",
      "Thomas Pethick",
      "Volkan Cevher"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24e.html": {
    "title": "Implicit Bias of AdamW: $\\ell_∞$-Norm Constrained Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Xie",
      "Zhiyuan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24f.html": {
    "title": "Local Causal Structure Learning in the Presence of Latent Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Xie",
      "Zheng Li",
      "Peng Wu",
      "Yan Zeng",
      "Chunchen Liu",
      "Zhi Geng"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24g.html": {
    "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chulin Xie",
      "Zinan Lin",
      "Arturs Backurs",
      "Sivakanth Gopi",
      "Da Yu",
      "Huseyin A Inan",
      "Harsha Nori",
      "Haotian Jiang",
      "Huishuai Zhang",
      "Yin Tat Lee",
      "Bo Li",
      "Sergey Yekhanin"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24h.html": {
    "title": "MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luyuan Xie",
      "Manqing Lin",
      "Tianyu Luan",
      "Cong Li",
      "Yuejian Fang",
      "Qingni Shen",
      "Zhonghai Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24i.html": {
    "title": "Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming-Kun Xie",
      "Jia-Hao Xiao",
      "Pei Peng",
      "Gang Niu",
      "Masashi Sugiyama",
      "Sheng-Jun Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24j.html": {
    "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Xie",
      "Kai Zhang",
      "Jiangjie Chen",
      "Tinghui Zhu",
      "Renze Lou",
      "Yuandong Tian",
      "Yanghua Xiao",
      "Yu Su"
    ]
  },
  "https://proceedings.mlr.press/v235/xie24k.html": {
    "title": "Reflected Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Xie",
      "Yu Zhu",
      "Longlin Yu",
      "Tong Yang",
      "Ziheng Cheng",
      "Shiyue Zhang",
      "Xiangyu Zhang",
      "Cheng Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/xing24a.html": {
    "title": "Federated Neuro-Symbolic Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengwei Xing",
      "Songtao Lu",
      "Han Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/xing24b.html": {
    "title": "Less is More: on the Over-Globalizing Problem in Graph Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Xing",
      "Xiao Wang",
      "Yibo Li",
      "Hai Huang",
      "Chuan Shi"
    ]
  },
  "https://proceedings.mlr.press/v235/xing24c.html": {
    "title": "HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanxiang Xing",
      "Haixu Wu",
      "Yuezhou Ma",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://proceedings.mlr.press/v235/xing24d.html": {
    "title": "SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingrun Xing",
      "Zheng Zhang",
      "Ziyi Ni",
      "Shitao Xiao",
      "Yiming Ju",
      "Siqi Fan",
      "Yequan Wang",
      "Jiajun Zhang",
      "Guoqi Li"
    ]
  },
  "https://proceedings.mlr.press/v235/xiong24a.html": {
    "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Xiong",
      "Hanze Dong",
      "Chenlu Ye",
      "Ziqi Wang",
      "Han Zhong",
      "Heng Ji",
      "Nan Jiang",
      "Tong Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/xiong24b.html": {
    "title": "Provably Efficient Reinforcement Learning for Adversarial Restless Multi-Armed Bandits with Unknown Transitions and Bandit Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guojun Xiong",
      "Jian Li"
    ]
  },
  "https://proceedings.mlr.press/v235/xiong24c.html": {
    "title": "Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Xiong",
      "Risto Vuorio",
      "Jacob Beck",
      "Matthieu Zimmer",
      "Kun Shao",
      "Shimon Whiteson"
    ]
  },
  "https://proceedings.mlr.press/v235/xiong24d.html": {
    "title": "A General Framework for Sequential Decision-Making under Adaptivity Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuoya Xiong",
      "Zhaoran Wang",
      "Zhuoran Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24a.html": {
    "title": "Parameter-Dependent Competitive Analysis for Online Capacitated Coverage Maximization through Boostings and Attenuations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pan Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24b.html": {
    "title": "Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Xu",
      "Andrzej Banburski",
      "Nebojsa Jojic"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24c.html": {
    "title": "Stochastic Bandits with ReLU Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kan Xu",
      "Hamsa Bastani",
      "Surbhi Goel",
      "Osbert Bastani"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24d.html": {
    "title": "Intersectional Unfairness Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gezheng Xu",
      "Qi Chen",
      "Charles Ling",
      "Boyu Wang",
      "Changjian Shui"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24e.html": {
    "title": "Semantic-Aware Human Object Interaction Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Xu",
      "Qingchao Chen",
      "Yuxin Peng",
      "Yang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24f.html": {
    "title": "DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Xu",
      "Gabriele Corso",
      "Tommi Jaakkola",
      "Arash Vahdat",
      "Karsten Kreis"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24g.html": {
    "title": "Adapting Static Fairness to Sequential Decision-Making: Bias Mitigation Strategies towards Equal Long-term Benefit Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuancheng Xu",
      "Chenghao Deng",
      "Yanchao Sun",
      "Ruijie Zheng",
      "Xiyao Wang",
      "Jieyu Zhao",
      "Furong Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24h.html": {
    "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shusheng Xu",
      "Wei Fu",
      "Jiaxuan Gao",
      "Wenjie Ye",
      "Weilin Liu",
      "Zhiyu Mei",
      "Guangju Wang",
      "Chao Yu",
      "Yi Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24i.html": {
    "title": "Practical Hamiltonian Monte Carlo on Riemannian Manifolds via Relativity Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Xu",
      "Hong Ge"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24j.html": {
    "title": "Equivariant Graph Neural Operator for Modeling 3D Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minkai Xu",
      "Jiaqi Han",
      "Aaron Lou",
      "Jean Kossaifi",
      "Arvind Ramanathan",
      "Kamyar Azizzadenesheli",
      "Jure Leskovec",
      "Stefano Ermon",
      "Anima Anandkumar"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24k.html": {
    "title": "Aligned Objective for Soft-Pseudo-Label Generation in Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Xu",
      "Yihao Hu",
      "Congyu Qiao",
      "Xin Geng"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24l.html": {
    "title": "BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenwei Xu",
      "Yu-Chao Huang",
      "Jerry Yao-Chieh Hu",
      "Weijian Li",
      "Ammar Gilani",
      "Hsi-Sheng Goan",
      "Han Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24m.html": {
    "title": "Conformal prediction for multi-dimensional time series by ellipsoidal sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Xu",
      "Hanyang Jiang",
      "Yao Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24n.html": {
    "title": "Enhancing Vision Transformer: Amplifying Non-Linearity in Feedforward Network Module",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixing Xu",
      "Chao Li",
      "Dong Li",
      "Xiao Sheng",
      "Fan Jiang",
      "Lu Tian",
      "Ashish Sirasao",
      "Emad Barsoum"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24o.html": {
    "title": "Meta-Reinforcement Learning Robust to Distributional Shift Via Performing Lifelong In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengye Xu",
      "Zihao Li",
      "Qinyuan Ren"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24p.html": {
    "title": "Prompt-guided Precise Audio Editing with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manjie Xu",
      "Chenxing Li",
      "Duzhen Zhang",
      "Dan Su",
      "Wei Liang",
      "Dong Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24q.html": {
    "title": "Low-Rank Similarity Mining for Multimodal Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Xu",
      "Zhilin Lin",
      "Yusong Qiu",
      "Cewu Lu",
      "Yong-Lu Li"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24r.html": {
    "title": "Robust Inverse Constrained Reinforcement Learning under Model Misspecification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Xu",
      "Guiliang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24s.html": {
    "title": "Soft Prompt Recovers Compressed LLMs, Transferably",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaozhuo Xu",
      "Zirui Liu",
      "Beidi Chen",
      "Shaochen Zhong",
      "Yuxin Tang",
      "Jue Wang",
      "Kaixiong Zhou",
      "Xia Hu",
      "Anshumali Shrivastava"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24t.html": {
    "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Xu",
      "Amr Sharaf",
      "Yunmo Chen",
      "Weiting Tan",
      "Lingfeng Shen",
      "Benjamin Van Durme",
      "Kenton Murray",
      "Young Jin Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24u.html": {
    "title": "Adaptive Group Personalization for Federated Mutual Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqing Xu",
      "Dian Shen",
      "Meng Wang",
      "Beilun Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24v.html": {
    "title": "Robust Universal Adversarial Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changming Xu",
      "Gagandeep Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24w.html": {
    "title": "Learning Exceptional Subgroups by End-to-End Maximizing KL-Divergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sascha Xu",
      "Nils Philipp Walter",
      "Janis Kalofolias",
      "Jilles Vreeken"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24x.html": {
    "title": "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyu Xu",
      "Yu-Xiang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24y.html": {
    "title": "Principled Preferential Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Xu",
      "Wenbin Wang",
      "Yuning Jiang",
      "Bratislav Svetozarevic",
      "Colin Jones"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24z.html": {
    "title": "Learning 1-Bit Tiny Object Detector with Discriminative Feature Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Xu",
      "Mingze Wang",
      "Yanjing Li",
      "Mingbao Lin",
      "Baochang Zhang",
      "David Doermann",
      "Xiao Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24aa.html": {
    "title": "SLOG: An Inductive Spectral Graph Neural Network Beyond Polynomial Filter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobo Xu",
      "Yuchen Yan",
      "Dingsu Wang",
      "Zhe Xu",
      "Zhichen Zeng",
      "Tarek F. Abdelzaher",
      "Jiawei Han",
      "Hanghang Tong"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24ab.html": {
    "title": "Libra: Building Decoupled Vision System on Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Xu",
      "Xiaoshan Yang",
      "Yaguang Song",
      "Changsheng Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24ac.html": {
    "title": "A Sparsity Principle for Partially Observable Causal Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danru Xu",
      "Dingling Yao",
      "Sebastien Lachapelle",
      "Perouz Taslakian",
      "Julius Von Kügelgen",
      "Francesco Locatello",
      "Sara Magliacane"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24ad.html": {
    "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelai Xu",
      "Chao Yu",
      "Fei Fang",
      "Yu Wang",
      "Yi Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24ae.html": {
    "title": "Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhui Xu",
      "Fuxun Yu",
      "Zirui Xu",
      "Nathan Inkawhich",
      "Xiang Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24af.html": {
    "title": "Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Xu",
      "Delu Zeng",
      "John Paisley"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24ag.html": {
    "title": "Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Xu",
      "Jingzhao Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/xu24ah.html": {
    "title": "Exponential Spectral Pursuit: An Effective Initialization Method for Sparse Phase Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengchu Xu",
      "Yuxuan Zhang",
      "Jian Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/xudong24a.html": {
    "title": "Iterative Regularized Policy Optimization with Imperfect Demonstrations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gong Xudong",
      "Feng Dawei",
      "Kele Xu",
      "Yuanzhao Zhai",
      "Chengkang Yao",
      "Weijia Wang",
      "Bo Ding",
      "Huaimin Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/xue24a.html": {
    "title": "Few-shot Adaptation to Distribution Shifts By Mixing Source and Target Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Xue",
      "Ali Payani",
      "Yu Yang",
      "Baharan Mirzasoleiman"
    ]
  },
  "https://proceedings.mlr.press/v235/xue24b.html": {
    "title": "Offline Multi-Objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Xue",
      "Rongxi Tan",
      "Xiaobin Huang",
      "Chao Qian"
    ]
  },
  "https://proceedings.mlr.press/v235/xue24c.html": {
    "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuzhao Xue",
      "Zian Zheng",
      "Yao Fu",
      "Jinjie Ni",
      "Zangwei Zheng",
      "Wangchunshu Zhou",
      "Yang You"
    ]
  },
  "https://proceedings.mlr.press/v235/xue24d.html": {
    "title": "Unifying Bayesian Flow Networks and Diffusion Models through Stochastic Differential Equations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Xue",
      "Yuhao Zhou",
      "Shen Nie",
      "Xu Min",
      "Xiaolu Zhang",
      "Jun Zhou",
      "Chongxuan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/yadav24a.html": {
    "title": "FairProof : Confidential and Certifiable Fairness for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chhavi Yadav",
      "Amrita Roy Chowdhury",
      "Dan Boneh",
      "Kamalika Chaudhuri"
    ]
  },
  "https://proceedings.mlr.press/v235/yamamoto24a.html": {
    "title": "Mean Field Langevin Actor-Critic: Faster Convergence and Global Optimality beyond Lazy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kakei Yamamoto",
      "Kazusato Oko",
      "Zhuoran Yang",
      "Taiji Suzuki"
    ]
  },
  "https://proceedings.mlr.press/v235/yan24a.html": {
    "title": "Balancing Similarity and Complementarity for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunda Yan",
      "Sen Cui",
      "Abudukelimu Wuerkaixi",
      "Jingfeng Zhang",
      "Bo Han",
      "Gang Niu",
      "Masashi Sugiyama",
      "Changshui Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yan24b.html": {
    "title": "Probabilistic Time Series Modeling with Decomposable Denoising Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tijin Yan",
      "Hengheng Gong",
      "He Yongping",
      "Yufeng Zhan",
      "Yuanqing Xia"
    ]
  },
  "https://proceedings.mlr.press/v235/yan24c.html": {
    "title": "Exploring the LLM Journey from Cognition to Expression with Linear Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzi Yan",
      "Jialian Li",
      "Yipin Zhang",
      "Dong Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/yan24d.html": {
    "title": "A Space Group Symmetry Informed Network for O(3) Equivariant Crystal Tensor Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keqiang Yan",
      "Alexandra Saxton",
      "Xiaofeng Qian",
      "Xiaoning Qian",
      "Shuiwang Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/yan24e.html": {
    "title": "Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Yan",
      "Alex Schwing",
      "Yu-Xiong Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/yan24f.html": {
    "title": "Handling Heterogeneous Curvatures in Bandit LQR Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Hu Yan",
      "Jing Wang",
      "Peng Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/yan24g.html": {
    "title": "Foundations of Testing for Finite-Sample Causal Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Yan",
      "Ziyu Xu",
      "Zachary Chase Lipton"
    ]
  },
  "https://proceedings.mlr.press/v235/yan24h.html": {
    "title": "Retrieval Across Any Domains via Large-scale Pre-trained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiexi Yan",
      "Zhihui Yin",
      "Chenghao Xu",
      "Cheng Deng",
      "Heng Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/yan24i.html": {
    "title": "Reducing Balancing Error for Causal Inference via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuguang Yan",
      "Hao Zhou",
      "Zeqin Yang",
      "Weilin Chen",
      "Ruichu Cai",
      "Zhifeng Hao"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24a.html": {
    "title": "Do Efficient Transformers Really Save Computation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Yang",
      "Jan Ackermann",
      "Zhenyu He",
      "Guhao Feng",
      "Bohang Zhang",
      "Yunzhen Feng",
      "Qiwei Ye",
      "Di He",
      "Liwei Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24b.html": {
    "title": "Mind the Boundary: Coreset Selection via Reconstructing the Decision Boundary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Yang",
      "Zhe Cao",
      "Sheng Guo",
      "Ruiheng Zhang",
      "Ping Luo",
      "Shengping Zhang",
      "Liqiang Nie"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24c.html": {
    "title": "Sample-Efficient Multiagent Reinforcement Learning with Reset Replay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaodong Yang",
      "Guangyong Chen",
      "Jianye Hao",
      "Pheng-Ann Heng"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24d.html": {
    "title": "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models (Exemplified as A Video Agent)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongxin Yang",
      "Guikun Chen",
      "Xiaodi Li",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24e.html": {
    "title": "A Dense Reward View on Aligning Text-to-Image Diffusion with Preference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentao Yang",
      "Tianqi Chen",
      "Mingyuan Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24f.html": {
    "title": "Lyapunov-stable Neural Control for State and Output Feedback: A Novel Formulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujie Yang",
      "Hongkai Dai",
      "Zhouxing Shi",
      "Cho-Jui Hsieh",
      "Russ Tedrake",
      "Huan Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24g.html": {
    "title": "Latent Space Symmetry Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianke Yang",
      "Nima Dehmamy",
      "Robin Walters",
      "Rose Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24h.html": {
    "title": "Guidance with Spherical Gaussian Constraint for Conditional Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingxiao Yang",
      "Shutong Ding",
      "Yifan Cai",
      "Jingyi Yu",
      "Jingya Wang",
      "Ye Shi"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24i.html": {
    "title": "Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhan Yang",
      "Jingdong Gao",
      "Baharan Mirzasoleiman"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24j.html": {
    "title": "Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yahong Yang",
      "Juncai He"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24k.html": {
    "title": "SAM as the Guide: Mastering Pseudo-Label Refinement in Semi-Supervised Referring Expression Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danni Yang",
      "Jiayi Ji",
      "Yiwei Ma",
      "Tianyu Guo",
      "Haowei Wang",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24l.html": {
    "title": "Small-loss Adaptive Regret for Online Convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Yang",
      "Wei Jiang",
      "Yibo Wang",
      "Ping Yang",
      "Yao Hu",
      "Lijun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24m.html": {
    "title": "Towards Interpretable Deep Local Learning with Successive Gradient Reconciliation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Yang",
      "Xiaojie Li",
      "Motasem Alfarra",
      "Hasan Abed Al Kader Hammoud",
      "Adel Bibi",
      "Philip Torr",
      "Bernard Ghanem"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24n.html": {
    "title": "Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenzhi Yang",
      "Bin Liang",
      "An Liu",
      "Lin Gui",
      "Xingkai Yao",
      "Xiaofang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24o.html": {
    "title": "Position: Towards Implicit Prompt For Text-To-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Yang",
      "Yuqi Lin",
      "Hong Liu",
      "Wenqi Shao",
      "Runjian Chen",
      "Hailong Shang",
      "Yu Wang",
      "Yu Qiao",
      "Kaipeng Zhang",
      "Ping Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24p.html": {
    "title": "Position: Towards Unified Alignment Between Agents, Humans, and Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonghan Yang",
      "An Liu",
      "Zijun Liu",
      "Kaiming Liu",
      "Fangzhou Xiong",
      "Yile Wang",
      "Zeyuan Yang",
      "Qingyuan Hu",
      "Xinrui Chen",
      "Zhenhe Zhang",
      "Fuwen Luo",
      "Zhicheng Guo",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24q.html": {
    "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yang",
      "Xiaoman Pan",
      "Feng Luo",
      "Shuang Qiu",
      "Han Zhong",
      "Dong Yu",
      "Jianshu Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24r.html": {
    "title": "Understanding Server-Assisted Federated Learning in the Presence of Incomplete Client Participation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Yang",
      "Peiwen Qiu",
      "Prashant Khanduri",
      "Minghong Fang",
      "Jia Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24s.html": {
    "title": "ILILT: Implicit Learning of Inverse Lithography Technologies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Yang",
      "Haoxing Ren"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24t.html": {
    "title": "Representation Surgery for Multi-Task Model Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enneng Yang",
      "Li Shen",
      "Zhenyi Wang",
      "Guibing Guo",
      "Xiaojun Chen",
      "Xingwei Wang",
      "Dacheng Tao"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24u.html": {
    "title": "Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Yang",
      "Yingdong Shi",
      "Cheems Wang",
      "Xiantong Zhen",
      "Yuxuan Shi",
      "Jun Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24v.html": {
    "title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxia Yang",
      "Bing Su",
      "Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24w.html": {
    "title": "Exploration and Anti-Exploration with Distributional Random Network Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Yang",
      "Jian Tao",
      "Jiafei Lyu",
      "Xiu Li"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24x.html": {
    "title": "UniAudio: Towards Universal Audio Generation with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongchao Yang",
      "Jinchuan Tian",
      "Xu Tan",
      "Rongjie Huang",
      "Songxiang Liu",
      "Haohan Guo",
      "Xuankai Chang",
      "Jiatong Shi",
      "Sheng Zhao",
      "Jiang Bian",
      "Zhou Zhao",
      "Xixin Wu",
      "Helen M. Meng"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24y.html": {
    "title": "Explain Temporal Black-Box Models via Functional Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linxiao Yang",
      "Yunze Tong",
      "Xinyue Gu",
      "Liang Sun"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24z.html": {
    "title": "Position: Video as the New Language for Real-World Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherry Yang",
      "Jacob C Walker",
      "Jack Parker-Holder",
      "Yilun Du",
      "Jake Bruce",
      "Andre Barreto",
      "Pieter Abbeel",
      "Dale Schuurmans"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24aa.html": {
    "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianlan Yang",
      "Yu-Xiong Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24ab.html": {
    "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songlin Yang",
      "Bailin Wang",
      "Yikang Shen",
      "Rameswar Panda",
      "Yoon Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24ac.html": {
    "title": "Analysis for Abductive Learning and Neural-Symbolic Reasoning Shortcuts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao-Wen Yang",
      "Wen-Da Wei",
      "Jie-Jing Shao",
      "Yu-Feng Li",
      "Zhi-Hua Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24ad.html": {
    "title": "Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Yang",
      "Xiyuan Wei",
      "Tianbao Yang",
      "Yiming Ying"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24ae.html": {
    "title": "How Graph Neural Networks Learn: Lessons from Training Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxiao Yang",
      "Qitian Wu",
      "David Wipf",
      "Ruoyu Sun",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24af.html": {
    "title": "Harnessing Hierarchical Label Distribution Variations in Test Agnostic Long-tail Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyong Yang",
      "Qianqian Xu",
      "Zitai Wang",
      "Sicong Li",
      "Boyu Han",
      "Shilong Bao",
      "Xiaochun Cao",
      "Qingming Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24ag.html": {
    "title": "Neuro-Symbolic Temporal Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yang",
      "Chao Yang",
      "Boyang Li",
      "Yinghao Fu",
      "Shuang Li"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24ah.html": {
    "title": "One Meta-tuned Transformer is What You Need for Few-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Yang",
      "Huaxiu Yao",
      "Ying Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24ai.html": {
    "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Yang",
      "Zhaochen Yu",
      "Chenlin Meng",
      "Minkai Xu",
      "Stefano Ermon",
      "Bin Cui"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24aj.html": {
    "title": "Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangda Yang",
      "Vitaly Zankin",
      "Maximilian Balandat",
      "Stefan Scherer",
      "Kevin Thomas Carlberg",
      "Neil Walton",
      "Kody J. H. Law"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24ak.html": {
    "title": "MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nianzu Yang",
      "Kaipeng Zeng",
      "Haotian Lu",
      "Yexin Wu",
      "Zexin Yuan",
      "Danni Chen",
      "Shengdian Jiang",
      "Jiaxiang Wu",
      "Yimin Wang",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24al.html": {
    "title": "Mol-AE: Auto-Encoder Based Molecular Representation Learning With 3D Cloze Test Objective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwei Yang",
      "Kangjie Zheng",
      "Siyu Long",
      "Zaiqing Nie",
      "Ming Zhang",
      "Xinyu Dai",
      "Wei-Ying Ma",
      "Hao Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/yang24am.html": {
    "title": "What is Dataset Distillation Learning?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Yang",
      "Ye Zhu",
      "Zhiwei Deng",
      "Olga Russakovsky"
    ]
  },
  "https://proceedings.mlr.press/v235/wang24cv.html": {
    "title": "Protein Conformation Generation via Force-Guided SE(3) Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Wang",
      "Lihao Wang",
      "Yuning Shen",
      "Yiqun Wang",
      "Huizhuo Yuan",
      "Yue Wu",
      "Quanquan Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/yao24a.html": {
    "title": "Empowering Graph Invariance Learning with Deep Spurious Infomax",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianjun Yao",
      "Yongqiang Chen",
      "Zhenhao Chen",
      "Kai Hu",
      "Zhiqiang Shen",
      "Kun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yao24b.html": {
    "title": "Human vs. Generative AI in Content Creation Competition: Symbiosis or Conflict?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Yao",
      "Chuanhao Li",
      "Denis Nekipelov",
      "Hongning Wang",
      "Haifeng Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/yao24c.html": {
    "title": "Mobile Attention: Mobile-Friendly Linear-Attention for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Yao",
      "Jian Wang",
      "Haixu Wu",
      "Jingdong Wang",
      "Mingsheng Long"
    ]
  },
  "https://proceedings.mlr.press/v235/yao24d.html": {
    "title": "Socialized Learning: Making Each Other Better Through Multi-Agent Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinjie Yao",
      "Yu Wang",
      "Pengfei Zhu",
      "Wanyu Lin",
      "Jialu Li",
      "Weihao Li",
      "Qinghua Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/yaras24a.html": {
    "title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Yaras",
      "Peng Wang",
      "Laura Balzano",
      "Qing Qu"
    ]
  },
  "https://proceedings.mlr.press/v235/yau24a.html": {
    "title": "EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chung-Yiu Yau",
      "Hoi To Wai",
      "Parameswaran Raman",
      "Soumajyoti Sarkar",
      "Mingyi Hong"
    ]
  },
  "https://proceedings.mlr.press/v235/ye24a.html": {
    "title": "Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenlu Ye",
      "Jiafan He",
      "Quanquan Gu",
      "Tong Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yen24a.html": {
    "title": "Adaptive Sampling of k-Space in Magnetic Resonance for Rapid Pathology Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Yu Yen",
      "Raghav Singhal",
      "Umang Sharma",
      "Rajesh Ranganath",
      "Sumit Chopra",
      "Lerrel Pinto"
    ]
  },
  "https://proceedings.mlr.press/v235/yin24a.html": {
    "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyu Yin",
      "Xuzheng He",
      "Xiang Zhuang",
      "Yu Zhao",
      "Jianhua Yao",
      "Xiaoyu Shen",
      "Qiang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yin24b.html": {
    "title": "Junk DNA Hypothesis: Pruning Small Pre-Trained Weights $\\textitIrreversibly$ and $\\textitMonotonically$ Impairs \"Difficult\" Downstream Tasks in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Yin",
      "Ajay Kumar Jaiswal",
      "Shiwei Liu",
      "Souvik Kundu",
      "Zhangyang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/yin24c.html": {
    "title": "Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Yin",
      "Jayanth Srinivasa",
      "Kai-Wei Chang"
    ]
  },
  "https://proceedings.mlr.press/v235/yin24d.html": {
    "title": "High-Dimensional Bayesian Optimization via Semi-Supervised Learning with Optimized Unlabeled Data Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Yin",
      "Yu Wang",
      "Peng Li"
    ]
  },
  "https://proceedings.mlr.press/v235/yin24e.html": {
    "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Yin",
      "You Wu",
      "Zhenyu Zhang",
      "Cheng-Yu Hsieh",
      "Yaqing Wang",
      "Yiling Jia",
      "Gen Li",
      "Ajay Kumar Jaiswal",
      "Mykola Pechenizkiy",
      "Yi Liang",
      "Michael Bendersky",
      "Zhangyang Wang",
      "Shiwei Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/ying24a.html": {
    "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaining Ying",
      "Fanqing Meng",
      "Jin Wang",
      "Zhiqian Li",
      "Han Lin",
      "Yue Yang",
      "Hao Zhang",
      "Wenbo Zhang",
      "Yuqi Lin",
      "Shuo Liu",
      "Jiayi Lei",
      "Quanfeng Lu",
      "Runjian Chen",
      "Peng Xu",
      "Renrui Zhang",
      "Haozhe Zhang",
      "Peng Gao",
      "Yali Wang",
      "Yu Qiao",
      "Ping Luo",
      "Kaipeng Zhang",
      "Wenqi Shao"
    ]
  },
  "https://proceedings.mlr.press/v235/yoo24a.html": {
    "title": "Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsoo Yoo",
      "Yunpeng Liu",
      "Frank Wood",
      "Geoff Pleiss"
    ]
  },
  "https://proceedings.mlr.press/v235/yoon24a.html": {
    "title": "Uncertainty Estimation by Density Aware Evidential Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeseong Yoon",
      "Heeyoung Kim"
    ]
  },
  "https://proceedings.mlr.press/v235/yoon24b.html": {
    "title": "Optimal Acceleration for Minimax and Fixed-Point Problems is Not Unique",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeho Yoon",
      "Jaeyeon Kim",
      "Jaewook J. Suh",
      "Ernest K. Ryu"
    ]
  },
  "https://proceedings.mlr.press/v235/yoon24c.html": {
    "title": "FRAG: Frequency Adapting Group for Diffusion Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunjae Yoon",
      "Gwanhyeong Koo",
      "Geonwoo Kim",
      "Chang D. Yoo"
    ]
  },
  "https://proceedings.mlr.press/v235/yoon24d.html": {
    "title": "Breadth-First Exploration on Adaptive Grid for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngsik Yoon",
      "Gangbok Lee",
      "Sungsoo Ahn",
      "Jungseul Ok"
    ]
  },
  "https://proceedings.mlr.press/v235/you24a.html": {
    "title": "When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran You",
      "Yichao Fu",
      "Zheng Wang",
      "Amir Yazdanbakhsh",
      "Yingyan Celine Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/you24b.html": {
    "title": "SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang You",
      "Zekai Xu",
      "Chen Nie",
      "Zhijie Deng",
      "Qinghai Guo",
      "Xiang Wang",
      "Zhezhi He"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24a.html": {
    "title": "Efficient Algorithms for Empirical Group Distributionally Robust Optimization and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingzhi Yu",
      "Yunuo Cai",
      "Wei Jiang",
      "Lijun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24b.html": {
    "title": "Towards Resource-friendly, Extensible and Stable Incomplete Multi-view Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengju Yu",
      "Zhibin Dong",
      "Siwei Wang",
      "Xinhang Wan",
      "Yue Liu",
      "Weixuan Liang",
      "Pei Zhang",
      "Wenxuan Tu",
      "Xinwang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24c.html": {
    "title": "Activation-Descent Regularization for Input Optimization of ReLU Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongzhan Yu",
      "Sicun Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24d.html": {
    "title": "Collage: Light-Weight Low-Precision Strategy for LLM Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Yu",
      "Gaurav Gupta",
      "Karthick Gopalswamy",
      "Amith R Mamidala",
      "Hao Zhou",
      "Jeffrey Huynh",
      "Youngsuk Park",
      "Ron Diamant",
      "Anoop Deoras",
      "Luke Huan"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24e.html": {
    "title": "Privacy-Preserving Instructions for Aligning Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Yu",
      "Peter Kairouz",
      "Sewoong Oh",
      "Zheng Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24f.html": {
    "title": "Learning Latent Structures in Network Games via Data-Dependent Gated-Prior Graph Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xue Yu",
      "Muchen Li",
      "Yan Leng",
      "Renjie Liao"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24g.html": {
    "title": "Learning Scale-Aware Spatio-temporal Implicit Representation for Event-based Motion Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Yu",
      "Jianing Li",
      "Shengping Zhang",
      "Xiangyang Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24h.html": {
    "title": "Enabling Few-Shot Learning with PID Control: A Layer Adaptive Optimizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Yu",
      "Xinde Li",
      "Pengfei Zhang",
      "Zhentong Zhang",
      "Fir Dunkin"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24i.html": {
    "title": "Generalization Bound and New Algorithm for Clean-Label Backdoor Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijia Yu",
      "Shuang Liu",
      "Yibo Miao",
      "Xiao-Shan Gao",
      "Lijun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24j.html": {
    "title": "Learning Causal Dynamics Models in Object-Oriented Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Yu",
      "Jingqing Ruan",
      "Dengpeng Xing"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24k.html": {
    "title": "ViP: A Differentially Private Foundation Model for Computer Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaodong Yu",
      "Maziar Sanjabi",
      "Yi Ma",
      "Kamalika Chaudhuri",
      "Chuan Guo"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24l.html": {
    "title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongzhi Yu",
      "Zheng Wang",
      "Yonggan Fu",
      "Huihong Shi",
      "Khalid Shaikh",
      "Yingyan Celine Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24m.html": {
    "title": "Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yu",
      "Yufei Wang",
      "Song Xia",
      "Wenhan Yang",
      "Shijian Lu",
      "Yap-Peng Tan",
      "Alex Kot"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24n.html": {
    "title": "Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mo Yu",
      "Qiujing Wang",
      "Shunchi Zhang",
      "Yisi Sang",
      "Kangsheng Pu",
      "Zekai Wei",
      "Han Wang",
      "Liyan Xu",
      "Jing Li",
      "Yue Yu",
      "Jie Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24o.html": {
    "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Yu",
      "Zhengyuan Yang",
      "Linjie Li",
      "Jianfeng Wang",
      "Kevin Lin",
      "Zicheng Liu",
      "Xinchao Wang",
      "Lijuan Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24p.html": {
    "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Yu",
      "Bowen Yu",
      "Haiyang Yu",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24q.html": {
    "title": "Improving Sharpness-Aware Minimization by Lookahead",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runsheng Yu",
      "Youzhi Zhang",
      "James Kwok"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24r.html": {
    "title": "A Unified Adaptive Testing System Enabled by Hierarchical Structure Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Yu",
      "Yan Zhuang",
      "Zhenya Huang",
      "Qi Liu",
      "Xin Li",
      "Rui Li",
      "Enhong Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/yu24s.html": {
    "title": "Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoqi Yu",
      "Jing Zou",
      "Xiaowei Hu",
      "Angelica I Aviles-Rivero",
      "Jing Qin",
      "Shujun Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/yuan24a.html": {
    "title": "Smoothing Proximal Gradient Methods for Nonsmooth Sparsity Constrained Optimization: Optimality Conditions and Global Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ganzhao Yuan"
    ]
  },
  "https://proceedings.mlr.press/v235/yuan24b.html": {
    "title": "A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dehao Yuan",
      "Cornelia Fermuller",
      "Tahseen Rabbani",
      "Furong Huang",
      "Yiannis Aloimonos"
    ]
  },
  "https://proceedings.mlr.press/v235/yuan24c.html": {
    "title": "SHINE: Shielding Backdoors in Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuowen Yuan",
      "Wenbo Guo",
      "Jinyuan Jia",
      "Bo Li",
      "Dawn Song"
    ]
  },
  "https://proceedings.mlr.press/v235/yuan24d.html": {
    "title": "Self-Rewarding Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhe Yuan",
      "Richard Yuanzhe Pang",
      "Kyunghyun Cho",
      "Xian Li",
      "Sainbayar Sukhbaatar",
      "Jing Xu",
      "Jason E Weston"
    ]
  },
  "https://proceedings.mlr.press/v235/yuan24e.html": {
    "title": "Not Just Pretty Pictures: Toward Interventional Data Augmentation Using Text-to-Image Generators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhao Yuan",
      "Francesco Pinto",
      "Adam Davies",
      "Philip Torr"
    ]
  },
  "https://proceedings.mlr.press/v235/yuan24f.html": {
    "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuowen Yuan",
      "Zidi Xiong",
      "Yi Zeng",
      "Ning Yu",
      "Ruoxi Jia",
      "Dawn Song",
      "Bo Li"
    ]
  },
  "https://proceedings.mlr.press/v235/yue24a.html": {
    "title": "OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Yue",
      "Xingyuan Hua",
      "Ju Ren",
      "Sen Lin",
      "Junshan Zhang",
      "Yaoxue Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yue24b.html": {
    "title": "Federated Self-Explaining GNNs with Anti-shortcut Augmentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linan Yue",
      "Qi Liu",
      "Weibo Gao",
      "Ye Liu",
      "Kai Zhang",
      "Yichao Du",
      "Li Wang",
      "Fangzhou Yao"
    ]
  },
  "https://proceedings.mlr.press/v235/yue24c.html": {
    "title": "How to Leverage Diverse Demonstrations in Offline Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Yue",
      "Jiani Liu",
      "Xingyuan Hua",
      "Ju Ren",
      "Sen Lin",
      "Junshan Zhang",
      "Yaoxue Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/yue24d.html": {
    "title": "Image Restoration Through Generalized Ornstein-Uhlenbeck Bridge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Conghan Yue",
      "Zhengwei Peng",
      "Junlong Ma",
      "Shiyan Du",
      "Pengxu Wei",
      "Dongyu Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/zaher24a.html": {
    "title": "Manifold Integrated Gradients: Riemannian Geometry for Feature Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eslam Zaher",
      "Maciej Trzaskowski",
      "Quan Nguyen",
      "Fred Roosta"
    ]
  },
  "https://proceedings.mlr.press/v235/zahid24a.html": {
    "title": "Sample as you Infer: Predictive Coding with Langevin Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Umais Zahid",
      "Qinghai Guo",
      "Zafeirios Fountas"
    ]
  },
  "https://proceedings.mlr.press/v235/zakerinia24a.html": {
    "title": "More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Zakerinia",
      "Amin Behjati",
      "Christoph H. Lampert"
    ]
  },
  "https://proceedings.mlr.press/v235/zamboni24a.html": {
    "title": "How to Explore with Belief: State Entropy Maximization in POMDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Zamboni",
      "Duilio Cirino",
      "Marcello Restelli",
      "Mirco Mutti"
    ]
  },
  "https://proceedings.mlr.press/v235/zamfir24a.html": {
    "title": "See More Details: Efficient Image Super-Resolution by Experts Mining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduard Zamfir",
      "Zongwei Wu",
      "Nancy Mehta",
      "Yulun Zhang",
      "Radu Timofte"
    ]
  },
  "https://proceedings.mlr.press/v235/zang24a.html": {
    "title": "DiffAug: Enhance Unsupervised Contrastive Learning with Domain-Knowledge-Free Diffusion-based Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Zang",
      "Hao Luo",
      "Kai Wang",
      "Panpan Zhang",
      "Fan Wang",
      "Stan Z. Li",
      "Yang You"
    ]
  },
  "https://proceedings.mlr.press/v235/zarifis24a.html": {
    "title": "Robustly Learning Single-Index Models via Alignment Sharpness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikos Zarifis",
      "Puqian Wang",
      "Ilias Diakonikolas",
      "Jelena Diakonikolas"
    ]
  },
  "https://proceedings.mlr.press/v235/zarifzadeh24a.html": {
    "title": "Low-Cost High-Power Membership Inference Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sajjad Zarifzadeh",
      "Philippe Liu",
      "Reza Shokri"
    ]
  },
  "https://proceedings.mlr.press/v235/zeighami24a.html": {
    "title": "Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sepanta Zeighami",
      "Cyrus Shahabi"
    ]
  },
  "https://proceedings.mlr.press/v235/zeng24a.html": {
    "title": "Continuous Treatment Effects with Surrogate Outcomes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Zeng",
      "David Arbour",
      "Avi Feller",
      "Raghavendra Addanki",
      "Ryan A. Rossi",
      "Ritwik Sinha",
      "Edward Kennedy"
    ]
  },
  "https://proceedings.mlr.press/v235/zeng24b.html": {
    "title": "tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhua Zeng",
      "Chao Li",
      "Zhun Sun",
      "Qibin Zhao",
      "Guoxu Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/zeng24c.html": {
    "title": "Token-level Direct Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongcheng Zeng",
      "Guoqing Liu",
      "Weiyu Ma",
      "Ning Yang",
      "Haifeng Zhang",
      "Jun Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zeng24d.html": {
    "title": "Learning Reward for Robot Skills Using Large Language Models via Self-Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Zeng",
      "Yao Mu",
      "Lin Shao"
    ]
  },
  "https://proceedings.mlr.press/v235/zeng24e.html": {
    "title": "Graph Mixup on Approximate Gromov–Wasserstein Geodesics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichen Zeng",
      "Ruizhong Qiu",
      "Zhe Xu",
      "Zhining Liu",
      "Yuchen Yan",
      "Tianxin Wei",
      "Lei Ying",
      "Jingrui He",
      "Hanghang Tong"
    ]
  },
  "https://proceedings.mlr.press/v235/zeng24f.html": {
    "title": "Interacting Diffusion Processes for Event Sequence Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mai Zeng",
      "Florence Regol",
      "Mark Coates"
    ]
  },
  "https://proceedings.mlr.press/v235/zeng24g.html": {
    "title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanpeng Zeng",
      "Karthikeyan Sankaralingam",
      "Vikas Singh"
    ]
  },
  "https://proceedings.mlr.press/v235/zenn24a.html": {
    "title": "Differentiable Annealed Importance Sampling Minimizes The Jensen-Shannon Divergence Between Initial and Target Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Zenn",
      "Robert Bamler"
    ]
  },
  "https://proceedings.mlr.press/v235/zeynali24a.html": {
    "title": "Robust Learning-Augmented Dictionaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Zeynali",
      "Shahin Kamali",
      "Mohammad Hajiesmaili"
    ]
  },
  "https://proceedings.mlr.press/v235/zhai24a.html": {
    "title": "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Zhai",
      "Lucy Liao",
      "Xing Liu",
      "Yueming Wang",
      "Rui Li",
      "Xuan Cao",
      "Leon Gao",
      "Zhaojie Gong",
      "Fangda Gu",
      "Jiayuan He",
      "Yinghai Lu",
      "Yu Shi"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24a.html": {
    "title": "Tight Partial Identification of Causal Effects with Marginal Distribution of Unmeasured Confounders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24b.html": {
    "title": "DAG-Based Column Generation for Adversarial Team Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youzhi Zhang",
      "Bo An",
      "Daniel Dajun Zeng"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24c.html": {
    "title": "SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Zhang",
      "Chenjia Bai",
      "Haoran He",
      "Zhigang Wang",
      "Bin Zhao",
      "Xiu Li",
      "Xuelong Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24d.html": {
    "title": "Efficient Stochastic Approximation of Minimax Excess Risk Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Zhang",
      "Haomin Bai",
      "Wei-Wei Tu",
      "Ping Yang",
      "Yao Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24e.html": {
    "title": "Discounted Adaptive Online Learning: Towards Better Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Zhang",
      "David Bombara",
      "Heng Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24f.html": {
    "title": "Improving Equivariant Graph Neural Networks on Large Geometric Graphs via Virtual Nodes Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuelin Zhang",
      "Jiacheng Cen",
      "Jiaqi Han",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Wenbing Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24g.html": {
    "title": "Provably Efficient Partially Observable Risk-sensitive Reinforcement Learning with Hindsight Observation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tonghe Zhang",
      "Yu Chen",
      "Longbo Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24h.html": {
    "title": "Verification of Machine Unlearning is Fragile",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binchi Zhang",
      "Zihan Chen",
      "Cong Shen",
      "Jundong Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24i.html": {
    "title": "Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bonan Zhang",
      "Chia-Yu Chen",
      "Naveen Verma"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24j.html": {
    "title": "LQER: Low-Rank Quantization Error Reconstruction for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Zhang",
      "Jianyi Cheng",
      "George Anthony Constantinides",
      "Yiren Zhao"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24k.html": {
    "title": "Random Scaling and Momentum for Non-smooth Non-convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinzi Zhang",
      "Ashok Cutkosky"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24l.html": {
    "title": "Towards Certified Unlearning for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binchi Zhang",
      "Yushun Dong",
      "Tianhao Wang",
      "Jundong Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24m.html": {
    "title": "Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive and Masked Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Tianqi Du",
      "Haotian Huang",
      "Yifei Wang",
      "Yisen Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24n.html": {
    "title": "CaM: Cache Merging for Memory-efficient LLMs Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Zhang",
      "Yuxuan Du",
      "Gen Luo",
      "Yunshan Zhong",
      "Zhenyu Zhang",
      "Shiwei Liu",
      "Rongrong Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24o.html": {
    "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Zhang",
      "Benjamin L. Edelman",
      "Danilo Francati",
      "Daniele Venturi",
      "Giuseppe Ateniese",
      "Boaz Barak"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24p.html": {
    "title": "MILP-FBGen: LP/MILP Instance Generation with Feasibility/Boundedness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yahong Zhang",
      "Chenchen Fan",
      "Donghui Chen",
      "Congrui Li",
      "Wenli Ouyang",
      "Mingda Zhu",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24q.html": {
    "title": "SF-DQN: Provable Knowledge Transfer using Successor Feature for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Zhang",
      "Heshan Devaka Fernando",
      "Miao Liu",
      "Keerthiram Murugesan",
      "Songtao Lu",
      "Pin-Yu Chen",
      "Tianyi Chen",
      "Meng Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24r.html": {
    "title": "Model-based Reinforcement Learning for Parameterized Action Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renhao Zhang",
      "Haotian Fu",
      "Yilin Miao",
      "George Konidaris"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24s.html": {
    "title": "Easing Concept Bleeding in Diffusion via Entity Localization and Anchoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiewei Zhang",
      "Song Guo",
      "Peiran Dong",
      "Jie Zhang",
      "Ziming Liu",
      "Yue Yu",
      "Xiao-Ming Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24t.html": {
    "title": "Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen-Yu Zhang",
      "Siwei Han",
      "Huaxiu Yao",
      "Gang Niu",
      "Masashi Sugiyama"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24u.html": {
    "title": "Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanhua Zhang",
      "Moritz Hardt"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24v.html": {
    "title": "Advancing DRL Agents in Commercial Fighting Games: Training, Integration, and Agent-Human Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Qiang He",
      "Yuan Zhou",
      "Elvis S. Liu",
      "Hong Wang",
      "Jian Zhao",
      "Yang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24w.html": {
    "title": "On the Duality Between Sharpness-Aware Minimization and Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Zhang",
      "Hangzhou He",
      "Jingyu Zhu",
      "Huanran Chen",
      "Yifei Wang",
      "Zeming Wei"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24x.html": {
    "title": "Towards Causal Foundation Model: on Duality between Optimal Balancing and Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Zhang",
      "Joel Jennings",
      "Agrin Hilmkil",
      "Nick Pawlowski",
      "Cheng Zhang",
      "Chao Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24y.html": {
    "title": "Parameter-Efficient Fine-Tuning with Controls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Zhang",
      "Cheng Jingpu",
      "Yanyu Xu",
      "Qianxiao Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24z.html": {
    "title": "Deep Regression Representation Learning with Topology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Zhang",
      "Kenji Kawaguchi",
      "Angela Yao"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24aa.html": {
    "title": "Understanding Unimodal Bias in Multimodal Deep Linear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yedi Zhang",
      "Peter E. Latham",
      "Andrew M Saxe"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ab.html": {
    "title": "Guarantees for Nonlinear Representation Learning: Non-identical Covariates, Dependent Data, Fewer Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Tck Zhang",
      "Bruce D Lee",
      "Ingvar Ziemann",
      "George J. Pappas",
      "Nikolai Matni"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ac.html": {
    "title": "An Interpretable Evaluation of Entropy-based Novelty of Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Zhang",
      "Cheuk Ting Li",
      "Farzan Farnia"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ad.html": {
    "title": "Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihua Zhang",
      "Pingzhi Li",
      "Junyuan Hong",
      "Jiaxiang Li",
      "Yimeng Zhang",
      "Wenqing Zheng",
      "Pin-Yu Chen",
      "Jason D. Lee",
      "Wotao Yin",
      "Mingyi Hong",
      "Zhangyang Wang",
      "Sijia Liu",
      "Tianlong Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ae.html": {
    "title": "S3O: A Dual-Phase Approach for Reconstructing Dynamic Shape and Skeleton of Articulated Objects from Single Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang",
      "Fang Li",
      "Samyak Rawlekar",
      "Narendra Ahuja"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24af.html": {
    "title": "DPZero: Private Fine-Tuning of Language Models without Backpropagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Zhang",
      "Bingcong Li",
      "Kiran Koshy Thekumparampil",
      "Sewoong Oh",
      "Niao He"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ag.html": {
    "title": "Conditional Language Learning with Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Zhang",
      "Miao Li",
      "Ji Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ah.html": {
    "title": "Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanpeng Zhang",
      "Yilin Li",
      "Boyu Yang",
      "Zongqing Lu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ai.html": {
    "title": "BLO-SAM: Bi-level Optimization Based Finetuning of the Segment Anything Model for Overfitting-Preventing Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Zhang",
      "Youwei Liang",
      "Ruiyi Zhang",
      "Amirhosein Javadi",
      "Pengtao Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24aj.html": {
    "title": "Multi-Factor Adaptive Vision Selection for Egocentric Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Zhang",
      "Meng Liu",
      "Zixin Liu",
      "Xuemeng Song",
      "Yaowei Wang",
      "Liqiang Nie"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ak.html": {
    "title": "Self-Consistency Training for Density-Functional-Theory Hamiltonian Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He Zhang",
      "Chang Liu",
      "Zun Wang",
      "Xinran Wei",
      "Siyuan Liu",
      "Nanning Zheng",
      "Bin Shao",
      "Tie-Yan Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24al.html": {
    "title": "UP2ME: Univariate Pre-training to Multivariate Fine-tuning as a General-purpose Framework for Multivariate Time Series Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Zhang",
      "Minghao Liu",
      "Shengyang Zhou",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24am.html": {
    "title": "Improving Accuracy-robustness Trade-off via Pixel Reweighted Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Zhang",
      "Feng Liu",
      "Dawei Zhou",
      "Jingfeng Zhang",
      "Tongliang Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24an.html": {
    "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhang",
      "Yi Luan",
      "Hexiang Hu",
      "Kenton Lee",
      "Siyuan Qiao",
      "Wenhu Chen",
      "Yu Su",
      "Ming-Wei Chang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ao.html": {
    "title": "Wukong: Towards a Scaling Law for Large-Scale Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Buyun Zhang",
      "Liang Luo",
      "Yuxin Chen",
      "Jade Nie",
      "Xi Liu",
      "Shen Li",
      "Yanli Zhao",
      "Yuchen Hao",
      "Yantao Yao",
      "Ellie Dingqiao Wen",
      "Jongsoo Park",
      "Maxim Naumov",
      "Wenlin Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ap.html": {
    "title": "Nonparametric Teaching of Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Steven Tin Sui Luo",
      "Jason Chun Lok Li",
      "Yik Chung Wu",
      "Ngai Wong"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24aq.html": {
    "title": "Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dake Zhang",
      "Boxiang Lyu",
      "Shuang Qiu",
      "Mladen Kolar",
      "Tong Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ar.html": {
    "title": "Sparse-to-dense Multimodal Image Registration via Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaining Zhang",
      "Jiayi Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24as.html": {
    "title": "Amend to Alignment: Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Zhang",
      "Xiaosong Ma",
      "Song Guo",
      "Peng Li",
      "Wenchao Xu",
      "Xueyang Tang",
      "Zicong Hong"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24at.html": {
    "title": "In-Context Principle Learning from Mistakes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianjun Zhang",
      "Aman Madaan",
      "Luyu Gao",
      "Steven Zheng",
      "Swaroop Mishra",
      "Yiming Yang",
      "Niket Tandon",
      "Uri Alon"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24au.html": {
    "title": "Sequential Asynchronous Action Coordination in Multi-Agent Systems: A Stackelberg Decision Transformer Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Zhang",
      "Hangyu Mao",
      "Lijuan Li",
      "Zhiwei Xu",
      "Dapeng Li",
      "Rui Zhao",
      "Guoliang Fan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24av.html": {
    "title": "Sparsest Models Elude Pruning: An Exposé of Pruning's Current Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Zhang",
      "Vardan Papyan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24aw.html": {
    "title": "A Federated Stochastic Multi-level Compositional Minimax Algorithm for Deep AUC Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwen Zhang",
      "Ali Payani",
      "Myungjin Lee",
      "Richard Souvenir",
      "Hongchang Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ax.html": {
    "title": "Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangzhao Zhang",
      "Mert Pilanci"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ay.html": {
    "title": "How Language Model Hallucinations Can Snowball",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muru Zhang",
      "Ofir Press",
      "William Merrill",
      "Alisa Liu",
      "Noah A. Smith"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24az.html": {
    "title": "Enhancing Storage and Computational Efficiency in Federated Multimodal Learning for Large-Scale Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixin Zhang",
      "Fan Qi",
      "Changsheng Xu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ba.html": {
    "title": "Online Resource Allocation with Non-Stationary Customers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyue Zhang",
      "Hanzhang Qin",
      "Mabel Chou"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bb.html": {
    "title": "Flexible Residual Binarization for Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulun Zhang",
      "Haotong Qin",
      "Zixiang Zhao",
      "Xianglong Liu",
      "Martin Danelljan",
      "Fisher Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bc.html": {
    "title": "Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhang",
      "Wenjie Qiu",
      "Yi-Chen Li",
      "Lei Yuan",
      "Chengxing Jia",
      "Zongzhang Zhang",
      "Yang Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bd.html": {
    "title": "Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongming Zhang",
      "Tongzheng Ren",
      "Chenjun Xiao",
      "Dale Schuurmans",
      "Bo Dai"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24be.html": {
    "title": "Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujing Zhang",
      "Aaron Roth",
      "Linjun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bf.html": {
    "title": "Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiankun Zhang",
      "Aocheng Shen",
      "Boyu Zhang",
      "Hanrui Jiang",
      "Bingqian Du"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bg.html": {
    "title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongmeng Zhang",
      "Yufeng Shi",
      "Jinhua Zhu",
      "Wengang Zhou",
      "Xiang Qi",
      "Peng Zhang",
      "Houqiang Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bh.html": {
    "title": "Learning Low-dimensional Latent Dynamics from High-dimensional Observations: Non-asymptotics and Lower Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Zhang",
      "Shahriar Talebi",
      "Na Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bi.html": {
    "title": "Matrix Information Theory for Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhang",
      "Zhiquan Tan",
      "Jingqin Yang",
      "Weiran Huang",
      "Yang Yuan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bj.html": {
    "title": "Switchable Decision: Dynamic Neural Generation Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shujian Zhang",
      "Korawat Tanwisuth",
      "Chengyue Gong",
      "Pengcheng He",
      "Mingyuan Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bk.html": {
    "title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhang",
      "Chaoqun Wan",
      "Yonggang Zhang",
      "Yiu-Ming Cheung",
      "Xinmei Tian",
      "Xu Shen",
      "Jieping Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bl.html": {
    "title": "Directly Denoising Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Zhang",
      "Jingjing Wang",
      "Feng Luo"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bm.html": {
    "title": "Disentangled Continual Graph Neural Architecture Search with Invariant Modular Supernet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyang Zhang",
      "Xin Wang",
      "Yijian Qin",
      "Hong Chen",
      "Ziwei Zhang",
      "Xu Chu",
      "Wenwu Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bn.html": {
    "title": "GroupCover: A Secure, Efficient and Scalable Inference Framework for On-device Model Protection based on TEEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "Na Wang",
      "Ziqi Zhang",
      "Yao Zhang",
      "Tianyi Zhang",
      "Jianwei Liu",
      "Ye Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bo.html": {
    "title": "Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahan Zhang",
      "Qi Wei",
      "Feng Liu",
      "Lei Feng"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bp.html": {
    "title": "EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhang",
      "Zhewei Wei",
      "Ye Yuan",
      "Chongxuan Li",
      "Wenbing Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bq.html": {
    "title": "Exploring the Benefit of Activation Sparsity in Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyan Zhang",
      "Chaojun Xiao",
      "Qiujieli Qin",
      "Yankai Lin",
      "Zhiyuan Zeng",
      "Xu Han",
      "Zhiyuan Liu",
      "Ruobing Xie",
      "Maosong Sun",
      "Jie Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24br.html": {
    "title": "Causal Representation Learning from Multiple Distributions: A General Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhang",
      "Shaoan Xie",
      "Ignavier Ng",
      "Yujia Zheng"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bs.html": {
    "title": "FESSNC: Fast Exponentially Stable and Safe Neural Controller",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingdong Zhang",
      "Luan Yang",
      "Qunxi Zhu",
      "Wei Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bt.html": {
    "title": "Rethinking Guidance Information to Utilize Unlabeled Samples: A Label Encoding Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulong Zhang",
      "Yuan Yao",
      "Shuhao Chen",
      "Pengrong Jin",
      "Yu Zhang",
      "Jian Jin",
      "Jiangang Lu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bu.html": {
    "title": "NExT-Chat: An LMM for Chat, Detection and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ao Zhang",
      "Yuan Yao",
      "Wei Ji",
      "Zhiyuan Liu",
      "Tat-Seng Chua"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bv.html": {
    "title": "Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaihong Zhang",
      "Heqi Yin",
      "Feng Liang",
      "Jingbo Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bw.html": {
    "title": "Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Zhang",
      "Chenlong Yin",
      "Hao Liu",
      "Xiaofang Zhou",
      "Hui Xiong"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bx.html": {
    "title": "Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guibin Zhang",
      "Yanwei Yue",
      "Kun Wang",
      "Junfeng Fang",
      "Yongduo Sui",
      "Kai Wang",
      "Yuxuan Liang",
      "Dawei Cheng",
      "Shirui Pan",
      "Tianlong Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24by.html": {
    "title": "Generalization Analysis for Multi-Label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhang",
      "Min-Ling Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24bz.html": {
    "title": "Quantum Algorithms and Lower Bounds for Finite-Sum Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexin Zhang",
      "Chenyi Zhang",
      "Cong Fang",
      "Liwei Wang",
      "Tongyang Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ca.html": {
    "title": "PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Zhang",
      "Qizhen Zhang",
      "Jakob Nicolaus Foerster"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cb.html": {
    "title": "MLIP: Efficient Multi-Perspective Language-Image Pretraining with Exhaustive Data Utilization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Qi Zhang",
      "Zixuan Gong",
      "Yiwei Shi",
      "Yepeng Liu",
      "Duoqian Miao",
      "Yang Liu",
      "Ke Liu",
      "Kun Yi",
      "Wei Fan",
      "Liang Hu",
      "Changwei Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cc.html": {
    "title": "Lightweight Image Super-Resolution via Flexible Meta Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulun Zhang",
      "Kai Zhang",
      "Luc Van Gool",
      "Martin Danelljan",
      "Fisher Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cd.html": {
    "title": "Offline Training of Language Model Agents with Functions as Learnable Weights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaokun Zhang",
      "Jieyu Zhang",
      "Jiale Liu",
      "Linxin Song",
      "Chi Wang",
      "Ranjay Krishna",
      "Qingyun Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ce.html": {
    "title": "Efficient Contextual Bandits with Uninformed Feedback Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxiao Zhang",
      "Yuheng Zhang",
      "Haipeng Luo",
      "Paul Mineiro"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cf.html": {
    "title": "Efficient Denoising Diffusion via Probabilistic Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhong Zhang",
      "Zhiwei Zhang",
      "Renjie Pi",
      "Zhongming Jin",
      "Yuan Gao",
      "Jieping Ye",
      "Kani Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cg.html": {
    "title": "Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Zhang",
      "Tianle Zhang",
      "Kai Wang",
      "Ziyao Guo",
      "Yuxuan Liang",
      "Xavier Bresson",
      "Wei Jin",
      "Yang You"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ch.html": {
    "title": "Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Zhang",
      "Sen Zhang",
      "Yibing Zhan",
      "Yong Luo",
      "Yonggang Wen",
      "Dacheng Tao"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ci.html": {
    "title": "Uncertainty-Aware Reward-Free Exploration with General Function Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkai Zhang",
      "Weitong Zhang",
      "Dongruo Zhou",
      "Quanquan Gu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cj.html": {
    "title": "Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianren Zhang",
      "Chujie Zhao",
      "Guanyu Chen",
      "Yizhou Jiang",
      "Feng Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24ck.html": {
    "title": "On the Expressive Power of Spectral Invariant Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohang Zhang",
      "Lingxiao Zhao",
      "Haggai Maron"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cl.html": {
    "title": "Position: Social Environment Design Should be Further Developed for AI-based Policy-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edwin Zhang",
      "Sadie Zhao",
      "Tonghan Wang",
      "Safwan Hossain",
      "Henry Gasztowtt",
      "Stephan Zheng",
      "David C. Parkes",
      "Milind Tambe",
      "Yiling Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cm.html": {
    "title": "Neural Jump-Diffusion Temporal Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Zhang",
      "Chuan Zhou",
      "Yang Aron Liu",
      "Peng Zhang",
      "Xixun Lin",
      "Zhi-Ming Ma"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cn.html": {
    "title": "The Emergence of Reproducibility and Consistency in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huijie Zhang",
      "Jinfan Zhou",
      "Yifu Lu",
      "Minzhe Guo",
      "Peng Wang",
      "Liyue Shen",
      "Qing Qu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24co.html": {
    "title": "Transferable Facial Privacy Protection against Blind Face Restoration via Domain-Consistent Adversarial Obfuscation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kui Zhang",
      "Hang Zhou",
      "Jie Zhang",
      "Wenbo Zhou",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cp.html": {
    "title": "Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinlu Zhang",
      "Yiyi Zhou",
      "Qiancheng Zheng",
      "Xiaoxiong Du",
      "Gen Luo",
      "Jun Peng",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/zhang24cq.html": {
    "title": "Accelerating Iterative Retrieval-augmented Language Model Serving with Speculation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Zhang",
      "Alan Zhu",
      "Lijie Yang",
      "Yihua Xu",
      "Lanting Li",
      "Phitchaya Mangpo Phothilimthana",
      "Zhihao Jia"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24a.html": {
    "title": "Position: Measure Dataset Diversity, Don't Just Claim It",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dora Zhao",
      "Jerone Andrews",
      "Orestis Papakyriakopoulos",
      "Alice Xiang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24b.html": {
    "title": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhao",
      "Maksym Andriushchenko",
      "Francesco Croce",
      "Nicolas Flammarion"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24c.html": {
    "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Zhao",
      "Rob Brekelmans",
      "Alireza Makhzani",
      "Roger Baker Grosse"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24d.html": {
    "title": "Image Fusion via Vision-Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiang Zhao",
      "Lilun Deng",
      "Haowen Bai",
      "Yukun Cui",
      "Zhipeng Zhang",
      "Yulun Zhang",
      "Haotong Qin",
      "Dongdong Chen",
      "Jiangshe Zhang",
      "Peng Wang",
      "Luc Van Gool"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24e.html": {
    "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Zhao",
      "Zhun Deng",
      "David Madras",
      "James Zou",
      "Mengye Ren"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24f.html": {
    "title": "VideoPrism: A Foundational Visual Encoder for Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Zhao",
      "Nitesh Bharadwaj Gundavarapu",
      "Liangzhe Yuan",
      "Hao Zhou",
      "Shen Yan",
      "Jennifer J. Sun",
      "Luke Friedman",
      "Rui Qian",
      "Tobias Weyand",
      "Yue Zhao",
      "Rachel Hornung",
      "Florian Schroff",
      "Ming-Hsuan Yang",
      "David A Ross",
      "Huisheng Wang",
      "Hartwig Adam",
      "Mikhail Sirotenko",
      "Ting Liu",
      "Boqing Gong"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24g.html": {
    "title": "APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhao",
      "Hannaneh Hajishirzi",
      "Qingqing Cao"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24h.html": {
    "title": "Subgoal-based Demonstration Learning for Formal Theorem Proving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueliang Zhao",
      "Wenda Li",
      "Lingpeng Kong"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24i.html": {
    "title": "Absolute Policy Optimization: Enhancing Lower Probability Bound of Performance with High Confidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiye Zhao",
      "Feihan Li",
      "Yifan Sun",
      "Rui Chen",
      "Tianhao Wei",
      "Changliu Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24j.html": {
    "title": "Spider: A Unified Framework for Context-dependent Concept Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqi Zhao",
      "Youwei Pang",
      "Wei Ji",
      "Baicheng Sheng",
      "Jiaming Zuo",
      "Lihe Zhang",
      "Huchuan Lu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24k.html": {
    "title": "Rethinking Adversarial Robustness in the Context of the Right to be Forgotten",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxu Zhao",
      "Wei Qian",
      "Yangyi Li",
      "Aobo Chen",
      "Mengdi Huai"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24l.html": {
    "title": "Quantum Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Zhao",
      "Wenbo Qiao",
      "Peng Zhang",
      "Hui Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24m.html": {
    "title": "Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhao",
      "Mengdi Wang",
      "Yu Bai"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24n.html": {
    "title": "A Statistical Theory of Regularization-Based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuyang Zhao",
      "Huiyuan Wang",
      "Weiran Huang",
      "Wei Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24o.html": {
    "title": "Two Fists, One Heart: Multi-Objective Optimization Based Strategy Fusion for Long-tailed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Zhao",
      "Pengkun Wang",
      "Haibin Wen",
      "Wei Xu",
      "Lai Song",
      "Qingfu Zhang",
      "Yang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24p.html": {
    "title": "Gradient-based Visual Explanation for Transformer-based CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang Zhao",
      "Kun Wang",
      "Xingyu Zeng",
      "Rui Zhao",
      "Antoni B. Chan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24q.html": {
    "title": "CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinlin Zhao",
      "Jindong Wang",
      "Yixuan Zhang",
      "Yiqiao Jin",
      "Kaijie Zhu",
      "Hao Chen",
      "Xing Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24r.html": {
    "title": "Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyi Zhao",
      "Depeng Xu",
      "Shuhan Yuan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24s.html": {
    "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Zhao",
      "Zhenyu Zhang",
      "Beidi Chen",
      "Zhangyang Wang",
      "Anima Anandkumar",
      "Yuandong Tian"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24t.html": {
    "title": "When Will Gradient Regularization Be Harmful?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhao",
      "Hao Zhang",
      "Xiuyuan Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24u.html": {
    "title": "LangCell: Language-Cell Pre-training for Cell Identity Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyuan Zhao",
      "Jiahuan Zhang",
      "Yushuai Wu",
      "Yizhen Luo",
      "Zaiqing Nie"
    ]
  },
  "https://proceedings.mlr.press/v235/zhao24v.html": {
    "title": "Optimistic Multi-Agent Policy Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshuai Zhao",
      "Yi Zhao",
      "Zhiyuan Li",
      "Juho Kannala",
      "Joni Pajarinen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhdanov24a.html": {
    "title": "Clifford-Steerable Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksim Zhdanov",
      "David Ruhe",
      "Maurice Weiler",
      "Ana Lucic",
      "Johannes Brandstetter",
      "Patrick Forré"
    ]
  },
  "https://proceedings.mlr.press/v235/zhen24a.html": {
    "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Zhen",
      "Xiaowen Qiu",
      "Peihao Chen",
      "Jincheng Yang",
      "Xin Yan",
      "Yilun Du",
      "Yining Hong",
      "Chuang Gan"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24a.html": {
    "title": "How Does Goal Relabeling Improve Sample Efficiency?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Zheng",
      "Chenjia Bai",
      "Zhuoran Yang",
      "Zhaoran Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24b.html": {
    "title": "PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Zheng",
      "Ching-An Cheng",
      "Hal Daumé Iii",
      "Furong Huang",
      "Andrey Kolobov"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24c.html": {
    "title": "Exploiting Negative Samples: A Catalyst for Cohort Discovery in Healthcare Analytics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiping Zheng",
      "Horng-Ruey Chua",
      "Melanie Herschel",
      "H. V. Jagadish",
      "Beng Chin Ooi",
      "James Wei Luen Yip"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24d.html": {
    "title": "Constrained Exploration via Reflected Replica Exchange Stochastic Gradient Langevin Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Zheng",
      "Hengrong Du",
      "Qi Feng",
      "Wei Deng",
      "Guang Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24e.html": {
    "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyuan Zheng",
      "Boyu Gou",
      "Jihyung Kil",
      "Huan Sun",
      "Yu Su"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24f.html": {
    "title": "Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Candi Zheng",
      "Yuan Lan"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24g.html": {
    "title": "Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Zheng",
      "Yongyuan Liang",
      "Xiyao Wang",
      "Shuang Ma",
      "Hal Daumé Iii",
      "Huazhe Xu",
      "John Langford",
      "Praveen Palanisamy",
      "Kalyan Shankar Basu",
      "Furong Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24h.html": {
    "title": "ESM All-Atom: Multi-Scale Protein Language Model for Unified Molecular Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangjie Zheng",
      "Siyu Long",
      "Tianyu Lu",
      "Junwei Yang",
      "Xinyu Dai",
      "Ming Zhang",
      "Zaiqing Nie",
      "Wei-Ying Ma",
      "Hao Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24i.html": {
    "title": "BAT: Learning to Reason about Spatial Sounds with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhisheng Zheng",
      "Puyuan Peng",
      "Ziyang Ma",
      "Xie Chen",
      "Eunsol Choi",
      "David Harwath"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24j.html": {
    "title": "Conformal Predictions under Markovian Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frédéric Zheng",
      "Alexandre Proutiere"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24k.html": {
    "title": "Detecting and Identifying Selection Structure in Sequential Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujia Zheng",
      "Zeyu Tang",
      "Yiwen Qiu",
      "Bernhard Schölkopf",
      "Kun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24l.html": {
    "title": "ContPhy: Continuum Physical Concept Learning and Reasoning from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicheng Zheng",
      "Xin Yan",
      "Zhenfang Chen",
      "Jingzhou Wang",
      "Qin Zhi Eddie Lim",
      "Joshua B. Tenenbaum",
      "Chuang Gan"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24m.html": {
    "title": "DPN: Decoupling Partition and Navigation for Neural Solvers of Min-max Vehicle Routing Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Zheng",
      "Shunyu Yao",
      "Zhenkun Wang",
      "Tong Xialiang",
      "Mingxuan Yuan",
      "Ke Tang"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24n.html": {
    "title": "On Prompt-Driven Safeguarding for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chujie Zheng",
      "Fan Yin",
      "Hao Zhou",
      "Fandong Meng",
      "Jie Zhou",
      "Kai-Wei Chang",
      "Minlie Huang",
      "Nanyun Peng"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24o.html": {
    "title": "Self-Infilling Code Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zheng",
      "Jianbo Yuan",
      "Zhi Zhang",
      "Hongxia Yang",
      "Lingpeng Kong"
    ]
  },
  "https://proceedings.mlr.press/v235/zheng24p.html": {
    "title": "Multi-layer Rehearsal Feature Augmentation for Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zheng",
      "Da-Wei Zhou",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhong24a.html": {
    "title": "ERQ: Error Reduction for Post-Training Quantization of Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunshan Zhong",
      "Jiawei Hu",
      "You Huang",
      "Yuxin Zhang",
      "Rongrong Ji"
    ]
  },
  "https://proceedings.mlr.press/v235/zhong24b.html": {
    "title": "Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhong",
      "Jiachen Hu",
      "Yecheng Xue",
      "Tongyang Li",
      "Liwei Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhong24c.html": {
    "title": "Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuyang Zhong",
      "Yixiao Huang",
      "Chen Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhong24d.html": {
    "title": "GNNs Also Deserve Editing, and They Need It More Than Once",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaochen Zhong",
      "Duy Le",
      "Zirui Liu",
      "Zhimeng Jiang",
      "Andrew Ye",
      "Jiamu Zhang",
      "Jiayi Yuan",
      "Kaixiong Zhou",
      "Zhaozhuo Xu",
      "Jing Ma",
      "Shuai Xu",
      "Vipin Chaudhary",
      "Xia Hu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhong24e.html": {
    "title": "Causal-IQA: Towards the Generalization of Image Quality Assessment Based on Causal Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Zhong",
      "Xingyu Wu",
      "Li Zhang",
      "Chenxi Yang",
      "Tingting Jiang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24a.html": {
    "title": "Jacobian Regularizer-based Neural Granger Causality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanqi Zhou",
      "Shuanghao Bai",
      "Shujian Yu",
      "Qibin Zhao",
      "Badong Chen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24b.html": {
    "title": "Differentially Private Worst-group Risk Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhou",
      "Raef Bassily"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24c.html": {
    "title": "MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianan Zhou",
      "Zhiguang Cao",
      "Yaoxin Wu",
      "Wen Song",
      "Yining Ma",
      "Jie Zhang",
      "Xu Chi"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24d.html": {
    "title": "MD tree: a model-diagnostic tree grown on loss landscape",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yefan Zhou",
      "Jianlong Chen",
      "Qinxue Cao",
      "Konstantin Schürholt",
      "Yaoqing Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24e.html": {
    "title": "On the Emergence of Cross-Task Linearity in Pretraining-Finetuning Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanpeng Zhou",
      "Zijun Chen",
      "Yilan Chen",
      "Bo Zhang",
      "Junchi Yan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24f.html": {
    "title": "RoboDreamer: Learning Compositional World Models for Robot Imagination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Zhou",
      "Yilun Du",
      "Jiaben Chen",
      "Yandong Li",
      "Dit-Yan Yeung",
      "Chuang Gan"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24g.html": {
    "title": "MultiMax: Sparse and Multi-Modal Attention Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Zhou",
      "Mario Fritz",
      "Margret Keuper"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24h.html": {
    "title": "Finite-Time Convergence and Sample Complexity of Actor-Critic Multi-Objective Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchen Zhou",
      "Fnu Hairi",
      "Haibo Yang",
      "Jia Liu",
      "Tian Tong",
      "Fan Yang",
      "Michinari Momma",
      "Yan Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24i.html": {
    "title": "Attack-free Evaluating and Enhancing Adversarial Robustness on Categorical Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujun Zhou",
      "Yufei Han",
      "Haomin Zhuang",
      "Hongyan Bao",
      "Xiangliang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24j.html": {
    "title": "Pedestrian Attribute Recognition as Label-balanced Multi-label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Zhou",
      "Hai-Miao Hu",
      "Yirong Xiang",
      "Xiaokang Zhang",
      "Haotian Wu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24k.html": {
    "title": "Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yvonne Zhou",
      "Mingyu Liang",
      "Ivan Brugere",
      "Danial Dervovic",
      "Antigoni Polychroniadou",
      "Min Wu",
      "Dana Dachman-Soled"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24l.html": {
    "title": "Conformalized Adaptive Forecasting of Heterogeneous Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfei Zhou",
      "Lars Lindemann",
      "Matteo Sesia"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24m.html": {
    "title": "Sequential Kernel Goodness-of-fit Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyu Zhou",
      "Weiwei Liu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24n.html": {
    "title": "RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Zhou",
      "Linye Lyu",
      "Daojing He",
      "Yu Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24o.html": {
    "title": "CurBench: Curriculum Learning Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Zhou",
      "Zirui Pan",
      "Xin Wang",
      "Hong Chen",
      "Haoyang Li",
      "Yanwen Huang",
      "Zhixiao Xiong",
      "Fangzhou Xiong",
      "Peiyang Xu",
      "Shengnan Liu",
      "Wenwu Zhu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24p.html": {
    "title": "GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Zhou",
      "Xingjian Ran",
      "Yajiao Xiong",
      "Jinlin He",
      "Zhiwei Lin",
      "Yongtao Wang",
      "Deqing Sun",
      "Ming-Hsuan Yang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24q.html": {
    "title": "Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangxin Zhou",
      "Liang Wang",
      "Yichi Zhou"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24r.html": {
    "title": "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy Zhou",
      "Kai Yan",
      "Michal Shlapentokh-Rothman",
      "Haohan Wang",
      "Yu-Xiong Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24s.html": {
    "title": "DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Zhou",
      "Ming Yang",
      "Jiang-Xin Shi",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24t.html": {
    "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhou",
      "Andrea Zanette",
      "Jiayi Pan",
      "Sergey Levine",
      "Aviral Kumar"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24u.html": {
    "title": "Graphon Mean Field Games with a Representative Player: Analysis and Learning Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuzhong Zhou",
      "Chenyu Zhang",
      "Xu Chen",
      "Xuan Di"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24v.html": {
    "title": "Effective Federated Graph Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhou",
      "Zijie Zhang",
      "Zeru Zhang",
      "Lingjuan Lyu",
      "Wei-Shinn Ku"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24w.html": {
    "title": "Exploring Training on Heterogeneous Data with Mixture of Low-rank Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Zhou",
      "Zihua Zhao",
      "Siyuan Du",
      "Haolin Li",
      "Jiangchao Yao",
      "Ya Zhang",
      "Yanfeng Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhou24x.html": {
    "title": "Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyuan Zhou",
      "Huangjie Zheng",
      "Zhendong Wang",
      "Mingzhang Yin",
      "Hai Huang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24a.html": {
    "title": "Iterative Search Attribution for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Zhu",
      "Huaming Chen",
      "Xinyi Wang",
      "Jiayu Zhang",
      "Zhibo Jin",
      "Jason Xue",
      "Jun Shen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24b.html": {
    "title": "Generative Active Learning for Long-tailed Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muzhi Zhu",
      "Chengxiang Fan",
      "Hao Chen",
      "Yang Liu",
      "Weian Mao",
      "Xiaogang Xu",
      "Chunhua Shen"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24c.html": {
    "title": "Asymmetry in Low-Rank Adapters of Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Zhu",
      "Kristjan Greenewald",
      "Kimia Nadjahi",
      "Haitz Sáez De Ocáriz Borde",
      "Rickard Brüel Gabrielsson",
      "Leshem Choshen",
      "Marzyeh Ghassemi",
      "Mikhail Yurochkin",
      "Justin Solomon"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24d.html": {
    "title": "Improving Open-Ended Text Generation via Adaptive Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhong Zhu",
      "Hongkun Hao",
      "Zhiwei He",
      "Yiming Ai",
      "Rui Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24e.html": {
    "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Banghua Zhu",
      "Michael Jordan",
      "Jiantao Jiao"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24f.html": {
    "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianghui Zhu",
      "Bencheng Liao",
      "Qian Zhang",
      "Xinlong Wang",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24g.html": {
    "title": "Switched Flow Matching: Eliminating Singularities via Switching ODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qunxi Zhu",
      "Wei Lin"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24h.html": {
    "title": "Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libin Zhu",
      "Chaoyue Liu",
      "Adityanarayanan Radhakrishnan",
      "Mikhail Belkin"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24i.html": {
    "title": "Toward Availability Attacks in 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhu",
      "Yibo Miao",
      "Yinpeng Dong",
      "Xiao-Shan Gao"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24j.html": {
    "title": "Antibody Design Using a Score-based Diffusion Model Guided by Evolutionary, Physical and Geometric Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Zhu",
      "Milong Ren",
      "Haicang Zhang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24k.html": {
    "title": "Online Learning in Betting Markets: Profit versus Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiqing Zhu",
      "Alexander Soen",
      "Yun Kuen Cheung",
      "Lexing Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24l.html": {
    "title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Didi Zhu",
      "Zhongyisun Sun",
      "Zexi Li",
      "Tao Shen",
      "Ke Yan",
      "Shouhong Ding",
      "Chao Wu",
      "Kun Kuang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24m.html": {
    "title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaijie Zhu",
      "Jindong Wang",
      "Qinlin Zhao",
      "Ruochen Xu",
      "Xing Xie"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24n.html": {
    "title": "CRoFT: Robust Fine-Tuning with Concurrent Optimization for OOD Generalization and Open-Set OOD Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zhu",
      "Yifeng Yang",
      "Qinying Gu",
      "Xinbing Wang",
      "Chenghu Zhou",
      "Nanyang Ye"
    ]
  },
  "https://proceedings.mlr.press/v235/zhu24o.html": {
    "title": "Language Models Represent Beliefs of Self and Others",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Zhu",
      "Zhining Zhang",
      "Yizhou Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhuang24a.html": {
    "title": "Stealthy Imitation: Reward-guided Environment-free Policy Stealing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiong Zhuang",
      "Maria-Irina Nicolae",
      "Mario Fritz"
    ]
  },
  "https://proceedings.mlr.press/v235/zhuang24b.html": {
    "title": "Reinformer: Max-Return Sequence Modeling for Offline RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifeng Zhuang",
      "Dengyun Peng",
      "Jinxin Liu",
      "Ziqi Zhang",
      "Donglin Wang"
    ]
  },
  "https://proceedings.mlr.press/v235/zhuang24c.html": {
    "title": "COALA: A Practical and Vision-Centric Federated Learning Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Zhuang",
      "Jian Xu",
      "Chen Chen",
      "Jingtao Li",
      "Lingjuan Lyu"
    ]
  },
  "https://proceedings.mlr.press/v235/zhuge24a.html": {
    "title": "GPTSwarm: Language Agents as Optimizable Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingchen Zhuge",
      "Wenyi Wang",
      "Louis Kirsch",
      "Francesco Faccio",
      "Dmitrii Khizbullin",
      "Jürgen Schmidhuber"
    ]
  },
  "https://proceedings.mlr.press/v235/zhuge24b.html": {
    "title": "Towards Efficient Spiking Transformer: a Token Sparsification Framework for Training and Inference Acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyang Zhuge",
      "Peisong Wang",
      "Xingting Yao",
      "Jian Cheng"
    ]
  },
  "https://proceedings.mlr.press/v235/ziemann24a.html": {
    "title": "Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ingvar Ziemann",
      "Stephen Tu",
      "George J. Pappas",
      "Nikolai Matni"
    ]
  },
  "https://proceedings.mlr.press/v235/zimerman24a.html": {
    "title": "Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itamar Zimerman",
      "Moran Baruch",
      "Nir Drucker",
      "Gilad Ezov",
      "Omri Soceanu",
      "Lior Wolf"
    ]
  },
  "https://proceedings.mlr.press/v235/zimerman24b.html": {
    "title": "Viewing Transformers Through the Lens of Long Convolutions Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itamar Zimerman",
      "Lior Wolf"
    ]
  },
  "https://proceedings.mlr.press/v235/zisman24a.html": {
    "title": "Emergence of In-Context Reinforcement Learning from Noise Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilya Zisman",
      "Vladislav Kurenkov",
      "Alexander Nikulin",
      "Viacheslav Sinii",
      "Sergey Kolesnikov"
    ]
  },
  "https://proceedings.mlr.press/v235/ziyin24a.html": {
    "title": "Symmetry Induces Structure and Constraint of Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liu Ziyin"
    ]
  },
  "https://proceedings.mlr.press/v235/zong24a.html": {
    "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongshuo Zong",
      "Ondrej Bohdal",
      "Tingyang Yu",
      "Yongxin Yang",
      "Timothy Hospedales"
    ]
  },
  "https://proceedings.mlr.press/v235/zong24b.html": {
    "title": "Fool Your (Vision and) Language Model with Embarrassingly Simple Permutations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongshuo Zong",
      "Tingyang Yu",
      "Ruchika Chavhan",
      "Bingchen Zhao",
      "Timothy Hospedales"
    ]
  },
  "https://proceedings.mlr.press/v235/zou24a.html": {
    "title": "Leveraging Attractor Dynamics in Spatial Navigation for Better Language Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolong Zou",
      "Xingxing Cao",
      "Xiaojiao Yang",
      "Bo Hong"
    ]
  },
  "https://proceedings.mlr.press/v235/zou24b.html": {
    "title": "Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic Response",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bob Junyi Zou",
      "Matthew E Levine",
      "Dessi P. Zaharieva",
      "Ramesh Johari",
      "Emily Fox"
    ]
  },
  "https://proceedings.mlr.press/v235/zou24c.html": {
    "title": "Compositional Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiong Zou",
      "Shanghang Zhang",
      "Haichen Zhou",
      "Yuhua Li",
      "Ruixuan Li"
    ]
  },
  "https://proceedings.mlr.press/v235/zou24d.html": {
    "title": "BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lancheng Zou",
      "Wenqian Zhao",
      "Shuo Yin",
      "Chen Bai",
      "Qi Sun",
      "Bei Yu"
    ]
  },
  "https://proceedings.mlr.press/v235/zrnic24a.html": {
    "title": "Active Statistical Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tijana Zrnic",
      "Emmanuel Candes"
    ]
  }
}