{
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Das_Embodied_Question_Answering_CVPR_2018_paper.html": {
    "title": "Embodied Question Answering",
    "volume": "main",
    "abstract": "We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question (\"What color is the car?\"). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question (\"orange\"). EmbodiedQA requires a range of AI skills -- language understanding, visual recognition, active perception, goal-driven navigation, commonsense reasoning, long-term memory, and grounding language into actions. In this work, we develop a dataset of questions and answers in House3D environments, evaluation metrics, and a hierarchical model trained with imitation and reinforcement learning",
    "checked": true,
    "id": "e5790afc079c6f36d6fe9235d6d253f3da631f51",
    "semantic_title": "embodied question answering",
    "citation_count": 0,
    "authors": [
      "Abhishek Das",
      "Samyak Datta",
      "Georgia Gkioxari",
      "Stefan Lee",
      "Devi Parikh",
      "Dhruv Batra"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Misra_Learning_by_Asking_CVPR_2018_paper.html": {
    "title": "Learning by Asking Questions",
    "volume": "main",
    "abstract": "We introduce an interactive learning framework for the development and testing of intelligent visual systems, called learning-by-asking (LBA). We explore LBA in context of the Visual Question Answering (VQA) task. LBA differs from standard VQA training in that most questions are not observed during training time, and the learner must ask questions it wants answers to. Thus, LBA more closely mimics natural learning and has the potential to be more data-efficient than the traditional VQA setting. We present a model that performs LBA on the CLEVR dataset, and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle. Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient. We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions",
    "checked": true,
    "id": "83f355634fac2b445a475ee81b8d16bf768188ec",
    "semantic_title": "learning by asking questions",
    "citation_count": 84,
    "authors": [
      "Ishan Misra",
      "Ross Girshick",
      "Rob Fergus",
      "Martial Hebert",
      "Abhinav Gupta",
      "Laurens van der Maaten"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bai_Finding_Tiny_Faces_CVPR_2018_paper.html": {
    "title": "Finding Tiny Faces in the Wild With Generative Adversarial Network",
    "volume": "main",
    "abstract": "Face detection techniques have been developed for decades, and one of remaining open challenges is detecting small faces in unconstrained conditions. The reason is that tiny faces are often lacking detailed information and blurring. In this paper, we proposed an algorithm to directly generate a clear high-resolution face from a blurry small one by adopting a generative adversarial network (GAN). Toward this end, the basic GAN formulation achieves it by super-resolving and refining sequentially (e.g. SR-GAN and cycle-GAN). However, we design a novel network to address the problem of super-resolving and refining jointly. We also introduce new training losses to guide the generator network to recover fine details and to promote the discriminator network to distinguish real vs. fake and face vs. non-face simultaneously. Extensive experiments on the challenging dataset WIDER FACE demonstrate the effectiveness of our proposed method in restoring a clear high-resolution face from a blurry small one, and show that the detection performance outperforms other state-of-the-art methods",
    "checked": true,
    "id": "13b91ecf02ffeb91497b45a890732818052f5541",
    "semantic_title": "finding tiny faces in the wild with generative adversarial network",
    "citation_count": 199,
    "authors": [
      "Yancheng Bai",
      "Yongqiang Zhang",
      "Mingli Ding",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Learning_Face_Age_CVPR_2018_paper.html": {
    "title": "Learning Face Age Progression: A Pyramid Architecture of GANs",
    "volume": "main",
    "abstract": "The two underlying requirements of face age progression, i.e. aging accuracy and identity permanence, are not well studied in the literature. In this paper, we present a novel generative adversarial network based approach. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while simultaneously keeping personalized properties stable. Further, to generate more lifelike facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects in a finer manner. The proposed method is applicable to diverse face samples in the presence of variations in pose, expression, makeup, etc., and remarkably vivid aging effects are achieved. Both visual fidelity and quantitative evaluations show that the approach advances the state-of-the-art",
    "checked": true,
    "id": "dd175b48707ba12c2174c3ceec183a743b057549",
    "semantic_title": "learning face age progression: a pyramid architecture of gans",
    "citation_count": 172,
    "authors": [
      "Hongyu Yang",
      "Di Huang",
      "Yunhong Wang",
      "Anil K. Jain"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chang_PairedCycleGAN_Asymmetric_Style_CVPR_2018_paper.html": {
    "title": "PairedCycleGAN: Asymmetric Style Transfer for Applying and Removing Makeup",
    "volume": "main",
    "abstract": "This paper introduces an automatic method for editing a portrait photo so that the subject appears to be wearing makeup in the style of another person in a reference photo. Our unsupervised learning approach relies on a new framework of cycle-consistent generative adversarial networks. Different from the image domain transfer problem, our style transfer problem involves two asymmetric functions: a forward function encodes example-based style transfer, whereas a backward function removes the style. We construct two coupled networks to implement these functions -- one that transfers makeup style and a second that can remove makeup -- such that the output of their successive application to an input photo will match the input. The learned style network can then quickly apply an arbitrary makeup style to an arbitrary photo. We demonstrate the effectiveness on a broad range of portraits and styles",
    "checked": true,
    "id": "14dddff53fb3d96e83656c31bd0b4a07036f3bfd",
    "semantic_title": "pairedcyclegan: asymmetric style transfer for applying and removing makeup",
    "citation_count": 256,
    "authors": [
      "Huiwen Chang",
      "Jingwan Lu",
      "Fisher Yu",
      "Adam Finkelstein"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mueller_GANerated_Hands_for_CVPR_2018_paper.html": {
    "title": "GANerated Hands for Real-Time 3D Hand Tracking From Monocular RGB",
    "volume": "main",
    "abstract": "We address the highly challenging problem of real-time 3D hand tracking based on a monocular RGB-only sequence. Our tracking method combines a convolutional neural network with a kinematic 3D hand model, such that it generalizes well to unseen data, is robust to occlusions and varying camera viewpoints, and leads to anatomically plausible as well as temporally smooth hand motions. For training our CNN we propose a novel approach for the synthetic generation of training data that is based on a geometrically consistent image-to-image translation network. To be more specific, we use a neural network that translates synthetic images to \"real\" images, such that the so-generated images follow the same statistical distribution as real-world hand images. For training this translation network we combine an adversarial loss and a cycle-consistency loss with a geometric consistency loss in order to preserve geometric properties (such as hand pose) during translation. We demonstrate that our hand tracking system outperforms the current state-of-the-art on challenging RGB-only footage",
    "checked": true,
    "id": "344a54da7dd406556d4b594337b87e3989a60da5",
    "semantic_title": "ganerated hands for real-time 3d hand tracking from monocular rgb",
    "citation_count": 521,
    "authors": [
      "Franziska Mueller",
      "Florian Bernard",
      "Oleksandr Sotnychenko",
      "Dushyant Mehta",
      "Srinath Sridhar",
      "Dan Casas",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Poier_Learning_Pose_Specific_CVPR_2018_paper.html": {
    "title": "Learning Pose Specific Representations by Predicting Different Views",
    "volume": "main",
    "abstract": "The labeled data required to learn pose estimation for articulated objects is difficult to provide in the desired quantity, realism, density, and accuracy. To address this issue, we develop a method to learn representations, which are very specific for articulated poses, without the need for labeled training data. We exploit the observation that the object pose of a known object is predictive for the appearance in any known view. That is, given only the pose and shape parameters of a hand, the hand's appearance from any viewpoint can be approximated. To exploit this observation, we train a model that - given input from one view - estimates a latent representation, which is trained to be predictive for the appearance of the object when captured from another viewpoint. Thus, the only necessary supervision is the second view. The training process of this model reveals an implicit pose representation in the latent space. Importantly, at test time the pose representation can be inferred using only a single view. In qualitative and quantitative experiments we show that the learned representations capture detailed pose information. Moreover, when training the proposed method jointly with labeled and unlabeled data, it consistently surpasses the performance of its fully supervised counterpart, while reducing the amount of needed labeled samples by at least one order of magnitude",
    "checked": true,
    "id": "fd630144f706e9383d8af5fd0cd4d267b4381b3a",
    "semantic_title": "learning pose specific representations by predicting different views",
    "citation_count": 26,
    "authors": [
      "Georg Poier",
      "David Schinagl",
      "Horst Bischof"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fang_Weakly_and_Semi_CVPR_2018_paper.html": {
    "title": "Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer",
    "volume": "main",
    "abstract": "Human body part parsing, or human semantic part segmentation, is fundamental to many computer vision tasks. In conventional semantic segmentation methods, the ground truth segmentations are provided, and fully convolutional networks (FCN) are trained in an end-to-end scheme. Although these methods have demonstrated impressive results, their performance highly depends on the quantity and quality of training data. In this paper, we present a novel method to generate synthetic human part segmentation data using easily-obtained human keypoint annotations. Our key idea is to exploit the anatomical similarity among human to transfer the parsing results of a person to another person with similar pose. Using these estimated results as additional training data, our semi-supervised model outperforms its strong-supervised counterpart by 6 mIOU on the PASCAL-Person-Part dataset, and we achieve state-of-the-art human parsing results. Our approach is general and can be readily extended to other object/animal parsing task assuming that their anatomical similarity can be annotated by keypoints. The proposed model and accompanying source code will be made publicly available",
    "checked": true,
    "id": "e0821e6bb9efb795b4593229e4eacd6d86cef7d9",
    "semantic_title": "weakly and semi supervised human body part parsing via pose-guided knowledge transfer",
    "citation_count": 122,
    "authors": [
      "Hao-Shu Fang",
      "Guansong Lu",
      "Xiaolin Fang",
      "Jianwen Xie",
      "Yu-Wing Tai",
      "Cewu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Person_Transfer_GAN_CVPR_2018_paper.html": {
    "title": "Person Transfer GAN to Bridge Domain Gap for Person Re-Identification",
    "volume": "main",
    "abstract": "Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT17 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN",
    "checked": true,
    "id": "7002d8c61be9f1ea210f88059df6955c88db62b7",
    "semantic_title": "person transfer gan to bridge domain gap for person re-identification",
    "citation_count": 1668,
    "authors": [
      "Longhui Wei",
      "Shiliang Zhang",
      "Wen Gao",
      "Qi Tian"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Spurr_Cross-Modal_Deep_Variational_CVPR_2018_paper.html": {
    "title": "Cross-Modal Deep Variational Hand Pose Estimation",
    "volume": "main",
    "abstract": "The human hand moves in complex and high-dimensional ways, making estimation of 3D hand pose configurations from images alone a challenging task. In this work we propose a method to learn a statistical hand model represented by a cross-modal trained latent space via a generative deep neural network. We derive an objective function from the variational lower bound of the VAE framework and jointly optimize the resulting cross-modal KL-divergence and the posterior reconstruction objective, naturally admitting a training regime that leads to a coherent latent space across multiple modalities such as RGB images, 2D keypoint detections or 3D hand configurations. Additionally, it grants a straightforward way of using semi-supervision. This latent space can be directly used to estimate 3D hand poses from RGB images, outperforming the state-of-the art in different settings. Furthermore, we show that our proposed method can be used without changes on depth images and performs comparably to specialized methods. Finally, the model is fully generative and can synthesize consistent pairs of hand configurations across modalities. We evaluate our method on both RGB and depth datasets and analyze the latent space qualitatively",
    "checked": true,
    "id": "75a32edc0fffd795d893a21c52a5b1f312217087",
    "semantic_title": "cross-modal deep variational hand pose estimation",
    "citation_count": 284,
    "authors": [
      "Adrian Spurr",
      "Jie Song",
      "Seonwook Park",
      "Otmar Hilliges"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_Disentangled_Person_Image_CVPR_2018_paper.html": {
    "title": "Disentangled Person Image Generation",
    "volume": "main",
    "abstract": "Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor, respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on the Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task",
    "checked": true,
    "id": "e7df0b3c8befa4263971d1b645545ebecdacec06",
    "semantic_title": "disentangled person image generation",
    "citation_count": 440,
    "authors": [
      "Liqian Ma",
      "Qianru Sun",
      "Stamatios Georgoulis",
      "Luc Van Gool",
      "Bernt Schiele",
      "Mario Fritz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bulat_Super-FAN_Integrated_Facial_CVPR_2018_paper.html": {
    "title": "Super-FAN: Integrated Facial Landmark Localization and Super-Resolution of Real-World Low Resolution Faces in Arbitrary Poses With GANs",
    "volume": "main",
    "abstract": "This paper addresses 2 challenging tasks: improving the quality of low resolution facial images and accurately locating the facial landmarks on such poor resolution images. To this end, we make the following 5 contributions: (a) we propose Super-FAN: the very first end-to-end system that addresses both tasks simultaneously, i.e. both improves face resolution and detects the facial landmarks. The novelty or Super-FAN lies in incorporating structural information in a GAN-based super-resolution algorithm via integrating a sub-network for face alignment through heatmap regression and optimizing a novel heatmap loss. (b) We illustrate the benefit of training the two networks jointly by reporting good results not only on frontal images (as in prior work) but on the whole spectrum of facial poses, and not only on synthetic low resolution images (as in prior work) but also on real-world images. (c) We improve upon the state-of-the-art in face super-resolution by proposing a new residual-based architecture. (d) Quantitatively, we show large improvement over the state-of-the-art for both face super-resolution and alignment. (e) Qualitatively, we show for the first time good results on real-world low resolution images",
    "checked": true,
    "id": "b2a6d94862035c0759ff73e183372dfb576ff946",
    "semantic_title": "super-fan: integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans",
    "citation_count": 305,
    "authors": [
      "Adrian Bulat",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Si_Multistage_Adversarial_Losses_CVPR_2018_paper.html": {
    "title": "Multistage Adversarial Losses for Pose-Based Human Image Synthesis",
    "volume": "main",
    "abstract": "Human image synthesis has extensive practical applications e.g. person re-identification and data augmentation for human pose estimation. However, it is much more challenging than rigid object synthesis, e.g. cars and chairs, due to the variability of human posture. In this paper, we propose a pose-based human image synthesis method which can keep the human posture unchanged in novel viewpoints. Furthermore, we adopt multistage adversarial losses separately for the foreground and background generation, which fully exploits the multi-modal characteristics of generative loss to generate more realistic looking images. We perform extensive experiments on the Human3.6M dataset and verify the effectiveness of each stage of our method. The generated human images not only keep the same pose as the input image, but also have clear detailed foreground and background. The quantitative comparison results illustrate that our approach achieves much better results than several state-of-the-art methods",
    "checked": true,
    "id": "5b908d99fbf779b85c35a2255abf76b52966070c",
    "semantic_title": "multistage adversarial losses for pose-based human image synthesis",
    "citation_count": 63,
    "authors": [
      "Chenyang Si",
      "Wei Wang",
      "Liang Wang",
      "Tieniu Tan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Eriksson_Rotation_Averaging_and_CVPR_2018_paper.html": {
    "title": "Rotation Averaging and Strong Duality",
    "volume": "main",
    "abstract": "In this paper we explore the role of duality principles within the problem of rotation averaging, a fundamental task in a wide range of computer vision applications. In its conventional form, rotation averaging is stated as a minimization over multiple rotation constraints. As these constraints are non-convex, this problem is generally considered challenging to solve globally. We show how to circumvent this difficulty through the use of Lagrangian duality. While such an approach is well-known it is normally not guaranteed to provide a tight relaxation. Based on spectral graph theory, we analytically prove that in many cases there is no duality gap unless the noise levels are severe. This allows us to obtain certifiably global solutions to a class of important non-convex problems in polynomial time. We also propose an efficient, scalable algorithm that out-performs general purpose numerical solvers and is able to handle the large problem instances commonly occurring in structure from motion settings. The potential of this proposed method is demonstrated on a number of different problems, consisting of both synthetic and real-world data",
    "checked": true,
    "id": "425e4520491475ee093d45c494a16452c7a3552c",
    "semantic_title": "rotation averaging and strong duality",
    "citation_count": 96,
    "authors": [
      "Anders Eriksson",
      "Carl Olsson",
      "Fredrik Kahl",
      "Tat-Jun Chin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Camposeco_Hybrid_Camera_Pose_CVPR_2018_paper.html": {
    "title": "Hybrid Camera Pose Estimation",
    "volume": "main",
    "abstract": "In this paper, we aim to solve the pose estimation problem of calibrated pinhole and generalized cameras w.r.t. a Structure-from-Motion (SfM) model by leveraging both 2D-3D correspondences as well as 2D-2D correspondences. Traditional approaches either focus on the use of 2D-3D matches, known as structure-based pose estimation or solely on 2D-2D matches (structure-less pose estimation). Absolute pose approaches are limited in their performance by the quality of the 3D point triangulations as well as the completeness of the 3D model. Relative pose approaches, on the other hand, while being more accurate, also tend to be far more computationally costly and often return dozens of possible solutions. This work aims to bridge the gap between these two paradigms. We propose a new RANSAC-based approach that automatically chooses the best type of solver to use at each iteration in a data-driven way. The solvers chosen by our RANSAC can range from pure structure-based or structure-less solvers, to any possible combination of hybrid solvers (i.e. using both types of matches) in between. A number of these new hybrid minimal solvers are also presented in this paper. Both synthetic and real data experiments show our approach to be as accurate as structure-less approaches, while staying close to the efficiency of structure-based methods",
    "checked": true,
    "id": "272243ecb19d1693ce0cb1996d7d6b2af4cdbe0a",
    "semantic_title": "hybrid camera pose estimation",
    "citation_count": 59,
    "authors": [
      "Federico Camposeco",
      "Andrea Cohen",
      "Marc Pollefeys",
      "Torsten Sattler"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Briales_A_Certifiably_Globally_CVPR_2018_paper.html": {
    "title": "A Certifiably Globally Optimal Solution to the Non-Minimal Relative Pose Problem",
    "volume": "main",
    "abstract": "Finding the relative pose between two calibrated views ranks among the most fundamental geometric vision problems. It therefore appears as somewhat a surprise that a globally optimal solver that minimizes a properly defined energy over non-minimal correspondence sets and in the original space of relative transformations has yet to be discovered. This, notably, is the contribution of the present paper. We formulate the problem as a Quadratically Constrained Quadratic Program (QCQP), which can be converted into a Semidefinite Program (SDP) using Shor's convex relaxation. While a theoretical proof for the tightness of this relaxation remains open, we prove through exhaustive validation on both simulated and real experiments that our approach always finds and certifies (a-posteriori) the global optimum of the cost function",
    "checked": true,
    "id": "bac046b04377696f191341d3fce10327f54064c1",
    "semantic_title": "a certifiably globally optimal solution to the non-minimal relative pose problem",
    "citation_count": 56,
    "authors": [
      "Jesus Briales",
      "Laurent Kneip",
      "Javier Gonzalez-Jimenez"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_Single_View_Stereo_CVPR_2018_paper.html": {
    "title": "Single View Stereo Matching",
    "volume": "main",
    "abstract": "Previous monocular depth estimation methods take a single view and directly regress the expected results. Though recent advances are made by applying geometrically inspired loss functions during training, the inference procedure does not explicitly impose any geometrical constraint. Therefore these models purely rely on the quality of data and the effectiveness of learning to generalize. This either leads to suboptimal results or the demand of huge amount of expensive ground truth labelled data to generate reasonable results. In this paper, we show for the first time that the monocular depth estimation problem can be reformulated as two sub-problems, a view synthesis procedure followed by stereo matching, with two intriguing properties, namely i) geometrical constraints can be explicitly imposed during inference; ii) demand on labelled depth data can be greatly alleviated. We show that the whole pipeline can still be trained in an end-to-end fashion and this new formulation plays a critical role in advancing the performance. The resulting model outperforms all the previous monocular depth estimation methods as well as the stereo block matching method in the challenging KITTI dataset by only using a small number of real training data. The model also generalizes well to other monocular depth estimation benchmarks. We also discuss the implications and the advantages of solving monocular depth estimation using stereo methods",
    "checked": true,
    "id": "c9d53a2d5c543f5a9bda6b3a5437d5ae5ad14b43",
    "semantic_title": "single view stereo matching",
    "citation_count": 194,
    "authors": [
      "Yue Luo",
      "Jimmy Ren",
      "Mude Lin",
      "Jiahao Pang",
      "Wenxiu Sun",
      "Hongsheng Li",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Haefner_Fight_Ill-Posedness_With_CVPR_2018_paper.html": {
    "title": "Fight Ill-Posedness With Ill-Posedness: Single-Shot Variational Depth Super-Resolution From Shading",
    "volume": "main",
    "abstract": "We put forward a principled variational approach for up-sampling a single depth map to the resolution of the companion color image provided by an RGB-D sensor. We combine heterogeneous depth and color data in order to jointly solve the ill-posed depth super-resolution and shape-from-shading problems. The low-frequency geometric information necessary to disambiguate shape-from-shading is extracted from the low-resolution depth measurements and, symmetrically, the high-resolution photometric clues in the RGB image provide the high-frequency information required to disambiguate depth super-resolution",
    "checked": true,
    "id": "45527a586939042821d3d6698032a21d2439e09d",
    "semantic_title": "fight ill-posedness with ill-posedness: single-shot variational depth super-resolution from shading",
    "citation_count": 41,
    "authors": [
      "Bjoern Haefner",
      "Yvain Quéau",
      "Thomas Möllenhoff",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Depth_Completion_CVPR_2018_paper.html": {
    "title": "Deep Depth Completion of a Single RGB-D Image",
    "volume": "main",
    "abstract": "The goal of our work is to complete the depth channel of an RGB-D image. Commodity-grade depth cameras often fail to sense depth for shiny, bright, transparent, and distant surfaces. To address this problem, we train a deep network that takes an RGB image as input and predicts dense surface normals and occlusion boundaries. Those predictions are then combined with raw depth observations provided by the RGB-D camera to solve for depths for all pixels, including those missing in the original observation. This method was chosen over others (e.g., inpainting depths directly) as the result of extensive experiments with a new depth completion benchmark dataset, where holes are filled in training data through the rendering of surface reconstructions created from multiview RGB-D scans. Experiments with different network inputs, depth representations, loss functions, optimization methods, inpainting methods, and deep depth estimation networks show that our proposed approach provides better depth completions than these alternatives",
    "checked": true,
    "id": "9766552d5d25519ed50bcafaf88b7701d2533477",
    "semantic_title": "deep depth completion of a single rgb-d image",
    "citation_count": 397,
    "authors": [
      "Yinda Zhang",
      "Thomas Funkhouser"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Multi-View_Harmonized_Bilinear_CVPR_2018_paper.html": {
    "title": "Multi-View Harmonized Bilinear Network for 3D Object Recognition",
    "volume": "main",
    "abstract": "View-based methods have achieved considerable success in $3$D object recognition tasks. Different from existing view-based methods pooling the view-wise features, we tackle this problem from the perspective of patches-to-patches similarity measurement. By exploiting the relationship between polynomial kernel and bilinear pooling, we obtain an effective $3$D object representation by aggregating local convolutional features through bilinear pooling. Meanwhile, we harmonize different components inherited in the pooled bilinear feature to obtain a more discriminative representation for a $3$D object. To achieve an end-to-end trainable framework, we incorporate the harmonized bilinear pooling operation as a layer of a network, constituting the proposed Multi-view Harmonized Bilinear Network (MHBN). Systematic experiments conducted on two public benchmark datasets demonstrate the efficacy of the proposed methods in $3$D object recognition",
    "checked": true,
    "id": "d68fb3a66b4e64abffa22d32e98378b82f601cc7",
    "semantic_title": "multi-view harmonized bilinear network for 3d object recognition",
    "citation_count": 257,
    "authors": [
      "Tan Yu",
      "Jingjing Meng",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_PPFNet_Global_Context_CVPR_2018_paper.html": {
    "title": "PPFNet: Global Context Aware Local Features for Robust 3D Point Matching",
    "volume": "main",
    "abstract": "We present PPFNet - Point Pair Feature NETwork for deeply learning a globally informed 3D local feature descriptor to find correspondences in unorganized point clouds. PPFNet learns local descriptors on pure geometry and is highly aware of the global context, an important cue in deep learning. Our 3D representation is computed as a collection of point-pair-features combined with the points and normals within a local vicinity. Our permutation invariant network design is inspired by PointNet and sets PPFNet to be ordering-free. As opposed to voxelization, our method is able to consume raw point clouds to exploit the full sparsity. PPFNet uses a novel N-tuple loss and architecture injecting the global information naturally into the local descriptor. It shows that context awareness also boosts the local feature representation. Qualitative and quantitative evaluations of our network suggest increased recall, improved robustness and invariance as well as a vital step in the 3D descriptor extraction performance",
    "checked": true,
    "id": "04258f92294d5dea89adb3e8607ae22eadb5332d",
    "semantic_title": "ppfnet: global context aware local features for robust 3d point matching",
    "citation_count": 550,
    "authors": [
      "Haowen Deng",
      "Tolga Birdal",
      "Slobodan Ilic"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_FoldingNet_Point_Cloud_CVPR_2018_paper.html": {
    "title": "FoldingNet: Point Cloud Auto-Encoder via Deep Grid Deformation",
    "volume": "main",
    "abstract": "Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet",
    "checked": true,
    "id": "572a5aa00f0569887469ffb7554699c21156ba0b",
    "semantic_title": "foldingnet: point cloud auto-encoder via deep grid deformation",
    "citation_count": 1159,
    "authors": [
      "Yaoqing Yang",
      "Chen Feng",
      "Yiru Shen",
      "Dong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.html": {
    "title": "A Papier-Mâché Approach to Learning 3D Surface Generation",
    "volume": "main",
    "abstract": "We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potentialfor other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation",
    "checked": true,
    "id": "840e804bb5ed1944e494959c2980a90bea0675c4",
    "semantic_title": "a papier-mache approach to learning 3d surface generation",
    "citation_count": 1183,
    "authors": [
      "Thibault Groueix",
      "Matthew Fisher",
      "Vladimir G. Kim",
      "Bryan C. Russell",
      "Mathieu Aubry"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_LEGO_Learning_Edge_CVPR_2018_paper.html": {
    "title": "LEGO: Learning Edge With Geometry All at Once by Watching Videos",
    "volume": "main",
    "abstract": "Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network is attracting significant attention. In this paper, we introduce a \"3D as-smooth-as-possible (3D-ASAP)\" prior inside the pipeline, which enables joint estimation of edges and 3D scene, yielding results with significant improvement in accuracy for fine detailed structures. Specifically, we define the 3D-ASAP prior by requiring that any two points recovered in 3D from an image should lie on an existing planar surface if no other cues provided. We design an unsupervised framework that Learns Edges and Geometry (depth, normal) all at Once (LEGO). The predicted edges are embedded into depth and surface normal smoothness terms, where pixels without edges in-between are constrained to satisfy the prior. In our framework, the predicted depths, normals and edges are forced to be consistent all the time. We conduct experiments on KITTI to evaluate our estimated geometry and CityScapes to perform edge evaluation. We show that in all of the tasks, i.e. depth, normal and edge, our algorithm vastly outperforms other state-of-the-art (SOTA) algorithms, demonstrating the benefits of our approach",
    "checked": true,
    "id": "4e8b5a15381da2592300ecdec8e20b6f99b00c02",
    "semantic_title": "lego: learning edge with geometry all at once by watching videos",
    "citation_count": 189,
    "authors": [
      "Zhenheng Yang",
      "Peng Wang",
      "Yang Wang",
      "Wei Xu",
      "Ram Nevatia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Barath_Five-Point_Fundamental_Matrix_CVPR_2018_paper.html": {
    "title": "Five-Point Fundamental Matrix Estimation for Uncalibrated Cameras",
    "volume": "main",
    "abstract": "We aim at estimating the fundamental matrix in two views from five correspondences of rotation invariant features obtained by e.g. the SIFT detector. The proposed minimal solver first estimates a homography from three correspondences assuming that they are co-planar and exploiting their rotational components. Then the fundamental matrix is obtained from the homography and two additional point pairs in general position. The proposed approach, combined with robust estimators like Graph-Cut RANSAC, is superior to other state-of-the-art algorithms both in terms of accuracy and number of iterations required. This is validated on synthesized data and 561 real image pairs. Moreover, the tests show that requiring three points on a plane is not too restrictive in urban environment and locally optimized robust estimators lead to accurate estimates even if the points are not entirely co-planar. As a potential application, we show that using the proposed method makes two-view multi-motion estimation more accurate",
    "checked": true,
    "id": "07081aed1c5faacd580156e74e226890ebd69a7a",
    "semantic_title": "five-point fundamental matrix estimation for uncalibrated cameras",
    "citation_count": 39,
    "authors": [
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.html": {
    "title": "PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation",
    "volume": "main",
    "abstract": "We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multi-stage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and application-agnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors. We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform on par or better than the state-of-the-art on these diverse datasets without any dataset-specific model tuning",
    "checked": true,
    "id": "8704a532929e309423060ebd7eff758b3a423837",
    "semantic_title": "pointfusion: deep sensor fusion for 3d bounding box estimation",
    "citation_count": 642,
    "authors": [
      "Danfei Xu",
      "Dragomir Anguelov",
      "Ashesh Jain"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kumar_Scalable_Dense_Non-Rigid_CVPR_2018_paper.html": {
    "title": "Scalable Dense Non-Rigid Structure-From-Motion: A Grassmannian Perspective",
    "volume": "main",
    "abstract": "This paper addresses the task of dense non-rigid structure-from-motion (NRSfM) using multiple images. State-of-the-art methods to this problem are often hurdled by scalability, expensive computations, and noisy measurements. Further, recent methods to NRSfM usually either assume a small number of sparse feature points or ignore local non-linearities of shape deformations, and thus cannot reliably model complex non-rigid deformations. To address these issues, in this paper, we propose a new approach for dense NRSfM by modeling the problem on a Grassmann manifold. Specifically, we assume the complex non-rigid deformations lie on a union of local linear subspaces both spatially and temporally. This naturally allows for a compact representation of the complex non-rigid deformation over frames. We provide experimental results on several synthetic and real benchmark datasets. The procured results clearly demonstrate that our method, apart from being scalable and more accurate than state-of-the-art methods, is also more robust to noise and generalizes to highly non-linear deformations",
    "checked": true,
    "id": "514b44a90e9317f532ffecb047ae34d792af9420",
    "semantic_title": "scalable dense non-rigid structure-from-motion: a grassmannian perspective",
    "citation_count": 58,
    "authors": [
      "Suryansh Kumar",
      "Anoop Cherian",
      "Yuchao Dai",
      "Hongdong Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Feng_GVCNN_Group-View_Convolutional_CVPR_2018_paper.html": {
    "title": "GVCNN: Group-View Convolutional Neural Networks for 3D Shape Recognition",
    "volume": "main",
    "abstract": "3D shape recognition has attracted much attention recently. Its recent advances advocate the usage of deep features and achieve the state-of-the-art performance. However, existing deep features for 3D shape recognition are restricted to a view-to-shape setting, which learns the shape descriptor from the view-level feature directly. Despite the exciting progress on view-based 3D shape description, the intrinsic hierarchical correlation and discriminability among views have not been well exploited, which is important for 3D shape representation. To tackle this issue, in this paper, we propose a group-view convolutional neural network (GVCNN) framework for hierarchical correlation modeling towards discriminative 3D shape description. The proposed GVCNN framework is composed of a hierarchical view-group-shape architecture, i.e., from the view level, the group level and the shape level, which are organized using a grouping strategy. Concretely, we first use an expanded CNN to extract a view level descriptor. Then, a grouping module is introduced to estimate the content discrimination of each view, based on which all views can be splitted into different groups according to their discriminative level. A group level description can be further generated by pooling from view descriptors. Finally, all group level descriptors are combined into the shape level descriptor according to their discriminative weights. Experimental results and comparison with state-of-the-art methods show that our proposed GVCNN method can achieve a significant performance gain on both the 3D shape classification and retrieval tasks",
    "checked": true,
    "id": "f60baea800d88a26f7224af76849cb117f3384e3",
    "semantic_title": "gvcnn: group-view convolutional neural networks for 3d shape recognition",
    "citation_count": 532,
    "authors": [
      "Yifan Feng",
      "Zizhao Zhang",
      "Xibin Zhao",
      "Rongrong Ji",
      "Yue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Depth_and_Transient_CVPR_2018_paper.html": {
    "title": "Depth and Transient Imaging With Compressive SPAD Array Cameras",
    "volume": "main",
    "abstract": "Time-of-flight depth imaging and transient imaging are two imaging modalities that have recently received a lot of interest. Despite much research, existing hardware systems are limited either in terms of temporal resolution or are prohibitively expensive. Arrays of Single Photon Avalanche Diodes (SPADs) promise to fill this gap by providing higher temporal resolution at an affordable cost. Unfortunately SPAD arrays are to date only available in relatively small resolutions. In this work we aim to overcome the spatial resolution limit of SPAD arrays by employing a compressive sensing camera design. Using a DMD and custom optics, we achieve an image resolution of up to 800*400 on SPAD Arrays of resolution 64*32. Using our new data fitting model for the time histograms, we suppress the noise while abstracting the phase and amplitude information, so as to realize a temporal resolution of a few tens of picoseconds",
    "checked": true,
    "id": "08be905a3436f2082374131317a27e9b971f0768",
    "semantic_title": "depth and transient imaging with compressive spad array cameras",
    "citation_count": 24,
    "authors": [
      "Qilin Sun",
      "Xiong Dun",
      "Yifan Peng",
      "Wolfgang Heidrich"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_GeoNet_Geometric_Neural_CVPR_2018_paper.html": {
    "title": "GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation",
    "volume": "main",
    "abstract": "In this paper, we propose Geometric Neural Network (GeoNet) to jointly predict depth and surface normal maps from a single image. Building on top of two-stream CNNs, our GeoNet incorporates geometric relation between depth and surface normal via the new depth-to-normal and normal- to-depth networks. Depth-to-normal network exploits the least square solution of surface normal from depth and im- proves its quality with a residual module. Normal-to-depth network, contrarily, refines the depth map based on the con- straints from the surface normal through a kernel regression module, which has no parameter to learn. These two net- works enforce the underlying model to efficiently predict depth and surface normal for high consistency and corre- sponding accuracy. Our experiments on NYU v2 dataset verify that our GeoNet is able to predict geometrically con- sistent depth and normal maps. It achieves top performance on surface normal estimation and is on par with state-of-the- art depth estimation methods",
    "checked": true,
    "id": "8a9c4f1b58258afa2016b0eca0b3bfd2dc2ba3d8",
    "semantic_title": "geonet: geometric neural network for joint depth and surface normal estimation",
    "citation_count": 353,
    "authors": [
      "Xiaojuan Qi",
      "Renjie Liao",
      "Zhengzhe Liu",
      "Raquel Urtasun",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tekin_Real-Time_Seamless_Single_CVPR_2018_paper.html": {
    "title": "Real-Time Seamless Single Shot 6D Object Pose Prediction",
    "volume": "main",
    "abstract": "We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task [Kehl et al. 2017] that only predicts an approximate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a result, it is much faster - 50 fps on a Titan X (Pascal) GPU - and more suitable for real-time processing. The key component of our method is a new CNN architecture inspired by [Redmon et al. 2016, Redmon and Farhadi 2017] that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm. For single object and multiple object pose estimation on the LineMod and Occlusion datasets, our approach substantially outperforms other recent CNN-based approaches [Kehl et al. 2017, Rad and Lepetit 2017] when they are all used without post-processing. During post-processing, a pose refinement step can be used to boost the accuracy of these two methods, but at 10 fps or less, they are much slower than our method",
    "checked": true,
    "id": "646a669f1dc38ae961fe41fbd3c83cab64ce9d53",
    "semantic_title": "real-time seamless single shot 6d object pose prediction",
    "citation_count": 795,
    "authors": [
      "Bugra Tekin",
      "Sudipta N. Sinha",
      "Pascal Fua"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tulsiani_Factoring_Shape_Pose_CVPR_2018_paper.html": {
    "title": "Factoring Shape, Pose, and Layout From the 2D Image of a 3D Scene",
    "volume": "main",
    "abstract": "The goal of this paper is to take a single 2D image of a scene and recover the 3D structure in terms of a small set of factors: a layout representing the enclosing surfaces as well as a set of objects represented in terms of shape and pose. We propose a convolutional neural network-based approach to predict this representation and benchmark it on a large dataset of indoor scenes. Our experiments evaluate a number of practical design questions, demonstrate that we can infer this representation, and quantitatively and qualitatively demonstrate its merits compared to alternate representations",
    "checked": true,
    "id": "81112cd8ec14d7b13673bea833448de611060648",
    "semantic_title": "factoring shape, pose, and layout from the 2d image of a 3d scene",
    "citation_count": 133,
    "authors": [
      "Shubham Tulsiani",
      "Saurabh Gupta",
      "David F. Fouhey",
      "Alexei A. Efros",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xian_Monocular_Relative_Depth_CVPR_2018_paper.html": {
    "title": "Monocular Relative Depth Perception With Web Stereo Data Supervision",
    "volume": "main",
    "abstract": "In this paper we study the problem of monocular relative depth perception in the wild. We introduce a simple yet effective method to automatically generate dense relative depth annotations from web stereo images, and propose a new dataset that consists of diverse images as well as corresponding dense relative depth maps. Further, an improved ranking loss is introduced to deal with imbalanced ordinal relations, enforcing the network to focus on a set of hard pairs. Experimental results demonstrate that our proposed approach not only achieves state-of-the-art accuracy of relative depth perception in the wild, but also benefits other dense per-pixel prediction tasks, e.g., metric depth estimation and semantic segmentation",
    "checked": true,
    "id": "9c39e9522f13fa3a9549a97a3850601386ff0de2",
    "semantic_title": "monocular relative depth perception with web stereo data supervision",
    "citation_count": 199,
    "authors": [
      "Ke Xian",
      "Chunhua Shen",
      "Zhiguo Cao",
      "Hao Lu",
      "Yang Xiao",
      "Ruibo Li",
      "Zhenbo Luo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ovren_Spline_Error_Weighting_CVPR_2018_paper.html": {
    "title": "Spline Error Weighting for Robust Visual-Inertial Fusion",
    "volume": "main",
    "abstract": "In this paper we derive and test a probability-based weighting that can balance residuals of different types in spline fitting. In contrast to previous formulations, the proposed spline error weighting scheme also incorporates a prediction of the approximation error of the spline fit. We demonstrate the effectiveness of the prediction in a synthetic experiment, and apply it to visual-inertial fusion on rolling shutter cameras. This results in a method that can estimate 3D structure with metric scale on generic first-person videos. We also propose a quality measure for spline fitting, that can be used to automatically select the knot spacing. Experiments verify that the obtained trajectory quality corresponds well with the requested quality. Finally, by linearly scaling the weights, we show that the proposed spline error weighting minimizes the estimation errors on real sequences, in terms of scale and end-point errors",
    "checked": true,
    "id": "4d9bf5d590c08edb6a95b76bafbaa9d05a4e9814",
    "semantic_title": "spline error weighting for robust visual-inertial fusion",
    "citation_count": 20,
    "authors": [
      "Hannes Ovrén",
      "Per-Erik Forssén"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Single-Image_Depth_Estimation_CVPR_2018_paper.html": {
    "title": "Single-Image Depth Estimation Based on Fourier Domain Analysis",
    "volume": "main",
    "abstract": "We propose a deep learning algorithm for single-image depth estimation based on the Fourier frequency domain analysis. First, we develop a convolutional neural network structure and propose a new loss function, called depth-balanced Euclidean loss, to train the network reliably for a wide range of depths. Then, we generate multiple depth map candidates by cropping input images with various cropping ratios. In general, a cropped image with a small ratio yields depth details more faithfully, while that with a large ratio provides the overall depth distribution more reliably. To take advantage of these complementary properties, we combine the multiple candidates in the frequency domain. Experimental results demonstrate that proposed algorithm provides the state-of-art performance. Furthermore, through the frequency domain analysis, we validate the efficacy of the proposed algorithm in most frequency bands",
    "checked": true,
    "id": "c90b38b0a64b22808ecd0e48170923b3bd459469",
    "semantic_title": "single-image depth estimation based on fourier domain analysis",
    "citation_count": 148,
    "authors": [
      "Jae-Han Lee",
      "Minhyeok Heo",
      "Kyung-Rae Kim",
      "Chang-Su Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.html": {
    "title": "Unsupervised Learning of Monocular Depth Estimation and Visual Odometry With Deep Feature Reconstruction",
    "volume": "main",
    "abstract": "Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/Depth-VO-Feat",
    "checked": true,
    "id": "f58592051db1f5d300d096f2cd7869258d2de9d8",
    "semantic_title": "unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction",
    "citation_count": 633,
    "authors": [
      "Huangying Zhan",
      "Ravi Garg",
      "Chamara Saroj Weerasekera",
      "Kejie Li",
      "Harsh Agarwal",
      "Ian Reid"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Girdhar_Detect-and-Track_Efficient_Pose_CVPR_2018_paper.html": {
    "title": "Detect-and-Track: Efficient Pose Estimation in Videos",
    "volume": "main",
    "abstract": "This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection and video understanding. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2% on the validation and 51.8% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge",
    "checked": true,
    "id": "5084ca7e4db3168e674414ae55cb6f4e682214e1",
    "semantic_title": "detect-and-track: efficient pose estimation in videos",
    "citation_count": 229,
    "authors": [
      "Rohit Girdhar",
      "Georgia Gkioxari",
      "Lorenzo Torresani",
      "Manohar Paluri",
      "Du Tran"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper.html": {
    "title": "Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors",
    "volume": "main",
    "abstract": "In this paper, we present supervision-by-registration, an unsupervised approach to improve the precision of facial landmark detectors on both images and video. Our key observation is that the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. Interestingly, coherency of optical flow is a source of supervision that does not require manual labeling, and can be leveraged during detector training. For example, we can enforce in the training loss function that a detected landmark at frame t-1 followed by optical flow tracking from frame t-1 to frame t should coincide with the location of the detection at frame t. Essentially, supervision-by-registration augments the training loss function with a registration loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos. End-to-end training with the registration loss is made possible by a differentiable Lucas-Kanade operation, which computes optical flow registration in the forward pass, and back-propagates gradients that encourage temporal coherency in the detector. The output of our method is a more precise image-based facial landmark detector, which can be applied to single images or video. With supervision-by-registration, we demonstrate (1) improvements in facial landmark detection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities), and (2) significant reduction of jittering in video detections",
    "checked": true,
    "id": "3534635d265932ed6c3e991cade973774e5e51ea",
    "semantic_title": "supervision-by-registration: an unsupervised approach to improve the precision of facial landmark detectors",
    "citation_count": 194,
    "authors": [
      "Xuanyi Dong",
      "Shoou-I Yu",
      "Xinshuo Weng",
      "Shih-En Wei",
      "Yi Yang",
      "Yaser Sheikh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Diversity_Regularized_Spatiotemporal_CVPR_2018_paper.html": {
    "title": "Diversity Regularized Spatiotemporal Attention for Video-Based Person Re-Identification",
    "volume": "main",
    "abstract": "Video-based person re-identification matches video clips of people across non-overlapping cameras. Most existing methods tackle this problem by encoding each video frame in its entirety and computing an aggregate representation across all frames. In practice, people are often partially occluded, which can corrupt the extracted features. Instead, we propose a new spatiotemporal attention model that automatically discovers a diverse set of distinctive body parts. This allows useful information to be extracted from all frames without succumbing to occlusions and misalignments. The network learns multiple spatial attention models and employs a diversity regularization term to ensure multiple models do not discover the same body part. Features extracted from local image regions are organized by spatial attention model and are combined using temporal attention. As a result, the network learns latent representations of the face, torso and other body parts using the best available image patches from the entire video sequence. Extensive evaluations on three datasets show that our framework outperforms the state-of-the-art approaches by large margins on multiple metrics",
    "checked": true,
    "id": "7c25f32e78d0a9ae75825445daf51b064c3f58e9",
    "semantic_title": "diversity regularized spatiotemporal attention for video-based person re-identification",
    "citation_count": 342,
    "authors": [
      "Shuang Li",
      "Slawomir Bak",
      "Peter Carr",
      "Xiaogang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Style_Aggregated_Network_CVPR_2018_paper.html": {
    "title": "Style Aggregated Network for Facial Landmark Detection",
    "volume": "main",
    "abstract": "Recent advances in facial landmark detection achieve success by learning discriminative features from rich deformation of face shapes and poses. Besides the variance of faces themselves, the intrinsic variance of image styles, e.g., grayscale vs. color images, light vs. dark, intense vs. dull, and so on, has constantly been overlooked. This issue becomes inevitable as increasing web images are collected from various sources for training neural networks. In this work, we propose a style-aggregated approach to deal with the large intrinsic variance of image styles for facial landmark detection. Our method transforms original face images to style-aggregated images by a generative adversarial module. The proposed scheme uses the style-aggregated image to maintain face images that are more robust to environmental changes. Then the original face images accompanying with style-aggregated ones play a duet to train a landmark detector which is complementary to each other. In this way, for each face, our method takes two images as input, i.e., one in its original style and the other in the aggregated style. In experiments, we observe that the large variance of image styles would degenerate the performance of facial landmark detectors. Moreover, we show the robustness of our method to the large variance of image styles by comparing to a variant of our approach, in which the generative adversarial module is removed, and no style-aggregated images are used. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on benchmark datasets AFLW and 300-W. Code is publicly available on GitHub: https://github.com/D-X-Y/SAN",
    "checked": true,
    "id": "77875d6e4d8c7ed3baeb259fd5696e921f59d7ad",
    "semantic_title": "style aggregated network for facial landmark detection",
    "citation_count": 304,
    "authors": [
      "Xuanyi Dong",
      "Yan Yan",
      "Wanli Ouyang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Learning_Deep_Models_CVPR_2018_paper.html": {
    "title": "Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision",
    "volume": "main",
    "abstract": "Face anti-spoofing is crucial to prevent face recognition systems from a security breach. Previous deep learning approaches formulate face anti-spoofing as a binary classification problem. Many of them struggle to grasp adequate spoofing cues and generalize poorly. In this paper, we argue the importance of auxiliary supervision to guide the learning toward discriminative and generalizable cues. A CNN-RNN model is learned to estimate the face depth with pixel-wise supervision, and to estimate rPPG signals with sequence-wise supervision. The estimated depth and rPPG are fused to distinguish live vs. spoof faces. Further, we introduce a new face anti-spoofing database that covers a large range of illumination, subject, and pose variations. Experiments show that our model achieves the state-of-the-art results on both intra- and cross-database testing",
    "checked": true,
    "id": "95e6b0aa545da44e7e5d0cde9cf8d69c5e846a82",
    "semantic_title": "learning deep models for face anti-spoofing: binary or auxiliary supervision",
    "citation_count": 578,
    "authors": [
      "Yaojie Liu",
      "Amin Jourabloo",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Deep_Cost-Sensitive_and_CVPR_2018_paper.html": {
    "title": "Deep Cost-Sensitive and Order-Preserving Feature Learning for Cross-Population Age Estimation",
    "volume": "main",
    "abstract": "Facial age estimation from a face image is an important yet very challenging task in computer vision, since humans with different races and/or genders, exhibit quite different patterns in their facial aging processes. To deal with the influence of race and gender, previous methods perform age estimation within each population separately. In practice, however, it is often very difficult to collect and label sufficient data for each population. Therefore, it would be helpful to exploit an existing large labeled dataset of one (source) population to improve the age estimation performance on another (target) population with only a small labeled dataset available. In this work, we propose a Deep Cross-Population (DCP) age estimation model to achieve this goal. In particular, our DCP model develops a two-stage training strategy. First, a novel cost-sensitive multi-task loss function is designed to learn transferable aging features by training on the source population. Second, a novel order-preserving pair-wise loss function is designed to align the aging features of the two populations. By doing so, our DCP model can transfer the knowledge encoded in the source population to the target population. Extensive experiments on the two of the largest benchmark datasets show that our DCP model outperforms several strong baseline methods and many state-of-the-art methods",
    "checked": true,
    "id": "9ee35b278064554a1e3c2f9d8396f5a2fe5caa8e",
    "semantic_title": "deep cost-sensitive and order-preserving feature learning for cross-population age estimation",
    "citation_count": 34,
    "authors": [
      "Kai Li",
      "Junliang Xing",
      "Chi Su",
      "Weiming Hu",
      "Yundong Zhang",
      "Stephen Maybank"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.html": {
    "title": "First-Person Hand Action Benchmark With RGB-D Videos and 3D Hand Pose Annotations",
    "volume": "main",
    "abstract": "In this work we study the use of 3D hand poses to recognize first-person dynamic hand actions interacting with 3D objects. Towards this goal, we collected RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. To obtain hand pose annotations, we used our own mo-cap system that automatically infers the 3D location of each of the 21 joints of a hand model via 6 magnetic sensors and inverse kinematics. Additionally, we recorded the 6D object poses and provide 3D object models for a subset of hand-object interaction sequences. To the best of our knowledge, this is the first benchmark that enables the study of first-person hand actions with the use of 3D hand poses. We present an extensive experimental evaluation of RGB-D and pose-based action recognition by 18 baselines/state-of-the-art approaches. The impact of using appearance features, poses, and their combinations are measured, and the different training/testing protocols are evaluated. Finally, we assess how ready the 3D hand pose estimation field is when hands are severely occluded by objects in egocentric views and its influence on action recognition. From the results, we see clear benefits of using hand pose as a cue for action recognition compared to other data modalities. Our dataset and experiments can be of interest to communities of 3D hand pose estimation, 6D object pose, and robotics as well as action recognition",
    "checked": true,
    "id": "e85327e43f8b7e052a52ff9ee6b845cc0bee990d",
    "semantic_title": "first-person hand action benchmark with rgb-d videos and 3d hand pose annotations",
    "citation_count": 486,
    "authors": [
      "Guillermo Garcia-Hernando",
      "Shanxin Yuan",
      "Seungryul Baek",
      "Tae-Kyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sarfraz_A_Pose-Sensitive_Embedding_CVPR_2018_paper.html": {
    "title": "A Pose-Sensitive Embedding for Person Re-Identification With Expanded Cross Neighborhood Re-Ranking",
    "volume": "main",
    "abstract": "Person re-identification is a challenging retrieval task that requires matching a person's acquired image across non-overlapping camera views. In this paper we propose an effective approach that incorporates both the fine and coarse pose information of the person to learn a discrim- inative embedding. In contrast to the recent direction of explicitly modeling body parts or correcting for misalignment based on these, we show that a rather straightforward inclusion of acquired camera view and/or the detected joint locations into a convolutional neural network helps to learn a very effective representation. To increase retrieval performance, re-ranking techniques based on computed distances have recently gained much attention. We propose a new unsupervised and automatic re-ranking framework that achieves state-of-the-art re-ranking performance. We show that in contrast to the current state-of-the-art re-ranking methods our approach does not require to compute new rank lists for each image pair (e.g., based on reciprocal neighbors) and performs well by using simple direct rank list based comparison or even by just using the already computed euclidean distances between the images. We show that both our learned representation and our re-ranking method achieve state-of-the-art performance on a number of challenging surveillance image and video datasets",
    "checked": true,
    "id": "a4958d26a3aaf77081fcb9b76393a30bf722e92e",
    "semantic_title": "a pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking",
    "citation_count": 496,
    "authors": [
      "M. Saquib Sarfraz",
      "Arne Schumann",
      "Andreas Eberle",
      "Rainer Stiefelhagen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kumar_Disentangling_3D_Pose_CVPR_2018_paper.html": {
    "title": "Disentangling 3D Pose in a Dendritic CNN for Unconstrained 2D Face Alignment",
    "volume": "main",
    "abstract": "Heatmap regression has been used for landmark localization for quite a while now. Most of the methods use a very deep stack of bottleneck modules for heatmap classification stage, followed by heatmap regression to extract the keypoints. In this paper, we present a single dendritic CNN, termed as Pose Conditioned Dendritic Convolution Neural Network (PCD-CNN), where a classification network is followed by a second and modular classification network, trained in an end to end fashion to obtain accurate landmark points. Following a Bayesian formulation, we disentangle the 3D pose of a face image explicitly by conditioning the landmark estimation on pose, making it different from multi-tasking approaches. Extensive experimentation shows that conditioning on pose reduces the localization error by making it agnostic to face pose. The proposed model can be extended to yield variable number of landmark points and hence broadening its applicability to other datasets. Instead of increasing depth or width of the network, we train the CNN efficiently with Mask-Softmax Loss and hard sample mining to achieve upto 15% reduction in error compared to state-of-the-art methods for extreme and medium pose face images from challenging datasets including AFLW, AFW, COFW and IBUG",
    "checked": true,
    "id": "8a85f0865930ea65239adb5ec2b97407c1446fa4",
    "semantic_title": "disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment",
    "citation_count": 125,
    "authors": [
      "Amit Kumar",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_A_Hierarchical_Generative_CVPR_2018_paper.html": {
    "title": "A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation",
    "volume": "main",
    "abstract": "In this work, we introduce a Hierarchical Generative Model (HGM) to enable realistic forward eye image synthe- sis, as well as effective backward eye gaze estimation. The proposed HGM consists of a hierarchical generative shape model (HGSM), and a conditional bidirectional generative adversarial network (c-BiGAN). The HGSM encodes eye ge- ometry knowledge and relates eye gaze with eye shape, while c-BiGAN leverages on big data and captures the dependency between eye shape and eye appearance. As an intermedi- ate component, eye shape connects knowledge-based model (HGSM) with data-driven model (c-BiGAN) and enables bidirectional inference. Through a top-down inference, the HGM can synthesize eye images consistent with the given eye gaze. Through a bottom-up inference, HGM can infer eye gaze effectively from a given eye image. Qualitative and quantitative evaluations on benchmark datasets demonstrate our model's effectiveness on both eye image synthesis and eye gaze estimation. In addition, the proposed model is not restricted to eye images only. It can be adapted to face images and any shape-appearance related fields",
    "checked": true,
    "id": "e6e22d8505b98257af88d5dd1863d6b805128913",
    "semantic_title": "a hierarchical generative model for eye image synthesis and eye gaze estimation",
    "citation_count": 54,
    "authors": [
      "Kang Wang",
      "Rui Zhao",
      "Qiang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.html": {
    "title": "MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition",
    "volume": "main",
    "abstract": "Human actions in videos are three-dimensional (3D) signals. Recent attempts use 3D convolutional neural networks (CNNs) to explore spatio-temporal information for human action recognition. Though promising, 3D CNNs have not achieved high performanceon on this task with respect to their well-established two-dimensional (2D) counterparts for visual recognition in still images. We argue that the high training complexity of spatio-temporal fusion and the huge memory cost of 3D convolution hinder current 3D CNNs, which stack 3D convolutions layer by layer, by outputting deeper feature maps that are crucial for high-level tasks. We thus propose a Mixed Convolutional Tube (MiCT) that integrates 2D CNNs with the 3D convolution module to generate deeper and more informative feature maps, while reducing training complexity in each round of spatio-temporal fusion. A new end-to-end trainable deep 3D network, MiCT-Net, is also proposed based on the MiCT to better explore spatio-temporal information in human actions. Evaluations on three well-known benchmark datasets (UCF101, Sport-1M and HMDB-51) show that the proposed MiCT-Net significantly outperforms the original 3D CNNs. Compared with state-of-the-art approaches for action recognition on UCF101 and HMDB51, our MiCT-Net yields the best performance",
    "checked": true,
    "id": "b85b79d0535da7e994e419a75f65b2758bf90f21",
    "semantic_title": "mict: mixed 3d/2d convolutional tube for human action recognition",
    "citation_count": 207,
    "authors": [
      "Yizhou Zhou",
      "Xiaoyan Sun",
      "Zheng-Jun Zha",
      "Wenjun Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Learning_to_Estimate_CVPR_2018_paper.html": {
    "title": "Learning to Estimate 3D Human Pose and Shape From a Single Color Image",
    "volume": "main",
    "abstract": "This work addresses the problem of estimating the full body 3D human pose and shape from a single color image. This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efficient and effective direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction. Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks. These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further refinement of the network, by optimizing for the consistency of the projection with 2D annotations (i.e., 2D keypoints or masks). The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image",
    "checked": true,
    "id": "13ad6164dba75845f0f397e9314ad596e74eb946",
    "semantic_title": "learning to estimate 3d human pose and shape from a single color image",
    "citation_count": 607,
    "authors": [
      "Georgios Pavlakos",
      "Luyang Zhu",
      "Xiaowei Zhou",
      "Kostas Daniilidis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.html": {
    "title": "Glimpse Clouds: Human Activity Recognition From Unstructured Feature Points",
    "volume": "main",
    "abstract": "We propose a method for human activity recognition from RGB data that does not rely on any pose information during test time, and does not explicitly calculate pose information internally. Instead, a visual attention module learns to predict glimpse sequences in each frame. These glimpses correspond to interest points in the scene that are relevant to the classified activities. No spatial coherence is forced on the glimpse locations, which gives the attention module liberty to explore different points at each frame and better optimize the process of scrutinizing visual information. Tracking and sequentially integrating this kind of unstructured data is a challenge, which we address by sep- arating the set of glimpses from a set of recurrent tracking/recognition workers. These workers receive glimpses, jointly performing subsequent motion tracking and activity prediction. The glimpses are soft-assigned to the workers, optimizing coherence of the assignments in space, time and feature space using an external memory module. No hard decisions are taken, i.e. each glimpse point is assigned to all existing workers, albeit with different importance. Our methods outperform the state-of-the-art on the largest human activity recognition dataset available to-date, NTU RGB+D, and on the Northwestern-UCLA Multiview Action 3D Dataset",
    "checked": true,
    "id": "ab45ab887b7c1379bba4179579568296448d16d6",
    "semantic_title": "glimpse clouds: human activity recognition from unstructured feature points",
    "citation_count": 154,
    "authors": [
      "Fabien Baradel",
      "Christian Wolf",
      "Julien Mille",
      "Graham W. Taylor"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Choi_Context-Aware_Deep_Feature_CVPR_2018_paper.html": {
    "title": "Context-Aware Deep Feature Compression for High-Speed Visual Tracking",
    "volume": "main",
    "abstract": "We propose a new context-aware correlation filter based tracking framework to achieve both high computational speed and state-of-the-art performance among real-time trackers. The major contribution to the high computational speed lies in the proposed deep feature compression that is achieved by a context-aware scheme utilizing multiple expert auto-encoders; a context in our framework refers to the coarse category of the tracking target according to appearance patterns. In the pre-training phase, one expert auto-encoder is trained per category. In the tracking phase, the best expert auto-encoder is selected for a given target, and only this auto-encoder is used. To achieve high tracking performance with the compressed feature map, we introduce extrinsic denoising processes and a new orthogonality loss term for pre-training and fine-tuning of the expert auto-encoders. We validate the proposed context-aware framework through a number of experiments, where our method achieves a comparable performance to state-of-the-art trackers which cannot run in real-time, while running at a significantly fast speed of over 100 fps",
    "checked": true,
    "id": "d56031beb07dc253eadc89ede2217c14b383f577",
    "semantic_title": "context-aware deep feature compression for high-speed visual tracking",
    "citation_count": 193,
    "authors": [
      "Jongwon Choi",
      "Hyung Jin Chang",
      "Tobias Fischer",
      "Sangdoo Yun",
      "Kyuewang Lee",
      "Jiyeoup Jeong",
      "Yiannis Demiris",
      "Jin Young Choi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Correlation_Tracking_via_CVPR_2018_paper.html": {
    "title": "Correlation Tracking via Joint Discrimination and Reliability Learning",
    "volume": "main",
    "abstract": "For visual tracking, an ideal filter learned by the correlation filter (CF) method should take both discrimination and reliability information. However, existing attempts usually focus on the former one while pay less attention to reliability learning. This may make the learned filter be dominated by the unexpected salient regions on the feature map, thereby resulting in model degradation. To address this issue, we propose a novel CF-based optimization problem to jointly model the discrimination and reliability information. First, we treat the filter as the element-wise product of a base filter and a reliability term. The base filter is aimed to learn the discrimination information between the target and backgrounds, and the reliability term encourages the final filter to focus on more reliable regions. Second, we introduce a local response consistency regular term to emphasize equal contributions of different regions and avoid the tracker being dominated by unreliable regions. The proposed optimization problem can be solved using the alternating direction method and speeded up in the Fourier domain. We conduct extensive experiments on the OTB-2013, OTB-2015 and VOT-2016 datasets to evaluate the proposed tracker. Experimental results show that our tracker performs favorably against other state-of-the-art trackers",
    "checked": true,
    "id": "3f0da079ac950a4dfb699c41a90c087000e6ac38",
    "semantic_title": "correlation tracking via joint discrimination and reliability learning",
    "citation_count": 157,
    "authors": [
      "Chong Sun",
      "Dong Wang",
      "Huchuan Lu",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Meyer_PhaseNet_for_Video_CVPR_2018_paper.html": {
    "title": "PhaseNet for Video Frame Interpolation",
    "volume": "main",
    "abstract": "Most approaches for video frame interpolation require accurate dense correspondences to synthesize an in-between frame. Therefore, they do not perform well in challenging scenarios with e.g. lighting changes or motion blur. Recent deep learning approaches that rely on kernels to represent motion can only alleviate these problems to some extent. In those cases, methods that use a per-pixel phase-based motion representation have been shown to work well. However, they are only applicable for a limited amount of motion. We propose a new approach, PhaseNet, that is designed to robustly handle challenging scenarios while also coping with larger motion. Our approach consists of a neural network decoder that directly estimates the phase decomposition of the intermediate frame. We show that this is superior to the hand-crafted heuristics previously used in phase-based methods and also compares favorably to recent deep learning based approaches for video frame interpolation on challenging datasets",
    "checked": true,
    "id": "b934bdaaaed7af7269a368a8c93c87c293f876f8",
    "semantic_title": "phasenet for video frame interpolation",
    "citation_count": 181,
    "authors": [
      "Simone Meyer",
      "Abdelaziz Djelouah",
      "Brian McWilliams",
      "Alexander Sorkine-Hornung",
      "Markus Gross",
      "Christopher Schroers"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bideau_The_Best_of_CVPR_2018_paper.html": {
    "title": "The Best of Both Worlds: Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation",
    "volume": "main",
    "abstract": "Traditional methods of motion segmentation use powerful geometric constraints to understand motion, but fail to leverage the semantics of high-level image understanding. Modern CNN methods of motion analysis, on the other hand, excel at identifying well-known structures, but may not precisely characterize well-known geometric constraints. In this work, we build a new statistical model of rigid motion flow based on classical perspective projection constraints. We then combine piecewise rigid motions into complex deformable and articulated objects, guided by semantic segmentation from CNNs and a second ``object-level\" statistical model. This combination of classical geometric knowledge combined with the pattern recognition abilities of CNNs yields excellent performance on a wide range of motion segmentation benchmarks, from complex geometric scenes to camouflaged animals",
    "checked": true,
    "id": "31c6cc1b36e7d554fce055134f26c1e7f1c53041",
    "semantic_title": "the best of both worlds: combining cnns and geometric constraints for hierarchical motion segmentation",
    "citation_count": 45,
    "authors": [
      "Pia Bideau",
      "Aruni RoyChowdhury",
      "Rakesh R. Menon",
      "Erik Learned-Miller"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Hyperparameter_Optimization_for_CVPR_2018_paper.html": {
    "title": "Hyperparameter Optimization for Tracking With Continuous Deep Q-Learning",
    "volume": "main",
    "abstract": "Hyperparameters are numerical presets whose values are assigned prior to the commencement of the learning process. Selecting appropriate hyperparameters is critical for the accuracy of tracking algorithms, yet it is difficult to determine their optimal values, in particular, adaptive ones for each specific video sequence. Most hyperparameter optimization algorithms depend on searching a generic range and they are imposed blindly on all sequences. Here, we propose a novel hyperparameter optimization method that can find optimal hyperparameters for a given sequence using an action-prediction network leveraged on Continuous Deep Q-Learning. Since the common state-spaces for object tracking tasks are significantly more complex than the ones in traditional control problems, existing Continuous Deep Q-Learning algorithms cannot be directly applied. To overcome this challenge, we introduce an efficient heuristic to accelerate the convergence behavior. We evaluate our method on several tracking benchmarks and demonstrate its superior performance",
    "checked": true,
    "id": "66b53fdbc752401f1a55986c0ea68acd463e5b5f",
    "semantic_title": "hyperparameter optimization for tracking with continuous deep q-learning",
    "citation_count": 141,
    "authors": [
      "Xingping Dong",
      "Jianbing Shen",
      "Wenguan Wang",
      "Yu Liu",
      "Ling Shao",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Scale-Transferrable_Object_Detection_CVPR_2018_paper.html": {
    "title": "Scale-Transferrable Object Detection",
    "volume": "main",
    "abstract": "Scale problem lies in the heart of object detection. In this work, we develop a novel Scale-Transferrable Detection Network (STDN) for detecting multi-scale objects in images. In contrast to previous methods that simply combine object predictions from multiple feature maps from different network depths, the proposed network is equipped with embedded super-resolution layers (named as scale-transfer layer/module in this work) to explicitly explore the inter-scale consistency nature across multiple detection scales. Scale-transfer module naturally fits the base network with little computational cost. This module is further integrated with a dense convolutional network (DenseNet) to yield a one-stage object detector. We evaluate our proposed architecture on PASCAL VOC 2007 and MS COCO benchmark tasks and STDN obtains significant improvements over the comparable state-of-the-art detection models",
    "checked": true,
    "id": "35bcae6b3843a65eb60af2e4051ea1357fd697a6",
    "semantic_title": "scale-transferrable object detection",
    "citation_count": 301,
    "authors": [
      "Peng Zhou",
      "Bingbing Ni",
      "Cong Geng",
      "Jianguo Hu",
      "Yi Xu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_A_Prior-Less_Method_CVPR_2018_paper.html": {
    "title": "A Prior-Less Method for Multi-Face Tracking in Unconstrained Videos",
    "volume": "main",
    "abstract": "This paper presents a prior-less method for tracking and clustering an unknown number of human faces and maintaining their individual identities in unconstrained videos. The key challenge is to accurately track faces with partial occlusion and drastic appearance changes in multiple shots resulting from significant variations of makeup, facial expression, head pose and illumination. To address this challenge, we propose a new multi-face tracking and re-identification algorithm, which provides high accuracy in face association in the entire video with automatic cluster number generation, and is robust to outliers. We develop a co-occurrence model of multiple body parts to seamlessly create face tracklets, and recursively link tracklets to construct a graph for extracting clusters. A Gaussian Process model is introduced to compensate the deep feature insufficiency, and is further used to refine the linking results. The advantages of the proposed algorithm are demonstrated using a variety of challenging music videos and newly introduced body-worn camera videos. The proposed method obtains significant improvements over the state of the art [51], while relying less on handling video-specific prior information to achieve high performance",
    "checked": true,
    "id": "1a2b5b52e4b378a4b8b6f5adfc4a8d5d161ab012",
    "semantic_title": "a prior-less method for multi-face tracking in unconstrained videos",
    "citation_count": 23,
    "authors": [
      "Chung-Ching Lin",
      "Ying Hung"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_End-to-End_Flow_Correlation_CVPR_2018_paper.html": {
    "title": "End-to-End Flow Correlation Tracking With Spatial-Temporal Attention",
    "volume": "main",
    "abstract": "Discriminative correlation filters (DCF) with deep convolutional features have achieved favorable performance in recent tracking benchmarks. However, most of existing DCF trackers only consider appearance features of current frame, and hardly benefit from motion and inter-frame information. The lack of temporal information degrades the tracking performance during challenges such as partial occlusion and deformation. In this paper, we propose the FlowTrack, which focuses on making use of the rich flow information in consecutive frames to improve the feature representation and the tracking accuracy. The FlowTrack formulates individual components, including optical flow estimation, feature extraction, aggregation and correlation filters tracking as special layers in network. To the best of our knowledge, this is the first work to jointly train flow and tracking task in deep learning framework. Then the historical feature maps at predefined intervals are warped and aggregated with current ones by the guiding of flow. For adaptive aggregation, we propose a novel spatial-temporal attention mechanism. In experiments, the proposed method achieves leading performance on OTB2013, OTB2015, VOT2015 and VOT2016",
    "checked": true,
    "id": "7ccbb845829234548bfa9b24c61297b4f0cd678e",
    "semantic_title": "end-to-end flow correlation tracking with spatial-temporal attention",
    "citation_count": 266,
    "authors": [
      "Zheng Zhu",
      "Wei Wu",
      "Wei Zou",
      "Junjie Yan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xue_Deep_Texture_Manifold_CVPR_2018_paper.html": {
    "title": "Deep Texture Manifold for Ground Terrain Recognition",
    "volume": "main",
    "abstract": "We present a texture network called Deep Encoding Pooling Network (DEP) for the task of ground terrain recognition. Recognition of ground terrain is an important task in establishing robot or vehicular control parameters, as well as for localization within an outdoor environment. The architecture of DEP integrates orderless texture details and local spatial information and the performance of DEP surpasses state-of-the-art methods for this task. The GTOS database (comprised of over 30,000 images of 40 classes of ground terrain in outdoor scenes) enables supervised recognition. For evaluation under realistic conditions, we use test images that are not from the existing GTOS dataset, but are instead from hand-held mobile phone videos of similar terrain. This new evaluation dataset, GTOS-mobile, consists of 81 videos of 31 classes of ground terrain such as grass, gravel, asphalt and sand. The resultant network shows excellent performance not only for GTOS-mobile, but also for more general databases (MINC and DTD). Leveraging the discriminant features learned from this network, we build a new texture manifold called DEP-manifold. We learn a parametric distribution in feature space in a fully supervised manner, which gives the distance relationship among classes and provides a means to implicitly represent ambiguous class boundaries. The source code and database are publicly available",
    "checked": true,
    "id": "e11a9dda7d9c5f45a71fad4f7e10d2a37289ba4f",
    "semantic_title": "deep texture manifold for ground terrain recognition",
    "citation_count": 124,
    "authors": [
      "Jia Xue",
      "Hang Zhang",
      "Kristin Dana"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tu_Learning_Superpixels_With_CVPR_2018_paper.html": {
    "title": "Learning Superpixels With Segmentation-Aware Affinity Loss",
    "volume": "main",
    "abstract": "Superpixel segmentation has been widely used in many computer vision tasks. Existing superpixel algorithms are mainly based on hand-crafted features, which often fail to preserve weak object boundaries. In this work, we leverage deep neural networks to facilitate extracting superpixels from images. We show a simple integration of deep features with existing superpixel algorithms does not result in better performance as these features do not model segmentation. Instead, we propose a segmentation-aware affinity learning approach for superpixel segmentation. Specifically, we propose a new loss function that takes the segmentation error into account for affinity learning. We also develop the Pixel Affinity Net for affinity prediction. Extensive experimental results show that the proposed algorithm based on the learned segmentation-aware loss performs favorably against the state-of-the-art methods. We also demonstrate the use of the learned superpixels in numerous vision applications with consistent improvements",
    "checked": true,
    "id": "fdb0c07947740e2b2326eeaaed63411bed09df07",
    "semantic_title": "learning superpixels with segmentation-aware affinity loss",
    "citation_count": 116,
    "authors": [
      "Wei-Chih Tu",
      "Ming-Yu Liu",
      "Varun Jampani",
      "Deqing Sun",
      "Shao-Yi Chien",
      "Ming-Hsuan Yang",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Interactive_Image_Segmentation_CVPR_2018_paper.html": {
    "title": "Interactive Image Segmentation With Latent Diversity",
    "volume": "main",
    "abstract": "Interactive image segmentation is characterized by multimodality. When the user clicks on a door, do they intend to select the door or the whole house? We present an end-to-end learning approach to interactive image segmentation that tackles this ambiguity. Our architecture couples two convolutional networks. The first is trained to synthesize a diverse set of plausible segmentations that conform to the user's input. The second is trained to select among these. By selecting a single solution, our approach retains compatibility with existing interactive segmentation interfaces. By synthesizing multiple diverse solutions before selecting one, the architecture is given the representational power to explore the multimodal solution space. We show that the proposed approach outperforms existing methods for interactive image segmentation, including prior work that applied convolutional networks to this problem, while being much faster",
    "checked": true,
    "id": "d151204a6b8f28ebce356d5601f00eb854def7a6",
    "semantic_title": "interactive image segmentation with latent diversity",
    "citation_count": 200,
    "authors": [
      "Zhuwen Li",
      "Qifeng Chen",
      "Vladlen Koltun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html": {
    "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
    "volume": "main",
    "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ``perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations",
    "checked": true,
    "id": "c468bbde6a22d961829e1970e6ad5795e05418d1",
    "semantic_title": "the unreasonable effectiveness of deep features as a perceptual metric",
    "citation_count": 11785,
    "authors": [
      "Richard Zhang",
      "Phillip Isola",
      "Alexei A. Efros",
      "Eli Shechtman",
      "Oliver Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/He_Local_Descriptors_Optimized_CVPR_2018_paper.html": {
    "title": "Local Descriptors Optimized for Average Precision",
    "volume": "main",
    "abstract": "Extraction of local feature descriptors is a vital stage in the solution pipelines for numerous computer vision tasks. Learning-based approaches improve performance in certain tasks, but still cannot replace handcrafted features in general. In this paper, we improve the learning of local feature descriptors by optimizing the performance of descriptor matching, which is a common stage that follows descriptor extraction in local feature based pipelines, and can be formulated as nearest neighbor retrieval. Specifically, we directly optimize a ranking-based retrieval performance metric, Average Precision, using deep neural networks. This general-purpose solution can also be viewed as a listwise learning to rank approach, which is advantageous compared to recent local ranking approaches. On standard benchmarks, descriptors learned with our formulation achieve state-of-the-art results in patch verification, patch retrieval, and image matching",
    "checked": true,
    "id": "02c8d35607470fc1eb28f4c8e763b207c63c5965",
    "semantic_title": "local descriptors optimized for average precision",
    "citation_count": 196,
    "authors": [
      "Kun He",
      "Yan Lu",
      "Stan Sclaroff"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Recovering_Realistic_Texture_CVPR_2018_paper.html": {
    "title": "Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform",
    "volume": "main",
    "abstract": "Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN and EnhanceNet",
    "checked": true,
    "id": "9e788f1530af08a1f2140e6016fd4aeaa8b29033",
    "semantic_title": "recovering realistic texture in image super-resolution by deep spatial feature transform",
    "citation_count": 979,
    "authors": [
      "Xintao Wang",
      "Ke Yu",
      "Chao Dong",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Maninis_Deep_Extreme_Cut_CVPR_2018_paper.html": {
    "title": "Deep Extreme Cut: From Extreme Points to Object Segmentation",
    "volume": "main",
    "abstract": "This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points. We demonstrate the usefulness of this approach for guided segmentation (grabcut-style), interactive segmentation, video object segmentation, and dense segmentation annotation. We show that we obtain the most precise results to date, also with less user input, in an extensive and varied selection of benchmarks and datasets. All our models and code are publicly available on http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr",
    "checked": true,
    "id": "c707310325848c7bd605b4937acd749669dc3b02",
    "semantic_title": "deep extreme cut: from extreme points to object segmentation",
    "citation_count": 418,
    "authors": [
      "Kevis-Kokitsi Maninis",
      "Sergi Caelles",
      "Jordi Pont-Tuset",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Learning_to_Parse_CVPR_2018_paper.html": {
    "title": "Learning to Parse Wireframes in Images of Man-Made Environments",
    "volume": "main",
    "abstract": "In this paper, we propose a learning-based approach to the task of automatically extracting a \"wireframe\" representation for images of cluttered man-made environments. The wireframe contains all salient straight lines and their junctions of the scene that encode efficiently and accurately large-scale geometry and object shapes. To this end, we have built a very large new dataset of over 5,000 images with wireframes thoroughly labelled by humans. We have proposed two convolutional neural networks that are suitable for extracting junctions and lines with large spatial support, respectively. The networks trained on our dataset have achieved significantly better performance than state-of-the-art methods for junction detection and line segment detection, respectively. We have conducted extensive experiments to evaluate quantitatively and qualitatively the wireframes obtained by our method, and have convincingly shown that effectively and efficiently parsing wireframes for images of man-made environments is a feasible goal within reach. Such wireframes could benefit many important visual tasks such as feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation",
    "checked": true,
    "id": "d5238bfb79bd03e0af4ca4b6fb7ff4e6de1f3ef5",
    "semantic_title": "learning to parse wireframes in images of man-made environments",
    "citation_count": 173,
    "authors": [
      "Kun Huang",
      "Yifan Wang",
      "Zihan Zhou",
      "Tianjiao Ding",
      "Shenghua Gao",
      "Yi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Vasu_Occlusion-Aware_Rolling_Shutter_CVPR_2018_paper.html": {
    "title": "Occlusion-Aware Rolling Shutter Rectification of 3D Scenes",
    "volume": "main",
    "abstract": "A vast majority of contemporary cameras employ rolling shutter (RS) mechanism to capture images. Due to the sequential mechanism, images acquired with a moving camera are subjected to rolling shutter effect which manifests as geometric distortions. In this work, we consider the specific scenario of a fast moving camera wherein the rolling shutter distortions not only are predominant but also become depth-dependent which in turn results in intra-frame occlusions. To this end, we develop a first-of-its-kind pipeline to recover the latent image of a 3D scene from a set of such RS distorted images. The proposed approach sequentially recovers both the camera motion and scene structure while accounting for RS and occlusion effects. Subsequently, we perform depth and occlusion-aware rectification of RS images to yield the desired latent image. Our experiments on synthetic and real image sequences reveal that the proposed approach achieves state-of-the-art results",
    "checked": true,
    "id": "33212ee8a256dad8613a6132a2ebb232c77e8109",
    "semantic_title": "occlusion-aware rolling shutter rectification of 3d scenes",
    "citation_count": 39,
    "authors": [
      "Subeesh Vasu",
      "Mahesh Mohan M. R.",
      "A. N. Rajagopalan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yi_Content-Sensitive_Supervoxels_via_CVPR_2018_paper.html": {
    "title": "Content-Sensitive Supervoxels via Uniform Tessellations on Video Manifolds",
    "volume": "main",
    "abstract": "Supervoxels are perceptually meaningful atomic regions in videos, obtained by grouping voxels that exhibit coherence in both appearance and motion. In this paper, we propose content-sensitive supervoxels (CSS), which are regularly-shaped 3D primitive volumes that possess the following characteristic: they are typically larger and longer in content-sparse regions (i.e., with homogeneous appearance and motion), and smaller and shorter in content-dense regions (i.e., with high variation of appearance and/or motion). To compute CSS, we map a video X to a 3-dimensional manifold M embedded in R^6, whose volume elements give a good measure of the content density in X. We propose an efficient Lloyd-like method with a splitting-merging scheme to compute a uniform tessellation on M, which induces the CSS in X. Theoretically our method has a good competitive ratio O(1). We also present a simple extension of CSS to stream CSS for processing long videos that cannot be loaded into main memory at once. We evaluate CSS, stream CSS and seven representative supervoxel methods on four video datasets. The results show that our method outperforms existing supervoxel methods",
    "checked": true,
    "id": "ffb4af35f8a104332a1d660eef0ab8e2c7d7e111",
    "semantic_title": "content-sensitive supervoxels via uniform tessellations on video manifolds",
    "citation_count": 8,
    "authors": [
      "Ran Yi",
      "Yong-Jin Liu",
      "Yu-Kun Lai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Intrinsic_Image_Transformation_CVPR_2018_paper.html": {
    "title": "Intrinsic Image Transformation via Scale Space Decomposition",
    "volume": "main",
    "abstract": "We introduce a new network structure for decomposing an image into its intrinsic albedo and shading. We treat this as an image-to-image transformation problem and explore the scale space of the input and output. By expanding the output images (albedo and shading) into their Laplacian pyramid components, we develop a multi-channel network structure that learns the image-to-image transformation function in successive frequency bands in parallel, within each channel is a fully convolutional neural network with skip connections. This network structure is general and extensible, and has demonstrated excellent performance on the intrinsic image decomposition problem. We evaluate the network on two benchmark datasets: the MPI-Sintel dataset and the MIT Intrinsic Images dataset. Both quantitative and qualitative results show our model delivers a clear progression over state-of-the-art",
    "checked": true,
    "id": "ce6a6f35e0b45b84ce6bb5ae3e0c213e5016dc9a",
    "semantic_title": "intrinsic image transformation via scale space decomposition",
    "citation_count": 39,
    "authors": [
      "Lechao Cheng",
      "Chengyi Zhang",
      "Zicheng Liao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Khan_Learned_Shape-Tailored_Descriptors_CVPR_2018_paper.html": {
    "title": "Learned Shape-Tailored Descriptors for Segmentation",
    "volume": "main",
    "abstract": "We address the problem of texture segmentation by grouping dense pixel-wise descriptors. We introduce and construct learned Shape-Tailored Descriptors that aggregate image statistics only within regions of interest to avoid mixing statistics of different textures, and that are invariant to complex nuisances (e.g., illumination, perspective and deformations). This is accomplished by training a neural network to discriminate base shape-tailored descriptors of oriented gradients at various scales. These descriptors are defined through partial differential equations to obtain data at various scales in arbitrarily shaped regions. We formulate and optimize a joint optimization problem in the segmentation and descriptors to discriminate these base descriptors using the learned metric, equivalent to grouping learned descriptors. We test the method on datasets to illustrate the effect of both the shape-tailored and learned properties of the descriptors. Experiments show that the descriptors learned on a small dataset of segmented images generalize well to unseen textures in other datasets, showing the generic nature of these descriptors. We show stateof- the-art results on texture segmentation benchmarks",
    "checked": true,
    "id": "338d28640ea58329c4be8b6305ff1cc96c92bbe5",
    "semantic_title": "learned shape-tailored descriptors for segmentation",
    "citation_count": 8,
    "authors": [
      "Naeemullah Khan",
      "Ganesh Sundaramoorthi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_PAD-Net_Multi-Tasks_Guided_CVPR_2018_paper.html": {
    "title": "PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing",
    "volume": "main",
    "abstract": "Depth estimation and scene parsing are two particularly important tasks in visual scene understanding. In this paper we tackle the problem of simultaneous depth estimation and scene parsing in a joint CNN. The task can be typically treated as a deep multi-task learning problem [42]. Different from previous methods directly optimizing multiple tasks given the input training data, this paper proposes a novel multi-task guided prediction-and-distillation network (PAD-Net), which first predicts a set of intermediate auxiliary tasks ranging from low level to high level, and then the predictions from these intermediate auxiliary tasks are utilized as multi-modal input via our proposed multi-modal distillation modules for the final tasks. During the joint learning, the intermediate tasks not only act as supervision for learning more robust deep representations but also pro- vide rich multi-modal information for improving the final tasks. Extensive experiments are conducted on two challenging datasets (i.e. NYUD-v2 and Cityscapes) for both the depth estimation and scene parsing tasks, demonstrating the effectiveness of the proposed approach",
    "checked": true,
    "id": "6cfa4ab327d42103195cb8e5c6181028cec8ee62",
    "semantic_title": "pad-net: multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing",
    "citation_count": 477,
    "authors": [
      "Dan Xu",
      "Wanli Ouyang",
      "Xiaogang Wang",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Multi-Image_Semantic_Matching_CVPR_2018_paper.html": {
    "title": "Multi-Image Semantic Matching by Mining Consistent Features",
    "volume": "main",
    "abstract": "This work proposes a multi-image matching method to estimate semantic correspondences across multiple images. In contrast to the previous methods that optimize all pairwise correspondences, the proposed method identifies and matches only a sparse set of reliable features in the image collection. In this way, the proposed method is able to prune nonrepeatable features and also highly scalable to handle thousands of images. We additionally propose a low-rank constraint to ensure the geometric consistency of feature correspondences over the whole image collection. Besides the competitive performance on multi-graph matching and semantic flow benchmarks, we also demonstrate the applicability of the proposed method for reconstructing object-class models and discovering object-class landmarks from images without using any annotation",
    "checked": true,
    "id": "697a615b140a9e1fbd869e7f30f9aa9801e8fa2d",
    "semantic_title": "multi-image semantic matching by mining consistent features",
    "citation_count": 48,
    "authors": [
      "Qianqian Wang",
      "Xiaowei Zhou",
      "Kostas Daniilidis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Density-Aware_Single_Image_CVPR_2018_paper.html": {
    "title": "Density-Aware Single Image De-Raining Using a Multi-Stream Dense Network",
    "volume": "main",
    "abstract": "Single image rain streak removal is an extremely challenging problem due to the presence of non-uniform rain densities in images. We present a novel density-aware multi-stream densely connected convolutional neural network-based algorithm, called DID-MDN, for joint rain density estimation and de-raining. The proposed method enables the network itself to automatically determine the rain-density information and then efficiently remove the corresponding rain-streaks guided by the estimated rain-density label. To better characterize rain-streaks with different scales and shapes, a multi-stream densely connected de-raining network is proposed which efficiently leverages features from different scales. Furthermore, a new dataset containing images with rain-density labels is created and used to train the proposed density-aware network. Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. In addition, an ablation study is performed to demonstrate the improvements obtained by different modules in the proposed method",
    "checked": true,
    "id": "34aaa735409b666b8dc678c23acb600b1d87913b",
    "semantic_title": "density-aware single image de-raining using a multi-stream dense network",
    "citation_count": 854,
    "authors": [
      "He Zhang",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Joint_Cuts_and_CVPR_2018_paper.html": {
    "title": "Joint Cuts and Matching of Partitions in One Graph",
    "volume": "main",
    "abstract": "As two fundamental problems, graph cuts and graph matching have been intensively investigated over the decades, resulting in vast literature in these two topics respectively. However the way of jointly applying and solving graph cuts and matching receives few attention. In this paper, we first formalize the problem of simultaneously cutting a graph into two partitions i.e. graph cuts and establishing their correspondence i.e. graph matching. Then we develop an optimization algorithm by updating matching and cutting alternatively, provided with theoretical analysis. The efficacy of our algorithm is verified on both synthetic dataset and real-world images containing similar regions or structures",
    "checked": true,
    "id": "8d1ab9c20048ea3f578e28adec2d10b6f24c475b",
    "semantic_title": "joint cuts and matching of partitions in one graph",
    "citation_count": 13,
    "authors": [
      "Tianshu Yu",
      "Junchi Yan",
      "Jieyi Zhao",
      "Baoxin Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Progressive_Attention_Guided_CVPR_2018_paper.html": {
    "title": "Progressive Attention Guided Recurrent Network for Salient Object Detection",
    "volume": "main",
    "abstract": "Effective convolutional features play an important role in saliency estimation but how to learn powerful features for saliency is still a challenging task. FCN-based methods directly apply multi-level convolutional features without distinction, which leads to sub-optimal results due to the distraction from redundant details. In this paper, we propose a novel attention guided network which selectively integrates multi-level contextual information in a progressive manner. Attentive features generated by our network can alleviate distraction of background thus achieve better performance. On the other hand, it is observed that most of existing algorithms conduct salient object detection by exploiting side-output features of the backbone feature extraction network. However, shallower layers of backbone network lack the ability to obtain global semantic information, which limits the effective feature learning. To address the problem, we introduce multi-path recurrent feedback to enhance our proposed progressive attention driven framework. Through multi-path recurrent connections, global semantic information from the top convolutional layer is transferred to shallower layers, which intrinsically refines the entire network. Experimental results on six benchmark datasets demonstrate that our algorithm performs favorably against the state-of-the-art approaches",
    "checked": true,
    "id": "297e7b1cd22ae14785a8e0b40339e41889294d25",
    "semantic_title": "progressive attention guided recurrent network for salient object detection",
    "citation_count": 545,
    "authors": [
      "Xiaoning Zhang",
      "Tiantian Wang",
      "Jinqing Qi",
      "Huchuan Lu",
      "Gang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hui_Fast_and_Accurate_CVPR_2018_paper.html": {
    "title": "Fast and Accurate Single Image Super-Resolution via Information Distillation Network",
    "volume": "main",
    "abstract": "Recently, deep convolutional neural networks (CNNs) have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few number of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance",
    "checked": true,
    "id": "8f684080d2b81d3178d681d6917cb077c082a9e1",
    "semantic_title": "fast and accurate single image super-resolution via information distillation network",
    "citation_count": 724,
    "authors": [
      "Zheng Hui",
      "Xiumei Wang",
      "Xinbo Gao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_Hallucinated-IQA_No-Reference_Image_CVPR_2018_paper.html": {
    "title": "Hallucinated-IQA: No-Reference Image Quality Assessment via Adversarial Learning",
    "volume": "main",
    "abstract": "No-reference image quality assessment (NR-IQA) is a fundamental yet challenging task in low-level computer vision community. The difficulty is particularly pronounced for the limited information, for which the corresponding reference for comparison is typically absent. Although various feature extraction mechanisms have been leveraged from natural scene statistics to deep neural networks in previous methods, the performance bottleneck still exists. In this work, we propose a hallucination-guided quality regression network to address the issue. We firstly generate a hallucinated reference constrained on the distorted image, to compensate the absence of the true reference. Then, we pair the information of hallucinated reference with the distorted image, and forward them to the regressor to learn the perceptual discrepancy with the guidance of an implicit ranking relationship within the generator, and therefore produce the precise quality prediction. To demonstrate the effectiveness of our approach, comprehensive experiments are evaluated on four popular image quality assessment benchmarks. Our method significantly outperforms all the previous state-of-the-art methods by large margins. The code and model are publicly available on the project page https://kwanyeelin.github.io/projects/HIQA/HIQA.html",
    "checked": true,
    "id": "2176c9cb436303041d12df18c75395f95a55da18",
    "semantic_title": "hallucinated-iqa: no-reference image quality assessment via adversarial learning",
    "citation_count": 226,
    "authors": [
      "Kwan-Yee Lin",
      "Guanxiang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mopuri_NAG_Network_for_CVPR_2018_paper.html": {
    "title": "NAG: Network for Adversary Generation",
    "volume": "main",
    "abstract": "Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations. However, for a given classifier, they generate one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. Also, in order to build robust models, it is essential to explore the manifold of adversarial perturbations. In this paper, we propose for the first time, a generative approach to model the distribution of adversarial perturbations. The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. Our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. Our experimental evaluation demonstrates that perturbations crafted by our model (i) achieve state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver excellent cross model generalizability. Our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations",
    "checked": true,
    "id": "6aab44a82cf37f690b71b709be0c8c70a098c99b",
    "semantic_title": "nag: network for adversary generation",
    "citation_count": 146,
    "authors": [
      "Konda Reddy Mopuri",
      "Utkarsh Ojha",
      "Utsav Garg",
      "R. Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liang_Dynamic-Structured_Semantic_Propagation_CVPR_2018_paper.html": {
    "title": "Dynamic-Structured Semantic Propagation Network",
    "volume": "main",
    "abstract": "Semantic concept hierarchy is yet under-explored for semantic segmentation due to the inefficiency and complicated optimization of incorporating structural inference into the dense prediction. This lack of modeling dependencies among concepts severely limits the generalization capability of segmentation models for open set large-scale vocabularies. Prior works thus must tune highly-specified models for each task due to the label discrepancy across datasets. In this paper, we propose a Dynamic-Structured Semantic Propagation Network (DSSPN) that builds a semantic neuron graph to explicitly incorporate the concept hierarchy into dynamic network construction, leading to an interpretable reasoning process. Each neuron for one super-class (eg food) represents the instantiated module for recognizing among fine-grained child concepts (eg editable fruit or pizza), and then its learned features flow into the child neurons (eg distinguishing between orange or apple) for hierarchical categorization in finer levels. A dense semantic-enhanced neural block propagates the learned knowledge of all ancestral neurons into each fine-grained child neuron for progressive feature evolving. During training, DSSPN performs the dynamic-structured neuron computational graph by only activating a sub-graph of neurons for each image. Another merit of such semantic explainable structure is the ability to learn a unified model concurrently on diverse datasets by selectively activating different neuron sub-graphs for each annotation at each step. Extensive experiments on four public semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape and Mapillary) demonstrate the superiority of DSSPN, and a universal segmentation model that is jointly trained on diverse datasets can surpass the common fine-tuning scheme for exploiting multi-domain knowledge",
    "checked": true,
    "id": "ab9a5b52c028678a1c921542c6dbfdc4c3224ca9",
    "semantic_title": "dynamic-structured semantic propagation network",
    "citation_count": 156,
    "authors": [
      "Xiaodan Liang",
      "Hongfei Zhou",
      "Eric Xing"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_Cross-Domain_Self-Supervised_Multi-Task_CVPR_2018_paper.html": {
    "title": "Cross-Domain Self-Supervised Multi-Task Feature Learning Using Synthetic Imagery",
    "volume": "main",
    "abstract": "In human learning, it is common to use multiple sources of information jointly. However, most existing feature learning approaches learn from only a single task. In this paper, we propose a novel multi-task deep network to learn generalizable high-level visual representations. Since multi-task learning requires annotations for multiple properties of the same training instance, we look to synthetic images to train our network. To overcome the domain difference between real and synthetic data, we employ an unsupervised feature space domain adaptation method based on adversarial learning. Given an input synthetic RGB image, our network simultaneously predicts its surface normal, depth, and instance contour, while also minimizing the feature space domain differences between real and synthetic data. Through extensive experiments, we demonstrate that our network learns more transferable representations compared to single-task baselines. Our learned representation produces state-of-the-art transfer learning results on PASCAL VOC 2007 classification and 2012 detection",
    "checked": true,
    "id": "91e89cedd4093bfe176532530ddb960f2767aca5",
    "semantic_title": "cross-domain self-supervised multi-task feature learning using synthetic imagery",
    "citation_count": 212,
    "authors": [
      "Zhongzheng Ren",
      "Yong Jae Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hadad_A_Two-Step_Disentanglement_CVPR_2018_paper.html": {
    "title": "A Two-Step Disentanglement Method",
    "volume": "main",
    "abstract": "We address the problem of disentanglement of factors that generate a given data into those that are correlated with the labeling and those that are not. Our solution is simpler than previous solutions and employs adversarial training. First, the part of the data that is correlated with the labels is extracted by training a classifier. Then, the other part is extracted such that it enables the reconstruction of the original data but does not contain label information. The utility of the new method is demonstrated on visual datasets as well as on financial data. Our code is available at https://github.com/naamahadad/A-Two-Step-Disentanglement-Method",
    "checked": true,
    "id": "1a200d0758ef74b72f696810e3f03893090a1a9d",
    "semantic_title": "a two-step disentanglement method",
    "citation_count": 80,
    "authors": [
      "Naama Hadad",
      "Lior Wolf",
      "Moni Shahar"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Merget_Robust_Facial_Landmark_CVPR_2018_paper.html": {
    "title": "Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network",
    "volume": "main",
    "abstract": "While fully-convolutional neural networks are very strong at modeling local features, they fail to aggregate global context due to their constrained receptive field. Modern methods typically address the lack of global context by introducing cascades, pooling, or by fitting a statistical model. In this work, we propose a new approach that introduces global context into a fully-convolutional neural network directly. The key concept is an implicit kernel convolution within the network. The kernel convolution blurs the output of a local-context subnet, which is then refined by a global-context subnet using dilated convolutions. The kernel convolution is crucial for the convergence of the network because it smoothens the gradients and reduces overfitting. In a postprocessing step, a simple PCA-based 2D shape model is fitted to the network output in order to filter outliers. Our experiments demonstrate the effectiveness of our approach, outperforming several state-of-the-art methods in facial landmark detection",
    "checked": true,
    "id": "489c5a50b58af004b0050bfedf10877199ed7ad5",
    "semantic_title": "robust facial landmark detection via a fully-convolutional local-global context network",
    "citation_count": 79,
    "authors": [
      "Daniel Merget",
      "Matthias Rock",
      "Gerhard Rigoll"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Decorrelated_Batch_Normalization_CVPR_2018_paper.html": {
    "title": "Decorrelated Batch Normalization",
    "volume": "main",
    "abstract": "Batch Normalization (BN) is capable of accelerating the training of deep models by centering and scaling activations within mini-batches. In this work, we propose Decorrelated Batch Normalization (DBN), which not just centers and scales activations but whitens them. We explore multiple whitening techniques, and find that PCA whitening causes a problem we call stochastic axis swapping, which is detrimental to learning. We show that ZCA whitening does not suffer from this problem, permitting successful learning. DBN retains the desirable qualities of BN and further improves BN's optimization efficiency and generalization ability. We design comprehensive experiments to show that DBN can improve the performance of BN on multilayer perceptrons and convolutional neural networks. Furthermore, we consistently improve the accuracy of residual networks on CIFAR-10, CIFAR-100, and ImageNet",
    "checked": true,
    "id": "e8270f523375d22fd417bb583a5b50411e5d5f58",
    "semantic_title": "decorrelated batch normalization",
    "citation_count": 192,
    "authors": [
      "Lei Huang",
      "Dawei Yang",
      "Bo Lang",
      "Jia Deng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Learning_to_Sketch_CVPR_2018_paper.html": {
    "title": "Learning to Sketch With Shortcut Cycle Consistency",
    "volume": "main",
    "abstract": "To see is to sketch -- free-hand sketching naturally builds ties between human and machine vision. In this paper, we present a novel approach for translating an object photo to a sketch, mimicking the human sketching process. This is an extremely challenging task because the photo and sketch domains differ significantly. Furthermore, human sketches exhibit various levels of sophistication and abstraction even when depicting the same object instance in a reference photo. This means that even if photo-sketch pairs are available, they only provide weak supervision signal to learn a translation model. Compared with existing supervised approaches that solve the problem of D(E(photo)) -> sketch, where E(·) and D(·) denote encoder and decoder respectively, we take advantage of the inverse problem (e.g., D(E(sketch)) -> photo), and combine with the unsupervised learning tasks of within-domain reconstruction, all within a multi-task learning framework. Compared with existing unsupervised approaches based on cycle consistency (i.e., D(E(D(E(photo)))) -> photo), we introduce a shortcut consistency enforced at the encoder bottleneck (e.g., D(E(photo)) -> photo) to exploit the additional self-supervision. Both qualitative and quantitative results show that the proposed model is superior to a number of state-of-the-art alternatives. We also show that the synthetic sketches can be used to train a better fine-grained sketch-based image retrieval (FG-SBIR) model, effectively alleviating the problem of sketch data scarcity",
    "checked": true,
    "id": "3c5c2e2ccf4c9bff1d4a335a0dcb91d1e6ba2410",
    "semantic_title": "learning to sketch with shortcut cycle consistency",
    "citation_count": 120,
    "authors": [
      "Jifei Song",
      "Kaiyue Pang",
      "Yi-Zhe Song",
      "Tao Xiang",
      "Timothy M. Hospedales"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Towards_a_Mathematical_CVPR_2018_paper.html": {
    "title": "Towards a Mathematical Understanding of the Difficulty in Learning With Feedforward Neural Networks",
    "volume": "main",
    "abstract": "Training deep neural networks for solving machine learning problems is one great challenge in the field, mainly due to its associated optimisation problem being highly non-convex. Recent developments have suggested that many training algorithms do not suffer from undesired local minima under certain scenario, and consequently led to great efforts in pursuing mathematical explanations for such observations. This work provides an alternative mathematical understanding of the challenge from a smooth optimisation perspective. By assuming exact learning of finite samples, sufficient conditions are identified via a critical point analysis to ensure any local minimum to be globally minimal as well. Furthermore, a state of the art algorithm, known as the Generalised Gauss-Newton (GGN) algorithm, is rigorously revisited as an approximate Newton's algorithm, which shares the property of being locally quadratically convergent to a global minimum under the condition of exact learning",
    "checked": true,
    "id": "c8025f49b33093d8ff81c7979c48260452bc12f8",
    "semantic_title": "towards a mathematical understanding of the difficulty in learning with feedforward neural networks",
    "citation_count": 35,
    "authors": [
      "Hao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_FaceID-GAN_Learning_a_CVPR_2018_paper.html": {
    "title": "FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis",
    "volume": "main",
    "abstract": "Face synthesis has achieved advanced development by using generative adversarial networks (GANs). Existing methods typically formulate GAN as a two-player game, where a discriminator distinguishes face images from the real and synthesized domains, while a generator reduces its discriminativeness by synthesizing a face of photo-realistic quality. Their competition converges when the discriminator is unable to differentiate these two domains. Unlike two-player GANs, this work generates identity-preserving faces by proposing FaceID-GAN, which treats a classifier of face identity as the third player, competing with the generator by distinguishing the identities of the real and synthesized faces (see Fig.1). A stationary point is reached when the generator produces faces that have high quality as well as preserve identity. Instead of simply modeling the identity classifier as an additional discriminator, FaceID-GAN is formulated by satisfying information symmetry, which ensures that the real and synthesized images are projected into the same feature space. In other words, the identity classifier is used to extract identity features from both input (real) and output (synthesized) face images of the generator, substantially alleviating training difficulty of GAN. Extensive experiments show that FaceID-GAN is able to generate faces of arbitrary viewpoint while preserve identity, outperforming recent advanced approaches",
    "checked": true,
    "id": "a35483c9becc95faa16bf70a8c6355566a205091",
    "semantic_title": "faceid-gan: learning a symmetry three-player gan for identity-preserving face synthesis",
    "citation_count": 171,
    "authors": [
      "Yujun Shen",
      "Ping Luo",
      "Junjie Yan",
      "Xiaogang Wang",
      "Xiaoou Tang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_A_Constrained_Deep_CVPR_2018_paper.html": {
    "title": "A Constrained Deep Neural Network for Ordinal Regression",
    "volume": "main",
    "abstract": "Ordinal regression is a supervised learning problem aiming to classify instances into ordinal categories. It is challenging to automatically extract high-level features for representing intraclass information and interclass ordinal relationship simultaneously. This paper proposes a constrained optimization formulation for the ordinal regression problem which minimizes the negative loglikelihood for multiple categories constrained by the order relationship between instances. Mathematically, it is equivalent to an unconstrained formulation with a pairwise regularizer. An implementation based on the CNN framework is proposed to solve the problem such that high-level features can be extracted automatically, and the optimal solution can be learned through the traditional back-propagation method. The proposed pairwise constraints make the algorithm work even on small datasets, and a proposed efficient implementation make it be scalable for large datasets. Experimental results on four real-world benchmarks demonstrate that the proposed algorithm outperforms the traditional deep learning approaches and other state-of-the-art approaches based on hand-crafted features",
    "checked": true,
    "id": "8a917903b0a1d47f24bc7776ab0bd00aa8ec88f3",
    "semantic_title": "a constrained deep neural network for ordinal regression",
    "citation_count": 75,
    "authors": [
      "Yanzhu Liu",
      "Adams Wai Kin Kong",
      "Chi Keong Goh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Modulated_Convolutional_Networks_CVPR_2018_paper.html": {
    "title": "Modulated Convolutional Networks",
    "volume": "main",
    "abstract": "Despite great effectiveness of very deep and wide Convolutional Neural Networks (CNNs) in various computer vision tasks, the significant cost in terms of storage requirement of such networks impedes the deployment on computationally limited devices. In this paper, we propose new Modulated Convolutional Networks (MCNs) to improve the portability of CNNs via binarized filters. In MCNs, we propose a new loss function which considers the filter loss, center loss and softmax loss in an end-to-end framework. We first introduce modulation filters (M-Filters) to recover the unbinarized filters, which leads to a new architecture to calculate the network model. The convolution operation is further approximated by considering intra-class compactness in the loss function. As a result, our MCNs can reduce the size of required storage space of convolutional filters by a factor of 32, in contrast to the full-precision model, while achieving much better performances than state-of-the-art binarized models. Most importantly, MCNs achieve a comparable performance to the full-precision ResNets and Wide-ResNets. The code will be available publicly soon",
    "checked": true,
    "id": "2e38c5b7d424a023acd072e83fe3c2f75dee6e84",
    "semantic_title": "modulated convolutional networks",
    "citation_count": 36,
    "authors": [
      "Xiaodi Wang",
      "Baochang Zhang",
      "Ce Li",
      "Rongrong Ji",
      "Jungong Han",
      "Xianbin Cao",
      "Jianzhuang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Weiler_Learning_Steerable_Filters_CVPR_2018_paper.html": {
    "title": "Learning Steerable Filters for Rotation Equivariant CNNs",
    "volume": "main",
    "abstract": "In many machine learning tasks it is desirable that a model's prediction transforms in an equivariant way under transformations of its input. Convolutional neural networks (CNNs) implement translational equivariance by construction; for other transformations, however, they are compelled to learn the proper mapping. In this work, we develop Steerable Filter CNNs (SFCNNs) which achieve joint equivariance under translations and rotations by design. The proposed architecture employs steerable filters to efficiently compute orientation dependent responses for many orientations without suffering interpolation artifacts from filter rotation. We utilize group convolutions which guarantee an equivariant mapping. In addition, we generalize He's weight initialization scheme to filters which are defined as a linear combination of a system of atomic filters. Numerical experiments show a substantial enhancement of the sample complexity with a growing number of sampled filter orientations and confirm that the network generalizes learned patterns over orientations. The proposed approach achieves state-of-the-art on the rotated MNIST benchmark and on the ISBI 2012 2D EM segmentation challenge",
    "checked": true,
    "id": "5509a33f7bd18797a5c48caa488940a41d12d61e",
    "semantic_title": "learning steerable filters for rotation equivariant cnns",
    "citation_count": 387,
    "authors": [
      "Maurice Weiler",
      "Fred A. Hamprecht",
      "Martin Storath"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Acuna_Efficient_Interactive_Annotation_CVPR_2018_paper.html": {
    "title": "Efficient Interactive Annotation of Segmentation Datasets With Polygon-RNN++",
    "volume": "main",
    "abstract": "Manually labeling datasets with object masks is extremely time consuming. In this work, we follow the idea of Polygon-RNN to produce polygonal annotations of objects interactively using humans-in-the-loop. We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high-resolution objects in images. Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10% absolute and 16% relative improvement in mean IoU) and interactive modes (requiring 50% fewer clicks by annotators). We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains. The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods. Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice",
    "checked": true,
    "id": "a985aba7699d0904f51785608496526eede02d6e",
    "semantic_title": "efficient interactive annotation of segmentation datasets with polygon-rnn++",
    "citation_count": 403,
    "authors": [
      "David Acuna",
      "Huan Ling",
      "Amlan Kar",
      "Sanja Fidler"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fey_SplineCNN_Fast_Geometric_CVPR_2018_paper.html": {
    "title": "SplineCNN: Fast Geometric Deep Learning With Continuous B-Spline Kernels",
    "volume": "main",
    "abstract": "We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence. Our source code is available on GitHub",
    "checked": true,
    "id": "a73531abe4cafbccd5b3e949e84410a50016bd33",
    "semantic_title": "splinecnn: fast geometric deep learning with continuous b-spline kernels",
    "citation_count": 441,
    "authors": [
      "Matthias Fey",
      "Jan Eric Lenssen",
      "Frank Weichert",
      "Heinrich Müller"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kossaifi_GAGAN_Geometry-Aware_Generative_CVPR_2018_paper.html": {
    "title": "GAGAN: Geometry-Aware Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Deep generative models learned through adversarial training have become increasingly popular for their ability to generate naturalistic image textures. However, aside from their texture, the visual appearance of objects is significantly influenced by their shape geometry; information which is not taken into account by existing generative models. This paper introduces the Geometry-Aware Generative Adversarial Networks (GAGAN) for incorporating geometric information into the image generation process. Specifically, in GAGAN the generator samples latent variables from the probability space of a statistical shape model. By mapping the output of the generator to a canonical coordinate frame through a differentiable geometric transformation, we enforce the geometry of the objects and add an implicit connection from the prior to the generated object. Experimental results on face generation indicate that the GAGAN can generate realistic images of faces with arbitrary facial attributes such as facial expression, pose, and morphology, that are of better quality than current GAN-based methods. Our method can be used to augment any existing GAN architecture and improve the quality of the images generated",
    "checked": true,
    "id": "15e179a46591bb2aac160afc083999458c6a2296",
    "semantic_title": "gagan: geometry-aware generative adversarial networks",
    "citation_count": 47,
    "authors": [
      "Jean Kossaifi",
      "Linh Tran",
      "Yannis Panagakis",
      "Maja Pantic"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Arnab_On_the_Robustness_CVPR_2018_paper.html": {
    "title": "On the Robustness of Semantic Segmentation Models to Adversarial Attacks",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) have been demonstrated to perform exceptionally well on most recognition tasks such as image classification and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and complex tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing. In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets. We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task. Furthermore, we show how mean-field inference in deep structured models and multiscale processing naturally implement recently proposed adversarial defenses. Our observations will aid future efforts in understanding and defending against adversarial examples. Moreover, in the shorter term, we show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness",
    "checked": true,
    "id": "8067d5d3fc80abb010b239bbe012b50bae8e6611",
    "semantic_title": "on the robustness of semantic segmentation models to adversarial attacks",
    "citation_count": 266,
    "authors": [
      "Anurag Arnab",
      "Ondrej Miksik",
      "Philip H.S. Torr"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Feedback-Prop_Convolutional_Neural_CVPR_2018_paper.html": {
    "title": "Feedback-Prop: Convolutional Neural Network Inference Under Partial Evidence",
    "volume": "main",
    "abstract": "We propose an inference procedure for deep convolutional neural networks (CNNs) when partial evidence is available. Our method consists of a general feedback-based propagation approach (feedback-prop) that boosts the prediction accuracy for an arbitrary set of unknown target labels when the values for a non-overlapping arbitrary set of target labels are known. We show that existing models trained in a multi-label or multi-task setting can readily take advantage of feedback-prop without any retraining or fine-tuning. Our feedback-prop inference procedure is general, simple, reliable, and works on different challenging visual recognition tasks. We present two variants of feedback-prop based on layer-wise and residual iterative updates. We experiment using several multi-task models and show that feedback-prop is effective in all of them. Our results unveil a previously unreported but interesting dynamic property of deep CNNs. We also present an associated technical approach that takes advantage of this property for inference under partial evidence in general visual recognition tasks",
    "checked": true,
    "id": "e8613d16ae3ec7263a56a58b9ffdd7dc7fe4e18d",
    "semantic_title": "feedback-prop: convolutional neural network inference under partial evidence",
    "citation_count": 12,
    "authors": [
      "Tianlu Wang",
      "Kota Yamaguchi",
      "Vicente Ordonez"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Super-Resolving_Very_Low-Resolution_CVPR_2018_paper.html": {
    "title": "Super-Resolving Very Low-Resolution Face Images With Supplementary Attributes",
    "volume": "main",
    "abstract": "Given a tiny face image, conventional face hallucination methods aim to super-resolve its high-resolution (HR) counterpart by learning a mapping from an exemplar dataset. Since a low-resolution (LR) input patch may correspond to many HR candidate patches, this ambiguity may lead to erroneous HR facial details and thus distorts final results, such as gender reversal. An LR input contains low-frequency facial components of its HR version while its residual face image defined as the difference between the HR ground-truth and interpolated LR images contains the missing high-frequency facial details. We demonstrate that supplementing residual images or feature maps with facial attribute information can significantly reduce the ambiguity in face super-resolution. To explore this idea, we develop an attribute-embedded upsampling network, which consists of an upsampling network and a discriminative network. The upsampling network is composed of an autoencoder with skip-connections, which incorporates facial attribute vectors into the residual features of LR inputs at the bottleneck of the autoencoder and deconvolutional layers used for upsampling. The discriminative network is designed to examine whether super-resolved faces contain the desired attributes or not and then its loss is used for updating the upsampling network. In this manner, we can super-resolve tiny unaligned (16$ imes$16 pixels) face images with a large upscaling factor of 8$ imes$ while reducing the uncertainty of one-to-many mappings significantly. By conducting extensive evaluations on a large-scale dataset, we demonstrate that our method achieves superior face hallucination results and outperforms the state-of-the-art",
    "checked": true,
    "id": "54ab1b28175f527cb9d442b6cf9c5ce49ba339d5",
    "semantic_title": "super-resolving very low-resolution face images with supplementary attributes",
    "citation_count": 165,
    "authors": [
      "Xin Yu",
      "Basura Fernando",
      "Richard Hartley",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Frustum_PointNets_for_CVPR_2018_paper.html": {
    "title": "Frustum PointNets for 3D Object Detection From RGB-D Data",
    "volume": "main",
    "abstract": "In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability",
    "checked": true,
    "id": "526cf249c2760b7bdbb28f2a2a7c85851d3c2727",
    "semantic_title": "frustum pointnets for 3d object detection from rgb-d data",
    "citation_count": 0,
    "authors": [
      "Charles R. Qi",
      "Wei Liu",
      "Chenxia Wu",
      "Hao Su",
      "Leonidas J. Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_W2F_A_Weakly-Supervised_CVPR_2018_paper.html": {
    "title": "W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection",
    "volume": "main",
    "abstract": "Weakly-supervised object detection has attracted much attention lately, since it does not require bounding box annotations for training. Although significant progress has also been made, there is still a large gap in performance between weakly-supervised and fully-supervised object detection. Recently, some works use pseudo ground-truths which are generated by a weakly-supervised detector to train a supervised detector. Such approaches incline to find the most representative parts of objects, and only seek one ground-truth box per class even though many same-class instances exist. To overcome these issues, we propose a weakly-supervised to fully-supervised framework, where a weakly-supervised detector is implemented using multiple instance learning. Then, we propose a pseudo ground-truth excavation (PGE) algorithm to find the pseudo ground-truth of each instance in the image. Moreover, the pseudo ground-truth adaptation (PGA) algorithm is designed to further refine the pseudo ground-truths from PGE. Finally, we use these pseudo ground-truths to train a fully-supervised detector. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 benchmarks strongly demonstrate the effectiveness of our framework. We obtain 52.4% and 47.8% mAP on VOC2007 and VOC2012 respectively, a significant improvement over previous state-of-the-art methods",
    "checked": true,
    "id": "041755d1c14077ce18d8553aa40a415283edc825",
    "semantic_title": "w2f: a weakly-supervised to fully-supervised framework for object detection",
    "citation_count": 121,
    "authors": [
      "Yongqiang Zhang",
      "Yancheng Bai",
      "Mingli Ding",
      "Yongqiang Li",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_3D_Object_Detection_CVPR_2018_paper.html": {
    "title": "3D Object Detection With Latent Support Surfaces",
    "volume": "main",
    "abstract": "We develop a 3D object detection algorithm that uses latent support surfaces to capture contextual relationships in indoor scenes. Existing 3D representations for RGB-D images capture the local shape and appearance of object categories, but have limited power to represent objects with different visual styles. The detection of small objects is also challenging because the search space is very large in 3D scenes. However, we observe that much of the shape variation within 3D object categories can be explained by the location of a latent support surface, and smaller objects are often supported by larger objects. Therefore, we explicitly use latent support surfaces to better represent the 3D appearance of large objects, and provide contextual cues to improve the detection of small objects. We evaluate our model with 19 object categories from the SUN RGB-D database, and demonstrate state-of-the-art performance",
    "checked": true,
    "id": "7164b6220fb552c7229614126fdf10b594226266",
    "semantic_title": "3d object detection with latent support surfaces",
    "citation_count": 24,
    "authors": [
      "Zhile Ren",
      "Erik B. Sudderth"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Towards_Faster_Training_CVPR_2018_paper.html": {
    "title": "Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization",
    "volume": "main",
    "abstract": "Global covariance pooling in convolutional neural networks has achieved impressive improvement over the classical first-order pooling. Recent works have shown matrix square root normalization plays a central role in achieving state-of-the-art performance. However, existing methods depend heavily on eigendecomposition (EIG) or singular value decomposition (SVD), suffering from inefficient training due to limited support of EIG and SVD on GPU. Towards addressing this problem, we propose an iterative matrix square root normalization method for fast end-to-end training of global covariance pooling networks. At the core of our method is a meta-layer designed with loop-embedded directed graph structure. The meta-layer consists of three consecutive nonlinear structured layers, which perform pre-normalization, coupled matrix iteration and post-compensation, respectively. Our method is much faster than EIG or SVD based ones, since it involves only matrix multiplications, suitable for parallel implementation on GPU. Moreover, the proposed network with ResNet architecture can converge in much less epochs, further accelerating network training. On large-scale ImageNet, we achieve competitive performance superior to existing counterparts. By finetuning our models pre-trained on ImageNet, we establish state-of-the-art results on three challenging fine-grained benchmarks. The source code and network models will be available at http://www.peihuali.org/iSQRT-COV",
    "checked": true,
    "id": "2451db113552afb6d9ad15ef4009ec4133d28f74",
    "semantic_title": "towards faster training of global covariance pooling networks by iterative matrix square root normalization",
    "citation_count": 256,
    "authors": [
      "Peihua Li",
      "Jiangtao Xie",
      "Qilong Wang",
      "Zilin Gao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kong_Recurrent_Scene_Parsing_CVPR_2018_paper.html": {
    "title": "Recurrent Scene Parsing With Perspective Understanding in the Loop",
    "volume": "main",
    "abstract": "Objects may appear at arbitrary scales in perspective images of a scene, posing a challenge for recognition systems that process images at a fixed resolution. We propose a depth-aware gating module that adaptively selects the pooling field size in a convolutional network architecture according to the object scale (inversely proportional to the depth) so that small details are preserved for distant objects while larger receptive fields are used for those nearby. The depth gating signal is provided by stereo disparity or estimated directly from monocular input. We integrate this depth-aware gating into a recurrent convolutional neural network to perform semantic segmentation. Our recurrent module iteratively efines the segmentation results, leveraging the depth and semantic predictions from the previous iterations. Through extensive experiments on four popular large-scale datasets, we demonstrate this approach achieves competitive semantic segmentation performance with a model which is substantially more compact. We carry out extensive analysis of this architecture including variants that operate on monocular RGB but use depth as side-information during training, unsupervised gating as a generic attentional mechanism, and multi-resolution gating. We find that gated pooling for joint semantic segmentation and depth yields state-of-the-art results for quantitative monocular depth estimation",
    "checked": true,
    "id": "ad0bc2700e87592634fe995ec7c5ad0efb6937b7",
    "semantic_title": "recurrent scene parsing with perspective understanding in the loop",
    "citation_count": 105,
    "authors": [
      "Shu Kong",
      "Charless C. Fowlkes"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Noh_Improving_Occlusion_and_CVPR_2018_paper.html": {
    "title": "Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors",
    "volume": "main",
    "abstract": "We propose methods of addressing two critical issues of pedestrian detection: (i) occlusion of target objects as false negative failure, and (ii) confusion with hard negative examples like vertical structures as false positive failure. Our solutions to these two problems are general and flexible enough to be applicable to any single-stage detection models. We implement our methods into four state-of-the-art single-stage models, including SqueezeDet+, YOLOv2, SSD, and DSSD. We empirically validate that our approach indeed improves the performance of those four models on Caltech pedestrian and CityPersons dataset. Moreover, in some heavy occlusion settings, our approach achieves the best reported performance. Specifically, our two solutions are as follows. For better occlusion handling, we update the output tensors of single-stage models so that they include the prediction of part confidence scores, from which we compute a final occlusion-aware detection score. For reducing confusion with hard negative examples, we introduce average grid classifiers as post-refinement classifiers, trainable in an end-to-end fashion with little memory and time overhead (e.g. increase of 1--5 MB in memory and 1--2 ms in inference time)",
    "checked": true,
    "id": "673e1ed100fd6dfc37225ed5f6cf39c02f9e5193",
    "semantic_title": "improving occlusion and hard negative handling for single-stage pedestrian detectors",
    "citation_count": 81,
    "authors": [
      "Junhyug Noh",
      "Soochan Lee",
      "Beomsu Kim",
      "Gunhee Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chuang_Learning_to_Act_CVPR_2018_paper.html": {
    "title": "Learning to Act Properly: Predicting and Explaining Affordances From Images",
    "volume": "main",
    "abstract": "We address the problem of affordance reasoning in diverse scenes that appear in the real world. Affordances relate the agent's actions to their effects when taken on the surrounding objects. In our work, we take the egocentric view of the scene, and aim to reason about action-object affordances that respect both the physical world as well as the social norms imposed by the society. We also aim to teach artificial agents why some actions should not be taken in certain situations, and what would likely happen if these actions would be taken. We collect a new dataset that builds upon ADE20k, referred to as ADE-Affordance, which containing annotations enabling such rich visual reasoning. We propose a model that exploits Graph Neural Networks to propagate contextual information from the scene in order to perform detailed affordance reasoning about each object. Our model is showcased through various ablation studies, pointing to successes and challenges in this complex task",
    "checked": true,
    "id": "cfa58662750d752ed96184c9fde90150ec89d234",
    "semantic_title": "learning to act properly: predicting and explaining affordances from images",
    "citation_count": 102,
    "authors": [
      "Ching-Yao Chuang",
      "Jiaman Li",
      "Antonio Torralba",
      "Sanja Fidler"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hua_Pointwise_Convolutional_Neural_CVPR_2018_paper.html": {
    "title": "Pointwise Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Deep learning with 3D data such as reconstructed point clouds and CAD models has received great research interests recently. However, the capability of using point clouds with convolutional neural network has been so far not fully explored. In this paper, we present a convolutional neural network for semantic segmentation and object recognition with 3D point clouds. At the core of our network is pointwise convolution, a new convolution operator that can be applied at each point of a point cloud. Our fully convolutional network design, while being surprisingly simple to implement, can yield competitive accuracy in both semantic segmentation and object recognition task",
    "checked": true,
    "id": "23ddf3661847ed93feeb8fd2005a4b9d7b951afe",
    "semantic_title": "pointwise convolutional neural networks",
    "citation_count": 507,
    "authors": [
      "Binh-Son Hua",
      "Minh-Khoi Tran",
      "Sai-Kit Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_Image-Image_Domain_Adaptation_CVPR_2018_paper.html": {
    "title": "Image-Image Domain Adaptation With Preserved Self-Similarity and Domain-Dissimilarity for Person Re-Identification",
    "volume": "main",
    "abstract": "Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a ``learning via translation'' framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation. Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets",
    "checked": true,
    "id": "b3dbb682bb8b71723538f433b1dd007ed0b9b244",
    "semantic_title": "image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification",
    "citation_count": 901,
    "authors": [
      "Weijian Deng",
      "Liang Zheng",
      "Qixiang Ye",
      "Guoliang Kang",
      "Yi Yang",
      "Jianbin Jiao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_A_Generative_Adversarial_CVPR_2018_paper.html": {
    "title": "A Generative Adversarial Approach for Zero-Shot Learning From Noisy Texts",
    "volume": "main",
    "abstract": "Most existing zero-shot learning methods consider the problem as a visual semantic embedding one. Given the demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage GANs to imagine unseen categories from text descriptions and hence recognize novel classes with no examples being seen. Specifically, we propose a simple yet effective generative model that takes as input noisy text descriptions about an unseen class (e.g.Wikipedia articles) and generates synthesized visual features for this class. With added pseudo data, zero-shot learning is naturally converted to a traditional classification problem. Additionally, to preserve the inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision. Unlike previous methods using complex engineered regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that our method consistently outperforms the state of the art on the largest available benchmarks on Text-based Zero-shot Learning",
    "checked": true,
    "id": "18ffa15d72a9c2e1be3cfce66b69bbc8114f8ef0",
    "semantic_title": "a generative adversarial approach for zero-shot learning from noisy texts",
    "citation_count": 386,
    "authors": [
      "Yizhe Zhu",
      "Mohamed Elhoseiny",
      "Bingchen Liu",
      "Xi Peng",
      "Ahmed Elgammal"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hwang_Tensorize_Factorize_and_CVPR_2018_paper.html": {
    "title": "Tensorize, Factorize and Regularize: Robust Visual Relationship Learning",
    "volume": "main",
    "abstract": "Visual relationships provide higher-level information of objects and their relations in an image – this enables a semantic understanding of the scene and helps downstream applications. Given a set of localized objects in some training data, visual relationship detection seeks to detect the most likely \"relationship\" between objects in a given image. While the specific objects may be well represented in training data, their relationships may still be infrequent. The empirical distribution obtained from seeing these relationships in a dataset does not model the underlying distribution well — a serious issue for most learning methods. In this work, we start from a simple multi-relational learning model, which in principle, offers a rich formalization for deriving a strong prior for learning visual relationships. While the inference problem for deriving the regularizer is challenging, our main technical contribution is to show how adapting recent results in numerical linear algebra lead to efficient algorithms for a factorization scheme that yields highly informative priors. The factorization provides sample size bounds for inference (under mild conditions) for the underlying [[object, predicate, object]] relationship learning task on its own and surprisingly outperforms (in some cases) existing methods even without utilizing visual features. Then, when integrated with an end to-end architecture for visual relationship detection leveraging image data, we substantially improve the state-of-the-art",
    "checked": true,
    "id": "1b6d41795de1fd9a0da4227c83dc4dd038a229ec",
    "semantic_title": "tensorize, factorize and regularize: robust visual relationship learning",
    "citation_count": 61,
    "authors": [
      "Seong Jae Hwang",
      "Sathya N. Ravi",
      "Zirui Tao",
      "Hyunwoo J. Kim",
      "Maxwell D. Collins",
      "Vikas Singh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Transductive_Unbiased_Embedding_CVPR_2018_paper.html": {
    "title": "Transductive Unbiased Embedding for Zero-Shot Learning",
    "volume": "main",
    "abstract": "Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem, in which instances of unseen (target) classes tend to be categorized as one of the seen (source) classes. So they yield poor performance after being deployed in the generalized ZSL settings. In this paper, we propose a straightforward yet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviate the bias problem. Our method follows the way of transductive learning, which assumes that both the labeled source images and unlabeled target images are available for training. In the semantic embedding space, the labeled source images are mapped to several fixed points specified by the source categories, and the unlabeled target images are forced to be mapped to other points specified by the target categories. Experiments conducted on AwA2, CUB and SUN datasets demonstrate that our method outperforms existing state-of-the-art approaches by a huge margin of 9.3~24.5% following generalized ZSL settings, and by a large margin of 0.2~16.2% following conventional ZSL settings",
    "checked": true,
    "id": "fff80db5fd8f9b9d7b3ddc468907e7c47f225681",
    "semantic_title": "transductive unbiased embedding for zero-shot learning",
    "citation_count": 197,
    "authors": [
      "Jie Song",
      "Chengchao Shen",
      "Yezhou Yang",
      "Yang Liu",
      "Mingli Song"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Hierarchical_Novelty_Detection_CVPR_2018_paper.html": {
    "title": "Hierarchical Novelty Detection for Visual Object Recognition",
    "volume": "main",
    "abstract": "Deep neural networks have achieved impressive success in large-scale visual object recognition tasks with a predefined set of classes. However, recognizing objects of novel classes unseen during training still remains challenging. The problem of detecting such novel classes has been addressed in the literature, but most prior works have focused on providing simple binary or regressive decisions, e.g., the output would be \"known,\" \"novel,\" or corresponding confidence intervals. In this paper, we study more informative novelty detection schemes based on a hierarchical classification framework. For an object of a novel class, we aim for finding its closest super class in the hierarchical taxonomy of known classes. To this end, we propose two different approaches termed top-down and flatten methods, and their combination as well. The essential ingredients of our methods are confidence-calibrated classifiers, data relabeling, and the leave-one-out strategy for modeling novel classes under the hierarchical taxonomy. Furthermore, our method can generate a hierarchical embedding that leads to improved generalized zero-shot learning performance in combination with other commonly-used semantic embeddings",
    "checked": true,
    "id": "0a47481e741172f09eca86229561da8aa05bf6b2",
    "semantic_title": "hierarchical novelty detection for visual object recognition",
    "citation_count": 69,
    "authors": [
      "Kibok Lee",
      "Kimin Lee",
      "Kyle Min",
      "Yuting Zhang",
      "Jinwoo Shin",
      "Honglak Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper.html": {
    "title": "Zero-Shot Visual Recognition Using Semantics-Preserving Adversarial Embedding Networks",
    "volume": "main",
    "abstract": "We propose a novel framework called Semantics-Preserving Adversarial Embedding Network (SP-AEN) for zero-shot visual recognition (ZSL), where test images and their classes are both unseen during training. SP-AEN aims to tackle the inherent problem — semantic loss — in the prevailing family of embedding-based ZSL, where some semantics would be discarded during training if they are non-discriminative for training classes, but could become critical for recognizing test classes. Specifically, SP-AEN prevents the semantic loss by introducing an independent visual-to-semantic space embedder which disentangles the semantic space into two subspaces for the two arguably conflicting objectives: classification and reconstruction. Through adversarial learning of the two subspaces, SP-AEN can transfer the semantics from the reconstructive subspace to the discriminative one, accomplishing the improved zero-shot recognition of unseen classes. Comparing with prior works, SP-AEN can not only improve classification but also generate photo-realistic images, demonstrating the effectiveness of semantic preservation. On four popular benchmarks: CUB, AWA, SUN and aPY, SP-AEN considerably outperforms other state-of-the-art methods by an absolute performance difference of 12.2%, 9.3%, 4.0%, and 3.6% in terms of harmonic mean values",
    "checked": true,
    "id": "73131d9b977fac360316c5959a876d0e8cc11224",
    "semantic_title": "zero-shot visual recognition using semantics-preserving adversarial embedding networks",
    "citation_count": 266,
    "authors": [
      "Long Chen",
      "Hanwang Zhang",
      "Jun Xiao",
      "Wei Liu",
      "Shih-Fu Chang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Learning_Rich_Features_CVPR_2018_paper.html": {
    "title": "Learning Rich Features for Image Manipulation Detection",
    "volume": "main",
    "abstract": "Image manipulation detection is different from traditional semantic object detection because it pays more attention to tampering artifacts than to image content, which suggests that richer features need to be learned. We propose a two-stream Faster R-CNN network and train it end-to- end to detect the tampered regions given a manipulated image. One of the two streams is an RGB stream whose purpose is to extract features from the RGB image input to find tampering artifacts like strong contrast difference, unnatural tampered boundaries, and so on. The other is a noise stream that leverages the noise features extracted from a steganalysis rich model filter layer to discover the noise inconsistency between authentic and tampered regions. We then fuse features from the two streams through a bilinear pooling layer to further incorporate spatial co-occurrence of these two modalities. Experiments on four standard image manipulation datasets demonstrate that our two-stream framework outperforms each individual stream, and also achieves state-of-the-art performance compared to alternative methods with robustness to resizing and compression",
    "checked": true,
    "id": "afc4cc092f990644ff7a11dc7ab60519920cbc9d",
    "semantic_title": "learning rich features for image manipulation detection",
    "citation_count": 572,
    "authors": [
      "Peng Zhou",
      "Xintong Han",
      "Vlad I. Morariu",
      "Larry S. Davis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kalayeh_Human_Semantic_Parsing_CVPR_2018_paper.html": {
    "title": "Human Semantic Parsing for Person Re-Identification",
    "volume": "main",
    "abstract": "Person re-identification is a challenging task mainly due to factors such as background clutter, pose, illumination and camera point of view variations. These elements hinder the process of extracting robust and discriminative representations, hence preventing different identities from being successfully distinguished. To improve the representation learning, usually local features from human body parts are extracted. However, the common practice for such a process has been based on bounding box part detection. In this paper, we propose to adopt human semantic parsing which, due to its pixel-level accuracy and capability of modeling arbitrary contours, is naturally a better alternative. Our proposed SPReID integrates human semantic parsing in person re-identification and not only considerably outperforms its counter baseline, but achieves state-of-the-art performance. We also show that, by employing a simple yet effective training strategy, standard popular deep convolutional architectures such as Inception-V3 and ResNet-152, with no modification, while operating solely on full image, can dramatically outperform current state-of-the-art. Our proposed methods improve state-of-the-art person re-identification on: Market-1501 by ~17% in mAP and ~6% in rank-1, CUHK03 by ~4% in rank-1 and DukeMTMC-reID by ~24% in mAP and ~10% in rank-1",
    "checked": true,
    "id": "65da5a05fa6b49e8aa11b4acce965a0f439c1716",
    "semantic_title": "human semantic parsing for person re-identification",
    "citation_count": 591,
    "authors": [
      "Mahdi M. Kalayeh",
      "Emrah Basaran",
      "Muhittin Gökmen",
      "Mustafa E. Kamasak",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_Stacked_Latent_Attention_CVPR_2018_paper.html": {
    "title": "Stacked Latent Attention for Multimodal Reasoning",
    "volume": "main",
    "abstract": "Attention has shown to be a pivotal development in deep learning and has been used for a multitude of multimodal learning tasks such as visual question answering and image captioning. In this work, we pinpoint the potential limitations to the design of a traditional attention model. We identify that 1) current attention mechanisms discard the latent information from intermediate reasoning, losing the positional information already captured by the attention heatmaps and 2) stacked attention, a common way to improve spatial reasoning, may have suboptimal performance because of the vanishing gradient problem. We introduce a novel attention architecture to address these problems, in which all spatial configuration information contained in the intermediate reasoning process is retained in a pathway of convolutional layers. We show that this new attention leads to substantial improvements in multiple multimodal reasoning tasks, including achieving single model performance without using external knowledge comparable to the state-of-the-art on the VQA dataset, as well as clear gains for the image captioning task",
    "checked": true,
    "id": "135c71101af5d030f8cf470c454e7b655d699920",
    "semantic_title": "stacked latent attention for multimodal reasoning",
    "citation_count": 41,
    "authors": [
      "Haoqi Fan",
      "Jiatong Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Singh_R-FCN-3000_at_30fps_CVPR_2018_paper.html": {
    "title": "R-FCN-3000 at 30fps: Decoupling Detection and Classification",
    "volume": "main",
    "abstract": "We propose a modular approach towards large-scale real-time object detection by decoupling objectness detection and classification. We exploit the fact that many object classes are visually similar and share parts. Thus, a universal objectness detector can be learned for class-agnostic object detection followed by fine-grained classification using a (non)linear classifier. Our approach is a modification of the R-FCN architecture to learn shared filters for performing localization across different object classes. We trained a detector for 3000 object classes, called R-FCN-3000, that obtains an mAP of 34.9% on the ImageNet detection dataset. It outperforms YOLO-9000 by 18% while processing 30 images per second. We also show that the objectness learned by R-FCN-3000 generalizes to novel classes and the performance increases with the number of training object classes - supporting the hypothesis that it is possible to learn a universal objectness detector",
    "checked": true,
    "id": "56515b8eef38f6fe38f9e8303ae3afe0add96ddd",
    "semantic_title": "r-fcn-3000 at 30fps: decoupling detection and classification",
    "citation_count": 94,
    "authors": [
      "Bharat Singh",
      "Hengduo Li",
      "Abhishek Sharma",
      "Larry S. Davis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_CSRNet_Dilated_Convolutional_CVPR_2018_paper.html": {
    "title": "CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes",
    "volume": "main",
    "abstract": "We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method. We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach",
    "checked": true,
    "id": "1a964f13abb3cdc5675dbfd612fa0409608e28c7",
    "semantic_title": "csrnet: dilated convolutional neural networks for understanding the highly congested scenes",
    "citation_count": 1338,
    "authors": [
      "Yuhong Li",
      "Xiaofan Zhang",
      "Deming Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Uijlings_Revisiting_Knowledge_Transfer_CVPR_2018_paper.html": {
    "title": "Revisiting Knowledge Transfer for Training Object Class Detectors",
    "volume": "main",
    "abstract": "We propose to revisit knowledge transfer for training object detectors on target classes from weakly supervised training images, helped by a set of source classes with bounding-box annotations. We present a unified knowledge transfer framework based on training a single neural network multi-class object detector over all source classes, organized in a semantic hierarchy. This generates proposals with scores at multiple levels in the hierarchy, which we use to explore knowledge transfer over a broad range of generality, ranging from class-specific (bycicle to motorbike) to class-generic (objectness to any class). Experiments on the 200 object classes in the ILSVRC 2013 detection dataset show that our technique (1) leads to much better performance on the target classes (70.3% CorLoc, 36.9% mAP) than a weakly supervised baseline which uses manually engineered objectness [11] (50.5% CorLoc, 25.4% mAP). (2) delivers target object detectors reaching 80% of the mAP of their fully supervised counterparts. (3) outperforms the best reported transfer learning results on this dataset (+41% CorLoc and +3% mAP over [18, 46], +16.2% mAP over [32]). Moreover, we also carry out several across-dataset knowledge transfer experiments [27, 24, 35] and find that (4) our technique outperforms the weakly supervised baseline in all dataset pairs by 1.5 × −1.9×, establishing its general applicability",
    "checked": true,
    "id": "fa496716a5b8520e94a0126b5baa4f636623c997",
    "semantic_title": "revisiting knowledge transfer for training object class detectors",
    "citation_count": 71,
    "authors": [
      "Jasper Uijlings",
      "Stefan Popov",
      "Vittorio Ferrari"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_Deep_Sparse_Coding_CVPR_2018_paper.html": {
    "title": "Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons",
    "volume": "main",
    "abstract": "Deep feed-forward convolutional neural networks (CNNs) have become ubiquitous in virtually all machine learning and computer vision challenges; however, advancements in CNNs have arguably reached an engineering saturation point where incremental novelty results in minor performance gains. Although there is evidence that object classification has reached human levels on narrowly defined tasks, for general applications, the biological visual system is far superior to that of any computer. Research reveals there are numerous missing components in feed-forward deep neural networks that are critical in mammalian vision. The brain does not work solely in a feed-forward fashion, but rather all of the neurons are in competition with each other; neurons are integrating information in a bottom up and top down fashion and incorporating expectation and feedback in the modeling process. Furthermore, our visual cortex is working in tandem with our parietal lobe, integrating sensory information from various modalities. In our work, we sought to improve upon the standard feed-forward deep learning model by augmenting them with biologically inspired concepts of sparsity, top down feedback, and lateral inhibition. We define our model as a sparse coding problem using hierarchical layers. We solve the sparse coding problem with an additional top down feedback error driving the dynamics of the neural network. While building and observing the behavior of our model, we were fascinated that multimodal, invariant neurons naturally emerged that mimicked, \"Halle Berry neurons\" found in the human brain. These neurons trained in our sparse model learned to respond to high level concepts from multiple modalities, which is not the case with a standard feed-forward autoencoder. Furthermore, our sparse representation of multimodal signals demonstrates qualitative and quantitative superiority to the standard feed-forward joint embedding in common vision and machine learning tasks",
    "checked": true,
    "id": "310efb0e546502386cad99dc6673c7583c2020d5",
    "semantic_title": "deep sparse coding for invariant multimodal halle berry neurons",
    "citation_count": 25,
    "authors": [
      "Edward Kim",
      "Darryl Hannan",
      "Garrett Kenyon"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ehret_On_the_Convergence_CVPR_2018_paper.html": {
    "title": "On the Convergence of PatchMatch and Its Variants",
    "volume": "main",
    "abstract": "Many problems in image/video processing and computer vision require the computation of a dense k-nearest neighbor field (k-NNF) between two images. For each patch in a query image, the k-NNF determines the positions of the k most similar patches in a database image. With the introduction of the PatchMatch algorithm, Barnes et al. demonstrated that this large search problem can be approximated efficiently by collaborative search methods that exploit the local coherency of image patches. After its introduction, several variants of the original PatchMatch algorithm have been proposed, some of them reducing the computational time by two orders of magnitude. In this work we propose a theoretical framework for the analysis of PatchMatch and its variants, and apply it to derive bounds on their covergence rate. We consider a generic PatchMatch algorithm from which most specific instances found in the literature can be derived as particular cases. We also derive more specific bounds for two of these particular cases: the original PatchMatch and Coherency Sensitive Hashing. The proposed bounds are validated by contrasting them to the convergence observed in practice",
    "checked": true,
    "id": "e983fb6f96fe0462038e6a2b6eb7c17564c4d1f3",
    "semantic_title": "on the convergence of patchmatch and its variants",
    "citation_count": 2,
    "authors": [
      "Thibaud Ehret",
      "Pablo Arias"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Rethinking_the_Faster_CVPR_2018_paper.html": {
    "title": "Rethinking the Faster R-CNN Architecture for Temporal Action Localization",
    "volume": "main",
    "abstract": "We propose TAL-Net, an improved approach to temporal action localization in video that is inspired by the Faster R-CNN object detection framework. TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important. We achieve state-of-the-art performance for both action proposal and localization on THUMOS'14 detection benchmark and competitive performance on ActivityNet challenge",
    "checked": true,
    "id": "e85c01ff4979357b428538c9f224fa4259541c1a",
    "semantic_title": "rethinking the faster r-cnn architecture for temporal action localization",
    "citation_count": 647,
    "authors": [
      "Yu-Wei Chao",
      "Sudheendra Vijayanarasimhan",
      "Bryan Seybold",
      "David A. Ross",
      "Jia Deng",
      "Rahul Sukthankar"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xiao_MoNet_Deep_Motion_CVPR_2018_paper.html": {
    "title": "MoNet: Deep Motion Exploitation for Video Object Segmentation",
    "volume": "main",
    "abstract": "In this paper, we propose a novel MoNet model to deeply exploit motion cues for boosting video object segmentation performance from two aspects, i.e., frame representation learning and segmentation refinement. Concretely, MoNet exploits computed motion cue (i.e., optical flow) to reinforce the representation of the target frame by aligning and integrating representations from its neighbors. The new representation provides valuable temporal contexts for segmentation and improves robustness to various common contaminating factors, e.g., motion blur, appearance variation and deformation of video objects. Moreover, MoNet exploits motion inconsistency and transforms such motion cue into foreground/background prior to eliminate distraction from confusing instances and noisy regions. By introducing a distance transform layer, MoNet can effectively separate motion-inconstant instances/regions and thoroughly refine segmentation results. Integrating the proposed two motion exploitation components with a standard segmentation network, MoNet provides new state-of-the-art performance on three competitive benchmark datasets",
    "checked": true,
    "id": "8bd34358c0f28dc9a1c3aa3e6b6037c3bfa4f565",
    "semantic_title": "monet: deep motion exploitation for video object segmentation",
    "citation_count": 129,
    "authors": [
      "Huaxin Xiao",
      "Jiashi Feng",
      "Guosheng Lin",
      "Yu Liu",
      "Maojun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Video_Representation_Learning_CVPR_2018_paper.html": {
    "title": "Video Representation Learning Using Discriminative Pooling",
    "volume": "main",
    "abstract": "Popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---indeed, many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To this end, we learn a (nonlinear) hyperplane that separates this unknown, yet discriminative, feature from the rest. Applying multiple instance learning in a large-margin setup, we use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the features. We formulate a joint objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Our pooling scheme is end-to-end trainable within a deep framework. We report results from experiments on three benchmark datasets spanning a variety of challenges and demonstrate state-of-the-art performance across these tasks",
    "checked": true,
    "id": "7f36848ec69413253c2e76fa389424e9bf2d7054",
    "semantic_title": "video representation learning using discriminative pooling",
    "citation_count": 61,
    "authors": [
      "Jue Wang",
      "Anoop Cherian",
      "Fatih Porikli",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Recognizing_Human_Actions_CVPR_2018_paper.html": {
    "title": "Recognizing Human Actions as the Evolution of Pose Estimation Maps",
    "volume": "main",
    "abstract": "Most video-based action recognition approaches choose to extract features from the whole video to recognize actions. The cluttered background and non-action motions limit the performances of these methods, since they lack the explicit modeling of human body movements. With recent advances of human pose estimation, this work presents a novel method to recognize human action as the evolution of pose estimation maps. Instead of relying on the inaccurate human poses estimated from videos, we observe that pose estimation maps, the byproduct of pose estimation, preserve richer cues of human body to benefit action recognition. Specifically, the evolution of pose estimation maps can be decomposed as an evolution of heatmaps, e.g., probabilistic maps, and an evolution of estimated 2D human poses, which denote the changes of body shape and body pose, respectively. Considering the sparse property of heatmap, we develop spatial rank pooling to aggregate the evolution of heatmaps as a body shape evolution image. As body shape evolution image does not differentiate body parts, we design body guided sampling to aggregate the evolution of poses as a body pose evolution image. The complementary properties between both types of images are explored by deep convolutional neural networks to predict action label. Experiments on NTU RGB+D, UTD-MHAD and PennAction datasets verify the effectiveness of our method, which outperforms most state-of-the-art methods",
    "checked": true,
    "id": "63f7760e25420803ec969dad25ce40c796a21915",
    "semantic_title": "recognizing human actions as the evolution of pose estimation maps",
    "citation_count": 272,
    "authors": [
      "Mengyuan Liu",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Video_Person_Re-Identification_CVPR_2018_paper.html": {
    "title": "Video Person Re-Identification With Competitive Snippet-Similarity Aggregation and Co-Attentive Snippet Embedding",
    "volume": "main",
    "abstract": "In this paper, we address video-based person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding. Our approach divides long person sequences into multiple short video snippets and aggregates the top-ranked snippet similarities for sequence-similarity estimation. With this strategy, the intra-person visual variation of each sample could be minimized for similarity estimation, while the diverse appearance and temporal information are maintained. The snippet similarities are estimated by a deep neural network with a novel temporal co-attention for snippet embedding. The attention weights are obtained based on a query feature, which is learned from the whole probe snippet by an LSTM network, making the resulting embeddings less affected by noisy frames. The gallery snippet shares the same query feature with the probe snippet. Thus the embedding of gallery snippet can present more relevant features to compare with the probe snippet, yielding more accurate snippet similarity. Extensive ablation studies verify the effectiveness of competitive snippet-similarity aggregation as well as the temporal co-attentive embedding. Our method significantly outperforms the current state-of-the-art approaches on multiple datasets",
    "checked": true,
    "id": "3849c22a21e475d7ecba0afd80b9129eb006a72e",
    "semantic_title": "video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding",
    "citation_count": 200,
    "authors": [
      "Dapeng Chen",
      "Hongsheng Li",
      "Tong Xiao",
      "Shuai Yi",
      "Xiaogang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.html": {
    "title": "Mask-Guided Contrastive Attention Model for Person Re-Identification",
    "volume": "main",
    "abstract": "Person Re-identification (ReID) is an important yet challenging task in computer vision. Due to the diverse background clutters, variations on viewpoints and body poses, it is far from solved. How to extract discriminative and robust features invariant to background clutters is the core problem. In this paper, we first introduce the binary segmentation masks to construct synthetic RGB-Mask pairs as inputs, then we design a mask-guided contrastive attention model (MGCAM) to learn features separately from the body and background regions. Moreover, we propose a novel region-level triplet loss to restrain the features learnt from different regions, i.e., pulling the features from the full image and body region close, whereas pushing the features from backgrounds away. We may be the first one to successfully introduce the binary mask into person ReID task and the first one to propose region-level contrastive learning. We evaluate the proposed method on three public datasets, including MARS, Market-1501 and CUHK03. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results. Mask and code will be released upon request",
    "checked": true,
    "id": "17092ce59ed56eb5fcbf467a889c7ec3e930f4c2",
    "semantic_title": "mask-guided contrastive attention model for person re-identification",
    "citation_count": 569,
    "authors": [
      "Chunfeng Song",
      "Yan Huang",
      "Wanli Ouyang",
      "Liang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Blazingly_Fast_Video_CVPR_2018_paper.html": {
    "title": "Blazingly Fast Video Object Segmentation With Pixel-Wise Metric Learning",
    "volume": "main",
    "abstract": "This paper tackles the problem of video object segmentation, given some user annotation which indicates the object of interest. The problem is formulated as pixel-wise retrieval in a learned embedding space: we embed pixels of the same object instance into the vicinity of each other, using a fully convolutional network trained by a modified triplet loss as the embedding model. Then the annotated pixels are set as reference and the rest of the pixels are classified using a nearest-neighbor approach. The proposed method supports different kinds of user input such as segmentation mask in the first frame (semi-supervised scenario), or a sparse set of clicked points (interactive scenario). In the semi-supervised scenario, we achieve results competitive with the state of the art but at a fraction of computation cost (275 milliseconds per frame). In the interactive scenario where the user is able to refine their input iteratively, the proposed method provides instant response to each input, and reaches comparable quality to competing methods with much less interaction",
    "checked": true,
    "id": "d710777495f51144c5b9f0a7372d16e3843e1b25",
    "semantic_title": "blazingly fast video object segmentation with pixel-wise metric learning",
    "citation_count": 265,
    "authors": [
      "Yuhua Chen",
      "Jordi Pont-Tuset",
      "Alberto Montes",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sung_Learning_to_Compare_CVPR_2018_paper.html": {
    "title": "Learning to Compare: Relation Network for Few-Shot Learning",
    "volume": "main",
    "abstract": "We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks",
    "checked": true,
    "id": "bfe284e4338e62f0a61bb33398353efd687f206f",
    "semantic_title": "learning to compare: relation network for few-shot learning",
    "citation_count": 4049,
    "authors": [
      "Flood Sung",
      "Yongxin Yang",
      "Li Zhang",
      "Tao Xiang",
      "Philip H.S. Torr",
      "Timothy M. Hospedales"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.html": {
    "title": "COCO-Stuff: Thing and Stuff Classes in Context",
    "volume": "main",
    "abstract": "Semantic classes can be either things (objects with a well-defined shape, e.g. car, person) or stuff (amorphous background regions, e.g. grass, sky). While lots of classification and detection works focus on thing classes, less attention has been given to stuff classes. Nonetheless, stuff classes are important as they allow to explain important aspects of an image, including (1) scene type; (2) which thing classes are likely to be present and their location (through contextual reasoning); (3) physical attributes, material types and geometric properties of the scene. To understand stuff and things in context we introduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff annotation protocol based on superpixels, which leverages the original thing annotations. We quantify the speed versus quality trade-off of our protocol and explore the relation between annotation time and boundary complexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of stuff and thing classes in terms of their surface cover and how frequently they are mentioned in image captions; (b) the spatial relations between stuff and things, highlighting the rich contextual relations that make our dataset unique; (c) the performance of a modern semantic segmentation method on stuff and thing classes, and whether stuff is easier to segment than things",
    "checked": true,
    "id": "0095b9f73c000f2609fc81ffb7769df7cd77bda1",
    "semantic_title": "coco-stuff: thing and stuff classes in context",
    "citation_count": 1385,
    "authors": [
      "Holger Caesar",
      "Jasper Uijlings",
      "Vittorio Ferrari"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Johnson_Image_Generation_From_CVPR_2018_paper.html": {
    "title": "Image Generation From Scene Graphs",
    "volume": "main",
    "abstract": "To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on gen- erating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and rela- tionships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explic- itly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, com- putes a scene layout by predicting bounding boxes and seg- mentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to en- sure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, abla- tions, and user studies demonstrate our method's ability to generate complex images with multiple objects",
    "checked": true,
    "id": "46b5d408d950287637dd21ce04772d9b2bacfd14",
    "semantic_title": "image generation from scene graphs",
    "citation_count": 820,
    "authors": [
      "Justin Johnson",
      "Agrim Gupta",
      "Li Fei-Fei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Deep_Cauchy_Hashing_CVPR_2018_paper.html": {
    "title": "Deep Cauchy Hashing for Hamming Space Retrieval",
    "volume": "main",
    "abstract": "Due to its computation efficiency and retrieval quality, hashing has been widely applied to approximate nearest neighbor search for large-scale image retrieval, while deep hashing further improves the retrieval quality by end-to-end representation learning and hash coding. With compact hash codes, Hamming space retrieval enables the most efficient constant-time search that returns data points within a given Hamming radius to each query, by hash table lookups instead of linear scan. However, subject to the weak capability of concentrating relevant images to be within a small Hamming ball due to mis-specified loss functions, existing deep hashing methods may underperform for Hamming space retrieval. This work presents Deep Cauchy Hashing (DCH), a novel deep hashing model that generates compact and concentrated binary hash codes to enable efficient and effective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution, which penalizes significantly on similar image pairs with Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate that DCH can generate highly concentrated hash codes and yield state-of-the-art Hamming space retrieval performance on three datasets, NUS-WIDE, CIFAR-10, and MS-COCO",
    "checked": true,
    "id": "e7b92fc6e2f1a13a3076a48a78e03badacb0465b",
    "semantic_title": "deep cauchy hashing for hamming space retrieval",
    "citation_count": 294,
    "authors": [
      "Yue Cao",
      "Mingsheng Long",
      "Bin Liu",
      "Jianmin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jayaraman_Learning_to_Look_CVPR_2018_paper.html": {
    "title": "Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks",
    "volume": "main",
    "abstract": "It is common to implicitly assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is itself a major challenge. We address the problem of learning to look around: if an agent has the ability to voluntarily acquire new views to observe its environment, how can it learn efficient exploratory behaviors to acquire informative visual observations? We propose a reinforcement learning solution, where the agent is rewarded for actions that reduce its uncertainty about the unobserved portions of its environment. Based on this principle, we develop a recurrent neural network-based approach to perform active completion of panoramic natural scenes and 3D object shapes. Crucially, the learned policies are not tied to any recognition task nor to the particular semantic content seen during training. As a result, 1) the learned \"look around\" behavior is relevant even for new tasks in unseen environments, and 2) training data acquisition involves no manual labeling. Through tests in diverse settings, we demonstrate that our approach learns useful generic policies that transfer to new unseen tasks and environments",
    "checked": true,
    "id": "be6712aba5b1d35c1d4062664fbd6bcdad06c71f",
    "semantic_title": "learning to look around: intelligently exploring unseen environments for unknown tasks",
    "citation_count": 104,
    "authors": [
      "Dinesh Jayaraman",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Multi-Scale_Location-Aware_Kernel_CVPR_2018_paper.html": {
    "title": "Multi-Scale Location-Aware Kernel Representation for Object Detection",
    "volume": "main",
    "abstract": "Although Faster R-CNN and its variants have shown promising performance in object detection, they only exploit simple first order representation of object proposals for final classification and regression. Recent classification methods demonstrate that the integration of high order statistics into deep convolutional neural networks can achieve impressive improvement, but their goal is to model whole images by discarding location information so that they cannot be directly adopted to object detection. In this paper, we make an attempt to exploit high-order statistics in object detection, aiming at generating more discriminative representations for proposals to enhance the performance of detectors. To this end, we propose a novel Multi-scale Location-aware Kernel Representation (MLKP) to capture high-order statistics of deep features in proposals. Our MLKP can be efficiently computed on a modified multi-scale feature map using a low-dimensional polynomial kernel approximation. Moreover, different from existing orderless global representations based on high-order statistics, our proposed MLKP is location retentive and sensitive so that it can be flexibly adopted to object detection. Through integrating into Faster R-CNN schema, the proposed MLKP achieves very competitive performance with state-of-the-art methods, and improves Faster R-CNN by 4.9% (mAP), 4.7% (mAP) and 5.0 (AP at IOU=[0.5:0.05:0.95]) on PASCAL VOC 2007, VOC 2012 and MS COCO benchmarks, respectively. Code is available at: https://github.com/Hwang64/MLKP",
    "checked": true,
    "id": "c4fe488c0cff49e1ed6de1425ede27900005fd87",
    "semantic_title": "multi-scale location-aware kernel representation for object detection",
    "citation_count": 66,
    "authors": [
      "Hao Wang",
      "Qilong Wang",
      "Mingqi Gao",
      "Peihua Li",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Clinical_Skin_Lesion_CVPR_2018_paper.html": {
    "title": "Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria",
    "volume": "main",
    "abstract": "The skin is the largest organ in human body. Around 30%-70% of individuals worldwide have skin related health problems, for whom effective and efficient diagnosis is necessary. Recently, computer aided diagnosis (CAD) systems have been successfully applied to the recognition of skin cancers in dermatoscopic images. However, little work has concentrated on the commonly encountered skin diseases in clinical images captured by easily-accessed cameras or mobile phones. Meanwhile, for a CAD system, the representations of skin lesions are required to be understandable for dermatologists so that the predictions are convincing. To address this problem, we present effective representations inspired by the accepted dermatological criteria for diagnosing clinical skin lesions. We demonstrate that the dermatological criteria are highly correlated with measurable visual components. Accordingly, we design six medical representations considering different criteria for the recognition of skin lesions, and construct a diagnosis system for clinical skin disease images. Experimental results show that the proposed medical representations can not only capture the manifestations of skin lesions effectively, and consistently with the dermatological criteria, but also improve the prediction performance with respect to the state-of-the-art methods based on uninterpretable features",
    "checked": true,
    "id": "b8ddb2a7a2caee5da1afee474f71c6b76dbe9a30",
    "semantic_title": "clinical skin lesion diagnosis using representations inspired by dermatologist criteria",
    "citation_count": 89,
    "authors": [
      "Jufeng Yang",
      "Xiaoxiao Sun",
      "Jie Liang",
      "Paul L. Rosin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Compare_and_Contrast_CVPR_2018_paper.html": {
    "title": "Compare and Contrast: Learning Prominent Visual Differences",
    "volume": "main",
    "abstract": "Relative attribute models can compare images in terms of all detected properties or attributes, exhaustively predicting which image is fancier, more natural, and so on without any regard to ordering. However, when humans compare images, certain differences will naturally stick out and come to mind first. These most noticeable differences, or prominent differences, are likely to be described first. In addition, many differences, although present, may not be mentioned at all. In this work, we introduce and model prominent differences, a rich new functionality for comparing images. We collect instance-level annotations of most noticeable differences, and build a model trained on relative attribute features that predicts prominent differences for unseen pairs. We test our model on the challenging UT-Zap50K shoes and LFW-10 faces datasets, and outperform an array of baseline methods. We then demonstrate how our prominence model improves two vision tasks, image search and description generation, enabling more natural communication between people and vision systems",
    "checked": true,
    "id": "d3a0240a2dde30e3c7b977358843a5013b3f4d42",
    "semantic_title": "compare and contrast: learning prominent visual differences",
    "citation_count": 6,
    "authors": [
      "Steven Chen",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ge_Multi-Evidence_Filtering_and_CVPR_2018_paper.html": {
    "title": "Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning",
    "volume": "main",
    "abstract": "Supervised object detection and semantic segmentation require object or even pixel level annotations. When there exist image level labels only, it is challenging for weakly supervised algorithms to achieve accurate predictions. The accuracy achieved by top weakly supervised algorithms is still significantly lower than their fully supervised counterparts. In this paper, we propose a novel weakly supervised curriculum learning pipeline for multi-label object recognition, detection and semantic segmentation. In this pipeline, we first obtain intermediate object localization and pixel labeling results for the training images, and then use such results to train task-specific deep networks in a fully supervised manner. The entire process consists of four stages, including object localization in the training images, filtering and fusing object instances, pixel labeling for the training images, and task-specific network training. To obtain clean object instances in the training images, we propose a novel algorithm for filtering, fusing and classifying object instances collected from multiple solution mechanisms. In this algorithm, we incorporate both metric learning and density-based clustering to filter detected object instances. Experiments show that our weakly supervised pipeline achieves state-of-the-art results in multi-label image classification as well as weakly supervised object detection and very competitive results in weakly supervised semantic segmentation on MS-COCO, PASCAL VOC 2007 and PASCAL VOC 2012",
    "checked": true,
    "id": "f8305f7ce465bfdb129550c43d6f5bc053e7ebcf",
    "semantic_title": "multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning",
    "citation_count": 189,
    "authors": [
      "Weifeng Ge",
      "Sibei Yang",
      "Yizhou Yu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_HashGAN_Deep_Learning_CVPR_2018_paper.html": {
    "title": "HashGAN: Deep Learning to Hash With Pair Conditional Wasserstein GAN",
    "volume": "main",
    "abstract": "Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information. Subject to the scarcity of similarity information that is often expensive to collect for many application domains, existing deep learning to hash methods may overfit the training data and result in substantial loss of retrieval quality. This paper presents HashGAN, a novel architecture for deep learning to hash, which learns compact binary hash codes from both real images and diverse images synthesized by generative models. The main idea is to augment the training data with nearly real images synthesized from a new Pair Conditional Wasserstein GAN (PC-WGAN) conditioned on the pairwise similarity information. Extensive experiments demonstrate that HashGAN can generate high-quality binary hash codes and yield state-of-the-art image retrieval performance on three benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO",
    "checked": true,
    "id": "ecd546f774f29a2e9154dd64d4d51fc04f2a31f8",
    "semantic_title": "hashgan: deep learning to hash with pair conditional wasserstein gan",
    "citation_count": 107,
    "authors": [
      "Yue Cao",
      "Bin Liu",
      "Mingsheng Long",
      "Jianmin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wan_Min-Entropy_Latent_Model_CVPR_2018_paper.html": {
    "title": "Min-Entropy Latent Model for Weakly Supervised Object Detection",
    "volume": "main",
    "abstract": "Weakly supervised object detection is a challenging task when provided with image category supervision but required to learn, at the same time, object locations and object detectors. The inconsistency between the weak supervision and learning objectives introduces randomness to object locations and ambiguity to detectors. In this paper, a min-entropy latent model (MELM) is proposed for weakly supervised object detection. Min-entropy is used as a metric to measure the randomness of object localization during learning, as well as serving as a model to learn object locations. It aims to principally reduce the variance of positive instances and alleviate the ambiguity of detectors. MELM is deployed as two sub-models, which respectively discovers and localizes objects by minimizing the global and local entropy. MELM is unified with feature learning and optimized with a recurrent learning algorithm, which progressively transfers the weak supervision to object locations. Experiments demonstrate that MELM significantly improves the performance of weakly supervised detection, weakly supervised localization, and image classification, against the state-of-the-art approaches",
    "checked": true,
    "id": "288aea9479e7e76bfd6dee01c0f8e5c6ed76a18c",
    "semantic_title": "min-entropy latent model for weakly supervised object detection",
    "citation_count": 217,
    "authors": [
      "Fang Wan",
      "Pengxu Wei",
      "Jianbin Jiao",
      "Zhenjun Han",
      "Qixiang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_MAttNet_Modular_Attention_CVPR_2018_paper.html": {
    "title": "MAttNet: Modular Attention Network for Referring Expression Comprehension",
    "volume": "main",
    "abstract": "In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit, we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: language-based attention that learns the module weights as well as the word/phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components. Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-the-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks",
    "checked": true,
    "id": "fdce9cbe5c726201575b3c8a8c1af0752f1af53f",
    "semantic_title": "mattnet: modular attention network for referring expression comprehension",
    "citation_count": 827,
    "authors": [
      "Licheng Yu",
      "Zhe Lin",
      "Xiaohui Shen",
      "Jimei Yang",
      "Xin Lu",
      "Mohit Bansal",
      "Tamara L. Berg"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.html": {
    "title": "AttnGAN: Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks",
    "volume": "main",
    "abstract": "In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different sub-regions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image",
    "checked": true,
    "id": "8b35c00edfa4edfd7a99d816e671023d2c000d55",
    "semantic_title": "attngan: fine-grained text to image generation with attentional generative adversarial networks",
    "citation_count": 1716,
    "authors": [
      "Tao Xu",
      "Pengchuan Zhang",
      "Qiuyuan Huang",
      "Han Zhang",
      "Zhe Gan",
      "Xiaolei Huang",
      "Xiaodong He"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper.html": {
    "title": "Adversarial Complementary Learning for Weakly Supervised Object Localization",
    "volume": "main",
    "abstract": "In this work, we propose Adversarial Complementary Learning (ACoL) to automatically localize integral objects of semantic interest with weak supervision. We first mathematically prove that class localization maps can be obtained by directly selecting the class-specific feature maps of the last convolutional layer, which paves a simple way to identify object regions. We then present a simple network architecture including two parallel-classifiers for object localization. Specifically, we leverage one classification branch to dynamically localize some discriminative object regions during the forward pass. Although it is usually responsive to sparse parts of the target objects, this classifier can drive the counterpart classifier to discover new and complementary object regions by erasing its discovered regions from the feature maps. With such an adversarial learning, the two parallel-classifiers are forced to leverage complementary object regions for classification and can finally generate integral object localization together. The merits of ACoL are mainly two-fold: 1) it can be trained in an end-to-end manner; 2) dynamically erasing enables the counterpart classifier to discover complementary object regions more effectively. We demonstrate the superiority of our ACoL approach in a variety of experiments. In particular, the Top-1 localization error rate on the ILSVRC dataset is 45.14%, which is the new state-of-the-art",
    "checked": true,
    "id": "4690424c5c5d73b95da46b38a28a195f82860cdd",
    "semantic_title": "adversarial complementary learning for weakly supervised object localization",
    "citation_count": 572,
    "authors": [
      "Xiaolin Zhang",
      "Yunchao Wei",
      "Jiashi Feng",
      "Yi Yang",
      "Thomas S. Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.html": {
    "title": "Conditional Generative Adversarial Network for Structured Domain Adaptation",
    "volume": "main",
    "abstract": "In recent years, deep neural nets have triumphed over many computer vision problems, including semantic segmentation, which is a critical task in emerging autonomous driving and medical image diagnostics applications. In general, training deep neural nets requires a humongous amount of labeled data, which is laborious and costly to collect and annotate. Recent advances in computer graphics shed light on utilizing photo-realistic synthetic data with computer generated annotations to train neural nets. Nevertheless, the domain mismatch between real images and synthetic ones is the major challenge against harnessing the generated data and labels. In this paper, we propose a principled way to conduct structured domain adaption for semantic segmentation, i.e., integrating GAN into the FCN framework to mitigate the gap between source and target domains. Specifically, we learn a conditional generator to transform features of synthetic images to real-image like features, and a discriminator to distinguish them. For each training batch, the conditional generator and the discriminator compete against each other so that the generator learns to produce real-image like features to fool the discriminator; afterwards, the FCN parameters are updated to accommodate the changes of GAN. In experiments, without using labels of real image data, our method significantly outperforms the baselines as well as state-of-the-art methods by 12% ∼ 20% mean IoU on the Cityscapes dataset",
    "checked": true,
    "id": "9d5a5517650d5f9a7d9818bcc1eb59ba65d316e1",
    "semantic_title": "conditional generative adversarial network for structured domain adaptation",
    "citation_count": 287,
    "authors": [
      "Weixiang Hong",
      "Zhenzhen Wang",
      "Ming Yang",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_GroupCap_Group-Based_Image_CVPR_2018_paper.html": {
    "title": "GroupCap: Group-Based Image Captioning With Structured Relevance and Diversity Constraints",
    "volume": "main",
    "abstract": "Most image captioning models focus on one-line (single image) captioning, where the correlations like relevance and diversity among group images (e.g., within the same album or event) are simply neglected, resulting in less accurate and diverse captions. Recent works mainly consider imposing the diversity during the online inference only, which neglect the correlation among visual structures in offline training. In this paper, we propose a novel group-based image captioning scheme (termed GroupCap), which jointly models the structured relevance and diversity among visual contents of group images towards an optimal collaborative captioning. In particular, we first propose a visual tree parser (VP-Tree) to construct the structured semantic correlations within individual images. Then, the relevance and diversity among images are well modeled by exploiting the correlations among their tree structures. Finally, such correlations are modeled as constraints and sent into the LSTM-based captioning generator. In offline optimization, we adopt an end-to-end formulation, which jointly trains the visual tree parser, the structured relevance and diversity constraints, as well as the LSTM based captioning model. To facilitate quantitative evaluation, we further release two group captioning datasets derived from the MS-COCO benchmark, serving as the first of their kind. Quantitative results show that the proposed GroupCap model outperforms the state-of-the-art and alternative approaches, which can generate much more accurate and discriminative captions under various evaluation metrics",
    "checked": true,
    "id": "2115fe369b3a6b859c6992ba023d5c11b1689801",
    "semantic_title": "groupcap: group-based image captioning with structured relevance and diversity constraints",
    "citation_count": 77,
    "authors": [
      "Fuhai Chen",
      "Rongrong Ji",
      "Xiaoshuai Sun",
      "Yongjian Wu",
      "Jinsong Su"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.html": {
    "title": "Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features",
    "volume": "main",
    "abstract": "Weakly-supervised semantic segmentation under image tags supervision is a challenging task as it directly associates high-level semantic to low-level appearance. To bridge this gap, in this paper, we propose an iterative bottom-up and top-down framework which alternatively expands object regions and optimizes segmentation network. We start from initial localization produced by classification networks. While classification networks are only responsive to small and coarse discriminative object regions, we argue that, these regions contain significant common features about objects. So in the bottom-up step, we mine common object features from the initial localization and expand object regions with the mined features. To supplement non-discriminative regions, saliency maps are then considered under Bayesian framework to refine the object regions. Then in the top-down step, the refined object regions are used as supervision to train the segmentation network and to predict object masks. These object masks provide more accurate localization and contain more regions of object. Further, we take these object masks as initial localization and mine common object features from them. These processes are conducted iteratively to progressively produce fine object masks and optimize segmentation networks. Experimental results on Pascal VOC 2012 dataset demonstrate that the proposed method outperforms previous state-of-the-art methods by a large margin",
    "checked": true,
    "id": "affe7e93c31abf7a28088e5b3a83ddf24bf07a3f",
    "semantic_title": "weakly-supervised semantic segmentation by iteratively mining common object features",
    "citation_count": 303,
    "authors": [
      "Xiang Wang",
      "Shaodi You",
      "Xi Li",
      "Huimin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Bootstrapping_the_Performance_CVPR_2018_paper.html": {
    "title": "Bootstrapping the Performance of Webly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Fully supervised methods for semantic segmentation require pixel-level class masks to train, the creation of which are expensive in terms of manual labour and time. In this work, we focus on weak supervision, developing a method for training a high-quality pixel-level classifier for semantic segmentation, using only image-level class labels as the provided ground-truth. Our method is formulated as a two-stage approach in which we first aim to create accurate pixel-level masks for the training images via a bootstrapping process, and then use these now-accurately segmented images as a proxy ground-truth in a more standard supervised setting. The key driver for our work is that in the target dataset we typically have reliable ground-truth image-level labels, while data crawled from the web may have unreliable labels, but can be filtered to comprise only easy images to segment, therefore having reliable boundaries. These two forms of information are complementary and we use this observation to build a novel bi-directional transfer learning. This framework transfers knowledge between two domains, target domain and web domain, bootstrapping the performance of weakly supervised semantic segmentation. Conducting experiments on the popular benchmark dataset PASCAL VOC 2012 based on both a VGG16 network and on ResNet50, we reach state-of-the-art performance with scores of 60.2% IoU and 63.9% IoU respectively",
    "checked": true,
    "id": "1113be0bab406bcd3f86e0eefd283a119c6cd344",
    "semantic_title": "bootstrapping the performance of webly supervised semantic segmentation",
    "citation_count": 71,
    "authors": [
      "Tong Shen",
      "Guosheng Lin",
      "Chunhua Shen",
      "Ian Reid"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_DeepVoting_A_Robust_CVPR_2018_paper.html": {
    "title": "DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection Under Partial Occlusion",
    "volume": "main",
    "abstract": "In this paper, we study the task of detecting semantic parts of an object, e.g., a wheel of a car, under partial occlusion. We propose that all models should be trained without seeing occlusions while being able to transfer the learned knowledge to deal with occlusions. This setting alleviates the difficulty in collecting an exponentially large dataset to cover occlusion patterns and is more essential. In this scenario, the proposal-based deep networks, like RCNN-series, often produce unsatisfactory results, because both the proposal extraction and classification stages may be confused by the irrelevant occluders. To address this, [25] proposed a voting mechanism that combines multiple local visual cues to detect semantic parts. The semantic parts can still be detected even though some visual cues are missing due to occlusions. However, this method is manually-designed, thus is hard to be optimized in an end-to-end manner. In this paper, we present DeepVoting, which incorporates the robustness shown by [25] into a deep network, so that the whole pipeline can be jointly optimized. Specifically, it adds two layers after the intermediate features of a deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the evidence of local visual cues, and the second layer performs a voting mechanism by utilizing the spatial relationship between visual cues and semantic parts. We also propose an improved version DeepVoting+ by learning visual cues from context outside objects. In experiments, DeepVoting achieves significantly better performance than several baseline methods, including Faster-RCNN, for semantic part detection under occlusion. In addition, DeepVoting enjoys explainability as the detection results can be diagnosed via looking up the voting cues",
    "checked": true,
    "id": "adecc9cb7c4e71a401099b26ed5420b8d4f4e90a",
    "semantic_title": "deepvoting: a robust and explainable deep network for semantic part detection under partial occlusion",
    "citation_count": 41,
    "authors": [
      "Zhishuai Zhang",
      "Cihang Xie",
      "Jianyu Wang",
      "Lingxi Xie",
      "Alan L. Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Geometry-Aware_Scene_Text_CVPR_2018_paper.html": {
    "title": "Geometry-Aware Scene Text Detection With Instance Transformation Network",
    "volume": "main",
    "abstract": "Localizing text in the wild is challenging in the situations of complicated geometric layout of the targets like random orientation and large aspect ratio. In this paper, we propose a geometry-aware modeling approach tailored for scene text representation with an end-to-end learning scheme. In our approach, a novel Instance Transformation Network (ITN) is presented to learn the geometry-aware representation encoding the unique geometric configurations of scene text instances with in-network transformation embedding, resulting in a robust and elegant framework to detect words or text lines at one pass. An end-to-end multi-task learning strategy with transformation regression, text/non-text classification and coordinate regression is adopted in the ITN. Experiments on the benchmark datasets demonstrate the effectiveness of the proposed approach in detecting scene text in various geometric configurations",
    "checked": true,
    "id": "bc10d16228ed204dd28e54f1bff7567d05a6fb5e",
    "semantic_title": "geometry-aware scene text detection with instance transformation network",
    "citation_count": 94,
    "authors": [
      "Fangfang Wang",
      "Liming Zhao",
      "Xi Li",
      "Xinchao Wang",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Optical_Flow_Guided_CVPR_2018_paper.html": {
    "title": "Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition",
    "volume": "main",
    "abstract": "Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. The derivation also provides theoretical support for using the difference between two frames. By directly calculating pixel-wise spatio-temporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatio-temporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96.0% and 74.2% accuracy on UCF-101 and HMDB-51 respectively. The code for this project is available at: https://github.com/kevin-ssy/Optical-Flow-Guided-Feature",
    "checked": true,
    "id": "42517e072406ea6f8d2c579d98ee3f9918a8a1d3",
    "semantic_title": "optical flow guided feature: a fast and robust motion representation for video action recognition",
    "citation_count": 296,
    "authors": [
      "Shuyang Sun",
      "Zhanghui Kuang",
      "Lu Sheng",
      "Wanli Ouyang",
      "Wei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Motion-Guided_Cascaded_Refinement_CVPR_2018_paper.html": {
    "title": "Motion-Guided Cascaded Refinement Network for Video Object Segmentation",
    "volume": "main",
    "abstract": "Deep CNNs have achieved superior performance in many tasks of computer vision and image understanding. However, it is still difficult to effectively apply deep CNNs to video object segmentation(VOS) since treating video frames as separate and static will lose the information hidden in motion. To tackle this problem, we propose a Motion-guided Cascaded Refinement Network for VOS. By assuming the object motion is normally different from the background motion, for a video frame we first apply an active contour model on optical flow to coarsely segment objects of interest. Then, the proposed Cascaded Refinement Network(CRN) takes the coarse segmentation as guidance to generate an accurate segmentation of full resolution. In this way, the motion information and the deep CNNs can well complement each other to accurately segment objects from video frames. Furthermore, in CRN we introduce a Single-channel Residual Attention Module to incorporate the coarse segmentation map as attention, making our network effective and efficient in both training and testing. We perform experiments on the popular benchmarks and the results show that our method achieves state-of-the-art performance at a much faster speed",
    "checked": true,
    "id": "14d0a53ede10cb42cf3ef8429e24340ef18d0814",
    "semantic_title": "motion-guided cascaded refinement network for video object segmentation",
    "citation_count": 9,
    "authors": [
      "Ping Hu",
      "Gang Wang",
      "Xiangfei Kong",
      "Jason Kuen",
      "Yap-Peng Tan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_A_Memory_Network_CVPR_2018_paper.html": {
    "title": "A Memory Network Approach for Story-Based Temporal Summarization of 360° Videos",
    "volume": "main",
    "abstract": "We address the problem of story-based temporal summarization of long 360° videos. We propose a novel memory network model named Past-Future Memory Network (PFMN), in which we first compute the scores of 81 normal field of view (NFOV) region proposals cropped from the input 360° video, and then recover a latent, collective summary using the network with two external memories that store the embeddings of previously selected subshots and future candidate subshots. Our major contributions are two-fold. First, our work is the first to address story-based temporal summarization of 360° videos. Second, our model is the first attempt to leverage memory networks for video summarization tasks. For evaluation, we perform three sets of experiments. First, we investigate the view selection capability of our model on the Pano2Vid dataset. Second, we evaluate the temporal summarization with a newly collected 360° video dataset. Finally, we experiment our model's performance in another domain, with image-based storytelling VIST dataset. We verify that our model achieves state-of-the-art performance on all the tasks",
    "checked": true,
    "id": "e68c133947bbf14834f5353126ae85cc048642db",
    "semantic_title": "a memory network approach for story-based temporal summarization of 360° videos",
    "citation_count": 68,
    "authors": [
      "Sangho Lee",
      "Jinyoung Sung",
      "Youngjae Yu",
      "Gunhee Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Cube_Padding_for_CVPR_2018_paper.html": {
    "title": "Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos",
    "volume": "main",
    "abstract": "Automatic saliency prediction in 360° videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) unsupervisedly trained and (2) tailor-made for 360° viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360° sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360° view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, PC introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360° video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms all baseline methods in both speed and quality",
    "checked": true,
    "id": "d502f17bf778b153cb7aec6eaba97a7129d96a02",
    "semantic_title": "cube padding for weakly-supervised saliency prediction in 360° videos",
    "citation_count": 192,
    "authors": [
      "Hsien-Tzu Cheng",
      "Chun-Hung Chao",
      "Jin-Dong Dong",
      "Hao-Kai Wen",
      "Tyng-Luh Liu",
      "Min Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Appearance-and-Relation_Networks_for_CVPR_2018_paper.html": {
    "title": "Appearance-and-Relation Networks for Video Classification",
    "volume": "main",
    "abstract": "Spatiotemporal feature learning in videos is a fundamental problem in computer vision. This paper presents a new architecture, termed as Appearance-and-Relation Network (ARTNet), to learn video representation in an end-to-end manner. ARTNets are constructed by stacking multiple generic building blocks, called as SMART, whose goal is to simultaneously model appearance and relation from RGB input in a separate and explicit manner. Specifically, SMART blocks decouple the spatiotemporal learning module into an appearance branch for spatial modeling and a relation branch for temporal modeling. The appearance branch is implemented based on the linear combination of pixels or filter responses in each frame, while the relation branch is designed based on the multiplicative interactions between pixels or filter responses across multiple frames. We perform experiments on three action recognition benchmarks: Kinetics, UCF101, and HMDB51, demonstrating that SMART blocks obtain an evident improvement over 3D convolutions for spatiotemporal feature learning. Under the same training setting, ARTNets achieve superior performance on these three datasets to the existing state-of-the-art methods",
    "checked": true,
    "id": "e6b534d8838f92461a478a3e737b73e08db94748",
    "semantic_title": "appearance-and-relation networks for video classification",
    "citation_count": 351,
    "authors": [
      "Limin Wang",
      "Wei Li",
      "Wen Li",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bargal_Excitation_Backprop_for_CVPR_2018_paper.html": {
    "title": "Excitation Backprop for RNNs",
    "volume": "main",
    "abstract": "Deep models are state-of-the-art or many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing/training for these tasks",
    "checked": true,
    "id": "ad7fc80a2cc26221c077851f44afbd47e3ab6b46",
    "semantic_title": "excitation backprop for rnns",
    "citation_count": 48,
    "authors": [
      "Sarah Adel Bargal",
      "Andrea Zunino",
      "Donghyun Kim",
      "Jianming Zhang",
      "Vittorio Murino",
      "Stan Sclaroff"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_One-Shot_Action_Localization_CVPR_2018_paper.html": {
    "title": "One-Shot Action Localization by Learning Sequence Matching Network",
    "volume": "main",
    "abstract": "Learning based temporal action localization methods require vast amounts of training data. However, such large-scale video datasets, which are expected to capture the dynamics of every action category, are not only very expensive to acquire but are also not practical simply because there exists an uncountable number of action classes. This poses a critical restriction to the current methods when the training samples are few and rare (e.g. when the target action classes are not present in the current publicly available datasets). To address this challenge, we conceptualize a new example-based action detection problem where only a few examples are provided, and the goal is to find the occurrences of these examples in an untrimmed video sequence. Towards this objective, we introduce a novel one-shot action localization method that alleviates the need for large amounts of training samples. Our solution adopts the one-shot learning technique of Matching Network and utilizes correlations to mine and localize actions of previously unseen classes. We evaluate our one-shot action localization method on the THUMOS14 and ActivityNet datasets, of which we modified the configuration to fit our one-shot problem setup",
    "checked": true,
    "id": "bf69a9a967fcefaf66f4ca216de4d9afc68a496a",
    "semantic_title": "one-shot action localization by learning sequence matching network",
    "citation_count": 49,
    "authors": [
      "Hongtao Yang",
      "Xuming He",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Structure_Preserving_Video_CVPR_2018_paper.html": {
    "title": "Structure Preserving Video Prediction",
    "volume": "main",
    "abstract": "Despite recent emergence of adversarial based methods for video prediction, existing algorithms often produce unsatisfied results in image regions with rich structural information (i.e., object boundary) and detailed motion (i.e., articulated body movement). To this end, we present a structure preserving video prediction framework to explicitly address above issues and enhance video prediction quality. On one hand, our framework contains a two-stream generation architecture which deals with high frequency video content (i.e., detailed object or articulated motion structure) and low frequency video content (i.e., location or moving directions) in two separate streams. On the other hand, we propose a RNN structure for video prediction, which employs temporal-adaptive convolutional kernels to capture time-varying motion patterns as well as the tiny object within a scene. Extensive experiments on diverse scene, ranging from human motion to semantic layout prediction, demonstrate the effectiveness of the proposed video prediction approach",
    "checked": true,
    "id": "6ff497677f27d3e45e4633550c1a0e5243b8fa3b",
    "semantic_title": "structure preserving video prediction",
    "citation_count": 56,
    "authors": [
      "Jingwei Xu",
      "Bingbing Ni",
      "Zefan Li",
      "Shuo Cheng",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Person_Re-Identification_With_CVPR_2018_paper.html": {
    "title": "Person Re-Identification With Cascaded Pairwise Convolutions",
    "volume": "main",
    "abstract": "In this paper, a novel deep architecture named BraidNet is proposed for person re-identification. BraidNet has a specially designed WConv layer, and the cascaded WConv structure learns to extract the comparison features of two images, which are robust to misalignments and color differences across cameras. Furthermore, a Channel Scaling layer is designed to optimize the scaling factor of each input channel, which helps mitigate the zero gradient problem in the training phase. To solve the problem of imbalanced volume of negative and positive training samples, a Sample Rate Learning strategy is proposed to adaptively update the ratio between positive and negative samples in each batch. Experiments conducted on CUHK03-Detected, CUHK03-Labeled, CUHK01, Market-1501 and DukeMTMC-reID datasets demonstrate that our method achieves competitive performance when compared to state-of-the-art methods",
    "checked": true,
    "id": "90c18409b7a3be2cd6da599d02accba4c769e94e",
    "semantic_title": "person re-identification with cascaded pairwise convolutions",
    "citation_count": 128,
    "authors": [
      "Yicheng Wang",
      "Zhenzhong Chen",
      "Feng Wu",
      "Gang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zlateski_On_the_Importance_CVPR_2018_paper.html": {
    "title": "On the Importance of Label Quality for Semantic Segmentation",
    "volume": "main",
    "abstract": "Convolutional networks (ConvNets) have become the dominant approach to semantic image segmentation. Producing accurate, pixel--level labels required for this task is a tedious and time consuming process; however, producing approximate, coarse labels could take only a fraction of the time and effort. We investigate the relationship between the quality of labels and the performance of ConvNets for semantic segmentation. We create a very large synthetic dataset with perfectly labeled street view scenes. From these perfect labels, we synthetically coarsen labels with different qualities and estimate human--hours required for producing them. We perform a series of experiments by training ConvNets with a varying number of training images and label quality. We found that the performance of ConvNets mostly depends on the time spent creating the training labels. That is, a larger coarsely--annotated dataset can yield the same performance as a smaller finely--annotated one. Furthermore, fine--tuning coarsely pre--trained ConvNets with few finely-annotated labels can yield comparable or superior performance to training it with a large amount of finely-annotated labels alone, at a fraction of the labeling cost. We demonstrate that our result is also valid for different network architectures, and various object classes in an urban scene",
    "checked": true,
    "id": "28cbb86087738a038e5cd723203c73132a1918ba",
    "semantic_title": "on the importance of label quality for semantic segmentation",
    "citation_count": 79,
    "authors": [
      "Aleksandar Zlateski",
      "Ronnachai Jaroensri",
      "Prafull Sharma",
      "Frédo Durand"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chang_Scalable_and_Effective_CVPR_2018_paper.html": {
    "title": "Scalable and Effective Deep CCA via Soft Decorrelation",
    "volume": "main",
    "abstract": "Recently the widely used multi-view learning model, Canonical Correlation Analysis (CCA) has been generalised to the non-linear setting via deep neural networks. Existing deep CCA models typically first decorrelate the feature dimensions of each view before the different views are maximally correlated in a common latent space. This feature decorrelation is achieved by enforcing an exact decorrelation constraint; these models are thus computationally expensive due to the matrix inversion or SVD operations required for exact decorrelation at each training iteration. Furthermore, the decorrelation step is often separated from the gradient descent based optimisation, resulting in sub-optimal solutions. We propose a novel deep CCA model Soft CCA to overcome these problems. Specifically, exact decorrelation is replaced by soft decorrelation via a mini-batch based Stochastic Decorrelation Loss (SDL) to be optimised jointly with the other training objectives. Extensive experiments show that the proposed soft CCA is more effective and efficient than existing deep CCA models. In addition, our SDL loss can be applied to other deep models beyond multi-view learning, and obtains superior performance compared to existing decorrelation losses",
    "checked": true,
    "id": "39153bcd060957a5e79e511cd4d39ec8ada4ed6b",
    "semantic_title": "scalable and effective deep cca via soft decorrelation",
    "citation_count": 60,
    "authors": [
      "Xiaobin Chang",
      "Tao Xiang",
      "Timothy M. Hospedales"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Duplex_Generative_Adversarial_CVPR_2018_paper.html": {
    "title": "Duplex Generative Adversarial Network for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Domain adaptation attempts to transfer the knowledge obtained from the source domain to the target domain, i.e., the domain where the testing data are. The main challenge lies in the distribution discrepancy between source and target domain. Most existing works endeavor to learn domain invariant representation usually by minimizing a distribution distance, e.g., MMD and the discriminator in the recently proposed generative adversarial network (GAN). Following the similar idea of GAN, this work proposes a novel GAN architecture with duplex adversarial discriminators (referred to as DupGAN), which can achieve domain-invariant representation and domain transformation. Specifically, our proposed network consists of three parts, an encoder, a generator and two discriminators. The encoder embeds samples from both domains into the latent representation, and the generator decodes the latent representation to both source and target domains respectively conditioned on a domain code, i.e., achieves domain transformation. The generator is pitted against duplex discriminators, one for source domain and the other for target, to ensure the reality of domain transformation, the latent representation domain invariant and the category information of it preserved as well. Our proposed work achieves the state-of-the-art performance on unsupervised domain adaptation of digit classification and object recognition",
    "checked": true,
    "id": "57f80d7c6c750789dc3d75cd782e0413d91418b5",
    "semantic_title": "duplex generative adversarial network for unsupervised domain adaptation",
    "citation_count": 164,
    "authors": [
      "Lanqing Hu",
      "Meina Kan",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bai_Edit_Probability_for_CVPR_2018_paper.html": {
    "title": "Edit Probability for Scene Text Recognition",
    "volume": "main",
    "abstract": "We consider the scene text recognition problem under the attention-based encoder-decoder framework, which is the state of the art. The existing methods usually employ a frame-wise maximal likelihood loss to optimize the models. When we train the model, the misalignment between the ground truth strings and the attention's output sequences of probability distribution, which is caused by missing or superfluous characters, will confuse and mislead the training process, and consequently make the training costly and degrade the recognition accuracy. To handle this problem, we propose a novel method called edit probability (EP) for scene text recognition. EP tries to effectively estimate the probability of generating a string from the output sequence of probability distribution conditioned on the input image, while considering the possible occurrences of missing/superfluous characters. The advantage lies in that the training process can focus on the missing, superfluous and unrecognized characters, and thus the impact of the misalignment problem can be alleviated or even overcome. We conduct extensive experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets. Experimental results show that the EP can substantially boost scene text recognition performance",
    "checked": true,
    "id": "2a6a4cd7623d12b467571461e8c19a3138474908",
    "semantic_title": "edit probability for scene text recognition",
    "citation_count": 166,
    "authors": [
      "Fan Bai",
      "Zhanzhan Cheng",
      "Yi Niu",
      "Shiliang Pu",
      "Shuigeng Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Global_Versus_Localized_CVPR_2018_paper.html": {
    "title": "Global Versus Localized Generative Adversarial Nets",
    "volume": "main",
    "abstract": "In this paper, we present a novel localized Generative Adversarial Net (GAN) to learn on the manifold of real data. Compared with the classic GAN that {em globally} parameterizes a manifold, the Localized GAN (LGAN) uses local coordinate charts to parameterize distinct local geometry of how data points can transform at different locations on the manifold. Specifically, around each point there exists a {em local} generator that can produce data following diverse patterns of transformations on the manifold. The locality nature of LGAN enables local generators to adapt to and directly access the local geometry without need to invert the generator in a global GAN. Furthermore, it can prevent the manifold from being locally collapsed to a dimensionally deficient tangent subspace by imposing an orthonormality prior between tangents. This provides a geometric approach to alleviating mode collapse at least locally on the manifold by imposing independence between data transformations in different tangent directions. We will also demonstrate the LGAN can be applied to train a robust classifier that prefers locally consistent classification decisions on the manifold, and the resultant regularizer is closely related with the Laplace-Beltrami operator. Our experiments show that the proposed LGANs can not only produce diverse image transformations, but also deliver superior classification performances",
    "checked": true,
    "id": "4b3fb3d9a46062c7a463fe7612607ad16da07bbf",
    "semantic_title": "global versus localized generative adversarial nets",
    "citation_count": 80,
    "authors": [
      "Guo-Jun Qi",
      "Liheng Zhang",
      "Hao Hu",
      "Marzieh Edraki",
      "Jingdong Wang",
      "Xian-Sheng Hua"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.html": {
    "title": "MoCoGAN: Decomposing Motion and Content for Video Generation",
    "volume": "main",
    "abstract": "Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion. Our code is available at https://github.com/sergeytulyakov/mocogan",
    "checked": true,
    "id": "e76edb86f270c3a77ed9f5a1e1b305461f36f96f",
    "semantic_title": "mocogan: decomposing motion and content for video generation",
    "citation_count": 1147,
    "authors": [
      "Sergey Tulyakov",
      "Ming-Yu Liu",
      "Xiaodong Yang",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Recurrent_Residual_Module_CVPR_2018_paper.html": {
    "title": "Recurrent Residual Module for Fast Inference in Videos",
    "volume": "main",
    "abstract": "Deep convolutional neural networks (CNNs) have made impressive progress in many video recognition tasks such as video pose estimation and video object detection. However, running CNN inference on video requires numerous computation and is usually slow. In this work, we propose a framework called Recurrent Residual Module (RRM) to accelerate the CNN inference for video recognition tasks. This framework has a novel design of using the similarity of the intermediate feature maps of two consecutive frames to largely reduce the redundant computation. One unique property of the proposed method compared to previous work is that feature maps of each frame are precisely computed. The experiments show that, while maintaining the similar recognition performance, our RRM yields averagely 2× acceleration on the commonly used CNNs such as AlexNet, ResNet, deep compression model (thus 8−12× faster than the original dense models on the efﬁcient inference engine), and impressively 9× acceleration on some binary networks such as XNOR-Nets (thus 500× faster than the original model). We further verify the effectiveness of the RRM on speeding CNNs for video pose estimation and video object detection",
    "checked": true,
    "id": "a640bc0faef2c0ee44074bfd17e813d9b538bd17",
    "semantic_title": "recurrent residual module for fast inference in videos",
    "citation_count": 33,
    "authors": [
      "Bowen Pan",
      "Wuwei Lin",
      "Xiaolin Fang",
      "Chaoqin Huang",
      "Bolei Zhou",
      "Cewu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Honari_Improving_Landmark_Localization_CVPR_2018_paper.html": {
    "title": "Improving Landmark Localization With Semi-Supervised Learning",
    "volume": "main",
    "abstract": "We present two techniques to improve landmark localization in images from partially annotated datasets. Our primary goal is to leverage the common situation where precise landmark locations are only provided for a small data subset, but where class labels for classification or regression tasks related to the landmarks are more abundantly available. First, we propose the framework of sequential multitasking and explore it here through an architecture for landmark localization where training with class labels acts as an auxiliary signal to guide the landmark localization on unlabeled data. A key aspect of our approach is that errors can be backpropagated through a complete landmark localization model. Second, we propose and explore an unsupervised learning technique for landmark localization based on having a model predict equivariant landmarks with respect to transformations applied to the image. We show that these techniques, improve landmark prediction considerably and can learn effective detectors even when only a small fraction of the dataset has landmark labels. We present results on two toy datasets and four real datasets, with hands and faces, and report new state-of-the-art on two datasets in the wild, e.g. with only 5% of labeled images we outperform previous state-of-the-art trained on the AFLW dataset",
    "checked": true,
    "id": "dcd2ac544a8336d73e4d3d80b158477c783e1e50",
    "semantic_title": "improving landmark localization with semi-supervised learning",
    "citation_count": 162,
    "authors": [
      "Sina Honari",
      "Pavlo Molchanov",
      "Stephen Tyree",
      "Pascal Vincent",
      "Christopher Pal",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pal_Adversarial_Data_Programming_CVPR_2018_paper.html": {
    "title": "Adversarial Data Programming: Using GANs to Relax the Bottleneck of Curated Labeled Data",
    "volume": "main",
    "abstract": "Paucity of large curated hand labeled training data forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label, given a set of weak labeling functions. We validated our method on the MNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many state-of-the-art models. We conducted extensive experiments to study its usefulness, as well as showed how the proposed ADP framework can be used for transfer learning as well as multitask learning, where data from two domains are generated simultaneously using the framework along with the label information. Our future work will involve understanding the theoretical implications of this new framework from a game-theoretic perspective, as well as explore the performance of the method on more complex datasets",
    "checked": true,
    "id": "07497ddf60a409f89fe5ee323c3049a23e5c49ed",
    "semantic_title": "adversarial data programming: using gans to relax the bottleneck of curated labeled data",
    "citation_count": 15,
    "authors": [
      "Arghya Pal",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Plotz_Stochastic_Variational_Inference_CVPR_2018_paper.html": {
    "title": "Stochastic Variational Inference With Gradient Linearization",
    "volume": "main",
    "abstract": "Variational inference has experienced a recent surge in popularity owing to stochastic approaches, which have yielded practical tools for a wide range of model classes. A key benefit is that stochastic variational inference obviates the tedious process of deriving analytical expressions for closed-form variable updates. Instead, one simply needs to derive the gradient of the log-posterior, which is often much easier. Yet for certain model classes, the log-posterior itself is difficult to optimize using standard gradient techniques. One such example are random field models, where optimization based on gradient linearization has proven popular, since it speeds up convergence significantly and can avoid poor local optima. In this paper we propose stochastic variational inference with gradient linearization (SVIGL). It is similarly convenient as standard stochastic variational inference - all that is required is a local linearization of the energy gradient. Its benefit over stochastic variational inference with conventional gradient methods is a clear improvement in convergence speed, while yielding comparable or even better variational approximations in terms of KL divergence. We demonstrate the benefits of SVIGL in three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction",
    "checked": true,
    "id": "5dc01086acedcf96a94a20a33e9928c38bda679b",
    "semantic_title": "stochastic variational inference with gradient linearization",
    "citation_count": 2,
    "authors": [
      "Tobias Plötz",
      "Anne S. Wannenwetsch",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Multi-Label_Zero-Shot_Learning_CVPR_2018_paper.html": {
    "title": "Multi-Label Zero-Shot Learning With Structured Knowledge Graphs",
    "volume": "main",
    "abstract": "In this paper, we propose a novel deep learning architecture for multi-label zero-shot learning (ML-ZSL), which is able to predict multiple unseen class labels for each input instance. Inspired by the way humans utilize semantic knowledge between objects of interests, we propose a framework that incorporates knowledge graphs for describing the relationships between multiple labels. Our model learns an information propagation mechanism from the semantic label space, which can be applied to model the interdependencies between seen and unseen class labels. With such investigation of structured knowledge graphs for visual reasoning, we show that our model can be applied for solving multi-label classification and ML-ZSL tasks. Compared to state-of-the-art approaches, comparable or improved performances can be achieved by our method",
    "checked": true,
    "id": "17a8dd804ff254662cea2450c25dfa0bb7adceb2",
    "semantic_title": "multi-label zero-shot learning with structured knowledge graphs",
    "citation_count": 280,
    "authors": [
      "Chung-Wei Lee",
      "Wei Fang",
      "Chih-Kuan Yeh",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gordon_MorphNet_Fast__CVPR_2018_paper.html": {
    "title": "MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks",
    "volume": "main",
    "abstract": "We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint",
    "checked": true,
    "id": "e60f693cb12132c7fffc34dc141bcc3c9dfd4961",
    "semantic_title": "morphnet: fast & simple resource-constrained structure learning of deep networks",
    "citation_count": 337,
    "authors": [
      "Ariel Gordon",
      "Elad Eban",
      "Ofir Nachum",
      "Bo Chen",
      "Hao Wu",
      "Tien-Ju Yang",
      "Edward Choi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Deep_Adversarial_Subspace_CVPR_2018_paper.html": {
    "title": "Deep Adversarial Subspace Clustering",
    "volume": "main",
    "abstract": "Most existing subspace clustering methods hinge on self-expression of handcrafted representations and are unaware of potential clustering errors. Thus they perform unsatisfactorily on real data with complex underlying subspaces. To solve this issue, we propose a novel deep adversarial subspace clustering (DASC) model, which learns more favorable sample representations by deep learning for subspace clustering, and more importantly introduces adversarial learning to supervise sample representation learning and subspace clustering. Specifically, DASC consists of a subspace clustering generator and a quality-verifying discriminator, which learn against each other. The generator produces subspace estimation and sample clustering. The discriminator evaluates current clustering performance by inspecting whether the re-sampled data from estimated subspaces have consistent subspace properties, and supervises the generator to progressively improve subspace clustering. Experimental results on the handwritten recognition, face and object clustering tasks demonstrate the advantages of DASC over shallow and few deep subspace clustering models. Moreover, to our best knowledge, this is the first successful application of GAN-alike model for unsupervised subspace clustering, which also paves the way for deep learning to solve other unsupervised learning problems",
    "checked": true,
    "id": "5ecaea6d7b6fbc26308701d9f862353d17fb57d1",
    "semantic_title": "deep adversarial subspace clustering",
    "citation_count": 165,
    "authors": [
      "Pan Zhou",
      "Yunqing Hou",
      "Jiashi Feng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Towards_Human-Machine_Cooperation_CVPR_2018_paper.html": {
    "title": "Towards Human-Machine Cooperation: Self-Supervised Sample Mining for Object Detection",
    "volume": "main",
    "abstract": "Though quite challenging, leveraging large-scale unlabeled or partially labeled images in a cost-effective way has increasingly attracted interests for its great importance to computer vision. To tackle this problem, many Active Learning (AL) methods have been developed. However, these methods mainly define their sample selection criteria within a single image context, leading to the suboptimal robustness and impractical solution for large-scale object detection. In this paper, aiming to remedy the drawbacks of existing AL methods, we present a principled Self-supervised Sample Mining (SSM) process accounting for the real challenges in object detection. Specifically, our SSM process concentrates on automatically discovering and pseudo-labeling reliable region proposals for enhancing the object detector via the introduced cross image validation, i.e., pasting these proposals into different labeled images to comprehensively measure their values under different image contexts. By resorting to the SSM process, we propose a new AL framework for gradually incorporating unlabeled or partially labeled data into the model learning while minimizing the annotating effort of users. Extensive experiments on two public benchmarks clearly demonstrate our proposed framework can achieve the comparable performance to the state-of-the-art methods with significantly fewer annotations",
    "checked": true,
    "id": "ad4490c122358cb30ad062358c44a2736b39cbe4",
    "semantic_title": "towards human-machine cooperation: self-supervised sample mining for object detection",
    "citation_count": 101,
    "authors": [
      "Keze Wang",
      "Xiaopeng Yan",
      "Dongyu Zhang",
      "Lei Zhang",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Laude_Discrete-Continuous_ADMM_for_CVPR_2018_paper.html": {
    "title": "Discrete-Continuous ADMM for Transductive Inference in Higher-Order MRFs",
    "volume": "main",
    "abstract": "This paper introduces a novel algorithm for transductive inference in higher-order MRFs, where the unary energies are parameterized by a variable classifier. The considered task is posed as a joint optimization problem in the continuous classifier parameters and the discrete label variables. In contrast to prior approaches such as convex relaxations, we propose an advantageous decoupling of the objective function into discrete and continuous subproblems and a novel, efficient optimization method related to ADMM. This approach preserves integrality of the discrete label variables and guarantees global convergence to a critical point. We demonstrate the advantages of our approach in several experiments including video object segmentation on the DAVIS data set and interactive image segmentation",
    "checked": true,
    "id": "59a0a18a204d1e89ca29aff54cbc732b4d5bb5b5",
    "semantic_title": "discrete-continuous admm for transductive inference in higher-order mrfs",
    "citation_count": 7,
    "authors": [
      "Emanuel Laude",
      "Jan-Hendrik Lange",
      "Jonas Schüpfer",
      "Csaba Domokos",
      "Laura Leal-Taixé",
      "Frank R. Schmidt",
      "Bjoern Andres",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper.html": {
    "title": "Robust Physical-World Attacks on Deep Learning Visual Classification",
    "volume": "main",
    "abstract": "Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations. Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm, Robust Physical Perturbations (RP 2 ), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP 2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. With a perturbation in the form of only black and white stickers, we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8% of the captured video frames obtained on a moving vehicle (field test) for the target classifier",
    "checked": true,
    "id": "f0c5991dbb130fa6b5de011cf7a04f6ed815ef68",
    "semantic_title": "robust physical-world attacks on deep learning visual classification",
    "citation_count": 1859,
    "authors": [
      "Kevin Eykholt",
      "Ivan Evtimov",
      "Earlence Fernandes",
      "Bo Li",
      "Amir Rahmati",
      "Chaowei Xiao",
      "Atul Prakash",
      "Tadayoshi Kohno",
      "Dawn Song"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Joo_Generating_a_Fusion_CVPR_2018_paper.html": {
    "title": "Generating a Fusion Image: One's Identity and Another's Shape",
    "volume": "main",
    "abstract": "Generating a novel image by manipulating two input images is an interesting research problem in the study of generative adversarial networks (GANs). We propose a new GAN-based network that generates a fusion image with the identity of input image x and the shape of input image y. Our network can simultaneously train on more than two image datasets in an unsupervised manner. We define an identity loss LI to catch the identity of image x and a shape loss LS to get the shape of y. In addition, we propose a novel training method called Min-Patch training to focus the generator on crucial parts of an image, rather than its entirety. We show qualitative results on the VGG Youtube Pose dataset , Eye dataset (MPIIGaze and UnityEyes), and the Photo–Sketch–Cartoon dataset",
    "checked": true,
    "id": "b480807e11bb4e2bb7ab6d70fefd03ab39901cf1",
    "semantic_title": "generating a fusion image: one's identity and another's shape",
    "citation_count": 35,
    "authors": [
      "DongGyu Joo",
      "Doyeon Kim",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zeng_Learning_to_Promote_CVPR_2018_paper.html": {
    "title": "Learning to Promote Saliency Detectors",
    "volume": "main",
    "abstract": "The categories and appearance of salient objects vary from image to image, therefore, saliency detection is an image-specific task. Due to lack of large-scale saliency training data, using deep neural networks (DNNs) with pre-training is difficult to precisely capture the image-specific saliency cues. To solve this issue, we formulate a zero-shot learning problem to promote existing saliency detectors. Concretely, a DNN is trained as an embedding function to map pixels and the attributes of the salient/background regions of an image into the same metric space, in which an image-specific classifier is learned to classify the pixels. Since the image-specific task is performed by the classifier, the DNN embedding effectively plays the role of a general feature extractor. Compared with transferring the learning to a new recognition task using limited data, this formulation makes the DNN learn more effectively from small data. Extensive experiments on five data sets show that our method significantly improves accuracy of existing methods and compares favorably against state-of-the-art approaches",
    "checked": true,
    "id": "fed8d4ace4d062dac9f1d72b6a7638e8734023d3",
    "semantic_title": "learning to promote saliency detectors",
    "citation_count": 64,
    "authors": [
      "Yu Zeng",
      "Huchuan Lu",
      "Lihe Zhang",
      "Mengyang Feng",
      "Ali Borji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Han_Image_Super-Resolution_via_CVPR_2018_paper.html": {
    "title": "Image Super-Resolution via Dual-State Recurrent Networks",
    "volume": "main",
    "abstract": "Advances in image super-resolution (SR) have recently benefited significantly from rapid developments in deep neural networks. Inspired by these recent discoveries, we note that many state-of-the-art deep SR architectures can be reformulated as a single-state recurrent neural network (RNN) with finite unfoldings. In this paper, we explore new structures for SR based on this compact RNN view, leading us to a dual-state design, the Dual-State Recurrent Network (DSRN). Compared to its single-state counterparts that op- erate at a fixed spatial resolution, DSRN exploits both low- resolution (LR) and high-resolution (HR) signals jointly. Recurrent signals are exchanged between these states in both directions (both LR to HR and HR to LR) via de- layed feedback. Extensive quantitative and qualitative eval- uations on benchmark datasets and on a recent challenge demonstrate that the proposed DSRN performs favorably against state-of-the-art algorithms in terms of both mem- ory consumption and predictive accuracy",
    "checked": true,
    "id": "175f74a09241b6cb5101a2a09978095720db7d5f",
    "semantic_title": "image super-resolution via dual-state recurrent networks",
    "citation_count": 215,
    "authors": [
      "Wei Han",
      "Shiyu Chang",
      "Ding Liu",
      "Mo Yu",
      "Michael Witbrock",
      "Thomas S. Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Haris_Deep_Back-Projection_Networks_CVPR_2018_paper.html": {
    "title": "Deep Back-Projection Networks for Super-Resolution",
    "volume": "main",
    "abstract": "The feed-forward architectures of recently proposed deep super-resolution networks learn representations of low-resolution inputs, and the non-linear mapping from those to high-resolution output. However, this approach does not fully address the mutual dependencies of low- and high-resolution images. We propose Deep Back-Projection Networks (DBPN), that exploit iterative up- and down-sampling layers, providing an error feedback mechanism for projection errors at each stage. We construct mutually-connected up- and down-sampling stages each of which represents different types of image degradation and high-resolution components. We show that extending this idea to allow concatenation of features across up- and down-sampling stages (Dense DBPN) allows us to reconstruct further improve super-resolution, yielding superior results and in particular establishing new state of the art results for large scaling factors such as 8x across multiple data sets",
    "checked": true,
    "id": "3afa826a594d90ee0f4fe062c988289bb213a114",
    "semantic_title": "deep back-projection networks for super-resolution",
    "citation_count": 1329,
    "authors": [
      "Muhammad Haris",
      "Gregory Shakhnarovich",
      "Norimichi Ukita"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Focus_Manipulation_Detection_CVPR_2018_paper.html": {
    "title": "Focus Manipulation Detection via Photometric Histogram Analysis",
    "volume": "main",
    "abstract": "With the rise of misinformation spread via social media channels, enabled by the increasing automation and realism of image manipulation tools, image forensics is an increasingly relevant problem. Classic image forensic methods leverage low-level cues such as metadata, sensor noise fingerprints, and others that are easily fooled when the image is re-encoded upon upload to facebook, etc. This necessitates the use of higher-level physical and semantic cues that, once hard to estimate reliably in the wild, have become more effective due to the increasing power of computer vision. In particular, we detect manipulations introduced by artificial blurring of the image, which creates inconsistent photometric relationships between image intensity and various cues. We achieve 98% accuracy on the most challenging cases in a new dataset of blur manipulations, where the blur is geometrically correct and consistent with the scene's physical arrangement. Such manipulations are now easily generated, for instance, by smartphone cameras having hardware to measure depth, e.g. `Portrait Mode' of the iPhone7Plus. We also demonstrate good performance on a challenge dataset evaluating a wider range of manipulations in imagery representing `in the wild' conditions",
    "checked": true,
    "id": "da7dd2b93395cda37989803710cffbc559b4aee3",
    "semantic_title": "focus manipulation detection via photometric histogram analysis",
    "citation_count": 9,
    "authors": [
      "Can Chen",
      "Scott McCloskey",
      "Jingyi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html": {
    "title": "Compassionately Conservative Balanced Cuts for Image Segmentation",
    "volume": "main",
    "abstract": "The Normalized Cut (NCut) objective function, widely used in data clustering and image segmentation, quantifies the cost of graph partitioning in a way that biases clusters or segments that are balanced towards having lower values than unbalanced partitionings. However, this bias is so strong that it avoids any singleton partitions, even when vertices are very weakly connected to the rest of the graph. Motivated by the Buehler-Hein family of balanced cut costs, we propose the family of Compassionately Conservative Balanced (CCB) Cut costs, which are indexed by a parameter that can be used to strike a compromise between the desire to avoid too many singleton partitions and the notion that all partitions should be balanced. We show that CCB-Cut minimization can be relaxed into an orthogonally constrained $ell_{ au}$-minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value, and we present an algorithm for solving the relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Using images from the BSDS500 database, we show that image segmentation based on CCB-Cut minimization provides better accuracy with respect to ground truth and greater variability in region size than NCut-based image segmentation",
    "checked": true,
    "id": "605b4d870a84b3dc5ba457aed23e3a05f4054149",
    "semantic_title": "compassionately conservative balanced cuts for image segmentation",
    "citation_count": 3,
    "authors": [
      "Nathan D. Cahill",
      "Tyler L. Hayes",
      "Renee T. Meinhold",
      "John F. Hamilton"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Abdelhamed_A_High-Quality_Denoising_CVPR_2018_paper.html": {
    "title": "A High-Quality Denoising Dataset for Smartphone Cameras",
    "volume": "main",
    "abstract": "The last decade has seen an astronomical shift from imaging with DSLR and point-and-shoot cameras to imaging with smartphone cameras. Due to the small aperture and sensor size, smartphone images have notably more noise than their DSLR counterparts. While denoising for smartphone images is an active research area, the research community currently lacks a denoising image dataset representative of real noisy images from smartphone cameras with high-quality ground truth. We address this issue in this paper with the following contributions. We propose a systematic procedure for estimating ground truth for noisy images that can be used to benchmark denoising performance for smartphone cameras. Using this procedure, we have captured a dataset, the Smartphone Image Denoising Dataset (SIDD), of ~30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras and generated their ground truth images. We used this dataset to benchmark a number of denoising algorithms. We show that CNN-based methods perform better when trained on our high-quality dataset than when trained using alternative strategies, such as low-ISO images used as a proxy for ground truth data",
    "checked": true,
    "id": "ebf35073e122782f685a0d6c231622412f28a53b",
    "semantic_title": "a high-quality denoising dataset for smartphone cameras",
    "citation_count": 757,
    "authors": [
      "Abdelrahman Abdelhamed",
      "Stephen Lin",
      "Michael S. Brown"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Niklaus_Context-Aware_Synthesis_for_CVPR_2018_paper.html": {
    "title": "Context-Aware Synthesis for Video Frame Interpolation",
    "volume": "main",
    "abstract": "Video frame interpolation algorithms typically estimate optical flow or its variations and then use it to guide the synthesis of an intermediate frame between two consecutive original frames. To handle challenges like occlusion, bidirectional flow between the two input frames is often estimated and used to warp and blend the input frames. However, how to effectively blend the two warped frames still remains a challenging problem. This paper presents a context-aware synthesis approach that warps not only the input frames but also their pixel-wise contextual information and uses them to interpolate a high-quality intermediate frame. Specifically, we first use a pre-trained neural network to extract per-pixel contextual information for input frames. We then employ a state-of-the-art optical flow algorithm to estimate bidirectional flow between them and pre-warp both input frames and their context maps. Finally, unlike common approaches that blend the pre-warped frames, our method feeds them and their context maps to a video frame synthesis neural network to produce the interpolated frame in a context-aware fashion. Our neural network is fully convolutional and is trained end to end. Our experiments show that our method can handle challenging scenarios such as occlusion and large motion and outperforms representative state-of-the-art approaches",
    "checked": true,
    "id": "65fadccad0fc743876a259c2b779622636c2ffde",
    "semantic_title": "context-aware synthesis for video frame interpolation",
    "citation_count": 410,
    "authors": [
      "Simon Niklaus",
      "Feng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Salient_Object_Detection_CVPR_2018_paper.html": {
    "title": "Salient Object Detection Driven by Fixation Prediction",
    "volume": "main",
    "abstract": "Research in visual saliency has been focused on two major types of models namely fixation prediction and salient object detection. The relationship between the two, however, has been less explored. In this paper, we propose to employ the former model type to identify and segment salient objects in scenes. We build a novel neural network called Attentive Saliency Network (ASNet) that learns to detect salient objects from fixation maps. The fixation map, derived at the upper network layers, captures a high-level understanding of the scene. Salient object detection is then viewed as fine-grained object-level saliency segmentation and is progressively optimized with the guidance of the fixation map in a top-down manner. ASNet is based on a hierarchy of convolutional LSTMs (convLSTMs) that offers an efficient recurrent mechanism for sequential refinement of the segmentation map. Several loss functions are introduced for boosting the performance of the ASNet. Extensive experimental evaluation shows that our proposed ASNet is capable of generating accurate segmentation maps with the help of the computed fixation map. Our work offers a deeper insight into the mechanisms of attention and narrows the gap between salient object detection and fixation prediction",
    "checked": true,
    "id": "41aa48241071f8fa15145c3452679aa91c13459e",
    "semantic_title": "salient object detection driven by fixation prediction",
    "citation_count": 177,
    "authors": [
      "Wenguan Wang",
      "Jianbing Shen",
      "Xingping Dong",
      "Ali Borji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jeon_Enhancing_the_Spatial_CVPR_2018_paper.html": {
    "title": "Enhancing the Spatial Resolution of Stereo Images Using a Parallax Prior",
    "volume": "main",
    "abstract": "We present a novel method that can enhance the spatial resolution of stereo images using a parallax prior. While traditional stereo imaging has focused on estimating depth from stereo images, our method utilizes stereo images to enhance spatial resolution instead of estimating disparity. The critical challenge for enhancing spatial resolution from stereo images: how to register corresponding pixels with subpixel accuracy. Since disparity in traditional stereo imaging is calculated per pixel, it is directly inappropriate for enhancing spatial resolution. We, therefore, learn a parallax prior from stereo image datasets by jointly training two-stage networks. The first network learns how to enhance the spatial resolution of stereo images in luminance, and the second network learns how to reconstruct a high-resolution color image from high-resolution luminance and chrominance of the input image. Our two-stage joint network enhances the spatial resolution of stereo images significantly more than single-image super-resolution methods. The proposed method is directly applicable to any stereo depth imaging methods, enabling us to enhance the spatial resolution of stereo images",
    "checked": true,
    "id": "f773e057c6702f24612e03ae529c76875f60a9dd",
    "semantic_title": "enhancing the spatial resolution of stereo images using a parallax prior",
    "citation_count": 122,
    "authors": [
      "Daniel S. Jeon",
      "Seung-Hwan Baek",
      "Inchang Choi",
      "Min H. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sironi_HATS_Histograms_of_CVPR_2018_paper.html": {
    "title": "HATS: Histograms of Averaged Time Surfaces for Robust Event-Based Object Classification",
    "volume": "main",
    "abstract": "Event-based cameras have recently drawn the attention of the Computer Vision community thanks to their advantages in terms of high temporal resolution, low power consumption and high dynamic range, compared to traditional frame-based cameras. These properties make event-based cameras an ideal choice for autonomous vehicles, robot navigation or UAV vision, among others. However, the accuracy of event-based object classification algorithms, which is of crucial importance for any reliable system working in real-world conditions, is still far behind their frame-based counterparts. Two main reasons for this performance gap are: 1. The lack of effective low-level representations and architectures for event-based object classification and 2. The absence of large real-world event-based datasets. In this paper we address both problems. First, we introduce a novel event-based feature representation together with a new machine learning architecture. Compared to previous approaches, we use local memory units to efficiently leverage past temporal information and build a robust event-based representation. Second, we release the first large real-world event-based dataset for object classification. We compare our method to the state-of-the-art with extensive experiments, showing better classification performance and real-time computation",
    "checked": true,
    "id": "a60f20adc4fc9f363465de327dc2f3f1ab7b624c",
    "semantic_title": "hats: histograms of averaged time surfaces for robust event-based object classification",
    "citation_count": 446,
    "authors": [
      "Amos Sironi",
      "Manuele Brambilla",
      "Nicolas Bourdis",
      "Xavier Lagorce",
      "Ryad Benosman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_A_Bi-Directional_Message_CVPR_2018_paper.html": {
    "title": "A Bi-Directional Message Passing Model for Salient Object Detection",
    "volume": "main",
    "abstract": "Recent progress on salient object detection is beneficial from Fully Convolutional Neural Network (FCN). The saliency cues contained in multi-level convolutional features are complementary for detecting salient objects. How to integrate multi-level features becomes an open problem in saliency detection. In this paper, we propose a novel bi-directional message passing model to integrate multi-level features for salient object detection. At first, we adopt a Multi-scale Context-aware Feature Extraction Module (MCFEM) for multi-level feature maps to capture rich context information. Then a bi-directional structure is designed to pass messages between multi-level features, and a gate function is exploited to control the message passing rate. We use the features after message passing, which simultaneously encode semantic information and spatial details, to predict saliency maps. Finally, the predicted results are efficiently combined to generate the final saliency map. Quantitative and qualitative experiments on five benchmark datasets demonstrate that our proposed model performs favorably against the state-of-the-art methods under different evaluation metrics",
    "checked": true,
    "id": "5d30c05436f5b101694ac7815b2dbec1a725d5f9",
    "semantic_title": "a bi-directional message passing model for salient object detection",
    "citation_count": 421,
    "authors": [
      "Lu Zhang",
      "Ju Dai",
      "Huchuan Lu",
      "You He",
      "Gang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kat_Matching_Pixels_Using_CVPR_2018_paper.html": {
    "title": "Matching Pixels Using Co-Occurrence Statistics",
    "volume": "main",
    "abstract": "We propose a new error measure for matching pixels that is based on co-occurrence statistics. The measure relies on a co-occurrence matrix that counts the number of times pairs of pixel values co-occur within a window. The error incurred by matching a pair of pixels is inverse proportional to the probability that their values co-occur together, and not their color difference. This measure also works with features other than color, e.g. deep features. We show that this improves the state-of-the-art performance of template matching on standard benchmarks. We then propose an embedding scheme that maps the input image to an embedded image such that the Euclidean distance between pixel values in the embedded space resembles the co-occurrence statistics in the original space. This lets us run existing vision algorithms on the embedded images and enjoy the power of co-occurrence statistics for free. We demonstrate this on two algorithms, the Lucas-Kanade image registration and the Kernelized Correlation Filter (KCF) tracker. Experiments show that performance of each algorithm improves by about 10%",
    "checked": true,
    "id": "a696405891b117d869ec5e19fd98ccea8f470321",
    "semantic_title": "matching pixels using co-occurrence statistics",
    "citation_count": 37,
    "authors": [
      "Rotal Kat",
      "Roy Jevnisek",
      "Shai Avidan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Song_SeedNet_Automatic_Seed_CVPR_2018_paper.html": {
    "title": "SeedNet: Automatic Seed Generation With Deep Reinforcement Learning for Robust Interactive Segmentation",
    "volume": "main",
    "abstract": "In this paper, we propose an automatic seed generation technique with deep reinforcement learning to solve the interactive segmentation problem. One of the main issues of the interactive segmentation problem is robust and consistent object extraction with less human effort. Most of the existing algorithms highly depend on the distribution of inputs, which differs from one user to another and hence need sequential user interactions to achieve adequate performance. In our system, when a user first specifies a point on the desired object and a point in the background, a sequence of artificial user input is automatically generated for precisely segmenting the desired object. The proposed system allows the user to reduce the number of input significantly. This problem is difficult to cast as a supervised learning problem because it is not possible to define globally optimal user input at some stage of the interactive segmentation task. Hence, we formulate automatic seed generation problem as Markov Decision Process (MDP) and then optimize it by reinforcement learning with Deep Q-Network (DQN). We train our network on the MSRA10K dataset and show that the network achieves notable performance improvement from inaccurate initial segmentation on both seen and unseen datasets",
    "checked": true,
    "id": "e41a1c8165e84d576b34e06b7941f0dfde479cf7",
    "semantic_title": "seednet: automatic seed generation with deep reinforcement learning for robust interactive segmentation",
    "citation_count": 65,
    "authors": [
      "Gwangmo Song",
      "Heesoo Myeong",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Takeda_Jerk-Aware_Video_Acceleration_CVPR_2018_paper.html": {
    "title": "Jerk-Aware Video Acceleration Magnification",
    "volume": "main",
    "abstract": "Video magnification reveals subtle changes invisible to the naked eye, but such tiny yet meaningful changes are often hidden under large motions: small deformation of the muscles in doing sports, or tiny vibrations of strings in ukulele playing. For magnifying subtle changes under large motions, video acceleration magnification method has recently been proposed. This method magnifies subtle acceleration changes and ignores slow large motions. However, quick large motions severely distort this method. In this paper, we present a novel use of jerk to make the acceleration method robust to quick large motions. Jerk has been used to assess smoothness of time series data in the neuroscience and mechanical engineering fields. On the basis of our observation that subtle changes are smoother than quick large motions at temporal scale, we used jerk-based smoothness to design a jerk-aware filter that passes subtle changes only under quick large motions. By applying our filter to the acceleration method, we obtain impressive magnification results better than those obtained with state-of-the-art",
    "checked": true,
    "id": "8e507a9dc15957f61f038b7cfce681996d0b4413",
    "semantic_title": "jerk-aware video acceleration magnification",
    "citation_count": 29,
    "authors": [
      "Shoichiro Takeda",
      "Kazuki Okami",
      "Dan Mikami",
      "Megumi Isogai",
      "Hideaki Kimata"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Defense_Against_Adversarial_CVPR_2018_paper.html": {
    "title": "Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser",
    "volume": "main",
    "abstract": "Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose high-level representation guided denoiser (HGD) as a defense for image classification. Standard denoiser suffers from the error amplification effect, in which small residual adversarial noise is progressively amplified and leads to wrong classifications. HGD overcomes this problem by using a loss function defined as the difference between the target model's outputs activated by the clean image and denoised image. Compared with ensemble adversarial training which is the state-of-the-art defending method on large images, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either white-box or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images and unseen classes. Third, HGD can be transferred to defend models other than the one guiding it. In NIPS competition on defense against adversarial attacks, our HGD solution won the first place and outperformed other models by a large margin. footnote{Code: url{https://github.com/lfz/Guided-Denoise}.}",
    "checked": true,
    "id": "ca9c1224636b0a7dd37340a4691c34a9914b5af8",
    "semantic_title": "defense against adversarial attacks using high-level representation guided denoiser",
    "citation_count": 886,
    "authors": [
      "Fangzhou Liao",
      "Ming Liang",
      "Yinpeng Dong",
      "Tianyu Pang",
      "Xiaolin Hu",
      "Jun Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.html": {
    "title": "Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal",
    "volume": "main",
    "abstract": "Understanding shadows from a single image consists of two types of task in previous studies, containing shadow detection and shadow removal. In this paper, we present a multi-task perspective, which is not embraced by any existing work, to jointly learn both detection and removal in an end-to-end fashion that aims at enjoying the mutually improved benefits from each other. Our framework is based on a novel STacked Conditional Generative Adversarial Network (ST-CGAN), which is composed of two stacked CGANs, each with a generator and a discriminator. Specifically, a shadow image is fed into the first generator which produces a shadow detection mask. That shadow image, concatenated with its predicted mask, goes through the second generator in order to recover its shadow-free image consequently. In addition, the two corresponding discriminators are very likely to model higher level relationships and global scene characteristics for the detected shadow region and reconstruction via removing shadows, respectively. More importantly, for multi-task learning, our design of stacked paradigm provides a novel view which is notably different from the commonly used one as the multi-branch version. To fully evaluate the performance of our proposed framework, we construct the first large-scale benchmark with 1870 image triplets (shadow image, shadow mask image, and shadow-free image) under 135 scenes. Extensive experimental results consistently show the advantages of ST-CGAN over several representative state-of-the-art methods on two large-scale publicly available datasets and our newly released one",
    "checked": true,
    "id": "7ae613ab4925975be47c6d5418a5153b73b4ba55",
    "semantic_title": "stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal",
    "citation_count": 403,
    "authors": [
      "Jifeng Wang",
      "Xiang Li",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Image_Correction_via_CVPR_2018_paper.html": {
    "title": "Image Correction via Deep Reciprocating HDR Transformation",
    "volume": "main",
    "abstract": "Image correction aims to adjust an input image into a visually pleasing one with the detail in the under/over exposed regions recovered. However, existing image correction methods are mainly based on image pixel operations, and attempting to recover the lost detail from these under/over exposed regions is challenging. We, therefore, revisit the image formation procedure and notice that detail is contained in the high dynamic range (HDR) light intensities(which can be perceived by human eyes) but is lost during the nonlinear imaging process by of the camera in the low dynamic range (LDR) domain. Inspired by this observation, we formulate the image correction problem as the Deep Reciprocating HDR Transformation (DRHT) process and propose a novel approach to first reconstruct the lost detail in the HDR domain and then transfer them back to the LDR image as the output image with the recovered detail preserved. To this end, we propose an end-to-end DRHT model, which contains two CNNs, one for HDR detail reconstruction and the other for LDR detail correction. Experiments on the standard benchmarks demonstrate the effectiveness of the proposed method, compared with state-of-the-art image correction methods",
    "checked": true,
    "id": "89f548f6b654ef001b3045b9882b6114d5948e2b",
    "semantic_title": "image correction via deep reciprocating hdr transformation",
    "citation_count": 108,
    "authors": [
      "Xin Yang",
      "Ke Xu",
      "Yibing Song",
      "Qiang Zhang",
      "Xiaopeng Wei",
      "Rynson W.H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper.html": {
    "title": "PieAPP: Perceptual Image-Error Assessment Through Pairwise Preference",
    "volume": "main",
    "abstract": "The ability to estimate the perceptual error between images is an important problem in computer vision with many applications. Although it has been studied extensively, however, no method currently exists that can robustly predict visual differences like humans. Some previous approaches used hand-coded models, but they fail to model the complexity of the human visual system. Others used machine learning to train models on human-labeled datasets, but creating large, high-quality datasets is difficult because people are unable to assign consistent error labels to distorted images. In this paper, we present a new learning-based method that is the first to predict perceptual image error like human observers. Since it is much easier for people to compare two given images and identify the one more similar to a reference than to assign quality scores to each, we propose a new, large-scale dataset labeled with the probability that humans will prefer one image over another. We then train a deep-learning model using a novel, pairwise-learning framework to predict the preference of one distorted image over the other. Our key observation is that our trained network can then be used separately with only one distorted image and a reference to predict its perceptual error, without ever being trained on explicit human perceptual-error labels. The perceptual error estimated by our new metric, PieAPP, is well-correlated with human opinion. Furthermore, it significantly outperforms existing algorithms, beating the state-of-the-art by almost 3x on our test set in terms of binary error rate, while also generalizing to new kinds of distortions, unlike previous learning-based methods",
    "checked": true,
    "id": "a41175002926ccc7754c0fb3c989ff38846434f1",
    "semantic_title": "pieapp: perceptual image-error assessment through pairwise preference",
    "citation_count": 276,
    "authors": [
      "Ekta Prashnani",
      "Hong Cai",
      "Yasamin Mostofi",
      "Pradeep Sen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Normalized_Cut_Loss_CVPR_2018_paper.html": {
    "title": "Normalized Cut Loss for Weakly-Supervised CNN Segmentation",
    "volume": "main",
    "abstract": "Most recent semantic segmentation methods train deep convolutional neural networks with fully annotated masks requiring pixel-accuracy for good quality training. Common weakly-supervised approaches generate full masks from partial input (e.g. scribbles or seeds) using standard interactive segmentation methods as preprocessing. But, errors in such masks result in poorer training since standard loss functions (e.g. cross-entropy) do not distinguish seeds from potentially mislabeled other pixels. Inspired by the general ideas in semi-supervised learning, we address these problems via a new principled loss function evaluating network output with criteria standard in ``shallow'' segmentation, e.g. normalized cut. Unlike prior work, the cross entropy part of our loss evaluates only seeds where labels are known while normalized cut softly evaluates consistency of all pixels. We focus on normalized cut loss where dense Gaussian kernel is efficiently implemented in linear time by fast Bilateral filtering. Our normalized cut loss approach to segmentation brings the quality of weakly-supervised training significantly closer to fully supervised methods",
    "checked": true,
    "id": "9831dc24bba0aaaf32218989a5259d9110437950",
    "semantic_title": "normalized cut loss for weakly-supervised cnn segmentation",
    "citation_count": 318,
    "authors": [
      "Meng Tang",
      "Abdelaziz Djelouah",
      "Federico Perazzi",
      "Yuri Boykov",
      "Christopher Schroers"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ISTA-Net_Interpretable_Optimization-Inspired_CVPR_2018_paper.html": {
    "title": "ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing",
    "volume": "main",
    "abstract": "With the aim of developing a fast yet accurate algorithm for compressive sensing (CS) reconstruction of natural images, we combine in this paper the merits of two existing categories of CS methods: the structure insights of traditional optimization-based methods and the performance/speed of recent network-based ones. Specifically, we propose a novel structured deep network, dubbed ISTA-Net, which is inspired by the Iterative Shrinkage-Thresholding Algorithm (ISTA) for optimizing a general L1 norm CS reconstruction model. To cast ISTA into deep network form, we develop an effective strategy to solve the proximal mapping associated with the sparsity-inducing regularizer using nonlinear transforms. All the parameters in ISTA-Net (e.g. nonlinear transforms, shrinkage thresholds, step sizes, etc.) are learned end-to-end, rather than being hand-crafted. Moreover, considering that the residuals of natural images are more compressible, an enhanced version of ISTA-Net in the residual domain, dubbed ISTA-Net+, is derived to further improve CS reconstruction. Extensive CS experiments demonstrate that the proposed ISTA-Nets outperform existing state-of-the-art optimization-based and network-based CS methods by large margins, while maintaining fast computational speed",
    "checked": true,
    "id": "2cdc9bbde0f847094582b212f980fa4dbc48950d",
    "semantic_title": "ista-net: interpretable optimization-inspired deep network for image compressive sensing",
    "citation_count": 1043,
    "authors": [
      "Jian Zhang",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Fast_End-to-End_Trainable_CVPR_2018_paper.html": {
    "title": "Fast End-to-End Trainable Guided Filter",
    "volume": "main",
    "abstract": "Image processing and pixel-wise dense prediction have been advanced by harnessing the capabilities of deep learning. One central issue of deep learning is the limited capacity to handle joint upsampling. We present a deep learning building block for joint upsampling, namely guided filtering layer. This layer aims at efficiently generating the high-resolution output given the corresponding low-resolution one and a high-resolution guidance map. The proposed layer is composed of a guided filter, which is reformulated as a fully differentiable block. To this end, we show that a guided filter can be expressed as a group of spatial varying linear transformation matrices. This layer could be integrated with the convolutional neural networks (CNNs) and jointly optimized through end-to-end training. To further take advantage of end-to-end training, we plug in a trainable transformation function that generates task-specific guidance maps. By integrating the CNNs and the proposed layer, we form deep guided filtering networks. The proposed networks are evaluated on five advanced image processing tasks. Experiments on MIT-Adobe FiveK Dataset demonstrate that the proposed approach runs 10-100 times faster and achieves the state-of-the-art performance. We also show that the proposed guided filtering layer helps to improve the performance of multiple pixel-wise dense prediction tasks",
    "checked": true,
    "id": "68e07fdfa8de4eb7ace6374ce9c16168317414e3",
    "semantic_title": "fast end-to-end trainable guided filter",
    "citation_count": 228,
    "authors": [
      "Huikai Wu",
      "Shuai Zheng",
      "Junge Zhang",
      "Kaiqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gilbert_Disentangling_Structure_and_CVPR_2018_paper.html": {
    "title": "Disentangling Structure and Aesthetics for Style-Aware Image Completion",
    "volume": "main",
    "abstract": "Content-aware image completion or in-painting is a fundamental tool for the correction of defects or removal of objects in images. We propose a non-parametric in-painting algorithm that enforces both structural and aesthetic (style) consistency within the resulting image. Our contributions are two-fold: 1) we explicitly disentangle image structure and style during patch search and selection to ensure a visually consistent look and feel within the target image; 2) we perform adaptive stylization of patches to conform the aesthetics of selected patches to the target image, so harmonising the integration of selected patches into the final composition. We show that explicit consideration of visual style during in-painting delivers excellent qualitative and quantitative results across the varied image styles and content, over the Places2 photographic dataset and a challenging new in-painting dataset of artwork derived from BAM!",
    "checked": true,
    "id": "35155115bdd579c109a5b2c35e7b26ed672858c0",
    "semantic_title": "disentangling structure and aesthetics for style-aware image completion",
    "citation_count": 14,
    "authors": [
      "Andrew Gilbert",
      "John Collomosse",
      "Hailin Jin",
      "Brian Price"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Learning_a_Discriminative_CVPR_2018_paper.html": {
    "title": "Learning a Discriminative Feature Network for Semantic Segmentation",
    "volume": "main",
    "abstract": "Most existing methods of semantic segmentation still suffer from two aspects of challenges: intra-class inconsistency and inter-class indistinction. To tackle these two problems, we propose a Discriminative Feature Network (DFN), which contains two sub-networks: Smooth Network and Border Network. Specifically, to handle the intra-class inconsistency problem, we specially design a Smooth Network with Channel Attention Block and global average pooling to select the more discriminative features. Furthermore, we propose a Border Network to make the bilateral features of boundary distinguishable with deep semantic boundary supervision. Based on our proposed DFN, we achieve state-of-the-art performance 86.2% mean IOU on PASCAL VOC 2012 and 80.3% mean IOU on Cityscapes dataset",
    "checked": true,
    "id": "343a128beb49db94169f24ea6c8cb22d6d59ecc4",
    "semantic_title": "learning a discriminative feature network for semantic segmentation",
    "citation_count": 761,
    "authors": [
      "Changqian Yu",
      "Jingbo Wang",
      "Chao Peng",
      "Changxin Gao",
      "Gang Yu",
      "Nong Sang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Kernelized_Subspace_Pooling_CVPR_2018_paper.html": {
    "title": "Kernelized Subspace Pooling for Deep Local Descriptors",
    "volume": "main",
    "abstract": "Representing local image patches in an invariant and discriminative manner is an active research topic in computer vision. It has recently been demonstrated that local feature learning based on deep Convolutional Neural Network (CNN) can significantly improve the matching performance. Previous works on learning such descriptors have focused on developing various loss functions, regularizations and data mining strategies to learn discriminative CNN representations. Such methods, however, have little analysis on how to increase geometric invariance of their generated descriptors. In this paper, we propose a descriptor that has both highly invariant and discriminative power. The abilities come from a novel pooling method, dubbed Subspace Pooling (SP) which is invariant to a range of geometric deformations. To further increase the discriminative power of our descriptor, we propose a simple distance kernel integrated to the marginal triplet loss that helps to focus on hard examples in CNN training. Finally, we show that by combining SP with the projection distance metric, the generated feature descriptor is equivalent to that of the Bilinear CNN model, but outperforms the latter with much lower memory and computation consumptions. The proposed method is simple, easy to understand and achieves good performance. Experimental results on several patch matching benchmarks show that our method outperforms the state-of-the-arts significantly",
    "checked": true,
    "id": "945f84632b2f60eb190c3d01e672b6777f27fcfe",
    "semantic_title": "kernelized subspace pooling for deep local descriptors",
    "citation_count": 46,
    "authors": [
      "Xing Wei",
      "Yue Zhang",
      "Yihong Gong",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hong_pOSE_Pseudo_Object_CVPR_2018_paper.html": {
    "title": "pOSE: Pseudo Object Space Error for Initialization-Free Bundle Adjustment",
    "volume": "main",
    "abstract": "Bundle adjustment is a nonlinear refinement method for camera poses and 3D structure requiring sufficiently good initialization. In recent years, it was experimentally observed that useful minima can be reached even from arbitrary initialization for affine bundle adjustment problems (and fixed-rank matrix factorization instances in general). The key success factor lies in the use of the variable projection (VarPro) method, which is known to have a wide basin of convergence for such problems. In this paper, we propose the Pseudo Object Space Error (pOSE), which is an objective with cameras represented as a hybrid between the affine and projective models. This formulation allows us to obtain 3D reconstructions that are close to the true projective reconstructions while retaining a bilinear problem structure suitable for the VarPro method. Experimental results show that using pOSE has a high success rate to yield faithful 3D reconstructions from random initializations, taking one step towards initialization-free structure from motion",
    "checked": true,
    "id": "602362b94fc35bd4af4f093355c449f17469d707",
    "semantic_title": "pose: pseudo object space error for initialization-free bundle adjustment",
    "citation_count": 15,
    "authors": [
      "Je Hyeong Hong",
      "Christopher Zach"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Litany_Deformable_Shape_Completion_CVPR_2018_paper.html": {
    "title": "Deformable Shape Completion With Graph Convolutional Autoencoders",
    "volume": "main",
    "abstract": "The availability of affordable and portable depth sensors has made scanning objects and people simpler than ever. However, dealing with occlusions and missing parts is still a significant challenge. The problem of reconstructing a (possibly non-rigidly moving) 3D object from a single or multiple partial scans has received increasing attention in recent years. In this work, we propose a novel learningbased method for the completion of partial shapes. Unlike the majority of existing approaches, our method focuses on objects that can undergo non-rigid deformations. The core of our method is a variational autoencoder with graph convolutional operations that learns a latent space for complete realistic shapes. At inference, we optimize to find the representation in this latent space that best fits the generated shape to the known partial input. The completed shape exhibits a realistic appearance on the unknown part. We show promising results towards the completion of synthetic and real scans of human body and face meshes exhibiting different styles of articulation and partiality",
    "checked": true,
    "id": "66bc2f209dfa9b547b0523311dd69f8aaafda971",
    "semantic_title": "deformable shape completion with graph convolutional autoencoders",
    "citation_count": 223,
    "authors": [
      "Or Litany",
      "Alex Bronstein",
      "Michael Bronstein",
      "Ameesh Makadia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gilani_Learning_From_Millions_CVPR_2018_paper.html": {
    "title": "Learning From Millions of 3D Scans for Large-Scale 3D Face Recognition",
    "volume": "main",
    "abstract": "Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem",
    "checked": true,
    "id": "4d46fd59364ed5ec8f50abe68cd7886379bfd80a",
    "semantic_title": "learning from millions of 3d scans for large-scale 3d face recognition",
    "citation_count": 135,
    "authors": [
      "Syed Zulqarnain Gilani",
      "Ajmal Mian"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.html": {
    "title": "CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles",
    "volume": "main",
    "abstract": "Despite significant research in the area, reconstruction of multiple dynamic rigid objects (eg. vehicles) observed from wide-baseline, uncalibrated and unsynchronized cameras, remains hard. On one hand, feature tracking works well within each view but is hard to correspond across multiple cameras with limited overlap in fields of view or due to occlusions. On the other hand, advances in deep learning have resulted in strong detectors that work across different viewpoints but are still not precise enough for triangulation-based reconstruction. In this work, we develop a framework to fuse both the single-view feature tracks and multi-view detected part locations to significantly improve the detection, localization and reconstruction of moving vehicles, even in the presence of strong occlusions. We demonstrate our framework at a busy traffic intersection by reconstructing over 62 vehicles passing within a 3-minute window. We evaluate the different components within our framework and compare to alternate approaches such as reconstruction using tracking-by-detection",
    "checked": true,
    "id": "13a282f42aecf824d8664facb97f42b3b6b8c00e",
    "semantic_title": "carfusion: combining point tracking and part detection for dynamic 3d reconstruction of vehicles",
    "citation_count": 76,
    "authors": [
      "N. Dinesh Reddy",
      "Minh Vo",
      "Srinivasa G. Narasimhan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhi_Deep_Material-Aware_Cross-Spectral_CVPR_2018_paper.html": {
    "title": "Deep Material-Aware Cross-Spectral Stereo Matching",
    "volume": "main",
    "abstract": "Cross-spectral imaging provides strong benefits for recognition and detection tasks. Often, multiple cameras are used for cross-spectral imaging, thus requiring image alignment, or disparity estimation in a stereo setting. Increasingly, multi-camera cross-spectral systems are embedded in active RGBD devices (e.g. RGB-NIR cameras in Kinect and iPhone X). Hence, stereo matching also provides an opportunity to obtain depth without an active projector source. However, matching images from different spectral bands is challenging because of large appearance variations. We develop a novel deep learning framework to simultaneously transform images across spectral bands and estimate disparity. A material-aware loss function is incorporated within the disparity prediction network to handle regions with unreliable matching such as light sources, glass windshields and glossy surfaces. No depth supervision is required by our method. To evaluate our method, we used a vehicle-mounted RGB-NIR stereo system to collect 13.7 hours of video data across a range of areas in and around a city. Experiments show that our method achieves strong performance and reaches real-time speed",
    "checked": true,
    "id": "c5fcc36768428185c0ec0f04c82c9204c9ba962c",
    "semantic_title": "deep material-aware cross-spectral stereo matching",
    "citation_count": 50,
    "authors": [
      "Tiancheng Zhi",
      "Bernardo R. Pires",
      "Martial Hebert",
      "Srinivasa G. Narasimhan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Price_Augmenting_Crowd-Sourced_3D_CVPR_2018_paper.html": {
    "title": "Augmenting Crowd-Sourced 3D Reconstructions Using Semantic Detections",
    "volume": "main",
    "abstract": "Image-based 3D reconstruction for Internet photo collections has become a robust technology to produce impressive virtual representations of real-world scenes. However, several fundamental challenges remain for Structure-from-Motion (SfM) pipelines, namely: the placement and reconstruction of transient objects only observed in single views, estimating the absolute scale of the scene, and (suprisingly often) recovering ground surfaces in the scene. We propose a method to jointly address these remaining open problems of SfM. In particular, we focus on detecting people in individual images and accurately placing them into an existing 3D model. As part of this placement, our method also estimates the absolute scale of the scene from object semantics, which in this case constitutes the height distribution of the population. Further, we obtain a smooth approximation of the ground surface and recover the gravity vector of the scene directly from the individual person detections. We demonstrate the results of our approach on a number of unordered Internet photo collections, and we quantitatively evaluate the obtained absolute scene scales",
    "checked": true,
    "id": "c3b375d6b4d47886b1bef4d675d80f4b7f15304c",
    "semantic_title": "augmenting crowd-sourced 3d reconstructions using semantic detections",
    "citation_count": 7,
    "authors": [
      "True Price",
      "Johannes L. Schönberger",
      "Zhen Wei",
      "Marc Pollefeys",
      "Jan-Michael Frahm"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Richter_Matryoshka_Networks_Predicting_CVPR_2018_paper.html": {
    "title": "Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers",
    "volume": "main",
    "abstract": "In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixel-prediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memory-efficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octree-based approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling",
    "checked": true,
    "id": "21d38af5a81a5d14f96512b0eef34ef762691cc0",
    "semantic_title": "matryoshka networks: predicting 3d geometry via nested shape layers",
    "citation_count": 139,
    "authors": [
      "Stephan R. Richter",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/He_Triplet-Center_Loss_for_CVPR_2018_paper.html": {
    "title": "Triplet-Center Loss for Multi-View 3D Object Retrieval",
    "volume": "main",
    "abstract": "Most existing 3D object recognition algorithms focus on leveraging the strong discriminative power of deep learning models with softmax loss for the classification of 3D data, while learning discriminative features with deep metric learning for 3D object retrieval is more or less neglected. In the paper, we study variants of deep metric learning losses for 3D object retrieval, which did not receive enough attention from this area. First , two kinds of representative losses, triplet loss and center loss, are introduced which could learn more discriminative features than traditional classification loss. Then we propose a novel loss named triplet-center loss, which can further enhance the discriminative power of the features. The proposed triplet-center loss learns a center for each class and requires that the distances between samples and centers from the same class are closer than those from different classes. Extensive experimental results on two popular 3D object retrieval benchmarks and two widely-adopted sketch-based 3D shape retrieval benchmarks consistently demonstrate the effectiveness of our proposed loss, and significant improvements have been achieved compared to the state-of-the-arts",
    "checked": true,
    "id": "abb6e069538588bd1214bf2744e41181d164f41b",
    "semantic_title": "triplet-center loss for multi-view 3d object retrieval",
    "citation_count": 330,
    "authors": [
      "Xinwei He",
      "Yang Zhou",
      "Zhichao Zhou",
      "Song Bai",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Stutz_Learning_3D_Shape_CVPR_2018_paper.html": {
    "title": "Learning 3D Shape Completion From Laser Scan Data With Weak Supervision",
    "volume": "main",
    "abstract": "3D shape completion from partial point clouds is a fundamental problem in computer vision and computer graphics. Recent approaches can be characterized as either data-driven or learning-based. Data-driven approaches rely on a shape model whose parameters are optimized to fit the observations. Learning-based approaches, in contrast, avoid the expensive optimization step and instead directly predict the complete shape from the incomplete observations using deep neural networks. However, full supervision is required which is often not available in practice. In this work, we propose a weakly-supervised learning-based approach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, i.e., learn, maximum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. Tackling 3D shape completion of cars on ShapeNet and KITTI, we demonstrate that the proposed amortized maximum likelihood approach is able to compete with a fully supervised baseline and a state-of-the-art data-driven approach while being significantly faster. On ModelNet, we additionally show that the approach is able to generalize to other object categories as well",
    "checked": true,
    "id": "1335190de78e0b58fbf7a7c7c52fa1a15f14858a",
    "semantic_title": "learning 3d shape completion from laser scan data with weak supervision",
    "citation_count": 225,
    "authors": [
      "David Stutz",
      "Andreas Geiger"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Georgakis_End-to-End_Learning_of_CVPR_2018_paper.html": {
    "title": "End-to-End Learning of Keypoint Detector and Descriptor for Pose Invariant 3D Matching",
    "volume": "main",
    "abstract": "Finding correspondences between images or 3D scans is at the heart of many computer vision and image retrieval applications and is often enabled by matching local keypoint descriptors. Various learning approaches have been applied in the past to different stages of the matching pipeline, considering detection, description, or metric learning objectives. These objectives were typically addressed separately and most previous work has focused on image data. This paper proposes an end-to-end learning framework for keypoint detection and its representation (descriptor) for 3D depth maps or 3D scans, where the two can be jointly optimized towards task-specific objectives without a need for separate annotations. We employ a Siamese architecture augmented by a sampling layer and a novel score loss function which in turn affects the selection of region proposals. The positive and negative examples are obtained automatically by sampling corresponding region proposals based on their consistency with known 3D pose labels. Matching experiments with depth data on multiple benchmark datasets demonstrate the efficacy of the proposed approach, showing significant improvements over state-of-the-art methods",
    "checked": true,
    "id": "9a7af2f96a77a922a04cfb472b5565816f03fd2f",
    "semantic_title": "end-to-end learning of keypoint detector and descriptor for pose invariant 3d matching",
    "citation_count": 63,
    "authors": [
      "Georgios Georgakis",
      "Srikrishna Karanam",
      "Ziyan Wu",
      "Jan Ernst",
      "Jana Košecká"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.html": {
    "title": "ICE-BA: Incremental, Consistent and Efficient Bundle Adjustment for Visual-Inertial SLAM",
    "volume": "main",
    "abstract": "Modern visual-inertial SLAM (VI-SLAM) achieves higher accuracy and robustness than pure visual SLAM, thanks to the complementariness of visual features and inertial measurements. However, jointly using visual and inertial measurements to optimize SLAM objective functions is a problem of high computational complexity. In many VI-SLAM applications, the conventional optimization solvers can only use a very limited number of recent measurements for real time pose estimation, at the cost of suboptimal localization accuracy. In this work, we renovate the numerical solver for VI-SLAM. Compared to conventional solvers, our proposal provides an exact solution with significantly higher computational efficiency. Our solver allows us to use remarkably larger number of measurements to achieve higher accuracy and robustness. Furthermore, our method resolves the global consistency problem that is unaddressed by many state-of-the-art SLAM systems: to guarantee the minimization of re-projection function and inertial constraint function during loop closure. Experiments demonstrate our novel formulation renders lower localization error and more than 10x speedup compared to alternatives. We release the source code of our implementation to benefit the community",
    "checked": true,
    "id": "9f4549defd693d8f9e969fad55d21ff2d370da88",
    "semantic_title": "ice-ba: incremental, consistent and efficient bundle adjustment for visual-inertial slam",
    "citation_count": 128,
    "authors": [
      "Haomin Liu",
      "Mingyu Chen",
      "Guofeng Zhang",
      "Hujun Bao",
      "Yingze Bao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper.html": {
    "title": "GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose",
    "volume": "main",
    "abstract": "We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical flow and ego-motion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Specifically, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones",
    "checked": true,
    "id": "4e1b1b98d9b0c73eaca4e71f553e72503fbd4aba",
    "semantic_title": "geonet: unsupervised learning of dense depth, optical flow and camera pose",
    "citation_count": 1142,
    "authors": [
      "Zhichao Yin",
      "Jianping Shi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pritts_Radially-Distorted_Conjugate_Translations_CVPR_2018_paper.html": {
    "title": "Radially-Distorted Conjugate Translations",
    "volume": "main",
    "abstract": "This paper introduces the first minimal solvers that jointly solve for affine-rectification and radial lens distortion from coplanar repeated patterns. Even with imagery from moderately distorted lenses, plane rectification using the pinhole camera model is inaccurate or invalid. The proposed solvers incorporate lens distortion into the camera model and extend accurate rectification to wide-angle imagery, which is now common from consumer cameras. The solvers are derived from constraints induced by the conjugate translations of an imaged scene plane, which are integrated with the division model for radial lens distortion. The hidden-variable trick with ideal saturation is used to reformulate the constraints so that the solvers generated by the Gr{\\\"o}bner-basis method are stable, small and fast. The proposed solvers are used in a RANSAC-based estimator. Rectification and lens distortion are recovered from either one conjugately translated affine-covariant feature or two independently translated similarity-covariant features. Experiments confirm that RANSAC accurately estimates the rectification and radial distortion with very few iterations. The proposed solvers are evaluated against the state-of-the-art for affine rectification and radial distortion estimation",
    "checked": true,
    "id": "3c53240c5a2c7ba8effe4c66b54fede7f1d44860",
    "semantic_title": "radially-distorted conjugate translations",
    "citation_count": 40,
    "authors": [
      "James Pritts",
      "Zuzana Kukelova",
      "Viktor Larsson",
      "Ondřej Chum"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fu_Deep_Ordinal_Regression_CVPR_2018_paper.html": {
    "title": "Deep Ordinal Regression Network for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed prob- lem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DCNNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skip-connections or multi- layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and faster convergence in synch. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel. The proposed deep ordinal regression network (DORN) achieves state-of-the-art results on three challenging benchmarks, i.e., KITTI [16], Make3D [49], and NYU Depth v2 [41], and outperforms existing methods by a large margin",
    "checked": true,
    "id": "63b059cdad77906ff381515b3cfac21757e5e64c",
    "semantic_title": "deep ordinal regression network for monocular depth estimation",
    "citation_count": 1728,
    "authors": [
      "Huan Fu",
      "Mingming Gong",
      "Chaohui Wang",
      "Kayhan Batmanghelich",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Miraldo_Analytical_Modeling_of_CVPR_2018_paper.html": {
    "title": "Analytical Modeling of Vanishing Points and Curves in Catadioptric Cameras",
    "volume": "main",
    "abstract": "Vanishing points and vanishing lines are classical geometrical concepts in perspective cameras that have a lineage dating back to 3 centuries. A vanishing point is a point on the image space where parallel lines in 3D space appear to converge, whereas a vanishing line passes through 2 or more vanishing points. While such concepts are simple and intuitive in perspective cameras, their counterparts in catadioptric cameras (obtained using mirrors and lenses) are more involved. For example, lines in the 3D space map to higher degree curves in catadioptric cameras. The projection of a set of 3D parallel lines converges on a single point in perspective images, whereas they converge to more than one point in catadioptric cameras. To the best of our knowledge, we are not aware of any systematic development of analytical models for vanishing points and vanishing curves in different types of catadioptric cameras. In this paper, we derive parametric equations for vanishing points and vanishing curves using the calibration parameters, mirror shape coefficients, and direction vectors of parallel lines in 3D space. We show compelling experimental results on vanishing point estimation and absolute pose estimation for a wide variety of catadioptric cameras in both simulations and real experiments",
    "checked": true,
    "id": "8054e41f1789adf60157006ec5baa91f8bca267d",
    "semantic_title": "analytical modeling of vanishing points and curves in catadioptric cameras",
    "citation_count": 8,
    "authors": [
      "Pedro Miraldo",
      "Francisco Eiras",
      "Srikumar Ramalingam"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Learning_Depth_From_CVPR_2018_paper.html": {
    "title": "Learning Depth From Monocular Videos Using Direct Methods",
    "volume": "main",
    "abstract": "The ability to predict depth from a single image - using recent advances in CNNs - is of increasing interest to the vision community. Unsupervised strategies to learning are particularly appealing as they can utilize much larger and varied monocular video datasets during learning without the need for ground truth depth or stereo. In previous works, separate pose and depth CNN predictors had to be determined such that their joint outputs minimized the photometric error. Inspired by recent advances in direct visual odometry (DVO), we argue that the depth CNN predictor can be learned without a pose CNN predictor. Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO, along with a novel depth normalization strategy - substantially improves performance over state of the art that use monocular videos for training",
    "checked": true,
    "id": "71acad655c616df320faa16234c7221a41869679",
    "semantic_title": "learning depth from monocular videos using direct methods",
    "citation_count": 581,
    "authors": [
      "Chaoyang Wang",
      "José Miguel Buenaposada",
      "Rui Zhu",
      "Simon Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Salience_Guided_Depth_CVPR_2018_paper.html": {
    "title": "Salience Guided Depth Calibration for Perceptually Optimized Compressive Light Field 3D Display",
    "volume": "main",
    "abstract": "Multi-layer light field displays are a type of computational three-dimensional (3D) display which has recently gained increasing interest for its holographic-like effect and natural compatibility with 2D displays. However, the major shortcoming, depth limitation, still cannot be overcome in the traditional light field modeling and reconstruction based on multi-layer liquid crystal displays (LCDs). Considering this disadvantage, our paper incorporates a salience guided depth optimization over a limited display range to calibrate the displayed depth and present the maximum area of salience region for multi-layer light field display. Different from previously reported cascaded light field displays that use the fixed initialization plane as the depth center of display content, our method automatically calibrates the depth initialization based on the salience results derived from the proposed contrast enhanced salience detection method. Experiments demonstrate that the proposed method provides a promising advantage in visual perception for the compressive light field displays from both software simulation and prototype demonstration",
    "checked": true,
    "id": "6c41c6fe57a78431db0c4d660be8dfb3c501a7cb",
    "semantic_title": "salience guided depth calibration for perceptually optimized compressive light field 3d display",
    "citation_count": 14,
    "authors": [
      "Shizheng Wang",
      "Wenjuan Liao",
      "Phil Surman",
      "Zhigang Tu",
      "Yuanjin Zheng",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_MegaDepth_Learning_Single-View_CVPR_2018_paper.html": {
    "title": "MegaDepth: Learning Single-View Depth Prediction From Internet Photos",
    "volume": "main",
    "abstract": "Single-view depth prediction is a fundamental problem in computer vision. Recently, deep learning methods have led to significant progress, but such methods are limited by the available training data. Current datasets based on 3D sensors have key limitations, including indoor-only images (NYU), small numbers of training examples (Make3D), and sparse sampling (KITTI). We propose to use multi-view Internet photo collections, a virtually unlimited data source, to generate training data via modern structure-from-motion and multi-view stereo (MVS) methods, and present a large depth dataset called MegaDepth based on this idea. Data derived from MVS comes with its own challenges, including noise and unreconstructable objects. We address these challenges with new data cleaning methods, as well as automatically augmenting our data with ordinal depth relations generated using semantic segmentation. We validate the use of large amounts of Internet data by showing that models trained on MegaDepth exhibit strong generalization—not only to novel scenes, but also to other diverse datasets including Make3D, KITTI, and DIW, even when no images from those datasets are seen during training",
    "checked": true,
    "id": "3395de3126d9b6b9d75f0d85d8c4ebab8ff84686",
    "semantic_title": "megadepth: learning single-view depth prediction from internet photos",
    "citation_count": 1017,
    "authors": [
      "Zhengqi Li",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zou_LayoutNet_Reconstructing_the_CVPR_2018_paper.html": {
    "title": "LayoutNet: Reconstructing the 3D Room Layout From a Single RGB Image",
    "volume": "main",
    "abstract": "We propose an algorithm to predict room layout from a single image that generalizes across panoramas and perspective images, cuboid layouts and more general layouts (e.g. \"L\"-shape room). Our method operates directly on the panoramic image, rather than decomposing into perspective images as do recent works. Our network architecture is similar to that of RoomNet, but we show improvements due to aligning the image based on vanishing points, predicting multiple layout elements (corners, boundaries, size and translation), and fitting a constrained Manhattan layout to the resulting predictions. Our method compares well in speed and accuracy to other existing work on panoramas, achieves among the best accuracy for perspective images, and can handle both cuboid-shaped and more general Manhattan layouts",
    "checked": true,
    "id": "baadfc54c79fabfec0bc8c7d2a5e5a76f6fc9aea",
    "semantic_title": "layoutnet: reconstructing the 3d room layout from a single rgb image",
    "citation_count": 238,
    "authors": [
      "Chuhang Zou",
      "Alex Colburn",
      "Qi Shan",
      "Derek Hoiem"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Batsos_CBMV_A_Coalesced_CVPR_2018_paper.html": {
    "title": "CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation",
    "volume": "main",
    "abstract": "Recently, there has been a paradigm shift in stereo matching with learning-based methods achieving the best results on all popular benchmarks. The success of these methods is due to the availability of training data with ground truth; training learning-based systems on these datasets has allowed them to surpass the accuracy of conventional approaches based on heuristics and assumptions. Many of these assumptions, however, had been validated extensively and hold for the majority of possible inputs. In this paper, we generate a matching volume leveraging both data with ground truth and conventional wisdom. We accomplish this by coalescing diverse evidence from a bidirectional matching process via random forest classifiers. We show that the resulting matching volume estimation method achieves similar accuracy to purely data-driven alternatives on benchmarks and that it generalizes to unseen data much better. In fact, the results we submitted to the KITTI benchmarks were generated using a classifier trained on the Middlebury dataset",
    "checked": true,
    "id": "18610275bd5b5b8a12182b9d347b69f35524404c",
    "semantic_title": "cbmv: a coalesced bidirectional matching volume for disparity estimation",
    "citation_count": 64,
    "authors": [
      "Konstantinos Batsos",
      "Changjiang Cai",
      "Philippos Mordohai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pang_Zoom_and_Learn_CVPR_2018_paper.html": {
    "title": "Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains",
    "volume": "main",
    "abstract": "Despite the recent success of stereo matching with convolutional neural networks (CNNs), it remains arduous to generalize a pre-trained deep stereo model to a novel domain. A major difficulty is to collect accurate ground-truth disparities for stereo pairs in the target domain. In this work, we propose a self-adaptation approach for CNN training, utilizing both synthetic training data (with ground-truth disparities) and stereo pairs in the new domain (without ground-truths). Our method is driven by two empirical observations. By feeding real stereo pairs of different domains to stereo models pre-trained with synthetic data, we see that: i) a pre-trained model does not generalize well to the new domain, producing artifacts at boundaries and ill-posed regions; however, ii) feeding an up-sampled stereo pair leads to a disparity map with extra details. To avoid i) while exploiting ii), we formulate an iterative optimization problem with graph Laplacian regularization. At each iteration, the CNN adapts itself better to the new domain: we let the CNN learn its own higher-resolution output; at the meanwhile, a graph Laplacian regularization is imposed to discriminatively keep the desired edges while smoothing out the artifacts. We demonstrate the effectiveness of our method in two domains: daily scenes collected by smartphone cameras, and street views captured in a driving car",
    "checked": true,
    "id": "6aa52c8bb8ef308a90cf23cb08ab83c3af7e6ce2",
    "semantic_title": "zoom and learn: generalizing deep stereo matching to novel domains",
    "citation_count": 56,
    "authors": [
      "Jiahao Pang",
      "Wenxiu Sun",
      "Chengxi Yang",
      "Jimmy Ren",
      "Ruichao Xiao",
      "Jin Zeng",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Exploring_Disentangled_Feature_CVPR_2018_paper.html": {
    "title": "Exploring Disentangled Feature Representation Beyond Face Identification",
    "volume": "main",
    "abstract": "This paper proposes learning disentangled but complementary face features with a minimal supervision by face identification. Specifically, we construct an identity Distilling and Dispelling Auto-Encoder (D^2AE) framework that adversarially learns the identity-distilled features for identity verification and the identity-dispelled features to fool the verification system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only preserve state-of-the-art identity verification performance on LFW, but also acquire comparable discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner",
    "checked": true,
    "id": "1fd5d08394a3278ef0a89639e9bfec7cb482e0bf",
    "semantic_title": "exploring disentangled feature representation beyond face identification",
    "citation_count": 156,
    "authors": [
      "Yu Liu",
      "Fangyin Wei",
      "Jing Shao",
      "Lu Sheng",
      "Junjie Yan",
      "Xiaogang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Learning_Facial_Action_CVPR_2018_paper.html": {
    "title": "Learning Facial Action Units From Web Images With Scalable Weakly Supervised Clustering",
    "volume": "main",
    "abstract": "We present a scalable weakly supervised clustering approach to learn facial action units (AUs) from large, freely available web images. Unlike most existing methods (e.g., CNNs) that rely on fully annotated data, our method exploits web images with inaccurate annotations. Specifically, we derive a weakly-supervised spectral algorithm that learns an embedding space to couple image appearance and semantics. The algorithm has efficient gradient update, and scales up to large quantities of images with a stochastic extension. With the learned embedding space, we adopt rank-order clustering to identify groups of visually and semantically similar images, and re-annotate these groups for training AU classifiers. Evaluation on the 1 millon EmotioNet dataset demonstrates the effectiveness of our approach: (1) our learned annotations reach on average 91.3% agreement with human annotations on 7 common AUs, (2) classifiers trained with re-annotated images perform comparably to, sometimes even better than, its supervised CNN-based counterpart, and (3) our method offers intuitive outlier/noise pruning instead of forcing one annotation to every image. Code is available",
    "checked": true,
    "id": "35816cf9475a8f7f5e2f108b605adc6579e416b1",
    "semantic_title": "learning facial action units from web images with scalable weakly supervised clustering",
    "citation_count": 59,
    "authors": [
      "Kaili Zhao",
      "Wen-Sheng Chu",
      "Aleix M. Martinez"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Nie_Human_Pose_Estimation_CVPR_2018_paper.html": {
    "title": "Human Pose Estimation With Parsing Induced Learner",
    "volume": "main",
    "abstract": "Human pose estimation still faces various difficulties in challenging scenarios. Human parsing, as a closely related task, can provide valuable cues for better pose estimation, which however has not been fully exploited. In this paper, we propose a novel Parsing Induced Learner to exploit parsing information to effectively assist pose estimation by learning to fast adapt the base pose estimation model. The proposed Parsing Induced Learner is composed of a parsing encoder and a pose model parameter adapter, which together learn to predict dynamic parameters of the pose model to extract complementary useful features for more accurate pose estimation. Comprehensive experiments on benchmarks LIP and extended PASCAL-Person-Part show that the proposed Parsing Induced Learner can improve performance of both single- and multi-person pose estimation to new state-of-the-art. Cross-dataset experiments also show that the proposed Parsing Induced Learner from LIP dataset can accelerate learning of a human pose estimation model on MPII benchmark in addition to achieving outperforming performance",
    "checked": true,
    "id": "dce84034bf4a8139ec8676fda4d5eb43cbeb6a2d",
    "semantic_title": "human pose estimation with parsing induced learner",
    "citation_count": 91,
    "authors": [
      "Xuecheng Nie",
      "Jiashi Feng",
      "Yiming Zuo",
      "Shuicheng Yan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chang_Multi-Level_Factorisation_Net_CVPR_2018_paper.html": {
    "title": "Multi-Level Factorisation Net for Person Re-Identification",
    "volume": "main",
    "abstract": "Key to effective person re-identification (Re-ID) is modelling discriminative and view-invariant factors of person appearance at both high and low semantic levels. Recently developed deep Re-ID models either learn a holistic single semantic level feature representation and/or require laborious human annotation of these factors as attributes. We propose Multi-Level Factorisation Net (MLFN), a novel network architecture that factorises the visual appearance of a person into latent discriminative factors at multiple semantic levels without manual annotation. MLFN is composed of multiple stacked blocks. Each block contains multiple factor modules to model latent factors at a specific level, and factor selection modules that dynamically select the factor modules to interpret the content of each input image. The outputs of the factor selection modules also provide a compact latent factor descriptor that is complementary to the conventional deeply learned features. MLFN achieves state-of-the-art results on three Re-ID datasets, as well as compelling results on the general object categorisation CIFAR-100 dataset",
    "checked": true,
    "id": "004acfec16c36649408c561faa102dd9de76f085",
    "semantic_title": "multi-level factorisation net for person re-identification",
    "citation_count": 506,
    "authors": [
      "Xiaobin Chang",
      "Timothy M. Hospedales",
      "Tao Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Attention-Aware_Compositional_Network_CVPR_2018_paper.html": {
    "title": "Attention-Aware Compositional Network for Person Re-Identification",
    "volume": "main",
    "abstract": "Person re-identification (ReID) is to identify pedestrians observed from different camera views based on visual appearance. It is a challenging task due to large pose variations, complex background clutters and severe occlusions. Recently, human pose estimation by predicting joint locations was largely improved in accuracy. It is reasonable to use pose estimation results for handling pose variations and background clutters, and such attempts have obtained great improvement in ReID performance. However, we argue that the pose information was not well utilized and hasn't yet been fully exploited for person ReID. In this work, we introduce a novel framework called Attention-Aware Compositional Network (AACN) for person ReID. AACN consists of two main components: Pose-guided Part Attention (PPA) and Attention-aware Feature Composition (AFC). PPA is learned and applied to mask out undesirable background features in pedestrian feature maps. Furthermore, pose-guided visibility scores are estimated for body parts to deal with part occlusion in the proposed AFC module. Extensive experiments with ablation analysis show the effectiveness of our method, and state-of-the-art results are achieved on several public datasets, including Market-1501, CUHK03, CUHK01, SenseReID, CUHK03-NP and DukeMTMC-reID",
    "checked": true,
    "id": "81a2614003fd8b241bc18581c4a0777f9e50d84e",
    "semantic_title": "attention-aware compositional network for person re-identification",
    "citation_count": 443,
    "authors": [
      "Jing Xu",
      "Rui Zhao",
      "Feng Zhu",
      "Huaming Wang",
      "Wanli Ouyang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Look_at_Boundary_CVPR_2018_paper.html": {
    "title": "Look at Boundary: A Boundary-Aware Face Alignment Algorithm",
    "volume": "main",
    "abstract": "We present a novel boundary-aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation. Unlike the conventional heatmap based method and regression based method, our approach derives face landmarks from boundary lines which remove the ambiguities in the landmark definition. Three questions are explored and answered by this work: 1. Why using boundary? 2. How to use boundary? 3. What is the relationship between boundary estimation and landmarks localisation? Our boundary-aware face alignment algorithm achieves 3.49% mean error on 300-W Fullset, which outperforms state-of-the-art methods by a large margin. Our method can also easily integrate information from other datasets. By utilising boundary information of 300-W dataset, our method achieves 3.92% mean error with 0.39% failure rate on COFW dataset, and 1.25% mean error on AFLW-Full dataset. Moreover, we propose a new dataset WFLW to unify training and testing across different factors, including poses, expressions, illuminations, makeups, occlusions, and blurriness. Dataset and model are publicly available at https://wywu.github.io/projects/LAB/LAB.html",
    "checked": true,
    "id": "e7265c560b3f10013bf70aacbbf0eb4631b7e2aa",
    "semantic_title": "look at boundary: a boundary-aware face alignment algorithm",
    "citation_count": 438,
    "authors": [
      "Wayne Wu",
      "Chen Qian",
      "Shuo Yang",
      "Quan Wang",
      "Yici Cai",
      "Qiang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fang_Demo2Vec_Reasoning_Object_CVPR_2018_paper.html": {
    "title": "Demo2Vec: Reasoning Object Affordances From Online Videos",
    "volume": "main",
    "abstract": "Watching expert demonstrations is an important way for humans and robots to reason about affordances of unseen objects. In this paper, we consider the problem of reasoning object affordances through the feature embedding of demonstration videos. We design the Demo2Vec model which learns to extract embedded vectors of demonstration videos and predicts the interaction region and the action label on a target image of the same object. We introduce the Online Product Review dataset for Affordance (OPRA) by collecting and labeling diverse YouTube product review videos. Our Demo2Vec model outperforms various recurrent neural network baselines on the collected dataset",
    "checked": true,
    "id": "4c0d3055decc8a6433918920127512e0fcbc06f0",
    "semantic_title": "demo2vec: reasoning object affordances from online videos",
    "citation_count": 107,
    "authors": [
      "Kuan Fang",
      "Te-Lin Wu",
      "Daniel Yang",
      "Silvio Savarese",
      "Joseph J. Lim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zanfir_Monocular_3D_Pose_CVPR_2018_paper.html": {
    "title": "Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes - The Importance of Multiple Scene Constraints",
    "volume": "main",
    "abstract": "Human sensing has greatly benefited from recent advances in deep learning, parametric human modeling, and large scale 2d and 3d datasets. However, existing 3d models make strong assumptions about the scene, considering either a single person per image, full views of the person, a simple background or many cameras. In this paper, we leverage state-of-the-art deep multi-task neural networks and parametric human and scene modeling, towards a fully automatic monocular visual sensing system for multiple interacting people, which (i) infers the 2d and 3d pose and shape of multiple people from a single image, relying on detailed semantic representations at both model and image level, to guide a combined optimization with feedforward and feedback components, (ii) automatically integrates scene constraints including ground plane support and simultaneous volume occupancy by multiple people, and (iii) extends the single image model to video by optimally solving the temporal person assignment problem and imposing coherent temporal pose and motion reconstructions while preserving image alignment fidelity. We perform experiments on both single and multi-person datasets, and systematically evaluate each component of the model, showing improved performance and extensive multiple human sensing capability. We also apply our method to images with multiple people, severe occlusions and diverse backgrounds captured in challenging natural scenes, and obtain results of good perceptual quality",
    "checked": true,
    "id": "dda4ca5b584de2bf6a11ad024cecf62c7da99b64",
    "semantic_title": "monocular 3d pose and shape estimation of multiple people in natural scenes: the importance of multiple scene constraints",
    "citation_count": 282,
    "authors": [
      "Andrei Zanfir",
      "Elisabeta Marinoiu",
      "Cristian Sminchisescu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Marinoiu_3D_Human_Sensing_CVPR_2018_paper.html": {
    "title": "3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children With Autism",
    "volume": "main",
    "abstract": "We introduce new, fine-grained action and emotion recognition tasks defined on non-staged videos, recorded during robot-assisted therapy sessions of children with autism. The tasks present several challenges: a large dataset with long videos, a large number of highly variable actions, children that are only partially visible, have different ages and may show unpredictable behaviour, as well as non-standard camera viewpoints. We investigate how state-of-the-art 3d human pose reconstruction methods perform on the newly introduced tasks and propose extensions to adapt them to deal with these challenges. We also analyze multiple approaches in action and emotion recognition from 3d human pose data, establish several baselines, and discuss results and their implications in the context of child-robot interaction",
    "checked": true,
    "id": "ac5fa28687217818e698aa102577166609a2c5c4",
    "semantic_title": "3d human sensing, action and emotion recognition in robot assisted therapy of children with autism",
    "citation_count": 84,
    "authors": [
      "Elisabeta Marinoiu",
      "Mihai Zanfir",
      "Vlad Olaru",
      "Cristian Sminchisescu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Facial_Expression_Recognition_CVPR_2018_paper.html": {
    "title": "Facial Expression Recognition by De-Expression Residue Learning",
    "volume": "main",
    "abstract": "A facial expression is a combination of an expressive component and a neutral component of a person. In this paper, we propose to recognize facial expressions by extracting information of the expressive component through a de-expression learning procedure, called De-expression Residue Learning (DeRL). First, a generative model is trained by cGAN. This model generates the corresponding neutral face image for any input face image. We call this procedure de-expression because the expressive information is filtered out by the generative model; however, the expressive information is still recorded in the intermediate layers. Given the neutral face image, unlike previous works using pixel-level or feature-level difference for facial expression classification, our new method learns the deposition (or residue) that remains in the intermediate layers of the generative model. Such a residue is essential as it contains the expressive component deposited in the generative model from any input facial expression images. Seven public facial expression databases are employed in our experiments. With two databases (BU-4DFE and BP4D-spontaneous) for pre-training, the DeRL method has been evaluated on five databases, CK+, Oulu-CASIA, MMI, BU- 3DFE, and BP4D+. The experimental results demonstrate the superior performance of the proposed method",
    "checked": true,
    "id": "df096e221cc92c481d34443f4788c96ee412b189",
    "semantic_title": "facial expression recognition by de-expression residue learning",
    "citation_count": 352,
    "authors": [
      "Huiyuan Yang",
      "Umur Ciftci",
      "Lijun Yin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_A_Causal_And-Or_CVPR_2018_paper.html": {
    "title": "A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects",
    "volume": "main",
    "abstract": "Tracking humans that are interacting with the other subjects or environment remains unsolved in visual tracking, because the visibility of the human of interests in videos is unknown and might vary over time. In particular, it is still difficult for state-of-the-art human trackers to recover complete human trajectories in crowded scenes with frequent human interactions. In this work, we consider the visibility status of a subject as a fluent variable, whose change is mostly attributed to the subject's interaction with the surrounding, e.g., crossing behind another object, entering a building, or getting into a vehicle, etc. We introduce a Causal And-Or Graph (C-AOG) to represent the causal-effect relations between an object's visibility fluent and its activities, and develop a probabilistic graph model to jointly reason the visibility fluent change (e.g., from visible to invisible) and track humans in videos. We formulate this joint task as an iterative search of a feasible causal graph structure that enables fast search algorithm, e.g., dynamic programming method. We apply the proposed method on challenging video sequences to evaluate its capabilities of estimating visibility fluent changes of subjects and tracking subjects of interests over time. Results with comparisons demonstrate that our method outperforms the alternative trackers and can recover complete trajectories of humans in complicated scenarios with frequent human interactions",
    "checked": true,
    "id": "71a353116bcd6c23333d311a5e54e4e6a48b7cd7",
    "semantic_title": "a causal and-or graph model for visibility fluent reasoning in tracking interacting objects",
    "citation_count": 28,
    "authors": [
      "Yuanlu Xu",
      "Lei Qin",
      "Xiaobai Liu",
      "Jianwen Xie",
      "Song-Chun Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Peng_Weakly_Supervised_Facial_CVPR_2018_paper.html": {
    "title": "Weakly Supervised Facial Action Unit Recognition Through Adversarial Training",
    "volume": "main",
    "abstract": "Current works on facial action unit (AU) recognition typically require fully AU-annotated facial images for supervised AU classifier training. AU annotation is a time-consuming, expensive, and error-prone process. While AUs are hard to annotate, facial expression is relatively easy to label. Furthermore, there exist strong probabilistic dependencies between expressions and AUs as well as dependencies among AUs. Such dependencies are referred to as domain knowledge. In this paper, we propose a novel AU recognition method that learns AU classifiers from domain knowledge and expression-annotated facial images through adversarial training. Specifically, we first generate pseudo AU labels according to the probabilistic dependencies between expressions and AUs as well as correlations among AUs summarized from domain knowledge. Then we propose a weakly supervised AU recognition method via an adversarial process, in which we simultaneously train two models: a recognition model R, which learns AU classifiers, and a discrimination model D, which estimates the probability that AU labels generated from domain knowledge rather than the recognized AU labels from R. The training procedure for R maximizes the probability of D making a mistake. By leveraging the adversarial mechanism, the distribution of recognized AUs is closed to AU prior distribution from domain knowledge. Furthermore, the proposed weakly supervised AU recognition can be extended to semi-supervised learning scenarios with partially AU-annotated images. Experimental results on three benchmark databases demonstrate that the proposed method successfully leverages the summarized domain knowledge to weakly supervised AU classifier learning through an adversarial process, and thus achieves state-of-the-art performance",
    "checked": true,
    "id": "20e3bd5b0fb6b1430927d7618f03a6234cb3bcd7",
    "semantic_title": "weakly supervised facial action unit recognition through adversarial training",
    "citation_count": 57,
    "authors": [
      "Guozhu Peng",
      "Shangfei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cherian_Non-Linear_Temporal_Subspace_CVPR_2018_paper.html": {
    "title": "Non-Linear Temporal Subspace Representations for Activity Recognition",
    "volume": "main",
    "abstract": "Representations that can compactly and effectively capture the temporal evolution of semantic content are important to computer vision and machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in a reproducing kernel Hilbert space, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective. We then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results",
    "checked": true,
    "id": "13d93feb5431eda200ac482b5230f51667c0146a",
    "semantic_title": "non-linear temporal subspace representations for activity recognition",
    "citation_count": 44,
    "authors": [
      "Anoop Cherian",
      "Suvrit Sra",
      "Stephen Gould",
      "Richard Hartley"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Towards_Pose_Invariant_CVPR_2018_paper.html": {
    "title": "Towards Pose Invariant Face Recognition in the Wild",
    "volume": "main",
    "abstract": "Pose variation is one key challenge in face recognition. As opposed to current techniques for pose invariant face recognition, which either directly extract pose invariant features for recognition, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporated with an unsupervised cross-domain adversarial training and a \"learning to learn\" strategy for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representation. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts",
    "checked": true,
    "id": "95df57cf3e15d75a8526b4fd3212d39538d31100",
    "semantic_title": "towards pose invariant face recognition in the wild",
    "citation_count": 225,
    "authors": [
      "Jian Zhao",
      "Yu Cheng",
      "Yan Xu",
      "Lin Xiong",
      "Jianshu Li",
      "Fang Zhao",
      "Karlekar Jayashree",
      "Sugiri Pranata",
      "Shengmei Shen",
      "Junliang Xing",
      "Shuicheng Yan",
      "Jiashi Feng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Unifying_Identification_and_CVPR_2018_paper.html": {
    "title": "Unifying Identification and Context Learning for Person Recognition",
    "volume": "main",
    "abstract": "Despite the great success of face recognition techniques, recognizing persons under unconstrained settings remains challenging. Issues like profile views, unfavorable lighting, and occlusions can cause substantial difficulties. Previous works have attempted to tackle this problem by exploiting the context, e.g. clothes and social relations. While showing promising improvement, they are usually limited in two important aspects, relying on simple heuristics to combine different cues and separating the construction of context from people identities. In this work, we aim to move beyond such limitations and propose a new framework to leverage context for person recognition. In particular, we propose a Region Attention Network, which is learned to adaptively combine visual cues with instance-dependent weights. We also develop a unified formulation, where the social contexts are learned along with the reasoning of people identities. These models substantially improve the robustness when working with the complex contextual relations in unconstrained environments. On two large datasets, PIPA and Cast In Movies (CIM), a new dataset proposed in this work, our method consistently achieves state-of-the-art performance under multiple evaluation policies",
    "checked": true,
    "id": "2fe7105ef8e61330a3ddc7f7b35955ca62fc1ab3",
    "semantic_title": "unifying identification and context learning for person recognition",
    "citation_count": 38,
    "authors": [
      "Qingqiu Huang",
      "Yu Xiong",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Peng_Jointly_Optimize_Data_CVPR_2018_paper.html": {
    "title": "Jointly Optimize Data Augmentation and Network Training: Adversarial Data Augmentation in Human Pose Estimation",
    "volume": "main",
    "abstract": "Random data augmentation is a critical technique to avoid overfitting in training deep models. Yet, data augmentation and network training are often two isolated processes in most settings, yielding to a suboptimal training. Why not jointly optimize the two? We propose adversarial data augmentation to address this limitation. The key idea is to design a generator (e.g. an augmentation network) that competes against a discriminator (e.g. a target network) by generating hard examples online. The generator explores weaknesses of the discriminator, while the discriminator learns from hard augmentations to achieve better performance. A reward/penalty strategy is also proposed for efficient joint training. We investigate human pose estimation and carry out comprehensive ablation studies to validate our method. The results prove that our method can effectively improve state-of-the-art models without additional data effort",
    "checked": true,
    "id": "be12660abcbbbe0d88f3966608655e0d538bffc8",
    "semantic_title": "jointly optimize data augmentation and network training: adversarial data augmentation in human pose estimation",
    "citation_count": 204,
    "authors": [
      "Xi Peng",
      "Zhiqiang Tang",
      "Fei Yang",
      "Rogerio S. Feris",
      "Dimitris Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Feng_Wing_Loss_for_CVPR_2018_paper.html": {
    "title": "Wing Loss for Robust Facial Landmark Localisation With Convolutional Neural Networks",
    "volume": "main",
    "abstract": "We present a new loss function, namely Wing loss, for robust facial landmark localisation with Convolutional Neural Networks (CNNs). We first compare and analyse different loss functions including L2, L1 and smooth L1. The analysis of these loss functions suggests that, for the training of a CNN-based localisation model, more attention should be paid to small and medium range errors. To this end, we design a piece-wise loss function. The new loss amplifies the impact of errors from the interval (-w, w) by switching from L1 loss to a modified logarithm function. To address the problem of under-representation of samples with large out-of-plane head rotations in the training set, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation approaches. Last, the proposed approach is extended to create a two-stage framework for robust facial landmark localisation. The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function, and prove the superiority of the proposed method over the state-of-the-art approaches",
    "checked": true,
    "id": "822346af4fafcc54d7d95da2cbbb0bbe189405b4",
    "semantic_title": "wing loss for robust facial landmark localisation with convolutional neural networks",
    "citation_count": 344,
    "authors": [
      "Zhen-Hua Feng",
      "Josef Kittler",
      "Muhammad Awais",
      "Patrik Huber",
      "Xiao-Jun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yao_Multiple_Granularity_Group_CVPR_2018_paper.html": {
    "title": "Multiple Granularity Group Interaction Prediction",
    "volume": "main",
    "abstract": "Most human activity analysis works (i.e., recognition or prediction) only focus on a single granularity, i.e., either modelling global motion based on the coarse level movement such as human trajectories or forecasting future detailed action based on body parts' movement such as skeleton motion. In contrast, in this work, we propose a multi-granularity interaction prediction network which integrates both global motion and detailed local action. Built on a bi- directional LSTM network, the proposed method possesses between granularities links which encourage feature sharing as well as cross-feature consistency between both global and local granularity (e.g., trajectory or local action), and in turn predict long-term global location and local dynamics of each individual. We validate our method on several public datasets with promising performance",
    "checked": true,
    "id": "ec4a673cebfd566b22ed98fe6f9dd6c930b83854",
    "semantic_title": "multiple granularity group interaction prediction",
    "citation_count": 19,
    "authors": [
      "Taiping Yao",
      "Minsi Wang",
      "Bingbing Ni",
      "Huawei Wei",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gupta_Social_GAN_Socially_CVPR_2018_paper.html": {
    "title": "Social GAN: Socially Acceptable Trajectories With Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity",
    "checked": true,
    "id": "49c076bbc21ab76720b610ab3840c15ce3dc4e6c",
    "semantic_title": "social gan: socially acceptable trajectories with generative adversarial networks",
    "citation_count": 1913,
    "authors": [
      "Agrim Gupta",
      "Justin Johnson",
      "Li Fei-Fei",
      "Silvio Savarese",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.html": {
    "title": "Deep Group-Shuffling Random Walk for Person Re-Identification",
    "volume": "main",
    "abstract": "Person re-identification aims at finding a person of interest in an image gallery by comparing the probe image of this person with all the gallery images. It is generally treated as a retrieval problem, where the affinities between the probe image and gallery images (P2G affinities) are used to rank the retrieved gallery images. However, most existing methods only consider P2G affinities but ignore the affinities between all the gallery images (G2G affinity). Some frameworks incorporated G2G affinities into the testing process, which is not end-to-end trainable for deep neural networks. In this paper, we propose a novel group-shuffling random walk network for fully utilizing the affinity information between gallery images in both the training and testing processes. The proposed approach aims at end-to-end refining the P2G affinities based on G2G affinity information with a simple yet effective matrix operation, which can be integrated into deep neural networks. Feature grouping and group shuffle are also proposed to apply rich supervisions for learning better person features. The proposed approach outperforms state-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets by large margins, which demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "b5a9f87daed5b3a4b0d0381dc2b2480766a581a3",
    "semantic_title": "deep group-shuffling random walk for person re-identification",
    "citation_count": 119,
    "authors": [
      "Yantao Shen",
      "Hongsheng Li",
      "Tong Xiao",
      "Shuai Yi",
      "Dapeng Chen",
      "Xiaogang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Transferable_Joint_Attribute-Identity_CVPR_2018_paper.html": {
    "title": "Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification",
    "volume": "main",
    "abstract": "Most existing person re-identification (re-id) methods require supervised model learning from a separate large set of pairwise labelled training data for every single camera pair. This significantly limits their scalability and usability in real-world large scale deployments with the need for performing re-id across many camera views. To address this scalability problem, we develop a novel deep learning method for transferring the labelled information of an existing dataset to a new unseen (unlabelled) target domain for person re-id without any supervised learning in the target domain. Specifically, we introduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for simultaneously learning an attribute-semantic and identitydiscriminative feature representation space transferrable to any new (unseen) target domain for re-id tasks without the need for collecting new labelled training data from the target domain (i.e. unsupervised learning in the target domain). Extensive comparative evaluations validate the superiority of this new TJ-AIDL model for unsupervised person re-id over a wide range of state-of- the-art methods on four challenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID",
    "checked": true,
    "id": "7d8d0b47db6078ad30ee09bd18ea4efe15cd1214",
    "semantic_title": "transferable joint attribute-identity deep learning for unsupervised person re-identification",
    "citation_count": 571,
    "authors": [
      "Jingya Wang",
      "Xiatian Zhu",
      "Shaogang Gong",
      "Wei Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Harmonious_Attention_Network_CVPR_2018_paper.html": {
    "title": "Harmonious Attention Network for Person Re-Identification",
    "volume": "main",
    "abstract": "Existing person re-identiﬁcation (re-id) methods either assume the availability of well-aligned person bounding box images as model input or rely on constrained attention selection mechanisms to calibrate misaligned images. They are therefore sub-optimal for re-id matching in arbitrarily aligned person images potentially with large human pose variations and unconstrained auto-detection errors. In this work, we show the advantages of jointly learning attention selection and feature representation in a Convolutional Neural Network (CNN) by maximising the complementary information of different levels of visual attention subject to re-id discriminative learning constraints. Speciﬁcally, we formulate a novel Harmonious Attention CNN (HA-CNN) model for joint learning of soft pixel attention and hard regional attention along with simultaneous optimisation of feature representations, dedicated to optimise person re-id in uncontrolled (misaligned) images. Extensive comparative evaluations validate the superiority of this new HACNN model for person re-id over a wide variety of state-of-the-art methods on three large-scale benchmarks including CUHK03, Market-1501, and DukeMTMC-ReID",
    "checked": true,
    "id": "1be42f20ff086a04092b4be73e105a318ffa4322",
    "semantic_title": "harmonious attention network for person re-identification",
    "citation_count": 1350,
    "authors": [
      "Wei Li",
      "Xiatian Zhu",
      "Shaogang Gong"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shi_Real-Time_Rotation-Invariant_Face_CVPR_2018_paper.html": {
    "title": "Real-Time Rotation-Invariant Face Detection With Progressive Calibration Networks",
    "volume": "main",
    "abstract": "Rotation-invariant face detection, i.e. detecting faces with arbitrary rotation-in-plane (RIP) angles, is widely required in unconstrained applications but still remains as a challenging task, due to the large variations of face appearances. Most existing methods compromise with speed or accuracy to handle the large RIP variations. To address this problem more efficiently, we propose Progressive Calibration Networks (PCN) to perform rotation-invariant face detection in a coarse-to-fine manner. PCN consists of three stages, each of which not only distinguishes the faces from non-faces, but also calibrates the RIP orientation of each face candidate to upright progressively. By dividing the calibration process into several progressive steps and only predicting coarse orientations in early stages, PCN can achieve precise and fast calibration. By performing binary classification of face vs. non-face with gradually decreasing RIP ranges, PCN can accurately detect faces with full $360^{circ}$ RIP angles. Such designs lead to a real-time rotation-invariant face detector. The experiments on multi-oriented FDDB and a challenging subset of WIDER FACE containing rotated faces in the wild show that our PCN achieves quite promising performance",
    "checked": true,
    "id": "544c5a8d30cd39b7797105b1e7c7e79264c1b1c3",
    "semantic_title": "real-time rotation-invariant face detection with progressive calibration networks",
    "citation_count": 100,
    "authors": [
      "Xuepeng Shi",
      "Shiguang Shan",
      "Meina Kan",
      "Shuzhe Wu",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Regression_Forests_CVPR_2018_paper.html": {
    "title": "Deep Regression Forests for Age Estimation",
    "volume": "main",
    "abstract": "Age estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is inhomogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with inhomogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by fixing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by fixing the split nodes, the leaf nodes are optimized by iterating a step-size free update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them",
    "checked": true,
    "id": "b17daa0bedfeb0502adf6638b8fe3a64eebb5696",
    "semantic_title": "deep regression forests for age estimation",
    "citation_count": 145,
    "authors": [
      "Wei Shen",
      "Yilu Guo",
      "Yan Wang",
      "Kai Zhao",
      "Bo Wang",
      "Alan L. Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Weakly-Supervised_Deep_Convolutional_CVPR_2018_paper.html": {
    "title": "Weakly-Supervised Deep Convolutional Neural Network Learning for Facial Action Unit Intensity Estimation",
    "volume": "main",
    "abstract": "Facial action unit (AU) intensity estimation plays an important role in affective computing and human-computer interaction. Recent works have introduced deep neural networks for AU intensity estimation, but they require a large amount of intensity annotations. AU annotation needs strong domain expertise and it is expensive to construct a large database to learn deep models. We propose a novel knowledge-based semi-supervised deep convolutional neural network for AU intensity estimation with extremely limited AU annotations. Only the intensity annotations of peak and valley frames in training sequences are needed. To provide additional supervision for model learning, we exploit naturally existing constraints on AUs, including relative appearance similarity, temporal intensity ordering, facial symmetry, and contrastive appearance difference. Experimental evaluations are performed on two public benchmark databases. With around 2% of intensity annotations in FERA 2015 and around 1% in DISFA for training, our method can achieve comparable or even better performance than the state-of-the-art methods which use 100% of intensity annotations in the training set",
    "checked": true,
    "id": "dd6bdc336a8cbcde80725922b637617b5abb26af",
    "semantic_title": "weakly-supervised deep convolutional neural network learning for facial action unit intensity estimation",
    "citation_count": 53,
    "authors": [
      "Yong Zhang",
      "Weiming Dong",
      "Bao-Gang Hu",
      "Qiang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pernici_Memory_Based_Online_CVPR_2018_paper.html": {
    "title": "Memory Based Online Learning of Deep Representations From Video Streams",
    "volume": "main",
    "abstract": "We present a novel online unsupervised method for face identity learning from video streams. The method exploits deep face descriptors together with a memory based learning mechanism that takes advantage of the temporal coherence of visual data. Specifically, we introduce a discriminative descriptor matching solution based on Reverse Nearest Neighbour and a forgetting strategy that detect redundant descriptors and discard them appropriately while time progresses. It is shown that the proposed learning procedure is asymptotically stable and can be effectively used in relevant applications like multiple face identification and tracking from unconstrained video streams. Experimental results show that the proposed method achieves comparable results in the task of multiple face tracking and better performance in face identification with offline approaches exploiting future information. Code will be publicly available",
    "checked": true,
    "id": "e4cf6a559b4002b650dde376005a64b064963b19",
    "semantic_title": "memory based online learning of deep representations from video streams",
    "citation_count": 30,
    "authors": [
      "Federico Pernici",
      "Federico Bartoli",
      "Matteo Bruni",
      "Alberto Del Bimbo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Guo_Efficient_and_Deep_CVPR_2018_paper.html": {
    "title": "Efficient and Deep Person Re-Identification Using Multi-Level Similarity",
    "volume": "main",
    "abstract": "Person Re-Identification (ReID) requires comparing two images of person captured under different conditions. Existing work based on neural networks often computes the similarity of feature maps from one single convolutional layer. In this work, we propose an efficient, end-to-end fully convolutional Siamese network that computes the similarities at multiple levels. We demonstrate that multi-level similarity can improve the accuracy considerably using low-complexity network structures in ReID problem. Specifically, first, we use several convolutional layers to extract the features of two input images. Then, we propose Convolution Similarity Network to compute the similarity score maps for the inputs. We use spatial transformer networks (STNs) to determine spatial attention. We propose to apply efficient depth-wise convolution to compute the similarity. The proposed Convolution Similarity Networks can be inserted into different convolutional layers to extract visual similarities at different levels. Furthermore, we use an improved ranking loss to further improve the performance. Our work is the first to propose to compute visual similarities at low, middle and high levels for ReID. With extensive experiments and analysis, we demonstrate that our system, compact yet effective, can achieve competitive results with much smaller model size and computational complexity",
    "checked": true,
    "id": "3f872e36f5b455c97aa620ce52c447460c6b3360",
    "semantic_title": "efficient and deep person re-identification using multi-level similarity",
    "citation_count": 96,
    "authors": [
      "Yiluan Guo",
      "Ngai-Man Cheung"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Multi-Level_Fusion_Based_CVPR_2018_paper.html": {
    "title": "Multi-Level Fusion Based 3D Object Detection From Monocular Images",
    "volume": "main",
    "abstract": "In this paper, we present an end-to-end deep learning based framework for 3D object detection from a single monocular image. A deep convolutional neural network is introduced for simultaneous 2D and 3D object detection. First, 2D region proposals are generated through a region proposal network. Then the shared features are learned within the proposals to predict the class probability, 2D bounding box, orientation, dimension, and 3D location. We adopt a stand-alone module to predict the disparity and extract features from the computed point cloud. Thus features from the original image and the point cloud will be fused in different levels for accurate 3D localization. The estimated disparity is also used for front view feature encoding to enhance the input image,regarded as an input-fusionprocess. The proposed algorithm can directly output both 2D and 3D object detection results in an end-to-end fashion with only a single RGB image as the input. The experimental results on the challenging KITTI benchmark demonstrate that our algorithm signiﬁcantly outperforms the state-of-the-art methods with only monocular images",
    "checked": true,
    "id": "bf2793fc09176f8bf23b3a2b3c6b32185e8a8329",
    "semantic_title": "multi-level fusion based 3d object detection from monocular images",
    "citation_count": 302,
    "authors": [
      "Bin Xu",
      "Zhenzhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hold-Geoffroy_A_Perceptual_Measure_CVPR_2018_paper.html": {
    "title": "A Perceptual Measure for Deep Single Image Camera Calibration",
    "volume": "main",
    "abstract": "Most current single image camera calibration methods rely on specific image features or user input, and cannot be applied to natural images captured in uncontrolled settings. We propose inferring directly camera calibration parameters from a single image using a deep convolutional neural network. This network is trained using automatically generated samples from a large-scale panorama dataset, and considerably outperforms other methods, including recent deep learning-based approaches, in terms of standard L2 error. However, we argue that in many cases it is more important to consider how humans perceive errors in camera estimation. To this end, we conduct a large-scale human perception study where we ask users to judge the realism of 3D objects composited with and without ground truth camera calibration. Based on this study, we develop a new perceptual measure for camera calibration, and demonstrate that our deep calibration network outperforms other methods on this measure. Finally, we demonstrate the use of our calibration network for a number of applications including virtual object insertion, image retrieval and compositing",
    "checked": true,
    "id": "cf87f285693e7ab518b50abfa93b09556107a4a4",
    "semantic_title": "a perceptual measure for deep single image camera calibration",
    "citation_count": 106,
    "authors": [
      "Yannick Hold-Geoffroy",
      "Kalyan Sunkavalli",
      "Jonathan Eisenmann",
      "Matthew Fisher",
      "Emiliano Gambaretto",
      "Sunil Hadap",
      "Jean-François Lalonde"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xiong_Learning_to_Generate_CVPR_2018_paper.html": {
    "title": "Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Taking a photo outside, can we predict the immediate future, e.g., how would the cloud move in the sky? We address this problem by presenting a generative adversarial network (GAN) based two-stage approach to generating realistic time-lapse videos of high resolution. Given the first frame, our model learns to generate long-term future frames. The first stage generates videos of realistic contents for each frame. The second stage refines the generated video from the first stage by enforcing it to be closer to real videos with regard to motion dynamics. To further encourage vivid motion in the final generated video, Gram matrix is employed to model the motion more precisely. We build a large scale time-lapse dataset, and test our approach on this new dataset. Using our model, we are able to generate realistic videos of up to $128 imes 128$ resolution for 32 frames. Quantitative and qualitative experiment results have demonstrated the superiority of our model over the state-of-the-art models",
    "checked": true,
    "id": "87a818723a2ada66a1193baf17b0383d9766781b",
    "semantic_title": "learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks",
    "citation_count": 181,
    "authors": [
      "Wei Xiong",
      "Wenhan Luo",
      "Lin Ma",
      "Wei Liu",
      "Jiebo Luo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kligler_Document_Enhancement_Using_CVPR_2018_paper.html": {
    "title": "Document Enhancement Using Visibility Detection",
    "volume": "main",
    "abstract": "This paper re-visits classical problems in document enhancement. Rather than proposing a new algorithm for a specific problem, we introduce a novel general approach. The key idea is to modify any state- of-the-art algorithm, by providing it with new information (input), improving its own results. Interestingly, this information is based on a solution to a seemingly unrelated problem of visibility detection in R3. We show that a simple representation of an image as a 3D point cloud, gives visibility detection on this cloud a new interpretation. What does it mean for a point to be visible? Although this question has been widely studied within computer vision, it has always been assumed that the point set is a sampling of a real scene. We show that the answer to this question in our context reveals unique and useful information about the image. We demonstrate the benefit of this idea for document binarization and for unshadowing",
    "checked": true,
    "id": "23d05ba7b8ebdff14c6eae7cf679932e38980e97",
    "semantic_title": "document enhancement using visibility detection",
    "citation_count": 59,
    "authors": [
      "Netanel Kligler",
      "Sagi Katz",
      "Ayellet Tal"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Silva_A_Weighted_Sparse_CVPR_2018_paper.html": {
    "title": "A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos",
    "volume": "main",
    "abstract": "Thanks to the advances in the technology of low-cost digital cameras and the popularity of the self-recording culture, the amount of visual data on the Internet is going to the opposite side of the available time and patience of the users. Thus, most of the uploaded videos are doomed to be forgotten and unwatched in a computer folder or website. In this work, we address the problem of creating smooth fast-forward videos without losing the relevant content. We present a new adaptive frame selection formulated as a weighted minimum reconstruction problem, which combined with a smoothing frame transition method accelerates first-person videos emphasizing the relevant segments and avoids visual discontinuities. The experiments show that our method is able to fast-forward videos to retain as much relevant information and smoothness as the state-of-the-art techniques in less time. We also present a new 80-hour multimodal (RGB-D, IMU, and GPS) dataset of first-person videos with annotations for recorder profile, frame scene, activities, interaction, and attention",
    "checked": true,
    "id": "14e9ee09765fcb99fd6ad2fd7360a90c94c9b5e2",
    "semantic_title": "a weighted sparse sampling and smoothing frame transition approach for semantic fast-forward first-person videos",
    "citation_count": 32,
    "authors": [
      "Michel Silva",
      "Washington Ramos",
      "João Ferreira",
      "Felipe Chamone",
      "Mario Campos",
      "Erickson R. Nascimento"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ding_Context_Contrasted_Feature_CVPR_2018_paper.html": {
    "title": "Context Contrasted Feature and Gated Multi-Scale Aggregation for Scene Segmentation",
    "volume": "main",
    "abstract": "Scene segmentation is a challenging task as it need label every pixel in the image. It is crucial to exploit discriminative context and aggregate multi-scale features to achieve better segmentation. In this paper, we first propose a novel context contrasted local feature that not only leverages the informative context but also spotlights the local information in contrast to the context. The proposed context contrasted local feature greatly improves the parsing performance, especially for inconspicuous objects and background stuff. Furthermore, we propose a scheme of gated sum to selectively aggregate multi-scale features for each spatial position. The gates in this scheme control the information flow of different scale features. Their values are generated from the testing image by the proposed network learnt from the training data so that they are adaptive not only to the training data, but also to the specific testing image. Without bells and whistles, the proposed approach achieves the state-of-the-arts consistently on the three popular scene segmentation datasets, Pascal Context, SUN-RGBD and COCO Stuff",
    "checked": true,
    "id": "ea743597a5f48babef1982259566d76a9bf66bf2",
    "semantic_title": "context contrasted feature and gated multi-scale aggregation for scene segmentation",
    "citation_count": 359,
    "authors": [
      "Henghui Ding",
      "Xudong Jiang",
      "Bing Shuai",
      "Ai Qun Liu",
      "Gang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.html": {
    "title": "Deep Layer Aggregation",
    "volume": "main",
    "abstract": "Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been ``shallow'' themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes",
    "checked": true,
    "id": "37c5a90619eb4bfd7c41b674e4fdf2317622e0f7",
    "semantic_title": "deep layer aggregation",
    "citation_count": 1327,
    "authors": [
      "Fisher Yu",
      "Dequan Wang",
      "Evan Shelhamer",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Convolutional_Neural_Networks_CVPR_2018_paper.html": {
    "title": "Convolutional Neural Networks With Alternately Updated Clique",
    "volume": "main",
    "abstract": "Improving information flow in deep networks helps to ease the training difficulties and utilize parameters more efficiently. Here we propose a new convolutional neural network architecture with alternately updated clique (CliqueNet). In contrast to prior networks, there are both forward and backward connections between any two layers in the same block. The layers are constructed as a loop and are updated alternately. The CliqueNet has some unique properties. For each layer, it is both the input and output of any other layer in the same block, so that the information flow among layers is maximized. During propagation, the newly updated layers are concatenated to re-update previously updated layer, and parameters are reused for multiple times. This recurrent feedback structure is able to bring higher level visual information back to refine low-level filters and achieve spatial attention. We analyze the features generated at different stages and observe that using refined features leads to a better result. We adopt a multi-scale feature strategy that effectively avoids the progressive growth of parameters. Experiments on image recognition datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet show that our proposed models achieve the state-of-the-art performance with fewer parameters",
    "checked": true,
    "id": "aa46c05868d6ccb404f4f6378cc11ae77f702d45",
    "semantic_title": "convolutional neural networks with alternately updated clique",
    "citation_count": 128,
    "authors": [
      "Yibo Yang",
      "Zhisheng Zhong",
      "Tiancheng Shen",
      "Zhouchen Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhong_Practical_Block-Wise_Neural_CVPR_2018_paper.html": {
    "title": "Practical Block-Wise Neural Network Architecture Generation",
    "volume": "main",
    "abstract": "Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy.The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classification, additionally, the best network generated by BlockQNN achieves 3.54% top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset",
    "checked": true,
    "id": "8a1ce657dd41a4f49990a4769000dc8049b83404",
    "semantic_title": "practical block-wise neural network architecture generation",
    "citation_count": 505,
    "authors": [
      "Zhao Zhong",
      "Junjie Yan",
      "Wei Wu",
      "Jing Shao",
      "Cheng-Lin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kligvasser_xUnit_Learning_a_CVPR_2018_paper.html": {
    "title": "xUnit: Learning a Spatial Activation Function for Efficient Image Restoration",
    "volume": "main",
    "abstract": "In recent years, deep neural networks (DNNs) achieved unprecedented performance in many low-level vision tasks. However, state-of-the-art results are typically achieved by very deep networks, which can reach tens of layers with tens of millions of parameters. To make DNNs implementable on platforms with limited resources, it is necessary to weaken the tradeoff between performance and efficiency. In this paper, we propose a new activation unit, which is particularly suitable for image restoration problems. In contrast to the widespread per-pixel activation units, like ReLUs and sigmoids, our unit implements a learnable nonlinear function with spatial connections. This enables the net to capture much more complex features, thus requiring a significantly smaller number of layers in order to reach the same performance. We illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising, de-raining, and super resolution, which are already considered to be very small. With our approach, we are able to further reduce these models by nearly 50% without incurring any degradation in performance",
    "checked": true,
    "id": "a184f6878139065b7854752db0c34e539143cd3e",
    "semantic_title": "xunit: learning a spatial activation function for efficient image restoration",
    "citation_count": 58,
    "authors": [
      "Idan Kligvasser",
      "Tamar Rott Shaham",
      "Tomer Michaeli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Crafting_a_Toolchain_CVPR_2018_paper.html": {
    "title": "Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "We investigate a novel approach for image restoration by reinforcement learning. Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks. Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image. We formulate a step-wise reward function proportional to how well the image is restored at each step to learn the action policy. We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty. In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain",
    "checked": true,
    "id": "9fb3707a0f90c6620251d202a972a2c626dce976",
    "semantic_title": "crafting a toolchain for image restoration by deep reinforcement learning",
    "citation_count": 176,
    "authors": [
      "Ke Yu",
      "Chao Dong",
      "Liang Lin",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shaham_Deformation_Aware_Image_CVPR_2018_paper.html": {
    "title": "Deformation Aware Image Compression",
    "volume": "main",
    "abstract": "Lossy compression algorithms aim to compactly encode images in a way which enables to restore them with minimal error. We show that a key limitation of existing algorithms is that they rely on error measures that are extremely sensitive to geometric deformations (e.g. SSD, SSIM). These force the encoder to invest many bits in describing the exact geometry of every fine detail in the image, which is obviously wasteful, because the human visual system is indifferent to small local translations. Motivated by this observation, we propose a deformation-insensitive error measure that can be easily incorporated into any existing compression scheme. As we show, optimal compression under our criterion involves slightly deforming the input image such that it becomes more \"compressible\". Surprisingly, while these small deformations are barely noticeable, they enable the CODEC to preserve details that are otherwise completely lost. Our technique uses the CODEC as a \"black box\", thus allowing simple integration with arbitrary compression methods. Extensive experiments, including user studies, confirm that our approach significantly improves the visual quality of many CODECs. These include JPEG, JPEG~2000, WebP, BPG, and a recent deep-net method",
    "checked": true,
    "id": "6a7b31b62fb226dfea7d41ee51a5391ad1b68692",
    "semantic_title": "deformation aware image compression",
    "citation_count": 15,
    "authors": [
      "Tamar Rott Shaham",
      "Tomer Michaeli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Distributable_Consistent_Multi-Object_CVPR_2018_paper.html": {
    "title": "Distributable Consistent Multi-Object Matching",
    "volume": "main",
    "abstract": "In this paper we propose an optimization-based framework to multiple object matching. The framework takes maps computed between pairs of objects as input, and outputs maps that are consistent among all pairs of objects. The central idea of our approach is to divide the input object collection into overlapping sub-collections and enforce map consistency among each sub-collection. This leads to a distributed formulation, which is scalable to large-scale datasets. We also present an equivalence condition between this decoupled scheme and the original scheme. Experiments on both synthetic and real-world datasets show that our framework is competitive against state-of-the-art multi-object matching techniques",
    "checked": true,
    "id": "e74b7a51f941cfea567b5b6e189f4a56273edebb",
    "semantic_title": "distributable consistent multi-object matching",
    "citation_count": 28,
    "authors": [
      "Nan Hu",
      "Qixing Huang",
      "Boris Thibert",
      "Leonidas J. Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Residual_Dense_Network_CVPR_2018_paper.html": {
    "title": "Residual Dense Network for Image Super-Resolution",
    "volume": "main",
    "abstract": "In this paper, we propose dense feature fusion (DFF) for image super-resolution (SR). As the same content in different natural images often have various scales and angles of view, jointly leaning hierarchical features is essential for image SR. On the other hand, very deep convolutional neural network (CNN) has recently achieved great success for image SR and offered hierarchical features as well. However, most of deep CNN based SR models neglect to jointly make full use of the hierarchical features. In addition, dense connected layers would allow the network to be deeper, efficient to train, and more powerful. To embrace these observations, in our proposed DFF model, we fully exploit all the meaningful convolutional features in local and global manners. Specifically, we use dense connected convolutional layers to extract abundant local features. We use local feature fusion to adaptively learn more efficient features from preceding and current local features. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Extensive experiments on benchmark datasets show that our DFF achieves favorable performance against state-of-the-art methods quantitatively and visually",
    "checked": true,
    "id": "4ef1476dec02c62227187edbba88615278b3edba",
    "semantic_title": "residual dense network for image super-resolution",
    "citation_count": 3318,
    "authors": [
      "Yulun Zhang",
      "Yapeng Tian",
      "Yu Kong",
      "Bineng Zhong",
      "Yun Fu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qian_Attentive_Generative_Adversarial_CVPR_2018_paper.html": {
    "title": "Attentive Generative Adversarial Network for Raindrop Removal From a Single Image",
    "volume": "main",
    "abstract": "Raindrops adhered to a glass window or camera lens can severely hamper the visibility of a background scene and degrade an image considerably. In this paper, we address the problem by visually removing raindrops, and thus transforming a raindrop degraded image into a clean one. The problem is intractable, since first the regions occluded by raindrops are not given. Second, the information about the background scene of the occluded regions is completely lost for most part. To resolve the problem, we apply an attentive generative network using adversarial training. Our main idea is to inject visual attention into both the generative and discriminative networks. During the training, our visual attention learns about raindrop regions and their surroundings. Hence, by injecting this information, the generative network will pay more attention to the raindrop regions and the surrounding structures, and the discriminative network will be able to assess the local consistency of the restored regions. This injection of visual attention to both generative and discriminative networks is the main contribution of this paper. Our experiments show the effectiveness of our approach, which outperforms the state of the art methods quantitatively and qualitatively",
    "checked": true,
    "id": "34e38559df559bfedfc3e1edc2b120ab2b5d444e",
    "semantic_title": "attentive generative adversarial network for raindrop removal from a single image",
    "citation_count": 635,
    "authors": [
      "Rui Qian",
      "Robby T. Tan",
      "Wenhan Yang",
      "Jiajun Su",
      "Jiaying Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html": {
    "title": "FSRNet: End-to-End Learning Face Super-Resolution With Facial Priors",
    "volume": "main",
    "abstract": "Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to superresolve very low-resolution (LR) face images without wellaligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively",
    "checked": true,
    "id": "1d49ee6afb82b5fbacf69628f52e76dc33341643",
    "semantic_title": "fsrnet: end-to-end learning face super-resolution with facial priors",
    "citation_count": 481,
    "authors": [
      "Yu Chen",
      "Ying Tai",
      "Xiaoming Liu",
      "Chunhua Shen",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mildenhall_Burst_Denoising_With_CVPR_2018_paper.html": {
    "title": "Burst Denoising With Kernel Prediction Networks",
    "volume": "main",
    "abstract": "We present a technique for jointly denoising bursts of images taken from a handheld camera. In particular, we propose a convolutional neural network architecture for predicting spatially varying kernels that can both align and denoise frames, a synthetic data generation approach based on a realistic noise formation model, and an optimization guided by an annealed loss function to avoid undesirable local minima. Our model matches or outperforms the state-of-the-art across a wide range of noise levels on both real and synthetic data",
    "checked": true,
    "id": "84fc681efa7a7030098daf0c122ce6b770351c85",
    "semantic_title": "burst denoising with kernel prediction networks",
    "citation_count": 395,
    "authors": [
      "Ben Mildenhall",
      "Jonathan T. Barron",
      "Jiawen Chen",
      "Dillon Sharlet",
      "Ren Ng",
      "Robert Carroll"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qu_Unsupervised_Sparse_Dirichlet-Net_CVPR_2018_paper.html": {
    "title": "Unsupervised Sparse Dirichlet-Net for Hyperspectral Image Super-Resolution",
    "volume": "main",
    "abstract": "In many computer vision applications, obtaining images of high resolution in both the spatial and spectral domains are equally important. However, due to hardware limitations, one can only expect to acquire images of high resolution in either the spatial or spectral domains. This paper focuses on hyperspectral image super-resolution (HSI-SR), where a hyperspectral image (HSI) with low spatial resolution (LR) but high spectral resolution is fused with a multispectral image (MSI) with high spatial resolution (HR) but low spectral resolution to obtain HR HSI. Existing deep learning-based solutions are all supervised that would need a large training set and the availability of HR HSI, which is unrealistic. Here, we make the first attempt to solving the HSI-SR problem using an unsupervised encoder-decoder architecture that carries the following uniquenesses. First, it is composed of two encoder-decoder networks, coupled through a shared decoder, in order to preserve the rich spectral information from the HSI network. Second, the network encourages the representations from both modalities to follow a sparse Dirichlet distribution which naturally incorporates the two physical constraints of HSI and MSI. Third, the angular difference between representations are minimized in order to reduce the spectral distortion. We refer to the proposed architecture as unsupervised Sparse Dirichlet-Net, or uSDN. Extensive experimental results demonstrate the superior performance of uSDN as compared to the state-of-the-art",
    "checked": true,
    "id": "02e7010c0520f40ca8e51f4622657aa6f7e332d4",
    "semantic_title": "unsupervised sparse dirichlet-net for hyperspectral image super-resolution",
    "citation_count": 206,
    "authors": [
      "Ying Qu",
      "Hairong Qi",
      "Chiman Kwan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Dynamic_Scene_Deblurring_CVPR_2018_paper.html": {
    "title": "Dynamic Scene Deblurring Using Spatially Variant Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Due to the spatially variant blur caused by camera shake and object motions under different scene depths, deblurring images captured from dynamic scenes is challenging. Although recent works based on deep neural networks have shown great progress on this problem, their models are usually large and computationally expensive. In this paper, we propose a novel spatially variant neural network to address the problem. The proposed network is composed of three deep convolutional neural networks (CNNs) and a recurrent neural network (RNN). RNN is used as a deconvolution operator performed on feature maps extracted from the input image by one of the CNNs. Another CNN is used to learn the weights for the RNN at every location. As a result, the RNN is spatially variant and could implicitly model the deblurring process with spatially variant kernels. The third CNN is used to reconstruct the final deblurred feature maps into restored image. The whole network is end-to-end trainable. Our analysis shows that the proposed network has a large receptive field even with a small model size. Quantitative and qualitative evaluations on public datasets demonstrate that the proposed method performs favorably against state-of-the-art algorithms in terms of accuracy, speed, and model size",
    "checked": true,
    "id": "79b0ee6a2aeca169786ab6a4952bbffd311757a8",
    "semantic_title": "dynamic scene deblurring using spatially variant recurrent neural networks",
    "citation_count": 345,
    "authors": [
      "Jiawei Zhang",
      "Jinshan Pan",
      "Jimmy Ren",
      "Yibing Song",
      "Linchao Bao",
      "Rynson W.H. Lau",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.html": {
    "title": "SPLATNet: Sparse Lattice Networks for Point Cloud Processing",
    "volume": "main",
    "abstract": "We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Naively applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques",
    "checked": true,
    "id": "9ebdd272db2596f4f4bee741d6399363b2b6559f",
    "semantic_title": "splatnet: sparse lattice networks for point cloud processing",
    "citation_count": 750,
    "authors": [
      "Hang Su",
      "Varun Jampani",
      "Deqing Sun",
      "Subhransu Maji",
      "Evangelos Kalogerakis",
      "Ming-Hsuan Yang",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kostrikov_Surface_Networks_CVPR_2018_paper.html": {
    "title": "Surface Networks",
    "volume": "main",
    "abstract": "We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs, namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the Laplacian operator. Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric deformations, making it unsuitable for many applications. To overcome this limitation, we propose several upgrades to GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions --- this is in stark contrast with the classical Laplace operator, which directly measures mean curvature. We coin the resulting models emph{Surface Networks (SN)}. We prove that these models define shape representations that are stable to deformation and to discretization, and we demonstrate the efficiency and versatility of SNs on two challenging tasks: temporal prediction of mesh deformations under non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by SNs",
    "checked": true,
    "id": "000e210c806721db5c6d3e2c72a83d91e084d5a4",
    "semantic_title": "surface networks",
    "citation_count": 100,
    "authors": [
      "Ilya Kostrikov",
      "Zhongshi Jiang",
      "Daniele Panozzo",
      "Denis Zorin",
      "Joan Bruna"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tewari_Self-Supervised_Multi-Level_Face_CVPR_2018_paper.html": {
    "title": "Self-Supervised Multi-Level Face Model Learning for Monocular Reconstruction at Over 250 Hz",
    "volume": "main",
    "abstract": "The reconstruction of dense 3D models of face geometry and appearance from a single image is highly challenging and ill-posed. To constrain the problem, many approaches rely on strong priors, such as parametric face models learned from limited 3D scan data. However, prior models restrict generalization of the true diversity in facial geometry, skin reflectance and illumination. To alleviate this problem, we present the first approach that jointly learns 1) a regressor for face shape, expression, reflectance and illumination on the basis of 2) a concurrently learned parametric face model. Our multi-level face model combines the advantage of 3D Morphable Models for regularization with the out-of-space generalization of a learned corrective space. We train end-to-end on in-the-wild images without dense annotations by fusing a convolutional encoder with a differentiable expert-designed renderer and a self-supervised training loss, both defined at multiple detail levels. Our approach compares favorably to the state-of-the-art in terms of reconstruction quality, better generalizes to real world faces, and runs at over 250Hz",
    "checked": true,
    "id": "3f7a18a33eecf82630fdebe4c62899eccb359f42",
    "semantic_title": "self-supervised multi-level face model learning for monocular reconstruction at over 250 hz",
    "citation_count": 264,
    "authors": [
      "Ayush Tewari",
      "Michael Zollhöfer",
      "Pablo Garrido",
      "Florian Bernard",
      "Hyeongwoo Kim",
      "Patrick Pérez",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bloesch_CodeSLAM_--_Learning_CVPR_2018_paper.html": {
    "title": "CodeSLAM — Learning a Compact, Optimisable Representation for Dense Visual SLAM",
    "volume": "main",
    "abstract": "The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only. We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM",
    "checked": true,
    "id": "ebf6ca059b61267e5745127ce0553aa7cecedf4d",
    "semantic_title": "codeslam - learning a compact, optimisable representation for dense visual slam",
    "citation_count": 373,
    "authors": [
      "Michael Bloesch",
      "Jan Czarnowski",
      "Ronald Clark",
      "Stefan Leutenegger",
      "Andrew J. Davison"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_SGPN_Similarity_Group_CVPR_2018_paper.html": {
    "title": "SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation",
    "volume": "main",
    "abstract": "We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance",
    "checked": true,
    "id": "ae8370d1010aaa44c685d64fd40d730c3197d44f",
    "semantic_title": "sgpn: similarity group proposal network for 3d point cloud instance segmentation",
    "citation_count": 552,
    "authors": [
      "Weiyue Wang",
      "Ronald Yu",
      "Qiangui Huang",
      "Ulrich Neumann"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper.html": {
    "title": "PlaneNet: Piece-Wise Planar Reconstruction From a Single RGB Image",
    "volume": "main",
    "abstract": "This paper proposes a deep neural network (DNN) for piece-wise planar depthmap reconstruction from a single RGB image. While DNNs have brought remarkable progress to single-image pixel-wise depth prediction, piece-wise planar depthmap reconstruction requires a structured geometry representation, and has been a difficult task to master even for DNNs. The proposed end-to-end DNN learns to directly infer a set of plane parameters and corresponding plane segmentation masks from a single RGB image. We have generated more than 50,000 piece-wise planar depth maps for training and testing from ScanNet, a large-scale indoor capture database. Our qualitative and quantitative evaluations demonstrate that the proposed approach outperforms baseline methods in terms of both plane segmentation and depth estimation accuracy. To the best of our knowledge, this paper presents the first end-to-end neural architecture for piece-wise planar reconstruction from a single RGB image",
    "checked": true,
    "id": "b924dfd212848d248840960abb07e2cfaa80cea3",
    "semantic_title": "planenet: piece-wise planar reconstruction from a single rgb image",
    "citation_count": 198,
    "authors": [
      "Chen Liu",
      "Jimei Yang",
      "Duygu Ceylan",
      "Ersin Yumer",
      "Yasutaka Furukawa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.html": {
    "title": "Deep Parametric Continuous Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Standard convolutional neural networks assume a grid structured input is available and exploit discrete convolutions as their fundamental building blocks. This limits their applicability to many real-world applications. In this paper we propose Parametric Continuous Convolution, a new learnable operator that operates over non-grid structured data. The key idea is to exploit parameterized kernel functions that span the full continuous vector space. This generalization allows us to learn over arbitrary data structures as long as their support relationship is computable. Our experiments show significant improvement over the state-of-the-art in point cloud segmentation of indoor and outdoor scenes, and lidar motion estimation of driving scenes",
    "checked": true,
    "id": "7bbcd665f4847652e307ca717b209de92cd95dd2",
    "semantic_title": "deep parametric continuous convolutional neural networks",
    "citation_count": 467,
    "authors": [
      "Shenlong Wang",
      "Simon Suo",
      "Wei-Chiu Ma",
      "Andrei Pokrovsky",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Verma_FeaStNet_Feature-Steered_Graph_CVPR_2018_paper.html": {
    "title": "FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNNs) have massively impacted visual recognition in 2D images, and are now ubiquitous in state-of-the-art approaches. CNNs do not easily extend, however, to data that are not represented by regular grids, such as 3D shape meshes or other graph-structured data, to which traditional local convolution operators do not directly apply. To address this problem, we propose a novel graph-convolution operator to establish correspondences between filter weights and graph neighborhoods with arbitrary connectivity. The key novelty of our approach is that these correspondences are dynamically computed from features learned by the network, rather than relying on predefined static coordinates over the graph as in previous work. We obtain excellent experimental results that significantly improve over previous state-of-the-art shape correspondence results. This shows that our approach can learn effective shape representations from raw input coordinates, without relying on shape descriptors",
    "checked": true,
    "id": "ba2a33107afba54adc2d92aa7ad9f747d5258e95",
    "semantic_title": "feastnet: feature-steered graph convolutions for 3d shape analysis",
    "citation_count": 337,
    "authors": [
      "Nitika Verma",
      "Edmond Boyer",
      "Jakob Verbeek"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Agudo_Image_Collection_Pop-Up_CVPR_2018_paper.html": {
    "title": "Image Collection Pop-Up: 3D Reconstruction and Clustering of Rigid and Non-Rigid Categories",
    "volume": "main",
    "abstract": "This paper introduces an approach to simultaneously estimate 3D shape, camera pose, and object and type of deformation clustering, from partial 2D annotations in a multi-instance collection of images. Furthermore, we can indistinctly process rigid and non-rigid categories. This advances existing work, which only addresses the problem for one single object or, if multiple objects are considered, they are assumed to be clustered a priori. To handle this broader version of the problem, we model object deformation using a formulation based on multiple unions of subspaces, able to span from small rigid motion to complex deformations. The parameters of this model are learned via Augmented Lagrange Multipliers, in a completely unsupervised manner that does not require any training data at all. Extensive validation is provided in a wide variety of synthetic and real scenarios, including rigid and non-rigid categories with small and large deformations. In all cases our approach outperforms state-of-the-art in terms of 3D reconstruction accuracy, while also providing clustering results that allow segmenting the images into object instances and their associated type of deformation (or action the object is performing)",
    "checked": true,
    "id": "d8e1fd0613a471b1d662e88f9f59b121bdb4198f",
    "semantic_title": "image collection pop-up: 3d reconstruction and clustering of rigid and non-rigid categories",
    "citation_count": 31,
    "authors": [
      "Antonio Agudo",
      "Melcior Pijoan",
      "Francesc Moreno-Noguer"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Brahmbhatt_Geometry-Aware_Learning_of_CVPR_2018_paper.html": {
    "title": "Geometry-Aware Learning of Maps for Camera Localization",
    "volume": "main",
    "abstract": "Maps are a key component in image-based camera localization and visual SLAM systems: they are used to establish geometric constraints between images, correct drift in relative pose estimation, and relocalize cameras after lost tracking. The exact definitions of maps, however, are often application-specific and hand-crafted for different scenarios (e.g. 3D landmarks, lines, planes, bags of visual words). We propose to represent maps as a deep neural net called MapNet, which enables learning a data-driven map representation. Unlike prior work on learning maps, MapNet exploits cheap and ubiquitous sensory inputs like visual odometry and GPS in addition to images and fuses them together for camera localization. Geometric constraints expressed by these inputs, which have traditionally been used in bundle adjustment or pose-graph optimization, are formulated as loss terms in MapNet training and also used during inference. In addition to directly improving localization accuracy, this allows us to update the MapNet (i.e., maps) in a self-supervised manner using additional unlabeled video sequences from the scene. We also propose a novel parameterization for camera rotation which is better suited for deep-learning based camera pose regression. Experimental results on both the indoor 7-Scenes and the outdoor Oxford RobotCar datasets show significant improvement over prior work. The MapNet project webpage is https://goo.gl/mRB3Au",
    "checked": true,
    "id": "8c0514cbf116afc4363152514b3ccedb05195a59",
    "semantic_title": "geometry-aware learning of maps for camera localization",
    "citation_count": 388,
    "authors": [
      "Samarth Brahmbhatt",
      "Jinwei Gu",
      "Kihwan Kim",
      "James Hays",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Recurrent_Slice_Networks_CVPR_2018_paper.html": {
    "title": "Recurrent Slice Networks for 3D Segmentation of Point Clouds",
    "volume": "main",
    "abstract": "Point clouds are an efficient data format for 3D data. However, existing 3D segmentation methods for point clouds either do not model local dependencies or require added computations. This work presents a novel 3D segmentation framework, RSNet, to efficiently model local structures in point clouds. The key component of the RSNet is a lightweight local dependency module. It is a combination of a novel slice pooling layer, Recurrent Neural Network (RNN) layers, and a slice unpooling layer. The slice pooling layer is designed to project features of unordered points onto an ordered sequence of feature vectors so that traditional end-to-end learning algorithms (RNNs) can be applied. The performance of RSNet is validated by comprehensive experiments on the S3DIS, ScanNet, and ShapeNet datasets. In its simplest form, RSNets surpass all previous state-of-the-art methods on these benchmarks. And comparisons against previous state-of-the-art methods demonstrate the efficiency of RSNets",
    "checked": true,
    "id": "2a33d0939cb4f9200d401a453146bb120ee3b403",
    "semantic_title": "recurrent slice networks for 3d segmentation of point clouds",
    "citation_count": 455,
    "authors": [
      "Qiangui Huang",
      "Weiyue Wang",
      "Ulrich Neumann"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yuan_Depth-Based_3D_Hand_CVPR_2018_paper.html": {
    "title": "Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future Goals",
    "volume": "main",
    "abstract": "In this paper, we strive to answer two questions: What is the current state of 3D hand pose estimation from depth images? And, what are the next challenges that need to be tackled? Following the successful Hands In the Million Challenge (HIM2017), we investigate the top 10 state-of-the-art methods on three tasks: single frame 3D pose estimation, 3D hand tracking, and hand pose estimation during object interaction. We analyze the performance of different CNN structures with regard to hand shape, joint visibility, view point and articulation distributions. Our findings include: (1) isolated 3D hand pose estimation achieves low mean errors (10 mm) in the view point range of [70, 120] degrees, but it is far from being solved for extreme view points; (2) 3D volumetric representations outperform 2D CNNs, better capturing the spatial structure of the depth data; (3) Discriminative methods still generalize poorly to unseen hand shapes; (4) While joint occlusions pose a challenge for most methods, explicit modeling of structure constraints can significantly narrow the gap between errors on visible and occluded joints",
    "checked": true,
    "id": "15909f3d1a1860a9b29906a1b58a6d5eef47182f",
    "semantic_title": "3d hand pose estimation: from current achievements to future goals",
    "citation_count": 201,
    "authors": [
      "Shanxin Yuan",
      "Guillermo Garcia-Hernando",
      "Björn Stenger",
      "Gyeongsik Moon",
      "Ju Yong Chang",
      "Kyoung Mu Lee",
      "Pavlo Molchanov",
      "Jan Kautz",
      "Sina Honari",
      "Liuhao Ge",
      "Junsong Yuan",
      "Xinghao Chen",
      "Guijin Wang",
      "Fan Yang",
      "Kai Akiyama",
      "Yang Wu",
      "Qingfu Wan",
      "Meysam Madadi",
      "Sergio Escalera",
      "Shile Li",
      "Dongheui Lee",
      "Iason Oikonomidis",
      "Antonis Argyros",
      "Tae-Kyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Slavcheva_SobolevFusion_3D_Reconstruction_CVPR_2018_paper.html": {
    "title": "SobolevFusion: 3D Reconstruction of Scenes Undergoing Free Non-Rigid Motion",
    "volume": "main",
    "abstract": "We present a system that builds 3D models of non-rigidly moving surfaces from scratch in real time using a single RGB-D stream. Our solution is based on the variational level set method, thus it copes with arbitrary geometry, including topological changes. It warps a given truncated signed distance field (TSDF) to a target TSDF via gradient flow. Unlike previous approaches that define the gradient using an L2 inner product, our method relies on gradient flow in Sobolev space. Its favourable regularity properties allow for a more straightforward energy formulation that is faster to compute and that achieves higher geometric detail, mitigating the over-smoothing effects introduced by other regularization schemes. In addition, the coarse-to-fine evolution behaviour of the flow is able to handle larger motions, making few frames sufficient for a high-fidelity reconstruction. Last but not least, our pipeline determines voxel correspondences between partial shapes by matching signatures in a low-dimensional embedding of their Laplacian eigenfunctions, and is thus able to reliably colour the output model. A variety of quantitative and qualitative evaluations demonstrate the advantages of our technique",
    "checked": true,
    "id": "b63a951b984597be23000cf10475e6655a4d7c97",
    "semantic_title": "sobolevfusion: 3d reconstruction of scenes undergoing free non-rigid motion",
    "citation_count": 95,
    "authors": [
      "Miroslava Slavcheva",
      "Maximilian Baust",
      "Slobodan Ilic"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kundu_AdaDepth_Unsupervised_content_cvpr_2018_paper.html": {
    "title": "AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation",
    "volume": "main",
    "abstract": "Supervised deep learning methods have shown promising results for the task of monocular depth estimation; but acquiring ground truth is costly, and prone to noise as well as inaccuracies. While synthetic datasets have been used to circumvent above problems, the resultant models do not generalize well to natural scenes due to the inherent domain shift. Recent adversarial approaches for domain adaption have performed well in mitigating the differences between the source and target domains. But these methods are mostly limited to a classification setup and do not scale well for fully-convolutional architectures. In this work, we propose AdaDepth - an unsupervised domain adaptation strategy for the pixel-wise regression task of monocular depth estimation. The proposed approach is devoid of above limitations through a) adversarial learning and b) explicit imposition of content consistency on the adapted target representation. Our unsupervised approach performs competitively with other established approaches on depth estimation tasks and achieves state-of-the-art results in a semi-supervised setting",
    "checked": true,
    "id": "6323db5f1281376029714b51f876d58dd4abd4d0",
    "semantic_title": "adadepth: unsupervised content congruent adaptation for depth estimation",
    "citation_count": 185,
    "authors": [
      "Jogendra Nath Kundu",
      "Phani Krishna Uppala",
      "Anuj Pahuja",
      "R. Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yi_Learning_to_Find_CVPR_2018_paper.html": {
    "title": "Learning to Find Good Correspondences",
    "volume": "main",
    "abstract": "We develop a deep architecture to learn to find good correspondences for wide-baseline stereo. Given a set of putative sparse matches and the camera intrinsics, we train our network in an end-to-end fashion to label the correspondences as inliers or outliers, while simultaneously using them to recover the relative pose, as encoded by the essential matrix. Our architecture is based on a multi-layer perceptron operating on pixel coordinates rather than directly on the image, and is thus simple and small. We introduce a novel normalization technique, called Context Normalization, which allows us to process each data point separately while embedding global information in it, and also makes the network invariant to the order of the correspondences. Our experiments on multiple challenging datasets demonstrate that our method is able to drastically improve the state of the art with little training data",
    "checked": true,
    "id": "901e8381aae4b1fbb0d4dcef714d39fbf02f9681",
    "semantic_title": "learning to find good correspondences",
    "citation_count": 480,
    "authors": [
      "Kwang Moo Yi",
      "Eduard Trulls",
      "Yuki Ono",
      "Vincent Lepetit",
      "Mathieu Salzmann",
      "Pascal Fua"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Korman_OATM_Occlusion_Aware_CVPR_2018_paper.html": {
    "title": "OATM: Occlusion Aware Template Matching by Consensus Set Maximization",
    "volume": "main",
    "abstract": "We present a novel approach to template matching that is efficient, can handle partial occlusions, and comes with provable performance guarantees. A key component of the method is a reduction that transforms the problem of searching a nearest neighbor among $N$ high-dimensional vectors, to searching neighbors among two sets of order $sqrt{N}$ vectors, which can be found efficiently using range search techniques. This allows for a quadratic improvement in search complexity, and makes the method scalable in handling large search spaces. The second contribution is a hashing scheme based on consensus set maximization, which allows us to handle occlusions. The resulting scheme can be seen as a randomized hypothesize-and-test algorithm, which is equipped with guarantees regarding the number of iterations required for obtaining an optimal solution with high probability. The predicted matching rates are validated empirically and the algorithm shows a significant improvement over the state-of-the-art in both speed and robustness to occlusions",
    "checked": true,
    "id": "0cbd9c8cbce30bf221e4cb3d721f003b1c29afcb",
    "semantic_title": "oatm: occlusion aware template matching by consensus set maximization",
    "citation_count": 19,
    "authors": [
      "Simon Korman",
      "Mark Milam",
      "Stefano Soatto"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zanfir_Deep_Learning_of_CVPR_2018_paper.html": {
    "title": "Deep Learning of Graph Matching",
    "volume": "main",
    "abstract": "The problem of graph matching under node and pair-wise constraints is fundamental in areas as diverse as combinatorial optimization, machine learning or computer vision, where representing both the relations between nodes and their neighborhood structure is essential. We present an end-to-end model that makes it possible to learn all parameters of the graph matching process, including the unary and pairwise node neighborhoods, represented as deep feature extraction hierarchies. The challenge is in the formulation of the different matrix computation layers of the model in a way that enables the consistent, efficient propagation of gradients in the complete pipeline from the loss function, through the combinatorial optimization layer solving the matching problem, and the feature extraction hierarchy. Our computer vision experiments and ablation studies on challenging datasets like PASCAL VOC keypoints, Sintel and CUB show that matching models refined end-to-end are superior to counterparts based on feature hierarchies trained for other problems",
    "checked": true,
    "id": "d6c1e14e8bea932f821352ea9e33928129f7d065",
    "semantic_title": "deep learning of graph matching",
    "citation_count": 246,
    "authors": [
      "Andrei Zanfir",
      "Cristian Sminchisescu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.html": {
    "title": "Unsupervised Discovery of Object Landmarks as Structural Representations",
    "volume": "main",
    "abstract": "Deep neural networks can model images with rich latent representations, but they cannot naturally conceptualize structures of object categories in a human-perceptible way. This paper addresses the problem of learning object structures in an image modeling process without supervision. We propose an autoencoding formulation to discover landmarks as explicit structural representations. The encoding module outputs landmark coordinates, whose validity is ensured by constraints that reflect the necessary properties for landmarks. The decoding module takes the landmarks as a part of the learnable input representations in an end-to-end differentiable framework. Our discovered landmarks are semantically meaningful and more predictive of manually annotated landmarks than those discovered by previous methods. The coordinates of our landmarks are also complementary features to pretrained deep-neuralnetwork representations in recognizing visual attributes. In addition, the proposed method naturally creates an unsupervised, perceptible interface to manipulate object shapes and decode images with controllable structures",
    "checked": true,
    "id": "a92c833d4d9d53812a1da342a5f599d4679f0ce1",
    "semantic_title": "unsupervised discovery of object landmarks as structural representations",
    "citation_count": 194,
    "authors": [
      "Yuting Zhang",
      "Yijie Guo",
      "Yixin Jin",
      "Yijun Luo",
      "Zhiyuan He",
      "Honglak Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html": {
    "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference",
    "volume": "main",
    "abstract": "The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based visual recognition models call for efficient on-device inference schemes. We propose a quantization scheme along with a co-designed training procedure allowing inference to be carried out using integer-only arithmetic while preserving an end-to-end model accuracy that is close to floating-point inference. Inference using integer-only arithmetic performs better than floating-point arithmetic on typical ARM CPUs and can be implemented on integer-arithmetic-only hardware such as mobile accelerators (e.g. Qualcomm Hexagon). By quantizing both activations and weights as 8-bit integers, we obtain a close to 4x memory footprint reduction compared to 32-bit floating-point representations. Even on MobileNets, a model family known for runtime efficiency, our quantization approach results in an improved tradeoff between latency and accuracy on popular ARM CPUs for ImageNet classification and COCO detection",
    "checked": true,
    "id": "59d0d7ccec2db66cad20cac5721ce54a8a058294",
    "semantic_title": "quantization and training of neural networks for efficient integer-arithmetic-only inference",
    "citation_count": 3121,
    "authors": [
      "Benoit Jacob",
      "Skirmantas Kligys",
      "Bo Chen",
      "Menglong Zhu",
      "Matthew Tang",
      "Andrew Howard",
      "Hartwig Adam",
      "Dmitry Kalenichenko"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Van_Horn_Lean_Multiclass_Crowdsourcing_CVPR_2018_paper.html": {
    "title": "Lean Multiclass Crowdsourcing",
    "volume": "main",
    "abstract": "We introduce a method for efficiently crowdsourcing multiclass annotations in challenging, real world image datasets. Our method is designed to minimize the number of human annotations that are necessary to achieve a desired level of confidence on class labels. It is based on combining models of worker behavior with computer vision. Our method is general: it can handle a large number of classes, worker labels that come from a taxonomy rather than a flat list, and can model the dependence of labels when workers can see a history of previous annotations. Our method may be used as a drop-in replacement for the majority vote algorithms used in online crowdsourcing services that aggregate multiple human annotations into a final consolidated label. In experiments conducted on two real-life applications we find that our method can reduce the number of required annotations by as much as a factor of 5.4 and can reduce the residual annotation error by up to 90% when compared with majority voting. Furthermore, the online risk estimates of the models may be used to sort the annotated collection and minimize subsequent expert review effort",
    "checked": true,
    "id": "cb6c19a1131009e5cf2e5a3c2a27f094eaac81a0",
    "semantic_title": "lean multiclass crowdsourcing",
    "citation_count": 20,
    "authors": [
      "Grant Van Horn",
      "Steve Branson",
      "Scott Loarie",
      "Serge Belongie",
      "Pietro Perona"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Partial_Transfer_Learning_CVPR_2018_paper.html": {
    "title": "Partial Transfer Learning With Selective Adversarial Networks",
    "volume": "main",
    "abstract": "Adversarial learning has been successfully embedded into deep networks to learn transferable features, which reduce distribution discrepancy between the source and target domains. Existing domain adversarial networks assume fully shared label space across domains. In the presence of big data, there is strong motivation of transferring both classification and representation models from existing large-scale domains to unknown small-scale domains. This paper introduces partial transfer learning, which relaxes the shared label space assumption to that the target label space is only a subspace of the source label space. Previous methods typically match the whole source domain to the target domain, which are prone to negative transfer for the partial transfer problem. We present Selective Adversarial Network (SAN), which simultaneously circumvents negative transfer by selecting out the outlier source classes and promotes positive transfer by maximally matching the data distributions in the shared label space. Experiments demonstrate that our models exceed state-of-the-art results for partial transfer learning tasks on several benchmark datasets",
    "checked": true,
    "id": "40da74de5e6a3c45d06242ec8d135b8889421fac",
    "semantic_title": "partial transfer learning with selective adversarial networks",
    "citation_count": 440,
    "authors": [
      "Zhangjie Cao",
      "Mingsheng Long",
      "Jianmin Wang",
      "Michael I. Jordan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jenni_Self-Supervised_Feature_Learning_CVPR_2018_paper.html": {
    "title": "Self-Supervised Feature Learning by Learning to Spot Artifacts",
    "volume": "main",
    "abstract": "We introduce a novel self-supervised learning method based on adversarial training. Our objective is to train a discriminator network to distinguish real images from images with synthetic artifacts, and then to extract features from its intermediate layers that can be transferred to other data domains and tasks. To generate images with artifacts, we pre-train a high-capacity autoencoder and then we use a damage and repair strategy: First, we freeze the autoencoder and damage the output of the encoder by randomly dropping its entries. Second, we augment the decoder with a repair network, and train it in an adversarial manner against the discriminator. The repair network helps generate more realistic images by inpainting the dropped feature entries. To make the discriminator focus on the artifacts, we also make it predict what entries in the feature were dropped. We demonstrate experimentally that features learned by creating and spotting artifacts achieve state of the art performance in several benchmarks",
    "checked": true,
    "id": "5b2193c44f2925807c5978a24da3381324969c40",
    "semantic_title": "self-supervised feature learning by learning to spot artifacts",
    "citation_count": 127,
    "authors": [
      "Simon Jenni",
      "Paolo Favaro"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_LDMNet_Low_Dimensional_CVPR_2018_paper.html": {
    "title": "LDMNet: Low Dimensional Manifold Regularized Neural Networks",
    "volume": "main",
    "abstract": "Deep neural networks have proved very successful on archetypal tasks for which large training sets are available, but when the training data are scarce, their performance suffers from overfitting. Many existing methods of reducing overfitting are data-independent. Data-dependent regularizations are mostly motivated by the observation that data of interest lie close to a manifold, which is typically hard to parametrize explicitly. These methods usually only focus on the geometry of the input data, and do not necessarily encourage the networks to produce geometrically meaningful features. To resolve this, we propose the Low-Dimensional- Manifold-regularized neural Network (LDMNet), which incorporates a feature regularization method that focuses on the geometry of both the input data and the output features. In LDMNet, we regularize the network by encouraging the combination of the input data and the output features to sample a collection of low dimensional manifolds, which are searched efficiently without explicit parametrization. To achieve this, we directly use the manifold dimension as a regularization term in a variational functional. The resulting Euler-Lagrange equation is a Laplace-Beltrami equation over a point cloud, which is solved by the point integral method without increasing the computational complexity. In the experiments, we show that LDMNet significantly outperforms widely-used regularizers. Moreover, LDMNet can extract common features of an object imaged via different modalities, which is very useful in real-world applications such as cross-spectral face recognition",
    "checked": true,
    "id": "0d08341e9199495e709b1bd8c587448104a87ad9",
    "semantic_title": "ldmnet: low dimensional manifold regularized neural networks",
    "citation_count": 40,
    "authors": [
      "Wei Zhu",
      "Qiang Qiu",
      "Jiaji Huang",
      "Robert Calderbank",
      "Guillermo Sapiro",
      "Ingrid Daubechies"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_CondenseNet_An_Efficient_CVPR_2018_paper.html": {
    "title": "CondenseNet: An Efficient DenseNet Using Learned Group Convolutions",
    "volume": "main",
    "abstract": "Deep neural networks are increasingly used on mobile devices, where computational resources are limited. In this paper we develop CondenseNet, a novel network architecture with unprecedented efficiency. It combines dense connectivity with a novel module called learned group convolution. The dense connectivity facilitates feature re-use in the network, whereas learned group convolutions remove connections between layers for which this feature re-use is superfluous. At test time, our model can be implemented using standard group convolutions, allowing for efficient computation in practice. Our experiments show that CondenseNets are far more efficient than state-of-the-art compact convolutional networks such as MobileNets and ShuffleNets",
    "checked": true,
    "id": "efbac99adf8628aae7f070e5b4388a295956f9d2",
    "semantic_title": "condensenet: an efficient densenet using learned group convolutions",
    "citation_count": 798,
    "authors": [
      "Gao Huang",
      "Shichen Liu",
      "Laurens van der Maaten",
      "Kilian Q. Weinberger"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Keller_Learning_Deep_Descriptors_CVPR_2018_paper.html": {
    "title": "Learning Deep Descriptors With Scale-Aware Triplet Networks",
    "volume": "main",
    "abstract": "Research on learning suitable feature descriptors for Computer Vision has recently shifted to deep learning where the biggest challenge lies with the formulation of appropriate loss functions, especially since the descriptors to be learned are not known at training time. While approaches such as Siamese and triplet losses have been applied with success, it is still not well understood what makes a good loss function. In this spirit, this work demonstrates that many commonly used losses suffer from a range of problems. Based on this analysis, we introduce mixed-context losses and scale-aware sampling, two methods that when combined enable networks to learn consistently scaled descriptors for the first time",
    "checked": true,
    "id": "7a2c2298b036bfb63925836d6847f073f6ee9323",
    "semantic_title": "learning deep descriptors with scale-aware triplet networks",
    "citation_count": 55,
    "authors": [
      "Michel Keller",
      "Zetao Chen",
      "Fabiola Maffra",
      "Patrik Schmuck",
      "Margarita Chli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Decoupled_Networks_CVPR_2018_paper.html": {
    "title": "Decoupled Networks",
    "volume": "main",
    "abstract": "Inner product-based convolution has been a central component of convolutional neural networks (CNNs) and the key to learning visual representations. Inspired by the observation that CNN-learned features are naturally decoupled with the norm of features corresponding to the intra-class variation and the angle corresponding to the semantic difference, we propose a generic decoupled learning framework which models the intra-class variation and semantic difference independently. Specifically, we first reparametrize the inner product to a decoupled form and then generalize it to the decoupled convolution operator which serves as the building block of our decoupled networks. We present several effective instances of the decoupled convolution operator. Each decoupled operator is well motivated and has an intuitive geometric interpretation. Based on these decoupled operators, we further propose to directly learn the operator from data. Extensive experiments show that such decoupled reparameterization renders significant performance gain with easier convergence and stronger robustness",
    "checked": true,
    "id": "f6d1c7e7021b5fc4bf18cc9c1b203a567e2901a5",
    "semantic_title": "decoupled networks",
    "citation_count": 68,
    "authors": [
      "Weiyang Liu",
      "Zhen Liu",
      "Zhiding Yu",
      "Bo Dai",
      "Rongmei Lin",
      "Yisen Wang",
      "James M. Rehg",
      "Le Song"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.html": {
    "title": "Deep Adversarial Metric Learning",
    "volume": "main",
    "abstract": "Learning an effective distance metric between image pairs plays an important role in visual analysis, where the training procedure largely relies on hard negative samples. However, hard negatives in the training set usually account for the tiny minority, which may fail to fully describe the distribution of negative samples close to the margin. In this paper, we propose a deep adversarial metric learning (DAML) framework to generate synthetic hard negatives from the observed negative samples, which is widely applicable to supervised deep metric learning methods. Different from existing metric learning approaches which simply ignore numerous easy negatives, the proposed DAML exploits them to generate potential hard negatives adversary to the learned metric as complements. We simultaneously train the hard negative generator and feature embedding in an adversarial manner, so that more precise distance metrics can be learned with adequate and targeted synthetic hard negatives. Extensive experimental results on three benchmark datasets including CUB-200-2011, Cars196 and Stanford Online Products show that DAML effectively boosts the performance of existing deep metric learning approaches through adversarial learning",
    "checked": true,
    "id": "80ba35906f48f3bf95268b1b9da7995466cf3251",
    "semantic_title": "deep adversarial metric learning",
    "citation_count": 214,
    "authors": [
      "Yueqi Duan",
      "Wenzhao Zheng",
      "Xudong Lin",
      "Jiwen Lu",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_PU-Net_Point_Cloud_CVPR_2018_paper.html": {
    "title": "PU-Net: Point Cloud Upsampling Network",
    "volume": "main",
    "abstract": "Learning and analyzing 3D point clouds with deep networks is challenging due to the sparseness and irregularity of the data. In this paper, we present a data-driven point cloud upsampling technique. The key idea is to learn multi-level features per point and expand the point set via a multi-branch convolution unit implicitly in feature space. The expanded feature is then split to a multitude of features, which are then reconstructed to an upsampled point set. Our network is applied at a patch-level, with a joint loss function that encourages the upsampled points to remain on the underlying surface with a uniform distribution. We conduct various experiments using synthesis and scan data to evaluate our method and demonstrate its superiority over some baseline methods and an optimization-based method. Results show that our upsampled points have better uniformity and are located closer to the underlying surfaces",
    "checked": true,
    "id": "c82ca047a80dbdf10ec6a92f860c0c40035c6517",
    "semantic_title": "pu-net: point cloud upsampling network",
    "citation_count": 578,
    "authors": [
      "Lequan Yu",
      "Xianzhi Li",
      "Chi-Wing Fu",
      "Daniel Cohen-Or",
      "Pheng-Ann Heng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Atapour-Abarghouei_Real-Time_Monocular_Depth_CVPR_2018_paper.html": {
    "title": "Real-Time Monocular Depth Estimation Using Synthetic Data With Domain Adaptation via Image Style Transfer",
    "volume": "main",
    "abstract": "Monocular depth estimation using learning-based approaches has become promising in recent years. However, most monocular depth estimators either need to rely on large quantities of ground truth depth data, which is extremely expensive and difficult to obtain, or predict disparity as an intermediary step using a secondary supervisory signal leading to blurring and other artefacts. Training a depth estimation model using pixel-perfect synthetic data can resolve most of these issues but introduces the problem of domain bias. This is the inability to apply a model trained on synthetic data to real-world scenarios. With advances in image style transfer and its connections with domain adaptation (Maximum Mean Discrepancy), we take advantage of style transfer and adversarial training to predict pixel perfect depth from a single real-world color image based on training over a large corpus of synthetic environment data. Experimental results indicate the efficacy of our approach compared to contemporary state-of-the-art techniques",
    "checked": true,
    "id": "21547e038b383fb9d2ca14eb9978b1d4d8e6b178",
    "semantic_title": "real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer",
    "citation_count": 279,
    "authors": [
      "Amir Atapour-Abarghouei",
      "Toby P. Breckon"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liang_Learning_for_Disparity_CVPR_2018_paper.html": {
    "title": "Learning for Disparity Estimation Through Feature Constancy",
    "volume": "main",
    "abstract": "Stereo matching algorithms usually consist of four steps, including matching cost calculation, matching cost aggregation, disparity calculation, and disparity refinement. Existing CNN-based methods only adopt CNN to solve parts of the four steps, or use different networks to deal with different steps, making them difficult to obtain the overall optimal solution. In this paper, we propose a network architecture to incorporate all steps of stereo matching. The network consists of three parts. The first part calculates the multi-scale shared features. The second part performs matching cost calculation, matching cost aggregation and disparity calculation to estimate the initial disparity using shared features. The initial disparity and the shared features are used to calculate the feature constancy that measures correctness of the correspondence between two input images. The initial disparity and the feature constancy are then fed to a sub-network to refine the initial disparity. The proposed method has been evaluated on the Scene Flow and KITTI datasets. It achieves the state-of-the-art performance on the KITTI 2012 and KITTI 2015 benchmarks while maintaining a very fast running time. Source code is available at http://github.com/leonzfa/iResNet",
    "checked": true,
    "id": "33389b1da844f13aba317e8b16c814888c26c827",
    "semantic_title": "learning for disparity estimation through feature constancy",
    "citation_count": 328,
    "authors": [
      "Zhengfa Liang",
      "Yiliu Feng",
      "Yulan Guo",
      "Hengzhu Liu",
      "Wei Chen",
      "Linbo Qiao",
      "Li Zhou",
      "Jianfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper.html": {
    "title": "DeepMVS: Learning Multi-View Stereopsis",
    "volume": "main",
    "abstract": "We present DeepMVS, a deep convolutional neural network (ConvNet) for multi-view stereo reconstruction. Taking an arbitrary number of posed images as input, we first produce a set of plane-sweep volumes and use the proposed DeepMVS network to predict high-quality disparity maps. The key contributions that enable these results are (1) supervised pretraining on a photorealistic synthetic dataset, (2) an effective method for aggregating information across a set of unordered images, and (3) integrating multi-layer feature activations from the pre-trained VGG-19 network. We validate the efficacy of DeepMVS using the ETH3D Benchmark. Our results show that DeepMVS compares favorably against state-of-the-art conventional MVS algorithms and other ConvNet based methods, particularly for near-textureless regions and thin structures",
    "checked": true,
    "id": "11ca3e9fc044b2e1afe86b2c659daf9c016f7792",
    "semantic_title": "deepmvs: learning multi-view stereopsis",
    "citation_count": 469,
    "authors": [
      "Po-Han Huang",
      "Kevin Matzen",
      "Johannes Kopf",
      "Narendra Ahuja",
      "Jia-Bin Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Teo_Self-Calibrating_Polarising_Radiometric_CVPR_2018_paper.html": {
    "title": "Self-Calibrating Polarising Radiometric Calibration",
    "volume": "main",
    "abstract": "We present a self-calibrating polarising radiometric calibration method. From a set of images taken from a single viewpoint under different unknown polarising angles, we recover the inverse camera response function and the polarising angles relative to the first angle. The problem is solved in an integrated manner, recovering both of the unknowns simultaneously. The method exploits the fact that the intensity of polarised light should vary sinusoidally as the polarising filter is rotated, provided that the response is linear. It offers the first solution to demonstrate the possibility of radiometric calibration through polarisation. We evaluate the accuracy of our proposed method using synthetic data and real world objects captured using different cameras. The self-calibrated results were found to be comparable with those from multiple exposure sequence",
    "checked": true,
    "id": "6eaabcb3fc27b8f562b78abd1bb17261ef23ae85",
    "semantic_title": "self-calibrating polarising radiometric calibration",
    "citation_count": 8,
    "authors": [
      "Daniel Teo",
      "Boxin Shi",
      "Yinqiang Zheng",
      "Sai-Kit Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tanfous_Coding_Kendalls_Shape_CVPR_2018_paper.html": {
    "title": "Coding Kendall's Shape Trajectories for 3D Action Recognition",
    "volume": "main",
    "abstract": "Suitable shape representations as well as their temporal evolution, termed trajectories, often lie to non-linear manifolds. This puts an additional constraint (i.e., non-linearity) in using conventional machine learning techniques for the purpose of classification, event detection, prediction, etc. This paper accommodates the well-known Sparse Coding and Dictionary Learning to the Kendall's shape space and illustrates effective coding of 3D skeletal sequences for action recognition. Grounding on the Riemannian geometry of the shape space, an intrinsic sparse coding and dictionary learning formulation is proposed for static skeletal shapes to overcome the inherent non-linearity of the manifold. As a main result, initial trajectories give rise to sparse code functions with suitable computational properties, including sparsity and vector space representation. To achieve action recognition, two different classification schemes were adopted. A bi-directional LSTM is directly performed on sparse code functions, while a linear SVM is applied after representing sparse code functions using Fourier temporal pyramid. Experiments conducted on three publicly available datasets show the superiority of the proposed approach compared to existing Riemannian representations and its competitiveness with respect to other recently-proposed approaches. When the benefits of invariance are maintained from the Kendall's shape representation, our approach not only overcomes the problem of non-linearity but also yields to discriminative sparse code functions",
    "checked": true,
    "id": "c6e0184d510eb23fe890d9570226ecebcd209913",
    "semantic_title": "coding kendall's shape trajectories for 3d action recognition",
    "citation_count": 50,
    "authors": [
      "Amor Ben Tanfous",
      "Hassen Drira",
      "Boulbaba Ben Amor"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Turek_Efficient_Sparse_Representation_CVPR_2018_paper.html": {
    "title": "Efficient, Sparse Representation of Manifold Distance Matrices for Classical Scaling",
    "volume": "main",
    "abstract": "Geodesic distance matrices can reveal shape properties that are largely invariant to non-rigid deformations, and thus are often used to analyze and represent 3-D shapes. However, these matrices grow quadratically with the number of points. Thus for large point sets it is common to use a low-rank approximation to the distance matrix, which fits in memory and can be efficiently analyzed using methods such as multidimensional scaling (MDS). In this paper we present a novel sparse method for efficiently representing geodesic distance matrices using biharmonic interpolation. This method exploits knowledge of the data manifold to learn a sparse interpolation operator that approximates distances using a subset of points. We show that our method is 2x faster and uses 20x less memory than current leading methods for solving MDS on large point sets, with similar quality. This enables analyses of large point sets that were previously infeasible",
    "checked": true,
    "id": "96047db632a7118c5780ef9ef960fbe11b3cd288",
    "semantic_title": "efficient, sparse representation of manifold distance matrices for classical scaling",
    "citation_count": 0,
    "authors": [
      "Javier S. Turek",
      "Alexander G. Huth"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Motion_Segmentation_by_CVPR_2018_paper.html": {
    "title": "Motion Segmentation by Exploiting Complementary Geometric Models",
    "volume": "main",
    "abstract": "Many real-world sequences cannot be conveniently categorized as general or degenerate; in such cases, imposing a false dichotomy in using the fundamental matrix or homography model for motion segmentation would lead to difficulty. Even when we are confronted with a general scene-motion, the fundamental matrix approach as a model for motion segmentation still suffers from several defects, which we discuss in this paper. The full potential of the fundamental matrix approach could only be realized if we judiciously harness information from the simpler homography model. From these considerations, we propose a multi-view spectral clustering framework that synergistically combines multiple models together. We show that the performance can be substantially improved in this way. We perform extensive testing on existing motion segmentation datasets, achieving state-of-the-art performance on all of them; we also put forth a more realistic and challenging dataset adapted from the KITTI benchmark, containing real-world effects such as strong perspectives and strong forward translations not seen in the traditional datasets",
    "checked": true,
    "id": "7cb5a4b88235250424e4b7450e22140dac9ba535",
    "semantic_title": "motion segmentation by exploiting complementary geometric models",
    "citation_count": 38,
    "authors": [
      "Xun Xu",
      "Loong Fah Cheong",
      "Zhuwen Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shi_Estimation_of_Camera_CVPR_2018_paper.html": {
    "title": "Estimation of Camera Locations in Highly Corrupted Scenarios: All About That Base, No Shape Trouble",
    "volume": "main",
    "abstract": "We propose a strategy for improving camera location estimation in structure from motion. Our setting assumes highly corrupted pairwise directions (i.e., normalized relative location vectors), so there is a clear room for improving current state-of-the-art solutions for this problem. Our strategy identifies severely corrupted pairwise directions by using a geometric consistency condition. It then selects a cleaner set of pairwise directions as a preprocessing step for common solvers. We theoretically guarantee the successful performance of a basic version of our strategy under a synthetic corruption model. Numerical results on artificial and real data demonstrate the significant improvement obtained by our strategy",
    "checked": true,
    "id": "43755da28925f27a8431d9e0ed818aba3cc36654",
    "semantic_title": "estimation of camera locations in highly corrupted scenarios: all about that base, no shape trouble",
    "citation_count": 12,
    "authors": [
      "Yunpeng Shi",
      "Gilad Lerman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_4D_Human_Body_CVPR_2018_paper.html": {
    "title": "4D Human Body Correspondences From Panoramic Depth Maps",
    "volume": "main",
    "abstract": "The availability of affordable 3D full body reconstruction systems has given rise to free-viewpoint video (FVV) of human avatars. Most existing solutions produce temporally uncorrelated point clouds or meshes with unknown point/vertex correspondences. Individually compressing each frame is ineffective and still yields to ultra-large data sizes. We present an end-to-end deep learning scheme to establish dense shape correspondences and subsequently compress the data. Our approach uses sparse set of \"panoramic\" depth maps or PDMs, each emulating an inward-viewing concentric mosaics (CM). We then develop a learning-based technique to learn pixel-wise feature descriptors on PDMs. The results are fed into an autoencoder-based network for compression. Comprehensive experiments demonstrate our solution is robust and effective on both public and our newly captured datasets",
    "checked": true,
    "id": "e6cb800b49a82cc37f092dbed1ede96875765a90",
    "semantic_title": "4d human body correspondences from panoramic depth maps",
    "citation_count": 8,
    "authors": [
      "Zhong Li",
      "Minye Wu",
      "Wangyiteng Zhou",
      "Jingyi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Reconstructing_Thin_Structures_CVPR_2018_paper.html": {
    "title": "Reconstructing Thin Structures of Manifold Surfaces by Integrating Spatial Curves",
    "volume": "main",
    "abstract": "The manifold surface reconstruction in multi-view stereo often fails in retaining thin structures due to incomplete and noisy reconstructed point clouds. In this paper, we address this problem by leveraging spatial curves. The curve representation in nature is advantageous in modeling thin and elongated structures, implying topology and connectivity information of the underlying geometry, which exactly compensates the weakness of scattered point clouds. We present a novel surface reconstruction method using both curves and point clouds. First, we propose a 3D curve reconstruction algorithm based on the initialize-optimize-expand strategy. Then, tetrahedra are constructed from points and curves, where the volumes of thin structures are robustly preserved by the Curve-conformed Delaunay Refinement. Finally, the mesh surface is extracted from tetrahedra by a graph optimization. The method has been intensively evaluated on both synthetic and real-world datasets, showing significant improvements over state-of-the-art methods",
    "checked": true,
    "id": "7420afd882737d2fbc6be4854015a8c214252392",
    "semantic_title": "reconstructing thin structures of manifold surfaces by integrating spatial curves",
    "citation_count": 31,
    "authors": [
      "Shiwei Li",
      "Yao Yao",
      "Tian Fang",
      "Long Quan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tulsiani_Multi-View_Consistency_as_CVPR_2018_paper.html": {
    "title": "Multi-View Consistency as Supervisory Signal for Learning Shape and Pose Prediction",
    "volume": "main",
    "abstract": "We present a framework for learning single-view shape and pose prediction without using direct supervision for either. Our approach allows leveraging multi-view observations from unknown poses as supervisory signal during training. Our proposed training setup enforces geometric consistency between the independently predicted shape and pose from two views of the same instance. We consequently learn to predict shape in an emergent canonical (view-agnostic) frame along with a corresponding pose predictor. We show empirical and qualitative results using the ShapeNet dataset and observe encouragingly competitive performance to previous techniques which rely on stronger forms of supervision. We also demonstrate the applicability of our framework in a realistic setting which is beyond the scope of existing techniques: using a training dataset comprised of online product images where the underlying shape and pose are unknown",
    "checked": true,
    "id": "bd6c0d4673bfbc6e7a0f9aaf8cb0b5404f4a14b6",
    "semantic_title": "multi-view consistency as supervisory signal for learning shape and pose prediction",
    "citation_count": 205,
    "authors": [
      "Shubham Tulsiani",
      "Alexei A. Efros",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Isokane_Probabilistic_Plant_Modeling_CVPR_2018_paper.html": {
    "title": "Probabilistic Plant Modeling via Multi-View Image-to-Image Translation",
    "volume": "main",
    "abstract": "This paper describes a method for inferring three-dimensional (3D) plant branch structures that are hidden under leaves from multi-view observations. Unlike previous geometric approaches that heavily rely on the visibility of the branches or use parametric branching models, our method makes statistical inferences of branch structures in a probabilistic framework. By inferring the probability of branch existence using a Bayesian extension of image-to-image translation applied to each of multi-view images, our method generates a probabilistic plant 3D model, which represents the 3D branching pattern that cannot be directly observed. Experiments demonstrate the usefulness of the proposed approach in generating convincing branch structures in comparison to prior approaches",
    "checked": true,
    "id": "31ff75b0a046da7d192318ad8ba2f4aebfa2132f",
    "semantic_title": "probabilistic plant modeling via multi-view image-to-image translation",
    "citation_count": 33,
    "authors": [
      "Takahiro Isokane",
      "Fumio Okura",
      "Ayaka Ide",
      "Yasuyuki Matsushita",
      "Yasushi Yagi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Deep_Marching_Cubes_CVPR_2018_paper.html": {
    "title": "Deep Marching Cubes: Learning Explicit Surface Representations",
    "volume": "main",
    "abstract": "Existing learning based solutions to 3D surface prediction cannot be trained end-to-end as they operate on intermediate representations (e.g., TSDF) from which 3D surface meshes must be extracted in a post-processing step (e.g., via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface prediction. We first demonstrate that the marching cubes algorithm is not differentiable and propose an alternative differentiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demonstrate that the model allows for predicting sub-voxel accurate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object's inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques",
    "checked": true,
    "id": "7b183c11f1ba13f05db2050ab1abb1fa9f52c9a8",
    "semantic_title": "deep marching cubes: learning explicit surface representations",
    "citation_count": 266,
    "authors": [
      "Yiyi Liao",
      "Simon Donné",
      "Andreas Geiger"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Muralikrishnan_Tags2Parts_Discovering_Semantic_CVPR_2018_paper.html": {
    "title": "Tags2Parts: Discovering Semantic Regions From Shape Tags",
    "volume": "main",
    "abstract": "We propose a novel method for discovering shape regions that strongly correlate with user-prescribed tags. For example, given a collection of chairs tagged as either \"has armrest\" or \"lacks armrest\", our system correctly highlights the armrest regions as the main distinctive parts between the two chair types. To obtain point-wise predictions from shape-wise tags we develop a novel neural network architecture that is trained with tag classification loss, but is designed to rely on segmentation to predict the tag. Our network is inspired by U-Net, but we replicate shallow U structures several times with new skip connections and pooling layers, and call the resulting architecture \"WU-Net\". We test our method on segmentation benchmarks and show that even with weak supervision of whole shape tags, our method can infer meaningful semantic regions, without ever observing shape segmentations. Further, once trained, the model can process shapes for which the tag is entirely unknown. As a bonus, our architecture is directly operational under full supervision and performs strongly on standard benchmarks. We validate our method through experiments with many variant architectures and prior baselines, and demonstrate several applications",
    "checked": true,
    "id": "f81bda8c16da0974b04c2c06c4de9917d83edbc0",
    "semantic_title": "tags2parts: discovering semantic regions from shape tags",
    "citation_count": 24,
    "authors": [
      "Sanjeev Muralikrishnan",
      "Vladimir G. Kim",
      "Siddhartha Chaudhuri"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mo_Uncalibrated_Photometric_Stereo_CVPR_2018_paper.html": {
    "title": "Uncalibrated Photometric Stereo Under Natural Illumination",
    "volume": "main",
    "abstract": "This paper presents a photometric stereo method that works with unknown natural illuminations without any calibration object. To solve this challenging problem, we propose the use of an equivalent directional lighting model for small surface patches consisting of slowly varying normals, and solve each patch up to an arbitrary rotation ambiguity. Our method connects the resulting patches and unifies the local ambiguities to a global rotation one through angular distance propagation defined over the whole surface. After applying the integrability constraint, our final solution contains only a binary ambiguity, which could be easily removed. Experiments using both synthetic and real-world datasets show our method provides even comparable results to calibrated methods",
    "checked": true,
    "id": "c1e51e1a5c3484a5b49e49ba91ac4d1ef893f567",
    "semantic_title": "uncalibrated photometric stereo under natural illumination",
    "citation_count": 34,
    "authors": [
      "Zhipeng Mo",
      "Boxin Shi",
      "Feng Lu",
      "Sai-Kit Yeung",
      "Yasuyuki Matsushita"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Im_Robust_Depth_Estimation_CVPR_2018_paper.html": {
    "title": "Robust Depth Estimation From Auto Bracketed Images",
    "volume": "main",
    "abstract": "As demand for advanced photographic applications on hand-held devices grows, these electronics require the capture of high quality depth. However, under low-light conditions, most devices still suffer from low imaging quality and inaccurate depth acquisition. To address the problem, we present a robust depth estimation method from a short burst shot with varied intensity (i.e., Auto Bracketing) or strong noise (i.e., High ISO). We introduce a geometric transformation between flow and depth tailored for burst images, enabling our learning-based multi-view stereo matching to be performed effectively. We then describe our depth estimation pipeline that incorporates the geometric transformation into our residual-flow network. It allows our framework to produce an accurate depth map even with a bracketed image sequence. We demonstrate that our method outperforms state-of-the-art methods for various datasets captured by a smartphone and a DSLR camera. Moreover, we show that the estimated depth is applicable for image quality enhancement and photographic editing",
    "checked": true,
    "id": "f124a2e08f63baba9eabb2074ee2a7166dfcc043",
    "semantic_title": "robust depth estimation from auto bracketed images",
    "citation_count": 5,
    "authors": [
      "Sunghoon Im",
      "Hae-Gon Jeon",
      "In So Kweon"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Krahenbuhl_Free_Supervision_From_CVPR_2018_paper.html": {
    "title": "Free Supervision From Video Games",
    "volume": "main",
    "abstract": "Deep networks are extremely hungry for data. They devour hundreds of thousands of labeled images to learn robust and semantically meaningful feature representations. Current networks are so data hungry that collecting labeled data has become as important as designing the networks themselves. Unfortunately, manual data collection is both expensive and time consuming. We present an alternative, and show how ground truth labels for many vision tasks are easily extracted from video games in real time as we play them. We interface the popular Microsoft DirectX rendering API, and inject specialized rendering code into the game as it is running. This code produces ground truth labels for instance segmentation, semantic labeling, depth estimation, optical flow, intrinsic image decomposition, and instance tracking. Instead of labeling images, a researcher now simply plays video games all day long. Our method is general and works on a wide range of video games. We collected a dataset of 220k training images, and 60k test images across 3 video games, and evaluate state of the art optical flow, depth estimation and intrinsic image decomposition algorithms. Our video game data is visually closer to real world images, than other synthetic dataset",
    "checked": true,
    "id": "7027892928fbb95b0f69b6c4af6f426807f13af2",
    "semantic_title": "free supervision from video games",
    "citation_count": 92,
    "authors": [
      "Philipp Krähenbühl"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fang_Planar_Shape_Detection_CVPR_2018_paper.html": {
    "title": "Planar Shape Detection at Structural Scales",
    "volume": "main",
    "abstract": "Interpreting 3D data such as point clouds or surface meshes depends heavily on the scale of observation. Yet, existing algorithms for shape detection rely on trial-and-error parameter tunings to output configurations representative of a structural scale. We present a framework to automatically extract a set of representations that capture the shape and structure of man-made objects at different key abstraction levels. A shape-collapsing process first generates a fine-to-coarse sequence of shape representations by exploiting local planarity. This sequence is then analyzed to identify significant geometric variations between successive representations through a supervised energy minimization. Our framework is flexible enough to learn how to detect both existing structural formalisms such as the CityGML Levels Of Details, and expert-specified levels of abstraction. Experiments on different input data and classes of man-made objects, as well as comparisons with existing shape detection methods, illustrate the strengths of our approach in terms of efficiency and flexibility",
    "checked": true,
    "id": "2fac5fb4028253c2240b30026ee25f56702d4d69",
    "semantic_title": "planar shape detection at structural scales",
    "citation_count": 49,
    "authors": [
      "Hao Fang",
      "Florent Lafarge",
      "Mathieu Desbrun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Pix3D_Dataset_and_CVPR_2018_paper.html": {
    "title": "Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling",
    "volume": "main",
    "abstract": "We study 3D shape modeling from a single image and make contributions to it in three aspects. First, we present Pix3D, a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc. Building such a large-scale dataset, however, is highly challenging; existing datasets either contain only synthetic data, or lack precise alignment between 2D images and 3D shapes, or only have a small number of images. Second, we calibrate the evaluation criteria for 3D shape reconstruction through behavioral studies, and use them to objectively and systematically benchmark cutting-edge reconstruction algorithms on Pix3D. Third, we design a novel model that simultaneously performs 3D reconstruction and pose estimation; our multi-task learning approach achieves state-of-the-art performance on both tasks",
    "checked": true,
    "id": "28e1c5fa724db6c4d927799730d13229c4a74bac",
    "semantic_title": "pix3d: dataset and methods for single-image 3d shape modeling",
    "citation_count": 456,
    "authors": [
      "Xingyuan Sun",
      "Jiajun Wu",
      "Xiuming Zhang",
      "Zhoutong Zhang",
      "Chengkai Zhang",
      "Tianfan Xue",
      "Joshua B. Tenenbaum",
      "William T. Freeman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Larsson_Camera_Pose_Estimation_CVPR_2018_paper.html": {
    "title": "Camera Pose Estimation With Unknown Principal Point",
    "volume": "main",
    "abstract": "To estimate the 6-DoF extrinsic pose of a pinhole camera with partially unknown intrinsic parameters is a critical sub-problem in structure-from-motion and camera localization. In most of existing camera pose estimation solvers, the principal point is assumed to be in the image center. Unfortunately, this assumption is not always true, especially for asymmetrically cropped images. In this paper, we develop the first exactly minimal solver for the case of unknown principal point and focal length by using four and a half point correspondences (P4.5Pfuv). We also present an extremely fast solver for the case of unknown aspect ratio (P5Pfuva). The new solvers outperform the previous state-of-the-art in terms of stability and speed. Finally, we explore the extremely challenging case of both unknown principal point and radial distortion, and develop the first practical non-minimal solver by using seven point correspondences (P7Pfruv). Experimental results on both simulated data and real Internet images demonstrate the usefulness of our new solvers",
    "checked": true,
    "id": "741fe5fbc055b4bb172dc79be8516121bfb23e06",
    "semantic_title": "camera pose estimation with unknown principal point",
    "citation_count": 27,
    "authors": [
      "Viktor Larsson",
      "Zuzana Kukelova",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Vongkulbhisal_Inverse_Composition_Discriminative_CVPR_2018_paper.html": {
    "title": "Inverse Composition Discriminative Optimization for Point Cloud Registration",
    "volume": "main",
    "abstract": "Rigid Point Cloud Registration (PCReg) refers to the problem of finding the rigid transformation between two sets of point clouds. This problem is particularly important due to the advances in new 3D sensing hardware, and it is challenging because neither the correspondence nor the transformation parameters are known. Traditional local PCReg methods (e.g., ICP) rely on local optimization algorithms, which can get trapped in bad local minima in the presence of noise, outliers, bad initializations, etc. To alleviate these issues, this paper proposes Inverse Composition Discriminative Optimization (ICDO), an extension of Discriminative Optimization (DO), which learns a sequence of update steps from synthetic training data that search the parameter space for an improved solution. Unlike DO, ICDO is object-independent and generalizes even to unseen shapes. We evaluated ICDO on both synthetic and real data, and show that ICDO can match the speed and outperform the accuracy of state-of-the-art PCReg algorithms",
    "checked": true,
    "id": "85e7c598c1018833055128b08829917fad8d472f",
    "semantic_title": "inverse composition discriminative optimization for point cloud registration",
    "citation_count": 24,
    "authors": [
      "Jayakorn Vongkulbhisal",
      "Beñat Irastorza Ugalde",
      "Fernando De la Torre",
      "João P. Costeira"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chu_SurfConv_Bridging_3D_CVPR_2018_paper.html": {
    "title": "SurfConv: Bridging 3D and 2D Convolution for RGBD Images",
    "volume": "main",
    "abstract": "The last few years have seen approaches trying to combine the increasing popularity of depth sensors and the success of the convolutional neural networks. Using depth as additional channel alongside the RGB input has the scale variance problem present in image convolution based approaches. On the other hand, 3D convolution wastes a large amount of memory on mostly unoccupied 3D space, which consists of only the surface visible to the sensor. Instead, we propose SurfConv, which \"slides\" compact 2D filters along the visible 3D surface. SurfConv is formulated as a simple depth-aware multi-scale 2D convolution, through a new Data-Driven Depth Discretization (D4) scheme. We demonstrate the effectiveness of our method on indoor and outdoor 3D semantic segmentation datasets. Our method achieves state-of-the-art performance while using less than 30% parameters used by the 3D convolution based approaches",
    "checked": true,
    "id": "cbd1a47356d8dc1c46d569d55eb803945193f9b0",
    "semantic_title": "surfconv: bridging 3d and 2d convolution for rgbd images",
    "citation_count": 24,
    "authors": [
      "Hang Chu",
      "Wei-Chiu Ma",
      "Kaustav Kundu",
      "Raquel Urtasun",
      "Sanja Fidler"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_A_Fast_Resection-Intersection_CVPR_2018_paper.html": {
    "title": "A Fast Resection-Intersection Method for the Known Rotation Problem",
    "volume": "main",
    "abstract": "The known rotation problem refers to a special case of structure-from-motion where the absolute orientations of the cameras are known. When formulated as a minimax (l_infty) problem on reprojection errors, the problem is an instance of pseudo-convex programming. Though theoretically tractable, solving the known rotation problem on large-scale data (1,000's of views, 10,000's scene points) using existing methods can be very time-consuming. In this paper, we devise a fast algorithm for the known rotation problem. Our approach alternates between pose estimation and triangulation (i.e., resection-intersection) to break the problem into multiple simpler instances of pseudo-convex programming. The key to the vastly superior performance of our method lies in using a novel minimum enclosing ball (MEB) technique for the calculation of updating steps, which obviates the need for convex optimisation routines and greatly reduces memory footprint. We demonstrate the practicality of our method on large-scale problem instances which easily overwhelm current state-of-the-art algorithms (demo program available in supplementary)",
    "checked": true,
    "id": "01a82ac82c242fad426ff874a870f9a24d4177ae",
    "semantic_title": "a fast resection-intersection method for the known rotation problem",
    "citation_count": 10,
    "authors": [
      "Qianggong Zhang",
      "Tat-Jun Chin",
      "Huu Minh Le"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Grabner_3D_Pose_Estimation_CVPR_2018_paper.html": {
    "title": "3D Pose Estimation and 3D Model Retrieval for Objects in the Wild",
    "volume": "main",
    "abstract": "We propose a scalable, efficient and accurate approach to retrieve 3D models for objects in the wild. Our contribution is twofold. We first present a 3D pose estimation approach for object categories which significantly outperforms the state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior to retrieve 3D models which accurately represent the geometry of objects in RGB images. For this purpose, we render depth images from 3D models under our predicted pose and match learned image descriptors of RGB images against those of rendered depth images using a CNN-based multi-view metric learning approach. In this way, we are the first to report quantitative results for 3D model retrieval on Pascal3D+, where our method chooses the same models as human annotators for 50% of the validation images on average. In addition, we show that our method, which was trained purely on Pascal3D+, retrieves rich and accurate 3D models from ShapeNet given RGB images of objects in the wild",
    "checked": true,
    "id": "9bbb68103e75b93408b2916fde6e0b42e6c88a9b",
    "semantic_title": "3d pose estimation and 3d model retrieval for objects in the wild",
    "citation_count": 138,
    "authors": [
      "Alexander Grabner",
      "Peter M. Roth",
      "Vincent Lepetit"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Structure_From_Recurrent_CVPR_2018_paper.html": {
    "title": "Structure From Recurrent Motion: From Rigidity to Recurrency",
    "volume": "main",
    "abstract": "This paper proposes a new method for Non-rigidstructure-from-motion (NRSfM). Departing significantlyfrom the traditional idea of using linear low-order shapemodel for NRSfM, our method exploits the property of shaperecurrence (i.e. many dynamic shapes tend to repeat them-selves in time). We show that recurrency is in fact agen-eralized rigidity. Based on this, we show how to reduceNRSfM problems to rigid ones, provided that the recurrencecondition is satisfied. Given such a reduction, standardrigid-SFM techniques can be applied directly (without anychange) to reconstruct the non-rigid dynamic shape. To im-plement this idea as a practical approach, this paper de-velops efficient and reliable algorithm for automatic recur-rence detection, as well as new method for camera viewsclustering via rigidity-check. Experiments on both syntheticsequences and real data demonstrate the effectiveness of theproposed method. Since the method provides novel perspec-tive to look at Structure-from-Motion, we hope it will inspireother new researches in the field",
    "checked": true,
    "id": "49cc860a3effc5b04170ee435ef76463b184a418",
    "semantic_title": "structure from recurrent motion: from rigidity to recurrency",
    "citation_count": 35,
    "authors": [
      "Xiu Li",
      "Hongdong Li",
      "Hanbyul Joo",
      "Yebin Liu",
      "Yaser Sheikh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Poms_Learning_Patch_Reconstructability_CVPR_2018_paper.html": {
    "title": "Learning Patch Reconstructability for Accelerating Multi-View Stereo",
    "volume": "main",
    "abstract": "We present an approach to accelerate multi-view stereo (MVS) by prioritizing computation on image patches that are likely to produce accurate 3D surface reconstructions. Our key insight is that the accuracy of the surface reconstruction from a given image patch can be predicted significantly faster than performing the actual stereo matching. The intuition is that non-specular, fronto-parallel, in-focus patches are more likely to produce accurate surface reconstructions than highly specular, slanted, blurry patches --- and that these properties can be reliably predicted from the image itself. By prioritizing stereo matching on a subset of patches that are highly reconstructable and also cover the 3D surface, we are able to accelerate MVS with minimal reduction in accuracy and completeness. To predict the reconstructability score of an image patch from a single view, we train an image-to-reconstructability neural network: the I2RNet. This reconstructability score enables us to efficiently identify image patches that are likely to provide the most accurate surface estimates before performing stereo matching. We demonstrate that the I2RNet, when trained on the ScanNet dataset, generalizes to the DTU and Tanks and Temples MVS datasets. By using our I2RNet with an existing MVS implementation, we show that our method can achieve more than a 30x speed-up over the baseline with only an minimal loss in completeness",
    "checked": true,
    "id": "0c4ffdd7694c0c40f9a3dfd9ffc9ac9d0f9f469d",
    "semantic_title": "learning patch reconstructability for accelerating multi-view stereo",
    "citation_count": 6,
    "authors": [
      "Alex Poms",
      "Chenglei Wu",
      "Shoou-I Yu",
      "Yaser Sheikh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Progressively_Complementarity-Aware_Fusion_CVPR_2018_paper.html": {
    "title": "Progressively Complementarity-Aware Fusion Network for RGB-D Salient Object Detection",
    "volume": "main",
    "abstract": "How to incorporate cross-modal complementarity sufficiently is the cornerstone question for RGB-D salient object detection. Previous works mainly address this issue by simply concatenating multi-modal features or combining unimodal predictions. In this paper, we answer this question from two perspectives: (1) We argue that if the complementary part can be modelled more explicitly, the cross-modal complement is likely to be better captured. To this end, we design a novel complementarity-aware fusion (CA-Fuse) module when adopting the Convolutional Neural Network (CNN). By introducing cross-modal residual functions and complementarity-aware supervisions in each CA-Fuse module, the problem of learning complementary information from the paired modality is explicitly posed as asymptotically approximating the residual function. (2) Exploring the complement across all the levels. By cascading the CA-Fuse module and adding level-wise supervision from deep to shallow densely, the cross-level complement can be selected and combined progressively. The proposed RGB-D fusion network disambiguates both cross-modal and cross-level fusion processes and enables more sufficient fusion results. The experiments on public datasets show the effectiveness of the proposed CA-Fuse module and the RGB-D salient object detection network",
    "checked": true,
    "id": "a56ff3c9a84286a51a56023300e9d95e1dbebdd0",
    "semantic_title": "progressively complementarity-aware fusion network for rgb-d salient object detection",
    "citation_count": 280,
    "authors": [
      "Hao Chen",
      "Youfu Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shin_Pixels_Voxels_and_CVPR_2018_paper.html": {
    "title": "Pixels, Voxels, and Views: A Study of Shape Representations for Single View 3D Object Shape Prediction",
    "volume": "main",
    "abstract": "The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input. By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewer-centered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our findings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also find that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame significantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition",
    "checked": true,
    "id": "cc0d8144120ed94aa2e9f7b8355b0902e4ddd8b7",
    "semantic_title": "pixels, voxels, and views: a study of shape representations for single view 3d object shape prediction",
    "citation_count": 116,
    "authors": [
      "Daeyun Shin",
      "Charless C. Fowlkes",
      "Derek Hoiem"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Learning_Dual_Convolutional_CVPR_2018_paper.html": {
    "title": "Learning Dual Convolutional Neural Networks for Low-Level Vision",
    "volume": "main",
    "abstract": "In this paper, we propose a general dual convolutional neural network (DualCNN) for low-level vision problems, e.g., super-resolution, edge-preserving filtering, deraining and dehazing. These problems usually involve the estimation of two components of the target signals: structures and details. Motivated by this, our proposed DualCNN consists of two parallel branches, which respectively recovers the structures and details in an end-to-end manner. The recovered structures and details can generate the target signals according to the formation model for each particular application. The DualCNN is a flexible framework for low-level vision tasks and can be easily incorporated with existing CNNs. Experimental results show that the DualCNN can be effectively applied to numerous low-level vision tasks with favorable performance against the state-of-the-art methods",
    "checked": true,
    "id": "85ebb1a27abf180d2319cffb1afa96487dd121bf",
    "semantic_title": "learning dual convolutional neural networks for low-level vision",
    "citation_count": 189,
    "authors": [
      "Jinshan Pan",
      "Sifei Liu",
      "Deqing Sun",
      "Jiawei Zhang",
      "Yang Liu",
      "Jimmy Ren",
      "Zechao Li",
      "Jinhui Tang",
      "Huchuan Lu",
      "Yu-Wing Tai",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Defocus_Blur_Detection_CVPR_2018_paper.html": {
    "title": "Defocus Blur Detection via Multi-Stream Bottom-Top-Bottom Fully Convolutional Network",
    "volume": "main",
    "abstract": "Defocus blur detection (DBD) is the separation of infocus and out-of-focus regions in an image. This process has been paid considerable attention because of its remarkable potential applications. Accurate differentiation of homogeneous regions and detection of low-contrast focal regions, as well as suppression of background clutter, are challenges associated with DBD. To address these issues, we propose a multi-stream bottom-top-bottom fully convolutional network (BTBNet), which is the first attempt to develop an end-to-end deep network for DBD. First, we develop a fully convolutional BTBNet to integrate low-level cues and high-level semantic information. Then, considering that the degree of defocus blur is sensitive to scales, we propose multi-stream BTBNets that handle input images with different scales to improve the performance of DBD. Finally, we design a fusion and recursive reconstruction network to recursively refine the preceding blur detection maps. To promote further study and evaluation of the DBD models, we construct a new database of 500 challenging images and their pixel-wise defocus blur annotations. Experimental results on the existing and our new datasets demonstrate that the proposed method achieves significantly better performance than other state-of-the-art algorithms",
    "checked": true,
    "id": "82d2b42ea4ee9997480a1fd39e3ec59a78f42ef2",
    "semantic_title": "defocus blur detection via multi-stream bottom-top-bottom fully convolutional network",
    "citation_count": 73,
    "authors": [
      "Wenda Zhao",
      "Fan Zhao",
      "Dong Wang",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_PiCANet_Learning_Pixel-Wise_CVPR_2018_paper.html": {
    "title": "PiCANet: Learning Pixel-Wise Contextual Attention for Saliency Detection",
    "volume": "main",
    "abstract": "Contexts play an important role in the saliency detection task. However, given a context region, not all contextual information is helpful for the final task. In this paper, we propose a novel pixel-wise contextual attention network, i.e., the PiCANet, to learn to selectively attend to informative context locations for each pixel. Specifically, for each pixel, it can generate an attention map in which each attention weight corresponds to the contextual relevance at each context location. An attended contextual feature can then be constructed by selectively aggregating the contextual information. We formulate the proposed PiCANet in both global and local forms to attend to global and local contexts, respectively. Both models are fully differentiable and can be embedded into CNNs for joint training. We also incorporate the proposed models with the U-Net architecture to detect salient objects. Extensive experiments show that the proposed PiCANets can consistently improve saliency detection performance. The global and local PiCANets facilitate learning global contrast and homogeneousness, respectively. As a result, our saliency model can detect salient objects more accurately and uniformly, thus performing favorably against the state-of-the-art methods",
    "checked": true,
    "id": "557412c5aee48fb10f97fc95a4e097e40f10700d",
    "semantic_title": "picanet: learning pixel-wise contextual attention for saliency detection",
    "citation_count": 740,
    "authors": [
      "Nian Liu",
      "Junwei Han",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Barnea_Curve_Reconstruction_via_CVPR_2018_paper.html": {
    "title": "Curve Reconstruction via the Global Statistics of Natural Curves",
    "volume": "main",
    "abstract": "Reconstructing the missing parts of a curve has been the subject of much computational research, with applications in image inpainting, object synthesis, etc. Different approaches for solving that problem are typically based on processes that seek visually pleasing or perceptually plausible completions. In this work we focus on reconstructing the underlying physically likely shape by utilizing the global statistics of natural curves. More specifically, we develop a reconstruction model that seeks the mean physical curve for a given inducer configuration. This simple model is both straightforward to compute and it is receptive to diverse additional information, but it requires enough samples for all curve configurations, a practical requirement that limits its effective utilization. To address this practical issue we explore and exploit statistical geometrical properties of natural curves, and in particular, we show that in many cases the mean curve is scale invariant and often times it is extensible. This, in turn, allows to boost the number of examples and thus the robustness of the statistics and its applicability. The reconstruction results are not only more physically plausible but they also lead to important insights on the reconstruction problem, including an elegant explanation why certain inducer configurations are more likely to yield consistent perceptual completions than others",
    "checked": true,
    "id": "699705ffe4b902dcee9f0fe7d26b630f7852f7bc",
    "semantic_title": "curve reconstruction via the global statistics of natural curves",
    "citation_count": 2,
    "authors": [
      "Ehud Barnea",
      "Ohad Ben-Shahar"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Palacio_What_Do_Deep_CVPR_2018_paper.html": {
    "title": "What Do Deep Networks Like to See?",
    "volume": "main",
    "abstract": "We propose a novel way to measure and understand convolutional neural networks by quantifying the amount of input signal they let in. To do this, an autoencoder (AE) was fine-tuned on gradients from a pre-trained classifier with fixed parameters. We compared the reconstructed samples from AEs that were fine-tuned on a set of image classifiers (AlexNet, VGG16, ResNet-50, and Inception~v3) and found substantial differences. The AE learns which aspects of the input space to preserve and which ones to ignore, based on the information encoded in the backpropagated gradients. Measuring the changes in accuracy when the signal of one classifier is used by a second one, a relation of total order emerges. This order depends directly on each classifier's input signal but it does not correlate with classification accuracy or network size. Further evidence of this phenomenon is provided by measuring the normalized mutual information between original images and auto-encoded reconstructions from different fine-tuned AEs. These findings break new ground in the area of neural network understanding, opening a new way to reason, debug, and interpret their results. We present four concrete examples in the literature where observations can now be explained in terms of the input signal that a model uses",
    "checked": true,
    "id": "e2455b6f93640a3415c4e1bc8e34feef5ff19a6c",
    "semantic_title": "what do deep networks like to see?",
    "citation_count": 30,
    "authors": [
      "Sebastian Palacio",
      "Joachim Folz",
      "Jörn Hees",
      "Federico Raue",
      "Damian Borth",
      "Andreas Dengel"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shocher_Zero-Shot_Super-Resolution_Using_CVPR_2018_paper.html": {
    "title": "Zero-Shot\" Super-Resolution Using Deep Internal Learning",
    "volume": "main",
    "abstract": "Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance in the past few years. However, being supervised, these SR methods are restricted to specific training data, where the acquisition of the low-resolution (LR) images from their high-resolution (HR) counterparts is predetermined (e.g., bicubic downscaling), without any distracting artifacts (e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images, however, rarely obey these restrictions, resulting in poor SR results by SotA (State of the Art) methods. In this paper we introduce ``Zero-Shot'' SR, which exploits the power of Deep Learning, but does not rely on prior training. We exploit the internal recurrence of information inside a single image, and train a small image-specific CNN at test time, on examples extracted solely from the input image itself. As such, it can adapt itself to different settings per image. This allows to perform SR of real old photos, noisy images, biological data, and other images where the acquisition process is unknown or non-ideal. On such images, our method outperforms SotA CNN-based SR methods, as well as previous unsupervised SR methods. To the best of our knowledge, this is the first unsupervised CNN-based SR method",
    "checked": true,
    "id": "7a0feeee49ac7692257fb21041e25511bc45192a",
    "semantic_title": "zero-shot super-resolution using deep internal learning",
    "citation_count": 854,
    "authors": [
      "Assaf Shocher",
      "Nadav Cohen",
      "Michal Irani"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Detect_Globally_Refine_CVPR_2018_paper.html": {
    "title": "Detect Globally, Refine Locally: A Novel Approach to Saliency Detection",
    "volume": "main",
    "abstract": "Effective integration of contextual information is crucial for salient object detection. To achieve this, most existing methods based on 'skip' architecture mainly focus on how to integrate hierarchical features of Convolutional Neural Networks (CNNs). They simply apply concatenation or element-wise operation to incorporate high-level semantic cues and low-level detailed information. However, this can degrade the quality of predictions because cluttered and noisy information can also be passed through. To address this problem, we proposes a global Recurrent Localization Network (RLN) which exploits contextual information by the weighted response map in order to localize salient objects more accurately. % and emphasize more on useful ones. Particularly, a recurrent module is employed to progressively refine the inner structure of the CNN over multiple time steps. Moreover, to effectively recover object boundaries, we propose a local Boundary Refinement Network (BRN) to adaptively learn the local contextual information for each spatial position. The learned propagation coefficients can be used to optimally capture relations between each pixel and its neighbors. Experiments on five challenging datasets show that our approach performs favorably against all existing methods in terms of the popular evaluation metrics",
    "checked": true,
    "id": "239f8b0262622e90eb9353b101419027e69c6a50",
    "semantic_title": "detect globally, refine locally: a novel approach to saliency detection",
    "citation_count": 375,
    "authors": [
      "Tiantian Wang",
      "Lihe Zhang",
      "Shuo Wang",
      "Huchuan Lu",
      "Gang Yang",
      "Xiang Ruan",
      "Ali Borji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html": {
    "title": "Beyond the Pixel-Wise Loss for Topology-Aware Delineation",
    "volume": "main",
    "abstract": "Delineation of curvilinear structures is an important problem in Computer Vision with multiple practical applications. With the advent of Deep Learning, many current approaches on automatic delineation have focused on finding more powerful deep architectures, but have continued using the habitual pixel-wise losses such as binary cross-entropy. In this paper we claim that pixel-wise losses alone are unsuitable for this problem because of their inability to reflect the topological importance of prediction errors. Instead, we propose a new loss term that is aware of the higher-order topological features of the linear structures. We also introduce a refinement pipeline that iteratively applies the same model over the previous delineation to refine the predictions at each step while keeping the number of parameters and the complexity of the model constant. When combined with the standard pixel-wise loss, both our new loss term and iterative refinement boost the quality of the predicted delineations, in some cases almost doubling the accuracy as compared to the same classifier trained only with the binary cross-entropy. We show that our approach outperforms state-of-the-art methods on a wide range of data, from microscopy to aerial images",
    "checked": true,
    "id": "8c4b187bdaf91bf068adfe005a0463c4f9c36387",
    "semantic_title": "beyond the pixel-wise loss for topology-aware delineation",
    "citation_count": 232,
    "authors": [
      "Agata Mosinska",
      "Pablo Márquez-Neila",
      "Mateusz Koziński",
      "Pascal Fua"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bauchet_KIPPI_KInetic_Polygonal_CVPR_2018_paper.html": {
    "title": "KIPPI: KInetic Polygonal Partitioning of Images",
    "volume": "main",
    "abstract": "Recent works showed that floating polygons can be an interesting alternative to traditional superpixels, especially for analyzing scenes with strong geometric signatures, as man-made environments. Existing algorithms produce homogeneously-sized polygons that fail to capture thin geometric structures and over-partition large uniform areas. We propose a kinetic approach that brings more flexibility on polygon shape and size. The key idea consists in progressively extending pre-detected line-segments until they meet each other. Our experiments demonstrate that output partitions both contain less polygons and better capture geometric structures than those delivered by existing methods. We also show the applicative potential of the method when used as preprocessing in object contouring",
    "checked": true,
    "id": "4d3c2fab994f6a7237e69ebacd433fe66eeb6a01",
    "semantic_title": "kippi: kinetic polygonal partitioning of images",
    "citation_count": 29,
    "authors": [
      "Jean-Philippe Bauchet",
      "Florent Lafarge"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Image_Blind_Denoising_CVPR_2018_paper.html": {
    "title": "Image Blind Denoising With Generative Adversarial Network Based Noise Modeling",
    "volume": "main",
    "abstract": "In this paper, we consider a typical image blind denoising problem, which is to remove unknown noise from noisy images. As we all know, discriminative learning based methods, such as DnCNN, can achieve state-of-the-art denoising results, but they are not applicable to this problem due to the lack of paired training data. To tackle the barrier, we propose a novel two-step framework. First, a Generative Adversarial Network (GAN) is trained to estimate the noise distribution over the input noisy images and to generate noise samples. Second, the noise patches sampled from the first step are utilized to construct a paired training dataset, which is used, in turn, to train a deep Convolutional Neural Network (CNN) for denoising. Extensive experiments have been done to demonstrate the superiority of our approach in image blind denoising",
    "checked": true,
    "id": "22a979553afa02093cb1b084d1185d8d75a1e0c8",
    "semantic_title": "image blind denoising with generative adversarial network based noise modeling",
    "citation_count": 501,
    "authors": [
      "Jingwen Chen",
      "Jiawei Chen",
      "Hongyang Chao",
      "Ming Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yair_Multi-Scale_Weighted_Nuclear_CVPR_2018_paper.html": {
    "title": "Multi-Scale Weighted Nuclear Norm Image Restoration",
    "volume": "main",
    "abstract": "A prominent property of natural images is that groups of similar patches within them tend to lie on low-dimensional subspaces. This property has been previously used for image denoising, with particularly notable success via weighted nuclear norm minimization (WNNM). In this paper, we extend the WNNM method into a general image restoration algorithm, capable of handling arbitrary degradations (e.g. blur, missing pixels, etc.). Our approach is based on a novel regularization term which simultaneously penalizes for high weighted nuclear norm values of all the patch groups in the image. Our regularizer is isolated from the data-term, thus enabling convenient treatment of arbitrary degradations. Furthermore, it exploits the fractal property of natural images, by accounting for patch similarities also across different scales of the image. We propose a variable splitting method for solving the resulting optimization problem. This leads to an algorithm that is quite different from `plug-and-play' techniques, which solve image-restoration problems using a sequence of denoising steps. As we verify through extensive experiments, our algorithm achieves state of the art results in deblurring and inpainting, outperforming even the recent deep net based methods",
    "checked": true,
    "id": "c9a86dbb568ffda3dcdb907b56bccff187afa666",
    "semantic_title": "multi-scale weighted nuclear norm image restoration",
    "citation_count": 84,
    "authors": [
      "Noam Yair",
      "Tomer Michaeli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gou_MoNet_Moments_Embedding_CVPR_2018_paper.html": {
    "title": "MoNet: Moments Embedding Network",
    "volume": "main",
    "abstract": "Bilinear pooling has been recently proposed as a feature encoding layer, which can be used after the convolutional layers of a deep network, to improve performance in multiple vision tasks. Different from conventional global average pooling or fully connected layer, bilinear pooling gathers 2nd order information in a translation invariant fashion. However, a serious drawback of this family of pooling layers is their dimensionality explosion. Approximate pooling methods with compact properties have been explored towards resolving this weakness. Additionally, recent results have shown that significant performance gains can be achieved by adding 1st order information and applying matrix normalization to regularize unstable higher order information. However, combining compact pooling with matrix normalization and other order information has not been explored until now. In this paper, we unify bilinear pooling and the global Gaussian embedding layers through the empirical moment matrix. In addition, we propose a novel sub-matrix square-root layer, which can be used to normalize the output of the convolution layer directly and mitigate the dimensionality problem with off-the-shelf compact pooling methods. Our experiments on three widely used fine-grained classification datasets illustrate that our proposed architecture, MoNet, can achieve similar or better performance than with the state-of-art G2DeNet. Furthermore, when combined with compact pooling technique, MoNet obtains comparable performance with encoded features with 96% less dimensions",
    "checked": true,
    "id": "d8a54c9f9f81e771059530934c9740c865ec328c",
    "semantic_title": "monet: moments embedding network",
    "citation_count": 31,
    "authors": [
      "Mengran Gou",
      "Fei Xiong",
      "Octavia Camps",
      "Mario Sznaier"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wloka_Active_Fixation_Control_CVPR_2018_paper.html": {
    "title": "Active Fixation Control to Predict Saccade Sequences",
    "volume": "main",
    "abstract": "Visual attention is a field with a considerable history, with eye movement control and prediction forming an important subfield. Fixation modeling in the past decades has been largely dominated computationally by a number of highly influential bottom-up saliency models, such as the Itti-Koch-Niebur model. The accuracy of such models has dramatically increased recently due to deep learning. However, on static images the emphasis of these models has largely been based on non-ordered prediction of fixations through a saliency map. Very few implemented models can generate temporally ordered human-like sequences of saccades beyond an initial fixation point. Towards addressing these shortcomings we present STAR-FC, a novel multi-saccade generator based on the integration of central high-level and object-based saliency and peripheral lower-level feature-based saliency. We have evaluated our model using the CAT2000 database, successfully predicting human patterns of fixation with equivalent accuracy and quality compared to what can be achieved by using one human sequence to predict another",
    "checked": true,
    "id": "dddef2c9c6d8b38a7eba838f801e0855d50c85a4",
    "semantic_title": "active fixation control to predict saccade sequences",
    "citation_count": 39,
    "authors": [
      "Calden Wloka",
      "Iuliia Kotseruba",
      "John K. Tsotsos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Densely_Connected_Pyramid_CVPR_2018_paper.html": {
    "title": "Densely Connected Pyramid Dehazing Network",
    "volume": "main",
    "abstract": "We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incor- We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Net- work (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end- to-end learning is achieved by directly embedding the atmo- spheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense net- work that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi- level pyramid pooling module for estimating the transmis- sion map. This network is optimized using a newly in- troduced edge-preserving loss function. To further incor- porate the mutual structural information between the esti- mated transmission map and the dehazed result, we pro- pose a joint-discriminator based on generative adversar- ial network framework to decide whether the correspond- ing dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demon- strate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Exten- sive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the- art methods. Code and dataset is made available at: https://github.com/hezhangsprinter/DCPDN",
    "checked": true,
    "id": "f9661248e61f4e449e99df51c3f415dd33741358",
    "semantic_title": "densely connected pyramid dehazing network",
    "citation_count": 917,
    "authors": [
      "He Zhang",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lefkimmiatis_Universal_Denoising_Networks_CVPR_2018_paper.html": {
    "title": "Universal Denoising Networks : A Novel CNN Architecture for Image Denoising",
    "volume": "main",
    "abstract": "We design a novel network architecture for learning discriminative image models that are employed to efficiently tackle the problem of grayscale and color image denoising. Based on the proposed architecture, we introduce two different variants. The first network involves convolutional layers as a core component, while the second one relies instead on non-local filtering layers and thus it is able to exploit the inherent non-local self-similarity property of natural images. As opposed to most of the existing deep network approaches, which require the training of a specific model for each considered noise level, the proposed models are able to handle a wide range of noise levels using a single set of learned parameters, while they are very robust when the noise degrading the latent image does not match the statistics of the noise used during training. The latter argument is supported by results that we report on publicly available images corrupted by unknown noise and which we compare against solutions obtained by competing methods. At the same time the introduced networks achieve excellent results under additive white Gaussian noise (AWGN), which are comparable to those of the current state-of-the-art network, while they depend on a more shallow architecture with the number of trained parameters being one order of magnitude smaller. These properties make the proposed networks ideal candidates to serve as sub-solvers on restoration methods that deal with general inverse imaging problems such as deblurring, demosaicking, superresolution, etc",
    "checked": true,
    "id": "635fe55f70feffca01fec7e07e4968b98dba672f",
    "semantic_title": "universal denoising networks : a novel cnn architecture for image denoising",
    "citation_count": 275,
    "authors": [
      "Stamatios Lefkimmiatis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Learning_Convolutional_Networks_CVPR_2018_paper.html": {
    "title": "Learning Convolutional Networks for Content-Weighted Image Compression",
    "volume": "main",
    "abstract": "Lossy image compression is generally formulated as a joint rate-distortion optimization problem to learn encoder, quantizer, and decoder. Due to the non-differentiable quantizer and discrete entropy estimation, it is very challenging to develop a convolutional network (CNN)-based image compression system. In this paper, motivated by that the local information content is spatially variant in an image, we suggest that: (i) the bit rate of the different parts of the image is adapted to local content, and (ii) the content-aware bit rate is allocated under the guidance of a content-weighted importance map. The sum of the importance map can thus serve as a continuous alternative of discrete entropy estimation to control compression rate. The binarizer is adopted to quantize the output of encoder and a proxy function is introduced for approximating binary operation in backward propagation to make it differentiable. The encoder, decoder, binarizer and importance map can be jointly optimized in an end-to-end manner. And a convolutional entropy encoder is further presented for lossless compression of importance map and binary codes. In low bit rate image compression, experiments show that our system significantly outperforms JPEG and JPEG 2000 by structural similarity (SSIM) index, and can produce the much better visual result with sharp edges, rich textures, and fewer artifacts",
    "checked": true,
    "id": "8b16a326afff808284563692d5c3b7982caf68af",
    "semantic_title": "learning convolutional networks for content-weighted image compression",
    "citation_count": 399,
    "authors": [
      "Mu Li",
      "Wangmeng Zuo",
      "Shuhang Gu",
      "Debin Zhao",
      "David Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.html": {
    "title": "Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation",
    "volume": "main",
    "abstract": "Video super-resolution (VSR) has become even more important recently to provide high resolution (HR) contents for ultra high definition displays. While many deep learning based VSR methods have been proposed, most of them rely heavily on the accuracy of motion estimation and compensation. We introduce a fundamentally different framework for VSR in this paper. We propose a novel end-to-end deep neural network that generates dynamic upsampling filters and a residual image, which are computed depending on the local spatio-temporal neighborhood of each pixel to avoid explicit motion compensation. With our approach, an HR image is reconstructed directly from the input image using the dynamic upsampling filters, and the fine details are added through the computed residual. Our network with the help of a new data augmentation technique can generate much sharper HR videos with temporal consistency, compared with the previous methods. We also provide analysis of our network through extensive experiments to show how the network deals with motions implicitly",
    "checked": true,
    "id": "c6a8ff4d9dcdf9e5d82310b847af999ed6655f65",
    "semantic_title": "deep video super-resolution network using dynamic upsampling filters without explicit motion compensation",
    "citation_count": 524,
    "authors": [
      "Younghyun Jo",
      "Seoung Wug Oh",
      "Jaeyeon Kang",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Erase_or_Fill_CVPR_2018_paper.html": {
    "title": "Erase or Fill? Deep Joint Recurrent Rain Removal and Reconstruction in Videos",
    "volume": "main",
    "abstract": "In this paper, we address the problem of video rain removal by constructing deep recurrent convolutional networks. We visit the rain removal case by considering rain occlusion regions, i.e. light transmittance of rain streaks is low. Different from additive rain streaks, in such rain occlusion regions, the details of background images are completely lost. Therefore, we propose a hybrid rain model to depict both rain streaks and occlusions. With the wealth of temporal redundancy, we build a Joint Recurrent Rain Removal and Reconstruction Network (J4R-Net) that seamlessly integrates rain degradation classification, spatial texture appearances based rain removal and temporal coherence based background details reconstruction. The rain degradation classification provides a binary map that reveals whether a location degraded by linear additive streaks or occlusions. With this side information, the gate of the recurrent unit learns to make a trade-off between rain streak removal and background details reconstruction. Extensive experiments on a series of synthetic and real videos with rain streaks verify the superiority of the proposed method over previous state-of-the-art methods",
    "checked": true,
    "id": "c6d3e1873b1ec49c1b829e5e35a2e451f9cfd312",
    "semantic_title": "erase or fill? deep joint recurrent rain removal and reconstruction in videos",
    "citation_count": 162,
    "authors": [
      "Jiaying Liu",
      "Wenhan Yang",
      "Shuai Yang",
      "Zongming Guo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Flow_Guided_Recurrent_CVPR_2018_paper.html": {
    "title": "Flow Guided Recurrent Neural Encoder for Video Salient Object Detection",
    "volume": "main",
    "abstract": "Image saliency detection has recently witnessed significant progress due to deep convolutional neural networks. However, extending state-of-the-art saliency detectors from image to video is challenging. The performance of salient object detection suffers from object or camera motion and the dramatic change of the appearance contrast in videos. In this paper, we present flow guided recurrent neural encoder(FGRNE), an accurate and end-to-end learning framework for video salient object detection. It works by enhancing the temporal coherence of the per-frame feature by exploiting both motion information in terms of optical flow and sequential feature evolution encoding in terms of LSTM networks. It can be considered as a universal framework to extend any FCN based static saliency detector to video salient object detection. Intensive experimental results verify the effectiveness of each part of FGRNE and confirm that our proposed method significantly outperforms state-of-the-art methods on the public benchmarks of DAVIS and FBMS",
    "checked": true,
    "id": "e50298e1c2c21cd4f33bc4b68d5da6f6d04796b5",
    "semantic_title": "flow guided recurrent neural encoder for video salient object detection",
    "citation_count": 144,
    "authors": [
      "Guanbin Li",
      "Yuan Xie",
      "Tianhao Wei",
      "Keze Wang",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_Gated_Fusion_Network_CVPR_2018_paper.html": {
    "title": "Gated Fusion Network for Single Image Dehazing",
    "volume": "main",
    "abstract": "In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale based approach so that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms",
    "checked": true,
    "id": "dbd21420cd1299fdab9a04c6b4b3b5b4d4a38a6d",
    "semantic_title": "gated fusion network for single image dehazing",
    "citation_count": 732,
    "authors": [
      "Wenqi Ren",
      "Lin Ma",
      "Jiawei Zhang",
      "Jinshan Pan",
      "Xiaochun Cao",
      "Wei Liu",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Learning_a_Single_CVPR_2018_paper.html": {
    "title": "Learning a Single Convolutional Super-Resolution Network for Multiple Degradations",
    "volume": "main",
    "abstract": "Recent years have witnessed the unprecedented success of deep convolutional neural networks (CNNs) in single image super-resolution (SISR). However, existing CNN-based SISR methods mostly assume that a low-resolution (LR) image is bicubicly downsampled from a high-resolution (HR) image, thus inevitably giving rise to poor performance when the true degradation does not follow this assumption. Moreover, they lack scalability in learning a single model to non-blindly deal with multiple degradations. To address these issues, we propose a general framework with dimensionality stretching strategy that enables a single convolutional super-resolution network to take two key factors of the SISR degradation process, i.e., blur kernel and noise level, as input. Consequently, the super-resolver can handle multiple and even spatially variant degradations, which significantly improves the practicability. Extensive experimental results on synthetic and real LR images show that the proposed convolutional super-resolution network not only can produce favorable results on multiple degradations but also is computationally efficient, providing a highly effective and scalable solution to practical SISR applications",
    "checked": true,
    "id": "b26b4c9da872d3c6122dbf23b9d6e063dc6456b5",
    "semantic_title": "learning a single convolutional super-resolution network for multiple degradations",
    "citation_count": 791,
    "authors": [
      "Kai Zhang",
      "Wangmeng Zuo",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Vasu_Non-Blind_Deblurring_Handling_CVPR_2018_paper.html": {
    "title": "Non-Blind Deblurring: Handling Kernel Uncertainty With CNNs",
    "volume": "main",
    "abstract": "Blind motion deblurring methods are primarily responsible for recovering an accurate estimate of the blur kernel. Non-blind deblurring (NBD) methods, on the other hand, attempt to faithfully restore the original image, given the blur estimate. However, NBD is quite susceptible to errors in blur kernel. In this work, we present a convolutional neural network-based approach to handle kernel uncertainty in non-blind motion deblurring. We provide multiple latent image estimates corresponding to different prior strengths obtained from a given blurry observation in order to exploit the complementarity of these inputs for improved learning. To generalize the performance to tackle arbitrary kernel noise, we train our network with a large number of real and synthetic noisy blur kernels. Our network mitigates the effects of kernel noise so as to yield detail-preserving and artifact-free restoration. Our quantitative and qualitative evaluations on benchmark datasets demonstrate that the proposed method delivers state-of-the-art results. To further underscore the benefits that can be achieved from our network, we propose two adaptations of our method to improve kernel estimates, and image deblurring quality, respectively",
    "checked": true,
    "id": "e2f1f37fc4b79d9c3e8a9911f8e3b21e3278efc0",
    "semantic_title": "non-blind deblurring: handling kernel uncertainty with cnns",
    "citation_count": 61,
    "authors": [
      "Subeesh Vasu",
      "Venkatesh Reddy Maligireddy",
      "A. N. Rajagopalan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lei_Boundary_Flow_A_CVPR_2018_paper.html": {
    "title": "Boundary Flow: A Siamese Network That Predicts Boundary Motion Without Training on Motion",
    "volume": "main",
    "abstract": "Using deep learning, this paper addresses the problem of joint object boundary detection and boundary motion estimation in videos, which we named boundary flow estimation. Boundary flow is an important mid-level visual cue as boundaries characterize objects' spatial extents, and the flow indicates objects' motions and interactions. Yet, most prior work on motion estimation has focused on dense object motion or feature points that may not necessarily reside on boundaries. For boundary flow estimation, we specify a new fully convolutional Siamese network (FCSN) that jointly estimates object-level boundaries in two consecutive frames. Boundary correspondences in the two frames are predicted by the same FCSN with a new, unconventional deconvolution approach. Finally, the boundary flow estimate is improved with an edgelet-based filtering. Evaluation is conducted on three tasks: boundary detection in videos, boundary flow estimation, and optical flow estimation. On boundary detection, we achieve the state-of-the-art performance on the benchmark VSB100 dataset. On boundary flow estimation, we present the first results on the Sintel training dataset. For optical flow estimation, we run the recent approach CPM-Flow but on the augmented input with our boundary-flow matches, and achieve significant performance improvement on the Sintel benchmark",
    "checked": true,
    "id": "1fddb3344e75db6d864faf29c71bad5b55b236f2",
    "semantic_title": "boundary flow: a siamese network that predicts boundary motion without training on motion",
    "citation_count": 13,
    "authors": [
      "Peng Lei",
      "Fuxin Li",
      "Sinisa Todorovic"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Learning_to_See_CVPR_2018_paper.html": {
    "title": "Learning to See in the Dark",
    "volume": "main",
    "abstract": "Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can lead to blurry images and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work",
    "checked": true,
    "id": "2286a6ae670644c797ff1e6766b7af7df44a61d5",
    "semantic_title": "learning to see in the dark",
    "citation_count": 935,
    "authors": [
      "Chen Chen",
      "Qifeng Chen",
      "Jia Xu",
      "Vladlen Koltun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_BPGrad_Towards_Global_CVPR_2018_paper.html": {
    "title": "BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning",
    "volume": "main",
    "abstract": "Understanding the global optimality in deep learning (DL) has been attracting more and more attention recently. Conventional DL solvers, however, have not been developed intentionally to seek for such global optimality. In this paper we propose a novel approximation algorithm, {em BPGrad}, towards optimizing deep models globally via branch and pruning. Our BPGrad is based on the assumption of Lipschitz continuity in DL, and as a result it can adaptively determine the step size for current gradient given the history of previous updates, wherein theoretically no smaller steps can achieve the global optimality. We prove that by repeating such branch-and-pruning procedure, we can locate the global optimality within finite iterations. Empirically an efficient solver based on BPGrad for DL is proposed as well, and it outperforms conventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the tasks of object recognition, detection, and segmentation",
    "checked": true,
    "id": "1a7538ae59b24a0fbcf9c936272d8cd365ff9ed9",
    "semantic_title": "bpgrad: towards global optimality in deep learning via branch and pruning",
    "citation_count": 28,
    "authors": [
      "Ziming Zhang",
      "Yuanwei Wu",
      "Guanghui Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper.html": {
    "title": "Perturbative Neural Networks",
    "volume": "main",
    "abstract": "Convolutional neural networks are witnessing wide adoption in computer vision systems with numerous applications across a range of visual recognition tasks. Much of this progress is fueled through advances in convolutional neural network architectures and learning algorithms even as the basic premise of a convolutional layer has remained unchanged. In this paper, we seek to revisit the convolutional layer that has been the workhorse of state-of-the-art visual recognition models. We introduce a very simple, yet effective, module called a perturbation layer as an alternative to a convolutional layer. The perturbation layer does away with convolution in the traditional sense and instead computes its response as a weighted linear combination of non-linearly activated additive noise perturbed inputs. We demonstrate both analytically and empirically that this perturbation layer can be an effective replacement for a standard convolutional layer. Empirically, deep neural networks with perturbation layers, called Perturbative Neural Networks (PNNs), in lieu of convolutional layers perform comparably with standard CNNs on a range of visual datasets (MNIST, CIFAR-10, PASCAL VOC, and ImageNet) with fewer parameters",
    "checked": true,
    "id": "2e051a825b286a7b8a987262294f2ddb6c172c2a",
    "semantic_title": "perturbative neural networks",
    "citation_count": 38,
    "authors": [
      "Felix Juefei-Xu",
      "Vishnu Naresh Boddeti",
      "Marios Savvides"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hoshen_Unsupervised_Correlation_Analysis_CVPR_2018_paper.html": {
    "title": "Unsupervised Correlation Analysis",
    "volume": "main",
    "abstract": "Linking between two data sources is a basic building block in numerous computer vision problems. In this paper, we set to answer a fundamental cognitive question: are prior correspondences necessary for linking between different domains? One of the most popular methods for linking between domains is Canonical Correlation Analysis (CCA). All current CCA algorithms require correspondences between the views. We introduce a new method Unsupervised Correlation Analysis (UCA), which requires no prior correspondences between the two domains. The correlation maximization term in CCA is replaced by a combination of a reconstruction term (similar to autoencoders), full cycle loss, orthogonality and multiple domain confusion terms. Due to lack of supervision, the optimization leads to multiple alternative solutions with similar scores and we therefore introduce a consensus-based mechanism that is often able to recover the desired solution. Remarkably, this suffices in order to link remote domains such as text and images. We also present results on well accepted CCA benchmarks, showing that performance far exceeds other unsupervised baselines, and approaches supervised performance in some cases",
    "checked": true,
    "id": "5514a0c7a739fb52106c2975892d998510ef5dec",
    "semantic_title": "unsupervised correlation analysis",
    "citation_count": 8,
    "authors": [
      "Yedid Hoshen",
      "Lior Wolf"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mukherjee_A_Biresolution_Spectral_CVPR_2018_paper.html": {
    "title": "A Biresolution Spectral Framework for Product Quantization",
    "volume": "main",
    "abstract": "Product quantization (PQ) (and its variants) has been effec- tively used to encode high-dimensional data into compact codes for many problems in vision. In principle, PQ decomposes the given data into a number of lower-dimensional subspaces where the quantization proceeds independently for each subspace. While the original PQ approach does not explicitly optimize for these subspaces, later proposals have argued that the performance tends to benefit significantly if such subspaces are chosen in an optimal manner. Despite such consensus, existing approaches in the literature diverge in terms of which specific properties of these subspaces are desirable and how one should proceed to solve/optimize them. Nonetheless, despite the empirical support, there is less clarity regarding the theoretical properties that underlie these experimental benefits for quantization problems in general. In this paper, we study the quantization problem in the setting where subspaces are orthogonal and show that this problem is intricately related to a specific type of spectral decomposition of the data. This insight not only opens the door to a rich body of work in spectral analysis, but also leads to distinct computational benefits. Our resultant biresolution spectral formulation captures both the subspace projection error as well as the quantization error within the same framework. After a reformulation, the core steps of our algorithm involve a simple eigen decomposition step, which can be solved efficiently. We show that our method performs very favorably against a number of state of the art methods on standard data sets",
    "checked": true,
    "id": "be773b76fb29badf9a0043ed675ef26ba323490e",
    "semantic_title": "a biresolution spectral framework for product quantization",
    "citation_count": 2,
    "authors": [
      "Lopamudra Mukherjee",
      "Sathya N. Ravi",
      "Jiming Peng",
      "Vikas Singh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Domain_Adaptive_Faster_CVPR_2018_paper.html": {
    "title": "Domain Adaptive Faster R-CNN for Object Detection in the Wild",
    "volume": "main",
    "abstract": "Object detection typically assumes that training and test data are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch will lead to a significant performance drop. In this work, we aim to improve the cross-domain robustness of object detection. We tackle the domain shift on two levels: 1) the image-level shift, such as image style, illumination, etc, and 2) the instance-level shift, such as object appearance, size, etc. We build our approach based on the recent state-of-the-art Faster R-CNN model, and design two domain adaptation components, on image level and instance level, to reduce the domain discrepancy. The two domain adaptation components are based on H-divergence theory, and are implemented by learning a domain classifier in adversarial training manner. The domain classifiers on different levels are further reinforced with a consistency regularization to learn a domain-invariant region proposal network (RPN) in the Faster R-CNN model. We evaluate our newly proposed approach using multiple datasets including Cityscapes, KITTI, SIM10K, etc. The results demonstrate the effectiveness of our proposed approach for robust object detection in various domain shift scenarios",
    "checked": true,
    "id": "04a7cab83c5b1b5dd13dcb8632fae3f24150f873",
    "semantic_title": "domain adaptive faster r-cnn for object detection in the wild",
    "citation_count": 1300,
    "authors": [
      "Yuhua Chen",
      "Wen Li",
      "Christos Sakaridis",
      "Dengxin Dai",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Douze_Low-Shot_Learning_With_CVPR_2018_paper.html": {
    "title": "Low-Shot Learning With Large-Scale Diffusion",
    "volume": "main",
    "abstract": "This paper considers the problem of inferring image labels from images when only a few annotated examples are available at training time. This setup is often referred to as low-shot learning, where a standard approach is to re-train the last few layers of a convolutional neural network learned on separate classes for which training examples are abundant. We consider a semi-supervised setting based on a large collection of images to support label propagation. This is possible by leveraging the recent advances on large-scale similarity graph construction. We show that despite its conceptual simplicity, scaling label propagation up to hundred millions of images leads to state of the art accuracy in the low-shot learning regime",
    "checked": true,
    "id": "6e9680fe35a752590ad2d750ba1aa2b387cba135",
    "semantic_title": "low-shot learning with large-scale diffusion",
    "citation_count": 106,
    "authors": [
      "Matthijs Douze",
      "Arthur Szlam",
      "Bharath Hariharan",
      "Hervé Jégou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Joint_Pose_and_CVPR_2018_paper.html": {
    "title": "Joint Pose and Expression Modeling for Facial Expression Recognition",
    "volume": "main",
    "abstract": "Facial expression recognition (FER) is a challenging task due to different expressions under arbitrary poses. Most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifiers for each pose. Different from existing methods, in this paper, we propose an end-to-end deep learning model by exploiting different poses and expressions jointly for simultaneous facial image synthesis and pose-invariant facial expression recognition. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, the encoder-decoder structure of the generator can learn a generative and discriminative identity representation for face images. Second, the identity representation is explicitly disentangled from both expression and pose variations through the expression and pose codes. Third, our model can automatically generate face images with different expressions under arbitrary poses to enlarge and enrich the training set for FER. Quantitative and qualitative evaluations on both controlled and in-the-wild datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods",
    "checked": true,
    "id": "9da429ce36afc75f2c457880a9d7129220ac9225",
    "semantic_title": "joint pose and expression modeling for facial expression recognition",
    "citation_count": 196,
    "authors": [
      "Feifei Zhang",
      "Tianzhu Zhang",
      "Qirong Mao",
      "Changsheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gast_Lightweight_Probabilistic_Deep_CVPR_2018_paper.html": {
    "title": "Lightweight Probabilistic Deep Networks",
    "volume": "main",
    "abstract": "Even though probabilistic treatments of neural networks have a long history, they have not found widespread use in practice. Sampling approaches are often too slow already for simple networks. The size of the inputs and the depth of typical CNN architectures in computer vision only compound this problem. Uncertainty in neural networks has thus been largely ignored in practice, despite the fact that it may provide important information about the reliability of predictions and the inner workings of the network. In this paper, we introduce two lightweight approaches to making supervised learning with probabilistic deep networks practical: First, we suggest probabilistic output layers for classification and regression that require only minimal changes to existing networks. Second, we employ assumed density filtering and show that activation uncertainties can be propagated in a practical fashion through the entire network, again with minor changes. Both probabilistic networks retain the predictive power of the deterministic counterpart, but yield uncertainties that correlate well with the empirical error induced by their predictions. Moreover, the robustness to adversarial examples is significantly increased",
    "checked": true,
    "id": "970351c4c9820d1563e95493e1a7f11ff343b815",
    "semantic_title": "lightweight probabilistic deep networks",
    "citation_count": 181,
    "authors": [
      "Jochen Gast",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper.html": {
    "title": "Adversarially Learned One-Class Classifier for Novelty Detection",
    "volume": "main",
    "abstract": "Novelty detection is the process of identifying the observation(s) that differ in some respect from the training observations (the target class). In reality, the novelty class is often absent during training, poorly sampled or not well defined. Therefore, one-class classifiers can efficiently model such problems. However, due to the unavailability of data from the novelty class, training an end-to-end deep network is a cumbersome task. In this paper, inspired by the success of generative adversarial networks for training deep models in unsupervised and semi-supervised settings, we propose an end-to-end architecture for one-class classification. Our architecture is composed of two deep networks, each of which trained by competing with each other while collaborating to understand the underlying concept in the target class, and then classify the testing samples. One network works as the novelty detector, while the other supports it by enhancing the inlier samples and distorting the outliers. The intuition is that the separability of the enhanced inliers and distorted outliers is much better than deciding on the original samples. The proposed framework applies to different related applications of anomaly and outlier detection in images and videos. The results on MNIST and Caltech-256 image datasets, along with the challenging UCSD Ped2 dataset for video anomaly detection illustrate that our proposed method learns the target class effectively and is superior to the baseline and state-of-the-art methods",
    "checked": true,
    "id": "8381157eae4fbf8908d0312a9642f8e69e944449",
    "semantic_title": "adversarially learned one-class classifier for novelty detection",
    "citation_count": 693,
    "authors": [
      "Mohammad Sabokrou",
      "Mohammad Khalooei",
      "Mahmood Fathy",
      "Ehsan Adeli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Akhtar_Defense_Against_Universal_CVPR_2018_paper.html": {
    "title": "Defense Against Universal Adversarial Perturbations",
    "volume": "main",
    "abstract": "Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to `any' image can fool a state-of-the-art network classifier to change its prediction about the image label. These `Universal Adversarial Perturbations' pose a serious threat to the success of Deep Learning in practice. We present the first dedicated framework to effectively defend the networks against such perturbations. Our approach learns a Perturbation Rectifying Network (PRN) as `pre-input' layers to a targeted model, such that the targeted model needs no modification. The PRN is learned from real and synthetic image-agnostic perturbations, where an efficient method to compute the latter is also proposed. A perturbation detector is separately trained on the Discrete Cosine Transform of the input-output difference of the PRN. A query image is first passed through the PRN and verified by the detector. If a perturbation is detected, the output of the PRN is used for label prediction instead of the actual image. A rigorous evaluation shows that our framework can defend the network classifiers against unseen adversarial perturbations in the real-world scenarios with up to 96.4% success rate. The PRN also generalizes well in the sense that training for one targeted network defends another network with a comparable success rate",
    "checked": true,
    "id": "c4413dd4a51ab86d09c165d4b2fe1dc2168fc1ff",
    "semantic_title": "defense against universal adversarial perturbations",
    "citation_count": 208,
    "authors": [
      "Naveed Akhtar",
      "Jian Liu",
      "Ajmal Mian"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Disentangling_Factors_of_CVPR_2018_paper.html": {
    "title": "Disentangling Factors of Variation by Mixing Them",
    "volume": "main",
    "abstract": "We propose an approach to learn image representations that consist of disentangled factors of variation without exploiting any manual labeling or data domain knowledge. A factor of variation corresponds to an image attribute that can be discerned consistently across a set of images, such as the pose or color of objects. Our disentangled representation consists of a concatenation of feature chunks, each chunk representing a factor of variation. It supports applications such as transferring attributes from one image to another, by simply mixing and unmixing feature chunks, and classification or retrieval based on one or several attributes, by considering a user-specified subset of feature chunks. We learn our representation without any labeling or knowledge of the data domain, using an autoencoder architecture with two novel training objectives: first, we propose an invariance objective to encourage that encoding of each attribute, and decoding of each chunk, are invariant to changes in other attributes and chunks, respectively; second, we include a classification objective, which ensures that each chunk corresponds to a consistently discernible attribute in the represented image, hence avoiding degenerate feature mappings where some chunks are completely ignored. We demonstrate the effectiveness of our approach on the MNIST, Sprites, and CelebA datasets",
    "checked": true,
    "id": "e0f828376631b362aa550246c73d85e354126fed",
    "semantic_title": "disentangling factors of variation by mixing them",
    "citation_count": 76,
    "authors": [
      "Qiyang Hu",
      "Attila Szabó",
      "Tiziano Portenier",
      "Paolo Favaro",
      "Matthias Zwicker"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Siarohin_Deformable_GANs_for_CVPR_2018_paper.html": {
    "title": "Deformable GANs for Pose-Based Human Image Generation",
    "volume": "main",
    "abstract": "In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector",
    "checked": true,
    "id": "74b9632e8c7bc7c96af5561a017b40b9613f196d",
    "semantic_title": "deformable gans for pose-based human image generation",
    "citation_count": 453,
    "authors": [
      "Aliaksandr Siarohin",
      "Enver Sangineto",
      "Stéphane Lathuilière",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Homayounfar_Hierarchical_Recurrent_Attention_CVPR_2018_paper.html": {
    "title": "Hierarchical Recurrent Attention Networks for Structured Online Maps",
    "volume": "main",
    "abstract": "In this paper, we tackle the problem of online road network extraction from sparse 3D point clouds. Our method is inspired by how an annotator builds a lane graph, by first identifying how many lanes there are and then drawing each one in turn. We develop a hierarchical recurrent network that attends to initial regions of a lane boundary and traces them out completely by outputting a structured polyline. We also propose a novel differentiable loss function that measures the deviation of the edges of the ground truth polylines and their predictions. This is more suitable than distances on vertices, as there exists many ways to draw equivalent polylines. We demonstrate the effectiveness of our method on a 90 km stretch of highway, and show that we can recover the right topology 92% of the time",
    "checked": true,
    "id": "5f84985d2c9d950f3c878781d25a4603227a7e35",
    "semantic_title": "hierarchical recurrent attention networks for structured online maps",
    "citation_count": 67,
    "authors": [
      "Namdar Homayounfar",
      "Wei-Chiu Ma",
      "Shrinidhi Kowshika Lakshmikanth",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kolouri_Sliced_Wasserstein_Distance_CVPR_2018_paper.html": {
    "title": "Sliced Wasserstein Distance for Learning Gaussian Mixture Models",
    "volume": "main",
    "abstract": "Gaussian mixture models (GMM) are powerful parametric tools with many applications in machine learning and computer vision. Expectation maximization (EM) is the most popular algorithm for estimating the GMM parameters. However, EM guarantees only convergence to a stationary point of the log-likelihood function, which could be arbitrarily worse than the optimal solution. Inspired by the relationship between the negative log-likelihood function and the Kullback-Leibler (KL) divergence, we propose an alternative formulation for estimating the GMM parameters using the sliced Wasserstein distance, which gives rise to a new algorithm. Specifically, we propose minimizing the sliced-Wasserstein distance between the mixture model and the data distribution with respect to the GMM parameters. In contrast to the KL-divergence, the energy landscape for the sliced-Wasserstein distance is more well-behaved and therefore more suitable for a stochastic gradient descent scheme to obtain the optimal GMM parameters. We show that our formulation results in parameter estimates that are more robust to random initializations and demonstrate that it can estimate high-dimensional data distributions more faithfully than the EM algorithm",
    "checked": true,
    "id": "e7070b52403920d914d483285a42addb2de9ef2b",
    "semantic_title": "sliced wasserstein distance for learning gaussian mixture models",
    "citation_count": 123,
    "authors": [
      "Soheil Kolouri",
      "Gustavo K. Rohde",
      "Heiko Hoffmann"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Aligning_Infinite-Dimensional_Covariance_CVPR_2018_paper.html": {
    "title": "Aligning Infinite-Dimensional Covariance Matrices in Reproducing Kernel Hilbert Spaces for Domain Adaptation",
    "volume": "main",
    "abstract": "Domain shift, which occurs when there is a mismatch between the distributions of training (source) and testing (target) datasets, usually results in poor performance of the trained model on the target domain. Existing algorithms typically solve this issue by reducing the distribution discrepancy in the input spaces. However, for kernel-based learning machines, performance highly depends on the statistical properties of data in reproducing kernel Hilbert spaces (RKHS). Motivated by these considerations, we propose a novel strategy for matching distributions in RKHS, which is done by aligning the RKHS covariance matrices (descriptors) across domains. This strategy is a generalization of the correlation alignment problem in Euclidean spaces to (potentially) infinite-dimensional feature spaces. In this paper, we provide two alignment approaches, for both of which we obtain closed-form expressions via kernel matrices. Furthermore, our approaches are scalable to large datasets since they can naturally handle out-of-sample instances. We conduct extensive experiments (248 domain adaptation tasks) to evaluate our approaches. Experiment results show that our approaches outperform other state-of-the-art methods in both accuracy and computationally efficiency",
    "checked": true,
    "id": "f3a8c336c339a7bc9610d23b69486f51c801b3d5",
    "semantic_title": "aligning infinite-dimensional covariance matrices in reproducing kernel hilbert spaces for domain adaptation",
    "citation_count": 55,
    "authors": [
      "Zhen Zhang",
      "Mianzhi Wang",
      "Yan Huang",
      "Arye Nehorai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kozerawski_CLEAR_Cumulative_LEARning_CVPR_2018_paper.html": {
    "title": "CLEAR: Cumulative LEARning for One-Shot One-Class Image Recognition",
    "volume": "main",
    "abstract": "This work addresses the novel problem of one-shot one-class classification. The goal is to estimate a classification decision boundary for a novel class based on a single image example. Our method exploits transfer learning to model the transformation from a representation of the input, extracted by a Convolutional Neural Network, to a classification decision boundary. We use a deep neural network to learn this transformation from a large labelled dataset of images and their associated class decision boundaries generated from ImageNet, and then apply the learned decision boundary to classify subsequent query images. We tested our approach on several benchmark datasets and significantly outperformed the baseline methods",
    "checked": true,
    "id": "346a877564351e4014441a1dc174b0369a759ba5",
    "semantic_title": "clear: cumulative learning for one-shot one-class image recognition",
    "citation_count": 38,
    "authors": [
      "Jedrzej Kozerawski",
      "Matthew Turk"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ikami_Local_and_Global_CVPR_2018_paper.html": {
    "title": "Local and Global Optimization Techniques in Graph-Based Clustering",
    "volume": "main",
    "abstract": "The goal of graph-based clustering is to divide a dataset into disjoint subsets with members similar to each other from an affinity (similarity) matrix between data. The most popular method of solving graph-based clustering is spectral clustering. However, spectral clustering has drawbacks. Spectral clustering can only be applied to macro-average-based cost functions, which tend to generate undesirable small clusters. This study first introduces a novel cost function based on micro-average. We propose a local optimization method, which is widely applicable to graph-based clustering cost functions. We also propose an initial-guess-free algorithm to avoid its initialization dependency. Moreover, we present two global optimization techniques. The experimental results exhibit significant clustering performances from our proposed methods, including 100% clustering accuracy in the COIL-20 dataset",
    "checked": true,
    "id": "acf579a58e81b5e5e84ca9c060e1ff9dee941162",
    "semantic_title": "local and global optimization techniques in graph-based clustering",
    "citation_count": 4,
    "authors": [
      "Daiki Ikami",
      "Toshihiko Yamasaki",
      "Kiyoharu Aizawa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mejjati_Multi-Task_Learning_by_CVPR_2018_paper.html": {
    "title": "Multi-Task Learning by Maximizing Statistical Dependence",
    "volume": "main",
    "abstract": "We present a new multi-task learning (MTL) approach that can be applied to multiple heterogeneous task estimators. Our motivation is that the best task estimator could change depending on the task itself. For example, we may have a deep neural network for the first task and a Gaussian process for the second task. Classical MTL approaches cannot handle this case, as they require the same model or even the same parameter types for all tasks. We tackle this by considering task-specific estimators as random variables. Then, the task relationships are discovered by measuring the statistical dependence between each pair of random variables. By doing so, our model is independent of the parametric nature of each task, and is even agnostic to the existence of such parametric formulation. We compare our algorithm with existing MTL approaches on challenging real world ranking and regression datasets, and show that our approach achieves comparable or better performance without knowing the parametric form",
    "checked": true,
    "id": "60d9cf68e9be34db5e271a8b3ec01005ef3fdbe8",
    "semantic_title": "multi-task learning by maximizing statistical dependence",
    "citation_count": 11,
    "authors": [
      "Youssef A. Mejjati",
      "Darren Cosker",
      "Kwang In Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Robust_Classification_With_CVPR_2018_paper.html": {
    "title": "Robust Classification With Convolutional Prototype Learning",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNNs) have been widely used for image classification. Despite its high accuracies, CNN has been shown to be easily fooled by some adversarial examples, indicating that CNN is not robust enough for pattern classification. In this paper, we argue that the lack of robustness for CNN is caused by the softmax layer, which is a totally discriminative model and based on the assumption of closed world (i.e., with a fixed number of categories). To improve the robustness, we propose a novel learning framework called convolutional prototype learning (CPL). The advantage of using prototypes is that it can well handle the open world recognition problem and therefore improve the robustness. Under the framework of CPL, we design multiple classification criteria to train the network. Moreover, a prototype loss (PL) is proposed as a regularization to improve the intra-class compactness of the feature representation, which can be viewed as a generative model based on the Gaussian assumption of different classes. Experiments on several datasets demonstrate that CPL can achieve comparable or even better results than traditional CNN, and from the robustness perspective, CPL shows great advantages for both the rejection and incremental category learning tasks",
    "checked": true,
    "id": "d06c891ef40303e19f7caac78f77b93b1c490cc6",
    "semantic_title": "robust classification with convolutional prototype learning",
    "citation_count": 342,
    "authors": [
      "Hong-Ming Yang",
      "Xu-Yao Zhang",
      "Fei Yin",
      "Cheng-Lin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Deshpande_Generative_Modeling_Using_CVPR_2018_paper.html": {
    "title": "Generative Modeling Using the Sliced Wasserstein Distance",
    "volume": "main",
    "abstract": "Generative Adversarial Nets (GANs) are very successful at modeling distributions from given samples, even in the high-dimensional case. However, their formulation is also known to be hard to optimize and often not stable. While this is particularly true for early GAN formulations, there has been significant empirically motivated and theoretically founded progress to improve stability, for instance, by using the Wasserstein distance rather than the Jenson-Shannon divergence. Here, we consider an alternative formulation for generative modeling based on random projections which, in its simplest form, results in a single objective rather than a saddle-point formulation. By augmenting this approach with a discriminator we improve its accuracy. We found our ap- proach to be significantly more stable compared to even the improved Wasserstein GAN. Further, unlike the traditional GAN loss, the loss formulated in our method is a good mea- sure of the actual distance between the distributions and, for the first time for GAN training, we are able to show estimates for the same",
    "checked": true,
    "id": "f2fa3a0cd1ade8af75cba1c11e8236a8027e2ef1",
    "semantic_title": "generative modeling using the sliced wasserstein distance",
    "citation_count": 226,
    "authors": [
      "Ishan Deshpande",
      "Ziyu Zhang",
      "Alexander G. Schwing"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Veniat_Learning_TimeMemory-Efficient_Deep_CVPR_2018_paper.html": {
    "title": "Learning Time/Memory-Efficient Deep Architectures With Budgeted Super Networks",
    "volume": "main",
    "abstract": "We propose to focus on the problem of discovering neural network architectures efficient in terms of both prediction quality and cost. For instance, our approach is able to solve the following tasks: learn a neural network able to predict well in less than 100 milliseconds or learn an efficient model that fits in a 50 Mb memory. Our contribution is a novel family of models called Budgeted Super Networks (BSN). They are learned using gradient descent techniques applied on a budgeted learning objective function which integrates a maximum authorized cost, while making no assumption on the nature of this cost. We present a set of experiments on computer vision problems and analyze the ability of our technique to deal with three different costs: the computation cost, the memory consumption cost and a distributed computation cost. We particularly show that our model can discover neural network architectures that have a better accuracy than the ResNet and Convolutional Neural Fabrics architectures on CIFAR-10 and CIFAR-100, at a lower cost",
    "checked": true,
    "id": "3b8aa7a38bc60f7b42757e53a0801f6e71dcef5c",
    "semantic_title": "learning time/memory-efficient deep architectures with budgeted super networks",
    "citation_count": 76,
    "authors": [
      "Tom Véniat",
      "Ludovic Denoyer"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Regmi_Cross-View_Image_Synthesis_CVPR_2018_paper.html": {
    "title": "Cross-View Image Synthesis Using Conditional GANs",
    "volume": "main",
    "abstract": "Learning to generate natural scenes has always been a challenging task in computer vision. It is even more painstaking when the generation is conditioned on images with drastically different views. This is mainly because understanding, corresponding, and transforming appearance and semantic information across the views is not trivial. In this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cGAN). Two new architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq) are proposed to generate scenes with resolutions of 64×64 and 256×256 pixels. X-Fork architecture has a single discriminator and a single generator. The generator hallucinates both the image and its semantic segmentation in the target view. X-Seq architecture utilizes two cGANs. The first one generates the target image which is subsequently fed to the second cGAN for generating its corresponding semantic segmentation map. The feedback from the second cGAN helps the first cGAN generate sharper images. Both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. The proposed methods show that they are able to capture and maintain the true semantics of objects in source and target views better than the traditional image-to-image translation method which considers only the visual appearance of the scene. Extensive qualitative and quantitative evaluations support the effectiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views",
    "checked": true,
    "id": "f870dd4df583ce6fd6c69d4e842d491dc3898050",
    "semantic_title": "cross-view image synthesis using conditional gans",
    "citation_count": 158,
    "authors": [
      "Krishna Regmi",
      "Ali Borji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dekel_Sparse_Smart_Contours_CVPR_2018_paper.html": {
    "title": "Sparse, Smart Contours to Represent and Edit Images",
    "volume": "main",
    "abstract": "We study the problem of reconstructing an image from information stored at contour locations. We show that high-quality reconstructions with high fidelity to the source image can be obtained from sparse input, e.g., comprising less than 6% of image pixels. This is a significant improvement over existing contour-based reconstruction methods that require much denser input to capture subtle texture information and to ensure image quality. Our model, based on generative adversarial networks, synthesizes texture and details in regions where no input information is provided. The semantic knowledge encoded into our model and the sparsity of the input allows to use contours as an intuitive interface for semantically-aware image manipulation: local edits in contour domain translate to long-range and coherent changes in pixel space. We can perform complex structural changes such as changing facial expression by simple edits of contours. Our experiments demonstrate that humans as well as a face recognition system mostly cannot distinguish between our reconstructions and the source images",
    "checked": true,
    "id": "f7f80b49510ea5764bedd6f3252e0727e123c27c",
    "semantic_title": "sparse, smart contours to represent and edit images",
    "citation_count": 67,
    "authors": [
      "Tali Dekel",
      "Chuang Gan",
      "Dilip Krishnan",
      "Ce Liu",
      "William T. Freeman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Suzuki_Anticipating_Traffic_Accidents_CVPR_2018_paper.html": {
    "title": "Anticipating Traffic Accidents With Adaptive Loss and Large-Scale Incident DB",
    "volume": "main",
    "abstract": "In this paper, we propose a novel approach for traffic accident anticipation through (i) Adaptive Loss for Early Anticipation (AdaLEA) and (ii) a large-scale self-annotated incident database. The proposed AdaLEA allows us to gradually learn an earlier anticipation as training progresses. The loss function adaptively assigns penalty weights depending on how early the model can anticipate a traffic accident at each epoch. Additionally, a new Near-miss Incident DataBase (NIDB) that contains an enormous number of traffic near-miss incidents in which the four classes of cyclist, pedestrian, vehicle, and background class are labeled is discussed. The NIDB provides joint estimations of traffic incident anticipation and risk-factor categorization. In our experimental results, we found our proposal achieved the highest scores for anticipation (99.1% mean average precision (mAP) and 4.81 sec anticipation of the average time-to-collision (ATTC), values which are +6.6% better and 2.36 sec faster than previous work) and joint estimation (62.1% (mAP) and 3.65 sec anticipation (ATTC), values which are +4.3% better and 0.70 sec faster than previous work)",
    "checked": true,
    "id": "a7aa69b348701f8efb7bc04f36e93ef85ec8a017",
    "semantic_title": "anticipating traffic accidents with adaptive loss and large-scale incident db",
    "citation_count": 114,
    "authors": [
      "Tomoyuki Suzuki",
      "Hirokatsu Kataoka",
      "Yoshimitsu Aoki",
      "Yutaka Satoh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Birdal_A_Minimalist_Approach_CVPR_2018_paper.html": {
    "title": "A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds",
    "volume": "main",
    "abstract": "This paper proposes a segmentation-free, automatic and efficient procedure to detect general geometric quadric forms in point clouds, where clutter and occlusions are inevitable. Our everyday world is dominated by man-made objects which are designed using 3D primitives (such as planes, cones, spheres, cylinders, etc.). These objects are also omnipresent in industrial environments. This gives rise to the possibility of abstracting 3D scenes through primitives, thereby positions these geometric forms as an integral part of perception and high level 3D scene understanding. As opposed to state-of-the-art, where a tailored algorithm treats each primitive type separately, we propose to encapsulate all types in a single robust detection procedure. At the center of our approach lies a closed form 3D quadric fit, operating in both primal & dual spaces and requiring as low as 4 oriented-points. Around this fit, we design a novel, local null-space voting strategy to reduce the 4-point case to 3. Voting is coupled with the famous RANSAC and makes our algorithm orders of magnitude faster than its conventional counterparts. This is the first method capable of performing a generic cross-type multi-object primitive detection in difficult scenes. Results on synthetic and real datasets support the validity of our method",
    "checked": true,
    "id": "42ed1473436e137e9bf834271924109ca9c35f16",
    "semantic_title": "a minimalist approach to type-agnostic detection of quadrics in point clouds",
    "citation_count": 12,
    "authors": [
      "Tolga Birdal",
      "Benjamin Busam",
      "Nassir Navab",
      "Slobodan Ilic",
      "Peter Sturm"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Facelet-Bank_for_Fast_CVPR_2018_paper.html": {
    "title": "Facelet-Bank for Fast Portrait Manipulation",
    "volume": "main",
    "abstract": "Digital face manipulation has become a popular and fascinating way to touch images with the prevalence of smart phones and social networks. With a wide variety of user preferences, facial expressions, and accessories, a general and flexible model is necessary to accommodate different types of facial editing. In this paper, we propose a model to achieve this goal based on an end-to-end convolutional neural network that supports fast inference, edit-effect control, and quick partial-model update. In addition, this model learns from unpaired image sets with different attributes. Experimental results show that our framework can handle a wide range of expressions, accessories, and makeup effects. It produces high-resolution and high-quality results in fast speed",
    "checked": true,
    "id": "666f7b839001c0607b529cebde5b862d93aa9972",
    "semantic_title": "facelet-bank for fast portrait manipulation",
    "citation_count": 38,
    "authors": [
      "Ying-Cong Chen",
      "Huaijia Lin",
      "Michelle Shu",
      "Ruiyu Li",
      "Xin Tao",
      "Xiaoyong Shen",
      "Yangang Ye",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Visual_to_Sound_CVPR_2018_paper.html": {
    "title": "Visual to Sound: Generating Natural Sound for Videos in the Wild",
    "volume": "main",
    "abstract": "As two of the five traditional human senses (sight, hearing, taste, smell, and touch), vision and sound are basic sources through which humans understand the world. Often correlated during natural events, these two modalities combine to jointly affect human perception. In this paper, we pose the task of generating sound given visual input. Such capabilities could help enable applications in virtual reality (generating sound for virtual scenes automatically) or provide additional accessibility to images or videos for people with visual impairments. As a first step in this direction, we apply learning-based methods to generate raw waveform samples given input video frames. We evaluate our models on a dataset of videos containing a variety of sounds (such as ambient sounds and sounds from people/animals). Our experiments show that the generated sounds are fairly realistic and have good temporal synchronization with the visual inputs",
    "checked": true,
    "id": "f2d126e02401ec9f3c131eac423620529996df2f",
    "semantic_title": "visual to sound: generating natural sound for videos in the wild",
    "citation_count": 208,
    "authors": [
      "Yipin Zhou",
      "Zhaowen Wang",
      "Chen Fang",
      "Trung Bui",
      "Tamara L. Berg"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kundu_3D-RCNN_Instance-Level_3D_CVPR_2018_paper.html": {
    "title": "3D-RCNN: Instance-Level 3D Object Reconstruction via Render-and-Compare",
    "volume": "main",
    "abstract": "We present a fast inverse-graphics framework for instance-level 3D scene understanding. We train a deep convolutional network that learns to map image regions to the full 3D shape and pose of all object instances in the image. Our method produces a compact 3D representation of the scene, which can be readily used for applications like autonomous driving. Many traditional 2D vision outputs, like instance segmentations and depth-maps, can be obtained by simply rendering our output 3D scene model. We exploit class-specific shape priors by learning a low dimensional shape-space from collections of CAD models. We present novel representations of shape and pose, that strive towards better 3D equivariance and generalization. In order to exploit rich supervisory signals in the form of 2D annotations like segmentation, we propose a differentiable Render-and-Compare loss that allows 3D shape and pose to be learned with 2D supervision. We evaluate our method on the challenging real-world datasets of Pascal3D+ and KITTI, where we achieve state-of-the-art results",
    "checked": true,
    "id": "1ac302bb069b575d0ed212c407cf2cca09bd625f",
    "semantic_title": "3d-rcnn: instance-level 3d object reconstruction via render-and-compare",
    "citation_count": 329,
    "authors": [
      "Abhijit Kundu",
      "Yin Li",
      "James M. Rehg"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_Fast_and_Furious_CVPR_2018_paper.html": {
    "title": "Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net",
    "volume": "main",
    "abstract": "In this paper we propose a novel deep neural network that is able to jointly reason about 3D detection, tracking and motion forecasting given data captured by a 3D sensor. By jointly reasoning about these tasks, our holistic approach is more robust to occlusion as well as sparse data at range. Our approach performs 3D convolutions across space and time over a bird's eye view representation of the 3D world, which is very efficient in terms of both memory and computation. Our experiments on a new very large scale dataset captured in several north american cities, show that we can outperform the state-of-the-art by a large margin. Importantly, by sharing computation we can perform all tasks in as little as 30 ms",
    "checked": true,
    "id": "9bdc71eacf7440bab8d2852f229da537498c9546",
    "semantic_title": "fast and furious: real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net",
    "citation_count": 624,
    "authors": [
      "Wenjie Luo",
      "Bin Yang",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Singh_An_Analysis_of_CVPR_2018_paper.html": {
    "title": "An Analysis of Scale Invariance in Object Detection ­ SNIP",
    "volume": "main",
    "abstract": "An analysis of different techniques for recognizing and detecting objects under extreme scale variation is presented. Scale specific and scale invariant design of detectors are compared by training them with different configurations of input data. By evaluating the performance of different network architectures for classifying small objects on ImageNet, we show that CNNs are not robust to changes in scale. Based on this analysis, we propose to train and test detectors on the same scales of an image-pyramid. Since small and large objects are difficult to recognize at smaller and larger scales respectively, we present a novel training scheme called Scale Normalization for Image Pyramids (SNIP) which selectively back-propagates the gradients of object instances of different sizes as a function of the image scale. On the COCO dataset, our single model performance is 45.7% and an ensemble of 3 networks obtains an mAP of 48.3%. We use off-the-shelf ImageNet-1000 pre-trained models and only train with bounding box supervision. Our submission won the Best Student Entry in the COCO 2017 challenge. Code will be made available at url{http://bit.ly/2yXVg4c}",
    "checked": true,
    "id": "f11f609facfb650ed8e659236a04bc0a664cb665",
    "semantic_title": "an analysis of scale invariance in object detection - snip",
    "citation_count": 741,
    "authors": [
      "Bharat Singh",
      "Larry S. Davis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Relation_Networks_for_CVPR_2018_paper.html": {
    "title": "Relation Networks for Object Detection",
    "volume": "main",
    "abstract": "Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances extbf{individually}, without exploiting their relations during learning. This work proposes an object relation module. It processes a set of objects extbf{simultaneously} through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the extbf{first fully end-to-end object detector}",
    "checked": true,
    "id": "6a0aaefce8a27a8727d896fa444ba27558b2d381",
    "semantic_title": "relation networks for object detection",
    "citation_count": 1222,
    "authors": [
      "Han Hu",
      "Jiayuan Gu",
      "Zheng Zhang",
      "Jifeng Dai",
      "Yichen Wei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Zero-Shot_Sketch-Image_Hashing_CVPR_2018_paper.html": {
    "title": "Zero-Shot Sketch-Image Hashing",
    "volume": "main",
    "abstract": "Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods, where Hamming distance matching significantly speeds up the process of similarity search. Providing training and test data subjected to a fixed set of pre-defined categories, the cutting-edge SBIR and cross-modal hashing works obtain acceptable retrieval performance. However, most of the existing methods fail when the categories of query sketches have never been seen during training. In this paper, the above problem is briefed as a novel but realistic zero-shot SBIR hashing task. We elaborate the challenges of this special task and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An end-to-end three-network architecture is built, two of which are treated as the binary encoders. The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution, respectively. As an important part of ZSIH, we formulate a generative hashing scheme in reconstructing semantic knowledge representations for zero-shot retrieval. To the best of our knowledge, ZSIH is the first zero-shot hashing work suitable for SBIR and cross-modal search. Comprehensive experiments are conducted on two extended datasets, i.e., Sketchy and TU-Berlin with a novel zero-shot train-test split. The proposed model remarkably outperforms related works",
    "checked": true,
    "id": "1af4fb9ab061645cf0fb09d20b7cc7d834fd01ab",
    "semantic_title": "zero-shot sketch-image hashing",
    "citation_count": 152,
    "authors": [
      "Yuming Shen",
      "Li Liu",
      "Fumin Shen",
      "Ling Shao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gurari_VizWiz_Grand_Challenge_CVPR_2018_paper.html": {
    "title": "VizWiz Grand Challenge: Answering Visual Questions From Blind People",
    "volume": "main",
    "abstract": "The study of algorithms to automatically answer visual questions currently is motivated by visual question answering (VQA) datasets constructed in artificial VQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising from a natural VQA setting. VizWiz consists of 31,000 visual questions originating from blind people who each took a picture using a mobile phone and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. VizWiz differs from the many existing VQA datasets because (1) images are captured by blind photographers and so are often poor quality, (2) questions are spoken and so are more conversational, and (3) often visual questions cannot be answered. Evaluation of modern algorithms for answering visual questions and deciding if a visual question is answerable reveals that VizWiz is a challenging dataset. We introduce this dataset to encourage a larger community to develop more generalized algorithms that can assist blind people",
    "checked": true,
    "id": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c",
    "semantic_title": "vizwiz grand challenge: answering visual questions from blind people",
    "citation_count": 847,
    "authors": [
      "Danna Gurari",
      "Qing Li",
      "Abigale J. Stangl",
      "Anhong Guo",
      "Chi Lin",
      "Kristen Grauman",
      "Jiebo Luo",
      "Jeffrey P. Bigham"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sam_Divide_and_Grow_CVPR_2018_paper.html": {
    "title": "Divide and Grow: Capturing Huge Diversity in Crowd Images With Incrementally Growing CNN",
    "volume": "main",
    "abstract": "Automated counting of people in crowd images is a challenging task. The major difficulty stems from the large diversity in the way people appear in crowds. In fact, features available for crowd discrimination largely depend on the crowd density to the extent that people are only seen as blobs in a highly dense scene. We tackle this problem with a growing CNN which can progressively increase its capacity to account for the wide variability seen in crowd scenes. Our model starts from a base CNN density regressor, which is trained in equivalence on all types of crowd images. In order to adapt with the huge diversity, we create two child regressors which are exact copies of the base CNN. A differential training procedure divides the dataset into two clusters and fine-tunes the child networks on their respective specialties. Consequently, without any hand-crafted criteria for forming specialties, the child regressors become experts on certain types of crowds. The child networks are again split recursively, creating two experts at every division. This hierarchical training leads to a CNN tree, where the child regressors are more fine experts than any of their parents. The leaf nodes are taken as the final experts and a classifier network is then trained to predict the correct specialty for a given test image patch. The proposed model achieves higher count accuracy on major crowd datasets. Further, we analyse the characteristics of specialties mined automatically by our method",
    "checked": true,
    "id": "b006343184b25cdb3773b5f824db72d7fe279ff1",
    "semantic_title": "divide and grow: capturing huge diversity in crowd images with incrementally growing cnn",
    "citation_count": 219,
    "authors": [
      "Deepak Babu Sam",
      "Neeraj N. Sajjan",
      "R. Venkatesh Babu",
      "Mukundhan Srinivasan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Choi_Structured_Set_Matching_CVPR_2018_paper.html": {
    "title": "Structured Set Matching Networks for One-Shot Part Labeling",
    "volume": "main",
    "abstract": "Diagrams often depict complex phenomena and serve as a good test bed for visual and textual reasoning. However, understanding diagrams using natural image understanding approaches requires large training datasets of diagrams, which are very hard to obtain. Instead, this can be addressed as a matching problem either between labeled diagrams, images or both. This problem is very challenging since the absence of significant color and texture renders local cues ambiguous and requires global reasoning. We consider the problem of one-shot part labeling: labeling multiple parts of an object in a target image given only a single source image of that category. For this set-to-set matching problem, we introduce the Structured Set Matching Network (SSMN), a structured prediction model that incorporates convolutional neural networks. The SSMN is trained using global normalization to maximize local match scores between corresponding elements and a global consistency score among all matched elements, while also enforcing a matching constraint between the two sets. The SSMN significantly outperforms several strong baselines on three label transfer scenarios: diagram-to-diagram, evaluated on a new diagram dataset of over 200 categories; image-to-image, evaluated on a dataset built on top of the Pascal Part Dataset; and image-to-diagram, evaluated on transferring labels across these datasets",
    "checked": true,
    "id": "e0111b5c8d4b5770f805a24a3ecb77bd90205449",
    "semantic_title": "structured set matching networks for one-shot part labeling",
    "citation_count": 23,
    "authors": [
      "Jonghyun Choi",
      "Jayant Krishnamurthy",
      "Aniruddha Kembhavi",
      "Ali Farhadi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Novotny_Self-Supervised_Learning_of_CVPR_2018_paper.html": {
    "title": "Self-Supervised Learning of Geometrically Stable Features Through Probabilistic Introspection",
    "volume": "main",
    "abstract": "Self-supervision can dramatically cut back the amount of manually-labelled data required to train deep neural networks. While self-supervision has usually been considered for tasks such as image classification, in this paper we aim at extending it to geometry-oriented tasks such as semantic matching and part detection. We do so by building on several recent ideas in unsupervised landmark detection. Our approach learns dense distinctive visual descriptors from an unlabeled dataset of images using synthetic image transformations. It does so by means of a robust probabilistic formulation that can introspectively determine which image regions are likely to result in stable image matching. We show empirically that a network pre-trained in this manner requires significantly less supervision to learn semantic object parts compared to numerous pre-training alternatives. We also show that the pre-trained representation is excellent for semantic object matching",
    "checked": true,
    "id": "7d94103c5e3796721388200ddaf94ed6e984ab0f",
    "semantic_title": "self-supervised learning of geometrically stable features through probabilistic introspection",
    "citation_count": 65,
    "authors": [
      "David Novotny",
      "Samuel Albanie",
      "Diane Larlus",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Douze_Link_and_Code_CVPR_2018_paper.html": {
    "title": "Link and Code: Fast Indexing With Graphs and Compact Regression Codes",
    "volume": "main",
    "abstract": "Similarity search approaches based on graph walks have recently attained outstanding speed-accuracy trade-offs, taking aside the memory requirements. In this paper, we revisit these approaches by considering, additionally, the memory constraint required to index billions of images on a single server. This leads us to propose a method based both on graph traversal and compact representations. We encode the indexed vectors using quantization and exploit the graph structure to refine the similarity estimation. In essence, our method takes the best of these two worlds: the search strategy is based on nested graphs, thereby providing high precision with a relatively small set of comparisons. At the same time it offers a significant memory compression. As a result, our approach outperforms the state of the art on operating points considering 64--128 bytes per vector, as demonstrated by our results on two billion-scale public benchmarks",
    "checked": true,
    "id": "ca18146b3b3089084be811b74791800c56ededc5",
    "semantic_title": "link and code: fast indexing with graphs and compact regression codes",
    "citation_count": 46,
    "authors": [
      "Matthijs Douze",
      "Alexandre Sablayrolles",
      "Hervé Jégou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Textbook_Question_Answering_CVPR_2018_paper.html": {
    "title": "Textbook Question Answering Under Instructor Guidance With Memory Networks",
    "volume": "main",
    "abstract": "Textbook Question Answering (TQA) is a task to choose the most proper answers by reading a multi-modal context of abundant essays and images. TQA serves as a favorable test bed for visual and textual reasoning. However, most of the current methods are incapable of reasoning over the long contexts and images. To address this issue, we propose a novel approach of Instructor Guidance with Memory Networks (IGMN) which conducts the TQA task by finding contradictions between the candidate answers and their corresponding context. We build the Contradiction Entity-Relationship Graph (CERG) to extend the passage-level multi-modal contradictions to an essay level. The machine thus performs as an instructor to extract the essay-level contradictions as the Guidance. Afterwards, we exploit the memory networks to capture the information in the Guidance, and use the attention mechanisms to jointly reason over the global features of the multi-modal input. Extensive experiments demonstrate that our method outperforms the state-of-the-arts on the TQA dataset. The source code is available at https://github.com/freerailway/igmn",
    "checked": true,
    "id": "f783d93b983841804a9633a37dfbc624bf5d9bfb",
    "semantic_title": "textbook question answering under instructor guidance with memory networks",
    "citation_count": 22,
    "authors": [
      "Juzheng Li",
      "Hang Su",
      "Jun Zhu",
      "Siyu Wang",
      "Bo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dizaji_Unsupervised_Deep_Generative_CVPR_2018_paper.html": {
    "title": "Unsupervised Deep Generative Adversarial Hashing Network",
    "volume": "main",
    "abstract": "Unsupervised deep hash functions have not shown satisfactory improvements against the shallow alternatives, and usually, require supervised pretraining to avoid getting stuck in bad local minima. In this paper, we propose a deep unsupervised hashing function, called HashGAN, which outperforms unsupervised hashing models with significant margins without any supervised pretraining. HashGAN consists of three networks, a generator, a discriminator and an encoder. By sharing the parameters of the encoder and discriminator, we benefit from the adversarial loss as a data dependent regularization in training our deep hash function. Moreover, a novel loss function is introduced for hashing real images, resulting in minimum entropy, uniform frequency, consistent and independent hash bits. Furthermore, we train the generator conditioning on random binary inputs and also use these binary variables in a triplet ranking loss for improving hash codes. In our experiments, HashGAN outperforms the previous unsupervised hash functions in image retrieval and achieves the state-of-the-art performance in image clustering. We also provide an ablation study, showing the contribution of each component in our loss function",
    "checked": true,
    "id": "7410b5c50db3ab3a17ffd9ea2f78ca1fedd52267",
    "semantic_title": "unsupervised deep generative adversarial hashing network",
    "citation_count": 108,
    "authors": [
      "Kamran Ghasedi Dizaji",
      "Feng Zheng",
      "Najmeh Sadoughi",
      "Yanhua Yang",
      "Cheng Deng",
      "Heng Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html": {
    "title": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments",
    "volume": "main",
    "abstract": "A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset",
    "checked": true,
    "id": "c37c23b12e00168833eccff8025a830ce27c5abc",
    "semantic_title": "vision-and-language navigation: interpreting visually-grounded navigation instructions in real environments",
    "citation_count": 1306,
    "authors": [
      "Peter Anderson",
      "Qi Wu",
      "Damien Teney",
      "Jake Bruce",
      "Mark Johnson",
      "Niko Sünderhauf",
      "Ian Reid",
      "Stephen Gould",
      "Anton van den Hengel"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.html": {
    "title": "DenseASPP for Semantic Segmentation in Street Scenes",
    "volume": "main",
    "abstract": "Semantic image segmentation is a basic street scene understanding task in autonomous driving, where each pixel in a high resolution image is categorized into a set of semantic labels. Unlike other scenarios, objects in autonomous driving scene exhibit very large scale changes, which poses great challenges for high-level feature representation in a sense that multi-scale information must be correctly encoded. To remedy this problem, atrous convolutioncite{Deeplabv1} was introduced to generate features with larger receptive fields without sacrificing spatial resolution. Built upon atrous convolution, Atrous Spatial Pyramid Pooling (ASPP)cite{Deeplabv2} was proposed to concatenate multiple atrous-convolved features using different dilation rates into a final feature representation. Although ASPP is able to generate multi-scale features, we argue the feature resolution in the scale-axis is not dense enough for the autonomous driving scenario. To this end, we propose Densely connected Atrous Spatial Pyramid Pooling (DenseASPP), which connects a set of atrous convolutional layers in a dense way, such that it generates multi-scale features that not only cover a larger scale range, but also cover that scale range densely, without significantly increasing the model size. We evaluate DenseASPP on the street scene benchmark Cityscapescite{Cityscapes} and achieve state-of-the-art performance",
    "checked": true,
    "id": "1847249502bcc85d56870a72f9a4b1722e477046",
    "semantic_title": "denseaspp for semantic segmentation in street scenes",
    "citation_count": 1293,
    "authors": [
      "Maoke Yang",
      "Kun Yu",
      "Chi Zhang",
      "Zhiwei Li",
      "Kuiyuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mohapatra_Efficient_Optimization_for_CVPR_2018_paper.html": {
    "title": "Efficient Optimization for Rank-Based Loss Functions",
    "volume": "main",
    "abstract": "The accuracy of information retrieval systems is often measured using complex loss functions such as the average precision (AP) or the normalized discounted cumulative gain (NDCG). Given a set of positive and negative samples, the parameters of a retrieval system can be estimated by minimizing these loss functions. However, the non-differentiability and non-decomposability of these loss functions does not allow for simple gradient based optimization algorithms. This issue is generally circumvented by either optimizing a structured hinge-loss upper bound to the loss function or by using asymptotic methods like the direct-loss minimization framework. Yet, the high computational complexity of loss-augmented inference, which is necessary for both the frameworks, prohibits its use in large training data sets. To alleviate this deficiency, we present a novel quicksort flavored algorithm for a large class of non-decomposable loss functions. We provide a complete characterization of the loss functions that are amenable to our algorithm, and show that it includes both AP and NDCG based loss functions. Furthermore, we prove that no comparison based algorithm can improve upon the computational complexity of our approach asymptotically. We demonstrate the effectiveness of our approach in the context of optimizing the structured hinge loss upper bound of AP and NDCG loss for learning models for a variety of vision tasks. We show that our approach provides significantly better results than simpler decomposable loss functions, while requiring a comparable training time",
    "checked": true,
    "id": "cd06e0964181d1a537c7ca07f5327aa85804a610",
    "semantic_title": "efficient optimization for rank-based loss functions",
    "citation_count": 38,
    "authors": [
      "Pritish Mohapatra",
      "Michal Rolínek",
      "C.V. Jawahar",
      "Vladimir Kolmogorov",
      "M. Pawan Kumar"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Wasserstein_Introspective_Neural_CVPR_2018_paper.html": {
    "title": "Wasserstein Introspective Neural Networks",
    "volume": "main",
    "abstract": "We present Wasserstein introspective neural networks (WINN) that are both a generator and a discriminator within a single model. WINN provides a significant improvement over the recent introspective neural networks (INN) method by enhancing INN's generative modeling capability. WINN has three interesting properties: (1) A mathematical connection between the formulation of the INN algorithm and that of Wasserstein generative adversarial networks (WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN results in a large enhancement to INN, achieving compelling results even with a single classifier --- e.g., providing nearly a 20 times reduction in model size over INN for unsupervised generative modeling. (3) When applied to supervised classification, WINN also gives rise to improved robustness against adversarial examples in terms of the error reduction. In the experiments, we report encouraging results on unsupervised learning problems including texture, face, and object modeling, as well as a supervised classification task against adversarial attacks",
    "checked": true,
    "id": "21ae4ed57d395aba2158a4f86e93935be66dea20",
    "semantic_title": "wasserstein introspective neural networks",
    "citation_count": 57,
    "authors": [
      "Kwonjoon Lee",
      "Weijian Xu",
      "Fan Fan",
      "Zhuowen Tu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zamir_Taskonomy_Disentangling_Task_CVPR_2018_paper.html": {
    "title": "Taskonomy: Disentangling Task Transfer Learning",
    "volume": "main",
    "abstract": "Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable uses; it is the concept underlying transfer learning and, for example, can provide a principled way for reusing supervision among related tasks, finding what tasks transfer well to an arbitrary target task, or solving many tasks in one system without piling up the complexity. This paper proposes a fully computational approach for finding the structure of the space of visual tasks. This is done via a sampled dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks, and modeling their (1st and higher order) transfer dependencies in a latent space. The product can be viewed as a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. the nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 while keeping the performance nearly the same. Users can employ a provided Binary Integer Programming solver that leverages the taxonomy to find efficient supervision policies for their own use cases",
    "checked": true,
    "id": "2fe2cfd98e232f1396f01881853ed6b3d5e37d65",
    "semantic_title": "taskonomy: disentangling task transfer learning",
    "citation_count": 1217,
    "authors": [
      "Amir R. Zamir",
      "Alexander Sax",
      "William Shen",
      "Leonidas J. Guibas",
      "Jitendra Malik",
      "Silvio Savarese"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Saito_Maximum_Classifier_Discrepancy_CVPR_2018_paper.html": {
    "title": "Maximum Classifier Discrepancy for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics. To solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy. Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at url{https://github.com/mil-tokyo/MCD_DA}",
    "checked": true,
    "id": "0d725e4fea8bbaf332d6a8d424ebecbd547a3851",
    "semantic_title": "maximum classifier discrepancy for unsupervised domain adaptation",
    "citation_count": 1788,
    "authors": [
      "Kuniaki Saito",
      "Kohei Watanabe",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html": {
    "title": "Unsupervised Feature Learning via Non-Parametric Instance Discrimination",
    "volume": "main",
    "abstract": "Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsu- pervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time",
    "checked": true,
    "id": "155b7782dbd713982a4133df3aee7adfd0b6b304",
    "semantic_title": "unsupervised feature learning via non-parametric instance discrimination",
    "citation_count": 3452,
    "authors": [
      "Zhirong Wu",
      "Yuanjun Xiong",
      "Stella X. Yu",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Multi-Task_Adversarial_Network_CVPR_2018_paper.html": {
    "title": "Multi-Task Adversarial Network for Disentangled Feature Learning",
    "volume": "main",
    "abstract": "We address the problem of image feature learning for the applications where multiple factors exist in the image generation process and only some factors are of our interest. We present a novel multi-task adversarial network based on an encoder-discriminator-generator architecture. The encoder extracts a disentangled feature representation for the factors of interest. The discriminators classify each of the factors as individual tasks. The encoder and the discriminators are trained cooperatively on factors of interest, but in an adversarial way on factors of distraction. The generator provides further regularization on the learned feature by reconstructing images with shared factors as the input image. We design a new optimization scheme to stabilize the adversarial optimization process when multiple distributions need to be aligned. The experiments on face recognition and font recognition tasks show that our method outperforms the state-of-the-art methods in terms of both recognizing the factors of interest and generalization to images with unseen variations",
    "checked": true,
    "id": "2025392b7ebe267fbe13cb9fcd022ded8f7d1972",
    "semantic_title": "multi-task adversarial network for disentangled feature learning",
    "citation_count": 66,
    "authors": [
      "Yang Liu",
      "Zhaowen Wang",
      "Hailin Jin",
      "Ian Wassell"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper.html": {
    "title": "Learning From Synthetic Data: Addressing Domain Shift for Semantic Segmentation",
    "volume": "main",
    "abstract": "Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions",
    "checked": true,
    "id": "dfd72b994765a1979c6872fc8948657885a31752",
    "semantic_title": "learning from synthetic data: addressing domain shift for semantic segmentation",
    "citation_count": 490,
    "authors": [
      "Swami Sankaranarayanan",
      "Yogesh Balaji",
      "Arpit Jain",
      "Ser Nam Lim",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fawzi_Empirical_Study_of_CVPR_2018_paper.html": {
    "title": "Empirical Study of the Topology and Geometry of Deep Networks",
    "volume": "main",
    "abstract": "The goal of this paper is to analyze the geometric properties of deep neural network image classifiers in the input space. We specifically study the topology of classification regions created by deep networks, as well as their associated decision boundary. Through a systematic empirical study, we show that state-of-the-art deep nets learn connected classification regions, and that the decision boundary in the vicinity of datapoints is flat along most directions. We further draw an essential connection between two seemingly unrelated properties of deep networks: their sensitivity to additive perturbations of the inputs, and the curvature of their decision boundary. The directions where the decision boundary is curved in fact characterize the directions to which the classifier is the most vulnerable. We finally leverage a fundamental asymmetry in the curvature of the decision boundary of deep nets, and propose a method to discriminate between original images, and images perturbed with small adversarial examples. We show the effectiveness of this purely geometric approach for detecting small adversarial perturbations in images, and for recovering the labels of perturbed images",
    "checked": true,
    "id": "e5b4a134836f376fc368fb8cdb194c8ca2a8828e",
    "semantic_title": "empirical study of the topology and geometry of deep networks",
    "citation_count": 137,
    "authors": [
      "Alhussein Fawzi",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Pascal Frossard",
      "Stefano Soatto"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mancini_Boosting_Domain_Adaptation_CVPR_2018_paper.html": {
    "title": "Boosting Domain Adaptation by Discovering Latent Domains",
    "volume": "main",
    "abstract": "Current Domain Adaptation (DA) methods based on deep architectures assume that the source samples arise from a single distribution. However, in practice most datasets can be regarded as mixtures of multiple domains. In these cases exploiting single-source DA methods for learning target classifiers may lead to sub-optimal, if not poor, results. In addition, in many applications it is difficult to manually provide the domain labels for all source data points, i.e. latent domains should be automatically discovered. This paper introduces a novel Convolutional Neural Network (CNN) architecture which (i) automatically discovers latent domains in visual datasets and (ii) exploits this information to learn robust target classifiers. Our approach is based on the introduction of two main components, which can be embedded into any existing CNN architecture: (i) a side branch that automatically computes the assignment of a source sample to a latent domain and (ii) novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution. We test our approach on publicly-available datasets, showing that it outperforms state-of-the-art multi-source DA methods by a large margin",
    "checked": true,
    "id": "57ef32ae92193a4f1d2fd2aee729fa71052d6bc2",
    "semantic_title": "boosting domain adaptation by discovering latent domains",
    "citation_count": 155,
    "authors": [
      "Massimiliano Mancini",
      "Lorenzo Porzi",
      "Samuel Rota Bulò",
      "Barbara Caputo",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Shape_From_Shading_CVPR_2018_paper.html": {
    "title": "Shape From Shading Through Shape Evolution",
    "volume": "main",
    "abstract": "In this paper, we address the shape-from-shading problem by training deep networks with synthetic images. Unlike conventional approaches that combine deep learning and synthetic imagery, we propose an approach that does not need any external shape dataset to render synthetic images. Our approach consists of two synergistic processes: the evolution of complex shapes from simple primitives, and the training of a deep network for shape-from-shading. The evolution generates better shapes guided by the network training, while the training improves by using the evolved shapes. We show that our approach achieves state-of-the-art performance on a shape-from-shading benchmark",
    "checked": true,
    "id": "d74a576cc311841c3ff8070262e928c090e41f59",
    "semantic_title": "shape from shading through shape evolution",
    "citation_count": 28,
    "authors": [
      "Dawei Yang",
      "Jia Deng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Weakly_Supervised_Instance_CVPR_2018_paper.html": {
    "title": "Weakly Supervised Instance Segmentation Using Class Peak Response",
    "volume": "main",
    "abstract": "Weakly supervised instance segmentation with image-level labels, instead of expensive pixel-level masks, remains unexplored. In this paper, we tackle this challenging problem by exploiting class peak responses to enable a classification network for instance mask extraction. With image labels supervision only, CNN classifiers in a fully convolutional manner can produce class response maps, which specify classification confidence at each image location. We observed that local maximums, i.e., peaks, in a class response map typically correspond to strong visual cues residing inside each instance. Motivated by this, we first design a process to stimulate peaks to emerge from a class response map. The emerged peaks are then back-propagated and effectively mapped to highly informative regions of each object instance, such as instance boundaries. We refer to the above maps generated from class peak responses as Peak Response Maps (PRMs). PRMs provide a fine-detailed instance-level representation, which allows instance masks to be extracted even with some off-the-shelf methods. To the best of our knowledge, we for the first time report results for the challenging image-level supervised instance segmentation task. Extensive experiments show that our method also boosts weakly supervised pointwise localization as well as semantic segmentation performance, and reports state-of-the-art results on popular benchmarks, including PASCAL VOC 2012 and MS COCO",
    "checked": true,
    "id": "98a702211e52622a50691972e4aec51f996edcd5",
    "semantic_title": "weakly supervised instance segmentation using class peak response",
    "citation_count": 275,
    "authors": [
      "Yanzhao Zhou",
      "Yi Zhu",
      "Qixiang Ye",
      "Qiang Qiu",
      "Jianbin Jiao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Collaborative_and_Adversarial_CVPR_2018_paper.html": {
    "title": "Collaborative and Adversarial Network for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "In this paper, we propose a new unsupervised domain adaptation approach called Collaborative and Adversarial Network (CAN) through domain-collaborative and domain-adversarial training of neural networks. We use several domain classifiers on multiple CNN feature extraction layers/blocks, in which each domain classifier is connected to the hidden representations from one block and one loss function is defined based on the hidden presentation and the domain labels (e.g., source and target). We design a new loss function by integrating the losses from all blocks in order to learn informative representations from lower layers through collaborative learning and learn uninformative representations from higher layers through adversarial learning. We further extend our CAN method as Incremental CAN (iCAN), in which we iteratively select a set of pseudo-labelled target samples based on the image classifier and the last domain classifier from the previous training epoch and re-train our CAN model using the enlarged training set. Comprehensive experiments on two benchmark datasets Office and ImageCLEF-DA clearly demonstrate the effectiveness of our newly proposed approaches CAN and iCAN for unsupervised domain adaptation",
    "checked": true,
    "id": "b2b0e40f2bc62a6e87430e94ab6af9ddc67a5c72",
    "semantic_title": "collaborative and adversarial network for unsupervised domain adaptation",
    "citation_count": 448,
    "authors": [
      "Weichen Zhang",
      "Wanli Ouyang",
      "Wen Li",
      "Dong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xie_Environment_Upgrade_Reinforcement_CVPR_2018_paper.html": {
    "title": "Environment Upgrade Reinforcement Learning for Non-Differentiable Multi-Stage Pipelines",
    "volume": "main",
    "abstract": "Recent advances in multi-stage algorithms have shown great promise, but two important problems still remain. First of all, at inference time, information can't feed back from downstream to upstream. Second, at training time, end-to-end training is not possible if the overall pipeline involves non-differentiable functions, and so different stages can't be jointly optimized. In this paper, we propose a novel environment upgrade reinforcement learning framework to solve the feedback and joint optimization problems. Our framework re-links the downstream stage to the upstream stage by a reinforcement learning agent. While training the agent to improve final performance by refining the upstream stage's output, we also upgrade the downstream stage (environment) according to the agent's policy. In this way, agent policy and environment are jointly optimized. We propose a training algorithm for this framework to address the different training demands of agent and environment. Experiments on instance segmentation and human pose estimation demonstrate the effectiveness of the proposed framework",
    "checked": true,
    "id": "b0e9147416ebe901a46900f584820e843b10c655",
    "semantic_title": "environment upgrade reinforcement learning for non-differentiable multi-stage pipelines",
    "citation_count": 8,
    "authors": [
      "Shuqin Xie",
      "Zitian Chen",
      "Chao Xu",
      "Cewu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Aodha_Teaching_Categories_to_CVPR_2018_paper.html": {
    "title": "Teaching Categories to Human Learners With Visual Explanations",
    "volume": "main",
    "abstract": "We study the problem of computer-assisted teaching with explanations. Conventional approaches for machine teaching typically only provide feedback at the instance level e.g., the category or label of the instance. However, it is intuitive that clear explanations from a knowledgeable teacher can significantly improve a student's ability to learn a new concept. To address these existing limitations, we propose a teaching framework that provides interpretable explanations as feedback and models how the learner incorporates this additional information. In the case of images, we show that we can automatically generate explanations that highlight the parts of the image that are responsible for the class label. Experiments on human learners illustrate that, on average, participants achieve better test set performance on challenging categorization tasks when taught with our interpretable approach compared to existing methods",
    "checked": true,
    "id": "0b97804d35abd66bea720d8f4e047fc9f1d0fdb5",
    "semantic_title": "teaching categories to human learners with visual explanations",
    "citation_count": 70,
    "authors": [
      "Oisin Mac Aodha",
      "Shihan Su",
      "Yuxin Chen",
      "Pietro Perona",
      "Yisong Yue"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lawin_Density_Adaptive_Point_CVPR_2018_paper.html": {
    "title": "Density Adaptive Point Set Registration",
    "volume": "main",
    "abstract": "Probabilistic methods for point set registration have demonstrated competitive results in recent years. These techniques estimate a probability distribution model of the point clouds. While such a representation has shown promise, it is highly sensitive to variations in the density of 3D points. This fundamental problem is primarily caused by changes in the sensor location across point sets. We revisit the foundations of the probabilistic registration paradigm. Contrary to previous works, we model the underlying structure of the scene as a latent probability distribution, and thereby induce invariance to point set density changes. Both the probabilistic model of the scene and the registration parameters are inferred by minimizing the Kullback-Leibler divergence in an Expectation Maximization based framework. Our density-adaptive registration successfully handles severe density variations commonly encountered in terrestrial Lidar applications. We perform extensive experiments on several challenging real-world Lidar datasets. The results demonstrate that our approach outperforms state-of-the-art probabilistic methods for multi-view registration, without the need of re-sampling",
    "checked": true,
    "id": "bbd3084472ce49effc24770a3b7a856f7ec21246",
    "semantic_title": "density adaptive point set registration",
    "citation_count": 49,
    "authors": [
      "Felix Järemo Lawin",
      "Martin Danelljan",
      "Fahad Shahbaz Khan",
      "Per-Erik Forssén",
      "Michael Felsberg"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jie_Left-Right_Comparative_Recurrent_CVPR_2018_paper.html": {
    "title": "Left-Right Comparative Recurrent Model for Stereo Matching",
    "volume": "main",
    "abstract": "Leveraging the disparity information from both left and right views is crucial for stereo disparity estimation. Left-right consistency check is an effective way to enhance the disparity estimation by referring to the information from the opposite view. However, the conventional left-right consistency check is an isolated post-processing step and heavily hand-crafted. This paper proposes a novel left-right comparative recurrent model to perform left-right consistency checking jointly with disparity estimation. At each recurrent step, the model produces disparity results for both views, and then performs online left-right comparison to identify the mismatched regions which may probably contain erroneously labeled pixels. A soft attention mechanism is introduced, which employs the learned error maps for better guiding the model to selectively focus on refining the unreliable regions at the next recurrent step. In this way, the generated disparity maps are progressively improved by the proposed recurrent model. Extensive evaluations on KITTI 2015, Scene Flow and Middlebury benchmarks validate the effectiveness of our model, demonstrating that state-of-the-art stereo disparity estimation results can be achieved by this new model",
    "checked": true,
    "id": "e9b62d0b7a99d7a2ff0802d28720e4357b84dd52",
    "semantic_title": "left-right comparative recurrent model for stereo matching",
    "citation_count": 88,
    "authors": [
      "Zequn Jie",
      "Pengfei Wang",
      "Yonggen Ling",
      "Bo Zhao",
      "Yunchao Wei",
      "Jiashi Feng",
      "Wei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Im2Pano3D_Extrapolating_360deg_CVPR_2018_paper.html": {
    "title": "Im2Pano3D: Extrapolating 360° Structure and Semantics Beyond the Field of View",
    "volume": "main",
    "abstract": "We present Im2Pano3D, a convolutional neural network that generates a dense prediction of 3D structure and a probability distribution of semantic labels for a full 360 panoramic view of an indoor scene when given only a partial observation ( <=50%) in the form of an RGB-D image. To make this possible, Im2Pano3D leverages strong contextual priors learned from large-scale synthetic and real-world indoor scenes. To ease the prediction of 3D structure, we propose to parameterize 3D surfaces with their plane equations and train the model to predict these parameters directly. To provide meaningful training supervision, we make use of multiple loss functions that consider both pixel level accuracy and global context consistency. Experiments demonstrate that Im2Pano3D is able to predict the semantics and 3D structure of the unobserved scene with more than 56% pixel accuracy and less than 0.52m average distance error, which is significantly better than alternative approaches",
    "checked": true,
    "id": "bf01616fc323bba6b670494a40014aa7e88b1c2b",
    "semantic_title": "im2pano3d: extrapolating 360° structure and semantics beyond the field of view",
    "citation_count": 76,
    "authors": [
      "Shuran Song",
      "Andy Zeng",
      "Angel X. Chang",
      "Manolis Savva",
      "Silvio Savarese",
      "Thomas Funkhouser"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Polarimetric_Dense_Monocular_CVPR_2018_paper.html": {
    "title": "Polarimetric Dense Monocular SLAM",
    "volume": "main",
    "abstract": "This paper presents a novel polarimetric dense monocular SLAM (PDMS) algorithm based on a polarization camera. The algorithm exploits both photometric and polarimetric light information to produce more accurate and complete geometry. The polarimetric information allows us to recover the azimuth angle of surface normals from each video frame to facilitate dense reconstruction, especially at textureless or specular regions. There are two challenges in our approach: 1) surface azimuth angles from the polarization camera are very noisy; and 2) we need a near real-time solution for SLAM. Previous successful methods on polarimetric multi-view stereo are offline and require manually pre-segmented object masks to suppress the effects of erroneous angle information along boundaries. Our fully automatic approach efficiently iterates azimuth-based depth propagations, two-view depth consistency check, and depth optimization to produce a depthmap in real-time, where all the algorithmic steps are carefully designed to enable a GPU implementation. To our knowledge, this paper is the first to propose a photometric and polarimetric method for dense SLAM. We have qualitatively and quantitatively evaluated our algorithm against a few of competing methods, demonstrating the superior performance on various indoor and outdoor scenes",
    "checked": true,
    "id": "37cb7893e41624a5731e5b3c9f6dcaf07ef5e2b5",
    "semantic_title": "polarimetric dense monocular slam",
    "citation_count": 48,
    "authors": [
      "Luwei Yang",
      "Feitong Tan",
      "Ao Li",
      "Zhaopeng Cui",
      "Yasutaka Furukawa",
      "Ping Tan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gallego_A_Unifying_Contrast_CVPR_2018_paper.html": {
    "title": "A Unifying Contrast Maximization Framework for Event Cameras, With Applications to Motion, Depth, and Optical Flow Estimation",
    "volume": "main",
    "abstract": "We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras",
    "checked": true,
    "id": "f1d332de6c042836f036906f60d28f186df59c02",
    "semantic_title": "a unifying contrast maximization framework for event cameras, with applications to motion, depth, and optical flow estimation",
    "citation_count": 331,
    "authors": [
      "Guillermo Gallego",
      "Henri Rebecq",
      "Davide Scaramuzza"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bagautdinov_Modeling_Facial_Geometry_CVPR_2018_paper.html": {
    "title": "Modeling Facial Geometry Using Compositional VAEs",
    "volume": "main",
    "abstract": "We propose a method for learning non-linear face geometry representations using deep generative models. Our model is a variational autoencoder with multiple levels of hidden variables where lower layers capture global geometry and higher ones encode more local deformations. Based on that, we propose a new parameterization of facial geometry that naturally decomposes the structure of the human face into a set of semantically meaningful levels of detail. This parameterization enables us to do model fitting while capturing varying level of detail under different types of geometrical constraints",
    "checked": true,
    "id": "48e77ff21d8fccccd9709b02f2de94b331c503d6",
    "semantic_title": "modeling facial geometry using compositional vaes",
    "citation_count": 111,
    "authors": [
      "Timur Bagautdinov",
      "Chenglei Wu",
      "Jason Saragih",
      "Pascal Fua",
      "Yaser Sheikh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.html": {
    "title": "Tangent Convolutions for Dense Prediction in 3D",
    "volume": "main",
    "abstract": "We present an approach to semantic scene analysis using deep convolutional networks. Our approach is based on tangent convolutions - a new construction for convolutional networks on 3D data. In contrast to volumetric approaches, our method operates directly on surface geometry. Crucially, the construction is applicable to unstructured point clouds and other noisy real-world data. We show that tangent convolutions can be evaluated efficiently on large-scale point clouds with millions of points. Using tangent convolutions, we design a deep fully-convolutional network for semantic segmentation of 3D point clouds, and apply it to challenging real-world datasets of indoor and outdoor 3D environments. Experimental results show that the presented approach outperforms other recent deep network constructions in detailed analysis of large 3D scenes",
    "checked": true,
    "id": "b256e7155dd1a27ed9233aa4b47fd6334f5f243b",
    "semantic_title": "tangent convolutions for dense prediction in 3d",
    "citation_count": 544,
    "authors": [
      "Maxim Tatarchenko",
      "Jaesik Park",
      "Vladlen Koltun",
      "Qian-Yi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Paschalidou_RayNet_Learning_Volumetric_CVPR_2018_paper.html": {
    "title": "RayNet: Learning Volumetric 3D Reconstruction With Ray Potentials",
    "volume": "main",
    "abstract": "In this paper, we consider the problem of reconstructing a dense 3D model using images captured from different views. Recent methods based on convolutional neural networks (CNN) allow learning the entire task from data. However, they do not incorporate the physics of image formation such as perspective geometry and occlusion. Instead, classical approaches based on Markov Random Fields (MRF) with ray-potentials explicitly model these physical processes, but they cannot cope with large surface appearance variations across different viewpoints. In this paper, we propose RayNet, which combines the strengths of both frameworks. RayNet integrates a CNN that learns view-invariant feature representations with an MRF that explicitly encodes the physics of perspective projection and occlusion. We train RayNet end-to-end using empirical risk minimization. We thoroughly evaluate our approach on challenging real-world datasets and demonstrate its benefits over a piece-wise trained baseline, hand-crafted models as well as other learning-based approaches",
    "checked": true,
    "id": "1dcdfc4319297592f31df2291b3d6fac3b30bb5f",
    "semantic_title": "raynet: learning volumetric 3d reconstruction with ray potentials",
    "citation_count": 91,
    "authors": [
      "Despoina Paschalidou",
      "Osman Ulusoy",
      "Carolin Schmitt",
      "Luc Van Gool",
      "Andreas Geiger"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kato_Neural_3D_Mesh_CVPR_2018_paper.html": {
    "title": "Neural 3D Mesh Renderer",
    "volume": "main",
    "abstract": "For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer",
    "checked": true,
    "id": "7eb89cbdfdde8cb6071a48fb44173a757f51bfd5",
    "semantic_title": "neural 3d mesh renderer",
    "citation_count": 1045,
    "authors": [
      "Hiroharu Kato",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Structured_Attention_Guided_CVPR_2018_paper.html": {
    "title": "Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach benefits from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Our extensive experimental evaluation demonstrates the effectiveness of the proposed method which is competitive with previous methods on the KITTI benchmark and outperforms the state of the art on the NYU Depth V2 dataset",
    "checked": true,
    "id": "1231e9ea7fe18e8d6cc7fd0b0285c3644b5e9bed",
    "semantic_title": "structured attention guided convolutional neural fields for monocular depth estimation",
    "citation_count": 306,
    "authors": [
      "Dan Xu",
      "Wei Wang",
      "Hao Tang",
      "Hong Liu",
      "Nicu Sebe",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Automatic_3D_Indoor_CVPR_2018_paper.html": {
    "title": "Automatic 3D Indoor Scene Modeling From Single Panorama",
    "volume": "main",
    "abstract": "We describe a system that automatically extracts 3D geometry of an indoor scene from a single 2D panorama. Our system recovers the spatial layout by finding the floor, walls, and ceiling; it also recovers shapes of typical indoor objects such as furniture. Using sampled perspective sub-views, we extract geometric cues (lines, vanishing points, orientation map, and surface normals) and semantic cues (saliency and object detection information). These cues are used for ground plane estimation and occlusion reasoning. The global spatial layout is inferred through a constraint graph on line segments and planar superpixels. The recovered layout is then used to guide shape estimation of the remaining objects using their normal information. Experiments on synthetic and real datasets show that our approach is state-of-the-art in both accuracy and efficiency. Our system can handle cluttered scenes with complex geometry that are challenging to existing techniques",
    "checked": true,
    "id": "e20e2a0d451d56f645365013bcc9ffa9da17abd9",
    "semantic_title": "automatic 3d indoor scene modeling from single panorama",
    "citation_count": 44,
    "authors": [
      "Yang Yang",
      "Shi Jin",
      "Ruiyang Liu",
      "Sing Bing Kang",
      "Jingyi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_Extreme_3D_Face_CVPR_2018_paper.html": {
    "title": "Extreme 3D Face Reconstruction: Seeing Through Occlusions",
    "volume": "main",
    "abstract": "Existing single view, 3D face reconstruction methods can produce beautifully detailed 3D results, but typically only for near frontal, unobstructed viewpoints. We describe a system designed to provide detailed 3D reconstructions of faces viewed under extreme conditions, out of plane rotations, and occlusions. Motivated by the concept of bump mapping, we propose a layered approach which decouples estimation of a global shape from its mid-level details (e.g., wrinkles). We estimate a coarse 3D face shape which acts as a foundation and then separately layer this foundation with details represented by a bump map. We show how a deep convolutional encoder-decoder can be used to estimate such bump maps. We further show how this approach naturally extends to generate plausible details for occluded facial regions. We test our approach and its components extensively, quantitatively demonstrating the invariance of our estimated facial details. We further provide numerous qualitative examples showing that our method produces detailed 3D face shapes in viewing conditions where existing state of the art often break down",
    "checked": true,
    "id": "f49b4ab188dd090367d9f6762473879b2bba16cf",
    "semantic_title": "extreme 3d face reconstruction: seeing through occlusions",
    "citation_count": 173,
    "authors": [
      "Anh Tuấn Trần",
      "Tal Hassner",
      "Iacopo Masi",
      "Eran Paz",
      "Yuval Nirkin",
      "Gérard Medioni"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Larsson_Beyond_Grobner_Bases_CVPR_2018_paper.html": {
    "title": "Beyond Grobner Bases: Basis Selection for Minimal Solvers",
    "volume": "main",
    "abstract": "Many computer vision applications require robust estimation of the underlying geometry, in terms of camera motion and 3D structure of the scene. These robust methods often rely on running minimal solvers in a RANSAC framework. In this paper we show how we can make polynomial solvers based on the action matrix method faster, by careful selection of the monomial bases. These monomial bases have traditionally been based on a Grobner basis for the polynomial ideal. Here we describe how we can enumerate all such bases in an efficient way. We also show that going beyond Grobner bases leads to more efficient solvers in many cases. We present a novel basis sampling scheme that we evaluate on a number of problems",
    "checked": true,
    "id": "492fcea5cc0a9a9446c1caa1e14ec3864deeb454",
    "semantic_title": "beyond grobner bases: basis selection for minimal solvers",
    "citation_count": 64,
    "authors": [
      "Viktor Larsson",
      "Magnus Oskarsson",
      "Kalle Astrom",
      "Alge Wallis",
      "Zuzana Kukelova",
      "Tomas Pajdla"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zuffi_Lions_and_Tigers_CVPR_2018_paper.html": {
    "title": "Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape From Images",
    "volume": "main",
    "abstract": "Animals are widespread in nature and the analysis of their shape and motion is important in many fields and industries. Modeling 3D animal shape, however, is difficult because the 3D scanning methods used to capture human shape are not applicable to wild animals or natural settings. Consequently, we propose a method to capture the detailed 3D shape of animals from images alone. The articulated and deformable nature of animals makes this problem extremely challenging, particularly in unconstrained environments with moving and uncalibrated cameras. To make this possible, we use a strong prior model of articulated animal shape that we fit to the image data. We then deform the animal shape in a canonical reference pose such that it matches image evidence when articulated and projected into multiple images. Our method extracts significantly more 3D shape detail than previous methods and is able to model new species, including the shape of an extinct animal, using only a few video frames. Additionally, the projected 3D shapes are accurate enough to facilitate the extraction of a realistic texture map from multiple frames",
    "checked": true,
    "id": "d42f3aaa39da2dc47e361f1878c817bc7114e2b3",
    "semantic_title": "lions and tigers and bears: capturing non-rigid, 3d, articulated shape from images",
    "citation_count": 136,
    "authors": [
      "Silvia Zuffi",
      "Angjoo Kanazawa",
      "Michael J. Black"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Deep_Cocktail_Network_CVPR_2018_paper.html": {
    "title": "Deep Cocktail Network: Multi-Source Unsupervised Domain Adaptation With Category Shift",
    "volume": "main",
    "abstract": "Most existing unsupervised domain adaptation (UDA) methods are based upon the assumption that source labeled data come from an identical underlying distribution. Whereas in practical scenario, labeled instances are typically collected from diverse sources. Moreover, those sources may not completely share their categories, which further brings a category shift challenge to multi-source (unsupervised) domain adaptation (MDA). In this paper, we propose a deep cocktail network (DCTN), to battle the domain and category shifts among multiple sources. Motivated by the theoretical results in cite{mansour2009domain}, the target distribution can be represented as the weighted combination of source distributions, and, the training of MDA via DCTN is then performed as two alternating steps: i) It deploys multi-way adversarial learning to minimize the discrepancy between the target and each of the multiple source domains, which also obtains the source-specific perplexity scores to denote the possibilities that a target sample belongs to different source domains. ii) The multi-source category classifiers are integrated with the perplexity scores to classify target sample, and the pseudo-labeled target samples together with source samples are utilized to update the multi-source category classifier and the representation module. We evaluate DCTN in three domain adaptation benchmarks, which clearly demonstrate the superiority of our framework",
    "checked": true,
    "id": "153ab2363e6fe86749449b70d04fa5cbc82dd68f",
    "semantic_title": "deep cocktail network: multi-source unsupervised domain adaptation with category shift",
    "citation_count": 389,
    "authors": [
      "Ruijia Xu",
      "Ziliang Chen",
      "Wangmeng Zuo",
      "Junjie Yan",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xia_DOTA_A_Large-Scale_CVPR_2018_paper.html": {
    "title": "DOTA: A Large-Scale Dataset for Object Detection in Aerial Images",
    "volume": "main",
    "abstract": "Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect 2806 aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188,282 instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral. To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging",
    "checked": true,
    "id": "8ecf9e6428edd33986e8c6a143870e57142fd5a1",
    "semantic_title": "dota: a large-scale dataset for object detection in aerial images",
    "citation_count": 2173,
    "authors": [
      "Gui-Song Xia",
      "Xiang Bai",
      "Jian Ding",
      "Zhen Zhu",
      "Serge Belongie",
      "Jiebo Luo",
      "Mihai Datcu",
      "Marcello Pelillo",
      "Liangpei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Engilberge_Finding_Beans_in_CVPR_2018_paper.html": {
    "title": "Finding Beans in Burgers: Deep Semantic-Visual Embedding With Localization",
    "volume": "main",
    "abstract": "Several works have proposed to learn a two-path neural network that maps images and texts, respectively, to a same shared Euclidean space where geometry captures useful semantic relationships. Such a multi-modal embedding can be trained and used for various tasks, notably image captioning. In the present work, we introduce a new architecture of this type, with a visual path that leverages recent space-aware pooling mechanisms. Combined with a textual path which is jointly trained from scratch, our semantic-visual embedding offers a versatile model. Once trained under the supervision of captioned images, it yields new state-of-the-art performance on cross-modal retrieval. It also allows the localization of new concepts from the embedding space into any input image, delivering state-of-the-art result on the visual grounding of phrases",
    "checked": true,
    "id": "9645e8b4829c04879a642d8dd6b3cdf5cf264afb",
    "semantic_title": "finding beans in burgers: deep semantic-visual embedding with localization",
    "citation_count": 95,
    "authors": [
      "Martin Engilberge",
      "Louis Chevallier",
      "Patrick Pérez",
      "Matthieu Cord"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tan_Feature_Super-Resolution_Make_CVPR_2018_paper.html": {
    "title": "Feature Super-Resolution: Make Machine See More Clearly",
    "volume": "main",
    "abstract": "Identifying small size images or small objects is a notoriously challenging problem, as discriminative representations are difficult to learn from the limited information contained in them with poor-quality appearance and unclear object structure. Existing research works usually increase the resolution of low-resolution image in the pixel space in order to provide better visual quality for human viewing. However, the improved performance of such methods is usually limited or even trivial in the case of very small image size (we will show it in this paper explicitly). In this paper, different from image super-resolution (ISR), we propose a novel super-resolution technique called feature super-resolution (FSR), which aims at enhancing the discriminatory power of small size image in order to provide high recognition precision for machine. To achieve this goal, we propose a new Feature Super-Resolution Generative Adversarial Network (FSR-GAN) model that transforms the raw poor features of small size images to highly discriminative ones by performing super-resolution in the feature space. Our FSR-GAN consists of two subnetworks: a feature generator network G and a feature discriminator network D. By training the G and the D networks in an alternative manner, we encourage the G network to discover the latent distribution correlations between small size and large size images and then use G to improve the representations of small images. Extensive experiment results on Oxford5K, Paris, Holidays, and Flick100k datasets demonstrate that the proposed FSR approach can effectively enhance the discriminatory ability of features. Even when the resolution of query images is reduced greatly, e.g., 1/64 original size, the query feature enhanced by our FSR approach achieves surprisingly high retrieval performance at different image resolutions and increases the retrieval precision by 25% compared to the raw query feature",
    "checked": true,
    "id": "a9f736a8adee4670c029a297ce1eba1b24bc52b3",
    "semantic_title": "feature super-resolution: make machine see more clearly",
    "citation_count": 48,
    "authors": [
      "Weimin Tan",
      "Bo Yan",
      "Bahetiyaer Bare"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/LaLonde_ClusterNet_Detecting_Small_CVPR_2018_paper.html": {
    "title": "ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information",
    "volume": "main",
    "abstract": "Object detection in wide area motion imagery (WAMI) has drawn the attention of the computer vision research community for a number of years. WAMI proposes a number of unique challenges including extremely small object sizes, both sparse and densely-packed objects, and extremely large search spaces (large video frames). Nearly all state-of-the-art methods in WAMI object detection report that appearance-based classifiers fail in this challenging data and instead rely almost entirely on motion information in the form of background subtraction or frame-differencing. In this work, we experimentally verify the failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a heatmap-based fully convolutional neural network (CNN), and propose a novel two-stage spatio-temporal CNN which effectively and efficiently combines both appearance and motion information to significantly surpass the state-of-the-art in WAMI object detection. To reduce the large search space, the first stage (ClusterNet) takes in a set of extremely large video frames, combines the motion and appearance information within the convolutional architecture, and proposes regions of objects of interest (ROOBI). These ROOBI can contain from one to clusters of several hundred objects due to the large video frame size and varying object density in WAMI. The second stage (FoveaNet) then estimates the centroid location of all objects in that given ROOBI simultaneously via heatmap estimation. The proposed method exceeds state-of-the-art results on the WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped objects, as well as being the first proposed method in wide area motion imagery to detect completely stationary objects",
    "checked": true,
    "id": "c1adf18ce943f69b0f7c06d6effc4df3f4c29b1a",
    "semantic_title": "clusternet: detecting small objects in large scenes by exploiting spatio-temporal information",
    "citation_count": 117,
    "authors": [
      "Rodney LaLonde",
      "Dong Zhang",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_MaskLab_Instance_Segmentation_CVPR_2018_paper.html": {
    "title": "MaskLab: Instance Segmentation by Refining Object Detection With Semantic and Direction Features",
    "volume": "main",
    "abstract": "In this work, we tackle the problem of instance segmentation, the task of simultaneously solving object detection and semantic segmentation. Towards this goal, we present a model, called MaskLab, which produces three outputs: box detection, semantic segmentation, and direction prediction. Building on top of the Faster-RCNN object detector, the predicted boxes provide accurate localization of object instances. Within each region of interest, MaskLab performs foreground/background segmentation by combining semantic and direction prediction. Semantic segmentation assists the model in distinguishing between objects of different semantic classes including background, while the direction prediction, estimating each pixel's direction towards its corresponding center, allows separating instances of the same semantic class. Moreover, we explore the effect of incorporating recent successful methods from both segmentation and detection (eg, atrous convolution and hypercolumn). Our proposed model is evaluated on the COCO instance segmentation benchmark and shows comparable performance with other state-of-art models",
    "checked": true,
    "id": "6b89c45ed9e12d3392e28730f80234aa82e41cbc",
    "semantic_title": "masklab: instance segmentation by refining object detection with semantic and direction features",
    "citation_count": 348,
    "authors": [
      "Liang-Chieh Chen",
      "Alexander Hermans",
      "George Papandreou",
      "Florian Schroff",
      "Peng Wang",
      "Hartwig Adam"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/He_Hashing_as_Tie-Aware_CVPR_2018_paper.html": {
    "title": "Hashing as Tie-Aware Learning to Rank",
    "volume": "main",
    "abstract": "Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks",
    "checked": true,
    "id": "9d0a6e75afaa9ee521f092de827f6659588e9ed1",
    "semantic_title": "hashing as tie-aware learning to rank",
    "citation_count": 84,
    "authors": [
      "Kun He",
      "Fatih Cakir",
      "Sarah Adel Bargal",
      "Stan Sclaroff"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sharma_Classification-Driven_Dynamic_Image_CVPR_2018_paper.html": {
    "title": "Classification-Driven Dynamic Image Enhancement",
    "volume": "main",
    "abstract": "Convolutional neural networks rely on image texture and structure to serve as discriminative features to classify the image content. Image enhancement techniques can be used as preprocessing steps to help improve the overall image quality and in turn improve the overall effectiveness of a CNN. Existing image enhancement methods, however, are designed to improve the perceptual quality of an image for a human observer. In this paper, we are interested in learning CNNs that can emulate image enhancement and restoration, but with the overall goal to improve image classification and not necessarily human perception. To this end, we present a unified CNN architecture that uses a range of enhancement filters that can enhance image-specific details via end-to-end dynamic filter learning. We demonstrate the effectiveness of this strategy on four challenging benchmark datasets for fine-grained, object, scene and texture classification: CUB-200-2011, PASCAL-VOC2007, MIT-Indoor, and DTD. Experiments using our proposed enhancement shows promising results on all the datasets. In addition, our approach is capable of improving the performance of all generic CNN architectures",
    "checked": true,
    "id": "375d03a945273f486f0a05581eac0f50a344db36",
    "semantic_title": "classification-driven dynamic image enhancement",
    "citation_count": 67,
    "authors": [
      "Vivek Sharma",
      "Ali Diba",
      "Davy Neven",
      "Michael S. Brown",
      "Luc Van Gool",
      "Rainer Stiefelhagen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Knowledge_Aided_Consistency_CVPR_2018_paper.html": {
    "title": "Knowledge Aided Consistency for Weakly Supervised Phrase Grounding",
    "volume": "main",
    "abstract": "Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image. In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set. Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals. However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge. In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding. We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information. To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals. Experiments show that KAC Net provides a significant improvement on two popular datasets",
    "checked": true,
    "id": "0f66f124bcfa4c3d9d0e54af0c1103f1219c1c8c",
    "semantic_title": "knowledge aided consistency for weakly supervised phrase grounding",
    "citation_count": 90,
    "authors": [
      "Kan Chen",
      "Jiyang Gao",
      "Ram Nevatia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ehsani_Who_Let_the_CVPR_2018_paper.html": {
    "title": "Who Let the Dogs Out? Modeling Dog Behavior From Visual Data",
    "volume": "main",
    "abstract": "We introduce the task of directly modeling a visually intelligent agent. Computer vision typically focuses on solving various subtasks related to visual intelligence. We depart from this standard approach to computer vision; instead we directly model a visually intelligent agent. Our model takes visual information as input and directly predicts the actions of the agent. Toward this end we introduce DECADE, a large-scale dataset of ego-centric videos from a dog's perspective as well as her corresponding movements. Using this data we model how the dog acts and how the dog plans her movements. We show under a variety of metrics that given just visual input we can successfully model this intelligent agent in many situations. Moreover, the representation learned by our model encodes distinct information compared to representations trained on image classification, and our learned representation can generalize to other domains. In particular, we show strong results on the task of walkable surface estimation by using this dog modeling task as representation learning",
    "checked": true,
    "id": "a63b8429ebeef316a65a94b021ef9a214c705f83",
    "semantic_title": "who let the dogs out? modeling dog behavior from visual data",
    "citation_count": 59,
    "authors": [
      "Kiana Ehsani",
      "Hessam Bagherinezhad",
      "Joseph Redmon",
      "Roozbeh Mottaghi",
      "Ali Farhadi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Pseudo_Mask_Augmented_CVPR_2018_paper.html": {
    "title": "Pseudo Mask Augmented Object Detection",
    "volume": "main",
    "abstract": "In this work, we present a novel and effective framework to facilitate object detection with the instance-level segmentation information that is only supervised by bounding box annotation. Starting from the joint object detection and instance segmentation network, we propose to recursively estimate the pseudo ground-truth object masks from the instance-level object segmentation network training, and then enhance the detection network with top-down segmentation feedbacks. The pseudo ground truth mask and network parameters are optimized alternatively to mutually benefit each other. To obtain the promising pseudo masks in each iteration, we embed a graphical inference that incorporates the low-level image appearance consistency and the bounding box annotations to refine the segmentation masks predicted by the segmentation network. Our approach progressively improves the object detection performance by incorporating the detailed pixel-wise information learned from the weakly-supervised segmentation network. Extensive evaluation on the detection task in PASCAL VOC 2007 and 2012 verifies that the proposed approach is effective",
    "checked": true,
    "id": "50ec920cb4beca056e73858d2d10e67b4bb4824f",
    "semantic_title": "pseudo mask augmented object detection",
    "citation_count": 42,
    "authors": [
      "Xiangyun Zhao",
      "Shuang Liang",
      "Yichen Wei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Dual_Skipping_Networks_CVPR_2018_paper.html": {
    "title": "Dual Skipping Networks",
    "volume": "main",
    "abstract": "Inspired by the recent neuroscience studies on the left-right asymmetry of the human brain in processing low and high spatial frequency information, this paper introduces a dual skipping network which carries out coarse-to-fine object categorization. Such a network has two branches to simultaneously deal with both coarse and fine-grained classification tasks. Specifically, we propose a layer-skipping mechanism that learns a gating network to predict which layers to skip in the testing stage. This layer-skipping mechanism endows the network with good flexibility and capability in practice. Evaluations are conducted on several widely used coarse-to-fine object categorization benchmarks, and promising results are achieved by our proposed network model",
    "checked": true,
    "id": "60850bca77b3cdcd55ad2c9eb2d547125e0d2832",
    "semantic_title": "dual skipping networks",
    "citation_count": 8,
    "authors": [
      "Changmao Cheng",
      "Yanwei Fu",
      "Yu-Gang Jiang",
      "Wei Liu",
      "Wenlian Lu",
      "Jianfeng Feng",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cai_Memory_Matching_Networks_CVPR_2018_paper.html": {
    "title": "Memory Matching Networks for One-Shot Image Recognition",
    "volume": "main",
    "abstract": "In this paper, we introduce the new ideas of augmenting Convolutional Neural Networks (CNNs) with Memory and learning to learn the network parameters for the unlabelled images on the fly in one-shot learning. Specifically, we present Memory Matching Networks (MM-Net) --- a novel deep architecture that explores the training procedure, following the philosophy that training and test conditions must match. Technically, MM-Net writes the features of a set of labelled images (support set) into memory and reads from memory when performing inference to holistically leverage the knowledge in the set. Meanwhile, a Contextual Learner employs the memory slots in a sequential manner to predict the parameters of CNNs for unlabelled images. The whole architecture is trained by once showing only a few examples per class and switching the learning from minibatch to minibatch, which is tailored for one-shot learning when presented with a few examples of new categories at test time. Unlike the conventional one-shot learning approaches, our MM-Net could output one unified model irrespective of the number of shots and categories. Extensive experiments are conducted on two public datasets, i.e., Omniglot and emph{mini}ImageNet, and superior results are reported when compared to state-of-the-art approaches. More remarkably, our MM-Net improves one-shot accuracy on Omniglot from 98.95% to 99.28% and from 49.21% to 53.37% on emph{mini}ImageNet",
    "checked": true,
    "id": "78e41d3eb2acd47083a4ec4765ad443617a109ef",
    "semantic_title": "memory matching networks for one-shot image recognition",
    "citation_count": 264,
    "authors": [
      "Qi Cai",
      "Yingwei Pan",
      "Ting Yao",
      "Chenggang Yan",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gordon_IQA_Visual_Question_CVPR_2018_paper.html": {
    "title": "IQA: Visual Question Answering in Interactive Environments",
    "volume": "main",
    "abstract": "We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: \"Are there any apples in the fridge?\" The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR [35], a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98",
    "checked": true,
    "id": "b0cd469a06fb2eae3a5cc0c860aa592f71b13f6d",
    "semantic_title": "iqa: visual question answering in interactive environments",
    "citation_count": 390,
    "authors": [
      "Daniel Gordon",
      "Aniruddha Kembhavi",
      "Mohammad Rastegari",
      "Joseph Redmon",
      "Dieter Fox",
      "Ali Farhadi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Pose_Transferrable_Person_CVPR_2018_paper.html": {
    "title": "Pose Transferrable Person Re-Identification",
    "volume": "main",
    "abstract": "Person re-identification (ReID) is an important task in the field of intelligent security. A key challenge is how to capture human pose variations, while existing benchmarks (i.e., Market1501, DukeMTMC-reID, CUHK03, etc.) do NOT provide sufficient pose coverage to train a robust ReID system. To address this issue, we propose a pose-transferrable person ReID framework which utilizes pose-transferred sample augmentations (i.e., with ID supervision) to enhance ReID model training. On one hand, novel training samples with rich pose variations are generated via transferring pose instances from MARS dataset, and they are added into the target dataset to facilitate robust training. On the other hand, in addition to the conventional discriminator of GAN (i.e., to distinguish between REAL/FAKE samples), we propose a novel guider sub-network which encourages the generated sample (i.e., with novel pose) towards better satisfying the ReID loss (i.e., cross-entropy ReID loss, triplet ReID loss). In the meantime, an alternative optimization procedure is proposed to train the proposed Generator-Guider-Discriminator network. Experimental results on Market-1501, DukeMTMC-reID and CUHK03 show that our method achieves great performance improvement, and outperforms most state-of-the-art methods without elaborate designing the ReID model",
    "checked": true,
    "id": "df4ed9983f7114ca4f0ab71f1476c0bf7521e317",
    "semantic_title": "pose transferrable person re-identification",
    "citation_count": 356,
    "authors": [
      "Jinxian Liu",
      "Bingbing Ni",
      "Yichao Yan",
      "Peng Zhou",
      "Shuo Cheng",
      "Jianguo Hu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cui_Large_Scale_Fine-Grained_CVPR_2018_paper.html": {
    "title": "Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning",
    "volume": "main",
    "abstract": "Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via fine-tuning offers an effective solution for domain-specific fine-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car make & model). In such scenarios, data annotation often calls for specialized domain knowledge and thus is difficult to scale. In this work, we first tackle a problem in large scale FGVC. Our method won first place in iNaturalist 2017 large scale species classification challenge. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via fine-tuning from large scale datasets to small scale, domain-specific FGVC datasets. We propose a measure to estimate domain similarity via Earth Mover's Distance and demonstrate that transfer learning benefits from pre-training on a source domain that is similar to the target domain by this measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on multiple commonly used FGVC datasets",
    "checked": true,
    "id": "89c3355f5bc7130ae4ed090c8accc52dd885d558",
    "semantic_title": "large scale fine-grained categorization and domain-specific transfer learning",
    "citation_count": 479,
    "authors": [
      "Yin Cui",
      "Yang Song",
      "Chen Sun",
      "Andrew Howard",
      "Serge Belongie"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Radosavovic_Data_Distillation_Towards_CVPR_2018_paper.html": {
    "title": "Data Distillation: Towards Omni-Supervised Learning",
    "volume": "main",
    "abstract": "We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone",
    "checked": true,
    "id": "757b27a3ceb2293b8284fc24a7084a0c3fc2ae21",
    "semantic_title": "data distillation: towards omni-supervised learning",
    "citation_count": 419,
    "authors": [
      "Ilija Radosavovic",
      "Piotr Dollár",
      "Ross Girshick",
      "Georgia Gkioxari",
      "Kaiming He"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Vasudevan_Object_Referring_in_CVPR_2018_paper.html": {
    "title": "Object Referring in Videos With Language and Human Gaze",
    "volume": "main",
    "abstract": "We investigate the problem of object referring (OR) i.e. to localize a target object in a visual scene coming with a language description. Humans perceive the world more as continued video snippets than as static images, and describe objects not only by their appearance, but also by their spatio-temporal context and motion features. Humans also gaze at the object when they issue a referring expression. Existing works for OR mostly focus on static images only, which fall short in providing many such cues. This paper addresses OR in videos with language and human gaze. To that end, we present a new video dataset for OR, with 30,000 objects over 5,000 stereo video sequences annotated for their descriptions and gaze. We further propose a novel network model for OR in videos, by integrating appearance, motion, gaze, and spatio-temporal context into one network. Experimental results show that our method effectively utilizes motion cues, human gaze, and spatio-temporal context. Our method outperforms previous OR methods",
    "checked": true,
    "id": "7a82d83f818cdc4ac714e468446bc2499ff9caa7",
    "semantic_title": "object referring in videos with language and human gaze",
    "citation_count": 75,
    "authors": [
      "Arun Balajee Vasudevan",
      "Dengxin Dai",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhai_Feature_Selective_Networks_CVPR_2018_paper.html": {
    "title": "Feature Selective Networks for Object Detection",
    "volume": "main",
    "abstract": "Objects for detection usually have distinct characteristics in different sub-regions and different aspect ratios. However, in prevalent two-stage object detection methods, Region-of-Interest (RoI) features are extracted by RoI pooling with little emphasis on these translation-variant feature components. We present feature selective networks to reform the feature representations of RoIs by exploiting their disparities among sub-regions and aspect ratios. Our network produces the sub-region attention bank and aspect ratio attention bank for the whole image. The RoI-based sub-region attention map and aspect ratio attention map are selectively pooled from the banks, and then used to refine the original RoI features for RoI classification. Equipped with a light-weight detection subnetwork, our network gets a consistent boost in detection performance based on general ConvNet backbones (ResNet-101, GoogLeNet and VGG-16). Without bells and whistles, our detectors equipped with ResNet-101 achieve more than 3% mAP improvement compared to counterparts on PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO datasets",
    "checked": true,
    "id": "5afb214f062924543a2506e7bf094c4aa659aca5",
    "semantic_title": "feature selective networks for object detection",
    "citation_count": 18,
    "authors": [
      "Yao Zhai",
      "Jingjing Fu",
      "Yan Lu",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Learning_a_Discriminative_CVPR_2018_paper.html": {
    "title": "Learning a Discriminative Filter Bank Within a CNN for Fine-Grained Recognition",
    "volume": "main",
    "abstract": "Compared to earlier multistage frameworks using CNN features, recent end-to-end deep approaches for fine-grained recognition essentially enhance the mid-level learning capability of CNNs. Previous approaches achieve this by introducing an auxiliary network to infuse localization information into the main classification network, or a sophisticated feature encoding method to capture higher order feature statistics. We show that mid-level representation learning can be enhanced within the CNN framework, by learning a bank of convolutional filters that capture class-specific discriminative patches without extra part or bounding box annotations. Such a filter bank is well structured, properly initialized and discriminatively learned through a novel asymmetric multi-stream architecture with convolutional filter supervision and a non-random layer initialization. Experimental results show that our approach achieves state-of-the-art on three publicly available fine-grained recognition datasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and visualizations are further provided to understand our approach",
    "checked": true,
    "id": "32a40b045e665db39e120c12338f9f1238b0690b",
    "semantic_title": "learning a discriminative filter bank within a cnn for fine-grained recognition",
    "citation_count": 340,
    "authors": [
      "Yaming Wang",
      "Vlad I. Morariu",
      "Larry S. Davis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Grounding_Referring_Expressions_CVPR_2018_paper.html": {
    "title": "Grounding Referring Expressions in Images by Variational Context",
    "volume": "main",
    "abstract": "We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., ``largest elephant standing behind baby elephant''. This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context --- visual attributes (e.g., ``largest'', ``baby'') and relationships (e.g., ``behind'') that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Our model exploits the reciprocal relation between the referent and context, i.e., either of them influences the estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced. We also extend the model to the unsupervised setting where no annotation for the referent is available. Extensive experiments on various benchmarks show consistent improvement over state-of-the-art methods in both supervised and unsupervised settings. The code is available at url{https://github.com/yuleiniu/vc/",
    "checked": true,
    "id": "69d576ffe624f11fe4e84a03d4063856a5af838f",
    "semantic_title": "grounding referring expressions in images by variational context",
    "citation_count": 188,
    "authors": [
      "Hanwang Zhang",
      "Yulei Niu",
      "Shih-Fu Chang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_Dynamic_Graph_Generation_CVPR_2018_paper.html": {
    "title": "Dynamic Graph Generation Network: Generating Relational Knowledge From Diagrams",
    "volume": "main",
    "abstract": "In this work, we introduce a new algorithm for analyzing a diagram, which contains visual and textual information in an abstract and integrated way. Whereas diagrams contain richer information compared with individual image-based or language-based data, proper solutions for automatically understanding them have not been proposed due to their innate characteristics of multi-modality and arbitrariness of layouts. To tackle this problem, we propose a unified diagram-parsing network for generating knowledge from diagrams based on an object detector and a recurrent neural network designed for a graphical structure. Specifically, we propose a dynamic graph-generation network that is based on dynamic memory and graph theory. We explore the dynamics of information in a diagram with activation of gates in gated recurrent unit (GRU) cells. On publicly available diagram datasets, our model demonstrates a state-of-the-art result that outperforms other baselines. Moreover, further experiments on question answering shows potentials of the proposed method for various applications",
    "checked": true,
    "id": "b969ae8270f113477347de9778880ff91d75ed72",
    "semantic_title": "dynamic graph generation network: generating relational knowledge from diagrams",
    "citation_count": 25,
    "authors": [
      "Daesik Kim",
      "YoungJoon Yoo",
      "Jee-Soo Kim",
      "SangKuk Lee",
      "Nojun Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Roveri_A_Network_Architecture_CVPR_2018_paper.html": {
    "title": "A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation",
    "volume": "main",
    "abstract": "We propose a novel neural network architecture for point cloud classification. Our key idea is to automatically transform the 3D unordered input data into a set of useful 2D depth images, and classify them by exploiting well performing image classification CNNs. We present new differentiable module designs to generate depth images from a point cloud. These modules can be combined with any network architecture for processing point clouds. We utilize them in combination with state-of-the-art classification networks, and get results competitive with the state of the art in point cloud classification. Furthermore, our architecture automatically produces informative images representing the input point cloud, which could be used for further applications such as point cloud visualization",
    "checked": true,
    "id": "f581168a9f59debde256307569057a95c04b245b",
    "semantic_title": "a network architecture for point cloud classification via automatic depth images generation",
    "citation_count": 68,
    "authors": [
      "Riccardo Roveri",
      "Lukas Rahmann",
      "Cengiz Oztireli",
      "Markus Gross"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bozek_Towards_Dense_Object_CVPR_2018_paper.html": {
    "title": "Towards Dense Object Tracking in a 2D Honeybee Hive",
    "volume": "main",
    "abstract": "From human crowds to cells in a tissue, the detection and efficient tracking of multiple objects in dense configurations is an important and unsolved problem. In the past, limitations of image analysis have restricted studies of dense groups to tracking one individual, a set of marked individuals, or to coarse-grained group-level dynamics, all of which yield incomplete information. Here, we combine the power of convolutional neural networks (CNNs) with the model environment of a honeybee hive to develop an automated method for the recognition of all individuals in a dense group based on raw image data. In the proposed solution, we create new, adapted individual labeling and use segmentation architecture U-Net with a specific loss function to predict both object location and orientation. We additionally leverage time series image data to exploit both structural and temporal regularities in the the tracked objects in a recurrent manner. This allowed us to achieve near human-level performance on real-world image data while dramatically reducing original network size to 6% of the initial parameters. Given the novel application of CNNs in this study, we generate extensive problem-specific image data in which labeled examples are produced through a custom interface with Amazon Mechanical Turk. This dataset contains over 375,000 labeled bee instances moving across 720 video frames with 2 fps sampling and represents an extensive resource for development and testing of dense object recognition and tracking methods. With our method we correctly detect 96% of individuals with a location error of ~7% of a typical body dimension, and orientation error of 12 degrees, approximating the variability in labeling by human raters with ~9% body dimension variation in position and 8 degrees orientation variation. Our study represents an important step towards efficient image-based dense object tracking by allowing for the accurate determination of object location and orientation across time-series image data efficiently within one network architecture",
    "checked": true,
    "id": "af89e230af9b31559ebd787767a47dc1670012dc",
    "semantic_title": "towards dense object tracking in a 2d honeybee hive",
    "citation_count": 47,
    "authors": [
      "Katarzyna Bozek",
      "Laetitia Hebert",
      "Alexander S. Mikheyev",
      "Greg J. Stephens"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bhattacharyya_Long-Term_On-Board_Prediction_CVPR_2018_paper.html": {
    "title": "Long-Term On-Board Prediction of People in Traffic Scenes Under Uncertainty",
    "volume": "main",
    "abstract": "Progress towards advanced systems for assisted and autonomous driving is leveraging recent advances in recognition and segmentation methods. Yet, we are still facing challenges in bringing reliable driving to inner cities, as those are composed of highly dynamic scenes observed from a moving platform at considerable speeds. Anticipation becomes a key element in order to react timely and prevent accidents. In this paper we argue that it is necessary to predict at least 1 second and we thus propose a new model that jointly predicts ego motion and people trajectories over such large time horizons. We pay particular attention to modeling the uncertainty of our estimates arising from the non-deterministic nature of natural traffic scenes. Our experimental results show that it is indeed possible to predict people trajectories at the desired time horizons and that our uncertainty estimates are informative of the prediction error. We also show that both sequence modeling of trajectories as well as our novel method of long term odometry prediction are essential for best performance",
    "checked": true,
    "id": "1efc28925ec65879652ad9d5f8be53037bb2f306",
    "semantic_title": "long-term on-board prediction of people in traffic scenes under uncertainty",
    "citation_count": 203,
    "authors": [
      "Apratim Bhattacharyya",
      "Mario Fritz",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Single-Shot_Refinement_Neural_CVPR_2018_paper.html": {
    "title": "Single-Shot Refinement Neural Network for Object Detection",
    "volume": "main",
    "abstract": "For object detection, the two-stage approach (e.g., Faster R-CNN) has been achieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has the advantage of high efficiency. To inherit the merits of both while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called RefineDet, that achieves better accuracy than two-stage methods and maintains comparable efficiency of one-stage methods. RefineDet consists of two inter-connected modules, namely, the anchor refinement module and the object detection module. Specifically, the former aims to (1) filter out negative anchors to reduce search space for the classifier, and (2) coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor. The latter module takes the refined anchors as the input from the former to further improve the regression accuracy and predict multi-class label. Meanwhile, we design a transfer connection block to transfer the features in the anchor refinement module to predict locations, sizes and class labels of objects in the object detection module. The multi-task loss function enables us to train the whole network in an end-to-end way. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that RefineDet achieves state-of-the-art detection accuracy with high efficiency. Code is available at https://github.com/sfzhang15/RefineDet",
    "checked": true,
    "id": "a17542bfb13074d8a48e1ce2ff6b3ba744433422",
    "semantic_title": "single-shot refinement neural network for object detection",
    "citation_count": 1287,
    "authors": [
      "Shifeng Zhang",
      "Longyin Wen",
      "Xiao Bian",
      "Zhen Lei",
      "Stan Z. Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Video_Captioning_via_CVPR_2018_paper.html": {
    "title": "Video Captioning via Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset",
    "checked": true,
    "id": "74b284a66e75b65f5970d05bac000fe91243ee49",
    "semantic_title": "video captioning via hierarchical reinforcement learning",
    "citation_count": 228,
    "authors": [
      "Xin Wang",
      "Wenhu Chen",
      "Jiawei Wu",
      "Yuan-Fang Wang",
      "William Yang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Teney_Tips_and_Tricks_CVPR_2018_paper.html": {
    "title": "Tips and Tricks for Visual Question Answering: Learnings From the 2017 Challenge",
    "volume": "main",
    "abstract": "This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection",
    "checked": true,
    "id": "b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81",
    "semantic_title": "tips and tricks for visual question answering: learnings from the 2017 challenge",
    "citation_count": 382,
    "authors": [
      "Damien Teney",
      "Peter Anderson",
      "Xiaodong He",
      "Anton van den Hengel"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Learning_to_Segment_CVPR_2018_paper.html": {
    "title": "Learning to Segment Every Thing",
    "volume": "main",
    "abstract": "Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to ~100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world",
    "checked": true,
    "id": "ccd99008d942b890cecd308a31ba61240eac9e54",
    "semantic_title": "learning to segment every thing",
    "citation_count": 296,
    "authors": [
      "Ronghang Hu",
      "Piotr Dollár",
      "Kaiming He",
      "Trevor Darrell",
      "Ross Girshick"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Self-Supervised_Adversarial_Hashing_CVPR_2018_paper.html": {
    "title": "Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "Thanks to the success of deep learning, cross-modal retrieval has made significant progress recently. However, there still remains a crucial bottleneck: how to bridge the modality gap to further enhance the retrieval accuracy. In this paper, we propose a self-supervised adversarial hashing (SSAH) approach, which lies among the early attempts to incorporate adversarial learning into cross-modal hashing in a self-supervised fashion. The primary contribution of this work is that two adversarial networks are leveraged to maximize the semantic correlation and consistency of the representations between different modalities. In addition, we harness a self-supervised semantic network to discover high-level semantic information in the form of multi-label annotations. Such information guides the feature learning process and preserves the modality relationships in both the common semantic space and the Hamming space. Extensive experiments carried out on three benchmark datasets validate that the proposed SSAH surpasses the state-of-the-art methods",
    "checked": true,
    "id": "a6d0a248e37ad54ccc4277605118ba980266f23e",
    "semantic_title": "self-supervised adversarial hashing networks for cross-modal retrieval",
    "citation_count": 360,
    "authors": [
      "Chao Li",
      "Cheng Deng",
      "Ning Li",
      "Wei Liu",
      "Xinbo Gao",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhuang_Parallel_Attention_A_CVPR_2018_paper.html": {
    "title": "Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries",
    "volume": "main",
    "abstract": "Recognising objects according to a pre-defined fixed set of class labels has been well studied in the Computer Vision. There are a great many practical applications where the subjects that may be of interest are not known beforehand, or so easily delineated, however. In many of these cases natural language dialog is a natural way to specify the subject of interest, and the task achieving this capability (a.k.a, Referring Expression Comprehension) has recently attracted attention.To this end we propose a unified framework, the ParalleL AttentioN (PLAN) network, to discover the object in an image that is being referred to in variable length natural expression descriptions, from short phrases query to long multi-round dialogs. The PLAN network has two attention mechanisms that relate parts of the expressions to both the global visual content and also directly to object candidates. Furthermore, the attention mechanisms are recurrent, making the referring process visualizable and explainable. The attended information from these dual sources are combined to reason about the referred object. These two attention mechanisms can be trained in parallel and we find the combined system outperforms the state-of-art on several benchmarked datasets with different length language input, such as RefCOCO, RefCOCO+ and GuessWhat?!",
    "checked": true,
    "id": "7299465d70181e423480fdb252aa2e28c18aa012",
    "semantic_title": "parallel attention: a unified framework for visual object discovery through dialogs and queries",
    "citation_count": 134,
    "authors": [
      "Bohan Zhuang",
      "Qi Wu",
      "Chunhua Shen",
      "Ian Reid",
      "Anton van den Hengel"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Zigzag_Learning_for_CVPR_2018_paper.html": {
    "title": "Zigzag Learning for Weakly Supervised Object Detection",
    "volume": "main",
    "abstract": "This paper addresses weakly supervised object detection with only image-level supervision at training stage. Previous approaches train detection models with entire images all at once, making the models prone to being trapped in sub-optimums due to the introduced false positive examples. Unlike them, we propose a zigzag learning strategy to simultaneously discover reliable object instances and prevent the model from overfitting initial seeds. Towards this goal, we first develop a criterion named mean Energy Accumulation Scores (mEAS) to automatically measure and rank localization difficulty of an image containing the target object, and accordingly learn the detector progressively by feeding examples with increasing difficulty. In this way, the model can be well prepared by training on easy examples for learning from more difficult ones and thus gain a stronger detection ability more efficiently. Furthermore, we introduce a novel masking regularization strategy over the high level convolutional feature maps to avoid overfitting initial samples. These two modules formulate a zigzag learning process, where progressive learning endeavors to discover reliable object instances, and masking regularization increases the difficulty of finding object instances properly. We achieve 47.6% mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin",
    "checked": true,
    "id": "76cefb4f62b652b785b9baa97558b488dca438a9",
    "semantic_title": "zigzag learning for weakly supervised object detection",
    "citation_count": 113,
    "authors": [
      "Xiaopeng Zhang",
      "Jiashi Feng",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Attentive_Fashion_Grammar_CVPR_2018_paper.html": {
    "title": "Attentive Fashion Grammar Network for Fashion Landmark Detection and Clothing Category Classification",
    "volume": "main",
    "abstract": "This paper proposes a knowledge-guided fashion network to solve the problem of visual fashion analysis, e.g., fashion landmark localization and clothing category classification. The suggested fashion model is leveraged with high-level human knowledge in this domain. We propose two important fashion grammars: (i) dependency grammar capturing kinematics-like relation, and (ii) symmetry grammar accounting for the bilateral symmetry of clothes. We introduce Bidirectional Convolutional Recurrent Neural Networks (BCRNNs) for efficiently approaching message passing over grammar topologies, and producing regularized landmark layouts. For enhancing clothing category classification, our fashion network is encoded with two novel attention mechanisms, i.e., landmark-aware attention and category-driven attention. The former enforces our network to focus on the functional parts of clothes, and learns domain-knowledge centered representations, leading to a supervised attention mechanism. The latter is goal-driven, which directly enhances task-related features and can be learned in an implicit, top-down manner. Experimental results on large-scale fashion datasets demonstrate the superior performance of our fashion grammar network",
    "checked": true,
    "id": "71573d5dc03b28279c1a337e9fd91ffbeff47569",
    "semantic_title": "attentive fashion grammar network for fashion landmark detection and clothing category classification",
    "citation_count": 222,
    "authors": [
      "Wenguan Wang",
      "Yuanlu Xu",
      "Jianbing Shen",
      "Song-Chun Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Verma_Generalized_Zero-Shot_Learning_CVPR_2018_paper.html": {
    "title": "Generalized Zero-Shot Learning via Synthesized Examples",
    "volume": "main",
    "abstract": "We present a generative framework for generalized zero-shot learning where the training and test classes are not necessarily disjoint. Built upon a variational autoencoder based architecture, consisting of a probabilistic encoder and a probabilistic emph{conditional} decoder, our model can generate novel exemplars from seen/unseen classes, given their respective class attributes. These exemplars can subsequently be used to train any off-the-shelf classification model. One of the key aspects of our encoder-decoder architecture is a feedback-driven mechanism in which a discriminator (a multivariate regressor) learns to map the generated exemplars to the corresponding class attribute vectors, leading to an improved generator. Our model's ability to generate and leverage examples from unseen classes to train the classification model naturally helps to mitigate the bias towards predicting seen classes in generalized zero-shot learning settings. Through a comprehensive set of experiments, we show that our model outperforms several state-of-the-art methods, on several benchmark datasets, for both standard as well as generalized zero-shot learning",
    "checked": true,
    "id": "90259de5c5884f87a77b1c0252fa0b144cf037a9",
    "semantic_title": "generalized zero-shot learning via synthesized examples",
    "citation_count": 452,
    "authors": [
      "Vinay Kumar Verma",
      "Gundeep Arora",
      "Ashish Mishra",
      "Piyush Rai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Partially_Shared_Multi-Task_CVPR_2018_paper.html": {
    "title": "Partially Shared Multi-Task Convolutional Neural Network With Local Constraint for Face Attribute Learning",
    "volume": "main",
    "abstract": "In this paper, we study the face attribute learning problem by considering the identity information and attribute relationships simultaneously. In particular, we first introduce a Partially Shared Multi-task Convolutional Neural Network (PS-MCNN), in which four Task Specific Networks (TSNets) and one Shared Network (SNet) are connected by Partially Shared (PS) structures to learn better shared and task specific representations. To utilize identity information to further boost the performance, we introduce a local learning constraint which minimizes the difference between the representations of each sample and its local geometric neighbours with the same identity. Consequently, we present a local constraint regularized multi-task network, called Partially Shared Multi-task Convolutional Neural Network with Local Constraint (PS-MCNN-LC), where PS structure and local constraint are integrated together to help the framework learn better attribute representations. The experimental results on CelebA and LFWA demonstrate the promise of the proposed methods",
    "checked": true,
    "id": "731d33256d42dc4146019954c46c566aa666d7f2",
    "semantic_title": "partially shared multi-task convolutional neural network with local constraint for face attribute learning",
    "citation_count": 97,
    "authors": [
      "Jiajiong Cao",
      "Yingming Li",
      "Zhongfei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Faraone_SYQ_Learning_Symmetric_CVPR_2018_paper.html": {
    "title": "SYQ: Learning Symmetric Quantization for Efficient Deep Neural Networks",
    "volume": "main",
    "abstract": "Inference for state-of-the-art deep neural networks is computationally expensive, making them difficult to deploy on constrained hardware environments. An efficient way to reduce this complexity is to quantize the weight parameters and/or activations during training by approximating their distributions with a limited entry codebook. For very low-precisions, such as binary or ternary networks with 1-8-bit activations, the information loss from quantization leads to significant accuracy degradation due to large gradient mismatches between the forward and backward functions. In this paper, we introduce a quantization method to reduce this loss by learning a symmetric codebook for particular weight subgroups. These subgroups are determined based on their locality in the weight matrix, such that the hardware simplicity of the low-precision representations is preserved. Empirically, we show that symmetric quantization can substantially improve accuracy for networks with extremely low-precision weights and activations. We also demonstrate that this representation imposes minimal or no hardware implications to more coarse-grained approaches. Source code is available at https://www.github.com/julianfaraone/SYQ",
    "checked": true,
    "id": "bc980e4e4b23214cbf191e2eb615aa10d5a369e6",
    "semantic_title": "syq: learning symmetric quantization for efficient deep neural networks",
    "citation_count": 133,
    "authors": [
      "Julian Faraone",
      "Nicholas Fraser",
      "Michaela Blott",
      "Philip H.W. Leong"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bernard_DS_Tighter_Lifting-Free_CVPR_2018_paper.html": {
    "title": "DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching Problems",
    "volume": "main",
    "abstract": "In this work we study convex relaxations of quadratic optimisation problems over permutation matrices. While existing semidefinite programming approaches can achieve remarkably tight relaxations, they have the strong disadvantage that they lift the original n^2-dimensional variable to an n^4-dimensional variable, which limits their practical applicability. In contrast, here we present a lifting-free convex relaxation that is provably at least as tight as existing (lifting-free) convex relaxations. We demonstrate experimentally that our approach is superior to existing convex and non-convex methods for various problems, including image arrangement and multi-graph matching",
    "checked": true,
    "id": "e2e44adcfb8dfbe74f504c557b314f4aa70a68e4",
    "semantic_title": "ds*: tighter lifting-free convex relaxations for quadratic matching problems",
    "citation_count": 43,
    "authors": [
      "Florian Bernard",
      "Christian Theobalt",
      "Michael Moeller"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.html": {
    "title": "Deep Mutual Learning",
    "volume": "main",
    "abstract": "Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy. Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on both category and instance recognition tasks. Surprisingly, it is revealed that no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher",
    "checked": true,
    "id": "f06a12928307e17b1aff2b9f4a6c11791f19b6a7",
    "semantic_title": "deep mutual learning",
    "citation_count": 1651,
    "authors": [
      "Ying Zhang",
      "Tao Xiang",
      "Timothy M. Hospedales",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Coupled_End-to-End_Transfer_CVPR_2018_paper.html": {
    "title": "Coupled End-to-End Transfer Learning With Generalized Fisher Information",
    "volume": "main",
    "abstract": "In transfer learning, one seeks to transfer related information from source tasks with sufficient data to help with the learning of target task with only limited data. In this paper, we propose a novel Coupled End-to-end Transfer Learning (CETL) framework, which mainly consists of two convolutional neural networks (source and target) that connect to a shared decoder. A novel loss function, the coupled loss, is used for CETL training. From a theoretical perspective, we demonstrate the rationale of the coupled loss by establishing a learning bound for CETL. Moreover, we introduce the generalized Fisher information to improve multi-task optimization in CETL. From a practical aspect, CETL provides a unified and highly flexible solution for various learning tasks such as domain adaption and knowledge distillation. Empirical result shows the superior performance of CETL on cross-domain and cross-task image classification",
    "checked": true,
    "id": "d90b3deea042b4ce4948ec52f4eb03777ca3494f",
    "semantic_title": "coupled end-to-end transfer learning with generalized fisher information",
    "citation_count": 27,
    "authors": [
      "Shixing Chen",
      "Caojin Zhang",
      "Ming Dong"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Rozantsev_Residual_Parameter_Transfer_CVPR_2018_paper.html": {
    "title": "Residual Parameter Transfer for Deep Domain Adaptation",
    "volume": "main",
    "abstract": "The goal of Deep Domain Adaptation is to make it possible to use Deep Nets trained in one domain where there is enough annotated training data in another where there is little or none. Most current approaches have focused on learning feature representations that are invariant to the changes that occur when going from one domain to the other, which means using the same network parameters in both domains. While some recent algorithms explicitly model the changes by adapting the network parameters, they either severely restrict the possible domain changes, or significantly increase the number of model parameters. By contrast, we introduce a network architecture that includes auxiliary residual networks, which we train to predict the parameters in the domain with little annotated data from those in the other one. This architecture enables us to flexibly preserve the similarities between domains where they exist and model the differences when necessary. We demonstrate that our approach yields higher accuracy than state-of-the-art methods without undue complexity",
    "checked": true,
    "id": "fba2e67337e3636459666555559f77a948992740",
    "semantic_title": "residual parameter transfer for deep domain adaptation",
    "citation_count": 56,
    "authors": [
      "Artem Rozantsev",
      "Mathieu Salzmann",
      "Pascal Fua"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_High-Order_Tensor_Regularization_CVPR_2018_paper.html": {
    "title": "High-Order Tensor Regularization With Application to Attribute Ranking",
    "volume": "main",
    "abstract": "When learning functions on manifolds, we can improve performance by regularizing with respect to the intrinsic manifold geometry rather than the ambient space. However, when regularizing tensor learning, calculating the derivatives along this intrinsic geometry is not possible, and so existing approaches are limited to regularizing in Euclidean space. Our new method for intrinsically regularizing and learning tensors on Riemannian manifolds introduces a surrogate object to encapsulate the geometric characteristic of the tensor. Regularizing this instead allows us to learn non-symmetric and high-order tensors. We apply our approach to the relative attributes problem, and we demonstrate that explicitly regularizing high-order relationships between pairs of data points improves performance",
    "checked": true,
    "id": "a2921cb7832852df86ee851b19ed3d5576ca9bde",
    "semantic_title": "high-order tensor regularization with application to attribute ranking",
    "citation_count": 0,
    "authors": [
      "Kwang In Kim",
      "Juhyun Park",
      "James Tompkin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Senocak_Learning_to_Localize_CVPR_2018_paper.html": {
    "title": "Learning to Localize Sound Source in Visual Scenes",
    "volume": "main",
    "abstract": "Visual events are usually accompanied by sounds in our daily lives. We pose the question: Can the machine learn the correspondence between visual scene and the sound, and localize the sound source only by observing sound and visual scene pairs like human? In this paper, we propose a novel unsupervised algorithm to address the problem of localizing the sound source in visual scenes. A two-stream network structure which handles each modality, with attention mechanism is developed for sound source localization. Moreover, although our network is formulated within the unsupervised learning framework, it can be extended to a unified architecture with a simple modification for the supervised and semi-supervised learning settings as well. Meanwhile, a new sound source dataset is developed for performance evaluation. Our empirical evaluation shows that the unsupervised method eventually go through false conclusion in some cases. We show that even with a few supervision, i.e., semi-supervised setup, false conclusion is able to be corrected effectively",
    "checked": true,
    "id": "b91d738cd1f5d550c5b27f328e55308a0a73b2d2",
    "semantic_title": "learning to localize sound source in visual scenes",
    "citation_count": 344,
    "authors": [
      "Arda Senocak",
      "Tae-Hyun Oh",
      "Junsik Kim",
      "Ming-Hsuan Yang",
      "In So Kweon"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.html": {
    "title": "Dynamic Few-Shot Visual Learning Without Forgetting",
    "volume": "main",
    "abstract": "The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on \"unseen\" categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick where we also achieve state-of-the-art results",
    "checked": true,
    "id": "a40f97770296c7fca2e5361cbceba3f4aae399e0",
    "semantic_title": "dynamic few-shot visual learning without forgetting",
    "citation_count": 1130,
    "authors": [
      "Spyros Gidaris",
      "Nikos Komodakis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Two-Step_Quantization_for_CVPR_2018_paper.html": {
    "title": "Two-Step Quantization for Low-Bit Neural Networks",
    "volume": "main",
    "abstract": "Every bit matters in the hardware design of quantized neural networks. However, extremely-low-bit representation usually causes large accuracy drop. Thus, how to train extremely-low-bit neural networks with high accuracy is of central importance. Most existing network quantization approaches learn transformations (low-bit weights) as well as encodings (low-bit activations) simultaneously. This tight coupling makes the optimization problem difficult, and thus prevents the network from learning optimal representations. In this paper, we propose a simple yet effective Two-Step Quantization (TSQ) framework, by decomposing the network quantization problem into two steps: code learning and transformation function learning based on the learned codes. For the first step, we propose the sparse quantization method for code learning. The second step can be formulated as a non-linear least square regression problem with low-bit constraints, which can be solved efficiently in an iterative manner. Extensive experiments on CIFAR-10 and ILSVRC-12 datasets demonstrate that the proposed TSQ is effective and outperforms the state-of-the-art by a large margin. Especially, for 2-bit activation and ternary weight quantization of AlexNet, the accuracy of our TSQ drops only about 0.5 points compared with the full-precision counterpart, outperforming current state-of-the-art by more than 5 points",
    "checked": true,
    "id": "5cd92ddddf3b377ad7d3b0fb718d8b32eb806618",
    "semantic_title": "two-step quantization for low-bit neural networks",
    "citation_count": 124,
    "authors": [
      "Peisong Wang",
      "Qinghao Hu",
      "Yifan Zhang",
      "Chunjie Zhang",
      "Yang Liu",
      "Jian Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Johnston_Improved_Lossy_Image_CVPR_2018_paper.html": {
    "title": "Improved Lossy Image Compression With Priming and Spatially Adaptive Bit Rates for Recurrent Networks",
    "volume": "main",
    "abstract": "We propose a method for lossy image compression based on recurrent, convolutional neural networks that outper- forms BPG (4:2:0), WebP, JPEG2000, and JPEG as mea- sured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result us- ing a single model. First, we modify the recurrent architec- ture to improve spatial diffusion, which allows the network to more effectively capture and propagate image informa- tion through the network's hidden state. Second, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efficiently use the limited num- ber of bits to encode visually complex image regions. Fi- nally, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to sev- eral metrics. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well as recently published methods based on deep neural networks",
    "checked": true,
    "id": "b80469cfabe9f4d51b7d004eb4377a65364f906a",
    "semantic_title": "improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks",
    "citation_count": 373,
    "authors": [
      "Nick Johnston",
      "Damien Vincent",
      "David Minnen",
      "Michele Covell",
      "Saurabh Singh",
      "Troy Chinen",
      "Sung Jin Hwang",
      "Joel Shor",
      "George Toderici"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mentzer_Conditional_Probability_Models_CVPR_2018_paper.html": {
    "title": "Conditional Probability Models for Deep Image Compression",
    "volume": "main",
    "abstract": "Deep Neural Networks trained as image auto-encoders have recently emerged as a promising direction for advancing the state-of-the-art in image compression. The key challenge in learning such networks is twofold: To deal with quantization, and to control the trade-off between reconstruction error (distortion) and entropy (rate) of the latent image representation. In this paper, we focus on the latter challenge and propose a new technique to navigate the rate-distortion trade-off for an image compression auto-encoder. The main idea is to directly model the entropy of the latent representation by using a context model: A 3D-CNN which learns a conditional probability model of the latent distribution of the auto-encoder. During training, the auto-encoder makes use of the context model to estimate the entropy of its representation, and the context model is concurrently updated to learn the dependencies between the symbols in the latent representation. Our experiments show that this approach, when measured in MS-SSIM, yields a state-of-the-art image compression system based on a simple convolutional auto-encoder",
    "checked": true,
    "id": "c5f7039c922221fb44c08a9f39a2ed5d4e8627f4",
    "semantic_title": "conditional probability models for deep image compression",
    "citation_count": 487,
    "authors": [
      "Fabian Mentzer",
      "Eirikur Agustsson",
      "Michael Tschannen",
      "Radu Timofte",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Detlefsen_Deep_Diffeomorphic_Transformer_CVPR_2018_paper.html": {
    "title": "Deep Diffeomorphic Transformer Networks",
    "volume": "main",
    "abstract": "Spatial Transformer layers allow neural networks, at least in principle, to be invariant to large spatial transformations in image data. The model has, however, seen limited uptake as most practical implementations support only transformations that are too restricted, e.g. affine or homographic maps, and/or destructive maps, such as thin plate splines. We investigate the use of ﬂexible diffeomorphic image transformations within such networks and demonstrate that significant performance gains can be attained over currently-used models. The learned transformations are found to be both simple and intuitive, thereby providing insights into individual problem domains. With the proposed framework, a standard convolutional neural network matches state-of-the-art results on face veriﬁcation with only two extra lines of simple TensorFlow code",
    "checked": true,
    "id": "d071cd68361415a8d38e4de8b5f6f6bca5f5f94c",
    "semantic_title": "deep diffeomorphic transformer networks",
    "citation_count": 46,
    "authors": [
      "Nicki Skafte Detlefsen",
      "Oren Freifeld",
      "Søren Hauberg"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Berman_The_LovaSz-Softmax_Loss_CVPR_2018_paper.html": {
    "title": "The Lovász-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks",
    "volume": "main",
    "abstract": "The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lovász extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures",
    "checked": true,
    "id": "c7afd747b5c6b77dc22eaa87a8b22888243842b6",
    "semantic_title": "the lovasz-softmax loss: a tractable surrogate for the optimization of the intersection-over-union measure in neural networks",
    "citation_count": 775,
    "authors": [
      "Maxim Berman",
      "Amal Rannen Triki",
      "Matthew B. Blaschko"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.html": {
    "title": "Generative Adversarial Perturbations",
    "volume": "main",
    "abstract": "In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time",
    "checked": true,
    "id": "e8da4ff1519011ed018202bb96dee4b611f5d842",
    "semantic_title": "generative adversarial perturbations",
    "citation_count": 355,
    "authors": [
      "Omid Poursaeed",
      "Isay Katsman",
      "Bicheng Gao",
      "Serge Belongie"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Learning_Strict_Identity_CVPR_2018_paper.html": {
    "title": "Learning Strict Identity Mappings in Deep Residual Networks",
    "volume": "main",
    "abstract": "A family of super deep networks, referred to as residual networks or ResNet~cite{he2016deep}, achieved record-beating performance in various visual tasks such as image recognition, object detection, and semantic segmentation. The ability to train very deep networks naturally pushed the researchers to use enormous resources to achieve the best performance. Consequently, in many applications super deep residual networks were employed for just a marginal improvement in performance. In this paper, we propose $epsilon$-ResNet that allows us to automatically discard redundant layers, which produces responses that are smaller than a threshold $epsilon$, without any loss in performance. The $epsilon$-ResNet architecture can be achieved using a few additional rectified linear units in the original ResNet. Our method does not use any additional variables nor numerous trials like other hyper-parameter optimization techniques. The layer selection is achieved using a single training process and the evaluation is performed on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. In some instances, we achieve about 80% reduction in the number of parameters",
    "checked": true,
    "id": "9341380e1a60aa6d3fbe079096229abb9cb2dca7",
    "semantic_title": "learning strict identity mappings in deep residual networks",
    "citation_count": 31,
    "authors": [
      "Xin Yu",
      "Zhiding Yu",
      "Srikumar Ramalingam"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kanbak_Geometric_Robustness_of_CVPR_2018_paper.html": {
    "title": "Geometric Robustness of Deep Networks: Analysis and Improvement",
    "volume": "main",
    "abstract": "Deep convolutional neural networks have been shown to be vulnerable to arbitrary geometric transformations. However, there is no systematic method to measure the invariance properties of deep networks to such transformations. We propose ManiFool as a simple yet scalable algorithm to measure the invariance of deep networks. In particular, our algorithm measures the robustness of deep networks to geometric transformations in a worst-case regime as they can be problematic for sensitive applications. Our extensive experimental results show that ManiFool can be used to measure the invariance of fairly complex networks on high dimensional datasets and these values can be used for analyzing the reasons for it. Furthermore, we build on ManiFool to propose a new adversarial training scheme and we show its effectiveness on improving the invariance properties of deep neural networks",
    "checked": true,
    "id": "47eb8d7ea4f7c209040ddd82e264edf3945df6cb",
    "semantic_title": "geometric robustness of deep networks: analysis and improvement",
    "citation_count": 131,
    "authors": [
      "Can Kanbak",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Pascal Frossard"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_View_Extrapolation_of_CVPR_2018_paper.html": {
    "title": "View Extrapolation of Human Body From a Single Image",
    "volume": "main",
    "abstract": "We study how to synthesize novel views of human body from a single image. Though recent deep learning based methods work well for rigid objects, they often fail on objects with large articulation, like human bodies. The core step of existing methods is to fit a map from the observable views to novel views by CNNs; however, the rich articulation modes of human body make it rather challenging for CNNs to memorize and interpolate the data well. To address the problem, we propose a novel deep learning based pipeline that explicitly estimates and leverages the geometry of the underlying human body. Our new pipeline is a composition of a shape estimation network and an image generation network, and at the interface a perspective transformation is applied to generate a forward flow for pixel value transportation. Our design is able to factor out the space of data variation and makes learning at each step much easier. Empirically, we show that the performance for pose-varying objects can be improved dramatically. Our method can also be applied on real data captured by 3D sensors, and the flow generated by our methods can be used for generating high quality results in higher resolution",
    "checked": true,
    "id": "32df435c9dfbba7452a491d24eaf3be2f6fd4c49",
    "semantic_title": "view extrapolation of human body from a single image",
    "citation_count": 51,
    "authors": [
      "Hao Zhu",
      "Hao Su",
      "Peng Wang",
      "Xun Cao",
      "Ruigang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Roy_Geometry_Aware_Constrained_CVPR_2018_paper.html": {
    "title": "Geometry Aware Constrained Optimization Techniques for Deep Learning",
    "volume": "main",
    "abstract": "In this paper, we generalize the Stochastic Gradient Descent (SGD) and RMSProp algorithms to the setting of Riemannian optimization. SGD is a popular method for large scale optimization. In particular, it is widely used to train the weights of Deep Neural Networks. However, gradients computed using standard SGD can have large variance, which is detrimental for the convergence rate of the algorithm. Other methods such as RMSProp and ADAM address this issue. Nevertheless, these methods cannot be directly applied to constrained optimization problems. In this paper, we extend some popular optimization algorithm to the Riemannian (constrained) setting. We substantiate our proposed extensions with a range of relevant problems in machine learning such as incremental Principal Component Analysis, computating the Riemannian centroids of SPD matrices, and Deep Metric Learning. We achieve competitive results against the state of the art for fine-grained object recognition datasets",
    "checked": true,
    "id": "6ca10c3af356f76ed37ecce7ec25c7ba79e9cbfb",
    "semantic_title": "geometry aware constrained optimization techniques for deep learning",
    "citation_count": 65,
    "authors": [
      "Soumava Kumar Roy",
      "Zakaria Mhammedi",
      "Mehrtash Harandi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper.html": {
    "title": "PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition",
    "volume": "main",
    "abstract": "Unlike its image based counterpart, point cloud based retrieval for place recognition has remained as an unexplored and unsolved problem. This is largely due to the difficulty in extracting local feature descriptors from a point cloud that can subsequently be encoded into a global descriptor for the retrieval task. In this paper, we propose the PointNetVLAD where we leverage on the recent success of deep networks to solve point cloud based retrieval for place recognition. Specifically, our PointNetVLAD is a combination/modification of the existing PointNet and NetVLAD, which allows end-to-end training and inference to extract the global descriptor from a given 3D point cloud. Furthermore, we propose the \"lazy triplet and quadruplet\" loss functions that can achieve more discriminative and generalizable global descriptors to tackle the retrieval task. We create benchmark datasets for point cloud based retrieval for place recognition, and the experimental results on these datasets show the feasibility of our PointNetVLAD",
    "checked": true,
    "id": "e69fa2ec8a83fd256f7e5843dc31c125d90360dd",
    "semantic_title": "pointnetvlad: deep point cloud based retrieval for large-scale place recognition",
    "citation_count": 550,
    "authors": [
      "Mikaela Angelina Uy",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_An_Efficient_and_CVPR_2018_paper.html": {
    "title": "An Efficient and Provable Approach for Mixture Proportion Estimation Using Linear Independence Assumption",
    "volume": "main",
    "abstract": "In this paper, we study the mixture proportion estimation (MPE) problem in a new setting: given samples from the mixture and the component distributions, we identify the proportions of the components in the mixture distribution. To address this problem, we make use of a linear independence assumption, i.e., the component distributions are independent from each other, which is much weaker than assumptions exploited in the previous MPE methods. Based on this assumption, we propose a method (1) that uniquely identifies the mixture proportions, (2) whose output provably converges to the optimal solution, and (3) that is computationally efficient. We show the superiority of the proposed method over the state-of-the-art methods in two applications including learning with label noise and semi-supervised learning on both synthetic and real-world datasets",
    "checked": true,
    "id": "67f7033265a9e6babdbe28472634270297d2eb46",
    "semantic_title": "an efficient and provable approach for mixture proportion estimation using linear independence assumption",
    "citation_count": 53,
    "authors": [
      "Xiyu Yu",
      "Tongliang Liu",
      "Mingming Gong",
      "Kayhan Batmanghelich",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.html": {
    "title": "VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection",
    "volume": "main",
    "abstract": "Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR",
    "checked": true,
    "id": "80f5ee8578ee76e2c17824f211762ffec7e029d4",
    "semantic_title": "voxelnet: end-to-end learning for point cloud based 3d object detection",
    "citation_count": 3719,
    "authors": [
      "Yin Zhou",
      "Oncel Tuzel"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Murez_Image_to_Image_CVPR_2018_paper.html": {
    "title": "Image to Image Translation for Domain Adaptation",
    "volume": "main",
    "abstract": "We propose a general framework for unsupervised domain adaptation, which allows deep neural networks trained on a source domain to be tested on a different target domain without requiring any training annotations in the target domain. This is achieved by adding extra networks and losses that help regularize the features extracted by the backbone encoder network. To this end we propose the novel use of the recently proposed unpaired image-to-image translation framework to constrain the features extracted by the encoder network. Specifically, we require that the features extracted are able to reconstruct the images in both domains. In addition we require that the distribution of features extracted from images in the two domains are indistinguishable. Many recent works can be seen as specific cases of our general framework. We apply our method for domain adaptation between MNIST, USPS, and SVHN datasets, and Amazon, Webcam and DSLR Office datasets in classification tasks, and also between GTA5 and Cityscapes datasets for a segmentation task. We demonstrate state of the art performance on each of these datasets",
    "checked": true,
    "id": "243e77a43a4045735415c12b4f9d9d3410f56eea",
    "semantic_title": "image to image translation for domain adaptation",
    "citation_count": 529,
    "authors": [
      "Zak Murez",
      "Soheil Kolouri",
      "David Kriegman",
      "Ravi Ramamoorthi",
      "Kyungnam Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html": {
    "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
    "volume": "main",
    "abstract": "In this paper we describe a new mobile architecture, mbox{MobileNetV2}, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call mbox{SSDLite}. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of mbox{DeepLabv3} which we call Mobile mbox{DeepLabv3}. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on mbox{ImageNet}~cite{Russakovsky:2015:ILS:2846547.2846559} classification, COCO object detection cite{COCO}, VOC image segmentation cite{PASCAL}. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters",
    "checked": true,
    "id": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4",
    "semantic_title": "mobilenetv2: inverted residuals and linear bottlenecks",
    "citation_count": 19262,
    "authors": [
      "Mark Sandler",
      "Andrew Howard",
      "Menglong Zhu",
      "Andrey Zhmoginov",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Niu_Im2Struct_Recovering_3D_CVPR_2018_paper.html": {
    "title": "Im2Struct: Recovering 3D Shape Structure From a Single RGB Image",
    "volume": "main",
    "abstract": "We propose to recover 3D shape structures from single RGB images, where structure refers to shape parts represented by cuboids and part relations encompassing connectivity and symmetry. Given a single 2D image with an object depicted, our goal is automatically recover a cuboid structure of the object parts as well as their mutual relations. We develop a convolutional-recursive auto-encoder comprised of structure parsing of a 2D image followed by structure recovering of a cuboid hierarchy. The encoder is achieved by a multi-scale convolutional network trained with the task of shape contour estimation, thereby learning to discern object structures in various forms and scales. The decoder fuses the features of the structure parsing network and the original image, and recursively decodes a hierarchy of cuboids. Since the decoder network is learned to recover part relations including connectivity and symmetry explicitly, the plausibility and generality of part structure recovery can be ensured. The two networks are jointly trained using the training data of contour-mask and cuboid-structure pairs. Such pairs are generated by rendering stock 3D CAD models coming with part segmentation. Our method achieves unprecedentedly faithful and detailed recovery of diverse 3D part structures from single-view 2D images. We demonstrate two applications of our method including structure-guided completion of 3D volumes reconstructed from single-view images and structure-aware interactive editing of 2D images",
    "checked": true,
    "id": "20fdb80044b03861bcd8445c2c0d3b5491d3efdb",
    "semantic_title": "im2struct: recovering 3d shape structure from a single rgb image",
    "citation_count": 128,
    "authors": [
      "Chengjie Niu",
      "Jun Li",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Schilling_Trust_Your_Model_CVPR_2018_paper.html": {
    "title": "Trust Your Model: Light Field Depth Estimation With Inline Occlusion Handling",
    "volume": "main",
    "abstract": "We address the problem of depth estimation from light-field images. Our main contribution is a new way to handle occlusions which improves general accuracy and quality of object borders. In contrast to all prior work we work with a model which directly incorporates both depth and occlusion, using a local optimization scheme based on the PatchMatch algorithm. The key benefit of this joint approach is that we utilize all available data, and not erroneously discard valuable information in pre-processing steps. We see the benefit of our approach not only at improved object boundaries, but also at smooth surface reconstruction, where we outperform even methods which focus on good surface regularization. We have evaluated our method on a public light-field dataset, where we achieve state-of-the-art results in nine out of twelve error metrics, with a close tie for the remaining three",
    "checked": true,
    "id": "55db258ebce8615751d3f30a98658aafbd4d0b94",
    "semantic_title": "trust your model: light field depth estimation with inline occlusion handling",
    "citation_count": 56,
    "authors": [
      "Hendrik Schilling",
      "Maximilian Diebold",
      "Carsten Rother",
      "Bernd Jähne"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhuang_Baseline_Desensitizing_in_CVPR_2018_paper.html": {
    "title": "Baseline Desensitizing in Translation Averaging",
    "volume": "main",
    "abstract": "Many existing translation averaging algorithms are either sensitive to disparate camera baselines and have to rely on extensive preprocessing to improve the observed Epipolar Geometry graph, or if they are robust against disparate camera baselines, require complicated optimization to minimize the highly nonlinear angular error objective. In this paper, we carefully design a simple yet effective bilinear objective function, introducing a variable to perform the requisite normalization. The objective function enjoys the baseline-insensitive property of the angular error and yet is amenable to simple and efficient optimization by block coordinate descent, with good empirical performance. A rotation-assisted Iterative Reweighted Least Squares scheme is further put forth to help deal with outliers. We also contribute towards a better understanding of the behavior of two recent convex algorithms, LUD and Shapefit/kick, clarifying the underlying subtle difference that leads to the performance gap. Finally, we demonstrate that our algorithm achieves overall superior accuracies in benchmark dataset compared to state-of-the-art methods, and is also several times faster",
    "checked": true,
    "id": "a7bb0b60500e6bdbaa11b16f5fed9c4de5fc075e",
    "semantic_title": "baseline desensitizing in translation averaging",
    "citation_count": 39,
    "authors": [
      "Bingbing Zhuang",
      "Loong-Fah Cheong",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Mining_Point_Cloud_CVPR_2018_paper.html": {
    "title": "Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling",
    "volume": "main",
    "abstract": "Unlike on images, semantic learning on 3D point clouds using a deep network is challenging due to the naturally unordered data structure. Among existing works, PointNet has achieved promising results by directly learning on point sets. However, it does not take full advantage of a point's local neighborhood that contains fine-grained structural information which turns out to be helpful towards better semantic learning. In this regard, we present two new operations to improve PointNet with a more efficient exploitation of local structures. The first one focuses on local 3D geometric structures. In analogy to a convolution kernel for images, we define a point-set kernel as a set of learnable 3D points that jointly respond to a set of neighboring data points according to their geometric affinities measured by kernel correlation, adapted from a similar technique for point cloud registration. The second one exploits local high-dimensional feature structures by recursive feature aggregation on a nearest-neighbor-graph computed from 3D positions. Experiments show that our network can efficiently capture local information and robustly achieve better performances on major datasets. Our code is available at http://www.merl.com/research/license#KCNet",
    "checked": true,
    "id": "5eb4a5ac7faa2fc75e1739b45c511ba3fab5bb56",
    "semantic_title": "mining point cloud local structures by kernel correlation and graph pooling",
    "citation_count": 494,
    "authors": [
      "Yiru Shen",
      "Chen Feng",
      "Yaoqing Yang",
      "Dong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.html": {
    "title": "Large-Scale Point Cloud Semantic Segmentation With Superpoint Graphs",
    "volume": "main",
    "abstract": "We propose a novel deep learning-based framework to tackle the challenge of semantic segmentation of large-scale point clouds of millions of points. We argue that the organization of 3D point clouds can be efficiently captured by a structure called superpoint graph (SPG), derived from a partition of the scanned scene into geometrically homogeneous elements. SPGs offer a compact yet rich representation of contextual relationships between object parts, which is then exploited by a graph convolutional network. Our framework sets a new state of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for both Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the S3DIS dataset)",
    "checked": true,
    "id": "7a2e527b6d51071a54aac7a8bdb56ca735a1f78b",
    "semantic_title": "large-scale point cloud semantic segmentation with superpoint graphs",
    "citation_count": 1253,
    "authors": [
      "Loic Landrieu",
      "Martin Simonovsky"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Very_Large-Scale_Global_CVPR_2018_paper.html": {
    "title": "Very Large-Scale Global SfM by Distributed Motion Averaging",
    "volume": "main",
    "abstract": "Global Structure-from-Motion (SfM) techniques have demonstrated superior efficiency and accuracy than the conventional incremental approach in many recent studies. This work proposes a divide-and-conquer framework to solve very large global SfM at the scale of millions of images. Specifically, we first divide all images into multiple partitions that preserve strong data association for well posed and parallel local motion averaging. Then, we solve a global motion averaging that determines cameras at partition boundaries and a similarity transformation per partition to register all cameras in a single coordinate frame. Finally, local and global motion averaging are iterated until convergence. Since local camera poses are fixed during the global motion average, we can avoid caching the whole reconstruction in memory at once. This distributed framework significantly enhances the efficiency and robustness of large-scale motion averaging",
    "checked": true,
    "id": "ebb0ea4bf52bba3c232b416763fa95cf0693af48",
    "semantic_title": "very large-scale global sfm by distributed motion averaging",
    "citation_count": 140,
    "authors": [
      "Siyu Zhu",
      "Runze Zhang",
      "Lei Zhou",
      "Tianwei Shen",
      "Tian Fang",
      "Ping Tan",
      "Long Quan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper.html": {
    "title": "ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans",
    "volume": "main",
    "abstract": "We introduce ScanComplete, a novel data-driven approach for taking an incomplete 3D scan of a scene as input and predicting a complete 3D model along with per-voxel semantic labels. The key contribution of our method is its ability to handle large scenes with varying spatial extent, managing the cubic growth in data size as scene size increases. To this end, we devise a fully-convolutional generative 3D CNN model whose filter kernels are invariant to the overall scene size. The model can be trained on scene subvolumes but deployed on arbitrarily large scenes at test time. In addition, we propose a coarse-to-fine inference strategy in order to produce high-resolution output while also leveraging large input context sizes. In an extensive series of experiments, we carefully evaluate different model design choices, considering both deterministic and probabilistic models for completion and semantic inference. Our results show that we outperform other methods not only in the size of the environments handled and processing efficiency, but also with regard to completion quality and semantic segmentation performance by a significant margin",
    "checked": true,
    "id": "7615f13cee4ec0248e88cec2624e451b90a51574",
    "semantic_title": "scancomplete: large-scale scene completion and semantic segmentation for 3d scans",
    "citation_count": 296,
    "authors": [
      "Angela Dai",
      "Daniel Ritchie",
      "Martin Bokeloh",
      "Scott Reed",
      "Jürgen Sturm",
      "Matthias Nießner"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lan_Solving_the_Perspective-2-Point_CVPR_2018_paper.html": {
    "title": "Solving the Perspective-2-Point Problem for Flying-Camera Photo Composition",
    "volume": "main",
    "abstract": "Drone-mounted flying cameras will revolutionize photo-taking. The user, instead of holding a camera in hand and manually searching for a viewpoint, will interact directly with image contents in the viewfinder through simple gestures, and the flying camera will achieve the desired viewpoint through the autonomous flying capability of the drone. This work studies the underlying viewpoint search problem for composing a photo with two objects of interest, a common situation in photo-taking. We model it as a Perspective-2-Point (P2P) problem, which is under-constrained to determine the six degrees-of-freedom camera pose uniquely. By incorporating the user's composition requirements and minimizing the camera's flying distance, we form a constrained nonlinear optimization problem and solve it in closed form. Experiments on synthetic data sets and on a real flying camera system indicate promising results",
    "checked": true,
    "id": "baa5b6b55db3fcde5e48bcc6cf93ddab063d0280",
    "semantic_title": "solving the perspective-2-point problem for flying-camera photo composition",
    "citation_count": 0,
    "authors": [
      "Ziquan Lan",
      "David Hsu",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yun_Reflection_Removal_for_CVPR_2018_paper.html": {
    "title": "Reflection Removal for Large-Scale 3D Point Clouds",
    "volume": "main",
    "abstract": "Large-scale 3D point clouds (LS3DPCs) captured by terrestrial LiDAR scanners often exhibit reflection artifacts by glasses, which degrade the performance of related computer vision techniques. In this paper, we propose an efficient reflection removal algorithm for LS3DPCs. We first partition the unit sphere into local surface patches which are then classified into the ordinary patches and the glass patches according to the number of echo pulses from emitted laser pulses. Then we estimate the glass region of dominant reflection artifacts by measuring the reliability. We also detect and remove the virtual points using the conditions of the reflection symmetry and the geometric similarity. We test the performance of the proposed algorithm on LS3DPCs capturing real-world outdoor scenes, and show that the proposed algorithm estimates valid glass regions faithfully and removes the virtual points caused by reflection artifacts successfully",
    "checked": true,
    "id": "22a21f6a02021dc57c71f636914d197312a18765",
    "semantic_title": "reflection removal for large-scale 3d point clouds",
    "citation_count": 16,
    "authors": [
      "Jae-Seong Yun",
      "Jae-Young Sim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xie_Attentional_ShapeContextNet_for_CVPR_2018_paper.html": {
    "title": "Attentional ShapeContextNet for Point Cloud Recognition",
    "volume": "main",
    "abstract": "We tackle the problem of point cloud recognition. Unlike previous approaches where a point cloud is either converted into a volume/image or represented independently in a permutation-invariant set, we develop a new representation by adopting the concept of shape context as the building block in our network design. The resulting model, called ShapeContextNet, consists of a hierarchy with modules not relying on a fixed grid while still enjoying properties similar to those in convolutional neural networks --- being able to capture and propagate the object part information. In addition, we find inspiration from self-attention based models to include a simple yet effective contextual modeling mechanism --- making the contextual region selection, the feature aggregation, and the feature transformation process fully automatic. ShapeContextNet is an end-to-end model that can be applied to the general point cloud classification and segmentation problems. We observe competitive results on a number of benchmark datasets",
    "checked": true,
    "id": "36777066966899fb48c1850d5776af97f1c81942",
    "semantic_title": "attentional shapecontextnet for point cloud recognition",
    "citation_count": 329,
    "authors": [
      "Saining Xie",
      "Sainan Liu",
      "Zeyu Chen",
      "Zhuowen Tu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Geometry-Aware_Deep_Network_CVPR_2018_paper.html": {
    "title": "Geometry-Aware Deep Network for Single-Image Novel View Synthesis",
    "volume": "main",
    "abstract": "This paper tackles the problem of novel view synthesis from a single image. In particular, we target real-world scenes with rich geometric structure, a challenging task due to the large appearance variations of such scenes and the lack of simple 3D models to represent them. Modern, learning-based approaches mostly focus on appearance to synthesize novel views and thus tend to generate predictions that are inconsistent with the underlying scene structure. By contrast, in this paper, we propose to exploit the 3D geometry of the scene to synthesize a novel view. Specifically, we approximate a real-world scene by a fixed number of planes, and learn to predict a set of homographies and their corresponding region masks to transform the input image into a novel view. To this end, we develop a new region-aware geometric transform network that performs these multiple tasks in a common framework. Our results on the outdoor KITTI and the indoor ScanNet datasets demonstrate the effectiveness of our network to generate high-quality synthetic views that respect the scene geometry, thus outperforming the state-of-the-art methods",
    "checked": true,
    "id": "6f2f111d098ee6bda1207d0d50d2ebcc2294dfe1",
    "semantic_title": "geometry-aware deep network for single-image novel view synthesis",
    "citation_count": 66,
    "authors": [
      "Miaomiao Liu",
      "Xuming He",
      "Mathieu Salzmann"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_InverseFaceNet_Deep_Monocular_CVPR_2018_paper.html": {
    "title": "InverseFaceNet: Deep Monocular Inverse Face Rendering",
    "volume": "main",
    "abstract": "We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input image. By estimating all parameters from just a single image, advanced editing possibilities on a single face image, such as appearance editing and relighting, become feasible in real time. Most previous learning-based face reconstruction approaches do not jointly recover all dimensions, or are severely limited in terms of visual quality. In contrast, we propose to recover high-quality facial pose, shape, expression, reflectance and illumination using a deep neural network that is trained using a large, synthetically created training corpus. Our approach builds on a novel loss function that measures model-space similarity directly in parameter space and significantly improves reconstruction accuracy.We further propose a self-supervised bootstrapping process in the network training loop, which iteratively updates the synthetic training corpus to better reflect the distribution of real-world imagery. We demonstrate that this strategy outperforms completely synthetically trained networks. Finally, we show high-quality reconstructions and compare our approach to several state-of-the-art approaches",
    "checked": true,
    "id": "0b43c6998617ed424f968bcbcc65b8b860c2c86b",
    "semantic_title": "inversefacenet: deep monocular inverse face rendering",
    "citation_count": 80,
    "authors": [
      "Hyeongwoo Kim",
      "Michael Zollhöfer",
      "Ayush Tewari",
      "Justus Thies",
      "Christian Richardt",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Sparse_Photometric_3D_CVPR_2018_paper.html": {
    "title": "Sparse Photometric 3D Face Reconstruction Guided by Morphable Models",
    "volume": "main",
    "abstract": "We present a novel 3D face reconstruction technique that leverages sparse photometric stereo (PS) and latest advances on face registration / modeling from a single image. We observe that 3D morphable faces approach provides a reasonable geometry proxy for light position calibration. Specifically, we develop a robust optimization technique that can calibrate per-pixel lighting direction and illumination at a very high precision without assuming uniform surface albedos. Next, we apply semantic segmentation on input images and the geometry proxy to refine hairy vs. bare skin regions using tailored filter. Experiments on synthetic and real data show that by using a very small set of images, our technique is able to reconstruct fine geometric details such as wrinkles, eyebrows, whelks, pores, etc, comparable to and sometimes surpassing movie quality productions",
    "checked": true,
    "id": "5da74a13840393c53d2d930bf6524771e96ca4d6",
    "semantic_title": "sparse photometric 3d face reconstruction guided by morphable models",
    "citation_count": 34,
    "authors": [
      "Xuan Cao",
      "Zhang Chen",
      "Anpei Chen",
      "Xin Chen",
      "Shiying Li",
      "Jingyi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fu_Texture_Mapping_for_CVPR_2018_paper.html": {
    "title": "Texture Mapping for 3D Reconstruction With RGB-D Sensor",
    "volume": "main",
    "abstract": "Acquiring realistic texture details for 3D models is important in 3D reconstruction. However, the existence of geometric errors, caused by noisy RGB-D sensor data, always makes the color images cannot be accurately aligned onto reconstructed 3D models. In this paper, we propose a global-to-local correction strategy to obtain more desired texture mapping results. Our algorithm first adaptively selects an optimal image for each face of the 3D model, which can effectively remove blurring and ghost artifacts produced by multiple image blending. We then adopt a non-rigid global-to-local correction step to reduce the seaming effect between textures. This can effectively compensate for the texture and the geometric misalignment caused by camera pose drift and geometric errors. We evaluate the proposed algorithm in a range of complex scenes and demonstrate its effective performance in generating seamless high fidelity textures for 3D models",
    "checked": true,
    "id": "fd8f3bc244999e003c799aba9e2bf642f570faa7",
    "semantic_title": "texture mapping for 3d reconstruction with rgb-d sensor",
    "citation_count": 78,
    "authors": [
      "Yanping Fu",
      "Qingan Yan",
      "Long Yang",
      "Jie Liao",
      "Chunxia Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Brachmann_Learning_Less_Is_CVPR_2018_paper.html": {
    "title": "Learning Less Is More - 6D Camera Localization via 3D Surface Regression",
    "volume": "main",
    "abstract": "Popular research areas like autonomous driving and augmented reality have renewed the interest in image-based camera localization. In this work, we address the task of predicting the 6D camera pose from a single RGB image in a given 3D environment. With the advent of neural networks, previous works have either learned the entire camera localization process, or multiple components of a camera localization pipeline. Our key contribution is to demonstrate and explain that learning a single component of this pipeline is sufficient. This component is a fully convolutional neural network for densely regressing so-called scene coordinates, defining the correspondence between the input image and the 3D scene space. The neural network is prepended to a new end-to-end trainable pipeline. Our system is efficient, highly accurate, robust in training, and exhibits outstanding generalization capabilities. It exceeds state-of-the-art consistently on indoor and outdoor datasets. Interestingly, our approach surpasses existing techniques even without utilizing a 3D model of the scene during training, since the network is able to discover 3D scene geometry automatically, solely from single-view constraints",
    "checked": true,
    "id": "6ca43ff71b3c0039e7feb6e0b9fc7cf5d779e535",
    "semantic_title": "learning less is more - 6d camera localization via 3d surface regression",
    "citation_count": 379,
    "authors": [
      "Eric Brachmann",
      "Carsten Rother"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Rad_Feature_Mapping_for_CVPR_2018_paper.html": {
    "title": "Feature Mapping for Learning Fast and Accurate 3D Pose Inference From Synthetic Images",
    "volume": "main",
    "abstract": "We propose a simple and efficient method for exploiting synthetic images when training a Deep Network to predict a 3D pose from an image. The ability of using synthetic images for training a Deep Network is extremely valuable as it is easy to create a virtually infinite training set made of such images, while capturing and annotating real images can be very cumbersome. However, synthetic images do not resemble real images exactly, and using them for training can result in suboptimal performance. It was recently shown that for exemplar-based approaches, it is possible to learn a mapping from the exemplar representations of real images to the exemplar representations of synthetic images. In this paper, we show that this approach is more general, and that a network can also be applied after the mapping to infer a 3D pose: At run-time, given a real image of the target object, we first compute the features for the image, map them to the feature space of synthetic images, and finally use the resulting features as input to another network which predicts the 3D pose. Since this network can be trained very effectively by using synthetic images, it performs very well in practice, and inference is faster and more accurate than with an exemplar-based approach. We demonstrate our approach on the LINEMOD dataset for 3D object pose estimation from color images, and the NYU dataset for 3D hand pose estimation from depth maps. We show that it allows us to outperform the state-of-the-art on both datasets",
    "checked": true,
    "id": "6cac0550309b3afc42966cb2ab51cfb1a4318b93",
    "semantic_title": "feature mapping for learning fast and accurate 3d pose inference from synthetic images",
    "citation_count": 131,
    "authors": [
      "Mahdi Rad",
      "Markus Oberweger",
      "Vincent Lepetit"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_Indoor_RGB-D_Compass_CVPR_2018_paper.html": {
    "title": "Indoor RGB-D Compass From a Single Line and Plane",
    "volume": "main",
    "abstract": "We propose a novel approach to estimate the three degrees of freedom (DoF) drift-free rotational motion of an RGB-D camera from only a single line and plane in the Manhattan world (MW). Previous approaches exploit the surface normal vectors and vanishing points to achieve accurate 3-DoF rotation estimation. However, they require multiple orthogonal planes or many consistent lines to be visible throughout the entire rotation estimation process; otherwise, these approaches fail. To overcome these limitations, we present a new method that estimates absolute camera orientation from only a single line and a single plane in RANSAC, which corresponds to the theoretical minimal sampling for 3-DoF rotation estimation. Once we find an initial rotation estimate, we refine the camera orientation by minimizing the average orthogonal distance from the endpoints of the lines parallel to the MW axes. We demonstrate the effectiveness of the proposed algorithm through an extensive evaluation on a variety of RGB-D datasets and compare with other state-of-the-art methods",
    "checked": true,
    "id": "b584c432e09b1b79aeaaaa0a8a6752f4a5169358",
    "semantic_title": "indoor rgb-d compass from a single line and plane",
    "citation_count": 13,
    "authors": [
      "Pyojin Kim",
      "Brian Coltin",
      "H. Jin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pumarola_Geometry-Aware_Network_for_CVPR_2018_paper.html": {
    "title": "Geometry-Aware Network for Non-Rigid Shape Prediction From a Single View",
    "volume": "main",
    "abstract": "We propose a method for predicting the 3D shape of a deformable surface from a single view. By contrast with previous approaches, we do not need a pre-registered template of the surface, and our method is robust to the lack of texture and partial occlusions. At the core of our approach is a geometry-aware deep architecture that tackles the problem as usually done in analytic solutions: first perform 2D detection of the mesh and then estimate a 3D shape that is geometrically consistent with the image. We train this architecture in an end-to-end manner using a large dataset of synthetic renderings of shapes under different levels of deformation, material properties, textures and lighting conditions. We evaluate our approach on a test split of this dataset and available real benchmarks, consistently improving state-of-the-art solutions with a significantly lower computational time",
    "checked": true,
    "id": "2e92be75cc8a6ab82b2c2ec244b47645b9c6df3b",
    "semantic_title": "geometry-aware network for non-rigid shape prediction from a single view",
    "citation_count": 59,
    "authors": [
      "Albert Pumarola",
      "Antonio Agudo",
      "Lorenzo Porzi",
      "Alberto Sanfeliu",
      "Vincent Lepetit",
      "Francesc Moreno-Noguer"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sadeghi_Sim2Real_Viewpoint_Invariant_CVPR_2018_paper.html": {
    "title": "Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control",
    "volume": "main",
    "abstract": "Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints. In robotics, this ability is referred to as visual servoing: moving a tool or end-point to a desired location using primarily visual feedback. In this paper, we propose learning viewpoint invariant visual servoing skills in a robot manipulation task. We train a deep recurrent controller that can automatically determine which actions move the end-effector of a robotic arm to a desired object. This problem is fundamentally ambiguous: under severe variation in viewpoint, it may be impossible to determine the actions in a single feedforward operation. Instead, our visual servoing approach uses its memory of past movements to understand how the actions affect the robot motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This ability is in stark contrast to previous visual servoing methods, which assume known dynamics or require a calibration phase. We learn our recurrent controller using simulated data, synthetic demonstrations and reinforcement learning. We then describe how the resulting model can be transferred to a real-world robot by disentangling perception from control and only adapting the visual layers. The adapted model can servo to previously unseen objects from novel viewpoints on a real-world Kuka IIWA robotic arm. For supplementary videos, see: href{https://www.youtube.com/watch?v=oLgM2Bnb7fo}{https://www.youtube.com/watch?v=oLgM2Bnb7fo}",
    "checked": true,
    "id": "d3604a416738f6e94ddd30b70b9f9303dfb5042e",
    "semantic_title": "sim2real viewpoint invariant visual servoing by recurrent control",
    "citation_count": 65,
    "authors": [
      "Fereshteh Sadeghi",
      "Alexander Toshev",
      "Eric Jang",
      "Sergey Levine"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_DocUNet_Document_Image_CVPR_2018_paper.html": {
    "title": "DocUNet: Document Image Unwarping via a Stacked U-Net",
    "volume": "main",
    "abstract": "Capturing document images is a common way for digitizing and recording physical documents due to the ubiquitousness of mobile cameras. To make text recognition easier, it is often desirable to digitally flatten a document image when the physical document sheet is folded or curved. In this paper, we develop the first learning-based method to achieve this goal. We propose a stacked U-Net with intermediate supervision to directly predict the forward mapping from a distorted image to its rectified version. Because large-scale real-world data with ground truth deformation is difficult to obtain, we create a synthetic dataset with approximately 100 thousand images by warping non-distorted document images. The network is trained on this dataset with various data augmentations to improve its generalization ability. We further create a comprehensive benchmark that covers various real-world conditions. We evaluate the proposed model quantitatively and qualitatively on the proposed benchmark, and compare it with previous non-learning-based methods",
    "checked": true,
    "id": "00fc2f71a3fc3e67d8cc96b9a345f6b223c64aa6",
    "semantic_title": "docunet: document image unwarping via a stacked u-net",
    "citation_count": 128,
    "authors": [
      "Ke Ma",
      "Zhixin Shu",
      "Xue Bai",
      "Jue Wang",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Urooj_Analysis_of_Hand_CVPR_2018_paper.html": {
    "title": "Analysis of Hand Segmentation in the Wild",
    "volume": "main",
    "abstract": "A large number of works in egocentric vision have concentrated on action and object recognition. Detection and segmentation of hands in first-person videos, however, has less been explored. For many applications in this domain, it is necessary to accurately segment not only hands of the camera wearer but also the hands of others with whom he is interacting. Here, we take an in-depth look at the hand segmentation problem. In the quest for robust hand segmentation methods, we evaluated the performance of the state of the art semantic segmentation methods, off the shelf and fine-tuned, on existing datasets. We fine-tune RefineNet, a leading semantic segmentation method, for hand segmentation and find that it does much better than the best contenders. Existing hand segmentation datasets are collected in the laboratory settings. To overcome this limitation, we contribute by collecting two new datasets: a) EgoYouTubeHands including egocentric videos containing hands in the wild, and b) HandOverFace to analyze the performance of our models in presence of similar appearance occlusions. We further explore whether conditional random fields can help refine generated hand segmentations. To demonstrate the benefit of accurate hand maps, we train a CNN for hand-based activity recognition and achieve higher accuracy when a CNN was trained using hand maps produced by the fine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset for fine-grained action recognition and show that an accuracy of 58.6% can be achieved by just looking at a single hand pose which is much better than the chance level (12.5%)",
    "checked": true,
    "id": "c1d285c54b18834d4458c30669c24efae0fa7e9c",
    "semantic_title": "analysis of hand segmentation in the wild",
    "citation_count": 88,
    "authors": [
      "Aisha Urooj",
      "Ali Borji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bastani_RoadTracer_Automatic_Extraction_CVPR_2018_paper.html": {
    "title": "RoadTracer: Automatic Extraction of Road Networks From Aerial Images",
    "volume": "main",
    "abstract": "Mapping road networks is currently both expensive and labor-intensive. High-resolution aerial imagery provides a promising avenue to automatically infer a road network. Prior work uses convolutional neural networks (CNNs) to detect which pixels belong to a road (segmentation), and then uses complex post-processing heuristics to infer graph connectivity. We show that these segmentation methods have high error rates because noisy CNN outputs are difficult to correct. We propose RoadTracer, a new method to automatically construct accurate road network maps from aerial images. RoadTracer uses an iterative search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN. We compare our approach with a segmentation method on fifteen cities, and find that at a 5% error rate, RoadTracer correctly captures 45% more junctions across these cities",
    "checked": true,
    "id": "82d66fa2666e3e8798085fdc0155e6b45b707f32",
    "semantic_title": "roadtracer: automatic extraction of road networks from aerial images",
    "citation_count": 299,
    "authors": [
      "Favyen Bastani",
      "Songtao He",
      "Sofiane Abbar",
      "Mohammad Alizadeh",
      "Hari Balakrishnan",
      "Sanjay Chawla",
      "Sam Madden",
      "David DeWitt"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Paul_Alternating-Stereo_VINS_Observability_CVPR_2018_paper.html": {
    "title": "Alternating-Stereo VINS: Observability Analysis and Performance Evaluation",
    "volume": "main",
    "abstract": "One approach to improve the accuracy and robustness of vision-aided inertial navigation systems (VINS) that employ low-cost inertial sensors, is to obtain scale information from stereoscopic vision. Processing images from two cameras, however, is computationally expensive and increases latency. To address this limitation, in this work, a novel two-camera alternating-stereo VINS is presented. Specifically, the proposed system triggers the left-right cameras in an alternating fashion, estimates the poses corresponding to the left camera only, and introduces a linear interpolation model for processing the alternating right camera measurements. Although not a regular stereo system, the alternating visual observations when employing the proposed interpolation scheme, still provide scale information, as shown by analyzing the observability properties of the vision-only corresponding system. Finally, the performance gain, of the proposed algorithm over its monocular and stereo counterparts is assessed using various datasets",
    "checked": true,
    "id": "8d5e28f143edf611f8d0622e1e3fa371436510ef",
    "semantic_title": "alternating-stereo vins: observability analysis and performance evaluation",
    "citation_count": 15,
    "authors": [
      "Mrinal K. Paul",
      "Stergios I. Roumeliotis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Rematas_Soccer_on_Your_CVPR_2018_paper.html": {
    "title": "Soccer on Your Tabletop",
    "volume": "main",
    "abstract": "We present a system that transforms a monocular video of a soccer game into a moving 3D reconstruction, in which the players and field can be rendered interactively with a 3D viewer or through an Augmented Reality device. At the heart of our paper is an approach to estimate the depth map of each player, using a CNN that is trained on 3D player data extracted from soccer video games. We compare with state of the art body pose and depth estimation techniques, and show results on both synthetic ground truth benchmarks, and real YouTube soccer footage",
    "checked": true,
    "id": "a2ec80f4f5fdf455c96018e4950936fa054b7d7b",
    "semantic_title": "soccer on your tabletop",
    "citation_count": 92,
    "authors": [
      "Konstantinos Rematas",
      "Ira Kemelmacher-Shlizerman",
      "Brian Curless",
      "Steve Seitz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shin_EPINET_A_Fully-Convolutional_CVPR_2018_paper.html": {
    "title": "EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth From Light Field Images",
    "volume": "main",
    "abstract": "Light field cameras capture both the spatial and the angular properties of light rays in space. Due to its property, one can compute the depth from light fields in uncontrolled lighting environments, which is a big advantage over active sensing devices. Depth computed from light fields can be used for many applications including 3D modelling and refocusing. However, light field images from hand-held cameras have very narrow baselines with noise, making the depth estimation difficult. Many approaches have been proposed to overcome these limitations for the light field depth estimation, but there is a clear trade-off between the accuracy and the speed in these methods. In this paper, we introduce a fast and accurate light field depth estimation method based on a fully-convolutional neural network. Our network is designed by considering the light field geometry and we also overcome the lack of training data by proposing light field specific data augmentation methods. We achieved the top rank in the HCI 4D Light Field Benchmark on most metrics, and we also demonstrate the effectiveness of the proposed method on real-world light-field images",
    "checked": true,
    "id": "4d0a31b0f9d1eb28473b09d3e0e01b6325bcb866",
    "semantic_title": "epinet: a fully-convolutional neural network using epipolar geometry for depth from light field images",
    "citation_count": 244,
    "authors": [
      "Changha Shin",
      "Hae-Gon Jeon",
      "Youngjin Yoon",
      "In So Kweon",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liang_A_Hybrid_l1-l0_CVPR_2018_paper.html": {
    "title": "A Hybrid l1-l0 Layer Decomposition Model for Tone Mapping",
    "volume": "main",
    "abstract": "Tone mapping aims to reproduce a standard dynamic range image from a high dynamic range image with visual information preserved. State-of-the-art tone mapping algorithms mostly decompose an image into a base layer and a detail layer, and process them accordingly. These methods may have problems of halo artifacts and over-enhancement, due to the lack of proper priors imposed on the two layers. In this paper, we propose a hybrid L1-L0 decomposition model to address these problems. Specifically, an L1 sparsity term is imposed on the base layer to model its piecewise smoothness property. An L0 sparsity term is imposed on the detail layer as a structural prior, which leads to piecewise constant effect. We further propose a multiscale tone mapping scheme based on our layer decomposition model. Experiments show that our tone mapping algorithm achieves visually compelling results with little halo artifacts, outperforming the state-of-the-art tone mapping algorithms in both subjective and objective evaluations",
    "checked": true,
    "id": "ec357faf29f96b23e09945877f46d5ff916624e3",
    "semantic_title": "a hybrid l1-l0 layer decomposition model for tone mapping",
    "citation_count": 128,
    "authors": [
      "Zhetong Liang",
      "Jun Xu",
      "David Zhang",
      "Zisheng Cao",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Nie_Deeply_Learned_Filter_CVPR_2018_paper.html": {
    "title": "Deeply Learned Filter Response Functions for Hyperspectral Reconstruction",
    "volume": "main",
    "abstract": "Hyperspectral reconstruction from RGB imaging has recently achieved significant progress via sparse coding and deep learning. However, a largely ignored fact is that existing RGB cameras are tuned to mimic human richromatic perception, thus their spectral responses are not necessarily optimal for hyperspectral reconstruction. In this paper, rather than use RGB spectral responses, we simultaneously learn optimized camera spectral response functions (to be implemented in hardware) and a mapping for spectral reconstruction by using an end-to-end network. Our core idea is that since camera spectral filters act in effect like the convolution layer, their response functions could be optimized by training standard neural networks. We propose two types of designed filters: a three-chip setup without spatial mosaicing and a single-chip setup with a Bayer-style 2x2 filter array. Numerical simulations verify the advantages of deeply learned spectral responses compared to existing RGB cameras. More interestingly, by considering physical restrictions in the design process, we are able to realize the deeply learned spectral response functions by using modern film filter production technologies, and thus construct data-inspired multispectral cameras for snapshot hyperspectral imaging",
    "checked": true,
    "id": "c2b93903c07116101a587e85601d7c99b7f62949",
    "semantic_title": "deeply learned filter response functions for hyperspectral reconstruction",
    "citation_count": 80,
    "authors": [
      "Shijie Nie",
      "Lin Gu",
      "Yinqiang Zheng",
      "Antony Lam",
      "Nobutaka Ono",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wan_CRRN_Multi-Scale_Guided_CVPR_2018_paper.html": {
    "title": "CRRN: Multi-Scale Guided Concurrent Reflection Removal Network",
    "volume": "main",
    "abstract": "Removing the undesired reflections from images taken through the glass is of broad application to various computer vision tasks. Non-learning based methods utilize different handcrafted priors such as the separable sparse gradients caused by different levels of blurs, which often fail due to their limited description capability to the properties of real-world reflections. In this paper, we propose the Concurrent Reflection Removal Network (CRRN) to tackle this problem in a unified framework. Our network integrates image appearance information and multi-scale gradient information with human perception inspired loss function, and is trained on a new dataset with 3250 reflection images taken under diverse real-world scenes. Extensive experiments on a public benchmark dataset show that the proposed method performs favorably against state-of-the-art methods",
    "checked": true,
    "id": "189b02caa2bb9ffb303154e35f60797ec8576f84",
    "semantic_title": "crrn: multi-scale guided concurrent reflection removal network",
    "citation_count": 107,
    "authors": [
      "Renjie Wan",
      "Boxin Shi",
      "Ling-Yu Duan",
      "Ah-Hwee Tan",
      "Alex C. Kot"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Single_Image_Reflection_CVPR_2018_paper.html": {
    "title": "Single Image Reflection Separation With Perceptual Losses",
    "volume": "main",
    "abstract": "We present an approach to separating reflection from a single image. The approach uses a fully convolutional network trained end-to-end with losses that exploit low-level and high-level image information. Our loss function includes two perceptual losses: a feature loss from a visual perception network, and an adversarial loss that encodes characteristics of images in the transmission layers. We also propose a novel exclusion loss that enforces pixel-level layer separation. We create a dataset of real-world images with reflection and corresponding ground-truth transmission layers for quantitative evaluation and model training. We validate our method through comprehensive quantitative experiments and show that our approach outperforms state-of-the-art reflection removal methods in PSNR, SSIM, and perceptual user study. We also extend our method to two other image enhancement tasks to demonstrate the generality of our approach",
    "checked": true,
    "id": "1a70351b6fe7d14a6928e730416f7f28f84237f1",
    "semantic_title": "single image reflection separation with perceptual losses",
    "citation_count": 320,
    "authors": [
      "Xuaner Zhang",
      "Ren Ng",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lao_A_Robust_Method_CVPR_2018_paper.html": {
    "title": "A Robust Method for Strong Rolling Shutter Effects Correction Using Lines With Automatic Feature Selection",
    "volume": "main",
    "abstract": "We present a robust method which compensates RS distortions in a single image using a set of image curves, basing on the knowledge that they correspond to 3D straight lines. Unlike in existing work, no a priori knowledge about the line directions (e.g. Manhattan World assumption) is required. We first formulate a parametric equation for the projection of a 3D straight line viewed by a moving rolling shutter camera under a uniform motion model. Then we propose a method which efficiently estimates ego angular velocity separately from pose parameters, using at least 4 image curves. Moreover, we propose for the first time a RANSAC-like strategy to select image curves which really correspond to 3D straight lines and reject those corresponding to actual curves in 3D world. A comparative experimental study with both synthetic and real data from famous benchmarks shows that the proposed method outperforms all the existing techniques from the state-of-the-art",
    "checked": true,
    "id": "f271f1695d6f358a56a5122b4707fec77624e5e6",
    "semantic_title": "a robust method for strong rolling shutter effects correction using lines with automatic feature selection",
    "citation_count": 47,
    "authors": [
      "Yizhen Lao",
      "Omar Ait-Aider"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tanaka_Time-Resolved_Light_Transport_CVPR_2018_paper.html": {
    "title": "Time-Resolved Light Transport Decomposition for Thermal Photometric Stereo",
    "volume": "main",
    "abstract": "We present a novel time-resolved light transport decomposition method using thermal imaging. Because the speed of heat propagation is much slower than the speed of light propagation, transient transport of far infrared light can be observed at a video frame rate. A key observation is that the thermal image looks similar to the visible light image in an appropriately controlled environment. This implies that conventional computer vision techniques can be straightforwardly applied to the thermal image. We show that the diffuse component in the thermal image can be separated and, therefore, the surface normals of objects can be estimated by the Lambertian photometric stereo. The effectiveness of our method is evaluated by conducting real-world experiments, and its applicability to black body, transparent, and translucent objects is shown",
    "checked": true,
    "id": "33c2c8823b2f60ff199b97024c140475861d19c1",
    "semantic_title": "time-resolved light transport decomposition for thermal photometric stereo",
    "citation_count": 5,
    "authors": [
      "Kenichiro Tanaka",
      "Nobuhiro Ikeya",
      "Tsuyoshi Takatani",
      "Hiroyuki Kubo",
      "Takuya Funatomi",
      "Yasuhiro Mukaigawa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Meshgi_Efficient_Diverse_Ensemble_CVPR_2018_paper.html": {
    "title": "Efficient Diverse Ensemble for Discriminative Co-Tracking",
    "volume": "main",
    "abstract": "Ensemble discriminative tracking utilizes a committee of classifiers, to label data samples, which are in turn, used for retraining the tracker to localize the target using the collective knowledge of the committee. Committee members could vary in their features, memory update schemes, or training data, however, it is inevitable to have committee members that excessively agree because of large overlaps in their version space. To remove this redundancy and have an effective ensemble learning, it is critical for the committee to include consistent hypotheses that differ from one-another, covering the version space with minimum overlaps. In this study, we propose an online ensemble tracker that directly generates a diverse committee by generating an efficient set of artificial training. The artificial data is sampled from the empirical distribution of the samples taken from both target and background, whereas the process is governed by query-by-committee to shrink the overlap between classifiers. The experimental results demonstrate that the proposed scheme outperforms conventional ensemble trackers on public benchmarks",
    "checked": true,
    "id": "12357c17fd402b2a6bc5a55e6dc4993a3dd12975",
    "semantic_title": "efficient diverse ensemble for discriminative co-tracking",
    "citation_count": 23,
    "authors": [
      "Kourosh Meshgi",
      "Shigeyuki Oba",
      "Shin Ishii"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bapat_Rolling_Shutter_and_CVPR_2018_paper.html": {
    "title": "Rolling Shutter and Radial Distortion Are Features for High Frame Rate Multi-Camera Tracking",
    "volume": "main",
    "abstract": "Traditionally, camera-based tracking approaches have treated rolling shutter and radial distortion as imaging artifacts that have to be overcome and corrected for in order to apply standard camera models and scene reconstruction methods. In this paper, we introduce a novel multi-camera tracking approach that for the first time jointly leverages the information introduced by rolling shutter and radial distortion as a feature to achieve superior performance with respect to high-frequency camera pose estimation. In particular, our system is capable of attaining high tracking rates that were previously unachievable. Our approach explicitly leverages rolling shutter capture and radial distortion to process individual rows, rather than entire image frames, for accurate camera motion estimation. We estimate a per-row 6 DoF pose of a rolling shutter camera by tracking multiple points on a radially distorted row whose rays span a curved surface in 3D space. Although tracking systems for rolling shutter cameras exist, we are the first to leverage radial distortion to measure a per-row pose -- enabling us to use less than half the number of cameras required by the previous state of the art. We validate our system on both synthetic and real imagery",
    "checked": true,
    "id": "8762bcc31a030eb75a1eb8e288922ec52d93485a",
    "semantic_title": "rolling shutter and radial distortion are features for high frame rate multi-camera tracking",
    "citation_count": 11,
    "authors": [
      "Akash Bapat",
      "True Price",
      "Jan-Michael Frahm"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/He_A_Twofold_Siamese_CVPR_2018_paper.html": {
    "title": "A Twofold Siamese Network for Real-Time Object Tracking",
    "volume": "main",
    "abstract": "Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similarity learning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks",
    "checked": true,
    "id": "a3a4471e82260f573d240cc34aeff431cf236571",
    "semantic_title": "a twofold siamese network for real-time object tracking",
    "citation_count": 566,
    "authors": [
      "Anfeng He",
      "Chong Luo",
      "Xinmei Tian",
      "Wenjun Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Multi-Cue_Correlation_Filters_CVPR_2018_paper.html": {
    "title": "Multi-Cue Correlation Filters for Robust Visual Tracking",
    "volume": "main",
    "abstract": "In recent years, many tracking algorithms achieve impressive performance via fusing multiple types of features, however, most of them fail to fully explore the context among the adopted multiple features and the strength of them. In this paper, we propose an efficient multi-cue analysis framework for robust visual tracking. By combining different types of features, our approach constructs multiple experts through Discriminative Correlation Filter (DCF) and each of them tracks the target independently. With the proposed robustness evaluation strategy, the suitable expert is selected for tracking in each frame. Furthermore, the divergence of multiple experts reveals the reliability of the current tracking, which is quantified to update the experts adaptively to keep them from corruption. Through the proposed multi-cue analysis, our tracker with standard DCF and deep features achieves outstanding results on several challenging benchmarks: OTB-2013, OTB-2015, Temple-Color and VOT 2016. On the other hand, when evaluated with only simple hand-crafted features, our method demonstrates comparable performance amongst complex non-realtime trackers, but exhibits much better efficiency, with a speed of 45 FPS on a CPU",
    "checked": true,
    "id": "a5278fc76eff08668bc1957b01b22eb627fa2c36",
    "semantic_title": "multi-cue correlation filters for robust visual tracking",
    "citation_count": 365,
    "authors": [
      "Ning Wang",
      "Wengang Zhou",
      "Qi Tian",
      "Richang Hong",
      "Meng Wang",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Learning_Attentions_Residual_CVPR_2018_paper.html": {
    "title": "Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking",
    "volume": "main",
    "abstract": "Offline training for object tracking has recently shown great potentials in balancing tracking accuracy and speed. However, it is still difficult to adapt an offline trained model to a target tracked online. This work presents a Residual Attentional Siamese Network (RASNet) for high performance object tracking. The RASNet model reformulates the correlation filter within a Siamese tracking framework, and introduces different kinds of the attention mechanisms to adapt the model without updating the model online. In particular, by exploiting the offline trained general attention, the target adapted residual attention, and the channel favored feature attention, the RASNet not only mitigates the over-fitting problem in deep network training, but also enhances its discriminative capacity and adaptability due to the separation of representation learning and discriminator learning. The proposed deep architecture is trained from end to end and takes full advantage of the rich spatial temporal information to achieve robust visual tracking. Experimental results on two latest benchmarks, OTB-2015 and VOT2017, show that the RASNet tracker has the state-of-the-art tracking accuracy while runs at more than 80 frames per second",
    "checked": true,
    "id": "6683442ae358ae4261fdcde0164f83dd1ccd621b",
    "semantic_title": "learning attentions: residual attentional siamese network for high performance online visual tracking",
    "citation_count": 463,
    "authors": [
      "Qiang Wang",
      "Zhu Teng",
      "Junliang Xing",
      "Jin Gao",
      "Weiming Hu",
      "Stephen Maybank"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_SINT_Robust_Visual_CVPR_2018_paper.html": {
    "title": "SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation",
    "volume": "main",
    "abstract": "Existing visual trackers are easily disturbed by occlusion,blurandlargedeformation. Inthechallengesofocclusion, motion blur and large object deformation, the performance of existing visual trackers may be limited due to the followingissues: i)Adoptingthedensesamplingstrategyto generate positive examples will make them less diverse; ii) Thetrainingdatawithdifferentchallengingfactorsarelimited, even though through collecting large training dataset. Collecting even larger training dataset is the most intuitive paradigm, but it may still can not cover all situations and the positive samples are still monotonous. In this paper, we propose to generate hard positive samples via adversarial learning for visual tracking. Speciﬁcally speaking, we assume the target objects all lie on a manifold, hence, we introduce the positive samples generation network (PSGN) to sampling massive diverse training data through traversing over the constructed target object manifold. The generated diverse target object images can enrich the training dataset and enhance the robustness of visual trackers. To make the tracker more robust to occlusion, we adopt the hard positive transformation network (HPTN) which can generate hard samples for tracking algorithm to recognize. We train this network with deep reinforcement learning to automaticallyoccludethetargetobjectwithanegativepatch. Based on the generated hard positive samples, we train a Siamese network for visual tracking and our experiments validate the effectiveness of the introduced algorithm",
    "checked": true,
    "id": "8a075b0ed920f650315020d1420231172fbe5ed2",
    "semantic_title": "sint++: robust visual tracking via adversarial positive instance generation",
    "citation_count": 120,
    "authors": [
      "Xiao Wang",
      "Chenglong Li",
      "Bin Luo",
      "Jin Tang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tang_High-Speed_Tracking_With_CVPR_2018_paper.html": {
    "title": "High-Speed Tracking With Multi-Kernel Correlation Filters",
    "volume": "main",
    "abstract": "Correlation filter (CF) based trackers are currently ranked top in terms of their performances. Nevertheless, only some of them, such as KCF [henriques12&15] and MKCF[tang&Feng15}, are able to exploit the powerful discriminability of non-linear kernels. Although MKCF achieves more powerful discriminability than KCF through introducing multi-kernel learning (MKL) into KCF, its improvement over KCF is quite limited and its computational burden increases significantly in comparison with KCF. In this paper, we will introduce the MKL into KCF in a different way than MKCF. We reformulate the MKL version of CF objective function with its upper bound, alleviating the negative mutual interference of different kernels significantly. Our novel MKCF tracker, MKCFup, outperforms KCF and MKCF with large margins and can still work at very high fps. Extensive experiments on public data sets show that our method is superior to state-of-the-art algorithms for target objects of small move at very high speed",
    "checked": true,
    "id": "0dccbfe5a91e1d5610c46585270af0c648d2aa25",
    "semantic_title": "high-speed tracking with multi-kernel correlation filters",
    "citation_count": 83,
    "authors": [
      "Ming Tang",
      "Bin Yu",
      "Fan Zhang",
      "Jinqiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Occlusion_Aware_Unsupervised_CVPR_2018_paper.html": {
    "title": "Occlusion Aware Unsupervised Learning of Optical Flow",
    "volume": "main",
    "abstract": "It has been recently shown that a convolutional neural network can learn optical flow estimation with unsuper- vised learning. However, the performance of the unsuper- vised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsuper- vised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Espe- cially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning",
    "checked": true,
    "id": "0b197f323e7d5514e5ada388858e38890dce0148",
    "semantic_title": "occlusion aware unsupervised learning of optical flow",
    "citation_count": 310,
    "authors": [
      "Yang Wang",
      "Yi Yang",
      "Zhenheng Yang",
      "Liang Zhao",
      "Peng Wang",
      "Wei Xu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Revisiting_Video_Saliency_CVPR_2018_paper.html": {
    "title": "Revisiting Video Saliency: A Large-Scale Benchmark and a New Model",
    "volume": "main",
    "abstract": "In this work, we contribute to video saliency research in two ways. First, we introduce a new benchmark for predicting human eye movements during dynamic scene free-viewing, which is long-time urged in this field. Our dataset, named DHF1K~(Dynamic Human Fixation), consists of 1K high-quality, elaborately selected video sequences spanning a large range of scenes, motions, object types and background complexity. Existing video saliency datasets lack variety and generality of common dynamic scenes and fall short in covering challenging situations in unconstrained environments. In contrast, DHF1K~makes a significant leap in terms of scalability, diversity and difficulty, and is expected to boost video saliency modeling. Second, we propose a novel video saliency model that augments the CNN-LSTM network architecture with an attention mechanism to enable fast, end-to-end saliency learning. The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning more flexible temporal saliency representation across successive frames. Such a design fully leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance. We thoroughly examine the performance of our model, with respect to state-of-the-art saliency models, on three large-scale datasets (i.e., DHF1K, Hollywood2, UCF sports). Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that our model outperforms other competitors",
    "checked": true,
    "id": "fdf8c9c4c30c6005c2f0e92ce9db3de5ab8b5d29",
    "semantic_title": "revisiting video saliency: a large-scale benchmark and a new model",
    "citation_count": 239,
    "authors": [
      "Wenguan Wang",
      "Jianbing Shen",
      "Fang Guo",
      "Ming-Ming Cheng",
      "Ali Borji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Learning_Spatial-Temporal_Regularized_CVPR_2018_paper.html": {
    "title": "Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking",
    "volume": "main",
    "abstract": "Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Compared with SRDCF, STRCF with hand-crafted features provides a 5× speedup and achieves a gain of 5.4% and 3.6% AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF with deep features also performs favorably against state-of-the-art trackers and achieves an AUC score of 68.3% on OTB-2015",
    "checked": true,
    "id": "9f45b55af027503fab557f55f70e81e43c6c1db7",
    "semantic_title": "learning spatial-temporal regularized correlation filters for visual tracking",
    "citation_count": 697,
    "authors": [
      "Feng Li",
      "Cheng Tian",
      "Wangmeng Zuo",
      "Lei Zhang",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bouritsas_Multimodal_Visual_Concept_CVPR_2018_paper.html": {
    "title": "Multimodal Visual Concept Learning With Weakly Supervised Techniques",
    "volume": "main",
    "abstract": "Despite the availability of a huge amount of video data accompanied by descriptive texts, it is not always easy to exploit the information contained in natural language in order to automatically recognize video concepts. Towards this goal, in this paper we use textual cues as means of supervision, introducing two weakly supervised techniques that extend the Multiple Instance Learning (MIL) framework: the Fuzzy Sets Multiple Instance Learning (FSMIL) and the Probabilistic Labels Multiple Instance Learning (PLMIL). The former encodes the spatio-temporal imprecision of the linguistic descriptions with Fuzzy Sets, while the latter models different interpretations of each description's semantics with Probabilistic Labels, both formulated through a convex optimization algorithm. In addition, we provide a novel technique to extract weak labels in the presence of complex semantics, that consists of semantic similarity computations. We evaluate our methods on two distinct problems, namely face and action recognition, in the challenging and realistic setting of movies accompanied by their screenplays, contained in the COGNIMUSE database. We show that, on both tasks, our method considerably outperforms a state-of-the-art weakly supervised approach, as well as other baselines",
    "checked": true,
    "id": "9e1379e2f0509af074808c1e464ef78fb6abd5ba",
    "semantic_title": "multimodal visual concept learning with weakly supervised techniques",
    "citation_count": 7,
    "authors": [
      "Giorgos Bouritsas",
      "Petros Koutras",
      "Athanasia Zlatintsi",
      "Petros Maragos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Efficient_Large-Scale_Approximate_CVPR_2018_paper.html": {
    "title": "Efficient Large-Scale Approximate Nearest Neighbor Search on OpenCL FPGA",
    "volume": "main",
    "abstract": "We present a new method for Product Quantization (PQ) based approximated nearest neighbor search (ANN) in high dimensional spaces. Specifically, we first propose a quantization scheme for the codebook of coarse quantizer, product quantizer, and rotation matrix, to reduce the cost of accessing these codebooks. Our approach also combines a highly parallel k-selection method, which can be fused with the distance calculation to reduce the memory overhead. We implement the proposed method on Intel HARPv2 platform using OpenCL-FPGA. The proposed method significantly outperforms state-of-the-art methods on CPU and GPU for high dimensional nearest neighbor queries on billion-scale datasets in terms of query time and accuracy regardless of the batch size. To our best knowledge, this is the first work to demonstrate FPGA performance superior to CPU and GPU on high-dimensional, large-scale ANN datasets",
    "checked": true,
    "id": "b291d1b421879331e7358e5cc740d4fefefe269d",
    "semantic_title": "efficient large-scale approximate nearest neighbor search on opencl fpga",
    "citation_count": 40,
    "authors": [
      "Jialiang Zhang",
      "Soroosh Khoram",
      "Jing Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jain_Learning_a_Complete_CVPR_2018_paper.html": {
    "title": "Learning a Complete Image Indexing Pipeline",
    "volume": "main",
    "abstract": "To work at scale, a complete image indexing system comprises two components: An inverted file index to restrict the actual search to only a subset that should contain most of the items relevant to the query; An approximate distance computation mechanism to rapidly scan these lists. While supervised deep learning has recently enabled improvements to the latter, the former continues to be based on unsupervised clustering in the literature. In this work, we propose a first system that learns both components within a unifying neural framework of structured binary encoding",
    "checked": true,
    "id": "b8e7b4a8844edf8e0ce5f9f5eaae55210bd288c4",
    "semantic_title": "learning a complete image indexing pipeline",
    "citation_count": 11,
    "authors": [
      "Himalaya Jain",
      "Joaquin Zepeda",
      "Patrick Pérez",
      "Rémi Gribonval"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mascharka_Transparency_by_Design_CVPR_2018_paper.html": {
    "title": "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning",
    "volume": "main",
    "abstract": "Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art",
    "checked": true,
    "id": "cd0a7c58964905ccfddbad1614165320ccc56393",
    "semantic_title": "transparency by design: closing the gap between performance and interpretability in visual reasoning",
    "citation_count": 207,
    "authors": [
      "David Mascharka",
      "Philip Tran",
      "Ryan Soklaski",
      "Arjun Majumdar"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Fooling_Vision_and_CVPR_2018_paper.html": {
    "title": "Fooling Vision and Language Models Despite Localization and Attention Mechanism",
    "volume": "main",
    "abstract": "Adversarial attacks are known to succeed on classifiers, but it has been an open question whether more complex vision systems are vulnerable. In this paper, we study adversarial examples for vision and language models, which incorporate natural language understanding and complex structures such as attention, localization, and modular architectures. In particular, we investigate attacks on a dense captioning model and on two visual question answering (VQA) models. Our evaluation shows that we can generate adversarial examples with a high success rate (i.e., >90%) for these models. Our work sheds new light on understanding adversarial attacks on vision systems which have a language component and shows that attention, bounding box localization, and compositional internal structures are vulnerable to adversarial attacks. These observations will inform future work towards building effective defenses",
    "checked": true,
    "id": "2fa9b32ebc329d57fa2e3fabb9e12382f019f47a",
    "semantic_title": "fooling vision and language models despite localization and attention mechanism",
    "citation_count": 80,
    "authors": [
      "Xiaojun Xu",
      "Xinyun Chen",
      "Chang Liu",
      "Anna Rohrbach",
      "Trevor Darrell",
      "Dawn Song"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Categorizing_Concepts_With_CVPR_2018_paper.html": {
    "title": "Categorizing Concepts With Basic Level for Vision-to-Language",
    "volume": "main",
    "abstract": "Vision-to-language tasks require a unified semantic understanding of visual content. However, the information contained in image/video is essentially ambiguous on two perspectives manifested on the diverse understanding among different persons and the various understanding grains even for the same person. Inspired by the basic level in early cognition, a Basic Concept (BaC) category is proposed in this work that contains both consensus and proper level of visual content to help neural network tackle the above problems. Specifically, a salient concept category is firstly generated by intersecting the labels of ImageNet and the vocabulary of MSCOCO dataset. Then, according to the observation from human early cognition that children make fewer mistakes on the basic level, the salient category is further refined by clustering concepts with a defined confusion degree which measures the difficulty for convolutional neural network to distinguish class pairs. Finally, a pre-trained model based on GoogLeNet is produced with the proposed BaC category of 1,372 concept classes. To verify the effectiveness of the proposed categorizing method for vision-to-language tasks, two kinds of experiments are performed including image captioning and visual question answering with the benchmark datasets of MSCOCO, Flickr30k and COCO-QA. The experimental results demonstrate that the representations derived from the cognition-inspired BaC category promote representation learning of neural networks on vision-to-language tasks, and a performance improvement is gained without modifying standard models",
    "checked": true,
    "id": "7fdac774e51e0aa8f9921e857067801d73a8d2d0",
    "semantic_title": "categorizing concepts with basic level for vision-to-language",
    "citation_count": 6,
    "authors": [
      "Hanzhang Wang",
      "Hanli Wang",
      "Kaisheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_Just_Assume_CVPR_2018_paper.html": {
    "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering",
    "volume": "main",
    "abstract": "A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models",
    "checked": true,
    "id": "90873a97aa9a43775e5aeea01b03aea54b28bfbd",
    "semantic_title": "don't just assume; look and answer: overcoming priors for visual question answering",
    "citation_count": 585,
    "authors": [
      "Aishwarya Agrawal",
      "Dhruv Batra",
      "Devi Parikh",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ahn_Learning_Pixel-Level_Semantic_CVPR_2018_paper.html": {
    "title": "Learning Pixel-Level Semantic Affinity With Image-Level Supervision for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "The deficiency of segmentation labels is one of the main obstacles to semantic segmentation in the wild. To alleviate this issue, we present a novel framework that generates segmentation labels of images given their image-level class labels. In this weakly supervised setting, trained models have been known to segment local discriminative parts rather than the entire object area. Our solution is to propagate such local responses to nearby areas which belong to the same semantic entity. To this end, we propose a Deep Neural Network (DNN) called AffinityNet that predicts semantic affinity between a pair of adjacent image coordinates. The semantic propagation is then realized by random walk with the affinities predicted by AffinityNet. More importantly, the supervision employed to train AffinityNet is given by the initial discriminative part segmentation, which is incomplete as a segmentation annotation but sufficient for learning semantic affinities within small image areas. Thus the entire framework relies only on image-level class labels and does not require any extra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with segmentation labels generated by our method outperforms previous models trained with the same level of supervision, and is even as competitive as those relying on stronger supervision",
    "checked": true,
    "id": "e644867bc141453d1f0387c76ff5e7f7863c5f4f",
    "semantic_title": "learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation",
    "citation_count": 608,
    "authors": [
      "Jiwoon Ahn",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fouhey_From_Lifestyle_Vlogs_CVPR_2018_paper.html": {
    "title": "From Lifestyle Vlogs to Everyday Interactions",
    "volume": "main",
    "abstract": "A major stumbling block to progress in understanding basic human interactions, such as getting out of bed or opening a refrigerator, is lack of good training data. Most past efforts have gathered this data explicitly: starting with a laundry list of action labels, and then querying search engines for videos tagged with each label. In this work, we do the reverse and search implicitly: we start with a large collection of interaction-rich video data and then annotate and analyze it. We use Internet Lifestyle Vlogs as the source of surprisingly large and diverse interaction data. We show that by collecting the data first, we are able to achieve greater scale and far greater diversity in terms of actions and actors. Additionally, our data exposes biases built into common explicitly gathered data. We make sense of our data by analyzing the central component of interaction -- hands. We benchmark two tasks: identifying semantic object contact at the video level and non-semantic contact state at the frame level. We additionally demonstrate future prediction of hands",
    "checked": true,
    "id": "729fb92afe3cf7faaae1b079f7c7a2cd39c01dad",
    "semantic_title": "from lifestyle vlogs to everyday interactions",
    "citation_count": 114,
    "authors": [
      "David F. Fouhey",
      "Wei-cheng Kuo",
      "Alexei A. Efros",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Inoue_Cross-Domain_Weakly-Supervised_Object_CVPR_2018_paper.html": {
    "title": "Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation",
    "volume": "main",
    "abstract": "Can we detect common objects in a variety of image domains without instance-level annotations? In this paper, we present a framework for a novel task, cross-domain weakly supervised object detection, which addresses this question. For this paper, we have access to images with instance-level annotations in a source domain (e.g., natural image) and images with image-level annotations in a target domain (e.g., watercolor). In addition, the classes to be detected in the target domain are all or a subset of those in the source domain. Starting from a fully supervised object detector, which is pre-trained on the source domain, we propose a two-step progressive domain adaptation technique by fine-tuning the detector on two types of artificially and automatically generated samples. We test our methods on our newly collected datasets containing three image domains, and achieve an improvement of approximately 5 to 20 percentage points in terms of mean average precision (mAP) compared to the best-performing baselines",
    "checked": true,
    "id": "1c0150d2eb50ce33ad1e6e0e7c1bff0503e5bdc7",
    "semantic_title": "cross-domain weakly-supervised object detection through progressive domain adaptation",
    "citation_count": 428,
    "authors": [
      "Naoto Inoue",
      "Ryosuke Furuta",
      "Toshihiko Yamasaki",
      "Kiyoharu Aizawa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kanezaki_RotationNet_Joint_Object_CVPR_2018_paper.html": {
    "title": "RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews From Unsupervised Viewpoints",
    "volume": "main",
    "abstract": "We propose a Convolutional Neural Network (CNN)-based model ``RotationNet,'' which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet is designed to use only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves the state-of-the-art performance on an object pose estimation dataset",
    "checked": true,
    "id": "5e63a43ea77351f63a77cab684bd9ed7f810a9bf",
    "semantic_title": "rotationnet: joint object categorization and pose estimation using multiviews from unsupervised viewpoints",
    "citation_count": 478,
    "authors": [
      "Asako Kanezaki",
      "Yasuyuki Matsushita",
      "Yoshifumi Nishida"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/He_An_End-to-End_TextSpotter_CVPR_2018_paper.html": {
    "title": "An End-to-End TextSpotter With Explicit Alignment and Attention",
    "volume": "main",
    "abstract": "Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Jointly training two tasks is non-trivial due to significant differences in learning difficulties and convergence rates. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in a united framework. Our main contributions are three-fold: (1) we propose a novel textalignment layer that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance; (2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; (3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by sharing convolutional features, which is critical to identify challenging text instances. Our model obtains impressive results in end-to-end recognition on the ICDAR 2015, significantly advancing the most recent results, with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detector by achieving a new state-of-the-art detection performance on related benchmarks. Code is available at https://github. com/tonghe90/textspotter",
    "checked": true,
    "id": "89642d3bacccbe543e224ea139b69986048915ef",
    "semantic_title": "an end-to-end textspotter with explicit alignment and attention",
    "citation_count": 212,
    "authors": [
      "Tong He",
      "Zhi Tian",
      "Weilin Huang",
      "Chunhua Shen",
      "Yu Qiao",
      "Changming Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chavdarova_WILDTRACK_A_Multi-Camera_CVPR_2018_paper.html": {
    "title": "WILDTRACK: A Multi-Camera HD Dataset for Dense Unscripted Pedestrian Detection",
    "volume": "main",
    "abstract": "People detection methods are highly sensitive to occlusions between pedestrians, which are extremely frequent in many situations where cameras have to be mounted at a limited height. The reduction of camera prices allows for the generalization of static multi-camera set-ups. Using joint visual information from multiple synchronized cameras gives the opportunity to improve detection performance. In this paper, we present a new large-scale and high-resolution dataset. It has been captured with seven static cameras in a public open area, and unscripted dense groups of pedestrians standing and walking. Together with the camera frames, we provide an accurate joint (extrinsic and intrinsic) calibration, as well as 7 series of 400 annotated frames for detection at a rate of 2 frames per second. This results in over 40,000 bounding boxes delimiting every person present in the area of interest, for a total of more than 300 individuals. We provide a series of benchmark results using baseline algorithms published over the recent months for multi-view detection with deep neural networks, and trajectory estimation using a non-Markovian model",
    "checked": true,
    "id": "36bccfb2ad847096bc76777e544f305813cd8f5b",
    "semantic_title": "wildtrack: a multi-camera hd dataset for dense unscripted pedestrian detection",
    "citation_count": 188,
    "authors": [
      "Tatjana Chavdarova",
      "Pierre Baqué",
      "Stéphane Bouquet",
      "Andrii Maksai",
      "Cijo Jose",
      "Timur Bagautdinov",
      "Louis Lettry",
      "Pascal Fua",
      "Luc Van Gool",
      "François Fleuret"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Miao_Direct_Shape_Regression_CVPR_2018_paper.html": {
    "title": "Direct Shape Regression Networks for End-to-End Face Alignment",
    "volume": "main",
    "abstract": "Face alignment has been extensively studied in computer vision community due to its fundamental role in facial analysis, but it remains an unsolved problem. The major challenges lie in the highly nonlinear relationship between face images and associated facial shapes, which is coupled by underlying correlation of landmarks. Existing methods mainly rely on cascaded regression, suffering from intrinsic shortcomings, e.g., strong dependency on initialization and failure to exploit landmark correlations. In this paper, we propose the direct shape regression network (DSRN) for end-to-end face alignment by jointly handling the aforementioned challenges in a unified framework. Specifically, by deploying doubly convolutional layer and by using the Fourier feature pooling layer proposed in this paper, DSRN efficiently constructs strong representations to disentangle highly nonlinear relationships between images and shapes; by incorporating a linear layer of low-rank learning, DSRN effectively encodes correlations of landmarks to improve performance. DSRN leverages the strengths of kernels for nonlinear feature extraction and neural networks for structured prediction, and provides the first end-to-end learning architecture for direct face alignment. Its effectiveness and generality are validated by extensive experiments on five benchmark datasets, including AFLW, 300W, CelebA, MAFL, and 300VW. All empirical results demonstrate that DSRN consistently produces high performance and in most cases surpasses state-of-the-art",
    "checked": true,
    "id": "5a7e62fdea39a4372e25cbbadc01d9b2204af95a",
    "semantic_title": "direct shape regression networks for end-to-end face alignment",
    "citation_count": 108,
    "authors": [
      "Xin Miao",
      "Xiantong Zhen",
      "Xianglong Liu",
      "Cheng Deng",
      "Vassilis Athitsos",
      "Heng Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Natural_and_Effective_CVPR_2018_paper.html": {
    "title": "Natural and Effective Obfuscation by Head Inpainting",
    "volume": "main",
    "abstract": "As more and more personal photos are shared online, being able to obfuscate identities in such photos is becoming a necessity for privacy protection. People have largely resorted to blacking out or blurring head regions, but they result in poor user experience while being surprisingly ineffective against state of the art person recognizers[17]. In this work, we propose a novel head inpainting obfuscation technique. Generating a realistic head inpainting in social media photos is challenging because subjects appear in diverse activities and head orientations. We thus split the task into two sub-tasks: (1) facial landmark generation from image context (e.g. body pose) for seamless hypothesis of sensible head pose, and (2) facial landmark conditioned head inpainting. We verify that our inpainting method generates realistic person images, while achieving superior obfuscation performance against automatic person recognizers",
    "checked": true,
    "id": "503906ca940fa3b01e39d05879c9b6a36524aaf5",
    "semantic_title": "natural and effective obfuscation by head inpainting",
    "citation_count": 204,
    "authors": [
      "Qianru Sun",
      "Liqian Ma",
      "Seong Joon Oh",
      "Luc Van Gool",
      "Bernt Schiele",
      "Mario Fritz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yoon_3D_Semantic_Trajectory_CVPR_2018_paper.html": {
    "title": "3D Semantic Trajectory Reconstruction From 3D Pixel Continuum",
    "volume": "main",
    "abstract": "This paper presents a method to reconstruct dense semantic trajectory stream of human interactions in 3D from synchronized multiple videos. The interactions inherently introduce self-occlusion and illumination/appearance/shape changes, resulting in highly fragmented trajectory reconstruction with noisy and coarse semantic labels. Our conjecture is that among many views, there exists a set of views that can confidently recognize the visual semantic label of a 3D trajectory. We introduce a new representation called 3D semantic map---a probability distribution over the semantic labels per trajectory. We construct the 3D semantic map by reasoning about visibility and 2D recognition confidence based on view-pooling, i.e., finding the view that best represents the semantics of the trajectory. Using the 3D semantic map, we precisely infer all trajectory labels jointly by considering the affinity between long range trajectories via estimating their local rigid transformations. This inference quantitatively outperforms the baseline approaches in terms of predictive validity, representation robustness, and affinity effectiveness. We demonstrate that our algorithm can robustly compute the semantic labels of a large scale trajectory set involving real-world human interactions with object, scenes, and people",
    "checked": true,
    "id": "d018e62a633bba600e98855c49f53f4bdeac432d",
    "semantic_title": "3d semantic trajectory reconstruction from 3d pixel continuum",
    "citation_count": 10,
    "authors": [
      "Jae Shin Yoon",
      "Ziwei Li",
      "Hyun Soo Park"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Han_Optimizing_Filter_Size_CVPR_2018_paper.html": {
    "title": "Optimizing Filter Size in Convolutional Neural Networks for Facial Action Unit Recognition",
    "volume": "main",
    "abstract": "Recognizing facial action units (AUs) during spontaneous facial displays is a challenging problem. Most recently, Convolutional Neural Networks (CNNs) have shown promise for facial AU recognition, where predefined and fixed convolution filter sizes are employed. In order to achieve the best performance, the optimal filter size is often empirically found by conducting extensive experimental validation. Such a training process suffers from expensive training cost, especially as the network becomes deeper. This paper proposes a novel Optimized Filter Size CNN (OFS-CNN), where the filter sizes and weights of all convolutional layers are learned simultaneously from the training data along with learning convolution filters. Specifically, the filter size is defined as a continuous variable, which is optimized by minimizing the training loss. Experimental results on two AU-coded spontaneous databases have shown that the proposed OFS-CNN is capable of estimating optimal filter size for varying image resolution and outperforms traditional CNNs with the best filter size obtained by exhaustive search. The OFS-CNN also beats the CNN using multiple filter sizes and more importantly, is much more efficient during testing with the proposed forward-backward propagation algorithm",
    "checked": true,
    "id": "fcc657353dd37e2471c5d1c703bf3f88e95f79d9",
    "semantic_title": "optimizing filter size in convolutional neural networks for facial action unit recognition",
    "citation_count": 64,
    "authors": [
      "Shizhong Han",
      "Zibo Meng",
      "Zhiyuan Li",
      "James O'Reilly",
      "Jie Cai",
      "Xiaofeng Wang",
      "Yan Tong"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Moon_V2V-PoseNet_Voxel-to-Voxel_Prediction_CVPR_2018_paper.html": {
    "title": "V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation From a Single Depth Map",
    "volume": "main",
    "abstract": "Most of the existing deep learning-based methods for 3D hand and human pose estimation from a single depth map are based on a common framework that takes a 2D depth map and directly regresses the 3D coordinates of keypoints, such as hand or human body joints, via 2D convolutional neural networks (CNNs). The first weakness of this approach is the presence of perspective distortion in the 2D depth map. While the depth map is intrinsically 3D data, many previous methods treat depth maps as 2D images that can distort the shape of the actual object through projection from 3D to 2D space. This compels the network to perform perspective distortion-invariant estimation. The second weakness of the conventional approach is that directly regressing 3D coordinates from a 2D image is a highly non-linear mapping, which causes difficulty in the learning procedure. To overcome these weaknesses, we firstly cast the 3D hand and human pose estimation problem from a single depth map into a voxel-to-voxel prediction that uses a 3D voxelized grid and estimates the per-voxel likelihood for each keypoint. We design our model as a 3D CNN that provides accurate estimates while running in real-time. Our system outperforms previous methods in almost all publicly available 3D hand and human pose estimation datasets and placed first in the HANDS 2017 frame-based 3D hand pose estimation challenge. The code is available in https://github.com/mks0601/V2V-PoseNet_RELEASE",
    "checked": true,
    "id": "119ae73801f8bbecd8541f7794d2ee05ebcac108",
    "semantic_title": "v2v-posenet: voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map",
    "citation_count": 413,
    "authors": [
      "Gyeongsik Moon",
      "Ju Yong Chang",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zheng_Ring_Loss_Convex_CVPR_2018_paper.html": {
    "title": "Ring Loss: Convex Feature Normalization for Face Recognition",
    "volume": "main",
    "abstract": "We motivate and present Ring loss, a simple and elegant feature normalization approach for deep networks designed to augment standard loss functions such as Softmax. We argue that deep feature normalization is an important aspect of supervised classification problems where we require the model to represent each class in a multi-class problem equally well. The direct approach to feature normalization through the hard normalization operation results in a non-convex formulation. Instead, Ring loss applies soft normalization, where it gradually learns to constrain the norm to the scaled unit circle while preserving convexity leading to more robust features. We apply Ring loss to large-scale face recognition problems and present results on LFW, the challenging protocols of IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity Frontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss outperforms strong baselines, matches state-of-the-art performance on IJB-A Janus and outperforms all other results on the challenging Janus CS3 thereby achieving state-of-the-art. We also outperform strong baselines in handling extremely low resolution face matching",
    "checked": true,
    "id": "67a9659de0bf671fafccd7f39b7587f85fb6dfbd",
    "semantic_title": "ring loss: convex feature normalization for face recognition",
    "citation_count": 198,
    "authors": [
      "Yutong Zheng",
      "Dipan K. Pal",
      "Marios Savvides"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.html": {
    "title": "Adversarially Occluded Samples for Person Re-Identification",
    "volume": "main",
    "abstract": "Person re-identification (ReID) is the task of retrieving particular persons across different cameras. Despite its great progress in recent years, it is still confronted with challenges like pose variation, occlusion, and similar appearance among different persons. The large gap between training and testing performance with existing models implies the insufficiency of generalization. Considering this fact, we propose to augment the variation of training data by introducing Adversarially Occluded Samples. These special samples are both a) meaningful in that they resemble real-scene occlusions, and b) effective in that they are tough for the original model and thus provide the momentum to jump out of local optimum. We mine these samples based on a trained ReID model and with the help of network visualization techniques. Extensive experiments show that the proposed samples help the model discover new discriminative clues on the body and generalize much better at test time. Our strategy makes significant improvement over strong baselines on three large-scale ReID datasets, Market1501, CUHK03 and DukeMTMC-reID",
    "checked": true,
    "id": "1bfe59be5b42d6b7257da4b35a408239c01ab79d",
    "semantic_title": "adversarially occluded samples for person re-identification",
    "citation_count": 247,
    "authors": [
      "Houjing Huang",
      "Dangwei Li",
      "Zhang Zhang",
      "Xiaotang Chen",
      "Kaiqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Classifier_Learning_With_CVPR_2018_paper.html": {
    "title": "Classifier Learning With Prior Probabilities for Facial Action Unit Recognition",
    "volume": "main",
    "abstract": "Facial action units (AUs) play an important role in human emotion understanding. One big challenge for data-driven AU recognition approaches is the lack of enough AU annotations, since AU annotation requires strong domain expertise. To alleviate this issue, we propose a knowledge-driven method for jointly learning multiple AU classifiers without any AU annotation by leveraging prior probabilities on AUs, including expression-independent and expression-dependent AU probabilities. These prior probabilities are drawn from facial anatomy and emotion studies, and are independent of datasets. We incorporate the prior probabilities on AUs as the constraints into the objective function of multiple AU classifiers, and develop an efficient learning algorithm to solve the formulated problem. Experimental results on five benchmark expression databases demonstrate the effectiveness of the proposed method, especially its generalization ability, and the power of the prior probabilities",
    "checked": true,
    "id": "0ff71f1fadf4c35924e93174556e0a63439d94b9",
    "semantic_title": "classifier learning with prior probabilities for facial action unit recognition",
    "citation_count": 48,
    "authors": [
      "Yong Zhang",
      "Weiming Dong",
      "Bao-Gang Hu",
      "Qiang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_4DFAB_A_Large_CVPR_2018_paper.html": {
    "title": "4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications",
    "volume": "main",
    "abstract": "The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contain recordings of 180 subjects captured in four different sessions spanned over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database in various applications. The database will be made publicly available for research purposes",
    "checked": true,
    "id": "bc249c9be803af3d4a5a6de495e1c85578fce84b",
    "semantic_title": "4dfab: a large scale 4d database for facial expression analysis and biometric applications",
    "citation_count": 89,
    "authors": [
      "Shiyang Cheng",
      "Irene Kotsia",
      "Maja Pantic",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Seeing_Small_Faces_CVPR_2018_paper.html": {
    "title": "Seeing Small Faces From Robust Anchor's Perspective",
    "volume": "main",
    "abstract": "This paper introduces a novel anchor design principle to support anchor-based face detection for superior scale-invariant performance, especially on tiny faces. To achieve this, we explicitly address the problem that anchor-based detectors drop performance drastically on faces with tiny sizes, e.g. less than 16x16 pixels. In this paper, we investigate why this is the case. We discover that current anchor design cannot guarantee high overlaps between tiny faces and anchor boxes, which increases the difficulty of training. The new Expected Max Overlapping (EMO) score is proposed which can theoretically explain the low overlapping issue and inspire several effective strategies of new anchor design leading to higher face overlaps, including anchor stride reduction with new network architectures, extra shifted anchors, and stochastic face shifting. Comprehensive experiments show that our proposed method significantly outperforms the baseline anchor-based detector, while consistently achieving state-of-the-art results on challenging face detection datasets with competitive runtime speed",
    "checked": true,
    "id": "693e2dd25ec1d7953d1ca031cb9409f9dcf8ec0a",
    "semantic_title": "seeing small faces from robust anchor's perspective",
    "citation_count": 112,
    "authors": [
      "Chenchen Zhu",
      "Ran Tao",
      "Khoa Luu",
      "Marios Savvides"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Luvizon_2D3D_Pose_Estimation_CVPR_2018_paper.html": {
    "title": "2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning",
    "volume": "main",
    "abstract": "Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efficient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-to-end leads to significantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks",
    "checked": true,
    "id": "3d4f5c848b41160ac665d1991529a67a3208061e",
    "semantic_title": "2d/3d pose estimation and action recognition using multitask deep learning",
    "citation_count": 484,
    "authors": [
      "Diogo C. Luvizon",
      "David Picard",
      "Hedi Tabia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wan_Dense_3D_Regression_CVPR_2018_paper.html": {
    "title": "Dense 3D Regression for Hand Pose Estimation",
    "volume": "main",
    "abstract": "We present a simple and effective method for 3D hand pose estimation from a single depth frame. As opposed to previous state-of-arts based on holistic 3D regression, our method works on dense pixel-wise estimation. This is achieved by careful design choices in pose parameterization, which leverages both 2D and 3D properties of depth map. Specifically, we decompose the pose parameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat maps and unit 3D direction vector fields. The 2D/3D joint heat maps and 3D joint offsets are estimated via multi-task network cascades, which is trained end-to-end. The pixel-wise estimations can be directly translated into a vote casting scheme. A variant of mean shift is then used to aggregate local votes and explicitly handles the global 3D estimation in consensus with pixel-wise 2D and 3D estimations. Our method is efficient and highly accurate. On MSRA and NYU hand dataset, our method outperforms all previous state-of-arts by a large margin. On ICVL hand dataset, our method achieves similar accuracy compared to the state-of-art which is nearly saturated and outperforms other state-of-arts. Code will be made available",
    "checked": true,
    "id": "82e777f5a4baf8b5fc932118582474fe2213509f",
    "semantic_title": "dense 3d regression for hand pose estimation",
    "citation_count": 156,
    "authors": [
      "Chengde Wan",
      "Thomas Probst",
      "Luc Van Gool",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhong_Camera_Style_Adaptation_CVPR_2018_paper.html": {
    "title": "Camera Style Adaptation for Person Re-Identification",
    "volume": "main",
    "abstract": "Being a cross-camera retrieval task, person re-identification suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle can serve as a data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against over-fitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which over-fitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of over-fitting. We also report competitive accuracy compared with the state of the art",
    "checked": true,
    "id": "1822ca8db58b0382b0c64f310840f0f875ea02c0",
    "semantic_title": "camera style adaptation for person re-identification",
    "citation_count": 572,
    "authors": [
      "Zhun Zhong",
      "Liang Zheng",
      "Zhedong Zheng",
      "Shaozi Li",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Andriluka_PoseTrack_A_Benchmark_CVPR_2018_paper.html": {
    "title": "PoseTrack: A Benchmark for Human Pose Estimation and Tracking",
    "volume": "main",
    "abstract": "Existing systems for video-based pose estimation and tracking struggle to perform well on realistic videos with multiple people and often fail to output body-pose trajectories consistent over time. To address this shortcoming this paper introduces PoseTrack which is a new large-scale benchmark for video-based human pose estimation and articulated tracking. Our new benchmark encompasses three tasks focusing on i) single-frame multi-person pose estimation, ii) multi-person pose estimation in videos, and iii) multi-person articulated tracking. To establish the benchmark, we collect, annotate and release a new dataset that features videos with multiple people labeled with person tracks and articulated pose. A public centralized evaluation server is provided to allow the research community to evaluate on a held-out test set. Furthermore, we conduct an extensive experimental study on recent approaches to articulated pose tracking and provide analysis of the strengths and weaknesses of the state of the art. We envision that the proposed benchmark will stimulate productive research both by providing a large and representative training dataset as well as providing a platform to objectively evaluate and compare the proposed methods. The benchmark is freely accessible at https://posetrack.net/",
    "checked": true,
    "id": "09b2e7af73689dbdba1547e19111a6ee06767906",
    "semantic_title": "posetrack: a benchmark for human pose estimation and tracking",
    "citation_count": 458,
    "authors": [
      "Mykhaylo Andriluka",
      "Umar Iqbal",
      "Eldar Insafutdinov",
      "Leonid Pishchulin",
      "Anton Milan",
      "Juergen Gall",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Exploit_the_Unknown_CVPR_2018_paper.html": {
    "title": "Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning",
    "volume": "main",
    "abstract": "We focus on the one-shot learning for video-based person re-Identification (re-ID). Unlabeled tracklets for the person re-ID tasks can be easily obtained by pre-processing, such as pedestrian detection and tracking. In this paper, we propose an approach to exploiting unlabeled tracklets by gradually but steadily improving the discriminative capability of the Convolutional Neural Network (CNN) feature representation via stepwise learning. We first initialize a CNN model using one labeled tracklet for each identity. Then we update the CNN model by the following two steps iteratively: 1. sample a few candidates with most reliable pseudo labels from unlabeled tracklets; 2. update the CNN model according to the selected data. Instead of the static sampling strategy applied in existing works, we propose a progressive sampling method to increase the number of the selected pseudo-labeled candidates step by step. We systematically investigate the way how we should select pseudo-labeled tracklets into the training set to make the best use of them. Notably, the rank-1 accuracy of our method outperforms the state-of-the-art method by 21.46 points (absolute, i.e., 62.67% vs. 41.21%) on the MARS dataset, and 16.53 points on the DukeMTMC-VideoReID dataset",
    "checked": true,
    "id": "b83964876faa536489b4c189fdeb1f9d50172862",
    "semantic_title": "exploit the unknown gradually: one-shot video-based person re-identification by stepwise learning",
    "citation_count": 345,
    "authors": [
      "Yu Wu",
      "Yutian Lin",
      "Xuanyi Dong",
      "Yan Yan",
      "Wanli Ouyang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Pose-Robust_Face_Recognition_CVPR_2018_paper.html": {
    "title": "Pose-Robust Face Recognition via Deep Residual Equivariant Mapping",
    "volume": "main",
    "abstract": "Face recognition achieves exceptional success thanks to the emergence of deep learning. However, many contemporary face recognition models still perform relatively poor in processing profile faces compared to frontal faces. A key reason is that the number of frontal and profile training faces are highly imbalanced - there are extensively more frontal training samples compared to profile ones. In addition, it is intrinsically hard to learn a deep representation that is geometrically invariant to large pose variations. In this study, we hypothesize that there is an inherent mapping between frontal and profile faces, and consequently, their discrepancy in the deep representation space can be bridged by an equivariant mapping. To exploit this mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block, which is capable of adaptively adding residuals to the input deep representation to transform a profile face representation to a canonical pose that simplifies recognition. The DREAM block consistently enhances the performance of profile face recognition for many strong deep networks, including ResNet models, without deliberately augmenting training data of profile faces. The block is easy to use, light-weight, and can be implemented with a negligible computational overhead",
    "checked": true,
    "id": "d0f690b9ad1d66e06e4da18381d92443e9d15f11",
    "semantic_title": "pose-robust face recognition via deep residual equivariant mapping",
    "citation_count": 157,
    "authors": [
      "Kaidi Cao",
      "Yu Rong",
      "Cheng Li",
      "Xiaoou Tang",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_DecideNet_Counting_Varying_CVPR_2018_paper.html": {
    "title": "DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation",
    "volume": "main",
    "abstract": "In real-world crowd counting applications, the crowd densities vary greatly in spatial and temporal domains. A detection based counting method will estimate crowds accurately in low density scenes, while its reliability in congested areas is downgraded. A regression based approach, on the other hand, captures the general density information in crowded regions. Without knowing the location of each person, it tends to overestimate the count in low density areas. Thus, exclusively using either one of them is not sufficient to handle all kinds of scenes with varying densities. To address this issue, a novel end-to-end crowd counting framework, named DecideNet (DEteCtIon and Density Estimation Network) is proposed. It can adaptively decide the appropriate counting mode for different locations on the image based on its real density conditions. DecideNet starts with estimating the crowd density by generating detection and regression based density maps separately. To capture inevitable variation in densities, it incorporates an attention module, meant to adaptively assess the reliability of the two types of estimations. The final crowd counts are obtained with the guidance of the attention module to adopt suitable estimations from the two kinds of density maps. Experimental results show that our method achieves state-of-the-art performance on three challenging crowd counting datasets",
    "checked": true,
    "id": "9c3fd2815932a97f6813ed51c575b2f0cb394854",
    "semantic_title": "decidenet: counting varying density crowds through attention guided detection and density estimation",
    "citation_count": 321,
    "authors": [
      "Jiang Liu",
      "Chenqiang Gao",
      "Deyu Meng",
      "Alexander G. Hauptmann"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_LSTM_Pose_Machines_CVPR_2018_paper.html": {
    "title": "LSTM Pose Machines",
    "volume": "main",
    "abstract": "We observed that recent state-of-the-art results on single image human pose estimation were achieved by multi-stage Convolution Neural Networks (CNN). Notwithstanding the superior performance on static images, the application of these models on videos is not only computationally intensive, it also suffers from performance degeneration and flicking. Such suboptimal results are mainly attributed to the inability of imposing sequential geometric consistency, handling severe image quality degradation (e.g. motion blur and occlusion) as well as the inability of capturing the temporal correlation among video frames. In this paper, we proposed a novel recurrent network to tackle these problems. We showed that if we were to impose the weight sharing scheme to the multi-stage CNN, it could be re-written as a Recurrent Neural Network (RNN). This property decouples the relationship among multiple network stages and results in significantly faster speed in invoking the network for videos. It also enables the adoption of Long Short-Term Memory (LSTM) units between video frames. We found such memory augmented RNN is very effective in imposing geometric consistency among frames. It also well handles input quality degradation in videos while successfully stabilizes the sequential outputs. The experiments showed that our approach significantly outperformed current state-of-the-art methods on two large-scale video pose estimation benchmarks. We also explored the memory cells inside the LSTM and provided insights on why such mechanism would benefit the prediction for video-based pose estimations",
    "checked": true,
    "id": "038251b7c6ffee94181dd6584ae2a923a6b0be07",
    "semantic_title": "lstm pose machines",
    "citation_count": 120,
    "authors": [
      "Yue Luo",
      "Jimmy Ren",
      "Zhouxia Wang",
      "Wenxiu Sun",
      "Jinshan Pan",
      "Jianbo Liu",
      "Jiahao Pang",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Disentangling_Features_in_CVPR_2018_paper.html": {
    "title": "Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition",
    "volume": "main",
    "abstract": "This paper proposes an encoder-decoder network to disentangle shape features during 3D face shape reconstruction from single 2D images, such that the tasks of learning discriminative shape features for face recognition and reconstructing accurate 3D face shapes can be done simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. We develop a multi image 3D morphable model (3DMM) fitting method for multiple 2D images of a subject to construct training data. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy",
    "checked": true,
    "id": "6be2522fa708de1334fea647c9151149d16ec9a1",
    "semantic_title": "disentangling features in 3d face shapes for joint face reconstruction and recognition",
    "citation_count": 112,
    "authors": [
      "Feng Liu",
      "Ronghang Zhu",
      "Dan Zeng",
      "Qijun Zhao",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Convolutional_Sequence_to_CVPR_2018_paper.html": {
    "title": "Convolutional Sequence to Sequence Model for Human Dynamics",
    "volume": "main",
    "abstract": "Human motion modeling is a classic problem in com- puter vision and graphics. Challenges in modeling human motion include high dimensional prediction as well as extremely complicated dynamics.We present a novel approach to human motion modeling based on convolutional neural networks (CNN). The hierarchical structure of CNN makes it capable of capturing both spatial and temporal correlations effectively. In our proposed approach, a convolutional long-term encoder is used to encode the whole given motion sequence into a long-term hidden variable, which is used with a decoder to predict the remainder of the sequence. The decoder itself also has an encoder-decoder structure, in which the short-term encoder encodes a shorter sequence to a short-term hidden variable, and the spatial decoder maps the long and short-term hidden variable to motion predictions. By using such a model, we are able to capture both invariant and dynamic information of human motion, which results in more accurate predictions. Experiments show that our algorithm outperforms the state-of-the-art methods on the Human3.6M and CMU Motion Capture datasets. Our code is available at the project website",
    "checked": true,
    "id": "575d58c9d74a9a7cc62954c755309c7e37498baf",
    "semantic_title": "convolutional sequence to sequence model for human dynamics",
    "citation_count": 277,
    "authors": [
      "Chen Li",
      "Zhen Zhang",
      "Wee Sun Lee",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Narayana_Gesture_Recognition_Focus_CVPR_2018_paper.html": {
    "title": "Gesture Recognition: Focus on the Hands",
    "volume": "main",
    "abstract": "Gestures are a common form of human communication and important for human computer interfaces (HCI). Recent approaches to gesture recognition use deep learning methods, including multi-channel methods. We show that when spatial channels are focused on the hands, gesture recognition improves significantly, particularly when the channels are fused using a sparse network. Using this technique, we improve performance on the ChaLearn IsoGD dataset from a previous best of 67.71% to 82.07%, and on the NVIDIA dataset from 83.8% to 91.28%",
    "checked": true,
    "id": "1eb3134ad1dd03db46111f27c10fcadf9e9a9b73",
    "semantic_title": "gesture recognition: focus on the hands",
    "citation_count": 121,
    "authors": [
      "Pradyumna Narayana",
      "Ross Beveridge",
      "Bruce A. Draper"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Crowd_Counting_via_CVPR_2018_paper.html": {
    "title": "Crowd Counting via Adversarial Cross-Scale Consistency Pursuit",
    "volume": "main",
    "abstract": "Crowd counting or density estimation is a challenging task in computer vision due to large scale variations, perspective distortions and serious occlusions, etc. Existing methods generally suffers from two issues: 1) the model averaging effects in multi-scale CNNs induced by the widely adopted L2 regression loss; and 2) inconsistent estimation across different scaled inputs. To explicitly address these issues, we propose a novel crowd counting (density estimation) framework called Adversarial Cross-Scale Consistency Pursuit (ACSCP). On one hand, a U-net structural network is designed to generate density map from input patch, and an adversarial loss is employed to shrink the solution onto a realistic subspace, thus attenuating the blurry effects of density map estimation. On the other hand, we design a novel scale-consistency regularizer which enforces that the sum up of the crowd counts from local patches (i.e., small scale) is coherent with the overall count of their region union (i.e., large scale). The above losses are integrated via a joint training scheme, so as to help boost density estimation performance by further exploring the collaboration between both objectives. Extensive experiments on four benchmarks have well demonstrated the effectiveness of the proposed innovations as well as the superior performance over prior art",
    "checked": true,
    "id": "5101de7052206087dc256660d72cb05d0553b24c",
    "semantic_title": "crowd counting via adversarial cross-scale consistency pursuit",
    "citation_count": 315,
    "authors": [
      "Zan Shen",
      "Yi Xu",
      "Bingbing Ni",
      "Minsi Wang",
      "Jianguo Hu",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_3D_Human_Pose_CVPR_2018_paper.html": {
    "title": "3D Human Pose Estimation in the Wild by Adversarial Learning",
    "volume": "main",
    "abstract": "Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DCNNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difficult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of defining hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efficacy of our adversarial learning framework with the new geometric descriptor have been demonstrated through extensive experiments on two widely used public benchmarks. Our approach significantly improves the performance compared with previous state-of-the-art approaches",
    "checked": true,
    "id": "02cb44528213f4d5bd38fb2dcb59002680301e8f",
    "semantic_title": "3d human pose estimation in the wild by adversarial learning",
    "citation_count": 346,
    "authors": [
      "Wei Yang",
      "Wanli Ouyang",
      "Xiaolong Wang",
      "Jimmy Ren",
      "Hongsheng Li",
      "Xiaogang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_CosFace_Large_Margin_CVPR_2018_paper.html": {
    "title": "CosFace: Large Margin Cosine Loss for Deep Face Recognition",
    "volume": "main",
    "abstract": "Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by L2 normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which confirms the effectiveness of our proposed approach",
    "checked": true,
    "id": "9dc915697768dd1f7c7b97e2c25c90b02241958b",
    "semantic_title": "cosface: large margin cosine loss for deep face recognition",
    "citation_count": 2503,
    "authors": [
      "Hao Wang",
      "Yitong Wang",
      "Zheng Zhou",
      "Xing Ji",
      "Dihong Gong",
      "Jingchao Zhou",
      "Zhifeng Li",
      "Wei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Encoding_Crowd_Interaction_CVPR_2018_paper.html": {
    "title": "Encoding Crowd Interaction With Deep Neural Network for Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "Pedestrian trajectory prediction is a challenging task because of the complex nature of humans. In this paper, we tackle the problem within a deep learning framework by considering motion information of each pedestrian and its interaction with the crowd. Specifically, motivated by the residual learning in deep learning, we propose to predict displacement between neighboring frames for each pedestrian sequentially. To predict such displacement, we design a crowd interaction deep neural network (CIDNN) which considers the different importance of different pedestrians for the displacement prediction of a target pedestrian. Specifically, we use an LSTM to model motion information for all pedestrians and use a multi-layer perceptron to map the location of each pedestrian to a high dimensional feature space where the inner product between features is used as a measurement for the spatial affinity between two pedestrians. Then we weight the motion features of all pedestrians based on their spatial affinity to the target pedestrian for location displacement prediction. Extensive experiments on publicly available datasets validate the effectiveness of our method for trajectory prediction",
    "checked": true,
    "id": "31bac8e29c25d19c11303440834e59ca8ae5a45c",
    "semantic_title": "encoding crowd interaction with deep neural network for pedestrian trajectory prediction",
    "citation_count": 214,
    "authors": [
      "Yanyu Xu",
      "Zhixin Piao",
      "Shenghua Gao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Mean-Variance_Loss_for_CVPR_2018_paper.html": {
    "title": "Mean-Variance Loss for Deep Age Estimation From a Face",
    "volume": "main",
    "abstract": "Age estimation has broad application prospects of many fields, such as video surveillance, social networking, and human-computer interaction. However, many of the published age estimation approaches simply treat the age estimation as an exact age regression problem, and thus did not leverage a distribution's robustness in representing labels with ambiguity such as ages. In this paper, we propose a new loss function, called mean-variance loss, for robust age estimation via distribution learning. Specifically, the mean-variance loss consists of a mean loss, which penalizes difference between the mean of the estimated age distribution and the ground-truth age, and a variance loss, which penalizes the variance of the estimated age distribution to ensure a concentrated distribution. The proposed mean-variance loss and softmax loss are embedded jointly into Convolutional Neural Networks (CNNs) for age estimation, and the network weights are optimized via stochastic gradient descent (SGD) in an end-to-end learning way. Experimental results on a number of challenging face aging databases (FG-NET, MORPH Album II, and CLAP2016) show that the proposed approach outperforms the state-of-the-art methods by a large margin using a single model",
    "checked": true,
    "id": "bc8b7e63619563df83b3834d2cd722bf43f57091",
    "semantic_title": "mean-variance loss for deep age estimation from a face",
    "citation_count": 204,
    "authors": [
      "Hongyu Pan",
      "Hu Han",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Madsen_Probabilistic_Joint_Face-Skull_CVPR_2018_paper.html": {
    "title": "Probabilistic Joint Face-Skull Modelling for Facial Reconstruction",
    "volume": "main",
    "abstract": "We present a novel method for co-registration of two independent statistical shape models. We solve the problem of aligning a face model to a skull model with stochastic optimization based on Markov Chain Monte Carlo (MCMC). We create a probabilistic joint face-skull model and show how to obtain a distribution of plausible face shapes given a skull shape. Due to environmental and genetic factors, there exists a distribution of possible face shapes arising from the same skull. We pose facial reconstruction as a conditional distribution of plausible face shapes given a skull shape. Because it is very difficult to obtain the distribution directly from MRI or CT data, we create a dataset of artificial face-skull pairs. To do this, we propose to combine three data sources of independent origin to model the joint face-skull distribution: a face shape model, a skull shape model and tissue depth marker information. For a given skull, we compute the posterior distribution of faces matching the tissue depth distribution with Metropolis-Hastings. We estimate the joint face-skull distribution from samples of the posterior. To find faces matching to an unknown skull, we estimate the probability of the face under the joint face-skull model. To our knowledge, we are the first to provide a whole distribution of plausible faces arising from a skull instead of only a single reconstruction. We show how the face-skull model can be used to rank a face dataset and on average successfully identify the correct match in top 30%. The face ranking even works when obtaining the face shapes from 2D images. We furthermore show how the face-skull model can be useful to estimate the skull position in an MR-image",
    "checked": true,
    "id": "d9174e6a3860a2f1f5334112b0529ec64790b178",
    "semantic_title": "probabilistic joint face-skull modelling for facial reconstruction",
    "citation_count": 16,
    "authors": [
      "Dennis Madsen",
      "Marcel Lüthi",
      "Andreas Schneider",
      "Thomas Vetter"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper.html": {
    "title": "Learning Latent Super-Events to Detect Multiple Activities in Videos",
    "volume": "main",
    "abstract": "In this paper, we introduce the concept of learning latent super-events from activity videos, and present how it benefits activity detection in continuous videos. We define a super-event as a set of multiple events occurring together in videos with a particular temporal organization; it is the opposite concept of sub-events. Real-world videos contain multiple activities and are rarely segmented (e.g., surveillance videos), and learning latent super-events allows the model to capture how the events are temporally related in videos. We design emph{temporal structure filters} that enable the model to focus on particular sub-intervals of the videos, and use them together with a soft attention mechanism to learn representations of latent super-events. Super-event representations are combined with per-frame or per-segment CNNs to provide frame-level annotations. Our approach is designed to be fully differentiable, enabling end-to-end learning of latent super-event representations jointly with the activity detector using them. Our experiments with multiple public video datasets confirm that the proposed concept of latent super-event learning significantly benefits activity detection, advancing the state-of-the-arts",
    "checked": true,
    "id": "1c4d2759eb491073d2c26c3193fd62cd9cabd091",
    "semantic_title": "learning latent super-events to detect multiple activities in videos",
    "citation_count": 90,
    "authors": [
      "AJ Piergiovanni",
      "Michael S. Ryoo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Temporal_Hallucinating_for_CVPR_2018_paper.html": {
    "title": "Temporal Hallucinating for Action Recognition With Few Still Images",
    "volume": "main",
    "abstract": "Action recognition in still images has been recently promoted by deep learning. However, the success of these deep models heavily depends on huge amount of training images for various action categories, which may not be available in practice. Alternatively, humans can classify new action categories after seeing few images, since we may not only compare appearance similarities between images on hand, but also attempt to recall importance motion cues from relevant action videos in our memory. To mimic this capacity, we propose a novel Hybrid Video Memory (HVM) machine, which can hallucinate temporal features of still images from video memory, in order to boost action recognition with few still images. First, we design a temporal memory module consisting of temporal hallucinating and predicting. Temporal hallucinating can generate temporal features of still images in an unsupervised manner. Hence, it can be flexibly used in realistic scenarios, where image and video categories may not be consistent. Temporal predicting can effectively infer action categories for query image, by integrating temporal features of training images and videos within a domain-adaptation manner. Second, we design a spatial memory module for spatial predicting. As spatial and temporal features are complementary to represent different actions, we apply spatial-temporal prediction fusion to further boost performance. Finally, we design a video selection module to select strongly-relevant videos as memory. In this case, we can balance the number of images and videos to reduce prediction bias as well as preserve computation efficiency. To show the effectiveness, we conduct extensive experiments on three challenging data sets, where our HVM outperforms a number of recent approaches by temporal hallucinating from video memory",
    "checked": true,
    "id": "1b8b9332886ea661e5a46bb87118956f1f4c15f3",
    "semantic_title": "temporal hallucinating for action recognition with few still images",
    "citation_count": 21,
    "authors": [
      "Yali Wang",
      "Lei Zhou",
      "Yu Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html": {
    "title": "Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a deep progressive reinforcement learning (DPRL) method for action recognition in skeleton-based videos, which aims to distil the most informative frames and discard ambiguous frames in sequences for recognizing actions. Since the choices of selecting representative frames are multitudinous for each video, we model the frame selection as a progressive process through deep reinforcement learning, during which we progressively adjust the chosen frames by taking two important factors into account: (1) the quality of the selected frames and (2) the relationship between the selected frames to the whole video. Moreover, considering the topology of human body inherently lies in a graph-based structure, where the vertices and edges represent the hinged joints and rigid bones respectively, we employ the graph-based convolutional neural network to capture the dependency between the joints for action recognition. Our approach achieves very competitive performance on three widely used benchmarks",
    "checked": true,
    "id": "9bfefdf18016e146f2afb38f1807dffabb84a7c6",
    "semantic_title": "deep progressive reinforcement learning for skeleton-based action recognition",
    "citation_count": 359,
    "authors": [
      "Yansong Tang",
      "Yi Tian",
      "Jiwen Lu",
      "Peiyang Li",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Gaze_Prediction_in_CVPR_2018_paper.html": {
    "title": "Gaze Prediction in Dynamic 360° Immersive Videos",
    "volume": "main",
    "abstract": "This paper explores gaze prediction in dynamic $360^circ$ immersive videos, emph{i.e.}, based on the history scan path and VR contents, we predict where a viewer will look at an upcoming time. To tackle this problem, we first present the large-scale eye-tracking in dynamic VR scene dataset. Our dataset contains 208 $360^circ$ videos captured in dynamic scenes, and each video is viewed by at least 31 subjects. Our analysis shows that gaze prediction depends on its history scan path and image contents. In terms of the image contents, those salient objects easily attract viewers' attention. On the one hand, the saliency is related to both appearance and motion of the objects. Considering that the saliency measured at different scales is different, we propose to compute saliency maps at different spatial scales: the sub-image patch centered at current gaze point, the sub-image corresponding to the Field of View (FoV), and the panorama image. Then we feed both the saliency maps and the corresponding images into a Convolutional Neural Network (CNN) for feature extraction. Meanwhile, we also use a Long-Short-Term-Memory (LSTM) to encode the history scan path. Then we combine the CNN features and LSTM features for gaze displacement prediction between gaze point at a current time and gaze point at an upcoming time. Extensive experiments validate the effectiveness of our method for gaze prediction in dynamic VR scenes",
    "checked": true,
    "id": "cc3bd1659d005ee25c26e3f6aab3c7361f6172b7",
    "semantic_title": "gaze prediction in dynamic 360° immersive videos",
    "citation_count": 261,
    "authors": [
      "Yanyu Xu",
      "Yanbing Dong",
      "Junru Wu",
      "Zhengzhong Sun",
      "Zhiru Shi",
      "Jingyi Yu",
      "Shenghua Gao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Abu_Farha_When_Will_You_CVPR_2018_paper.html": {
    "title": "When Will You Do What? - Anticipating Temporal Occurrences of Activities",
    "volume": "main",
    "abstract": "Analyzing human actions in videos has gained increased attention recently. While most works focus on classifying and labeling observed video frames or anticipating the very recent future, making long-term predictions over more than just a few seconds is a task with many practical applications that has not yet been addressed. In this paper, we propose two methods to predict a considerably large amount of future actions and their durations. Both, a CNN and an RNN are trained to learn future video labels based on previously seen content. We show that our methods generate accurate predictions of the future even for long videos with a huge amount of different actions and can even deal with noisy or erroneous input information",
    "checked": true,
    "id": "33b1843afc8b76314c9fbe11ca11c23fa0966c08",
    "semantic_title": "when will you do what? - anticipating temporal occurrences of activities",
    "citation_count": 191,
    "authors": [
      "Yazan Abu Farha",
      "Alexander Richard",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_Fusing_Crowd_Density_CVPR_2018_paper.html": {
    "title": "Fusing Crowd Density Maps and Visual Object Trackers for People Tracking in Crowd Scenes",
    "volume": "main",
    "abstract": "While people tracking has been greatly improved over the recent years, crowd scenes remain particularly challenging for people tracking due to heavy occlusions, high crowd density, and significant appearance variation. To address these challenges, we first design a Sparse Kernelized Correlation Filter (S-KCF) to suppress target response variations caused by occlusions and illumination changes, and spurious responses due to similar distractor objects. We then propose a people tracking framework that fuses the S-KCF response map with an estimated crowd density map using a convolutional neural network (CNN), yielding a refined response map. To train the fusion CNN, we propose a two-stage strategy to gradually optimize the parameters. The first stage is to train a preliminary model in batch mode with image patches selected around the targets, and the second stage is to fine-tune the preliminary model using the real frame-by-frame tracking process. Our density fusion framework can significantly improves people tracking in crowd scenes, and can also be combined with other trackers to improve the tracking performance. We validate our framework on two crowd video datasets: UCSD and PETS2009",
    "checked": true,
    "id": "216bf8374a6fd745e623750583bb2ffbae6e7edd",
    "semantic_title": "fusing crowd density maps and visual object trackers for people tracking in crowd scenes",
    "citation_count": 21,
    "authors": [
      "Weihong Ren",
      "Di Kang",
      "Yandong Tang",
      "Antoni B. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Si_Dual_Attention_Matching_CVPR_2018_paper.html": {
    "title": "Dual Attention Matching Network for Context-Aware Feature Sequence Based Person Re-Identification",
    "volume": "main",
    "abstract": "Typical person re-identification (ReID) methods usually describe each pedestrian with a single feature vector and match them in a task-specific metric space. However, the methods based on a single feature vector are not sufficient enough to overcome visual ambiguity, which frequently occurs in real scenario. In this paper, we propose a novel end-to-end trainable framework, called Dual ATtention Matching network (DuATM), to learn context-aware feature sequences and perform attentive sequence comparison simultaneously. The core component of our DuATM framework is a dual attention mechanism, in which both intra-sequence and inter-sequence attention strategies are used for feature refinement and feature-pair alignment, respectively. Thus, detailed visual cues contained in the intermediate feature sequences can be automatically exploited and properly compared. We train the proposed DuATM network as a siamese network via a triplet loss assisted with a de-correlation loss and a cross-entropy loss. We conduct extensive experiments on both image and video based ReID benchmark datasets. Experimental results demonstrate the significant advantages of our approach compared to the state-of-the-art methods",
    "checked": true,
    "id": "7daa2c0f76fd3bfc7feadf313d6ac7504d4ecd20",
    "semantic_title": "dual attention matching network for context-aware feature sequence based person re-identification",
    "citation_count": 466,
    "authors": [
      "Jianlou Si",
      "Honggang Zhang",
      "Chun-Guang Li",
      "Jason Kuen",
      "Xiangfei Kong",
      "Alex C. Kot",
      "Gang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Easy_Identification_From_CVPR_2018_paper.html": {
    "title": "Easy Identification From Better Constraints: Multi-Shot Person Re-Identification From Reference Constraints",
    "volume": "main",
    "abstract": "Multi-shot person re-identification (MsP-RID) utilizes multiple images from the same person to facilitate identification. Considering the fact that motion information may not be discriminative nor reliable enough for MsP-RID, this paper is focused on handling the large variations in the visual appearances through learning discriminative visual metrics for identification. Existing metric learning-based methods usually exploit pair-wise or triple-wise similarity constraints, that generally demands intensive optimization in metric learning, or leads to degraded performances by using sub-optimal solutions. In addition, as the training data are significantly imbalanced, the learning can be largely dominated by the negative pairs and thus produces unstable and non-discriminative results. In this paper, we propose a novel type of similarity constraint. It assigns the sample points to a set of extbf{reference points} to produce a linear number of extbf{reference constraints}. Several optimal transport-based schemes for reference constraint generation are proposed and studied. Based on those constraints, by utilizing a typical regressive metric learning model, the closed-form solution of the learned metric can be easily obtained. Extensive experiments and comparative studies on several public MsP-RID benchmarks have validated the effectiveness of our method and its significant superiority over the state-of-the-art MsP-RID methods in terms of both identification accuracy and running speed",
    "checked": true,
    "id": "7f8d4494aba2a2b11a88bf7de4b8879b047dd69b",
    "semantic_title": "easy identification from better constraints: multi-shot person re-identification from reference constraints",
    "citation_count": 16,
    "authors": [
      "Jiahuan Zhou",
      "Bing Su",
      "Ying Wu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shi_Crowd_Counting_With_CVPR_2018_paper.html": {
    "title": "Crowd Counting With Deep Negative Correlation Learning",
    "volume": "main",
    "abstract": "Deep convolutional networks (ConvNets) have achieved unprecedented performances on many computer vision tasks. However, their adaptations to crowd counting on single images are still in their infancy and suffer from severe over-fitting. Here we propose a new learning strategy to produce generalizable features by way of deep negative correlation learning (NCL). More specifically, we deeply learn a pool of decorrelated regressors with sound generalization capabilities through managing their intrinsic diversities. Our proposed method, named decorrelated ConvNet (D-ConvNet), is end-to-end-trainable and independent of the backbone fully-convolutional network architectures. Extensive experiments on very deep VGGNet as well as our customized network structure indicate the superiority of D-ConvNet when compared with several state-of-the-art methods. Our implementation will be released at https://github.com/shizenglin/Deep-NCL",
    "checked": true,
    "id": "2ab8a7654f943c9d19e02eee0b2ce3b12d6bc3c1",
    "semantic_title": "crowd counting with deep negative correlation learning",
    "citation_count": 249,
    "authors": [
      "Zenglin Shi",
      "Le Zhang",
      "Yun Liu",
      "Xiaofeng Cao",
      "Yangdong Ye",
      "Ming-Ming Cheng",
      "Guoyan Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zanfir_Human_Appearance_Transfer_CVPR_2018_paper.html": {
    "title": "Human Appearance Transfer",
    "volume": "main",
    "abstract": "We propose an automatic person-to-person appearance transfer model based on explicit parametric 3d human representations and learned, constrained deep translation network architectures for photographic image synthesis. Given a single source image and a single target image, each corresponding to different human subjects, wearing different clothing and in different poses, our goal is to photo-realistically transfer the appearance from the source image onto the target image while preserving the target shape and clothing segmentation layout. Our solution to this new problem is formulated in terms of a computational pipeline that combines (1) 3d human pose and body shape estimation from monocular images, (2) identifying 3d surface colors elements (mesh triangles) visible in both images, that can be transferred directly using barycentric procedures, and (3) predicting surface appearance missing in the first image but visible in the second one using deep learning-based image synthesis techniques. Our model achieves promising results as supported by a perceptual user study where the participants rated around 65% of our results as good, very good or perfect, as well in automated tests (Inception scores and a Faster-RCNN human detector responding very similarly to real and model generated images). We further show how the proposed architecture can be profiled to automatically generate images of a person dressed with different clothing transferred from a person in another image, opening paths for applications in entertainment and photo-editing (e.g. embodying and posing as friends or famous actors), the fashion industry, or affordable online shopping of clothing",
    "checked": true,
    "id": "1185cec63813ddd371bc4b5e6c27998b6ae4b336",
    "semantic_title": "human appearance transfer",
    "citation_count": 93,
    "authors": [
      "Mihai Zanfir",
      "Alin-Ionut Popa",
      "Andrei Zanfir",
      "Cristian Sminchisescu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Domain_Generalization_With_CVPR_2018_paper.html": {
    "title": "Domain Generalization With Adversarial Feature Learning",
    "volume": "main",
    "abstract": "In this paper, we tackle the problem of domain generalization: how to learn a generalized feature representation for an \"unseen\" target domain by taking the advantage of multiple seen source-domain data. We present a novel framework based on adversarial autoencoders to learn a generalized latent feature representation across domains for domain generalization. To be specific, we extend adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to an arbitrary prior distribution via adversarial feature learning. In this way, the learned feature representation is supposed to be universal to the seen source domains because of the MMD regularization, and is expected to generalize well on the target domain because of the introduction of the prior distribution. We proposed an algorithm to jointly train different components of our proposed framework. Extensive experiments on various vision tasks demonstrate that our proposed framework can learn better generalized features for the unseen target domain compared with state of-the-art domain generalization methods",
    "checked": true,
    "id": "ba6ba7f488c1ece0803f4b9e1c83a3196d061610",
    "semantic_title": "domain generalization with adversarial feature learning",
    "citation_count": 1228,
    "authors": [
      "Haoliang Li",
      "Sinno Jialin Pan",
      "Shiqi Wang",
      "Alex C. Kot"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chang_Pyramid_Stereo_Matching_CVPR_2018_paper.html": {
    "title": "Pyramid Stereo Matching Network",
    "volume": "main",
    "abstract": "Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in ill-posed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet",
    "checked": true,
    "id": "316b1b9d96149e7bb3d9d6afc0295881c6123cc8",
    "semantic_title": "pyramid stereo matching network",
    "citation_count": 1507,
    "authors": [
      "Jia-Ren Chang",
      "Yong-Sheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Maqueda_Event-Based_Vision_Meets_CVPR_2018_paper.html": {
    "title": "Event-Based Vision Meets Deep Learning on Steering Prediction for Self-Driving Cars",
    "volume": "main",
    "abstract": "Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle's steering angle. To make the best out of this sensor–algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset (≈1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras",
    "checked": true,
    "id": "332ed1143ec551827f188c951f49f5b1823381fc",
    "semantic_title": "event-based vision meets deep learning on steering prediction for self-driving cars",
    "citation_count": 503,
    "authors": [
      "Ana I. Maqueda",
      "Antonio Loquercio",
      "Guillermo Gallego",
      "Narciso García",
      "Davide Scaramuzza"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Learning_Answer_Embeddings_CVPR_2018_paper.html": {
    "title": "Learning Answer Embeddings for Visual Question Answering",
    "volume": "main",
    "abstract": "We propose a novel probabilistic model for visual question answering (Visual QA). The key idea is to infer two sets of embeddings: one for the image and the question jointly and the other for the answers. The learning objective is to learn the best parameterization of those embeddings such that the correct answer has higher likelihood among all possible answers. In contrast to several existing approaches of treating Visual QA as multi-way classification, the proposed approach takes the semantic relationships (as characterized by the embeddings) among answers into consideration, instead of viewing them as independent ordinal numbers. Thus, the learned embedded function can be used to embed unseen answers (in the training dataset). These properties make the approach particularly appealing for transfer learning for open-ended Visual QA, where the source dataset on which the model is learned has limited overlapping with the target dataset in the space of answers. We have also developed large-scale optimization techniques for applying the model to datasets with a large number of answers, where the challenge is to properly normalize the proposed probabilistic models. We validate our approach on several Visual QA datasets and investigate its utility for transferring models across datasets. The empirical results have shown that the approach performs well not only on in-domain learning but also on transfer learning",
    "checked": true,
    "id": "04427d8371cb9e66e2cdcd2035756203398a8bf1",
    "semantic_title": "learning answer embeddings for visual question answering",
    "citation_count": 33,
    "authors": [
      "Hexiang Hu",
      "Wei-Lun Chao",
      "Fei Sha"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Good_View_Hunting_CVPR_2018_paper.html": {
    "title": "Good View Hunting: Learning Photo Composition From Dense View Pairs",
    "volume": "main",
    "abstract": "Finding views with good photo composition is a challenging task for machine learning methods. A key difficulty is the lack of well annotated large scale datasets. Most existing datasets only provide a limited number of annotations for good views, while ignoring the comparative nature of view selection. In this work, we present the first large scale Comparative Photo Composition dataset, which contains over one million comparative view pairs annotated using a cost-effective crowdsourcing workflow. We show that these comparative view annotations are essential for training a robust neural network model for composition. In addition, we propose a novel knowledge transfer framework to train a fast view proposal network, which runs at 75+ FPS and achieves state-of-the-art performance in image cropping and thumbnail generation tasks on three benchmark datasets. The superiority of our method is also demonstrated in a user study on a challenging experiment, where our method significantly outperforms the baseline methods in producing diversified well-composed views",
    "checked": true,
    "id": "b20dc4906fc021aaf8353df98f8828d99f951c3c",
    "semantic_title": "good view hunting: learning photo composition from dense view pairs",
    "citation_count": 66,
    "authors": [
      "Zijun Wei",
      "Jianming Zhang",
      "Xiaohui Shen",
      "Zhe Lin",
      "Radomír Mech",
      "Minh Hoai",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_CleanNet_Transfer_Learning_CVPR_2018_paper.html": {
    "title": "CleanNet: Transfer Learning for Scalable Image Classifier Training With Label Noise",
    "volume": "main",
    "abstract": "In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task. Source code and dataset will be available at kuanghuei.github.io/CleanNetProject",
    "checked": true,
    "id": "b4bd88c2a349d29f7aeaf5c1fcb355021058b3fa",
    "semantic_title": "cleannet: transfer learning for scalable image classifier training with label noise",
    "citation_count": 455,
    "authors": [
      "Kuang-Huei Lee",
      "Xiaodong He",
      "Lei Zhang",
      "Linjun Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Independently_Recurrent_Neural_CVPR_2018_paper.html": {
    "title": "Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN",
    "volume": "main",
    "abstract": "Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM",
    "checked": true,
    "id": "565ab57eede8bf6ef9c42df51216b9f85287c234",
    "semantic_title": "independently recurrent neural network (indrnn): building a longer and deeper rnn",
    "citation_count": 729,
    "authors": [
      "Shuai Li",
      "Wanqing Li",
      "Chris Cook",
      "Ce Zhu",
      "Yanbo Gao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Mix_and_Match_CVPR_2018_paper.html": {
    "title": "Mix and Match Networks: Encoder-Decoder Alignment for Zero-Pair Image Translation",
    "volume": "main",
    "abstract": "We address the problem of image translation between domains or modalities for which no direct paired data is available (i.e. zero-pair translation). We propose mix and match networks, based on multiple encoders and decoders aligned in such a way that other encoder-decoder pairs can be composed at test time to perform unseen image translation tasks between domains or modalities for which explicit paired samples were not seen during training. We study the impact of autoencoders, side information and losses in improving the alignment and transferability of trained pairwise translation models to unseen translations. We show our approach is scalable and can perform colorization and style transfer between unseen combinations of domains. We evaluate our system in a challenging cross-modal setting where semantic segmentation is estimated from depth images, without explicit access to any depth-semantic segmentation training pairs. Our model outperforms baselines based on pix2pix and CycleGAN models",
    "checked": true,
    "id": "18a429184af3d181e2de53d60a38e0314c25db5b",
    "semantic_title": "mix and match networks: encoder-decoder alignment for zero-pair image translation",
    "citation_count": 34,
    "authors": [
      "Yaxing Wang",
      "Joost van de Weijer",
      "Luis Herranz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dorta_Structured_Uncertainty_Prediction_CVPR_2018_paper.html": {
    "title": "Structured Uncertainty Prediction Networks",
    "volume": "main",
    "abstract": "This paper is the first work to propose a network to predict a structured uncertainty distribution for a synthesized image. Previous approaches have been mostly limited to predicting diagonal covariance matrices. Our novel model learns to predict a full Gaussian covariance matrix for each reconstruction, which permits efficient sampling and likelihood evaluation. We demonstrate that our model can accurately reconstruct ground truth correlated residual distributions for synthetic datasets and generate plausible high frequency samples for real face images. We also illustrate the use of these predicted covariances for structure preserving image denoising",
    "checked": true,
    "id": "8eebdf2cd35c708a24c8123cb4d750cc4417acaf",
    "semantic_title": "structured uncertainty prediction networks",
    "citation_count": 63,
    "authors": [
      "Garoe Dorta",
      "Sara Vicente",
      "Lourdes Agapito",
      "Neill D. F. Campbell",
      "Ivor Simpson"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tokozume_Between-Class_Learning_for_CVPR_2018_paper.html": {
    "title": "Between-Class Learning for Image Classification",
    "volume": "main",
    "abstract": "In this paper, we propose a novel learning method for image classification called Between-Class learning (BC learning). We generate between-class images by mixing two images belonging to different classes with a random ratio. We then input the mixed image to the model and train the model to output the mixing ratio. BC learning has the ability to impose constraints on the shape of the feature distributions, and thus the generalization ability is improved. BC learning is originally a method developed for sounds, which can be digitally mixed. Mixing two image data does not appear to make sense; however, we argue that because convolutional neural networks have an aspect of treating input data as waveforms, what works on sounds must also work on images. First, we propose a simple mixing method using internal divisions, which surprisingly proves to significantly improve performance. Second, we propose a mixing method that treats the images as waveforms, which leads to a further improvement in performance. As a result, we achieved 19.4% and 2.26% top-1 errors on ImageNet-1K and CIFAR-10, respectively",
    "checked": true,
    "id": "e41bbabd84bfbce6ef66825c1a2d7eb869bd1202",
    "semantic_title": "between-class learning for image classification",
    "citation_count": 205,
    "authors": [
      "Yuji Tokozume",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper.html": {
    "title": "Adversarial Feature Augmentation for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks",
    "checked": true,
    "id": "42563d601d30bb74a15855ef912692e45c72340e",
    "semantic_title": "adversarial feature augmentation for unsupervised domain adaptation",
    "citation_count": 234,
    "authors": [
      "Riccardo Volpi",
      "Pietro Morerio",
      "Silvio Savarese",
      "Vittorio Murino"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Generative_Image_Inpainting_CVPR_2018_paper.html": {
    "title": "Generative Image Inpainting With Contextual Attention",
    "volume": "main",
    "abstract": "Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting",
    "checked": true,
    "id": "6b0bbf3e7df725cc3b781d2648e41782cb3d8539",
    "semantic_title": "generative image inpainting with contextual attention",
    "citation_count": 2265,
    "authors": [
      "Jiahui Yu",
      "Zhe Lin",
      "Jimei Yang",
      "Xiaohui Shen",
      "Xin Lu",
      "Thomas S. Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sharma_CSGNet_Neural_Shape_CVPR_2018_paper.html": {
    "title": "CSGNet: Neural Shape Parser for Constructive Solid Geometry",
    "volume": "main",
    "abstract": "We present a neural architecture that takes as input a 2D or 3D shape and outputs a program that generates the shape. The instructions in our program are based on constructive solid geometry principles, i.e., a set of boolean operations on shape primitives defined recursively. Bottom-up techniques for this shape parsing task rely on primitive detection and are inherently slow since the search space over possible primitive combinations is large. In contrast, our model uses a recurrent neural network that parses the input shape in a top-down manner, which is significantly faster and yields a compact and easy-to-interpret sequence of modeling instructions. Our model is also more effective as a shape detector compared to existing state-of-the-art detection techniques. We finally demonstrate that our network can be trained on novel datasets without ground-truth program annotations through policy gradient techniques",
    "checked": true,
    "id": "7d64392e4bcc4beed2e046676e2fabc253818335",
    "semantic_title": "csgnet: neural shape parser for constructive solid geometry",
    "citation_count": 190,
    "authors": [
      "Gopal Sharma",
      "Rishabh Goyal",
      "Difan Liu",
      "Evangelos Kalogerakis",
      "Subhransu Maji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_Conditional_Image-to-Image_Translation_CVPR_2018_paper.html": {
    "title": "Conditional Image-to-Image Translation",
    "volume": "main",
    "abstract": "Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs) and dual learning. However, existing models lack the ability to control the translated results in the target domain and their results usually lack of diversity in the sense that a fixed image usually leads to (almost) deterministic translation result. In this paper, we study a new problem, conditional image-to-image translation, which is to translate an image from the source domain to the target domain conditioned on a given image in the target domain. It requires that the generated image should inherit some domain-specific features of the conditional image from the target domain. Therefore, changing the conditional image in the target domain will lead to diverse translation results for a fixed input image from the source domain, and therefore the conditional input image helps to control the translation results. We tackle this problem with unpaired data based on GANs and dual learning. We twist two conditional translation models (one translation from A domain to B domain, and the other one from B domain to A domain) together for inputs combination and reconstruction while preserving domain independent features. We carry out experiments on men's faces from-to women's faces translation and edges to shoes and bags translations. The results demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "4a5bd1f2935f9a3c33f1a103a1d43377e61265a0",
    "semantic_title": "conditional image-to-image translation",
    "citation_count": 136,
    "authors": [
      "Jianxin Lin",
      "Yingce Xia",
      "Tao Qin",
      "Zhibo Chen",
      "Tie-Yan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Le-Huu_Continuous_Relaxation_of_CVPR_2018_paper.html": {
    "title": "Continuous Relaxation of MAP Inference: A Nonconvex Perspective",
    "volume": "main",
    "abstract": "In this paper, we study a nonconvex continuous relaxation of MAP inference in discrete Markov random fields (MRFs). We show that for arbitrary MRFs, this relaxation is tight, and a discrete stationary point of it can be easily reached by a simple block coordinate descent algorithm. In addition, we study the resolution of this relaxation using popular gradient methods, and further propose a more effective solution using a multilinear decomposition framework based on the alternating direction method of multipliers (ADMM). Experiments on many real-world problems demonstrate that the proposed ADMM significantly outperforms other nonconvex relaxation based methods, and compares favorably with state of the art MRF optimization algorithms in different settings",
    "checked": true,
    "id": "bc0a9fa4e4207bdef9e546900e1f814d35f4c4a4",
    "semantic_title": "continuous relaxation of map inference: a nonconvex perspective",
    "citation_count": 3,
    "authors": [
      "D. Khuê Lê-Huu",
      "Nikos Paragios"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xian_Feature_Generating_Networks_CVPR_2018_paper.html": {
    "title": "Feature Generating Networks for Zero-Shot Learning",
    "volume": "main",
    "abstract": "Suffering from the extreme training data imbalance between seen and unseen classes, most of existing state-of-the-art approaches fail to achieve satisfactory results for the challenging generalized zero-shot learning task. To circumvent the need for labeled examples of unseen classes, we propose a novel generative adversarial network(GAN) that synthesizes CNN features conditioned on class-level semantic information, offering a shortcut directly from a semantic descriptor of a class to a class-conditional feature distribution. Our proposed approach, pairing a Wasserstein GAN with a classification loss, is able to generate sufficiently discriminative CNN features to train softmax classifiers or any multimodal embedding method. Our experimental results demonstrate a significant boost in accuracy over the state of the art on five challenging datasets -- CUB, FLO, SUN, AWA and ImageNet -- in both the zero-shot learning and generalized zero-shot learning settings",
    "checked": true,
    "id": "cb3f5defe2120076ebbcc89b9256bbfcb8b4d8a1",
    "semantic_title": "feature generating networks for zero-shot learning",
    "citation_count": 933,
    "authors": [
      "Yongqin Xian",
      "Tobias Lorenz",
      "Bernt Schiele",
      "Zeynep Akata"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tanaka_Joint_Optimization_Framework_CVPR_2018_paper.html": {
    "title": "Joint Optimization Framework for Learning With Noisy Labels",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) trained on large-scale datasets have exhibited significant performance in image classification. Many large-scale datasets are collected from websites, however they tend to contain inaccurate labels that are termed as noisy labels. Training on such noisy labeled datasets causes performance degradation because DNNs easily overfit to noisy labels. To overcome this problem, we propose a joint optimization framework of learning DNN parameters and estimating true labels. Our framework can correct labels during training by alternating update of network parameters and labels. We conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset. The results indicate that our approach significantly outperforms other state-of-the-art methods",
    "checked": true,
    "id": "c4c4bc0367ec099f1e00a7700332cd0bf393aa55",
    "semantic_title": "joint optimization framework for learning with noisy labels",
    "citation_count": 710,
    "authors": [
      "Daiki Tanaka",
      "Daiki Ikami",
      "Toshihiko Yamasaki",
      "Kiyoharu Aizawa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Aneja_Convolutional_Image_Captioning_CVPR_2018_paper.html": {
    "title": "Convolutional Image Captioning",
    "volume": "main",
    "abstract": "Image captioning is an important task, applicable to virtual assistants, editing tools, image indexing, and support of the disabled. In recent years significant progress has been made in image captioning, using Recurrent Neural Networks powered by long-short term-memory (LSTM) units. Despite mitigating the vanishing gradient problem, and despite their compelling ability to memorize dependencies, LSTM units are complex and inherently sequential across time. To address this issue, recent work has shown benefits of convolutional networks for machine translation and conditional image generation. Inspired by their success, in this paper, we develop a convolutional image captioning technique. We demonstrate its efficacy on the challenging MSCOCO dataset and demonstrate performance on par with the LSTM baseline, while having a faster training time per number of parameters. We also perform a detailed analysis, providing compelling reasons in favor of convolutional language generation approaches",
    "checked": true,
    "id": "9fb5e3db385588f671b11cfc8bf18efb90ee7b19",
    "semantic_title": "convolutional image captioning",
    "citation_count": 361,
    "authors": [
      "Jyoti Aneja",
      "Aditya Deshpande",
      "Alexander G. Schwing"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper.html": {
    "title": "AON: Towards Arbitrarily-Oriented Text Recognition",
    "volume": "main",
    "abstract": "Recognizing text from natural images is a hot research topic in computer vision due to its various applications. Despite the enduring research of several decades on optical character recognition (OCR), recognizing texts from natural images is still a challenging task. This is because scene texts are often in irregular (e.g. curved, arbitrarily-oriented or seriously distorted) arrangements, which have not yet been well addressed in the literature. Existing methods on text recognition mainly work with regular (horizontal and frontal) texts and cannot be trivially generalized to handle irregular texts. In this paper, we develop the arbitrary orientation network (AON) to directly capture the deep features of irregular texts, which are combined into an attention-based decoder to generate character sequence. The whole network can be trained end-to-end by using only images and word-level annotations. Extensive experiments on various benchmarks, including the CUTE80, SVT-Perspective, IIIT5k, SVT and ICDAR datasets, show that the proposed AON-based method achieves the-state-of-the-art performance in irregular datasets, and is comparable to major existing methods in regular datasets",
    "checked": true,
    "id": "7fadb96f317cd3e1b7c5ef7990c5ac258e2bca30",
    "semantic_title": "aon: towards arbitrarily-oriented text recognition",
    "citation_count": 267,
    "authors": [
      "Zhanzhan Cheng",
      "Yangliu Xu",
      "Fan Bai",
      "Yi Niu",
      "Shiliang Pu",
      "Shuigeng Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mallasto_Wrapped_Gaussian_Process_CVPR_2018_paper.html": {
    "title": "Wrapped Gaussian Process Regression on Riemannian Manifolds",
    "volume": "main",
    "abstract": "Gaussian process (GP) regression is a powerful tool in non-parametric regression providing uncertainty estimates. However, it is limited to data in vector spaces. In fields such as shape analysis and diffusion tensor imaging, the data often lies on a manifold, making GP regression non- viable, as the resulting predictive distribution does not live in the correct geometric space. We tackle the problem by defining wrapped Gaussian processes (WGPs) on Rieman- nian manifolds, using the probabilistic setting to general- ize GP regression to the context of manifold-valued targets. The method is validated empirically on diffusion weighted imaging (DWI) data, directional data on the sphere and in the Kendall shape space, endorsing WGP regression as an efficient and flexible tool for manifold-valued regression",
    "checked": true,
    "id": "fe9e558748592d559226478b0f3b5d47045874f6",
    "semantic_title": "wrapped gaussian process regression on riemannian manifolds",
    "citation_count": 42,
    "authors": [
      "Anton Mallasto",
      "Aasa Feragen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gan_Geometry_Guided_Convolutional_CVPR_2018_paper.html": {
    "title": "Geometry Guided Convolutional Neural Networks for Self-Supervised Video Representation Learning",
    "volume": "main",
    "abstract": "It is often laborious and costly to manually annotate videos for training high-quality video recognition models, so there has been some work and interest in exploring alternative, cheap, and yet often noisy and indirect, training signals for learning the video representations. However, these signals are still coarse, supplying supervision at the whole video frame level, and subtle, sometimes enforcing the learning agent to solve problems that are even hard for humans. In this paper, we instead explore geometry, a grand new type of auxiliary supervision for the self-supervised learning of video representations. In particular, we extract pixel-wise geometry information as flow fields and disparity maps from synthetic imagery and real 3D movies. Although the geometry and high-level semantics are seemingly distant topics, surprisingly, we find that the convolutional neural networks pre-trained by the geometry cues can be effectively adapted to semantic video understanding tasks. In addition, we also find that a progressive training strategy can foster a better neural network for the video recognition task than blindly pooling the distinct sources of geometry cues together. Extensive results on video dynamic scene recognition and action recognition tasks show that our geometry guided networks significantly outperform the competing methods that are trained with other types of labeling-free supervision signals",
    "checked": true,
    "id": "10befbb6c3f6183fe143830ac1479b7dcf119bae",
    "semantic_title": "geometry guided convolutional neural networks for self-supervised video representation learning",
    "citation_count": 123,
    "authors": [
      "Chuang Gan",
      "Boqing Gong",
      "Kun Liu",
      "Hao Su",
      "Leonidas J. Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Firman_DiverseNet_When_One_CVPR_2018_paper.html": {
    "title": "DiverseNet: When One Right Answer Is Not Enough",
    "volume": "main",
    "abstract": "Many structured prediction tasks in machine vision have a collection of acceptable answers, instead of one definitive ground truth answer. Segmentation of images, for example, is subject to human labeling bias. Similarly, there are multiple possible pixel values that could plausibly complete occluded image regions. State-of-the art supervised learning methods are typically optimized to make a single test-time prediction for each query, failing to find other modes in the output space. Existing methods that allow for sampling often sacrifice speed or accuracy. We introduce a simple method for training a neural network, which enables diverse structured predictions to be made for each test-time query. For a single input, we learn to predict a range of possible answers. We compare favorably to methods that seek diversity through an ensemble of networks. Such stochastic multiple choice learning faces mode collapse, where one or more ensemble members fail to receive any training signal. Our best performing solution can be deployed for various tasks, and just involves small modifications to the existing single-mode architecture, loss function, and training regime. We demonstrate that our method results in quantitative improvements across three challenging tasks: 2D image completion, 3D volume estimation, and flow prediction",
    "checked": true,
    "id": "7892f8d93816c78d2d68d8307112d3ea08275911",
    "semantic_title": "diversenet: when one right answer is not enough",
    "citation_count": 24,
    "authors": [
      "Michael Firman",
      "Neill D. F. Campbell",
      "Lourdes Agapito",
      "Gabriel J. Brostow"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jamal_Deep_Face_Detector_CVPR_2018_paper.html": {
    "title": "Deep Face Detector Adaptation Without Negative Transfer or Catastrophic Forgetting",
    "volume": "main",
    "abstract": "Arguably, no single face detector fits all real-life scenarios. It is often desirable to have some built-in schemes for a face detector to automatically adapt, e.g., to a particular user's photo album (the target domain). We propose a novel face detector adaptation approach that works as long as there are representative images of the target domain no matter they are labeled or not and, more importantly, without the need of accessing the training data of the source domain. Our approach explicitly accounts for the notorious negative transfer caveat in domain adaptation thanks to a residual loss by design. Moreover, it does not incur catastrophic interference with the knowledge learned from the source domain and, therefore, the adapted face detectors maintain about the same performance as the old detectors in the original source domain. As such, our adaption approach to face detectors is analogous to the popular interpolation techniques for language models; it may opens a new direction for progressively training the face detectors domain by domain. We report extensive experimental results to verify our approach on two massively benchmarked face detectors",
    "checked": true,
    "id": "be892645c24f800b9b615b496172cd9fac8d5fe8",
    "semantic_title": "deep face detector adaptation without negative transfer or catastrophic forgetting",
    "citation_count": 14,
    "authors": [
      "Muhammad Abdullah Jamal",
      "Haoxiang Li",
      "Boqing Gong"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kobayashi_Analyzing_Filters_Toward_CVPR_2018_paper.html": {
    "title": "Analyzing Filters Toward Efficient ConvNet",
    "volume": "main",
    "abstract": "Deep convolutional neural network (ConvNet) is a promising approach for high-performance image classification. The behavior of ConvNet is analyzed mainly based on the neuron activations, such as by visualizing them. In this paper, in contrast to the activations, we focus on filters which are main components of ConvNets. Through analyzing two types of filters at convolution and fully-connected (FC) layers, respectively, on various pre-trained ConvNets, we present the methods to efficiently reformulate the filters, contributing to improving both memory size and classification performance of the ConvNets. They render the filter bases formulated in a parameter-free form as well as the efficient representation for the FC layer. The experimental results on image classification show that the methods are favorably applied to improve various ConvNets, including ResNet, trained on ImageNet with exhibiting high transferability on the other datasets",
    "checked": true,
    "id": "1f05597606f0ceefd286aae32cb5194b21b9b16c",
    "semantic_title": "analyzing filters toward efficient convnet",
    "citation_count": 8,
    "authors": [
      "Takumi Kobayashi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mostajabi_Regularizing_Deep_Networks_CVPR_2018_paper.html": {
    "title": "Regularizing Deep Networks by Modeling and Predicting Label Structure",
    "volume": "main",
    "abstract": "We construct custom regularization functions for use in supervised training of deep neural networks. Our technique is applicable when the ground-truth labels themselves exhibit internal structure; we derive a regularizer by learning an autoencoder over the set of annotations. Training thereby becomes a two-phase procedure. The first phase models labels with an autoencoder. The second phase trains the actual network of interest by attaching an auxiliary branch that must predict output via a hidden layer of the autoencoder. After training, we discard this auxiliary branch. We experiment in the context of semantic segmentation, demonstrating this regularization strategy leads to consistent accuracy boosts over baselines, both when training from scratch, or in combination with ImageNet pretraining. Gains are also consistent over different choices of convolutional network architecture. As our regularizer is discarded after training, our method has zero cost at test time; the performance improvements are essentially free. We are simply able to learn better network weights by building an abstract model of the label space, and then training the network to understand this abstraction alongside the original task",
    "checked": true,
    "id": "a0c8f223bbe0e3f841a2dd964b05694144a659e6",
    "semantic_title": "regularizing deep networks by modeling and predicting label structure",
    "citation_count": 32,
    "authors": [
      "Mohammadreza Mostajabi",
      "Michael Maire",
      "Gregory Shakhnarovich"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper.html": {
    "title": "In-Place Activated BatchNorm for Memory-Optimized Training of DNNs",
    "volume": "main",
    "abstract": "In this work we present In-Place Activated Batch Normalization (InPlace-ABN) -- a novel approach to drastically reduce the training memory footprint of modern deep neural networks in a computationally efficient way. Our solution substitutes the conventionally used succession of BatchNorm + Activation layers with a single plugin layer, hence avoiding invasive framework surgery while providing straightforward applicability for existing deep learning frameworks. We obtain memory savings of up to 50% by dropping intermediate results and by recovering required information during the backward pass through the inversion of stored forward results, with only minor increase (0.8-2%) in computation time. Also, we demonstrate how frequently used checkpointing approaches can be made computationally as efficient as InPlace-ABN. In our experiments on image classification, we demonstrate on-par results on ImageNet-1k with state-of-the-art approaches. On the memory-demanding task of semantic segmentation, we report competitive results for COCO-Stuff and set new state-of-the-art results for Cityscapes and Mapillary Vistas. Code can be found at https://github.com/mapillary/inplace_abn",
    "checked": true,
    "id": "325093f2c5b33d7507c10aa422e96aa5b10a33f1",
    "semantic_title": "in-place activated batchnorm for memory-optimized training of dnns",
    "citation_count": 356,
    "authors": [
      "Samuel Rota Bulò",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kafle_DVQA_Understanding_Data_CVPR_2018_paper.html": {
    "title": "DVQA: Understanding Data Visualizations via Question Answering",
    "volume": "main",
    "abstract": "Bar charts are an effective way to convey numeric information, but today's algorithms cannot parse them. Existing methods fail when faced with even minor variations in appearance. Here, we present DVQA, a dataset that tests many aspects of bar chart understanding in a question answering framework. Unlike visual question answering (VQA), DVQA requires processing words and answers that are unique to a particular bar chart. State-of-the-art VQA algorithms perform poorly on DVQA, and we propose two strong baselines that perform considerably better. Our work will enable algorithms to automatically extract numeric and semantic information from vast quantities of bar charts found in scientific publications, Internet articles, business reports, and many other areas",
    "checked": true,
    "id": "7289a240c9425bc7cad87b3b835e5f0cac22f488",
    "semantic_title": "dvqa: understanding data visualizations via question answering",
    "citation_count": 388,
    "authors": [
      "Kushal Kafle",
      "Brian Price",
      "Scott Cohen",
      "Christopher Kanan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_DA-GAN_Instance-Level_Image_CVPR_2018_paper.html": {
    "title": "DA-GAN: Instance-Level Image Translation by Deep Attention Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Unsupervised image translation, which aims in translating two independent sets of images, is challenging in discovering the correct correspondences without paired data. Existing works build upon Generative Adversarial Networks (GANs) such that the distribution of the translated images are indistinguishable from the distribution of the target set. However, such set-level constraints cannot learn the instance-level correspondences (e.g. aligned semantic parts in object transfiguration task). This limitation often results in false positives (e.g. geometric or semantic artifacts), and further leads to mode collapse problem. To address the above issues, we propose a novel framework for instance-level image translation by Deep Attention GAN (DA-GAN). Such a design enables DA-GAN to decompose the task of translating samples from two sets into translating instances in a highly-structured latent space. Specifically, we jointly learn a deep attention encoder, and the instance-level correspondences could be consequently discovered through attending on the learned instances. Therefore, the constraints could be exploited on both set-level and instance-level. Comparisons against several state-of-the- arts demonstrate the superiority of our approach, and the broad application capability, e.g, pose morphing, data augmentation, etc., pushes the margin of domain translation problem",
    "checked": true,
    "id": "247d40bed85d09e752f60e5183f14b02e100360b",
    "semantic_title": "da-gan: instance-level image translation by deep attention generative adversarial networks",
    "citation_count": 154,
    "authors": [
      "Shuang Ma",
      "Jianlong Fu",
      "Chang Wen Chen",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.html": {
    "title": "Unsupervised Learning of Depth and Ego-Motion From Monocular Video Using 3D Geometric Constraints",
    "volume": "main",
    "abstract": "We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the whole scene, and enforce consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures. We combine this novel 3D-based loss with 2D losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames. We also incorporate validity masks to avoid penalizing areas in which no useful information exists. We test our algorithm on the KITTI dataset and on a video dataset captured on an uncalibrated mobile phone camera. Our proposed approach consistently improves depth estimates on both datasets, and outperforms the state-of-the-art for both depth and ego-motion. Because we only require a simple video, learning depth and ego-motion on large and varied datasets becomes possible. We demonstrate this by training on the low quality uncalibrated video dataset and evaluating on KITTI, ranking among top performing prior methods which are trained on KITTI itself",
    "checked": true,
    "id": "c33b61e4c4cae519fc65319ec0305e1b10d17219",
    "semantic_title": "unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints",
    "citation_count": 729,
    "authors": [
      "Reza Mahjourian",
      "Martin Wicke",
      "Anelia Angelova"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_FOTS_Fast_Oriented_CVPR_2018_paper.html": {
    "title": "FOTS: Fast Oriented Text Spotting With a Unified Network",
    "volume": "main",
    "abstract": "Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks. Specifically, RoIRotate is introduced to share convolutional features between detection and recognition. Benefiting from convolution sharing strategy, our FOTS has little computation overhead compared to baseline text detection network, and the joint training method makes our method perform better than these two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR 2013 datasets demonstrate that the proposed method outperforms state-of-the-art methods significantly, which further allows us to develop the first real-time oriented text spotting system which surpasses all previous state-of-the-art results by more than 5% on ICDAR 2015 text spotting task while keeping 22.6 fps",
    "checked": true,
    "id": "ee03d4a310e551c892dd4674b0dc36c7a11b8652",
    "semantic_title": "fots: fast oriented text spotting with a unified network",
    "citation_count": 500,
    "authors": [
      "Xuebo Liu",
      "Ding Liang",
      "Shi Yan",
      "Dagui Chen",
      "Yu Qiao",
      "Junjie Yan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Mobile_Video_Object_CVPR_2018_paper.html": {
    "title": "Mobile Video Object Detection With Temporally-Aware Feature Maps",
    "volume": "main",
    "abstract": "This paper introduces an online model for object detection in videos with real-time performance on mobile and embedded devices. Our approach combines fast single-image object detection with convolutional long short term memory (LSTM) layers to create an interweaved recurrent-convolutional architecture. Additionally, we propose an efficient Bottleneck-LSTM layer that significantly reduces computational cost compared to regular LSTMs. Our network achieves temporal awareness by using Bottleneck-LSTMs to refine and propagate feature maps across frames. This approach is substantially faster than existing detection methods in video, outperforming the fastest single-frame models in model size and computational cost while attaining accuracy comparable to much more expensive single-frame models on the Imagenet VID 2015 dataset. Our model reaches a real-time inference speed of up to 15 FPS on a mobile CPU",
    "checked": true,
    "id": "22449479317fa071ae37f12759c6fd524ac1d19c",
    "semantic_title": "mobile video object detection with temporally-aware feature maps",
    "citation_count": 196,
    "authors": [
      "Mason Liu",
      "Menglong Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Weakly_Supervised_Phrase_CVPR_2018_paper.html": {
    "title": "Weakly Supervised Phrase Localization With Multi-Scale Anchored Transformer Network",
    "volume": "main",
    "abstract": "In this paper, we propose a novel weakly supervised model, Multi-scale Anchored Transformer Network (MATN), to accurately localize free-form textual phrases with only image-level supervision. The proposed MATN takes region proposals as localization anchors, and learns a multi-scale correspondence network to continuously search for phrase regions referring to the anchors. In this way, MATN can exploit useful cues from these anchors to reliably reason about locations of the regions described by the phrases given only image-level supervision. Through differentiable sampling on image spatial feature maps, MATN introduces a novel training objective to simultaneously minimize a contrastive reconstruction loss between different phrases from a single image and a set of triplet losses among multiple images with similar phrases. Superior to existing region proposal based methods, MATN searches for the optimal bounding box over the entire feature map instead of selecting a sub-optimal one from discrete region proposals. We evaluate MATN on the Flickr30K Entities and ReferItGame datasets. The experimental results show that MATN significantly outperforms the state-of-the-art methods",
    "checked": true,
    "id": "d544cd6baa2dca816d860aa0c037911b71260e3c",
    "semantic_title": "weakly supervised phrase localization with multi-scale anchored transformer network",
    "citation_count": 62,
    "authors": [
      "Fang Zhao",
      "Jianshu Li",
      "Jian Zhao",
      "Jiashi Feng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Radenovic_Revisiting_Oxford_and_CVPR_2018_paper.html": {
    "title": "Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking",
    "volume": "main",
    "abstract": "In this paper we address issues with image retrieval benchmarking on standard and popular Oxford 5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varying difficulty are introduced. The protocols allow fair comparison between different methods, including those using a dataset pre-processing stage. For each dataset, 15 new challenging queries are introduced. Finally, a new set of 1M hard, semi-automatically cleaned distractors is selected. An extensive comparison of the state-of-the-art methods is performed on the new benchmark. Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods. The best results are achieved by taking the best of the two worlds. Most importantly, image retrieval appears far from being solved",
    "checked": true,
    "id": "73a3576e54e4ad0a00280e8c2daab9ba119352b1",
    "semantic_title": "revisiting oxford and paris: large-scale image retrieval benchmarking",
    "citation_count": 379,
    "authors": [
      "Filip Radenović",
      "Ahmet Iscen",
      "Giorgos Tolias",
      "Yannis Avrithis",
      "Ondřej Chum"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Cross-Dataset_Adaptation_for_CVPR_2018_paper.html": {
    "title": "Cross-Dataset Adaptation for Visual Question Answering",
    "volume": "main",
    "abstract": "We investigate the problem of cross-dataset adaptation for visual question answering (Visual QA). Our goal is to train a Visual QA model on a source dataset but apply it to another target one. Analogous to domain adaptation for visual recognition, this setting is appealing when the target dataset does not have a sufficient amount of labeled data to learn an ``in-domain'' model. The key challenge is that the two datasets are constructed differently, resulting in the cross-dataset mismatch on images, questions, or answers. We overcome this difficulty by proposing a novel domain adaptation algorithm. Our method reduces the difference in statistical distributions by transforming the feature representation of the data in the target dataset. Moreover, it maximizes the likelihood of answering questions (in the target dataset) correctly using the Visual QA model trained on the source dataset. We empirically studied the effectiveness of the proposed approach on adapting among several popular Visual QA datasets. We show that the proposed method improves over baselines where there is no adaptation and several other adaptation methods. We both quantitatively and qualitatively analyze when the adaptation can be mostly effective",
    "checked": true,
    "id": "1bfc74bad04b407d1792a70d73a3f5dc0be0506d",
    "semantic_title": "cross-dataset adaptation for visual question answering",
    "citation_count": 49,
    "authors": [
      "Wei-Lun Chao",
      "Hexiang Hu",
      "Fei Sha"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Joo_Globally_Optimal_Inlier_CVPR_2018_paper.html": {
    "title": "Globally Optimal Inlier Set Maximization for Atlanta Frame Estimation",
    "volume": "main",
    "abstract": "In this work, we describe man-made structures via an appropriate structure assumption, called Atlanta world, which contains a vertical direction (typically the gravity direction) and a set of horizontal directions orthogonal to the vertical direction. Contrary to the commonly used Manhattan world assumption, the horizontal directions in Atlanta world are not necessarily orthogonal to each other. While Atlanta world permits to encompass a wider range of scenes, this makes the solution space larger and the problem more challenging. Given a set of inputs, such as lines in a calibrated image or surface normals, we propose the first globally optimal method of inlier set maximization for Atlanta direction estimation. We define a novel search space for Atlanta world, as well as its parameterization, and solve this challenging problem by a branch-and-bound framework. Experimental results with synthetic and real-world datasets have successfully confirmed the validity of our approach",
    "checked": true,
    "id": "f581175e2619270e6ab8e63e7e892e520de48010",
    "semantic_title": "globally optimal inlier set maximization for atlanta frame estimation",
    "citation_count": 19,
    "authors": [
      "Kyungdon Joo",
      "Tae-Hyun Oh",
      "In So Kweon",
      "Jean-Charles Bazin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/You_End-to-End_Convolutional_Semantic_CVPR_2018_paper.html": {
    "title": "End-to-End Convolutional Semantic Embeddings",
    "volume": "main",
    "abstract": "Semantic embeddings for images and sentences have been widely studied recently. The ability of deep neural networks on learning rich and robust visual and textual representations offers the opportunity to develop effective semantic embedding models. Currently, the state-of-the-art approaches in semantic learning first employ deep neural networks to encode images and sentences into a common semantic space. Then, the learning objective is to ensure a larger similarity between matching image and sentence pairs than randomly sampled pairs. Usually, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are employed for learning image and sentence representations, respectively. On one hand, CNNs are known to produce robust visual features at different levels and RNNs are known for capturing dependencies in sequential data. Therefore, this simple framework can be sufficiently effective in learning visual and textual semantics. On the other hand, different from CNNs, RNNs cannot produce middle-level (e.g. phrase-level in text) representations. As a result, only global representations are available for semantic learning. This could potentially limit the performance of the model due to the hierarchical structures in images and sentences. In this work, we apply Convolutional Neural Networks to process both images and sentences. Consequently, we can employ mid-level representations to assist global semantic learning by introducing a new learning objective on the convolutional layers. The experimental results show that our proposed textual CNN models with the new learning objective lead to better performance than the state-of-the-art approaches",
    "checked": true,
    "id": "356c211af3bd0ce664bc2369b8489a43dfcf98a6",
    "semantic_title": "end-to-end convolutional semantic embeddings",
    "citation_count": 45,
    "authors": [
      "Quanzeng You",
      "Zhengyou Zhang",
      "Jiebo Luo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Referring_Image_Segmentation_CVPR_2018_paper.html": {
    "title": "Referring Image Segmentation via Recurrent Refinement Networks",
    "volume": "main",
    "abstract": "We address the problem of image segmentation from natural language descriptions. Existing deep learning-based methods encode image representations based on the output of the last convolutional layer. One general issue is that the resulting image representation lacks multi-scale semantics, which are key components in advanced segmentation systems. In this paper, we utilize the feature pyramids inherently existing in convolutional neural networks to capture the semantics at different scales. To produce suitable information flow through the path of feature hierarchy, we propose Recurrent Refinement Network (RRN) that takes pyramidal features as input to refine the segmentation mask progressively. Experimental results on four available datasets show that our approach outperforms multiple baselines and state-of-the-art",
    "checked": true,
    "id": "8f44799620bf22f6efa01c35497de7dda3e5d4ab",
    "semantic_title": "referring image segmentation via recurrent refinement networks",
    "citation_count": 219,
    "authors": [
      "Ruiyu Li",
      "Kaican Li",
      "Yi-Chun Kuo",
      "Michelle Shu",
      "Xiaojuan Qi",
      "Xiaoyong Shen",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jain_Two_Can_Play_CVPR_2018_paper.html": {
    "title": "Two Can Play This Game: Visual Dialog With Discriminative Question Generation and Answering",
    "volume": "main",
    "abstract": "Human conversation is a complex mechanism with subtle nuances. It is hence an ambitious goal to develop artificial intelligence agents that can participate fluently in a conversation. While we are still far from achieving this goal, recent progress in visual question answering, image captioning, and visual question generation shows that dialog systems may be realizable in the not too distant future. To this end, a novel dataset was introduced recently and encouraging results were demonstrated, particularly for question answering. In this paper, we demonstrate a simple symmetric discriminative baseline, that can be applied to both predicting an answer as well as predicting a question. We show that this method performs on par with the state of the art, even memory net based methods. In addition, for the first time on the visual dialog dataset, we assess the performance of a system asking questions, and demonstrate how visual dialog can be generated from discriminative question generation and question answering",
    "checked": true,
    "id": "36bb1983dceb84f33d9adcf4ef032c9b0aeed1e4",
    "semantic_title": "two can play this game: visual dialog with discriminative question generation and answering",
    "citation_count": 81,
    "authors": [
      "Unnat Jain",
      "Svetlana Lazebnik",
      "Alexander G. Schwing"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Generative_Adversarial_Learning_CVPR_2018_paper.html": {
    "title": "Generative Adversarial Learning Towards Fast Weakly Supervised Detection",
    "volume": "main",
    "abstract": "Weakly supervised object detection has attracted extensive research efforts in recent years. Without the need of annotating bounding boxes, the existing methods usually follow a two/multi-stage pipeline with an online compulsive stage to extract object proposals, which is an order of magnitude slower than fast fully supervised object detectors such as SSD [31] and YOLO [34]. In this paper, we speedup online weakly supervised object detectors by orders of magnitude by proposing a novel generative adversarial learning paradigm. In the proposed paradigm, the generator is a one-stage object detector to generate bounding boxes from images. To guide the learning of object-level generator, a surrogator is introduced to mine high-quality bounding boxes for training. We further adapt a structural similarity loss in combination with an adversarial loss into the training objective, which solves the challenge that the bounding boxes produced by the surrogator may not well capture their ground truth. Our one-stage detector outperforms all existing schemes in terms of detection accuracy, running at 118 frames per second, which is up to 438x faster than the state-of-the-art weakly supervised detectors [8, 30, 15, 27, 45]. The code will be available publicly soon",
    "checked": true,
    "id": "862b9feff7c5f40736d83bbf10abe32c2702c490",
    "semantic_title": "generative adversarial learning towards fast weakly supervised detection",
    "citation_count": 77,
    "authors": [
      "Yunhan Shen",
      "Rongrong Ji",
      "Shengchuan Zhang",
      "Wangmeng Zuo",
      "Yan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Koniusz_A_Deeper_Look_CVPR_2018_paper.html": {
    "title": "A Deeper Look at Power Normalizations",
    "volume": "main",
    "abstract": "Power Normalizations (PN) are very useful non-linear operators in the context of Bag-of-Words data representations as they tackle problems such as feature imbalance. In this paper, we reconsider these operators in the deep learning setup by introducing a novel layer that implements PN for non-linear pooling of feature maps. Specifically, by using a kernel formulation, our layer combines the feature vectors and their respective spatial locations in the feature maps produced by the last convolutional layer of CNN. Linearization of such a kernel results in a positive definite matrix capturing the second-order statistics of the feature vectors, to which PN operators are applied. We study two types of PN functions, namely (i) MaxExp and (ii) Gamma, addressing their role and meaning in the context of non-linear pooling. We also provide a probabilistic interpretation of these operators and derive their surrogates with well-behaved gradients for end-to-end CNN learning. We apply our theory to practice by implementing the PN layer on a ResNet-50 model and showcase experiments on four benchmarks for fine-grained recognition, scene recognition, and material classification. Our results demonstrate state-of-the-part performance across all these tasks",
    "checked": true,
    "id": "785861c0a0d11f8fed63163d56452ae05acbc69c",
    "semantic_title": "a deeper look at power normalizations",
    "citation_count": 56,
    "authors": [
      "Piotr Koniusz",
      "Hongguang Zhang",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_Dimensionalitys_Blessing_Clustering_CVPR_2018_paper.html": {
    "title": "Dimensionality's Blessing: Clustering Images by Underlying Distribution",
    "volume": "main",
    "abstract": "Many high dimensional vector distances tend to a constant. This is typically considered a negative \"contrast-loss\" phenomenon that hinders clustering and other machine learning techniques. We reinterpret \"contrast-loss\" as a blessing. Re-deriving \"contrast-loss\" using the law of large numbers, we show it results in a distribution's instances concentrating on a thin \"hyper-shell\". The hollow center means apparently chaotically overlapping distributions are actually intrinsically separable. We use this to develop distribution-clustering, an elegant algorithm for grouping of data points by their (unknown) underlying distribution. Distribution-clustering, creates notably clean clusters from raw unlabeled data, estimates the number of clusters for itself and is inherently robust to \"outliers\" which form their own clusters. This enables trawling for patterns in unorganized data and may be the key to enabling machine intelligence",
    "checked": true,
    "id": "d350a31ce029a06c7b023b0ba20b6262ae322f3e",
    "semantic_title": "dimensionality's blessing: clustering images by underlying distribution",
    "citation_count": 12,
    "authors": [
      "Wen-Yan Lin",
      "Siying Liu",
      "Jian-Huang Lai",
      "Yasuyuki Matsushita"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.html": {
    "title": "Eliminating Background-Bias for Robust Person Re-Identification",
    "volume": "main",
    "abstract": "Person re-identification is an important topic in intelligent surveillance and computer vision. It aims to accurately measure visual similarities between person images for determining whether two images correspond to the same person. State-of-the-art methods mainly utilize deep learning based approaches for learning visual features for describing person appearances. However, we observe that existing deep learning models are biased to capture too much relevance between background appearances of person images. We design a series of experiments with newly created datasets to validate the influence of background information. To solve the background bias problem, we propose a person-region guided pooling deep neural network based on human parsing maps to learn more discriminative person-part features, and propose to augment training data with person images with random background. Extensive experiments demonstrate the robustness and effectiveness of our proposed method",
    "checked": true,
    "id": "3f95710d17556d5064bda289dea5adbce2777a59",
    "semantic_title": "eliminating background-bias for robust person re-identification",
    "citation_count": 156,
    "authors": [
      "Maoqing Tian",
      "Shuai Yi",
      "Hongsheng Li",
      "Shihua Li",
      "Xuesen Zhang",
      "Jianping Shi",
      "Junjie Yan",
      "Xiaogang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cui_Learning_to_Evaluate_CVPR_2018_paper.html": {
    "title": "Learning to Evaluate Image Captioning",
    "volume": "main",
    "abstract": "Evaluation metrics for image captioning face two challenges. Firstly, commonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not correlate well with human judgments. Secondly, each metric has well known blind spots to pathological caption constructions, and rule-based metrics lack provisions to repair such blind spots once identified. For example, the newly proposed SPICE correlates well with human judgments, but fails to capture the syntactic structure of a sentence. To address these two challenges, we propose a novel learning based discriminative evaluation metric that is directly trained to distinguish between human and machine-generated captions. In addition, we further propose a data augmentation scheme to explicitly incorporate pathological transformations as negative examples during training. The proposed metric is evaluated with three kinds of robustness tests and its correlation with human judgments. Extensive experiments show that the proposed data augmentation scheme not only makes our metric more robust toward several pathological transformations, but also improves its correlation with human judgments. Our metric outperforms other metrics on both caption level human correlation in Flickr 8k and system level human correlation in COCO. The proposed approach could be served as a learning based evaluation metric that is complementary to existing rule-based metrics",
    "checked": true,
    "id": "6d3d61ef9b5ff6d41badbc3d40ea23acbbc9c3fe",
    "semantic_title": "learning to evaluate image captioning",
    "citation_count": 148,
    "authors": [
      "Yin Cui",
      "Guandao Yang",
      "Andreas Veit",
      "Xun Huang",
      "Serge Belongie"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Single-Shot_Object_Detection_CVPR_2018_paper.html": {
    "title": "Single-Shot Object Detection With Enriched Semantics",
    "volume": "main",
    "abstract": "We propose a novel single shot object detection network named Detection with Enriched Semantics (DES). Our motivation is to enrich the semantics of object detection features within a typical deep detector, by a semantic segmentation branch and a global activation module. The segmentation branch is supervised by weak segmentation ground-truth, i.e., no extra annotation is required. In conjunction with that, we employ a global activation module which learns relationship between channels and object classes in a self-supervised manner. Comprehensive experimental results on both PASCAL VOC and MS COCO detection datasets demonstrate the effectiveness of the proposed method. In particular, with a VGG16 based DES, we achieve an mAP of 81.7 on VOC2007 test and an mAP of 32.8 on COCO test-dev with an inference speed of 31.5 milliseconds per image on a Titan Xp GPU. With a lower resolution version, we achieve an mAP of 79.7 on VOC2007 with an inference speed of 13.0 milliseconds per image",
    "checked": true,
    "id": "f2666a91f6844d7815f758ec1afbac18caf392f8",
    "semantic_title": "single-shot object detection with enriched semantics",
    "citation_count": 208,
    "authors": [
      "Zhishuai Zhang",
      "Siyuan Qiao",
      "Cihang Xie",
      "Wei Shen",
      "Bo Wang",
      "Alan L. Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Low-Shot_Learning_With_CVPR_2018_paper.html": {
    "title": "Low-Shot Learning With Imprinted Weights",
    "volume": "main",
    "abstract": "Human vision is able to immediately recognize novel visual categories after seeing just one or a few training examples. We describe how to add a similar capability to ConvNet classifiers by directly setting the final layer weights from novel training examples during low-shot learning. We call this process weight imprinting as it directly sets weights for a new category based on an appropriately scaled copy of the embedding layer activations for that training example. The imprinting process provides a valuable complement to training with stochastic gradient descent, as it provides immediate good classification performance and an initialization for any further fine-tuning in the future. We show how this imprinting process is related to proxy-based embeddings. However, it differs in that only a single imprinted weight vector is learned for each novel category, rather than relying on a nearest-neighbor distance to training instances as typically used with embedding methods. Our experiments show that using averaging of imprinted weights provides better generalization than using nearest-neighbor instance embeddings",
    "checked": true,
    "id": "f1951f3c86493542be597182d194abcd7b936b9b",
    "semantic_title": "low-shot learning with imprinted weights",
    "citation_count": 467,
    "authors": [
      "Hang Qi",
      "Matthew Brown",
      "David G. Lowe"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zellers_Neural_Motifs_Scene_CVPR_2018_paper.html": {
    "title": "Neural Motifs: Scene Graph Parsing With Global Context",
    "volume": "main",
    "abstract": "We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also find that there are recurring patterns even in larger subgraphs: more than 50% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1% relative gain. Our code is available at github.com/rowanz/neural-motifs",
    "checked": true,
    "id": "0da8af8d81e84381ffe656a0bbf2f3937ffac618",
    "semantic_title": "neural motifs: scene graph parsing with global context",
    "citation_count": 996,
    "authors": [
      "Rowan Zellers",
      "Mark Yatskar",
      "Sam Thomson",
      "Yejin Choi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tan_Variational_Autoencoders_for_CVPR_2018_paper.html": {
    "title": "Variational Autoencoders for Deforming 3D Mesh Models",
    "volume": "main",
    "abstract": "3D geometric contents are becoming increasingly popular. In this paper, we study the problem of analyzing deforming 3D meshes using deep neural networks. Deforming 3D meshes are ﬂexible to represent 3D animation sequences as well as collections of objects of the same category, allowing diverse shapes with large-scale non-linear deformations. We propose a novel framework which we call mesh variational autoencoders (mesh VAE), to explore the probabilistic latent space of 3D surfaces. The framework is easy to train, and requires very few training examples. We also propose an extended model which allows ﬂexibly adjusting the signiﬁcance of different latent variables by altering the prior distribution. Extensive experiments demonstrate that our general framework is able to learn a reasonable representation for a collection of deformable shapes, and produce competitive results for a variety of applications, including shape generation, shape interpolation, shape space embedding and shape exploration, outperforming state-of-the-art methods",
    "checked": true,
    "id": "1fb722cd777a27b8ecccb0065c8edeceaf2f96a4",
    "semantic_title": "variational autoencoders for deforming 3d mesh models",
    "citation_count": 200,
    "authors": [
      "Qingyang Tan",
      "Lin Gao",
      "Yu-Kun Lai",
      "Shihong Xia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dhawale_Fast_Monte-Carlo_Localization_CVPR_2018_paper.html": {
    "title": "Fast Monte-Carlo Localization on Aerial Vehicles Using Approximate Continuous Belief Representations",
    "volume": "main",
    "abstract": "Size, weight, and power constrained platforms impose constraints on computational resources that introduce unique challenges in implementing localization algorithms. We present a framework to perform fast localization on such platforms enabled by the compressive capabilities of Gaussian Mixture Model representations of point cloud data. Given raw structural data from a depth sensor and pitch and roll estimates from an on-board attitude reference system, a multi-hypothesis particle filter localizes the vehicle by exploiting the likelihood of the data originating from the mixture model. We demonstrate analysis of this likelihood in the vicinity of the ground truth pose and detail its utilization in a particle filter-based vehicle localization strategy, and later present results of real-time implementations on a desktop system and an off-the-shelf embedded platform that outperform localization results from running a state-of-the-art algorithm on the same environment",
    "checked": true,
    "id": "35b1c629e54c0d623bd10d88b4837c083acb2f20",
    "semantic_title": "fast monte-carlo localization on aerial vehicles using approximate continuous belief representations",
    "citation_count": 11,
    "authors": [
      "Aditya Dhawale",
      "Kumar Shaurya Shankar",
      "Nathan Michael"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_DeLS-3D_Deep_Localization_CVPR_2018_paper.html": {
    "title": "DeLS-3D: Deep Localization and Segmentation With a 3D Semantic Map",
    "volume": "main",
    "abstract": "For applications such as augmented reality, autonomous driving, self-localization/camera pose estimation and scene parsing are crucial technologies. In this paper, we propose a unified framework to tackle these two problems simultaneously. The uniqueness of our design is a sensor fusion scheme which integrates camera videos, motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robustness and efficiency of the system.Specifically, we first have an initial coarse camera pose obtained from consumer-grade GPS/IMU, based on which a label map can be rendered from the 3D semantic map. Then, the rendered label map and the RGB image are jointly fed into a pose CNN, yielding a corrected camera pose. In addition, to incorporate temporal information, a multi-layer recurrent neural network (RNN) is further deployed improve the pose accuracy. Finally, based on the pose from RNN, we render a new label map, which is fed together with the RGB image into a segment CNN which produces per-pixel semantic label. In order to validate our approach, we build a dataset with registered 3D point clouds and video camera images. Both the point clouds and the images are semantically-labeled. Each video frame has ground truth pose from highly accurate motion sensors. We show that practically, pose estimation solely relying on images like PoseNet~cite{Kendall_2015_ICCV} may fail due to street view confusion, and it is important to fuse multiple sensors. Finally, various ablation studies are performed, which demonstrate the effectiveness of the proposed system. In particular, we show that scene parsing and pose estimation are mutually beneficial to achieve a more robust and accurate system",
    "checked": true,
    "id": "3ecc4821d55c0e528690777be3588fc9cf023882",
    "semantic_title": "dels-3d: deep localization and segmentation with a 3d semantic map",
    "citation_count": 58,
    "authors": [
      "Peng Wang",
      "Ruigang Yang",
      "Binbin Cao",
      "Wei Xu",
      "Yuanqing Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.html": {
    "title": "LiDAR-Video Driving Dataset: Learning Driving Policies Effectively",
    "volume": "main",
    "abstract": "Learning autonomous-driving policies is one of the most challenging but promising tasks for computer vision. Most researchers believe that future research and applications should combine cameras, video recorders and laser scanners to obtain comprehensive semantic understanding of real traffic. However, current approaches only learn from large-scale videos, due to the lack of benchmarks that consist of precise laser-scanner data. In this paper, we are the first to propose a LiDAR-Video dataset, which provides large-scale high-quality point clouds scanned by a Velodyne laser, videos recorded by a dashboard camera and standard drivers' behaviors. Extensive experiments demonstrate that extra depth information help networks to determine driving policies indeed",
    "checked": true,
    "id": "c40bfdc874cf4f8d3931f21e3975054ce8a75513",
    "semantic_title": "lidar-video driving dataset: learning driving policies effectively",
    "citation_count": 116,
    "authors": [
      "Yiping Chen",
      "Jingkang Wang",
      "Jonathan Li",
      "Cewu Lu",
      "Zhipeng Luo",
      "Han Xue",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sage_Logo_Synthesis_and_CVPR_2018_paper.html": {
    "title": "Logo Synthesis and Manipulation With Clustered Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training, and validate this approach on CIFAR-10 and ImageNet-small to demonstrate its generality. We are able to generate a high diversity of plausible logos and demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models are publicly available at https://data.vision.ee.ethz.ch/sagea/lld",
    "checked": true,
    "id": "30831a581be8b76a99ef079f82e3c1b5f8c2dc05",
    "semantic_title": "logo synthesis and manipulation with clustered generative adversarial networks",
    "citation_count": 62,
    "authors": [
      "Alexander Sage",
      "Eirikur Agustsson",
      "Radu Timofte",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bertasius_Egocentric_Basketball_Motion_CVPR_2018_paper.html": {
    "title": "Egocentric Basketball Motion Planning From a Single First-Person Image",
    "volume": "main",
    "abstract": "We present a model that uses a single first-person image to generate an egocentric basketball motion sequence in the form of a 12D camera configuration trajectory, which encodes a player's 3D location and 3D head orientation throughout the sequence. To do this, we first introduce a future convolutional neural network (CNN) that predicts an initial sequence of 12D camera configurations, aiming to capture how real players move during a one-on-one basketball game. We also introduce a goal verifier network, which is trained to verify that a given camera configuration is consistent with the final goals of real one-on-one basketball players. Next, we propose an inverse synthesis procedure to synthesize a refined sequence of 12D camera configurations that (1) sufficiently matches the initial configurations predicted by the future CNN, while (2) maximizing the output of the goal verifier network. Finally, by following the trajectory resulting from the refined camera configuration sequence, we obtain the complete 12D motion sequence. Our model generates realistic basketball motion sequences that capture the goals of real players, outperforming standard deep learning approaches such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and generative adversarial networks (GANs)",
    "checked": true,
    "id": "8bfa0c14c2ae48c1ee6b145008137e4d69688416",
    "semantic_title": "egocentric basketball motion planning from a single first-person image",
    "citation_count": 25,
    "authors": [
      "Gedas Bertasius",
      "Aaron Chan",
      "Jianbo Shi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Human-Centric_Indoor_Scene_CVPR_2018_paper.html": {
    "title": "Human-Centric Indoor Scene Synthesis Using Stochastic Grammar",
    "volume": "main",
    "abstract": "We present a human-centric method to sample and synthesize 3D room layouts and 2D images thereof, for the purpose of obtaining large-scale 2D/3D image data with the perfect per-pixel ground truth. An attributed spatial And-Or graph (S-AOG) is proposed to represent indoor scenes. The S-AOG is a probabilistic grammar model, in which the terminal nodes are object entities including room, furniture, and supported objects. Human contexts as contextual relations are encoded by Markov Random Fields (MRF) on the terminal nodes. We learn the distributions from an indoor scene dataset and sample new layouts using Monte Carlo Markov Chain. Experiments demonstrate that the proposed method can robustly sample a large variety of realistic room layouts based on three criteria: (i) visual realism comparing to a state-of-the-art room arrangement method, (ii) accuracy of the affordance maps with respect to ground-truth, and (ii) the functionality and naturalness of synthesized rooms evaluated by human subjects",
    "checked": true,
    "id": "bcffc36185ce86eb2e313df8e914c64f354567de",
    "semantic_title": "human-centric indoor scene synthesis using stochastic grammar",
    "citation_count": 184,
    "authors": [
      "Siyuan Qi",
      "Yixin Zhu",
      "Siyuan Huang",
      "Chenfanfu Jiang",
      "Song-Chun Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Rotation-Sensitive_Regression_for_CVPR_2018_paper.html": {
    "title": "Rotation-Sensitive Regression for Oriented Scene Text Detection",
    "volume": "main",
    "abstract": "Text in natural images is of arbitrary orientations, requiring detection in terms of oriented bounding boxes. Normally, a multi-oriented text detector often involves two key tasks: 1) text presence detection, which is a classification problem disregarding text orientation; 2) oriented bounding box regression, which concerns about text orientation. Previous methods rely on shared features for both tasks, resulting in degraded performance due to the incompatibility of the two tasks. To address this issue, we propose to perform classification and regression on features of different characteristics, extracted by two network branches of different designs. Concretely, the regression branch extracts rotation-sensitive features by actively rotating the convolutional filters, while the classification branch extracts rotation-invariant features by pooling the rotation-sensitive features. The proposed method named Rotation-sensitive Regression Detector (RRD) achieves state-of-the-art performance on several oriented scene text benchmark datasets, including ICDAR 2015, MSRA-TD500, RCTW-17, and COCO-Text. Furthermore, RRD achieves a significant improvement on a ship collection dataset, demonstrating its generality on oriented object detection",
    "checked": true,
    "id": "31f8c58679b52ff5c67bf4e46704c678a2968d6f",
    "semantic_title": "rotation-sensitive regression for oriented scene text detection",
    "citation_count": 450,
    "authors": [
      "Minghui Liao",
      "Zhen Zhu",
      "Baoguang Shi",
      "Gui-song Xia",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Veit_Separating_Self-Expression_and_CVPR_2018_paper.html": {
    "title": "Separating Self-Expression and Visual Content in Hashtag Supervision",
    "volume": "main",
    "abstract": "The variety, abundance, and structured nature of hashtags make them an interesting data source for training vision models. For instance, hashtags have the potential to significantly reduce the problem of manual supervision and annotation when learning vision models for a large number of concepts. However, a key challenge when learning from hashtags is that they are inherently subjective because they are provided by users as a form of self-expression. As a consequence, hashtags may have synonyms (different hashtags referring to the same visual content) and may be polysemous (the same hashtag referring to different visual content). These challenges limit the effectiveness of approaches that simply treat hashtags as image-label pairs. This paper presents an approach that extends upon modeling simple image-label pairs with a joint model of images, hashtags, and users. We demonstrate the efficacy of such approaches in image tagging and retrieval experiments, and show how the joint model can be used to perform user-conditional retrieval and tagging",
    "checked": true,
    "id": "655f587a59c835a7b6b5017016ea1c2123f266e6",
    "semantic_title": "separating self-expression and visual content in hashtag supervision",
    "citation_count": 29,
    "authors": [
      "Andreas Veit",
      "Maximilian Nickel",
      "Serge Belongie",
      "Laurens van der Maaten"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Park_Distort-and-Recover_Color_Enhancement_CVPR_2018_paper.html": {
    "title": "Distort-and-Recover: Color Enhancement Using Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Learning-based color enhancement approaches typically learn to map from input images to retouched images. Most of existing methods require expensive pairs of input-retouched images or produce results in a non-interpretable way. In this paper, we present a deep reinforcement learning (DRL) based method for color enhancement to explicitly model the step-wise nature of human retouching process. We cast a color enhancement process as a Markov Decision Process where actions are defined as global color adjustment operations. Then we train our agent to learn the optimal global enhancement sequence of the actions. In addition, we present a `distort-and-recover' training scheme which only requires high-quality reference images for training instead of input and retouched image pairs. Given high-quality reference images, we distort the images' color distribution and form distorted-reference image pairs for training. Through extensive experiments, we show that our method produces decent enhancement results and our DRL approach is more suitable for the `distort-and-recover' training scheme than previous supervised approaches. Supplementary material and code are available at https://sites.google.com/view/distort-and-recover/",
    "checked": true,
    "id": "90ac2e752661753b2b355f829db6aa8773b98534",
    "semantic_title": "distort-and-recover: color enhancement using deep reinforcement learning",
    "citation_count": 197,
    "authors": [
      "Jongchan Park",
      "Joon-Young Lee",
      "Donggeun Yoo",
      "In So Kweon"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.html": {
    "title": "Im2Flow: Motion Hallucination From Static Images for Action Recognition",
    "volume": "main",
    "abstract": "Existing methods to recognize actions in static images take the images at their face value, learning the appearances---objects, scenes, and body poses---that distinguish each action class. However, such models are deprived of the rich dynamic structure and motions that also define human activity. We propose an approach that hallucinates the unobserved future motion implied by a single snapshot to help static-image action recognition. The key idea is to learn a prior over short-term dynamics from thousands of unlabeled videos, infer the anticipated optical flow on novel static images, and then train discriminative models that exploit both streams of information. Our main contributions are twofold. First, we devise an encoder-decoder convolutional neural network and a novel optical flow encoding that can translate a static image into an accurate flow map. Second, we show the power of hallucinated flow for recognition, successfully transferring the learned motion into a standard two-stream network for activity recognition. On seven datasets, we demonstrate the power of the approach. It not only achieves state-of-the-art accuracy for dense optical flow prediction, but also consistently enhances recognition of actions and dynamic scenes",
    "checked": true,
    "id": "89498817a49d9c349ec9f67375023ead0411b865",
    "semantic_title": "im2flow: motion hallucination from static images for action recognition",
    "citation_count": 93,
    "authors": [
      "Ruohan Gao",
      "Bo Xiong",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Finding_It_Weakly-Supervised_CVPR_2018_paper.html": {
    "title": "Finding \"It\": Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos",
    "volume": "main",
    "abstract": "Grounding textual phrases in visual content with standalone image-sentence pairs is a challenging task. When we consider grounding in instructional videos, this problem becomes profoundly more complex: the latent temporal structure of instructional videos breaks independence assumptions and necessitates contextual understanding for resolving ambiguous visual-linguistic cues. Furthermore, dense annotations and video data scale mean supervised approaches are prohibitively costly. In this work, we propose to tackle this new task with a weakly-supervised framework for reference-aware visual grounding in instructional videos, where only the temporal alignment between the transcription and the video segment are available for supervision. We introduce the visually grounded action graph, a structured representation capturing the latent dependency between grounding and references in video. For optimization, we propose a new reference-aware multiple instance learning (RA-MIL) objective for weak supervision of grounding in videos. We evaluate our approach over unconstrained videos from YouCookII and RoboWatch, augmented with new reference-grounding test set annotations. We demonstrate that our jointly optimized, reference-aware approach simultaneously improves visual grounding, reference-resolution, and generalization to unseen instructional video categories",
    "checked": true,
    "id": "aeac614f10cb2a5dc000fdee30d857bbe5456ce5",
    "semantic_title": "finding \"it\": weakly-supervised reference-aware visual grounding in instructional videos",
    "citation_count": 100,
    "authors": [
      "De-An Huang",
      "Shyamal Buch",
      "Lucio Dery",
      "Animesh Garg",
      "Li Fei-Fei",
      "Juan Carlos Niebles"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gavrilyuk_Actor_and_Action_CVPR_2018_paper.html": {
    "title": "Actor and Action Video Segmentation From a Sentence",
    "volume": "main",
    "abstract": "This paper strives for pixel-level segmentation of actors and their actions in video content. Different from existing works, which all learn to segment from a fixed vocabulary of actor and action pairs, we infer the segmentation from a natural language input sentence. This allows to distinguish between fine-grained actors in the same super-category, identify actor and action instances, and segment pairs that are outside of the actor and action vocabulary. We propose a fully-convolutional model for pixel-level actor and action segmentation using an encoder-decoder architecture optimized for video. To show the potential of actor and action video segmentation from a sentence, we extend two popular actor and action datasets with more than 7,500 natural language descriptions. Experiments demonstrate the quality of the sentence-guided segmentations, the generalization ability of our model, and its advantage for traditional actor and action segmentation compared to the state-of-the-art",
    "checked": true,
    "id": "3dea5307b5e2de12489a9d567535a5fff2d2ea6c",
    "semantic_title": "actor and action video segmentation from a sentence",
    "citation_count": 149,
    "authors": [
      "Kirill Gavrilyuk",
      "Amir Ghodrati",
      "Zhenyang Li",
      "Cees G. M. Snoek"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Possas_Egocentric_Activity_Recognition_CVPR_2018_paper.html": {
    "title": "Egocentric Activity Recognition on a Budget",
    "volume": "main",
    "abstract": "Recent advances in embedded technology have enabled more pervasive machine learning. One of the common applications in this field is Egocentric Activity Recognition (EAR), where users wearing a device such as a smartphone or smartglasses are able to receive feedback from the embedded device. Recent research on activity recognition has mainly focused on improving accuracy by using resource intensive techniques such as multi-stream deep networks. Although this approach has provided state-of-the-art results, in most cases it neglects the natural resource constraints (e.g. battery) of wearable devices. We develop a Reinforcement Learning model-free method to learn energy-aware policies that maximize the use of low-energy cost predictors while keeping competitive accuracy levels. Our results show that a policy trained on an egocentric dataset is able use the synergy between motion sensors and vision to effectively tradeoff energy expenditure and accuracy on smartglasses operating in realistic, real-world conditions",
    "checked": true,
    "id": "bcafefd074937caa6a56f3ffe0068075c6f631e8",
    "semantic_title": "egocentric activity recognition on a budget",
    "citation_count": 43,
    "authors": [
      "Rafael Possas",
      "Sheila Pinto Caceres",
      "Fabio Ramos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bao_CNN_in_MRF_CVPR_2018_paper.html": {
    "title": "CNN in MRF: Video Object Segmentation via Inference in a CNN-Based Higher-Order Spatio-Temporal MRF",
    "volume": "main",
    "abstract": "This paper addresses the problem of video object segmentation, where the initial object mask is given in the first frame of an input video. We propose a novel spatio-temporal Markov Random Field (MRF) model defined over pixels to handle this problem. Unlike conventional MRF models, the spatial dependencies among pixels in our model are encoded by a Convolutional Neural Network (CNN). Specifically, for a given object, the probability of a labeling to a set of spatially neighboring pixels can be predicted by a CNN trained for this specific object. As a result, higher-order, richer dependencies among pixels in the set can be implicitly modeled by the CNN. With temporal dependencies established by optical flow, the resulting MRF model combines both spatial and temporal cues for tackling video object segmentation. However, performing inference in the MRF model is very difficult due to the very high-order dependencies. To this end, we propose a novel CNN-embedded algorithm to perform approximate inference in the MRF. This algorithm proceeds by alternating between a temporal fusion step and a feed-forward CNN step. When initialized with an appearance-based one-shot segmentation CNN, our model outperforms the winning entries of the DAVIS 2017 Challenge, without resorting to model ensembling or any dedicated detectors",
    "checked": true,
    "id": "b1f0899c6b70f29dc30a6c2b9e96646dd7b331db",
    "semantic_title": "cnn in mrf: video object segmentation via inference in a cnn-based higher-order spatio-temporal mrf",
    "citation_count": 206,
    "authors": [
      "Linchao Bao",
      "Baoyuan Wu",
      "Wei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Richard_Action_Sets_Weakly_CVPR_2018_paper.html": {
    "title": "Action Sets: Weakly Supervised Action Segmentation Without Ordering Constraints",
    "volume": "main",
    "abstract": "Action detection and temporal segmentation of actions in videos are topics of increasing interest. While fully supervised systems have gained much attention lately, full annotation of each action within the video is costly and impractical for large amounts of video data. Thus, weakly supervised action detection and temporal segmentation methods are of great importance. While most works in this area assume an ordered sequence of occurring actions to be given, our approach only uses a set of actions. Such action sets provide much less supervision since neither action ordering nor the number of action occurrences are known. In exchange, they can be easily obtained, for instance, from meta-tags, while ordered sequences still require human annotation. We introduce a system that automatically learns to temporally segment and label actions in a video, where the only supervision that is used are action sets. An evaluation on three datasets shows that our method still achieves good results although the amount of supervision is significantly smaller than for other related methods",
    "checked": true,
    "id": "96ce111119624888be47d998cf87c9df18988c4d",
    "semantic_title": "action sets: weakly supervised action segmentation without ordering constraints",
    "citation_count": 92,
    "authors": [
      "Alexander Richard",
      "Hilde Kuehne",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Low-Latency_Video_Semantic_CVPR_2018_paper.html": {
    "title": "Low-Latency Video Semantic Segmentation",
    "volume": "main",
    "abstract": "Recent years have seen remarkable progress in semantic segmentation. Yet, it remains a challenging task to apply segmentation techniques to video-based applications. Specifically, the high throughput of video streams, the sheer cost of running fully convolutional networks, together with the low-latency requirements in many real-world applications, e.g. autonomous driving, present a significant challenge to the design of the video segmentation framework. To tackle this combined challenge, we develop a framework for video semantic segmentation, which incorporates two novel components:(1) a feature propagation module that adaptively fuses features over time via spatially variant convolution, thus reducing the cost of per-frame computation; and (2) an adaptive scheduler that dynamically allocate computation based on accuracy prediction. Both components work together to ensure low latency while maintaining high segmentation quality. On both Cityscapes and CamVid, the proposed framework obtained competitive performance compared to the state of the art, while substantially reducing the latency, from 360 ms to 119 ms",
    "checked": true,
    "id": "c92a649e706583d676a073efbd4512728d5d27b4",
    "semantic_title": "low-latency video semantic segmentation",
    "citation_count": 161,
    "authors": [
      "Yule Li",
      "Jianping Shi",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Fine-Grained_Video_Captioning_CVPR_2018_paper.html": {
    "title": "Fine-Grained Video Captioning for Sports Narrative",
    "volume": "main",
    "abstract": "Despite recent emergence of video caption methods, how to generate fine-grained video descriptions (i.e., long and detailed commentary about individual movements of multiple subjects as well as their frequent interactions) is far from being solved, which however has great applications such as automatic sports narrative. To this end, this work makes the following contributions. First, to facilitate this novel research of fine-grained video caption, we collected a novel dataset called Fine-grained Sports Narrative dataset (FSN) that contains 2K sports videos with ground-truth narratives from YouTube.com. Second, we develop a novel performance evaluation metric named Fine-grained Captioning Evaluation (FCE) to cope with this novel task. Considered as an extension of the widely used METEOR, it measures not only the linguistic performance but also whether the action details and their temporal orders are correctly described. Third, we propose a new framework for fine-grained sports narrative task. This network features three branches: 1) a spatio-temporal entity localization and role discovering sub-network; 2) a fine-grained action modeling sub-network for local skeleton motion description; and 3) a group relationship modeling sub-network to model interactions between players. We further fuse the features and decode them into long narratives by a hierarchically recurrent structure. Extensive experiments on the FSN dataset demonstrates the validity of the proposed framework for fine-grained video caption",
    "checked": true,
    "id": "f5876f67129a80a1ee753f715efcd2e2109bf432",
    "semantic_title": "fine-grained video captioning for sports narrative",
    "citation_count": 60,
    "authors": [
      "Huanyu Yu",
      "Shuo Cheng",
      "Bingbing Ni",
      "Minsi Wang",
      "Jian Zhang",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_End-to-End_Learning_of_CVPR_2018_paper.html": {
    "title": "End-to-End Learning of Motion Representation for Video Understanding",
    "volume": "main",
    "abstract": "Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network, to learn optical-flow-like features from data. TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers. TVNet can therefore be used directly without any extra learning. Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk. Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition benchmarks verify the effectiveness of the proposed approach. Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time",
    "checked": true,
    "id": "259dcd1afe22e569132ccc41697ac368504c4dd1",
    "semantic_title": "end-to-end learning of motion representation for video understanding",
    "citation_count": 214,
    "authors": [
      "Lijie Fan",
      "Wenbing Huang",
      "Chuang Gan",
      "Stefano Ermon",
      "Boqing Gong",
      "Junzhou Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Compressed_Video_Action_CVPR_2018_paper.html": {
    "title": "Compressed Video Action Recognition",
    "volume": "main",
    "abstract": "Training robust deep video representations has proven to be much more challenging than learning deep image representations. This is in part due to the enormous size of raw video streams and the high temporal redundancy; the true and interesting signal is often drowned in too much irrelevant data. Motivated by that the superfluous information can be reduced by up to two orders of magnitude by video compression (using H.264, HEVC, etc.), we propose to train a deep network directly on the compressed video. This representation has a higher information density, and we found the training to be easier. In addition, the signals in a compressed video provide free, albeit noisy, motion information. We propose novel techniques to use them effectively. Our approach is about 4.6 times faster than Res3D and 2.7 times faster than ResNet-152. On the task of action recognition, our approach outperforms all the other methods on the UCF-101, HMDB-51, and Charades dataset",
    "checked": true,
    "id": "9d98a956aadaff727e495b14b7c532d40ea49e16",
    "semantic_title": "compressed video action recognition",
    "citation_count": 325,
    "authors": [
      "Chao-Yuan Wu",
      "Manzil Zaheer",
      "Hexiang Hu",
      "R. Manmatha",
      "Alexander J. Smola",
      "Philipp Krähenbühl"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ristani_Features_for_Multi-Target_CVPR_2018_paper.html": {
    "title": "Features for Multi-Target Multi-Camera Tracking and Re-Identification",
    "volume": "main",
    "abstract": "Multi-Target Multi-Camera Tracking (MTMCT) tracks many people through video taken from several cameras. Person Re-Identification (Re-ID) retrieves from a gallery images of people similar to a person query image. We learn good features for both MTMCT and Re-ID with a convolutional neural network. Our contributions include an adaptive weighted triplet loss for training and a new technique for hard-identity mining. Our method outperforms the state of the art both on the DukeMTMC benchmarks for tracking, and on the Market-1501 and DukeMTMC-ReID benchmarks for Re-ID. We examine the correlation between good Re-ID and good MTMCT scores, and perform ablation studies to elucidate the contributions of the main components of our system. Code is available",
    "checked": true,
    "id": "c0f01b8174a632448c20eb5472cd9d5b2c595e39",
    "semantic_title": "features for multi-target multi-camera tracking and re-identification",
    "citation_count": 518,
    "authors": [
      "Ergys Ristani",
      "Carlo Tomasi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gu_AVA_A_Video_CVPR_2018_paper.html": {
    "title": "AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions",
    "volume": "main",
    "abstract": "This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 437 15-minute video clips, where actions are localized in space and time, resulting in 1.59M action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips. AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 15.8% mAP, underscoring the need for developing new approaches for video understanding",
    "checked": true,
    "id": "54c7c3909c7e1e827befdbe8d2595a3b196ba1b8",
    "semantic_title": "ava: a video dataset of spatio-temporally localized atomic visual actions",
    "citation_count": 1030,
    "authors": [
      "Chunhui Gu",
      "Chen Sun",
      "David A. Ross",
      "Carl Vondrick",
      "Caroline Pantofaru",
      "Yeqing Li",
      "Sudheendra Vijayanarasimhan",
      "George Toderici",
      "Susanna Ricco",
      "Rahul Sukthankar",
      "Cordelia Schmid",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Doughty_Whos_Better_Whos_CVPR_2018_paper.html": {
    "title": "Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination",
    "volume": "main",
    "abstract": "This paper presents a method for assessing skill from video, applicable to a variety of tasks, ranging from surgery to drawing and rolling pizza dough. We formulate the problem as pairwise (who's better?) and overall (who's best?) ranking of video collections, using supervised deep ranking. We propose a novel loss function that learns discriminative features when a pair of videos exhibit variance in skill, and learns shared features when a pair of videos exhibit comparable skill levels. Results demonstrate our method is applicable across tasks, with the percentage of correctly ordered pairs of videos ranging from 70% to 83% for four datasets. We demonstrate the robustness of our approach via sensitivity analysis of its parameters. We see this work as effort toward the automated organization of how-to video collections and overall, generic skill determination in video",
    "checked": true,
    "id": "e849bab6bc8195370208b6a99acdb44078857ed8",
    "semantic_title": "who's better? who's best? pairwise deep ranking for skill determination",
    "citation_count": 130,
    "authors": [
      "Hazel Doughty",
      "Dima Damen",
      "Walterio Mayol-Cuevas"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hasan_MX-LSTM_Mixing_Tracklets_CVPR_2018_paper.html": {
    "title": "MX-LSTM: Mixing Tracklets and Vislets to Jointly Forecast Trajectories and Head Poses",
    "volume": "main",
    "abstract": "Recent approaches on trajectory forecasting use tracklets to predict the future positions of pedestrians exploiting Long Short Term Memory (LSTM) architectures. This paper shows that adding vislets, that is, short sequences of head pose estimations, allows to increase significantly the trajectory forecasting performance. We then propose to use vislets in a novel framework called MX-LSTM, capturing the interplay between tracklets and vislets thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. At the same time, MX-LSTM predicts the future head poses, increasing the standard capabilities of the long-term trajectory forecasting approaches. With standard head pose estimators and an attentional-based social pooling, Mixing-LSTM scores the new trajectory forecasting state-of-the-art in all the considered datasets (Zara01, Zara02, UCY, and TownCentre) with a dramatic margin when the pedestrians slow down, a case where most of the forecasting approaches struggle to provide an accurate solution",
    "checked": true,
    "id": "3a701cf34433781028e192e8f4ca9da050ac946e",
    "semantic_title": "mx-lstm: mixing tracklets and vislets to jointly forecast trajectories and head poses",
    "citation_count": 120,
    "authors": [
      "Irtiza Hasan",
      "Francesco Setti",
      "Theodore Tsesmelis",
      "Alessio Del Bue",
      "Fabio Galasso",
      "Marco Cristani"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html": {
    "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
    "volume": "main",
    "abstract": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge",
    "checked": true,
    "id": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8",
    "semantic_title": "bottom-up and top-down attention for image captioning and visual question answering",
    "citation_count": 4214,
    "authors": [
      "Peter Anderson",
      "Xiaodong He",
      "Chris Buehler",
      "Damien Teney",
      "Mark Johnson",
      "Stephen Gould",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Nguyen_Improved_Fusion_of_CVPR_2018_paper.html": {
    "title": "Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering",
    "volume": "main",
    "abstract": "A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction",
    "checked": true,
    "id": "f7cc85bed2a3d0b0ef1c0e0258f5b60ee4bb4622",
    "semantic_title": "improved fusion of visual and language representations by dense symmetric co-attention for visual question answering",
    "citation_count": 280,
    "authors": [
      "Duy-Kien Nguyen",
      "Takayuki Okatani"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Massiceti_FlipDial_A_Generative_CVPR_2018_paper.html": {
    "title": "FlipDial: A Generative Model for Two-Way Visual Dialogue",
    "volume": "main",
    "abstract": "We present FlipDial, a generative model for Visual Dialogue that simultaneously plays the role of both participants in a visually-grounded dialogue. Given context in the form of an image and an associated caption summarising the contents of the image, FlipDial learns both to answer questions and put forward questions, capable of generating entire sequences of dialogue (question-answer pairs) which are diverse and relevant to the image. To do this, FlipDial relies on a simple but surprisingly powerful idea: it uses convolutional neural networks (CNNs) to encode entire dialogues directly, implicitly capturing dialogue context, and conditional VAEs to learn the generative model. FlipDial outperforms the state-of-the-art model in the sequential answering task (1VD) on the VisDial dataset by 5 points in Mean Rank using the generated answers. We are the first to extend this paradigm to full two-way visual dialogue (2VD), where our model is capable of generating both questions and answers in sequence based on a visual input, for which we propose a set of novel evaluation measures and metrics",
    "checked": true,
    "id": "a7aa181fd7cadc7568d4fd87d2a1b12994ea1828",
    "semantic_title": "flipdial: a generative model for two-way visual dialogue",
    "citation_count": 41,
    "authors": [
      "Daniela Massiceti",
      "N. Siddharth",
      "Puneet K. Dokania",
      "Philip H.S. Torr"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Are_You_Talking_CVPR_2018_paper.html": {
    "title": "Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning",
    "volume": "main",
    "abstract": "The Visual Dialogue task requires an agent to engage in a conversation about an image with a human. It represents an extension of the Visual Question Answering task in that the agent needs to answer a question about an image, but it needs to do so in light of the previous dialogue that has taken place. The key challenge in Visual Dialogue is thus maintaining a consistent, and natural dialogue while continuing to answer questions correctly. We present a novel approach that combines Reinforcement Learning and Generative Adversarial Networks (GANs) to generate more human-like responses to questions. The GAN helps overcome the relative paucity of training data, and the tendency of the typical MLE-based approach to generate overly terse answers. Critically, the GAN is tightly integrated into the attention mechanism that generates human-interpretable reasons for each answer. This means that the discriminative model of the GAN has the task of assessing whether a candidate answer is generated by a human or not, given the provided reason. This is significant because it drives the generative model to produce high quality answers that are well supported by the associated reasoning. The method also generates the state-of-the-art results on the primary benchmark",
    "checked": true,
    "id": "9dde6ed569684356c46217fa53224272b668bae8",
    "semantic_title": "are you talking to me? reasoned visual dialog generation through adversarial learning",
    "citation_count": 129,
    "authors": [
      "Qi Wu",
      "Peng Wang",
      "Chunhua Shen",
      "Ian Reid",
      "Anton van den Hengel"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Visual_Question_Generation_CVPR_2018_paper.html": {
    "title": "Visual Question Generation as Dual Task of Visual Question Answering",
    "volume": "main",
    "abstract": "Visual question answering (VQA) and visual question generation (VQG) are two trending topics in the computer vision, but they are usually explored separately despite their intrinsic complementary relationship. In this paper, we propose an end-to-end unified model, the Invertible Question Answering Network (iQAN), to introduce question generation as a dual task of question answering to improve the VQA performance. With our proposed invertible bilinear fusion module and parameter sharing scheme, our iQAN can accomplish VQA and its dual task VQG simultaneously. By jointly trained on two tasks with our proposed dual regularizers~(termed as Dual Training), our model has a better understanding of the interactions among images, questions and answers. After training, iQAN can take either question or answer as input, and output the counterpart. Evaluated on the CLEVR and VQA2 datasets, our iQAN improves the top-1 accuracy of the prior art MUTAN VQA method by 1.33% and 0.88% (absolute increase). We also show that our proposed dual training framework can consistently improve model performances of many popular VQA architectures",
    "checked": true,
    "id": "47aff6477f05ec32fc163e1943fe9464a8379552",
    "semantic_title": "visual question generation as dual task of visual question answering",
    "citation_count": 164,
    "authors": [
      "Yikang Li",
      "Nan Duan",
      "Bolei Zhou",
      "Xiao Chu",
      "Wanli Ouyang",
      "Xiaogang Wang",
      "Ming Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yeh_Unsupervised_Textual_Grounding_CVPR_2018_paper.html": {
    "title": "Unsupervised Textual Grounding: Linking Words to Image Concepts",
    "volume": "main",
    "abstract": "Textual grounding, i.e., linking words to objects in images, is a challenging but important task for robotics and human-computer interaction. Existing techniques benefit from recent progress in deep learning and generally formulate the task as a supervised learning problem, selecting a bounding box from a set of possible options. To train these deep net based approaches, access to a large-scale datasets is required, however, constructing such a dataset is time-consuming and expensive. Therefore, we develop a completely unsupervised mechanism for textual grounding using hypothesis testing as a mechanism to link words to detected image concepts. We demonstrate our approach on the ReferIt Game dataset and the Flickr30k data, outperforming baselines by 7.98% and 6.96% respectively",
    "checked": true,
    "id": "021b08b823700f8053afc54356e8d0ce57a3df71",
    "semantic_title": "unsupervised textual grounding: linking words to image concepts",
    "citation_count": 40,
    "authors": [
      "Raymond A. Yeh",
      "Minh N. Do",
      "Alexander G. Schwing"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.html": {
    "title": "Focal Visual-Text Attention for Visual Question Answering",
    "volume": "main",
    "abstract": "Recent insights on language and vision with neural networks have been successfully applied to simple single-image visual question answering. However, to tackle real-life question answering problems on multimedia collections such as personal photos, we have to look at whole collections with sequences of photos or videos. When answering questions from a large collection, a natural problem is to identify snippets to support the answer. In this paper, we describe a novel neural network called Focal Visual-Text Attention network (FVTA) for collective reasoning in visual question answering, where both visual and text sequence information such as images and text metadata are presented. FVTA introduces an end-to-end approach that makes use of a hierarchical process to dynamically determine what media and what time to focus on in the sequential data to answer the question. FVTA can not only answer the questions well but also provides the justifications which the system results are based upon to get the answers. FVTA achieves state-of-the-art performance on the MemexQA dataset and competitive results on the MovieQA dataset",
    "checked": true,
    "id": "aee265f6a19f9774c65d296cf9ec0e169365dda5",
    "semantic_title": "focal visual-text attention for visual question answering",
    "citation_count": 111,
    "authors": [
      "Junwei Liang",
      "Lu Jiang",
      "Liangliang Cao",
      "Li-Jia Li",
      "Alexander G. Hauptmann"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ehsani_SeGAN_Segmenting_and_CVPR_2018_paper.html": {
    "title": "SeGAN: Segmenting and Generating the Invisible",
    "volume": "main",
    "abstract": "Objects often occlude each other in scenes; Inferring their appearance beyond their visible parts plays an important role in scene understanding, depth estimation, object interaction and manipulation. In this paper, we study the challenging problem of completing the appearance of occluded objects. Doing so requires knowing which pixels to paint (segmenting the invisible parts of objects) and what color to paint them (generating the invisible parts). Our proposed novel solution, SeGAN, jointly optimizes for both segmentation and generation of the invisible parts of objects. Our experimental results show that: (a) SeGAN can learn to generate the appearance of the occluded parts of objects; (b) SeGAN outperforms state-of-the-art segmentation baselines for the invisible parts of objects; (c) trained on synthetic photo realistic images, SeGAN can reliably segment natural images; (d) by reasoning about occluder-occludee relations, our method can infer depth layering",
    "checked": true,
    "id": "44f6fd433a0e64fe8ea6df77cae156adab6139e3",
    "semantic_title": "segan: segmenting and generating the invisible",
    "citation_count": 147,
    "authors": [
      "Kiana Ehsani",
      "Roozbeh Mottaghi",
      "Ali Farhadi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.html": {
    "title": "Cascade R-CNN: Delving Into High Quality Object Detection",
    "volume": "main",
    "abstract": "In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn",
    "checked": true,
    "id": "04957e40d47ca89d38653e97f728883c0ad26e5d",
    "semantic_title": "cascade r-cnn: delving into high quality object detection",
    "citation_count": 4924,
    "authors": [
      "Zhaowei Cai",
      "Nuno Vasconcelos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Learning_Semantic_Concepts_CVPR_2018_paper.html": {
    "title": "Learning Semantic Concepts and Order for Image and Sentence Matching",
    "volume": "main",
    "abstract": "Image and sentence matching has made great progress recently, but it remains challenging due to the large visual semantic discrepancy. This mainly arises from that the representation of pixel-level image usually lacks of high-level semantic information as in its matched sentence. In this work, we propose a semantic-enhanced image and sentence matching model, which can improve the image representation by learning semantic concepts and then organizing them in a correct semantic order. Given an image, we first use a multi-regional multi-label CNN to predict its semantic concepts, including objects, properties, actions, etc. Then, considering that different orders of semantic concepts lead to diverse semantic meanings, we use a context-gated sentence generation scheme for semantic order learning. It simultaneously uses the image global context containing concept relations as reference and the groundtruth semantic order in the matched sentence as supervision. After obtaining the improved image representation, we learn the sentence representation with a conventional LSTM, and then jointly perform image and sentence matching and sentence generation for model learning. Extensive experiments demonstrate the effectiveness of our learned semantic concepts and order, by achieving the state-of-the-art results on two public benchmark datasets",
    "checked": true,
    "id": "f322eef6a4c965910e03f6997b1bc2acd413e273",
    "semantic_title": "learning semantic concepts and order for image and sentence matching",
    "citation_count": 303,
    "authors": [
      "Yan Huang",
      "Qi Wu",
      "Chunfeng Song",
      "Liang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Christie_Functional_Map_of_CVPR_2018_paper.html": {
    "title": "Functional Map of the World",
    "volume": "main",
    "abstract": "We present a new dataset, Functional Map of the World (fMoW), which aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. The metadata provided with each image enables reasoning about location, time, sun angles, physical sizes, and other features when making predictions about objects in the image. Our dataset consists of over 1 million images from over 200 countries. For each image, we provide at least one bounding box annotation containing one of 63 categories, including a \"false detection\" category. We present an analysis of the dataset along with baseline approaches that reason about metadata and temporal views. Our data, code, and pretrained models have been made publicly available",
    "checked": true,
    "id": "a588d38ec81c0337b445931eadf6f443aea13380",
    "semantic_title": "functional map of the world",
    "citation_count": 392,
    "authors": [
      "Gordon Christie",
      "Neil Fendley",
      "James Wilson",
      "Ryan Mukherjee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Peng_MegDet_A_Large_CVPR_2018_paper.html": {
    "title": "MegDet: A Large Mini-Batch Object Detector",
    "volume": "main",
    "abstract": "The development of object detection in the era of deep learning, from R-CNN [11], Fast/Faster R-CNN [10, 31] to recent Mask R-CNN [14] and RetinaNet [24], mainly come from novel network, new framework, or loss design. How- ever, mini-batch size, a key factor for the training of deep neural networks, has not been well studied for object detec- tion. In this paper, we propose a Large Mini-Batch Object Detector (MegDet) to enable the training with a large mini- batch size up to 256, so that we can effectively utilize at most 128 GPUs to significantly shorten the training time. Technically, we suggest a warmup learning rate policy and Cross-GPU Batch Normalization, which together allow us to successfully train a large mini-batch detector in much less time (e.g., from 33 hours to 4 hours), and achieve even better accuracy. The MegDet is the backbone of our sub- mission (mmAP 52.5%) to COCO 2017 Challenge, where we won the 1st place of Detection task",
    "checked": true,
    "id": "642897c7d92262f9f40a1d6192d34c33487ac227",
    "semantic_title": "megdet: a large mini-batch object detector",
    "citation_count": 318,
    "authors": [
      "Chao Peng",
      "Tete Xiao",
      "Zeming Li",
      "Yuning Jiang",
      "Xiangyu Zhang",
      "Kai Jia",
      "Gang Yu",
      "Jian Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Rao_Learning_Globally_Optimized_CVPR_2018_paper.html": {
    "title": "Learning Globally Optimized Object Detector via Policy Gradient",
    "volume": "main",
    "abstract": "In this paper, we propose a simple yet effective method to learn globally optimized detector for object detection, which is a simple modification to the standard cross-entropy gradient inspired by the REINFORCE algorithm. In our approach, the cross-entropy gradient is adaptively adjusted according to overall mean Average Precision (mAP) of the current state for each detection candidate, which leads to more effective gradient and global optimization of detection results, and brings no computational overhead. Benefiting from more precise gradients produced by the global optimization method, our framework significantly improves state-of-the-art object detectors. Furthermore, since our method is based on scores and bounding boxes without modification on the architecture of object detector, it can be easily applied to off-the-shelf modern object detection frameworks",
    "checked": true,
    "id": "97c7dcce628d592dcf6d36a40a78a4b92f09655d",
    "semantic_title": "learning globally optimized object detector via policy gradient",
    "citation_count": 23,
    "authors": [
      "Yongming Rao",
      "Dahua Lin",
      "Jiwen Lu",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Photographic_Text-to-Image_Synthesis_CVPR_2018_paper.html": {
    "title": "Photographic Text-to-Image Synthesis With a Hierarchically-Nested Adversarial Network",
    "volume": "main",
    "abstract": "This paper presents a novel method to deal with the challenging task of generating photographic images conditioned on semantic image descriptions. Our method introduces accompanying hierarchical-nested adversarial objectives inside the network hierarchies, which regularize mid-level representations and assist generator training to capture the complex image statistics. We present an extensile single-stream generator architecture to better adapt the jointed discriminators and push generated images up to high resolutions. We adopt a multi-purpose adversarial loss to encourage more effective image and text information usage in order to improve the semantic consistency and image fidelity simultaneously. Furthermore, we introduce a new visual-semantic similarity measure to evaluate the semantic consistency of generated images. With extensive experimental validation on three public datasets, our method significantly improves previous state of the arts on all datasets over different evaluation metrics",
    "checked": true,
    "id": "d1e3f6d69c1a7bb2bc97273fb18470df92b1c7a4",
    "semantic_title": "photographic text-to-image synthesis with a hierarchically-nested adversarial network",
    "citation_count": 304,
    "authors": [
      "Zizhao Zhang",
      "Yuanpu Xie",
      "Lin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hui_Illuminant_Spectra-Based_Source_CVPR_2018_paper.html": {
    "title": "Illuminant Spectra-Based Source Separation Using Flash Photography",
    "volume": "main",
    "abstract": "Real-world lighting often consists of multiple illuminants with different spectra. Separating and manipulating these illuminants in post-process is a challenging problem that requires either significant manual input or calibrated scene geometry and lighting. In this work, we leverage a flash/no-flash image pair to analyze and edit scene illuminants based on their spectral differences. We derive a novel physics-based relationship between color variations in the observed flash/no-flash intensities and the spectra and surface shading corresponding to individual scene illuminants. Our technique uses this constraint to automatically separate an image into constituent images lit by each illuminant. This separation can be used to support applications like white balancing, lighting editing, and RGB photometric stereo, where we demonstrate results that outperform state-of-the-art techniques on a wide range of images",
    "checked": true,
    "id": "4ffad16e0bb48b2030b89647b0d27b52f7c0d987",
    "semantic_title": "illuminant spectra-based source separation using flash photography",
    "citation_count": 16,
    "authors": [
      "Zhuo Hui",
      "Kalyan Sunkavalli",
      "Sunil Hadap",
      "Aswin C. Sankaranarayanan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Trapping_Light_for_CVPR_2018_paper.html": {
    "title": "Trapping Light for Time of Flight",
    "volume": "main",
    "abstract": "We propose a novel imaging method for near-complete, surround, 3D reconstruction of geometrically complex objects, in a single shot. The key idea is to augment a time-of-flight (ToF) based 3D sensor with a multi-mirror system, called a light-trap. The shape of the trap is chosen so that light rays entering it bounce multiple times inside the trap, thereby visiting every position inside the trap multiple times from various directions. We show via simulations that this enables light rays to reach more than 99.9% of the surface of objects placed inside the trap, even those with strong occlusions, for example, lattice-shaped objects. The ToF sensor provides the path length for each light ray, which, along with the known shape of the trap, is used to reconstruct the complete paths of all the rays. This enables performing dense, surround 3D reconstructions of objects with highly complex 3D shapes, in a single shot. We have developed a proof-of-concept hardware prototype consisting of a pulsed ToF sensor, and a light trap built with planar mirrors. We demonstrate the effectiveness of the light trap based 3D reconstruction method on a variety of objects with a broad range of geometry and reflectance properties",
    "checked": true,
    "id": "dc052b693f4608b7a06f0873297c5d497d807efb",
    "semantic_title": "trapping light for time of flight",
    "citation_count": 6,
    "authors": [
      "Ruilin Xu",
      "Mohit Gupta",
      "Shree K. Nayar"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Blau_The_Perception-Distortion_Tradeoff_CVPR_2018_paper.html": {
    "title": "The Perception-Distortion Tradeoff",
    "volume": "main",
    "abstract": "Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. However, as we show experimentally, for some measures it is less severe (e.g. distance between VGG features). We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms",
    "checked": true,
    "id": "775bf115923037ab2515b17ce31dd203a1f4575c",
    "semantic_title": "the perception-distortion tradeoff",
    "citation_count": 811,
    "authors": [
      "Yochai Blau",
      "Tomer Michaeli"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Label_Denoising_Adversarial_CVPR_2018_paper.html": {
    "title": "Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Faces",
    "volume": "main",
    "abstract": "Lighting estimation from faces is an important task and has applications in many areas such as image editing, intrinsic image decomposition, and image forgery detection. We propose to train a deep Convolutional Neural Network (CNN) to regress lighting parameters from a single face image. Lacking massive ground truth lighting labels for face images in the wild, we use an existing method to estimate lighting parameters, which are treated as ground truth with noise. To alleviate the effect of such noise, we utilize the idea of Generative Adversarial Networks (GAN) and propose a Label Denoising Adversarial Network (LDAN). LDAN makes use of synthetic data with accurate ground truth to help train a deep CNN for lighting regression on real face images. Experiments show that our network outperforms existing methods in producing consistent lighting parameters of different faces under similar lighting conditions. To further evaluate the proposed method, we also apply it to regress object 2D key points where ground truth labels are available. Our experiments demonstrate its effectiveness on this application",
    "checked": true,
    "id": "b8942497abfc1025b3961d3e62bd326b829b068d",
    "semantic_title": "label denoising adversarial network (ldan) for inverse lighting of faces",
    "citation_count": 21,
    "authors": [
      "Hao Zhou",
      "Jin Sun",
      "Yaser Yacoob",
      "David W. Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mirdehghan_Optimal_Structured_Light_CVPR_2018_paper.html": {
    "title": "Optimal Structured Light à La Carte",
    "volume": "main",
    "abstract": "We consider the problem of automatically generating sequences of structured-light patterns for active stereo triangulation of a static scene. Unlike existing approaches that use predetermined patterns and reconstruction algorithms tied to them, we generate patterns on the fly in response to generic specifications: number of patterns, projector-camera arrangement, workspace constraints, spatial frequency content, etc. Our pattern sequences are specifically optimized to minimize the expected rate of correspondence errors under those specifications for an unknown scene, and are coupled to a sequence-independent algorithm for per-pixel disparity estimation. To achieve this, we derive an objective function that is easy to optimize and follows from first principles within a maximum-likelihood framework. By minimizing it, we demonstrate automatic discovery of pattern sequences, in under three minutes on a laptop, that can outperform state-of-the-art triangulation techniques",
    "checked": true,
    "id": "541fdf28d1069c8040741f554a74cbf424073de8",
    "semantic_title": "optimal structured light a la carte",
    "citation_count": 41,
    "authors": [
      "Parsa Mirdehghan",
      "Wenzheng Chen",
      "Kiriakos N. Kutulakos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Smith_Tracking_Multiple_Objects_CVPR_2018_paper.html": {
    "title": "Tracking Multiple Objects Outside the Line of Sight Using Speckle Imaging",
    "volume": "main",
    "abstract": "This paper presents techniques for tracking non-line-of-sight (NLOS) objects using speckle imaging. We develop a novel speckle formation and motion model where both the sensor and the source view objects only indirectly via a diffuse wall. We show that this NLOS imaging scenario is analogous to direct LOS imaging with the wall acting as a virtual, bare (lens-less) sensor. This enables tracking of a single, rigidly moving NLOS object using existing speckle-based motion estimation techniques. However, when imaging multiple NLOS objects, the speckle components due to different objects are superimposed on the virtual bare sensor image, and cannot be analyzed separately for recovering the motion of individual objects. We develop a novel clustering algorithm based on the statistical and geometrical properties of speckle images, which enables identifying the motion trajectories of multiple, independently moving NLOS objects. We demonstrate, for the first time, tracking individual trajectories of multiple objects around a corner with extreme precision (< 10 microns) using only off-the-shelf imaging components",
    "checked": true,
    "id": "ca160aaa7f3d6855283adcf6da43f83dc3362538",
    "semantic_title": "tracking multiple objects outside the line of sight using speckle imaging",
    "citation_count": 48,
    "authors": [
      "Brandon M. Smith",
      "Matthew O'Toole",
      "Mohit Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Baradad_Inferring_Light_Fields_CVPR_2018_paper.html": {
    "title": "Inferring Light Fields From Shadows",
    "volume": "main",
    "abstract": "We present a method for inferring a 4D light field of a hidden scene from 2D shadows cast by a known occluder on a diffuse wall. We do this by determining how light naturally reflected off surfaces in the hidden scene interacts with the occluder. By modeling the light transport as a linear system, and incorporating prior knowledge about light field structures, we can invert the system to recover the hidden scene. We demonstrate results of our inference method across simulations and experiments with different types of occluders. For instance, using the shadow cast by a real house plant, we are able to recover low resolution light fields with different levels of texture and parallax complexity. We provide two experimental results: a human subject and two planar elements at different depths",
    "checked": true,
    "id": "20d4dc5a8971eb9c9f2412ca50719d04a91cf268",
    "semantic_title": "inferring light fields from shadows",
    "citation_count": 60,
    "authors": [
      "Manel Baradad",
      "Vickie Ye",
      "Adam B. Yedidia",
      "Frédo Durand",
      "William T. Freeman",
      "Gregory W. Wornell",
      "Antonio Torralba"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tlusty_Modifying_Non-Local_Variations_CVPR_2018_paper.html": {
    "title": "Modifying Non-Local Variations Across Multiple Views",
    "volume": "main",
    "abstract": "We present an algorithm for modifying small non-local variations between repeating structures and patterns in multiple images of the same scene. The modification is consistent across views, even-though the images could have been photographed from different view points and under different lighting conditions. We show that when modifying each image independently the correspondence between them breaks and the geometric structure of the scene gets distorted. Our approach modifies the views while maintaining correspondence, hence, we succeed in modifying appearance and structure variations consistently. We demonstrate our methods on a number of challenging examples, photographed in different lighting, scales and view points",
    "checked": true,
    "id": "b8301c5611b842a29ae064b19ddd7f21b51df358",
    "semantic_title": "modifying non-local variations across multiple views",
    "citation_count": 9,
    "authors": [
      "Tal Tlusty",
      "Tomer Michaeli",
      "Tali Dekel",
      "Lihi Zelnik-Manor"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Robust_Video_Content_CVPR_2018_paper.html": {
    "title": "Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework",
    "volume": "main",
    "abstract": "Rain removal is important for improving the robustness of outdoor vision based systems. Current rain removal methods show limitations either for complex dynamic scenes shot from fast moving cameras, or under torrential rain fall with opaque occlusions. We propose a novel derain algorithm, which applies superpixel (SP) segmentation to decompose the scene into depth consistent units. Alignment of scene contents are done at the SP level, which proves to be robust towards rain occlusion and fast camera motion. Two alignment output tensors, i.e., optimal temporal match tensor and sorted spatial-temporal match tensor, provide informative clues for rain streak location and occluded background contents to generate an intermediate derain output. These tensors will be subsequently prepared as input features for a convolutional neural network to restore high frequency details to the intermediate output for compensation of misalignment blur. Extensive evaluations show that up to 5dB reconstruction PSNR advantage is achieved over state-of-the-art methods. Visual inspection shows that much cleaner rain removal is achieved especially for highly dynamic scenes with heavy and opaque rainfall from a fast moving camera",
    "checked": true,
    "id": "b10ebdd785b66572e4504b76436335097887bfda",
    "semantic_title": "robust video content alignment and compensation for rain removal in a cnn framework",
    "citation_count": 137,
    "authors": [
      "Jie Chen",
      "Cheen-Hau Tan",
      "Junhui Hou",
      "Lap-Pui Chau",
      "He Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sengupta_SfSNet_Learning_Shape_CVPR_2018_paper.html": {
    "title": "SfSNet: Learning Shape, Reflectance and Illuminance of Faces `in the Wild",
    "volume": "main",
    "abstract": "We present SfSNet, an end-to-end learning framework for producing an accurate decomposition of an unconstrained human face image into shape, reflectance and illuminance. SfSNet is designed to reflect a physical lambertian rendering model. SfSNet learns from a mixture of labeled synthetic and unlabeled real world images. This allows the network to capture low frequency variations from synthetic and high frequency details from real images through the photometric reconstruction loss. SfSNet consists of a new decomposition architecture with residual blocks that learns a complete separation of albedo and normal. This is used along with the original image to predict lighting. SfSNet produces significantly better quantitative and qualitative results than state-of-the-art methods for inverse rendering and independent normal and illumination estimation",
    "checked": true,
    "id": "074619ffc19894c13974321d4b31144acc212f91",
    "semantic_title": "sfsnet: learning shape, reflectance and illuminance of faces 'in the wild",
    "citation_count": 320,
    "authors": [
      "Soumyadip Sengupta",
      "Angjoo Kanazawa",
      "Carlos D. Castillo",
      "David W. Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Deep_Photo_Enhancer_CVPR_2018_paper.html": {
    "title": "Deep Photo Enhancer: Unpaired Learning for Image Enhancement From Photographs With GANs",
    "volume": "main",
    "abstract": "This paper proposes an unpaired learning method for image enhancement. Given a set of photographs with the desired characteristics, the proposed method learns a photo enhancer which transforms an input image into an enhanced image with those characteristics. The method is based on the framework of two-way generative adversarial networks (GANs) with several improvements. First, we augment the U-Net with global features and show that it is more effective. The global U-Net acts as the generator in our GAN model. Second, we improve Wasserstein GAN (WGAN) with an adaptive weighting scheme. With this scheme, training converges faster and better, and is less sensitive to parameters than WGAN-GP. Finally, we propose to use individual batch normalization layers for generators in two-way GANs. It helps generators better adapt to their own input distributions. All together, they significantly improve the stability of GAN training for our application. Both quantitative and visual results show that the proposed method is effective for enhancing images",
    "checked": true,
    "id": "34d6e3da80ee25840f6dadf36b27717da0bb609b",
    "semantic_title": "deep photo enhancer: unpaired learning for image enhancement from photographs with gans",
    "citation_count": 410,
    "authors": [
      "Yu-Sheng Chen",
      "Yu-Ching Wang",
      "Man-Hsin Kao",
      "Yung-Yu Chuang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Meka_LIME_Live_Intrinsic_CVPR_2018_paper.html": {
    "title": "LIME: Live Intrinsic Material Estimation",
    "volume": "main",
    "abstract": "We present the first end-to-end approach for real-time material estimation for general object shapes with uniform material that only requires a single color image as input. In addition to Lambertian surface properties, our approach fully automatically computes the specular albedo, material shininess, and a foreground segmentation. We tackle this challenging and ill-posed inverse rendering problem using recent advances in image-to-image translation techniques based on deep convolutional encoder–decoder architectures. The underlying core representations of our approach are specular shading, diffuse shading and mirror images, which allow to learn the effective and accurate separation of diffuse and specular albedo. In addition, we propose a novel highly efficient perceptual rendering loss that mimics real world image formation and obtains intermediate results even during run time. The estimation of material parameters at real-time frame rates enables exciting mixed reality applications, such as seamless illumination-consistent integration of virtual objects into realworld scenes, and virtual material cloning.We demonstrate our approach in a live setup, compare it to the state of the art, and demonstrate its effectiveness through quantitative and qualitative evaluation",
    "checked": true,
    "id": "7d0400e4c574a532bc9e7e2e98775b9f40feb934",
    "semantic_title": "lime: live intrinsic material estimation",
    "citation_count": 104,
    "authors": [
      "Abhimitra Meka",
      "Maxim Maximov",
      "Michael Zollhöfer",
      "Avishek Chatterjee",
      "Hans-Peter Seidel",
      "Christian Richardt",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Learning_to_Detect_CVPR_2018_paper.html": {
    "title": "Learning to Detect Features in Texture Images",
    "volume": "main",
    "abstract": "Local feature detection is a fundamental task in computer vision, and hand-crafted feature detectors such as SIFT have shown success in applications including image-based localization and registration. Recent work has used features detected in texture images for precise global localization, but is limited by the performance of existing feature detectors on textures, as opposed to natural images. We propose an effective and scalable method for learning feature detectors for textures, which combines an existing \"ranking\" loss with an efficient fully-convolutional architecture as well as a new training-loss term that maximizes the \"peakedness\" of the response map. We demonstrate that our detector is more repeatable than existing methods, leading to improvements in a real-world texture-based localization application",
    "checked": true,
    "id": "31f4e0aac74ba5e219cbea4458aa91f363aa03a6",
    "semantic_title": "learning to detect features in texture images",
    "citation_count": 67,
    "authors": [
      "Linguang Zhang",
      "Szymon Rusinkiewicz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jin_Learning_to_Extract_CVPR_2018_paper.html": {
    "title": "Learning to Extract a Video Sequence From a Single Motion-Blurred Image",
    "volume": "main",
    "abstract": "We present a method to extract a video sequence from a single motion-blurred image. Motion-blurred images are the result of an averaging process, where instant frames are accumulated over time during the exposure of the sensor. Unfortunately, reversing this process is nontrivial. Firstly, averaging destroys the temporal ordering of the frames. Secondly, the recovery of a single frame is a blind deconvolution task, which is highly ill-posed. We present a deep learning scheme that gradually reconstructs a temporal ordering by sequentially extracting pairs of frames. Our main contribution is to introduce loss functions invariant to the temporal order. This lets a neural network choose during training what frame to output among the possible combinations. We also address the ill-posedness of deblurring by designing a network with a large receptive field and implemented via resampling to achieve a higher computational efficiency. Our proposed method can successfully retrieve sharp image sequences from a single motion blurred image and can generalize well on synthetic and real datasets captured with different cameras",
    "checked": true,
    "id": "8210e9dcd766883478b3106e48db67f2c7101eb0",
    "semantic_title": "learning to extract a video sequence from a single motion-blurred image",
    "citation_count": 114,
    "authors": [
      "Meiguang Jin",
      "Givi Meishvili",
      "Paolo Favaro"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Anirudh_Lose_the_Views_CVPR_2018_paper.html": {
    "title": "Lose the Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion",
    "volume": "main",
    "abstract": "Computed Tomography (CT) reconstruction is a fundamental component to a wide variety of applications ranging from security, to healthcare. The classical techniques require measuring projections, called sinograms, from a full 180 degree view of the object. However, obtaining a full-view is not always feasible, such as when scanning irregular objects that limit flexibility of scanner rotation. The resulting limited angle sinograms are known to produce highly artifact-laden reconstructions with existing techniques. In this paper, we propose to address this problem using CTNet -- a system of 1D and 2D convolutional neural networks, that operates directly on a limited angle sinogram to predict the reconstruction. We use the x-ray transform on this prediction to obtain a ``completed'' sinogram, as if it came from a full 180 degree view. We feed this to standard analytical and iterative reconstruction techniques to obtain the final reconstruction. We show with extensive experimentation on a challenging real world dataset that this combined strategy outperforms many competitive baselines. We also propose a measure of confidence for the reconstruction that enables a practitioner to gauge the reliability of a prediction made by CTNet. We show that this measure is a strong indicator of quality as measured by the PSNR, while not requiring ground truth at test time. Finally, using a segmentation experiment, we show that our reconstruction also preserves the 3D structure of objects better than existing solutions",
    "checked": true,
    "id": "389b4c984735e433a9365763426e1cb3c59ac81b",
    "semantic_title": "lose the views: limited angle ct reconstruction via implicit sinogram completion",
    "citation_count": 102,
    "authors": [
      "Rushil Anirudh",
      "Hyojin Kim",
      "Jayaraman J. Thiagarajan",
      "K. Aditya Mohan",
      "Kyle Champley",
      "Timo Bremer"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Men_A_Common_Framework_CVPR_2018_paper.html": {
    "title": "A Common Framework for Interactive Texture Transfer",
    "volume": "main",
    "abstract": "In this paper, we present a general-purpose solution to interactive texture transfer problems that better preserves both local structure and visual richness. It is challenging due to the diversity of tasks and the simplicity of required user guidance. The core idea of our common framework is to use multiple custom channels to dynamically guide the synthesis process. For interactivity, users can control the spatial distribution of stylized textures via semantic channels. The structure guidance, acquired by two stages of automatic extraction and propagation of structure information, provides a prior for initialization and preserves the salient structure by searching the nearest neighbor fields (NNF) with structure coherence. Meanwhile, texture coherence is also exploited to maintain similar style with the source image. In addition, we leverage an improved PatchMatch with extended NNF and matrix operations to obtain transformable source patches with richer geometric information at high speed. We demonstrate the effectiveness and superiority of our method on a variety of scenes through extensive comparisons with state-of-the-art algorithms",
    "checked": true,
    "id": "d0052597917df29c56e0695624e17f68aeea6879",
    "semantic_title": "a common framework for interactive texture transfer",
    "citation_count": 28,
    "authors": [
      "Yifang Men",
      "Zhouhui Lian",
      "Yingmin Tang",
      "Jianguo Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper.html": {
    "title": "AMNet: Memorability Estimation With Attention",
    "volume": "main",
    "abstract": "In this paper we present the design and evaluation of an end to end trainable, deep neural network with a visual attention mechanism for memorability estimation in still images. We analyze the suitability of transfer learning of deep models from image classification to the memorability task. Further on we study the impact of the attention mechanism on the memorability estimation and evaluate our network on the SUN Memorability and the LaMem dataset, the only large dataset with memorability labels to this date. Our network outperforms the existing state of the art models on both, the LaMem and SUN datasets in the term of the Spearman's rank correlation as well as mean squared error, approaching human consistency",
    "checked": true,
    "id": "afac56d8272665c445b7fb6b31541b9dd578455e",
    "semantic_title": "amnet: memorability estimation with attention",
    "citation_count": 61,
    "authors": [
      "Jiri Fajtl",
      "Vasileios Argyriou",
      "Dorothy Monekosso",
      "Paolo Remagnino"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Blind_Predicting_Similar_CVPR_2018_paper.html": {
    "title": "Blind Predicting Similar Quality Map for Image Quality Assessment",
    "volume": "main",
    "abstract": "A key problem in blind image quality assessment (BIQA) is how to effectively model the properties of human visual system in a data-driven manner. In this paper, we propose a simple and efficient BIQA model based on a novel framework which consists of a fully convolutional neural network (FCNN) and a pooling network to solve this problem. In principle, FCNN is capable of predicting a pixel-by-pixel similar quality map only from a distorted image by using the intermediate similarity maps derived from conventional full-reference image quality assessment methods. The predicted pixel-by-pixel quality maps have good consistency with the distortion correlations between the reference and distorted images. Finally, a deep pooling network regresses the quality map into a score. Experiments have demonstrated that our predictions outperform many state-of-the-art BIQA methods",
    "checked": true,
    "id": "3f06efd478f2fbf8d970bfbc7f146238f17a7c3c",
    "semantic_title": "blind predicting similar quality map for image quality assessment",
    "citation_count": 103,
    "authors": [
      "Da Pan",
      "Ping Shi",
      "Ming Hou",
      "Zefeng Ying",
      "Sizhe Fu",
      "Yuan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Su_Deep_End-to-End_Time-of-Flight_CVPR_2018_paper.html": {
    "title": "Deep End-to-End Time-of-Flight Imaging",
    "volume": "main",
    "abstract": "We present an end-to-end image processing framework for time-of-flight (ToF) cameras. Existing ToF image processing pipelines consist of a sequence of operations including modulated exposures, denoising, phase unwrapping and multipath interference correction. While this cascaded modular design offers several benefits, such as closed-form solutions and power-efficient processing, it also suffers from error accumulation and information loss as each module can only observe the output from its direct predecessor, resulting in erroneous depth estimates. We depart from a conventional pipeline model and propose a deep convolutional neural network architecture that recovers scene depth directly from dual-frequency, raw ToF correlation measurements. To train this network, we simulate ToF images for a variety of scenes using a time-resolved renderer, devise depth-specific losses, and apply normalization and augmentation strategies to generalize this model to real captures. We demonstrate that the proposed network can efficiently exploit the spatio-temporal structures of ToF frequency measurements, and validate the performance of the joint multipath removal, denoising and phase unwrapping method on a wide range of challenging scenes",
    "checked": true,
    "id": "97bb0e63fa3a20c1624d9882c317d52a00a5e46b",
    "semantic_title": "deep end-to-end time-of-flight imaging",
    "citation_count": 96,
    "authors": [
      "Shuochen Su",
      "Felix Heide",
      "Gordon Wetzstein",
      "Wolfgang Heidrich"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Srinivasan_Aperture_Supervision_for_CVPR_2018_paper.html": {
    "title": "Aperture Supervision for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "We present a novel method to train machine learning algorithms to estimate scene depths from a single image, by using the information provided by a camera's aperture as supervision. Prior works use a depth sensor's outputs or images of the same scene from alternate viewpoints as supervision, while our method instead uses images from the same viewpoint taken with a varying camera aperture. To enable learning algorithms to use aperture effects as supervision, we introduce two differentiable aperture rendering functions that use the input image and predicted depths to simulate the depth-of-field effects caused by real camera apertures. We train a monocular depth estimation network end-to-end to predict the scene depths that best explain these finite aperture images as defocus-blurred renderings of the input all-in-focus image",
    "checked": true,
    "id": "01f5edca20d6f4ed094826afacf6caaeefff9bb9",
    "semantic_title": "aperture supervision for monocular depth estimation",
    "citation_count": 56,
    "authors": [
      "Pratul P. Srinivasan",
      "Rahul Garg",
      "Neal Wadhwa",
      "Ren Ng",
      "Jonathan T. Barron"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sakakibara_Seeing_Temporal_Modulation_CVPR_2018_paper.html": {
    "title": "Seeing Temporal Modulation of Lights From Standard Cameras",
    "volume": "main",
    "abstract": "In this paper, we propose a novel method for measuring the temporal modulation of lights by using off-the-shelf cameras. In particular, we show that the invisible flicker patterns of various lights such as fluorescent lights can be measured by a simple combination of an off-the-shelf camera and any moving object with specular reflection. Unlike the existing methods, we do not need high speed cameras nor specially designed coded exposure cameras. Based on the extracted flicker patterns of environment lights, we also propose an efficient method for deblurring motion blurs in images. The proposed method enables us to deblur images with better frequency characteristics, which are induced by the flicker patterns of environment lights. The real image experiments show the efficiency of the proposed method",
    "checked": true,
    "id": "5bec0f5732e4cb52f87ece81ad1a9fb4fbebcdb2",
    "semantic_title": "seeing temporal modulation of lights from standard cameras",
    "citation_count": 3,
    "authors": [
      "Naoki Sakakibara",
      "Fumihiko Sakaue",
      "Jun Sato"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Levis_Statistical_Tomography_of_CVPR_2018_paper.html": {
    "title": "Statistical Tomography of Microscopic Life",
    "volume": "main",
    "abstract": "We achieve tomography of 3D volumetric natural objects, where each projected 2D image corresponds to a different specimen. Each specimen has unknown random 3D orientation, location, and scale. This imaging scenario is relevant to microscopic and mesoscopic organisms, aerosols and hydrosols viewed naturally by a microscope. In-class scale variation inhibits prior single-particle reconstruction methods. We thus generalize tomographic recovery to account for all degrees of freedom of a similarity transformation. This enables geometric self-calibration in imaging of transparent objects. We make the computational load manageable and reach good quality reconstruction in a short time. This enables extraction of statistics that are important for a scientific study of specimen populations, specifically size distribution parameters. We apply the method to study of plankton",
    "checked": true,
    "id": "e5ab9ccda9185584f17bf91c1f32ea6a80892257",
    "semantic_title": "statistical tomography of microscopic life",
    "citation_count": 11,
    "authors": [
      "Aviad Levis",
      "Yoav Y. Schechner",
      "Ronen Talmon"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mohan_Divide_and_Conquer_CVPR_2018_paper.html": {
    "title": "Divide and Conquer for Full-Resolution Light Field Deblurring",
    "volume": "main",
    "abstract": "The increasing popularity of computational light field (LF) cameras has necessitated the need for tackling motion blur which is a ubiquitous phenomenon in hand-held photography. The state-of-the-art method for blind deblurring of LFs of general 3D scenes is limited to handling only downsampled LF, both in spatial and angular resolution. This is due to the computational overhead involved in processing data-hungry full-resolution 4D LF altogether. Moreover, the method warrants high-end GPUs for optimization and is ineffective for wide-angle settings and irregular camera motion. In this paper, we introduce a new blind motion deblurring strategy for LFs which alleviates these limitations significantly. Our model achieves this by isolating 4D LF motion blur across the 2D subaperture images, thus paving the way for independent deblurring of these subaperture images. Furthermore, our model accommodates common camera motion parameterization across the subaperture images. Consequently, blind deblurring of any single subaperture image elegantly paves the way for cost-effective non-blind deblurring of the other subaperture images. Our approach is CPU-efficient computationally and can effectively deblur full-resolution LFs",
    "checked": true,
    "id": "451715cdce3a3ec33995b3f56c1242a168fffc65",
    "semantic_title": "divide and conquer for full-resolution light field deblurring",
    "citation_count": 8,
    "authors": [
      "M. R. Mahesh Mohan",
      "A. N. Rajagopalan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Multispectral_Image_Intrinsic_CVPR_2018_paper.html": {
    "title": "Multispectral Image Intrinsic Decomposition via Subspace Constraint",
    "volume": "main",
    "abstract": "Multispectral images contain many clues of surface characteristics of the objects, thus can be used in many computer vision tasks, e.g., recolorization and segmentation. However, due to the complex geometry structure of natural scenes, the spectra curves of the same surface can look very different under different illuminations and from different angles. In this paper, a new Multispectral Image Intrinsic Decomposition model (MIID) is presented to decompose the shading and reflectance from a single multispectral image. We extend the Retinex model, which is proposed for RGB image intrinsic decomposition, for multispectral domain. Based on this, a subspace constraint is introduced to both the shading and reflectance spectral space to reduce the ill-posedness of the problem and make the problem solvable. A dataset of 22 scenes is given with the ground truth of shadings and reflectance to facilitate objective evaluations. The experiments demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "1caa26c1acee0721744795fe7721089caf21e8c2",
    "semantic_title": "multispectral image intrinsic decomposition via subspace constraint",
    "citation_count": 8,
    "authors": [
      "Qian Huang",
      "Weixin Zhu",
      "Yang Zhao",
      "Linsen Chen",
      "Yao Wang",
      "Tao Yue",
      "Xun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Karaimer_Improving_Color_Reproduction_CVPR_2018_paper.html": {
    "title": "Improving Color Reproduction Accuracy on Cameras",
    "volume": "main",
    "abstract": "One of the key operations performed on a digital camera is to map the sensor-specific color space to a standard perceptual color space. This procedure involves the application of a white-balance correction followed by a color space transform. The current approach for this colorimetric mapping is based on an interpolation of pre-calibrated color space transforms computed for two fixed illuminations (i.e., two white-balance settings). Images captured under different illuminations are subject to less color accuracy due to the use of this interpolation process. In this paper, we discuss the limitations of the current colorimetric mapping approach and propose two methods that are able to improve color accuracy. We evaluate our approach on seven different cameras and show improvements of up to 30% (DSLR cameras) and 59% (mobile phone cameras) in terms of color reproduction error",
    "checked": true,
    "id": "033e93c0c3a4bd0260a5a1d8e6cdc7c2840ee153",
    "semantic_title": "improving color reproduction accuracy on cameras",
    "citation_count": 35,
    "authors": [
      "Hakki Can Karaimer",
      "Michael S. Brown"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html": {
    "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
    "volume": "main",
    "abstract": "In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block ``R(2+1)D'' which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51",
    "checked": true,
    "id": "89c3050522a0bb9820c32dc7444e003ef0d3e2e4",
    "semantic_title": "a closer look at spatiotemporal convolutions for action recognition",
    "citation_count": 3029,
    "authors": [
      "Du Tran",
      "Heng Wang",
      "Lorenzo Torresani",
      "Jamie Ray",
      "Yann LeCun",
      "Manohar Paluri"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_Inferring_Shared_Attention_CVPR_2018_paper.html": {
    "title": "Inferring Shared Attention in Social Scene Videos",
    "volume": "main",
    "abstract": "This paper addresses a new problem of inferring shared attention in third-person social scene videos. Shared attention is a phenomenon that two or more individuals simultaneously look at a common target in social scenes. Perceiving and identifying shared attention in videos plays crucial roles in social activities and social scene understanding. We propose a spatial-temporal neural network to detect shared attention intervals in videos and predict shared attention locations in frames. In each video frame, human gaze directions and potential target boxes are two key features for spatially detecting shared attention in the social scene. In temporal domain, a convolutional Long Short- Term Memory network utilizes the temporal continuity and transition constraints to optimize the predicted shared attention heatmap. We collect a new dataset VideoCoAtt from public TV show videos, containing 380 complex video sequences with more than 492,000 frames that include diverse social scenes for shared attention study. Experiments on this dataset show that our model can effectively infer shared attention in videos. We also empirically verify the effectiveness of different components in our model",
    "checked": true,
    "id": "037ad2159097dcb7ff29b6c328b859fb64f7078e",
    "semantic_title": "inferring shared attention in social scene videos",
    "citation_count": 74,
    "authors": [
      "Lifeng Fan",
      "Yixin Chen",
      "Ping Wei",
      "Wenguan Wang",
      "Song-Chun Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Making_Convolutional_Networks_CVPR_2018_paper.html": {
    "title": "Making Convolutional Networks Recurrent for Visual Sequence Learning",
    "volume": "main",
    "abstract": "Recurrent neural networks (RNNs) have emerged as a powerful model for a broad range of machine learning problems that involve sequential data. While an abundance of work exists to understand and improve RNNs in the context of language and audio signals such as language modeling and speech recognition, relatively little attention has been paid to analyze or modify RNNs for visual sequences, which by nature have distinct properties. In this paper, we aim to bridge this gap and present the first large-scale exploration of RNNs for visual sequence learning. In particular, with the intention of leveraging the strong generalization capacity of pre-trained convolutional neural networks (CNNs), we propose a novel and effective approach, PreRNN, to make pre-trained CNNs recurrent by transforming convolutional layers or fully connected layers into recurrent layers. We conduct extensive evaluations on three representative visual sequence learning tasks: sequential face alignment, dynamic hand gesture recognition, and action recognition. Our experiments reveal that PreRNN consistently outperforms the traditional RNNs and achieves state-of-the-art results on the three applications, suggesting that PreRNN is more suitable for visual sequence learning",
    "checked": true,
    "id": "13f1a38bc8542eb7d9d5d3b13d326fbec1f01783",
    "semantic_title": "making convolutional networks recurrent for visual sequence learning",
    "citation_count": 41,
    "authors": [
      "Xiaodong Yang",
      "Pavlo Molchanov",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sultani_Real-World_Anomaly_Detection_CVPR_2018_paper.html": {
    "title": "Real-World Anomaly Detection in Surveillance Videos",
    "volume": "main",
    "abstract": "Surveillance videos are able to capture a variety of realistic anomalies. In this paper, we propose to learn anomalies by exploiting both normal and anomalous videos. To avoid annotating the anomalous segments or clips in training videos, which is very time consuming, we propose to learn anomaly through the deep multiple instance ranking framework by leveraging weakly labeled training videos, ie the training labels (anomalous or normal) are at video-level instead of clip-level. In our approach, we consider normal and anomalous videos as bags and video segments as instances in multiple instance learning (MIL), and automatically learn a deep anomaly ranking model that predicts high anomaly scores for anomalous video segments. Furthermore, we introduce sparsity and temporal smoothness constraints in the ranking loss function to better localize anomaly during training. We also introduce a new large-scale first of its kind dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies such as fighting, road accident, burglary, robbery, etc. as well as normal activities. This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities. Our experimental results show that our MIL method for anomaly detection achieves significant improvement on anomaly detection performance as compared to the state-of-the-art approaches. We provide the results of several recent deep learning baselines on anomalous activity recognition. The low recognition performance of these baselines reveals that our dataset is very challenging and opens more opportunities for future work",
    "checked": true,
    "id": "96ed8ce9ef9fc475db9e02c79f984dc110409b62",
    "semantic_title": "real-world anomaly detection in surveillance videos",
    "citation_count": 1486,
    "authors": [
      "Waqas Sultani",
      "Chen Chen",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Viewpoint-Aware_Attentive_Multi-View_CVPR_2018_paper.html": {
    "title": "Viewpoint-Aware Attentive Multi-View Inference for Vehicle Re-Identification",
    "volume": "main",
    "abstract": "Vehicle re-identification (re-ID) has the huge potential to contribute to the intelligent video surveillance. However, it suffers from challenges that different vehicle identities with a similar appearance have little inter-instance discrepancy while one vehicle usually has large intra-instance differences under viewpoint and illumination variations. Previous methods address vehicle re-ID by simply using visual features from originally captured views and usually exploit the spatial-temporal information of the vehicles to refine the results. In this paper, we propose a Viewpoint-aware Attentive Multi-view Inference (VAMI) model that only requires visual information to solve the multi-view vehicle re-ID problem. Given vehicle images of arbitrary viewpoints, the VAMI extracts the single-view feature for each input image and aims to transform the features into a global multi-view feature representation so that pairwise distance metric learning can be better optimized in such a viewpoint-invariant feature space. The VAMI adopts a viewpoint-aware attention model to select core regions at different viewpoints and implement effective multi-view feature inference by an adversarial training architecture. Extensive experiments validate the effectiveness of each proposed component and illustrate that our approach achieves consistent improvements over state-of-the-art vehicle re-ID methods on two public datasets: VeRi and VehicleID",
    "checked": true,
    "id": "95ae9f187044b0000ca371ce6557a076a5ddede3",
    "semantic_title": "viewpoint-aware attentive multi-view inference for vehicle re-identification",
    "citation_count": 193,
    "authors": [
      "Yi Zhou",
      "Ling Shao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Efficient_Video_Object_CVPR_2018_paper.html": {
    "title": "Efficient Video Object Segmentation via Network Modulation",
    "volume": "main",
    "abstract": "Video object segmentation targets segmenting a specific object throughout a video sequence when given only an annotated first frame. Recent deep learning based approaches find it effective to fine-tune a general-purpose segmentation model on the annotated frame using hundreds of iterations of gradient descent. Despite the high accuracy that these methods achieve, the fine-tuning process is inefficient and fails to meet the requirements of real world applications. We propose a novel approach that uses a single forward pass to adapt the segmentation model to the appearance of a specific object. Specifically, a second meta neural network named modulator is trained to manipulate the intermediate layers of the segmentation network given limited visual and spatial information of the target object. The experiments show that our approach is 70 times faster than fine-tuning approaches and achieves similar accuracy",
    "checked": true,
    "id": "4a70c20ad66e5f3bb12fccd84c63ba619053c811",
    "semantic_title": "efficient video object segmentation via network modulation",
    "citation_count": 350,
    "authors": [
      "Linjie Yang",
      "Yanran Wang",
      "Xuehan Xiong",
      "Jianchao Yang",
      "Aggelos K. Katsaggelos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ding_Weakly-Supervised_Action_Segmentation_CVPR_2018_paper.html": {
    "title": "Weakly-Supervised Action Segmentation With Iterative Soft Boundary Assignment",
    "volume": "main",
    "abstract": "In this work, we address the task of weakly-supervised human action segmentation in long, untrimmed videos. Recent methods have relied on expensive learning models, such as Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM). However, these methods suffer from expensive computational cost, thus are unable to be deployed in large scale. To overcome the limitations, the keys to our design are efficiency and scalability. We propose a novel action modeling framework, which consists of a new temporal convolutional network, named Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting frame-wise action labels, and a novel training strategy for weakly-supervised sequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align action sequences and update the network in an iterative fashion. The proposed framework is evaluated on two benchmark datasets, Breakfast and Hollywood Extended, with four different evaluation metrics. Extensive experimental results show that our methods achieve competitive or superior performance to state-of-the-art methods",
    "checked": true,
    "id": "6c6ce420976f958e7582a2f452c3a541faa82074",
    "semantic_title": "weakly-supervised action segmentation with iterative soft boundary assignment",
    "citation_count": 180,
    "authors": [
      "Li Ding",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Depth-Aware_Stereo_Video_CVPR_2018_paper.html": {
    "title": "Depth-Aware Stereo Video Retargeting",
    "volume": "main",
    "abstract": "As compared with traditional video retargeting, stereo video retargeting poses new challenges because stereo video contains the depth information of salient objects and its time dynamics. In this work, we propose a depth-aware stereo video retargeting method by imposing the depth fidelity constraint. The proposed depth-aware retargeting method reconstructs the 3D scene to obtain the depth information of salient objects. We cast it as a constrained optimization problem, where the total cost function includes the shape, temporal and depth distortions of salient objects. As a result, the solution can preserve the shape, temporal and depth fidelity of salient objects simultaneously. It is demonstrated by experimental results that the depth-aware retargeting method achieves higher retargeting quality and provides better user experience",
    "checked": true,
    "id": "5b77a5ba437ec049954ac49762c48bc440a3ee5a",
    "semantic_title": "depth-aware stereo video retargeting",
    "citation_count": 31,
    "authors": [
      "Bing Li",
      "Chia-Wen Lin",
      "Boxin Shi",
      "Tiejun Huang",
      "Wen Gao",
      "C.-C. Jay Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Instance_Embedding_Transfer_CVPR_2018_paper.html": {
    "title": "Instance Embedding Transfer to Unsupervised Video Object Segmentation",
    "volume": "main",
    "abstract": "We propose a method for unsupervised video object segmentation by transferring the knowledge encapsulated in image-based instance embedding networks. The instance embedding network produces an embedding vector for each pixel that enables identifying all pixels belonging to the same object. Though trained on static images, the instance embeddings are stable over consecutive video frames, which allows us to link objects together over time. Thus, we adapt the instance networks trained on static images to video object segmentation and incorporate the embeddings with objectness and optical flow features, without model retraining or online fine-tuning. The proposed method outperforms state-of-the-art unsupervised segmentation methods in the DAVIS dataset and the FBMS dataset",
    "checked": true,
    "id": "3db3adb88532c9b8f3de0046a16753e7230c01b0",
    "semantic_title": "instance embedding transfer to unsupervised video object segmentation",
    "citation_count": 105,
    "authors": [
      "Siyang Li",
      "Bryan Seybold",
      "Alexey Vorobyov",
      "Alireza Fathi",
      "Qin Huang",
      "C.-C. Jay Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Future_Frame_Prediction_CVPR_2018_paper.html": {
    "title": "Future Frame Prediction for Anomaly Detection – A New Baseline",
    "volume": "main",
    "abstract": "Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to tackle the anomaly detection problem within a video prediction framework. To the best of our knowledge, this is the first work that leverages the difference between a predicted future frame and its ground truth to detect an abnormal event. To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task. Such spatial and motion constraints facilitate the future frame prediction for normal events, and consequently facilitate to identify those abnormal events that do not conform the expectation. Extensive experiments on both a toy dataset and some publicly available datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events",
    "checked": true,
    "id": "8a6acba7fb2aad1299fcf35701417e063d410ed4",
    "semantic_title": "future frame prediction for anomaly detection - a new baseline",
    "citation_count": 1078,
    "authors": [
      "Wen Liu",
      "Weixin Luo",
      "Dongze Lian",
      "Shenghua Gao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.html": {
    "title": "Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?",
    "volume": "main",
    "abstract": "The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available. https://github.com/kenshohara/3D-ResNets-PyTorch",
    "checked": true,
    "id": "d716435f0cb0cac56237f74b1ced940aabce6a2b",
    "semantic_title": "can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?",
    "citation_count": 1934,
    "authors": [
      "Kensho Hara",
      "Hirokatsu Kataoka",
      "Yutaka Satoh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Dynamic_Video_Segmentation_CVPR_2018_paper.html": {
    "title": "Dynamic Video Segmentation Network",
    "volume": "main",
    "abstract": "In this paper, we present a detailed design of dynamic video segmentation network (DVSNet) for fast and efficient semantic video segmentation. DVSNet consists of two convolutional neural networks: a segmentation network and a flow network. The former generates highly accurate semantic segmentations, but is deeper and slower. The latter is much faster than the former, but its output requires further processing to generate less accurate semantic segmentations. We explore the use of a decision network to adaptively assign different frame regions to different networks based on a metric called expected confidence score. Frame regions with a higher expected confidence score traverse the flow network. Frame regions with a lower expected confidence score have to pass through the segmentation network. We have extensively performed experiments on various configurations of DVSNet, and investigated a number of variants for the proposed decision network. The experimental results show that our DVSNet is able to achieve up to 70.4% mIoU at 19.8 fps on the Cityscape dataset. A high speed version of DVSNet is able to deliver an fps of 30.4 with 63.2% mIoU on the same dataset. DVSNet is also able to reduce up to 95% of the computational workloads",
    "checked": true,
    "id": "60e7d66adeeadd8009ff2216a40fbf0cc94514c3",
    "semantic_title": "dynamic video segmentation network",
    "citation_count": 126,
    "authors": [
      "Yu-Syuan Xu",
      "Tsu-Jui Fu",
      "Hsuan-Kung Yang",
      "Chun-Yi Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Recognize_Actions_by_CVPR_2018_paper.html": {
    "title": "Recognize Actions by Disentangling Components of Dynamics",
    "volume": "main",
    "abstract": "Despite the remarkable progress in action recognition over the past several years, existing methods remain limited in efficiency and effectiveness. The methods treating appearance and motion as separate streams are usually subject to the cost of optical flow computation, while those relying on 3D convolution on the original video frames often yield inferior performance in practice. In this paper, we propose a new ConvNet architecture for video representation learning, which can derive disentangled components of dynamics purely from raw video frames, without the need of optical flow estimation. Particularly, the learned representation comprises three components for representing static appearance, apparent motion, and appearance changes. We introduce 3D pooling, cost volume processing, and warped feature differences, respectively for extracting the three components above. These modules are incorporated as three branches in our unified network, which share the underlying features and are learned jointly in an end-to-end manner. On two large datasets UCF101 and Kinetics our method obtained competitive performances with high efficiency, using only the RGB frame sequence as input",
    "checked": true,
    "id": "d66e13a5e128a4ecad78e0c1c128893684292dec",
    "semantic_title": "recognize actions by disentangling components of dynamics",
    "citation_count": 68,
    "authors": [
      "Yue Zhao",
      "Yuanjun Xiong",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gao_Motion-Appearance_Co-Memory_Networks_CVPR_2018_paper.html": {
    "title": "Motion-Appearance Co-Memory Networks for Video Question Answering",
    "volume": "main",
    "abstract": "Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based these observations, we propose a motion-appearance co-memory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-of-the-art significantly on all four tasks of TGIF-QA",
    "checked": true,
    "id": "f45c3a83e5c6276c6655c5df5833ab6b75e17bdf",
    "semantic_title": "motion-appearance co-memory networks for video question answering",
    "citation_count": 241,
    "authors": [
      "Jiyang Gao",
      "Runzhou Ge",
      "Kan Chen",
      "Ram Nevatia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Learning_to_Understand_CVPR_2018_paper.html": {
    "title": "Learning to Understand Image Blur",
    "volume": "main",
    "abstract": "While many approaches have been proposed to estimate and remove blur in a photo, few efforts were made to have an algorithm automatically understand the blur desirability: whether the blur is desired or not, and how it affects the quality of the photo. Such a task not only relies on low-level visual features to identify blurry regions, but also requires high-level understanding of the image content as well as user intent during photo capture. In this paper, we propose a unified framework to estimate a spatially-varying blur map and understand its desirability in terms of image quality at the same time. In particular, we use a dilated fully convolutional neural network with pyramid pooling and boundary refinement layers to generate high-quality blur response maps. If blur exists, we classify its desirability to three levels ranging from good to bad, by distilling high-level semantics and learning an attention map to adaptively localize the important content in the image. The whole framework is end-to-end jointly trained with both supervisions of pixel-wise blur responses and image-wise blur desirability levels. Considering the limitations of existing image blur datasets, we collected a new large-scale dataset with both annotations to facilitate training. The proposed methods are extensively evaluated on two datasets and demonstrate state-of-the-art performance on both tasks",
    "checked": true,
    "id": "f0df873880d70cbbb84e5a8798c50bda0f4c2b89",
    "semantic_title": "learning to understand image blur",
    "citation_count": 42,
    "authors": [
      "Shanghang Zhang",
      "Xiaohui Shen",
      "Zhe Lin",
      "Radomír Měch",
      "João P. Costeira",
      "José M. F. Moura"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.html": {
    "title": "Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation",
    "volume": "main",
    "abstract": "We propose a novel end-to-end trainable, deep, encoder-decoder architecture for single-pass semantic segmentation. Our approach is based on a cascaded architecture with feature-level long-range skip connections. The encoder incorporates the structure of ResNeXt's residual building blocks and adopts the strategy of repeating a building block that aggregates a set of transformations with the same topology. The decoder features a novel architecture, consisting of blocks, that (i) capture context information, (ii) generate semantic features, and (iii) enable fusion between different output resolutions. Crucially, we introduce dense decoder shortcut connections to allow decoder blocks to use semantic feature maps from all previous decoder levels, i.e. from all higher-level feature maps. The dense decoder connections allow for effective information propagation from one decoder block to another, as well as for multi-level feature fusion that significantly improves the accuracy. Importantly, these connections allow our method to obtain state-of-the-art performance on several challenging datasets, without the need of time-consuming multi-scale averaging of previous works",
    "checked": true,
    "id": "985b6a10c1bb9c8908d6d299c7ab0bba76df2259",
    "semantic_title": "dense decoder shortcut connections for single-pass semantic segmentation",
    "citation_count": 119,
    "authors": [
      "Piotr Bilinski",
      "Victor Prisacariu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kaneko_Generative_Adversarial_Image_CVPR_2018_paper.html": {
    "title": "Generative Adversarial Image Synthesis With Decision Tree Latent Controller",
    "volume": "main",
    "abstract": "This paper proposes the decision tree latent controller generative adversarial network (DTLC-GAN), an extension of a GAN that can learn hierarchically interpretable representations without relying on detailed supervision. To impose a hierarchical inclusion structure on latent variables, we incorporate a new architecture called the DTLC into the generator input. The DTLC has a multiple-layer tree structure in which the ON or OFF of the child node codes is controlled by the parent node codes. By using this architecture hierarchically, we can obtain the latent space in which the lower layer codes are selectively used depending on the higher layer ones. To make the latent codes capture salient semantic features of images in a hierarchically disentangled manner in the DTLC, we also propose a hierarchical conditional mutual information regularization and optimize it with a newly defined curriculum learning method that we propose as well. This makes it possible to discover hierarchically interpretable representations in a layer-by-layer manner on the basis of information gain by only using a single DTLC-GAN model. We evaluated the DTLC-GAN on various datasets, i.e., MNIST, CIFAR-10, Tiny ImageNet, 3D Faces, and CelebA, and confirmed that the DTLC-GAN can learn hierarchically interpretable representations with either unsupervised or weakly supervised settings. Furthermore, we applied the DTLC-GAN to image-retrieval tasks and showed its effectiveness in representation learning",
    "checked": true,
    "id": "b23254b32e36e7d9782b7a076d6ad7b475d56c45",
    "semantic_title": "generative adversarial image synthesis with decision tree latent controller",
    "citation_count": 25,
    "authors": [
      "Takuhiro Kaneko",
      "Kaoru Hiramatsu",
      "Kunio Kashino"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Learning_a_Discriminative_CVPR_2018_paper.html": {
    "title": "Learning a Discriminative Prior for Blind Image Deblurring",
    "volume": "main",
    "abstract": "We present an effective blind image deblurring method based on a data-driven discriminative prior. Our work is motivated by the fact that a good image prior should favor clear images over blurred images. To obtain such an image prior for deblurring, we formulate the image prior as a binary classifier which can be achieved by a deep convolutional neural network (CNN). The learned image prior has a significant discriminative property and is able to distinguish whether the image is clear or not. Embedded into the maximum a posterior (MAP) framework, it helps blind deblurring on various scenarios, including natural, face, text, and low-illumination images. However, it is difficult to optimize the deblurring method with the learned image prior as it involves a non-linear CNN. Therefore, we develop an efficient numerical approach based on the half-quadratic splitting method and gradient decent algorithm to solve the proposed model. Furthermore, the proposed model can be easily extended to non-uniform deblurring. Both qualitative and quantitative experimental results show that our method performs favorably against state-of-the-art algorithms as well as domain-specific image deblurring approaches",
    "checked": true,
    "id": "9c72e41a6df7b42882fa316ba428b21846516e7e",
    "semantic_title": "learning a discriminative prior for blind image deblurring",
    "citation_count": 143,
    "authors": [
      "Lerenhan Li",
      "Jinshan Pan",
      "Wei-Sheng Lai",
      "Changxin Gao",
      "Nong Sang",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sajjadi_Frame-Recurrent_Video_Super-Resolution_CVPR_2018_paper.html": {
    "title": "Frame-Recurrent Video Super-Resolution",
    "volume": "main",
    "abstract": "Recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution (LR) frames to generate high-quality images. Current state-of-the-art methods process a batch of LR frames to generate a single high-resolution (HR) frame and run this scheme in a sliding window fashion over the entire video, effectively treating the problem as a large number of separate multi-frame super-resolution tasks. This approach has two main weaknesses: 1) Each input frame is processed and warped multiple times, increasing the computational cost, and 2) each output frame is estimated independently conditioned on the input frames, limiting the system's ability to produce temporally consistent results. In this work, we propose an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame. This naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step. Furthermore, due to its recurrent nature, the proposed method has the ability to assimilate a large number of previous frames without increased computational demands. Extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art",
    "checked": true,
    "id": "aa3f7330f9b27e57745c4d14753893964ad91d5a",
    "semantic_title": "frame-recurrent video super-resolution",
    "citation_count": 507,
    "authors": [
      "Mehdi S. M. Sajjadi",
      "Raviteja Vemulapalli",
      "Matthew Brown"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Discovering_Point_Lights_CVPR_2018_paper.html": {
    "title": "Discovering Point Lights With Intensity Distance Fields",
    "volume": "main",
    "abstract": "We introduce the light localization problem. A scene is illuminated by a set of unobserved isotropic point lights. Given the geometry, materials, and illuminated appearance of the scene, the light localization problem is to completely recover the number, positions, and intensities of the lights. We first present a scene transform that identifies likely light positions. Based on this transform, we develop an iterative algorithm to locate remaining lights and determine all light intensities. We demonstrate the success of this method in a large set of 2D synthetic scenes, and show that it extends to 3D, in both synthetic scenes and real-world scenes",
    "checked": true,
    "id": "9845c042bd6789c3716c148bd303fef29e7c8acb",
    "semantic_title": "discovering point lights with intensity distance fields",
    "citation_count": 9,
    "authors": [
      "Edward Zhang",
      "Michael F. Cohen",
      "Brian Curless"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Video_Rain_Streak_CVPR_2018_paper.html": {
    "title": "Video Rain Streak Removal by Multiscale Convolutional Sparse Coding",
    "volume": "main",
    "abstract": "Videos captured by outdoor surveillance equipments sometimes contain unexpected rain streaks, which brings difficulty in subsequent video processing tasks. Rain streak removal from a video is thus an important topic in recent computer vision research. In this paper, we raise two intrinsic characteristics specifically possessed by rain streaks. Firstly, the rain streaks in a video contain repetitive local patterns sparsely scattered over different positions of the video. Secondly, the rain streaks are with multiscale configurations due to their occurrence on positions with different distances to the cameras. Based on such understanding, we specifically formulate both characteristics into a multiscale convolutional sparse coding (MS-CSC) model for the video rain streak removal task. Specifically, we use multiple convolutional filters convolved on the sparse feature maps to deliver the former characteristic, and further use multiscale filters to represent different scales of rain streaks. Such a new encoding manner makes the proposed method capable of properly extracting rain streaks from videos, thus getting fine video deraining effects. Experiments implemented on synthetic and real videos verify the superiority of the proposed method, as compared with the state-of-the-art ones along this research line, both visually and quantitatively",
    "checked": true,
    "id": "0204cbbf200204eec81f7fd3da70ab5cdf48f214",
    "semantic_title": "video rain streak removal by multiscale convolutional sparse coding",
    "citation_count": 167,
    "authors": [
      "Minghan Li",
      "Qi Xie",
      "Qian Zhao",
      "Wei Wei",
      "Shuhang Gu",
      "Jing Tao",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Stereoscopic_Neural_Style_CVPR_2018_paper.html": {
    "title": "Stereoscopic Neural Style Transfer",
    "volume": "main",
    "abstract": "This paper presents the first attempt at stereoscopic neural style transfer, which responds to the emerging demand for 3D movies or AR/VR. We start with a careful examination of applying existing monocular style transfer methods to left and right views of stereoscopic images separately. This reveals that the original disparity consistency cannot be well preserved in the final stylization results, which causes 3D fatigue to the viewers. To address this issue, we incorporate a new disparity loss into the widely adopted style loss function by enforcing the bidirectional disparity constraint in non-occluded regions. For a practical real-time solution, we propose the first feed-forward network by jointly training a stylization sub-network and a disparity sub-network, and integrate them in a feature level middle domain. Our disparity sub-network is also the first end-to-end network for simultaneous bidirectional disparity and occlusion mask estimation. Finally, our network is effectively extended to stereoscopic videos, by considering both temporal coherence and disparity consistency. We will show that the proposed method clearly outperforms the baseline algorithms both quantitatively and qualitatively",
    "checked": true,
    "id": "fa4568b0c02d63eea0ddc7faea9ce069f6d6285a",
    "semantic_title": "stereoscopic neural style transfer",
    "citation_count": 113,
    "authors": [
      "Dongdong Chen",
      "Lu Yuan",
      "Jing Liao",
      "Nenghai Yu",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Multi-Frame_Quality_Enhancement_CVPR_2018_paper.html": {
    "title": "Multi-Frame Quality Enhancement for Compressed Video",
    "volume": "main",
    "abstract": "The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, ignoring the similarity between consecutive frames. In this paper, we investigate that heavy quality fluctuation exists across compressed video frames, and thus low quality frames can be enhanced using the neighboring high quality frames, seen as Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as a first attempt in this direction. In our approach, we firstly develop a Support Vector Machine (SVM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are as the input. The MF-CNN compensates motion between the non-PQF and PQFs through the Motion Compensation subnet (MC-subnet). Subsequently, the Quality Enhancement subnet (QE-subnet) reduces compression artifacts of the non-PQF with the help of its nearest PQFs. Finally, the experiments validate the effectiveness and generality of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video. The code of our MFQE approach is available at https://github.com/ryangBUAA/MFQE.git",
    "checked": true,
    "id": "c7a81d62767c12e72e8ba0ea427921f57ce67c78",
    "semantic_title": "multi-frame quality enhancement for compressed video",
    "citation_count": 205,
    "authors": [
      "Ren Yang",
      "Mai Xu",
      "Zulin Wang",
      "Tianyi Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Baslamisli_CNN_Based_Learning_CVPR_2018_paper.html": {
    "title": "CNN Based Learning Using Reflection and Retinex Models for Intrinsic Image Decomposition",
    "volume": "main",
    "abstract": "Most of the traditional work on intrinsic image decomposition rely on deriving priors about scene characteristics. On the other hand, recent research use deep learning models as in-and-out black box and do not consider the well-established, traditional image formation process as the basis of their intrinsic learning process. As a consequence, although current deep learning approaches show superior performance when considering quantitative benchmark results, traditional approaches are still dominant in achieving high qualitative results. In this paper, the aim is to exploit the best of the two worlds. A method is proposed that (1) is empowered by deep learning capabilities, (2) considers a physics-based reflection model to steer the learning process, and (3) exploits the traditional approach to obtain intrinsic images by exploiting reflectance and shading gradient information. The proposed model is fast to compute and allows for the integration of all intrinsic components. To train the new model, an object centered large-scale datasets with intrinsic ground-truth images are created. The evaluation results demonstrate that the new model outperforms existing methods. Visual inspection shows that the image formation loss function augments color reproduction and the use of gradient information produces sharper edges. Datasets, models and higher resolution images are available at https://ivi.fnwi.uva.nl/cv/retinet",
    "checked": true,
    "id": "4526bdad891a628ff67700c18a82a2fdd7d9b8f1",
    "semantic_title": "cnn based learning using reflection and retinex models for intrinsic image decomposition",
    "citation_count": 66,
    "authors": [
      "Anil S. Baslamisli",
      "Hoang-An Le",
      "Theo Gevers"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yoo_Image_Restoration_by_CVPR_2018_paper.html": {
    "title": "Image Restoration by Estimating Frequency Distribution of Local Patches",
    "volume": "main",
    "abstract": "In this paper, we propose a method to solve the image restoration problem, which tries to restore the details of a corrupted image, especially due to the loss caused by JPEG compression. We have treated an image in the frequency domain to explicitly restore the frequency components lost during image compression. In doing so, the distribution in the frequency domain is learned using the cross entropy loss. Unlike recent approaches, we have reconstructed the details of an image without using the scheme of adversarial training. Rather, the image restoration problem is treated as a classification problem to determine the frequency coefficient for each frequency band in an image patch. In this paper, we show that the proposed method effectively restores a JPEG-compressed image with more detailed high frequency components, making the restored image more vivid",
    "checked": true,
    "id": "9533a46cb18a64d0fc87bb762cbaf9d75efaff60",
    "semantic_title": "image restoration by estimating frequency distribution of local patches",
    "citation_count": 57,
    "authors": [
      "Jaeyoung Yoo",
      "Sang-ho Lee",
      "Nojun Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Korman_Latent_RANSAC_CVPR_2018_paper.html": {
    "title": "Latent RANSAC",
    "volume": "main",
    "abstract": "We present a method that can evaluate a RANSAC hypothesis in constant time, i.e. independent of the size of the data. A key observation here is that correct hypotheses are tightly clustered together in the latent parameter domain. In a manner similar to the generalized Hough transform we seek to find this cluster, only that we need as few as two votes for a successful detection. Rapidly locating such pairs of similar hypotheses is made possible by adapting the recent \"Random Grids\" range-search technique. We only perform the usual (costly) hypothesis verification stage upon the discovery of a close pair of hypotheses. We show that this event rarely happens for incorrect hypotheses, enabling a significant speedup of the RANSAC pipeline. The suggested approach is applied and tested on three robust estimation problems: camera localization, 3D rigid alignment and 2D-homography estimation. We perform rigorous testing on both synthetic and real datasets, demonstrating an improvement in efficiency without a compromise in accuracy. Furthermore, we achieve state-of-the-art 3D alignment results on the challenging ``Redwood'' loop-closure challenge",
    "checked": true,
    "id": "0d46da704128df541b5bbcb3d2a90f72d65cb869",
    "semantic_title": "latent ransac",
    "citation_count": 29,
    "authors": [
      "Simon Korman",
      "Roee Litman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper.html": {
    "title": "Two-Stream Convolutional Networks for Dynamic Texture Synthesis",
    "volume": "main",
    "abstract": "We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a randomly initialized input sequence is optimized to match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture. Finally, we quantitatively evaluate our texture synthesis approach with a thorough user study",
    "checked": true,
    "id": "380f159cf407d2aa52232b8f7f29ae2405c35c65",
    "semantic_title": "two-stream convolutional networks for dynamic texture synthesis",
    "citation_count": 55,
    "authors": [
      "Matthew Tesfaldet",
      "Marcus A. Brubaker",
      "Konstantinos G. Derpanis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bao_Towards_Open-Set_Identity_CVPR_2018_paper.html": {
    "title": "Towards Open-Set Identity Preserving Face Synthesis",
    "volume": "main",
    "abstract": "We propose a framework based on Generative Adversarial Networks to disentangle the identity and attributes of faces, such that we can conveniently recombine different identities and attributes for identity preserving face synthesis in open domains. Previous identity preserving face synthesis processes are largely confined to synthesizing faces with known identities that are already in the training dataset. To synthesize a face with identity outside the training dataset, our framework requires one input image of that subject to produce an identity vector, and any other input face image to extract an attribute vector capturing, e.g., pose, emotion, illumination, and even the background. We then recombine the identity vector and the attribute vector to synthesize a new face of the subject with the extracted attribute. Our proposed framework does not need to annotate the attributes of faces in any way. It is trained with an asymmetric loss function to better preserve the identity and stabilize the training process. It can also effectively leverage large amounts of unlabeled training face images to further improve the fidelity of the synthesized faces for subjects that are not presented in the labeled training face dataset. Our experiments demonstrate the efficacy of the proposed framework. We also present its usage in a much broader set of applications including face frontalization, face attribute morphing, and face adversarial example detection",
    "checked": true,
    "id": "c5b324f7f9abdffc1be83f640674beda81b74315",
    "semantic_title": "towards open-set identity preserving face synthesis",
    "citation_count": 248,
    "authors": [
      "Jianmin Bao",
      "Dong Chen",
      "Fang Wen",
      "Houqiang Li",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Akkaynak_A_Revised_Underwater_CVPR_2018_paper.html": {
    "title": "A Revised Underwater Image Formation Model",
    "volume": "main",
    "abstract": "The current underwater image formation model descends from atmospheric dehazing equations where attenuation is a weak function of wavelength. We recently showed that this model introduces significant errors and dependencies in the estimation of the direct transmission signal because underwater, light attenuates in a wavelength-dependent manner. Here, we show that the backscattered signal derived from the current model also suffers from dependencies that were previously unaccounted for. In doing so, we use oceanographic measurements to derive the physically valid space of backscatter, and further show that the wideband coefficients that govern backscatter are different than those that govern direct transmission, even though the current model treats them to be the same. We propose a revised equation for underwater image formation that takes these differences into account, and validate it through in situ experiments underwater. This revised model might explain frequent instabilities of current underwater color reconstruction models, and calls for the development of new methods",
    "checked": true,
    "id": "88410da1636ed7ae840af5feb1dffad99286c0b4",
    "semantic_title": "a revised underwater image formation model",
    "citation_count": 286,
    "authors": [
      "Derya Akkaynak",
      "Tali Treibitz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Barath_Graph-Cut_RANSAC_CVPR_2018_paper.html": {
    "title": "Graph-Cut RANSAC",
    "volume": "main",
    "abstract": "A novel method for robust estimation, called Graph-Cut RANSAC, GC-RANSAC in short, is introduced. To separate inliers and outliers, it runs the graph-cut algorithm in the local optimization (LO) step which is applied when a so-far-the-best model is found. The proposed LO step is conceptually simple, easy to implement, globally optimal and efficient. GC-RANSAC is shown experimentally, both on synthesized tests and real image pairs, to be more geometrically accurate than state-of-the-art methods on a range of problems, e.g. line fitting, homography, affine transformation, fundamental and essential matrix estimation. It runs in real-time for many problems at a speed approximately equal to that of the less accurate alternatives (in milliseconds on standard CPU)",
    "checked": true,
    "id": "6c0f45f3f05265f72ccce3448530a84fbc7816a7",
    "semantic_title": "graph-cut ransac",
    "citation_count": 322,
    "authors": [
      "Daniel Barath",
      "Jiří Matas"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lei_Temporal_Deformable_Residual_CVPR_2018_paper.html": {
    "title": "Temporal Deformable Residual Networks for Action Segmentation in Videos",
    "volume": "main",
    "abstract": "This paper is about temporal segmentation of human actions in videos. We introduce a new model -- temporal deformable residual network (TDRN) -- aimed at analyzing video intervals at multiple temporal scales for labeling video frames. Our TDRN computes two parallel temporal streams: i) Residual stream that analyzes video information at its full temporal resolution, and ii) Pooling/unpooling stream that captures long-range video information at different scales. The former facilitates local, fine-scale action segmentation, and the latter uses multiscale context for improving accuracy of frame classification. These two streams are computed by a set of temporal residual modules with deformable convolutions, and fused by temporal residuals at the full video resolution. Our evaluation on the University of Dundee 50 Salads, Georgia Tech Egocentric Activities, and JHU-ISI Gesture and Skill Assessment Working Set demonstrates that TDRN outperforms the state of the art in frame-wise segmentation accuracy, segmental edit score, and segmental overlap F1 score",
    "checked": true,
    "id": "4bfcedc30c05b68ba109b9ae71db42f1f1985770",
    "semantic_title": "temporal deformable residual networks for action segmentation in videos",
    "citation_count": 181,
    "authors": [
      "Peng Lei",
      "Sinisa Todorovic"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Nguyen_Weakly_Supervised_Action_CVPR_2018_paper.html": {
    "title": "Weakly Supervised Action Localization by Sparse Temporal Pooling Network",
    "volume": "main",
    "abstract": "We propose a weakly supervised temporal action localization algorithm on untrimmed videos using convolutional neural networks. Our algorithm learns from video-level class labels and predicts temporal intervals of human actions with no requirement of temporal localization annotations. We design our network to identify a sparse subset of key segments associated with target actions in a video using an attention module and fuse the key segments through adaptive temporal pooling. Our loss function is comprised of two terms that minimize the video-level action classification error and enforce the sparsity of the segment selection. At inference time, we extract and score temporal proposals using temporal class activations and class-agnostic attentions to estimate the time intervals that correspond to target actions. The proposed algorithm attains state-of-the-art results on the THUMOS14 dataset and outstanding performance on ActivityNet1.3 even with its weak supervision",
    "checked": true,
    "id": "c661d1940518445f350aa5e49ed16f815d90bec2",
    "semantic_title": "weakly supervised action localization by sparse temporal pooling network",
    "citation_count": 350,
    "authors": [
      "Phuc Nguyen",
      "Ting Liu",
      "Gautam Prasad",
      "Bohyung Han"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_PoseFlow_A_Deep_CVPR_2018_paper.html": {
    "title": "PoseFlow: A Deep Motion Representation for Understanding Human Behaviors in Videos",
    "volume": "main",
    "abstract": "Motion of the human body is the critical cue for understanding and characterizing human behavior in videos. Most existing approaches explore the motion cue using optical flows. However, optical flow usually contains motion on both the interested human bodies and the undesired background. This \"noisy\" motion representation makes it very challenging for pose estimation and action recognition in real scenarios. To address this issue, this paper presents a novel deep motion representation, called PoseFlow, which reveals human motion in videos while suppressing background and motion blur, and being robust to occlusion. For learning PoseFlow with mild computational cost, we propose a functionally structured spatial-temporal deep network, PoseFlow Net (PFN), to jointly solve the skeleton localization and matching problems of PoseFlow. Comprehensive experiments show that PFN outperforms the state-of-the-art deep flow estimation models in generating PoseFlow. Moreover, PoseFlow demonstrates its potential on improving two challenging tasks in human video analysis: pose estimation and action recognition",
    "checked": true,
    "id": "87e3a8c0429a2d8c4349f9b55fce4ff2c8540deb",
    "semantic_title": "poseflow: a deep motion representation for understanding human behaviors in videos",
    "citation_count": 7,
    "authors": [
      "Dingwen Zhang",
      "Guangyu Guo",
      "Dong Huang",
      "Junwei Han"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lan_FFNet_Video_Fast-Forwarding_CVPR_2018_paper.html": {
    "title": "FFNet: Video Fast-Forwarding via Reinforcement Learning",
    "volume": "main",
    "abstract": "For many intelligent applications with limited computation, communication, storage and energy resources, there is an imperative need of vision methods that could select an informative subset of the input video for efficient processing at or near real time. In the literature, there are two relevant groups of approaches: generating a \"trailer\" for a video or fast-forwarding while watching/processing the video. The first group is supported by video summarization techniques, which require processing of the entire video to select an important subset for showing to users. In the second group, current fast-forwarding methods depend on either manual control or automatic adaptation of playback speed, which often do not present an accurate representation and may still require processing of every frame. In this paper, we introduce FastForwardNet (FFNet), a reinforcement learning agent that gets inspiration from video summarization and does fast-forwarding differently. It is an online framework that automatically fast-forwards a video and presents a representative subset of frames to users on the fly. It does not require processing the entire video but just the portion that is selected by the fast-forward agent, which makes the process very computationally efficient. The online nature of our proposed method also enables the users to begin fast-forwarding at any point of the video. Experiments on two real-world datasets demonstrate that our method can provide better representation of the input video (about 6%-20% improvement on coverage of important frames) with much less processing requirement (more than 80% reduction in the number of frames processed)",
    "checked": true,
    "id": "82662d6680ae0444f5d76e637c414721cc6f0583",
    "semantic_title": "ffnet: video fast-forwarding via reinforcement learning",
    "citation_count": 50,
    "authors": [
      "Shuyue Lan",
      "Rameswar Panda",
      "Qi Zhu",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Multi-Shot_Pedestrian_Re-Identification_CVPR_2018_paper.html": {
    "title": "Multi-Shot Pedestrian Re-Identification via Sequential Decision Making",
    "volume": "main",
    "abstract": "Multi-shot pedestrian re-identification problem is at the core of surveillance video analysis. It matches two tracks of pedestrians from different cameras. In contrary to existing works that aggregate single frames features by time series model such as recurrent neural network, in this paper, we propose an interpretable reinforcement learning based approach to this problem. Particularly, we train an agent to verify a pair of images at each time. The agent could choose to output the result (same or different) or request another pair of images to verify (unsure). By this way, our model implicitly learns the difficulty of image pairs, and postpone the decision when the model does not accumulate enough evidence. Moreover, by adjusting the reward for unsure action, we can easily trade off between speed and accuracy. In three open benchmarks, our method are competitive with the state-of-the-art methods while only using 3% to 6% images. These promising results demonstrate that our method is favorable in both efficiency and performance",
    "checked": true,
    "id": "b8b7f2ff5df6cfb07920fdae5f6649b6c3ec51ee",
    "semantic_title": "multi-shot pedestrian re-identification via sequential decision making",
    "citation_count": 69,
    "authors": [
      "Jianfu Zhang",
      "Naiyan Wang",
      "Liqing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_Attend_and_Interact_CVPR_2018_paper.html": {
    "title": "Attend and Interact: Higher-Order Object Interactions for Video Understanding",
    "volume": "main",
    "abstract": "Human actions often involve complex interactions across several inter-related objects in the scene. However, existing approaches to fine-grained video understanding or visual relationship detection often rely on single object representation or pairwise object relationships. Furthermore, learning interactions across multiple objects in hundreds of frames for video is computationally infeasible and performance may suffer since a large combinatorial space has to be modeled. In this paper, we propose to efficiently learn higher-order interactions between arbitrary subgroups of objects for fine-grained video understanding. We demonstrate that modeling object interactions significantly improves accuracy for both action recognition and video captioning, while saving more than 3-times the computation over traditional pairwise relationships. The proposed method is validated on two large-scale datasets: Kinetics and ActivityNet Captions. Our SINet and SINet-Caption achieve state-of-the-art performances on both datasets even though the videos are sampled at a maximum of 1 FPS. To the best of our knowledge, this is the first work modeling object interactions on open domain large-scale video datasets, and we additionally model higher-order object interactions which improves the performance with low computational costs",
    "checked": true,
    "id": "66aebb3af16aaa78579344784212ae10f60ec27e",
    "semantic_title": "attend and interact: higher-order object interactions for video understanding",
    "citation_count": 145,
    "authors": [
      "Chih-Yao Ma",
      "Asim Kadav",
      "Iain Melvin",
      "Zsolt Kira",
      "Ghassan AlRegib",
      "Hans Peter Graf"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Where_and_Why_CVPR_2018_paper.html": {
    "title": "Where and Why Are They Looking? Jointly Inferring Human Attention and Intentions in Complex Tasks",
    "volume": "main",
    "abstract": "This paper addresses a new problem - jointly inferring human attention, intentions, and tasks from videos. Given an RGB-D video where a human performs a task, we answer three questions simultaneously: 1) where the human is looking - attention prediction; 2) why the human is looking there - intention prediction; and 3) what task the human is performing - task recognition. We propose a hierarchical model of human-attention-object (HAO) which represents tasks, intentions, and attention under a unified framework. A task is represented as sequential intentions which transition to each other. An intention is composed of the human pose, attention, and objects. A beam search algorithm is adopted for inference on the HAO graph to output the attention, intention, and task results. We built a new video dataset of tasks, intentions, and attention. It contains 14 task classes, 70 intention categories, 28 object classes, 809 videos, and approximately 330,000 frames. Experiments show that our approach outperforms existing approaches",
    "checked": true,
    "id": "07098ec7f9b66ddeb21314fd3630a9e45decdd4e",
    "semantic_title": "where and why are they looking? jointly inferring human attention and intentions in complex tasks",
    "citation_count": 45,
    "authors": [
      "Ping Wei",
      "Yang Liu",
      "Tianmin Shu",
      "Nanning Zheng",
      "Song-Chun Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Fully_Convolutional_Adaptation_CVPR_2018_paper.html": {
    "title": "Fully Convolutional Adaptation Networks for Semantic Segmentation",
    "volume": "main",
    "abstract": "The recent advances in deep neural networks have convincingly demonstrated high capability in learning vision models on large datasets. Nevertheless, collecting expert labeled datasets especially with pixel-level annotations is an extremely expensive process. An appealing alternative is to render synthetic data (e.g., computer games) and generate ground truth automatically. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. In this paper, we facilitate this issue from the perspectives of both visual appearance-level and representation-level domain adaptation. The former adapts source-domain images to appear as if drawn from the ``style\" in the target domain and the latter attempts to learn domain-invariant representations. Specifically, we present Fully Convolutional Adaptation Networks (FCAN), a novel deep architecture for semantic segmentation which combines Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN). AAN learns a transformation from one domain to the other in the pixel space and RAN is optimized in an adversarial learning manner to maximally fool the domain discriminator with the learnt source and target representations. Extensive experiments are conducted on the transfer from GTA5 (game videos) to Cityscapes (urban street scenes) on semantic segmentation and our proposal achieves superior results when comparing to state-of-the-art unsupervised adaptation techniques. More remarkably, we obtain a new record: mIoU of 47.5% on BDDS (drive-cam videos) in an unsupervised setting",
    "checked": true,
    "id": "7c30ee6470500f47eefacfa0560a8918e229138c",
    "semantic_title": "fully convolutional adaptation networks for semantic segmentation",
    "citation_count": 351,
    "authors": [
      "Yiheng Zhang",
      "Zhaofan Qiu",
      "Ting Yao",
      "Dong Liu",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Nilsson_Semantic_Video_Segmentation_CVPR_2018_paper.html": {
    "title": "Semantic Video Segmentation by Gated Recurrent Flow Propagation",
    "volume": "main",
    "abstract": "Semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models. In this paper we present a deep, end-to-end trainable methodology for video segmentation that is capable of leveraging the information present in unlabeled data, besides sparsely labeled frames, in order to improve semantic estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that is able to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. The flow, the recognition and the gated temporal propagation modules can be trained jointly, end-to-end. The temporal, gated recurrent flow propagation component of our model can be plugged into any static semantic segmentation architecture and turn it into a weakly supervised video processing one. Our experiments in the challenging CityScapes and Camvid datasets, and for multiple deep architectures, indicate that the resulting model can leverage unlabeled temporal frames, next to a labeled one, in order to improve both the video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost and with little extra computation",
    "checked": true,
    "id": "c0006a2268d299644e9f1b455601bcbe89ddc2b5",
    "semantic_title": "semantic video segmentation by gated recurrent flow propagation",
    "citation_count": 222,
    "authors": [
      "David Nilsson",
      "Cristian Sminchisescu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Interpretable_Video_Captioning_CVPR_2018_paper.html": {
    "title": "Interpretable Video Captioning via Trajectory Structured Localization",
    "volume": "main",
    "abstract": "Automatically describing open-domain videos with natural language are attracting increasing interest in the field of artificial intelligence. Most existing methods simply borrow ideas from image captioning and obtain a compact video representation from an ensemble of global image feature before feeding to an RNN decoder which outputs a sentence of variable length. However, it is not only arduous for the generator to focus on specific salient objects at different time given the global video representation, it is more formidable to capture the fine-grained motion information and the relation between moving instances for more subtle linguistic descriptions. In this paper, we propose a Trajectory Structured Attentional Encoder-Decoder (TSA-ED) neural network framework for more elaborate video captioning which works by integrating local spatial-temporal representation at trajectory level through structured attention mechanism. Our proposed method is based on a LSTM-based encoder-decoder framework, which incorporates an attention modeling scheme to adaptively learn the correlation between sentence structure and the moving objects in videos, and consequently generates more accurate and meticulous statement description in the decoding stage. Experimental results demonstrate that the feature representation and structured attention mechanism based on the trajectory cluster can efficiently obtain the local motion information in the video to help generate a more fine-grained video description, and achieve the state-of-the-art performance on the well-known Charades and MSVD datasets",
    "checked": true,
    "id": "f66a2c5225551837b8894f94ae9feca0e406c9c1",
    "semantic_title": "interpretable video captioning via trajectory structured localization",
    "citation_count": 57,
    "authors": [
      "Xian Wu",
      "Guanbin Li",
      "Qingxing Cao",
      "Qingge Ji",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Deep_Hashing_via_CVPR_2018_paper.html": {
    "title": "Deep Hashing via Discrepancy Minimization",
    "volume": "main",
    "abstract": "This paper presents a discrepancy minimizing model to address the discrete optimization problem in hashing learning. The discrete optimization introduced by binary constraint is an NP-hard mixed integer programming problem. It is usually addressed by relaxing the binary variables into continuous variables to adapt to the gradient based learning of hashing functions, especially the training of deep neural networks. To deal with the objective discrepancy caused by relaxation, we transform the original binary optimization into differentiable optimization problem over hash functions through series expansion. This transformation decouples the binary constraint and the similarity preserving hashing function optimization. The transformed objective is optimized in a tractable alternating optimization framework with gradual discrepancy minimization. Extensive experimental results on three benchmark datasets validate the efficacy of the proposed discrepancy minimizing hashing",
    "checked": true,
    "id": "7abb316e3f1f8c6e6e85e4d416e2b41871aa4ba2",
    "semantic_title": "deep hashing via discrepancy minimization",
    "citation_count": 44,
    "authors": [
      "Zhixiang Chen",
      "Xin Yuan",
      "Jiwen Lu",
      "Qi Tian",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.html": {
    "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
    "volume": "main",
    "abstract": "We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet~cite{howard2017mobilenets} on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves $sim$13$ imes$ actual speedup over AlexNet while maintaining comparable accuracy",
    "checked": true,
    "id": "9da734397acd7ff7c557960c62fb1b400b27bd89",
    "semantic_title": "shufflenet: an extremely efficient convolutional neural network for mobile devices",
    "citation_count": 6865,
    "authors": [
      "Xiangyu Zhang",
      "Xinyu Zhou",
      "Mengxiao Lin",
      "Jian Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Zero-Shot_Recognition_via_CVPR_2018_paper.html": {
    "title": "Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs",
    "volume": "main",
    "abstract": "We consider the problem of zero-shot recognition: learning a visual classifier for a category with zero training examples, just using the word embedding of the category and its relationship to other categories, which visual data are provided. The key to dealing with the unfamiliar or novel category is to transfer knowledge obtained from familiar classes to describe the unfamiliar class. In this paper, we build upon the recently introduced Graph Convolutional Network (GCN) and propose an approach that uses both semantic embeddings and the categorical relationships to predict the classifiers. Given a learned knowledge graph (KG), our approach takes as input semantic embeddings for each node (representing visual category). After a series of graph convolutions, we predict the visual classifier for each category. During training, the visual classifiers for a few categories are given to learn the GCN parameters. At test time, these filters are used to predict the visual classifiers of unseen categories. We show that our approach is robust to noise in the KG. More importantly, our approach provides significant improvement in performance compared to the current state-of-the-art results (from 2 ~ 3% on some metrics to whopping 20% on a few)",
    "checked": true,
    "id": "ff65e3bf34e892ef75d91c5e3d7294e0b64d867d",
    "semantic_title": "zero-shot recognition via semantic embeddings and knowledge graphs",
    "citation_count": 589,
    "authors": [
      "Xiaolong Wang",
      "Yufei Ye",
      "Abhinav Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Krishna_Referring_Relationships_CVPR_2018_paper.html": {
    "title": "Referring Relationships",
    "volume": "main",
    "abstract": "Images are not simply sets of objects: each image represents a web of interconnected relationships. These relationships between entities carry semantic meaning and help a viewer differentiate between instances of an entity. For example, in an image of a soccer match, there may be multiple persons present, but each participates in different relationships: one is kicking the ball, and the other is guarding the goal. In this paper, we formulate the task of utilizing these \"referring relationships\" to disambiguate between entities of the same category. We introduce an iterative model that localizes the two entities in the referring relationship, conditioned on one another. We formulate the cyclic condition between the entities in a relationship by modelling predicates that connect the entities as shifts in attention from one entity to another. We demonstrate that our model can not only outperform existing approaches on three datasets --- CLEVR, VRD and Visual Genome --- but also that it produces visually meaningful predicate shifts, as an instance of interpretable neural networks. Finally, we show that by modelling predicates as attention shifts, we can even localize entities in the absence of their category, allowing our model to find completely unseen categories",
    "checked": true,
    "id": "08a4022e30f93cac2e55cf6b034a09a40550df45",
    "semantic_title": "referring relationships",
    "citation_count": 94,
    "authors": [
      "Ranjay Krishna",
      "Ines Chami",
      "Michael Bernstein",
      "Li Fei-Fei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tychsen-Smith_Improving_Object_Localization_CVPR_2018_paper.html": {
    "title": "Improving Object Localization With Fitness NMS and Bounded IoU Loss",
    "volume": "main",
    "abstract": "We demonstrate that many detection methods are designed to identify only a sufficently accurate bounding box, rather than the best available one. To address this issue we propose a simple and fast modification to the existing methods called Fitness NMS. This method is tested with the DeNet model and obtains a significantly improved MAP at greater localization accuracies without a loss in evaluation rate, and can be used in conjunction with Soft NMS for additional improvements. Next we derive a novel bounding box regression loss based on a set of IoU upper bounds that better matches the goal of IoU maximization while still providing good convergence properties. Following these novelties we investigate RoI clustering schemes for improving evaluation rates for the DeNet wide model variants and provide an analysis of localization performance at various input image dimensions. We obtain a MAP of 33.6%@79Hz and 41.8%@5Hz for MSCOCO and a Titan X (Maxwell)",
    "checked": true,
    "id": "e1c8f1d0e8d57c44bd732b1b3fbb8013e6d60d8a",
    "semantic_title": "improving object localization with fitness nms and bounded iou loss",
    "citation_count": 176,
    "authors": [
      "Lachlan Tychsen-Smith",
      "Lars Petersson"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.html": {
    "title": "End-to-End Deep Kronecker-Product Matching for Person Re-Identification",
    "volume": "main",
    "abstract": "Person re-identification aims to robustly measure similarities between person images. The significant variation of person poses and viewing angles challenges for accurate person re-identification. The spatial layout and correspondences between query person images are vital information for tackling this problem but are ignored by most state-of-the-art methods. In this paper, we propose a novel Kronecker Product Matching module to match feature maps of different persons in an end-to-end trainable deep neural network. A novel feature soft warping scheme is designed for aligning the feature maps based on matching results, which is shown to be crucial for achieving superior accuracy. The multi-scale features based on hourglass-like networks and self residual attention are also exploited to further boost the re-identification performance. The proposed approach outperforms state-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets, which demonstrates the effectiveness and generalization ability of our proposed approach",
    "checked": true,
    "id": "7dbb212555aa099673530b9298384bd562db7b6d",
    "semantic_title": "end-to-end deep kronecker-product matching for person re-identification",
    "citation_count": 117,
    "authors": [
      "Yantao Shen",
      "Tong Xiao",
      "Hongsheng Li",
      "Shuai Yi",
      "Xiaogang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Schonberger_Semantic_Visual_Localization_CVPR_2018_paper.html": {
    "title": "Semantic Visual Localization",
    "volume": "main",
    "abstract": "Robust visual localization under a wide range of viewing conditions is a fundamental problem in computer vision. Handling the difficult cases of this problem is not only very challenging but also of high practical relevance, e.g., in the context of life-long localization for augmented reality or autonomous robots. In this paper, we propose a novel approach based on a joint 3D geometric and semantic understanding of the world, enabling it to succeed under conditions where previous approaches failed. Our method leverages a novel generative model for descriptor learning, trained on semantic scene completion as an auxiliary task. The resulting 3D descriptors are robust to missing observations by encoding high-level 3D geometric and semantic information. Experiments on several challenging large-scale localization datasets demonstrate reliable localization under extreme viewpoint, illumination, and geometry changes",
    "checked": true,
    "id": "ed767a9d01f04a764e9146c0542216cc48624ec6",
    "semantic_title": "semantic visual localization",
    "citation_count": 260,
    "authors": [
      "Johannes L. Schönberger",
      "Marc Pollefeys",
      "Andreas Geiger",
      "Torsten Sattler"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gonzalez-Garcia_Objects_as_Context_CVPR_2018_paper.html": {
    "title": "Objects as Context for Detecting Their Semantic Parts",
    "volume": "main",
    "abstract": "We present a semantic part detection approach that effectively leverages object information. We use the object appearance and its class as indicators of what parts to expect. We also model the expected relative location of parts inside the objects based on their appearance. We achieve this with a new network module, called OffsetNet, that efficiently predicts a variable number of part locations within a given object. Our model incorporates all these cues to detect parts in the context of their objects. This leads to considerably higher performance for the challenging task of part detection compared to using part appearance alone (+5 mAP on the PASCAL-Part dataset). We also compare to other part detection methods on both PASCAL-Part and CUB200-2011 datasets",
    "checked": true,
    "id": "2c21bd988a1a7e330380da4f8be0d23681c5c1b2",
    "semantic_title": "objects as context for detecting their semantic parts",
    "citation_count": 11,
    "authors": [
      "Abel Gonzalez-Garcia",
      "Davide Modolo",
      "Vittorio Ferrari"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Rocco_End-to-End_Weakly-Supervised_Semantic_CVPR_2018_paper.html": {
    "title": "End-to-End Weakly-Supervised Semantic Alignment",
    "volume": "main",
    "abstract": "We tackle the task of semantic alignment where the goal is to compute dense semantic correspondence aligning two images depicting objects of the same category. This is a challenging task due to large intra-class variation, changes in viewpoint and background clutter. We present the following three principal contributions. First, we develop a convolutional neural network architecture for semantic alignment that is trainable in an end-to-end manner from weak image-level supervision in the form of matching image pairs. The outcome is that parameters are learnt from rich appearance variation present in different but semantically related images without the need for tedious manual annotation of correspondences at training time. Second, the main component of this architecture is a differentiable soft inlier scoring module, inspired by the RANSAC inlier scoring procedure, that computes the quality of the alignment based on only geometrically consistent correspondences thereby reducing the effect of background clutter. Third, we demonstrate that the proposed approach achieves state-of-the-art performance on multiple standard benchmarks for semantic alignment",
    "checked": true,
    "id": "5bc2624a73da20133996c9d416080517367f645f",
    "semantic_title": "end-to-end weakly-supervised semantic alignment",
    "citation_count": 179,
    "authors": [
      "Ignacio Rocco",
      "Relja Arandjelović",
      "Josef Sivic"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gao_Dynamic_Zoom-In_Network_CVPR_2018_paper.html": {
    "title": "Dynamic Zoom-In Network for Fast Object Detection in Large Images",
    "volume": "main",
    "abstract": "We introduce a generic framework that reduces the computational cost of object detection while retaining accuracy for scenarios where objects with varied sizes appear in high resolution images. Detection progresses in a coarse-to-fine manner, first on a down-sampled version of the image and then on a sequence of higher resolution regions identified as likely to improve the detection accuracy. Built upon reinforcement learning, our approach consists of a model (R-net) that uses coarse detection results to predict the potential accuracy gain for analyzing a region at a higher resolution and another model (Q-net) that sequentially selects regions to zoom in. Experiments on the Caltech Pedestrians dataset show that our approach reduces the number of processed pixels by over 50% without a drop in detection accuracy. The merits of our approach become more significant on a high resolution test set collected from YFCC100M dataset, where our approach maintains high detection performance while reducing the number of processed pixels by about 70% and the detection time by over 50%",
    "checked": true,
    "id": "14a48ff22f852379eb8a62e18a5c66bc9b60f2c7",
    "semantic_title": "dynamic zoom-in network for fast object detection in large images",
    "citation_count": 129,
    "authors": [
      "Mingfei Gao",
      "Ruichi Yu",
      "Ang Li",
      "Vlad I. Morariu",
      "Larry S. Davis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Learning_Markov_Clustering_CVPR_2018_paper.html": {
    "title": "Learning Markov Clustering Networks for Scene Text Detection",
    "volume": "main",
    "abstract": "A novel framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. MCN predicts instance-level bounding boxes by firstly converting an image into a Stochastic Flow Graph (SFG) and then performing Markov Clustering on this graph. Our method can detect text objects with arbitrary size and orientation without prior knowledge of object size. The stochastic flow graph encode objects' local correlation and semantic information. An object is modeled as strongly connected nodes, which allows flexible bottom-up detection for scale-varying and rotated objects. MCN generates bounding boxes without using Non-Maximum Suppression, and it can be fully parallelized on GPUs. The evaluation on public benchmarks shows that our method outperforms the existing methods by a large margin in detecting multioriented text objects. MCN achieves new state-of-art performance on challenging MSRA-TD500 dataset with precision of 0.88, recall of 0.79 and F-score of 0.83. Also, MCN achieves realtime inference with frame rate of 34 FPS, which is $1.5 imes$ speedup when compared with the fastest scene text detection algorithm",
    "checked": true,
    "id": "31d2f35c6817fdcb1d622bf4ecc957b921db05ea",
    "semantic_title": "learning markov clustering networks for scene text detection",
    "citation_count": 106,
    "authors": [
      "Zichuan Liu",
      "Guosheng Lin",
      "Sheng Yang",
      "Jiashi Feng",
      "Weisi Lin",
      "Wang Ling Goh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pirinen_Deep_Reinforcement_Learning_CVPR_2018_paper.html": {
    "title": "Deep Reinforcement Learning of Region Proposal Networks for Object Detection",
    "volume": "main",
    "abstract": "We propose drl-RPN, a deep reinforcement learning-based visual recognition model consisting of a sequential region proposal network (RPN) and an object detector. In contrast to typical RPNs, where candidate object regions (RoIs) are selected greedily via class-agnostic NMS, drl-RPN optimizes an objective closer to the final detection task. This is achieved by replacing the greedy RoI selection process with a sequential attention mechanism which is trained via deep reinforcement learning (RL). Our model is capable of accumulating class-specific evidence over time, potentially affecting subsequent proposals and classification scores, and we show that such context integration significantly boosts detection accuracy. Moreover, drl-RPN automatically decides when to stop the search process and has the benefit of being able to jointly learn the parameters of the policy and the detector, both represented as deep networks. Our model can further learn to search over a wide range of exploration-accuracy trade-offs making it possible to specify or adapt the exploration extent at test time. The resulting search trajectories are image- and category-dependent, yet rely only on a single policy over all object categories. Results on the MS COCO and PASCAL VOC challenges show that our approach outperforms established, typical state-of-the-art object detection pipelines",
    "checked": true,
    "id": "193ad8b6906441c86f4a237d733d13e19e54c47d",
    "semantic_title": "deep reinforcement learning of region proposal networks for object detection",
    "citation_count": 68,
    "authors": [
      "Aleksis Pirinen",
      "Cristian Sminchisescu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lu_Beyond_Holistic_Object_CVPR_2018_paper.html": {
    "title": "Beyond Holistic Object Recognition: Enriching Image Understanding With Part States",
    "volume": "main",
    "abstract": "Important high-level vision tasks require rich semantic descriptions of objects at part level. Based upon previous work on part localization, in this paper, we address the problem of inferring rich semantics imparted by an object part in still images. Specifically, we propose to tokenize the semantic space as a discrete set of part states. Our modeling of part state is spatially localized, therefore, we formulate the part state inference problem as a pixel-wise annotation problem. An iterative part-state inference neural network that is efficient in time and accurate in performance is specifically designed for this task. Extensive experiments demonstrate that the proposed method can effectively predict the semantic states of parts and simultaneously improve part segmentation, thus benefiting a number of visual understanding applications. The other contribution of this paper is our part state dataset which contains rich part-level semantic annotations",
    "checked": true,
    "id": "454b3f99a0bc310e8aaf6055fdcf4830eea8d05a",
    "semantic_title": "beyond holistic object recognition: enriching image understanding with part states",
    "citation_count": 33,
    "authors": [
      "Cewu Lu",
      "Hao Su",
      "Yonglu Li",
      "Yongyi Lu",
      "Li Yi",
      "Chi-Keung Tang",
      "Leonidas J. Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_Discriminability_Objective_for_CVPR_2018_paper.html": {
    "title": "Discriminability Objective for Training Descriptive Captions",
    "volume": "main",
    "abstract": "One property that remains lacking in image captions generated by contemporary methods is discriminability: being able to tell two images apart given the caption for one of them. We propose a way to improve this aspect of caption generation. By incorporating into the captioning training objective a loss component directly related to ability (by a machine) to disambiguate image/caption matches, we obtain systems that produce much more discriminative caption, according to human evaluation. Remarkably, our approach leads to improvement in other aspects of generated captions, reflected by a battery of standard scores such as BLEU, SPICE etc. Our approach is modular and can be applied to a variety of model/loss combinations commonly proposed for image captioning",
    "checked": true,
    "id": "7c1802d8d43dfe783650a03f03d41609fa5ae91e",
    "semantic_title": "discriminability objective for training descriptive captions",
    "citation_count": 203,
    "authors": [
      "Ruotian Luo",
      "Brian Price",
      "Scott Cohen",
      "Gregory Shakhnarovich"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_Visual_Question_Answering_CVPR_2018_paper.html": {
    "title": "Visual Question Answering With Memory-Augmented Networks",
    "volume": "main",
    "abstract": "In this paper, we exploit memory-augmented neural networks to predict accurate answers to visual questions, even when those answers rarely occur in the training set. The memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar. We show that memory-augmented neural networks are able to maintain a relatively long-term memory of scarce training exemplars, which is important for visual question answering due to the heavy-tailed distribution of answers in a general VQA setting. Experimental results in two large-scale benchmark datasets show the favorable performance of the proposed algorithm with the comparison to state of the art",
    "checked": true,
    "id": "3bb4f2013d99eaf2afc182fa482bd0f2d63f2d82",
    "semantic_title": "visual question answering with memory-augmented networks",
    "citation_count": 100,
    "authors": [
      "Chao Ma",
      "Chunhua Shen",
      "Anthony Dick",
      "Qi Wu",
      "Peng Wang",
      "Anton van den Hengel",
      "Ian Reid"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Structure_Inference_Net_CVPR_2018_paper.html": {
    "title": "Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships",
    "volume": "main",
    "abstract": "Context is important for accurate visual recognition. In this work we propose an object detection algorithm that not only considers object visual appearance, but also makes use of two kinds of context including scene contextual information and object relationships within a single image. Therefore, object detection is regarded as both a cognition problem and a reasoning problem when leveraging these structured information. Specifically, this paper formulates object detection as a problem of graph structure inference, where given an image the objects are treated as nodes in a graph and relationships between the objects are modeled as edges in such graph. To this end, we present a so-called Structure Inference Network (SIN), a detector that incorporates into a typical detection framework (e.g. Faster R-CNN) with a graphical model which aims to infer object state. Comprehensive experiments on PASCAL VOC and MS COCO datasets indicate that scene context and object relationships truly improve the performance of object detection with more desirable and reasonable outputs",
    "checked": true,
    "id": "8633ebfe1bd7f6a95233025c872e4a2cf660f54c",
    "semantic_title": "structure inference net: object detection using scene-level context and instance-level relationships",
    "citation_count": 209,
    "authors": [
      "Yong Liu",
      "Ruiping Wang",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.html": {
    "title": "Occluded Pedestrian Detection Through Guided Attention in CNNs",
    "volume": "main",
    "abstract": "Pedestrian detection has progressed significantly in the last years. However, occluded people are notoriously hard to detect, as their appearance varies substantially depending on a wide range of partial occlusions. In this paper, we aim to propose a simple and compact method based on the FasterRCNN architecture for occluded pedestrian detection. We start with interpreting CNN channel features of a pedestrian detector, and we find that different channels activate responses for different body parts respectively. These findings strongly motivate us to employ an attention mechanism across channels to represent various occlusion patterns in one single model, as each occlusion pattern can be formulated as some specific combination of body parts. Therefore, an attention network with self or external guidances is proposed as an add-on to the baseline FasterRCNN detector. When evaluating on the heavy occlusion subset, we achieve a significant improvement of 8pp to the baseline FasterRCNN detector on CityPersons and on Caltech we outperform the state-of-the-art method by 4pp",
    "checked": true,
    "id": "63f97279a463a3bd18cc3c284d7a8ab28a5aaa1d",
    "semantic_title": "occluded pedestrian detection through guided attention in cnns",
    "citation_count": 340,
    "authors": [
      "Shanshan Zhang",
      "Jian Yang",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tung_Reward_Learning_From_CVPR_2018_paper.html": {
    "title": "Reward Learning From Narrated Demonstrations",
    "volume": "main",
    "abstract": "Humans effortlessly \"program\" one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved [5], by providing RGB images of goal configurations [19], or supplying a demonstration to be imitated [17]. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and instructable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations (NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers' descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation. We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and-place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language",
    "checked": true,
    "id": "d2558c26915e834b60b06be7da1d9db5d0897343",
    "semantic_title": "reward learning from narrated demonstrations",
    "citation_count": 29,
    "authors": [
      "Hsiao-Yu Tung",
      "Adam W. Harley",
      "Liang-Kang Huang",
      "Katerina Fragkiadaki"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.html": {
    "title": "Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing",
    "volume": "main",
    "abstract": "This paper studies the problem of learning image semantic segmentation networks only using image-level labels as supervision, which is important since it can significantly reduce human annotation efforts. Recent state-of-the-art methods on this problem first infer the sparse and discriminative regions for each object class using a deep classification network, then train semantic a segmentation network using the discriminative regions as supervision. Inspired by the traditional image segmentation methods of seeded region growing, we propose to train a semantic segmentation network starting from the discriminative regions and progressively increase the pixel-level supervision using by seeded region growing. The seeded region growing module is integrated in a deep segmentation network and can benefit from deep features. Different from conventional deep networks which have fixed/static labels, the proposed weakly-supervised network generates new labels using the contextual information within an image. The proposed method significantly outperforms the weakly-supervised semantic segmentation methods using static labels, and obtains the state-of-the-art performance, which are 63.2% mIoU score on the PASCAL VOC 2012 test set and 26.0% mIoU score on the COCO dataset",
    "checked": true,
    "id": "b64511774315f4d1a2aa5472051db1b9248410a4",
    "semantic_title": "weakly-supervised semantic segmentation network with deep seeded region growing",
    "citation_count": 542,
    "authors": [
      "Zilong Huang",
      "Xinggang Wang",
      "Jiasi Wang",
      "Wenyu Liu",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html": {
    "title": "PoTion: Pose MoTion Representation for Action Recognition",
    "volume": "main",
    "abstract": "Most state-of-the-art methods for action recognition rely on a two-stream architecture that processes appearance and motion independently. In this paper, we claim that considering them jointly offers rich information for action recognition. We introduce a novel representation that gracefully encodes the movement of some semantic keypoints. We use the human joints as these keypoints and term our Pose moTion representation PoTion. Specifically, we first run a state-of-the-art human pose estimator and extract heatmaps for the human joints in each frame. We obtain our PoTion representation by temporally aggregating these probability maps. This is achieved by colorizing each of them depending on the relative time of the frames in the video clip and summing them. This fixed-size representation for an entire video clip is suitable to classify actions using a shallow convolutional neural network. Our experimental evaluation shows that PoTion outperforms other state-of-the-art pose representations. Furthermore, it is complementary to standard appearance and motion streams. When combining PoTion with the recent two-stream I3D approach [5], we obtain state-of-the-art performance on the JHMDB, HMDB and UCF101 datasets",
    "checked": true,
    "id": "6009bba115904bc3bf876224db90b232c4f0a48f",
    "semantic_title": "potion: pose motion representation for action recognition",
    "citation_count": 274,
    "authors": [
      "Vasileios Choutas",
      "Philippe Weinzaepfel",
      "Jérôme Revaud",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Bilateral_Ordinal_Relevance_CVPR_2018_paper.html": {
    "title": "Bilateral Ordinal Relevance Multi-Instance Regression for Facial Action Unit Intensity Estimation",
    "volume": "main",
    "abstract": "Automatic intensity estimation of facial action units (AUs) is challenging in two aspects. First, capturing subtle changes of facial appearance is quiet difficult. Second, the annotation of AU intensity is scarce and expensive. Intensity annotation requires strong domain knowledge thus only experts are qualified. The majority of methods directly apply supervised learning techniques to AU intensity estimation while few methods exploit unlabeled samples to improve the performance. In this paper, we propose a novel weakly supervised regression model-Bilateral Ordinal Relevance Multi-instance Regression (BORMIR), which learns a frame-level intensity estimator with weakly labeled sequences. From a new perspective, we introduce relevance to model sequential data and consider two bag labels for each bag. The AU intensity estimation is formulated as a joint regressor and relevance learning problem. Temporal dynamics of both relevance and AU intensity are leveraged to build connections among labeled and unlabeled image frames to provide weak supervision. We also develop an efficient algorithm for optimization based on the alternating minimization framework. Evaluations on three expression databases demonstrate the effectiveness of the proposed model",
    "checked": true,
    "id": "038e196b560b21f79b629a5a1c5cb89de27ca66b",
    "semantic_title": "bilateral ordinal relevance multi-instance regression for facial action unit intensity estimation",
    "citation_count": 44,
    "authors": [
      "Yong Zhang",
      "Rui Zhao",
      "Weiming Dong",
      "Bao-Gang Hu",
      "Qiang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Pulling_Actions_out_CVPR_2018_paper.html": {
    "title": "Pulling Actions out of Context: Explicit Separation for Effective Combination",
    "volume": "main",
    "abstract": "The ability to recognize human actions in video has many potential applications. Human action recognition, however, is tremendously challenging for computers due to the complexity of video data and the subtlety of human actions. Most current recognition systems flounder on the inability to separate human actions from co-occurring factors that usually dominate subtle human actions. In this paper, we propose a novel approach for training a human action recognizer, one that can: (1) explicitly factorize human actions from the co-occurring factors; (2) deliberately build a model for human actions and a separate model for all correlated contextual elements; and (3) effectively combine the models for human action recognition. Our approach exploits the benefits of conjugate samples of human actions, which are video clips that are contextually similar to human action samples, but do not contain the action. Experiments on ActionThread, PASCAL VOC, UCF101, and Hollywood2 datasets demonstrate the ability to separate action from context of the proposed approach",
    "checked": true,
    "id": "926dd1c4db38356ac5bf87c1a4c6b93625f9a850",
    "semantic_title": "pulling actions out of context: explicit separation for effective combination",
    "citation_count": 27,
    "authors": [
      "Yang Wang",
      "Minh Hoai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/He_Dynamic_Feature_Learning_CVPR_2018_paper.html": {
    "title": "Dynamic Feature Learning for Partial Face Recognition",
    "volume": "main",
    "abstract": "Partial face recognition (PFR) in unconstrained environment is a very important task, especially in video surveillance, mobile devices, etc. However, a few studies have tackled how to recognize an arbitrary patch of a face image. This study combines Fully Convolutional Network (FCN) with Sparse Representation Classification (SRC) to propose a novel partial face recognition approach, called Dynamic Feature Matching (DFM), to address partial face images regardless of sizes. Based on DFM, we propose a sliding loss to optimize FCN by reducing the intra-variation between a face patch and face images of a subject, which further improves the performance of DFM. The proposed DFM is evaluated on several partial face databases, including LFW, YTF and CASIA-NIR-Distance databases. Experimental results demonstrate the effectiveness and advantages of DFM in comparison with state-of-the-art PFR methods",
    "checked": true,
    "id": "6fefdfc7845e4c20212e42f476d2a514cbf6a529",
    "semantic_title": "dynamic feature learning for partial face recognition",
    "citation_count": 58,
    "authors": [
      "Lingxiao He",
      "Haiqing Li",
      "Qi Zhang",
      "Zhenan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.html": {
    "title": "Exploiting Transitivity for Learning Person Re-Identification Models on a Budget",
    "volume": "main",
    "abstract": "Minimization of labeling effort for person re-identification in camera networks is an important problem as most of the existing popular methods are supervised and they require large amount of manual annotations, acquiring which is a tedious job. In this work, we focus on this labeling effort minimization problem and approach it as a subset selection task where the objective is to select an optimal subset of image-pairs for labeling without compromising performance. Towards this goal, our proposed scheme first represents any camera network (with k number of cameras) as an edge weighted complete k-partite graph where each vertex denotes a person and similarity scores between persons are used as edge-weights. Then in the second stage, our algorithm selects an optimal subset of pairs by solving a triangle free subgraph maximization problem on the k-partite graph. This sub-graph weight maximization problem is NP-hard (at least for k > = 4) which means for large datasets the optimization problem becomes intractable. In order to make our framework scalable, we propose two polynomial time approximately-optimal algorithms. The first algorithm is a 1/2-approximation algorithm which runs in linear time in the number of edges. The second algorithm is a greedy algorithm with sub-quadratic (in number of edges) time-complexity. Experiments on three state-of-the-art datasets depict that the proposed approach requires on an average only 8-15 % manually labeled pairs in order to achieve the performance when all the pairs are manually annotated",
    "checked": true,
    "id": "d97a0343abdb6ff2cc1492c4c952654011736709",
    "semantic_title": "exploiting transitivity for learning person re-identification models on a budget",
    "citation_count": 21,
    "authors": [
      "Sourya Roy",
      "Sujoy Paul",
      "Neal E. Young",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/He_Deep_Spatial_Feature_CVPR_2018_paper.html": {
    "title": "Deep Spatial Feature Reconstruction for Partial Person Re-Identification: Alignment-Free Approach",
    "volume": "main",
    "abstract": "Partial person re-identification (re-id) is a challenging problem, where only a partial observation of a person image is available for matching. However, few studies have offered a solution of how to identify an arbitrary patch of a person image. In this paper, we propose a fast and accurate matching method to address this problem. The proposed method leverages Fully Convolutional Network (FCN) to generate correspondingly-size spatial feature maps such that pixel-level features are consistent. To match a pair of person images of different sizes, a novel method called Deep Spatial feature Reconstruction (DSR) is further developed to avoid explicit alignment. Specifically, we exploit the reconstructing error from dictionary learning to calculate the similarity between different spatial feature maps. In that way, we expect that the proposed FCN can decrease the similarity of coupled images from different persons and vice versa. Experimental results on two partial person datasets demonstrate the efficiency and effectiveness of the proposed method in comparison with several state-of-the-art partial person re-id approaches",
    "checked": true,
    "id": "dae580d5915cca24a321e4f8dc7209428f25720e",
    "semantic_title": "deep spatial feature reconstruction for partial person re-identification: alignment-free approach",
    "citation_count": 291,
    "authors": [
      "Lingxiao He",
      "Jian Liang",
      "Haiqing Li",
      "Zhenan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Every_Smile_Is_CVPR_2018_paper.html": {
    "title": "Every Smile Is Unique: Landmark-Guided Diverse Smile Generation",
    "volume": "main",
    "abstract": "Each smile is unique: one person surely smiles in different ways (e.g., closing/opening the eyes or mouth). Given one input image of a neutral face, can we generate multiple smile videos with distinctive characteristics? To tackle this one-to-many video generation problem, we propose a novel deep learning architecture named Conditional Multi-Mode Network (CMM-Net). To better encode the dynamics of facial expressions, CMM-Net explicitly exploits facial landmarks for generating smile sequences. Specifically, a variational auto-encoder is used to learn a facial landmark embedding. This single embedding is then exploited by a conditional recurrent network which generates a landmark embedding sequence conditioned on a specific expression (e.g., spontaneous smile). Next, the generated landmark embeddings are fed into a multi-mode recurrent landmark generator, producing a set of landmark sequences still associated to the given smile class but clearly distinct from each other. Finally, these landmark sequences are translated into face videos. Our experimental results demonstrate the effectiveness of our CMM-Net in generating realistic videos of multiple smile expressions",
    "checked": true,
    "id": "cb13cbbaa7647177746ab86983273317dd5bcf51",
    "semantic_title": "every smile is unique: landmark-guided diverse smile generation",
    "citation_count": 69,
    "authors": [
      "Wei Wang",
      "Xavier Alameda-Pineda",
      "Dan Xu",
      "Pascal Fua",
      "Elisa Ricci",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_UV-GAN_Adversarial_Facial_CVPR_2018_paper.html": {
    "title": "UV-GAN: Adversarial Facial UV Map Completion for Pose-Invariant Face Recognition",
    "volume": "main",
    "abstract": "Recently proposed robust 3D face alignment methods establish either dense or sparse correspondence between a 3D face model and a 2D facial image. The use of these methods presents new challenges as well as opportunities for facial texture analysis. In particular, by sampling the image using the fitted model, a facial UV can be created. Unfortunately, due to self-occlusion, such a UV map is always incomplete. In this paper, we propose a framework for training Deep Convolutional Neural Network (DCNN) to complete the facial UV map extracted from in-the-wild images. To this end, we first gather complete UV maps by fitting a 3D Morphable Model (3DMM) to various multiview image and video datasets, as well as leveraging on a new 3D dataset with over 3,000 identities. Second, we devise a meticulously designed architecture that combines local and global adversarial DCNNs to learn an identity-preserving facial UV completion model. We demonstrate that by attaching the completed UV to the fitted mesh and generating instances of arbitrary poses, we can increase pose variations for training deep face recognition/verification models, and minimise pose discrepancy during testing, which lead to better performance. Experiments on both controlled and in-the-wild UV datasets prove the effectiveness of our adversarial UV completion model. We achieve state-of-the-art verification accuracy, 94.05%, under the CFP frontal-profile protocol only by combining pose augmentation during training and pose discrepancy reduction during testing. We will release the first in-the-wild UV dataset (we refer as WildUV) that comprises of complete facial UV maps from 1,892 identities for research purposes",
    "checked": true,
    "id": "1929863fff917ee7f6dc428fc1ce732777668eca",
    "semantic_title": "uv-gan: adversarial facial uv map completion for pose-invariant face recognition",
    "citation_count": 181,
    "authors": [
      "Jiankang Deng",
      "Shiyang Cheng",
      "Niannan Xue",
      "Yuxiang Zhou",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Cascaded_Pyramid_Network_CVPR_2018_paper.html": {
    "title": "Cascaded Pyramid Network for Multi-Person Pose Estimation",
    "volume": "main",
    "abstract": "The topic of multi-person pose estimation has beenlargely improved recently, especially with the developmentof convolutional neural network. However, there still exista lot of challenging cases, such as occluded keypoints, in-visible keypoints and complex background, which cannot bewell addressed. In this paper, we present a novel networkstructure called Cascaded Pyramid Network (CPN) whichtargets to relieve the problem from these \"hard\" keypoints.More specifically, our algorithm includes two stages: Glob-alNet and RefineNet. GlobalNet is a feature pyramid net-work which can successfully localize the \"simple\" key-points like eyes and hands but may fail to precisely rec-ognize the occluded or invisible keypoints. Our RefineNettries explicitly handling the \"hard\" keypoints by integrat-ing all levels of feature representations from the Global-Net together with an online hard keypoint mining loss. Ingeneral, to address the multi-person pose estimation prob-lem, a top-down pipeline is adopted to first generate a setof human bounding boxes based on a detector, followed byour CPN for keypoint localization in each human boundingbox. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with averageprecision at 73.0 on the COCO test-dev dataset and 72.1 onthe COCO test-challenge dataset, which is a 19% relativeimprovement compared with 60.5 from the COCO 2016 key-point challenge. Code and the detection results for personused will be publicly available for further research",
    "checked": true,
    "id": "1deb7f96fc92d5c9e04d2cbb277473fee878e144",
    "semantic_title": "cascaded pyramid network for multi-person pose estimation",
    "citation_count": 1174,
    "authors": [
      "Yilun Chen",
      "Zhicheng Wang",
      "Yuxiang Peng",
      "Zhiqiang Zhang",
      "Gang Yu",
      "Jian Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chu_A_Face-to-Face_Neural_CVPR_2018_paper.html": {
    "title": "A Face-to-Face Neural Conversation Model",
    "volume": "main",
    "abstract": "Neural networks have recently become good at engaging in dialog. However, current approaches are based solely on verbal text, lacking the richness of a real face-to-face conversation. We propose a neural conversation model that aims to read and generate facial gestures alongside with text. This allows our model to adapt its response based on the \"mood\" of the conversation. In particular, we introduce an RNN encoder-decoder that exploits the movement of facial muscles, as well as the verbal conversation. The decoder consists of two layers, where the lower layer aims at generating the verbal response and coarse facial expressions, while the second layer fills in the subtle gestures, making the generated output more smooth and natural. We train our neural network by having it \"watch\" 250 movies. We showcase our joint face-text model in generating more natural conversations through automatic metrics and a human study. We demonstrate an example application with a face-to-face chatting avatar",
    "checked": true,
    "id": "b09a35b0ee3f1b51fa809ca6a1f70528d910b366",
    "semantic_title": "a face-to-face neural conversation model",
    "citation_count": 22,
    "authors": [
      "Hang Chu",
      "Daiqing Li",
      "Sanja Fidler"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper.html": {
    "title": "End-to-End Recovery of Human Shape and Pose",
    "volume": "main",
    "abstract": "We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation",
    "checked": true,
    "id": "e73696016b43314a7bef6015dacbe702af472d96",
    "semantic_title": "end-to-end recovery of human shape and pose",
    "citation_count": 1795,
    "authors": [
      "Angjoo Kanazawa",
      "Michael J. Black",
      "David W. Jacobs",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html": {
    "title": "Squeeze-and-Excitation Networks",
    "volume": "main",
    "abstract": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ∼25% relative improvement over the winning entry of 2016. Code and models are available at https: //github.com/hujie-frank/SENet",
    "checked": true,
    "id": "df67d46e78aae0d2fccfb6212d101a342259c01b",
    "semantic_title": "squeeze-and-excitation networks",
    "citation_count": 613,
    "authors": [
      "Jie Hu",
      "Li Shen",
      "Gang Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Islam_Revisiting_Salient_Object_CVPR_2018_paper.html": {
    "title": "Revisiting Salient Object Detection: Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects",
    "volume": "main",
    "abstract": "Salient object detection is a problem that has been considered in detail and many solutions proposed. In this paper, we argue that work to date has addressed a problem that is relatively ill-posed. Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried. This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects. The solution presented in this paper solves this more general problem that considers relative rank, and we propose data and metrics suitable to measuring success in a relative object saliency landscape. A novel deep learning solution is proposed based on a hierarchical representation of relative saliency and stage-wise refinement. We also show that the problem of salient object subitizing can be addressed with the same network, and our approach exceeds performance of any prior work across all metrics considered (both traditional and newly proposed)",
    "checked": true,
    "id": "da134a15d0e1806af876eb3e0d8e3bd71c6ad63a",
    "semantic_title": "revisiting salient object detection: simultaneous detection, ranking, and subitizing of multiple salient objects",
    "citation_count": 110,
    "authors": [
      "Md Amirul Islam",
      "Mahmoud Kalash",
      "Neil D. B. Bruce"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Context_Encoding_for_CVPR_2018_paper.html": {
    "title": "Context Encoding for Semantic Segmentation",
    "volume": "main",
    "abstract": "Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available",
    "checked": true,
    "id": "e746c8eec81384bd37dede9700be9c8a3700f936",
    "semantic_title": "context encoding for semantic segmentation",
    "citation_count": 1251,
    "authors": [
      "Hang Zhang",
      "Kristin Dana",
      "Jianping Shi",
      "Zhongyue Zhang",
      "Xiaogang Wang",
      "Ambrish Tyagi",
      "Amit Agrawal"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hsiao_Creating_Capsule_Wardrobes_CVPR_2018_paper.html": {
    "title": "Creating Capsule Wardrobes From Fashion Images",
    "volume": "main",
    "abstract": "We propose to automatically create emph{capsule wardrobes}. Given an inventory of candidate garments and accessories, the algorithm must assemble a minimal set of items that provides maximal mix-and-match outfits. We pose the task as a subset selection problem. To permit efficient subset selection over the space of all outfit combinations, we develop submodular objective functions capturing the key ingredients of visual compatibility, versatility, and user-specific preference. Since adding garments to a capsule only expands its possible outfits, we devise an iterative approach to allow near-optimal submodular function maximization. Finally, we present an unsupervised approach to learn visual compatibility from ``in the wild\" full body outfit photos; the compatibility metric translates well to cleaner catalog photos and improves over existing methods. Our results on thousands of pieces from popular fashion websites show that automatic capsule creation has potential to mimic skilled fashionistas in assembling flexible wardrobes, while being significantly more scalable",
    "checked": true,
    "id": "47b1498f9f5eba0c0488784ee177ad29cbae7177",
    "semantic_title": "creating capsule wardrobes from fashion images",
    "citation_count": 138,
    "authors": [
      "Wei-Lin Hsiao",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Niu_Webly_Supervised_Learning_CVPR_2018_paper.html": {
    "title": "Webly Supervised Learning Meets Zero-Shot Learning: A Hybrid Approach for Fine-Grained Classification",
    "volume": "main",
    "abstract": "Fine-grained image classification, which targets at distinguishing subtle distinctions among various subordinate categories, remains a very difficult task due to the high annotation cost of enormous fine-grained categories. To cope with the scarcity of well-labeled training images, existing works mainly follow two research directions: 1) utilize freely available web images without human annotation; 2) only annotate some fine-grained categories and transfer the knowledge to other fine-grained categories, which falls into the scope of zero-shot learning (ZSL). However, the above two directions have their own drawbacks. For the first direction, the labels of web images are very noisy and the data distribution between web images and test images are considerably different. For the second direction, the performance gap between ZSL and traditional supervised learning is still very large. The drawbacks of the above two directions motivate us to design a new framework which can jointly leverage both web data and auxiliary labeled categories to predict the test categories that are not associated with any well-labeled training images. Comprehensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed framework",
    "checked": true,
    "id": "d3da25e95bb149002f29ce0abff319f7c08c7849",
    "semantic_title": "webly supervised learning meets zero-shot learning: a hybrid approach for fine-grained classification",
    "citation_count": 67,
    "authors": [
      "Li Niu",
      "Ashok Veeraraghavan",
      "Ashutosh Sabharwal"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gu_Look_Imagine_and_CVPR_2018_paper.html": {
    "title": "Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval With Generative Models",
    "volume": "main",
    "abstract": "Textual-visual cross-modal retrieval has been a hot research topic in both computer vision and natural language processing communities. Learning appropriate representations for multi-modal data is crucial for the cross-modal retrieval performance. Unlike existing image-text retrieval approaches that embed image-text pairs as single feature vectors in a common representational space, we propose to incorporate generative processes into the cross-modal feature embedding, through which we are able to learn not only the global abstract features but also the local grounded features. Extensive experiments show that our framework can well match images and sentences with complex content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO dataset",
    "checked": true,
    "id": "724b253a55e86ad230ba05c7eb78f249e09258d9",
    "semantic_title": "look, imagine and match: improving textual-visual cross-modal retrieval with generative models",
    "citation_count": 360,
    "authors": [
      "Jiuxiang Gu",
      "Jianfei Cai",
      "Shafiq R. Joty",
      "Li Niu",
      "Gang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Bidirectional_Attentive_Fusion_CVPR_2018_paper.html": {
    "title": "Bidirectional Attentive Fusion With Context Gating for Dense Video Captioning",
    "volume": "main",
    "abstract": "Dense video captioning is a newly emerging task that aims at both localizing and describing all events in a video. We identify and tackle two challenges on this task, namely, (1) how to utilize both past and future contexts for accurate event proposal predictions, and (2) how to construct informative input to the decoder for generating natural event descriptions. First, previous works predominantly generate temporal event proposals in the forward direction, which neglects future video contexts. We propose a bidirectional proposal method that effectively exploits both past and future contexts to make proposal predictions. Second, different events ending at (nearly) the same time are indistinguishable in the previous works, resulting in the same captions. We solve this problem by representing each event with an attentive fusion of hidden states from the proposal module and video contents (e.g., C3D features). We further propose a novel context gating mechanism to balance the contributions from the current event and its surrounding contexts dynamically. We empirically show that our attentively fused event representation is superior to the proposal hidden states or video contents alone. By coupling proposal and captioning modules into one unified framework, our model outperforms the state-of-the-arts on the ActivityNet Captions dataset with a relative gain of over 100% (Meteor score increases from 4.82 to 9.65)",
    "checked": true,
    "id": "bb4e2d6a6e3e1067f21a4cad087fc91c671e495d",
    "semantic_title": "bidirectional attentive fusion with context gating for dense video captioning",
    "citation_count": 205,
    "authors": [
      "Jingwen Wang",
      "Wenhao Jiang",
      "Lin Ma",
      "Wei Liu",
      "Yong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Taira_InLoc_Indoor_Visual_CVPR_2018_paper.html": {
    "title": "InLoc: Indoor Visual Localization With Dense Matching and View Synthesis",
    "volume": "main",
    "abstract": "We seek to predict the 6 degree-of-freedom (6DoF) pose of a query photograph with respect to a large indoor 3D map. The contributions of this work are three-fold. First, we develop a new large-scale visual localization method targeted for indoor environments. The method proceeds along three steps: (i) efficient retrieval of candidate poses that ensures scalability to large-scale environments, (ii) pose estimation using dense matching rather than local features to deal with textureless indoor scenes, and (iii) pose verification by virtual view synthesis to cope with significant changes in viewpoint, scene layout, and occluders. Second, we collect a new dataset with reference 6DoF poses for large-scale indoor localization. Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario. Third, we demonstrate that our method significantly outperforms current state-of-the-art indoor localization approaches on this new challenging data",
    "checked": true,
    "id": "92c24b63a2d8383d69d95b1e7b21aaab9de10b9e",
    "semantic_title": "inloc: indoor visual localization with dense matching and view synthesis",
    "citation_count": 35,
    "authors": [
      "Hajime Taira",
      "Masatoshi Okutomi",
      "Torsten Sattler",
      "Mircea Cimpoi",
      "Marc Pollefeys",
      "Josef Sivic",
      "Tomas Pajdla",
      "Akihiko Torii"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Towards_High_Performance_CVPR_2018_paper.html": {
    "title": "Towards High Performance Video Object Detection",
    "volume": "main",
    "abstract": "There has been significant progresses for image object detection recently. Nevertheless, video object detection has received little attention, although it is more challenging and more important in practical scenarios. Built upon the recent works, this work proposes a unified viewpoint based on the principle of multi-frame end-to-end learning of features and cross-frame motion. Our approach extends prior works with three new techniques and steadily pushes forward the performance envelope (speed-accuracy tradeoff), towards high performance video object detection",
    "checked": true,
    "id": "7be8c7687a957837ba8ce4945afa76cb68dbe495",
    "semantic_title": "towards high performance video object detection",
    "citation_count": 243,
    "authors": [
      "Xizhou Zhu",
      "Jifeng Dai",
      "Lu Yuan",
      "Yichen Wei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lu_Neural_Baby_Talk_CVPR_2018_paper.html": {
    "title": "Neural Baby Talk",
    "volume": "main",
    "abstract": "We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence `template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different. Code has been made available at: https://github.com/jiasenlu/NeuralBabyTalk",
    "checked": true,
    "id": "3bf09b2e2639add154a9fe6ff98cc373d3e90e4e",
    "semantic_title": "neural baby talk",
    "citation_count": 435,
    "authors": [
      "Jiasen Lu",
      "Jianwei Yang",
      "Dhruv Batra",
      "Devi Parikh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qiao_Few-Shot_Image_Recognition_CVPR_2018_paper.html": {
    "title": "Few-Shot Image Recognition by Predicting Parameters From Activations",
    "volume": "main",
    "abstract": "In this paper, we are interested in the few-shot learning problem. In particular, we focus on a challenging scenario where the number of categories is large and the number of examples per novel category is very limited, e.g. 1, 2, or 3. Motivated by the close relationship between the parameters and the activations in a neural network associated with the same category, we propose a novel method that can adapt a pre-trained neural network to novel categories by directly predicting the parameters from the activations. Zero training is required in adaptation to novel categories, and fast inference is realized by a single forward pass. We evaluate our method by doing few-shot image recognition on the ImageNet dataset, which achieves the state-of-the-art classification accuracy on novel categories by a significant margin while keeping comparable performance on the large-scale categories. We also test our method on the MiniImageNet dataset and it strongly outperforms the previous state-of-the-art methods",
    "checked": true,
    "id": "3e08a3912ebe494242f6bcd772929cc65307129c",
    "semantic_title": "few-shot image recognition by predicting parameters from activations",
    "citation_count": 518,
    "authors": [
      "Siyuan Qiao",
      "Chenxi Liu",
      "Wei Shen",
      "Alan L. Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Iterative_Visual_Reasoning_CVPR_2018_paper.html": {
    "title": "Iterative Visual Reasoning Beyond Convolutions",
    "volume": "main",
    "abstract": "We present a novel framework for iterative visual reasoning. Our framework goes beyond current recognition systems that lack the capability to reason beyond stack of convolutions. The framework consists of two core modules: a local module that uses spatial memory to store previous beliefs in parallel; and a global graph-reasoning module. Our graph has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships between them; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to class nodes. Both the local module and the global module roll-out iteratively and cross-feed predictions to each other to refine estimates. The final predictions are made by combining the best of both modules with an attention mechanism. We show strong performance over plain ConvNets, eg achieving an $8.4%$ absolute improvement on ADE measured by per-class average precision. Analysis also shows that the framework is resilient to missing regions for reasoning",
    "checked": true,
    "id": "57c36f13b188816051a27478e2f56bb284f4fb13",
    "semantic_title": "iterative visual reasoning beyond convolutions",
    "citation_count": 216,
    "authors": [
      "Xinlei Chen",
      "Li-Jia Li",
      "Li Fei-Fei",
      "Abhinav Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Visual_Question_Reasoning_CVPR_2018_paper.html": {
    "title": "Visual Question Reasoning on General Dependency Tree",
    "volume": "main",
    "abstract": "The collaborative reasoning for understanding each image-question pair is very critical but under-explored for an interpretable Visual Question Answering (VQA) system. Although very recent works also tried the explicit compositional processes to assemble multiple sub-tasks embedded in the questions, their models heavily rely on the annotations or hand-crafted rules to obtain valid reasoning layout, leading to either heavy labor or poor performance on composition reasoning. In this paper, to enable global context reasoning for better aligning image and language domains in diverse and unrestricted cases, we propose a novel reasoning network called Adversarial Composition Modular Network (ACMN). This network comprises of two collaborative modules: i) an adversarial attention module to exploit the local visual evidence for each word parsed from the question; ii) a residual composition module to compose the previously mined evidence. Given a dependency parse tree for each question, the adversarial attention module progressively discovers salient regions of one word by densely combining regions of child word nodes in an adversarial manner. Then residual composition module merges the hidden representations of an arbitrary number of children through sum pooling and residual connection. Our ACMN is thus capable of building an interpretable VQA system that gradually dives the image cues following a question-driven reasoning route and makes global reasoning by incorporating the learned knowledge of all attention modules in a principled manner. Experiments on relational datasets demonstrate the superiority of our ACMN and visualization results show the explainable capability of our reasoning system",
    "checked": true,
    "id": "eff328e0ecfb9a7a2d6664ee38aa32a61c7b9f42",
    "semantic_title": "visual question reasoning on general dependency tree",
    "citation_count": 37,
    "authors": [
      "Qingxing Cao",
      "Xiaodan Liang",
      "Bailing Li",
      "Guanbin Li",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_CVM-Net_Cross-View_Matching_CVPR_2018_paper.html": {
    "title": "CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization",
    "volume": "main",
    "abstract": "The problem of localization on a geo-referenced aerial/satellite map given a query ground view image remains challenging due to the drastic change in viewpoint that causes traditional image descriptors based matching to fail. We leverage on the recent success of deep learning to propose the CVM-Net for the cross-view image-based ground-to-aerial geo-localization task. Specifically, our network is based on the Siamese architecture to do metric learning for the matching task. We first use the fully convolutional layers to extract local image features, which are then encoded into global image descriptors using the powerful NetVLAD. As part of the training procedure, we also introduce a simple yet effective weighted soft margin ranking loss function that not only speeds up the training convergence but also improves the final matching accuracy. Experimental results show that our proposed network significantly outperforms the state-of-the-art approaches on two existing benchmarking datasets",
    "checked": true,
    "id": "f7585d0c4eb361c49de21a84d3e7c66b12991a1f",
    "semantic_title": "cvm-net: cross-view matching network for image-based ground-to-aerial geo-localization",
    "citation_count": 194,
    "authors": [
      "Sixing Hu",
      "Mengdan Feng",
      "Rang M. H. Nguyen",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Revisiting_Dilated_Convolution_CVPR_2018_paper.html": {
    "title": "Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Despite remarkable progress, weakly supervised segmentation methods are still inferior to their fully supervised counterparts. We obverse that the performance gap mainly comes from the inability of producing dense and integral pixel-level object localization for training images only with image-level labels. In this work, we revisit the dilated convolution proposed in [1] and shed light on how it enables the classification network to generate dense object localization. By substantially enlarging the receptive fields of convolutional kernels with different dilation rates, the classification network can localize the object regions even when they are not so discriminative for classification and finally produce reliable object regions for benefiting both weakly- and semi- supervised semantic segmentation. Despite the apparent simplicity of dilated convolution, we are able to obtain superior performance for semantic segmentation tasks. In particular, it achieves 60.8% and 67.6% mean Intersection-over-Union (mIoU) on Pascal VOC 2012 test set in weakly- (only image-level labels are available) and semi- (1,464 segmentation masks are available) settings, which are the new state-of-the-arts",
    "checked": true,
    "id": "f875489e265efcab0e0bb958248f3d49ae299c7f",
    "semantic_title": "revisiting dilated convolution: a simple approach for weakly- and semi-supervised semantic segmentation",
    "citation_count": 492,
    "authors": [
      "Yunchao Wei",
      "Huaxin Xiao",
      "Honghui Shi",
      "Zequn Jie",
      "Jiashi Feng",
      "Thomas S. Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Low-Shot_Learning_From_CVPR_2018_paper.html": {
    "title": "Low-Shot Learning From Imaginary Data",
    "volume": "main",
    "abstract": "Humans can quickly learn new visual concepts, perhaps because they can easily visualize or imagine what novel objects look like from different views. Incorporating this ability to hallucinate novel instances of new concepts might help machine vision systems perform better low-shot learning, i.e., learning concepts from few examples. We present a novel approach to low-shot learning that uses this idea. Our approach builds on recent progress in meta-learning (''learning to learn'') by combining a meta-learner with a ''hallucinator'' that produces additional training examples, and optimizing both models jointly. Our hallucinator can be incorporated into a variety of meta-learners and provides significant gains: up to a 6 point boost in classification accuracy when only a single training example is available, yielding state-of-the-art performance on the challenging ImageNet low-shot classification benchmark",
    "checked": true,
    "id": "cf42c89fe45e31cdd83be78c22c28508e335b9aa",
    "semantic_title": "low-shot learning from imaginary data",
    "citation_count": 625,
    "authors": [
      "Yu-Xiong Wang",
      "Ross Girshick",
      "Martial Hebert",
      "Bharath Hariharan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_DoubleFusion_Real-Time_Capture_CVPR_2018_paper.html": {
    "title": "DoubleFusion: Real-Time Capture of Human Performances With Inner Body Shapes From a Single Depth Sensor",
    "volume": "main",
    "abstract": "We propose DoubleFusion, a new real-time system that combines volumetric dynamic reconstruction with data-driven template fitting to simultaneously reconstruct detailed geometry, non-rigid motion and the inner human body shape from a single depth camera. One of the key contributions of this method is a double layer representation consisting of a complete parametric body shape inside and a gradually fused outer surface layer. A pre-defined node graph on the body surface parameterizes the non-rigid deformations near the body and a free-form dynamically changing graph parameterizes the outer surface layer far from the body allowing more general reconstruction. We further propose a joint motion tracking method based on the double layer representation to enable robust and fast motion tracking performance. Moreover, the inner body shape is optimized online and forced to fit inside the outer surface layer. Overall, our method enables increasingly denoised, detailed and complete surface reconstructions, fast motion tracking performance and plausible inner body shape reconstruction in real-time. In particular, experiments show improved fast motion tracking and loop closure performance on more challenging scenarios",
    "checked": true,
    "id": "fd24b1b5f8b77c6373e654ce9ec8ffc680336291",
    "semantic_title": "doublefusion: real-time capture of human performances with inner body shapes from a single depth sensor",
    "citation_count": 247,
    "authors": [
      "Tao Yu",
      "Zerong Zheng",
      "Kaiwen Guo",
      "Jianhui Zhao",
      "Qionghai Dai",
      "Hao Li",
      "Gerard Pons-Moll",
      "Yebin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Guler_DensePose_Dense_Human_CVPR_2018_paper.html": {
    "title": "DensePose: Dense Human Pose Estimation in the Wild",
    "volume": "main",
    "abstract": "In this work we establish dense correspondences between an RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation. We gather dense correspondences for 50K persons appearing in the COCO dataset by introducing an efficient annotation pipeline. We then use our dataset to train CNN-based systems that deliver dense correspondence \"in the wild\", namely in the presence of background, occlusions and scale variations. We improve our training set's effectiveness by training an inpainting network that can fill in missing ground truth values and report improvements with respect to the best results that would be achievable in the past. We experiment with fully-convolutional networks and region-based models and observe a superiority of the latter. We further improve accuracy through cascading, obtaining a system that delivers highly-accurate results at multiple frames per second on a single gpu. Supplementary materials, data, code, and videos are provided on the project page http://densepose.org",
    "checked": true,
    "id": "8c94385d45f5896e748e43171eeaaa259009faab",
    "semantic_title": "densepose: dense human pose estimation in the wild",
    "citation_count": 1148,
    "authors": [
      "Rıza Alp Güler",
      "Natalia Neverova",
      "Iasonas Kokkinos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.html": {
    "title": "Ordinal Depth Supervision for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose",
    "checked": true,
    "id": "75512a0c23866cc2f0bcf349b770f0fb2ce17e57",
    "semantic_title": "ordinal depth supervision for 3d human pose estimation",
    "citation_count": 311,
    "authors": [
      "Georgios Pavlakos",
      "Xiaowei Zhou",
      "Kostas Daniilidis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Speciale_Consensus_Maximization_for_CVPR_2018_paper.html": {
    "title": "Consensus Maximization for Semantic Region Correspondences",
    "volume": "main",
    "abstract": "We propose a novel method for the geometric registration of semantically labeled regions. We approximate semantic regions by ellipsoids, and leverage their convexity to formulate the correspondence search effectively as a constrained optimization problem that maximizes the number of matched regions, and which we solve globally optimal in a branch-and-bound fashion. To this end, we derive suitable linear matrix inequality constraints which describe ellipsoid-to-ellipsoid assignment conditions. Our approach is robust to large percentages of outliers and thus applicable to difficult correspondence search problems. In multiple experiments we demonstrate the flexibility and robustness of our approach on a number of challenging vision problems",
    "checked": true,
    "id": "45cc6db8414209773d1c5cfa8a20d53675af3209",
    "semantic_title": "consensus maximization for semantic region correspondences",
    "citation_count": 7,
    "authors": [
      "Pablo Speciale",
      "Danda P. Paudel",
      "Martin R. Oswald",
      "Hayko Riemenschneider",
      "Luc Van Gool",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Vianello_Robust_Hough_Transform_CVPR_2018_paper.html": {
    "title": "Robust Hough Transform Based 3D Reconstruction From Circular Light Fields",
    "volume": "main",
    "abstract": "Light-field imaging is based on images taken on a regular grid. Thus, high-quality 3D reconstructions are obtainable by analyzing orientations in epipolar plane images (EPIs). Unfortunately, such data only allows to evaluate one side of the object. Moreover, a constant intensity along each orientation is mandatory for most of the approaches. This paper presents a novel method which allows to reconstruct depth information from data acquired with a circular camera motion, termed circular light fields. With this approach it is possible to determine the full 360 degree view of target objects. Additionally, circular light fields allow retrieving depth from datasets acquired with telecentric lenses, which is not possible with linear light fields. The proposed method finds trajectories of 3D points in the EPIs by means of a modified Hough transform. For this purpose, binary EPI-edge images are used, which not only allow to obtain reliable depth information, but also overcome the limitation of constant intensity along trajectories. Experimental results on synthetic and real datasets demonstrate the quality of the proposed algorithm",
    "checked": true,
    "id": "485165677e49e00a2ce75aa0e253435a2528834d",
    "semantic_title": "robust hough transform based 3d reconstruction from circular light fields",
    "citation_count": 11,
    "authors": [
      "Alessandro Vianello",
      "Jens Ackermann",
      "Maximilian Diebold",
      "Bernd Jähne"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Alive_Caricature_From_CVPR_2018_paper.html": {
    "title": "Alive Caricature From 2D to 3D",
    "volume": "main",
    "abstract": "Caricature is an art form that expresses subjects in abstract, simple and exaggerated views. While many caricatures are 2D images, this paper presents an algorithm for creating expressive 3D caricatures from 2D caricature images with minimum user interaction. The key idea of our approach is to introduce an intrinsic deformation representation that has the capability of extrapolation, enabling us to create a deformation space from standard face datasets, which maintains face constraints and meanwhile is sufficiently large for producing exaggerated face models. Built upon the proposed deformation representation, an optimization model is formulated to find the 3D caricature that captures the style of the 2D caricature image automatically. The experiments show that our approach has better capability in expressing caricatures than those fitting approaches directly using classical parametric face models such as 3DMM and FaceWareHouse. Moreover, our approach is based on standard face datasets and avoids constructing complicated 3D caricature training sets, which provides great flexibility in real applications",
    "checked": true,
    "id": "008f01a0b815001ddbe00be84319a83478bf56aa",
    "semantic_title": "alive caricature from 2d to 3d",
    "citation_count": 38,
    "authors": [
      "Qianyi Wu",
      "Juyong Zhang",
      "Yu-Kun Lai",
      "Jianmin Zheng",
      "Jianfei Cai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_Nonlinear_3D_Face_CVPR_2018_paper.html": {
    "title": "Nonlinear 3D Face Morphable Model",
    "volume": "main",
    "abstract": "As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction",
    "checked": true,
    "id": "09ec60f2eea5d43792b2bc9da63b1c9b7719f666",
    "semantic_title": "nonlinear 3d face morphable model",
    "citation_count": 343,
    "authors": [
      "Luan Tran",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Through-Wall_Human_Pose_CVPR_2018_paper.html": {
    "title": "Through-Wall Human Pose Estimation Using Radio Signals",
    "volume": "main",
    "abstract": "This paper demonstrates accurate human pose estimation through walls and occlusions. We leverage the fact that wireless signals in the WiFi frequencies traverse walls and reflect off the human body. We introduce a deep neural network approach that parses such radio signals to estimate 2D poses. Since humans cannot annotate radio signals, we use state-of-the-art vision model to provide cross-modal supervision. Specifically, during training the system uses synchronized wireless and visual inputs, extracts pose information from the visual stream, and uses it to guide the training process. Once trained, the network uses only the wireless signal for pose estimation. We show that, when tested on visible scenes, the radio-based system is almost as accurate as the vision-based system used to train it. Yet, unlike vision-based pose estimation, the radio-based system can estimate 2D poses through walls despite never trained on such scenarios. Demo videos are available at our website (http://rfpose.csail.mit.edu)",
    "checked": true,
    "id": "e42b2981f4e8de54213d624d1ef12bad4fe02f0a",
    "semantic_title": "through-wall human pose estimation using radio signals",
    "citation_count": 462,
    "authors": [
      "Mingmin Zhao",
      "Tianhong Li",
      "Mohammad Abu Alsheikh",
      "Yonglong Tian",
      "Hang Zhao",
      "Antonio Torralba",
      "Dina Katabi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_What_Makes_a_CVPR_2018_paper.html": {
    "title": "What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets",
    "volume": "main",
    "abstract": "The ability to capture temporal information has been critical to the development of video understanding models. While there have been numerous attempts at modeling motion in videos, an explicit analysis of the effect of temporal information for video understanding is still missing. In this work, we aim to bridge this gap and ask the following question: How important is the motion in the video for recognizing the action? To this end, we propose two novel frameworks: (i) class-agnostic temporal generator and (ii) motion-invariant frame selector to reduce/remove motion for an ablation analysis without introducing other artifacts. This isolates the analysis of motion from other aspects of the video. The proposed frameworks provide a much tighter estimate of the effect of motion (from 25% to 6% on UCF101 and 15% to 5% on Kinetics) compared to baselines in our analysis. Our analysis provides critical insights about existing models like C3D, and how it could be made to achieve comparable results with a sparser set of frames",
    "checked": true,
    "id": "c3e94bffaa786b099a37d58e64e8dc870c7526b9",
    "semantic_title": "what makes a video a video: analyzing temporal information in video understanding models and datasets",
    "citation_count": 121,
    "authors": [
      "De-An Huang",
      "Vignesh Ramanathan",
      "Dhruv Mahajan",
      "Lorenzo Torresani",
      "Manohar Paluri",
      "Li Fei-Fei",
      "Juan Carlos Niebles"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Oh_Fast_Video_Object_CVPR_2018_paper.html": {
    "title": "Fast Video Object Segmentation by Reference-Guided Mask Propagation",
    "volume": "main",
    "abstract": "We present an efficient method for the semi-supervised video object segmentation. Our method achieves accuracy competitive with state-of-the-art methods while running in a fraction of time compared to others. To this end, we propose a deep Siamese encoder-decoder network that is designed to take advantage of mask propagation and object detection while avoiding the weaknesses of both approaches. Our network, learned through a two-stage training process that exploits both synthetic and real data, works robustly without any online learning or post-processing. We validate our method on four benchmark sets that cover single and multiple object segmentation. On all the benchmark sets, our method shows comparable accuracy while having the order of magnitude faster runtime. We also provide extensive ablation and add-on studies to analyze and evaluate our framework",
    "checked": true,
    "id": "e8e7eb0ef502d5a456b2d573eb290791e7657b76",
    "semantic_title": "fast video object segmentation by reference-guided mask propagation",
    "citation_count": 340,
    "authors": [
      "Seoung Wug Oh",
      "Joon-Young Lee",
      "Kalyan Sunkavalli",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Richard_NeuralNetwork-Viterbi_A_Framework_CVPR_2018_paper.html": {
    "title": "NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning",
    "volume": "main",
    "abstract": "Video learning is an important task in computer vision and has experienced increasing interest over the recent years. Since even a small amount of videos easily comprises several million frames, methods that do not rely on a frame-level annotation are of special importance. In this work, we propose a novel learning algorithm with a Viterbi-based loss that allows for online and incremental learning of weakly annotated video data. We moreover show that explicit context and length modeling leads to huge improvements in video segmentation and labeling tasks and include these models into our framework. On several action segmentation benchmarks, we obtain an improvement of up to 10% compared to current state-of-the-art methods",
    "checked": true,
    "id": "80184c6a88fc97a09393b7336bc2ddb12e9b1030",
    "semantic_title": "neuralnetwork-viterbi: a framework for weakly supervised video learning",
    "citation_count": 118,
    "authors": [
      "Alexander Richard",
      "Hilde Kuehne",
      "Ahsan Iqbal",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sigurdsson_Actor_and_Observer_CVPR_2018_paper.html": {
    "title": "Actor and Observer: Joint Modeling of First and Third-Person Videos",
    "volume": "main",
    "abstract": "Several theories in cognitive neuroscience suggest that when people interact with the world, or simulate interactions, they do so from a first-person egocentric perspective, and seamlessly transfer knowledge between third-person (observer) and first-person (actor). Despite this, learning such models for human action recognition has not been achievable due to the lack of data. This paper takes a step in this direction, with the introduction of Charades-Ego, a large-scale dataset of paired first-person and third-person videos, involving 112 people, with 4000 paired videos. This enables learning the link between the two, actor and observer perspectives. Thereby, we address one of the biggest bottlenecks facing egocentric vision research, providing a link from first-person to the abundant third-person data on the web. We use this data to learn a joint representation of first and third-person videos, with only weak supervision, and show its effectiveness for transferring knowledge from the third-person to the first-person domain",
    "checked": true,
    "id": "e5bce81f9f0a4b962ae39205be9985bc5128fce9",
    "semantic_title": "actor and observer: joint modeling of first and third-person videos",
    "citation_count": 122,
    "authors": [
      "Gunnar A. Sigurdsson",
      "Abhinav Gupta",
      "Cordelia Schmid",
      "Ali Farhadi",
      "Karteek Alahari"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_HSA-RNN_Hierarchical_Structure-Adaptive_CVPR_2018_paper.html": {
    "title": "HSA-RNN: Hierarchical Structure-Adaptive RNN for Video Summarization",
    "volume": "main",
    "abstract": "Although video summarization has achieved great success in recent years, few approaches have realized the influence of video structure on the summarization results. As we know, the video data follow a hierarchical structure, i.e., a video is composed of shots, and a shot is composed of several frames. Generally, shots provide the activity-level information for people to understand the video content. While few existing summarization approaches pay attention to the shot segmentation procedure. They generate shots by some trivial strategies, such as fixed length segmentation, which may destroy the underlying hierarchical structure of video data and further reduce the quality of generated summaries. To address this problem, we propose a structure-adaptive video summarization approach that integrates shot segmentation and video summarization into a Hierarchical Structure-Adaptive RNN, denoted as HSA-RNN. We evaluate the proposed approach on four popular datasets, i.e., SumMe, TVsum, CoSum and VTW. The experimental results have demonstrated the effectiveness of HSA-RNN in the video summarization task",
    "checked": true,
    "id": "1ea6c0f1bde1f800e3c9c23573325e0d5283b12c",
    "semantic_title": "hsa-rnn: hierarchical structure-adaptive rnn for video summarization",
    "citation_count": 163,
    "authors": [
      "Bin Zhao",
      "Xuelong Li",
      "Xiaoqiang Lu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Fast_and_Accurate_CVPR_2018_paper.html": {
    "title": "Fast and Accurate Online Video Object Segmentation via Tracking Parts",
    "volume": "main",
    "abstract": "Online video object segmentation is a challenging task as it entails to process the image sequence timely and accurately. To segment a target object through the video, numerous CNN-based methods have been developed by heavily finetuning on the object mask in the first frame, which is time-consuming for online applications. In this paper, we propose a fast and accurate video object segmentation algorithm that can immediately start the segmentation process once receiving the images. We first utilize a part-based tracking method to deal with challenging factors such as large deformation, occlusion, and cluttered background. Based on the tracked bounding boxes of parts, we construct a region-of-interest segmentation network to generate part masks. Finally, a similarity-based scoring function is adopted to refine these object parts by comparing them to the visual information in the first frame. Our method performs favorably against state-of-the-art algorithms in accuracy on the DAVIS benchmark dataset, while achieving much faster runtime performance",
    "checked": true,
    "id": "12fae9a2c1ed867997e1ca70eba271b3c741c42f",
    "semantic_title": "fast and accurate online video object segmentation via tracking parts",
    "citation_count": 222,
    "authors": [
      "Jingchun Cheng",
      "Yi-Hsuan Tsai",
      "Wei-Chih Hung",
      "Shengjin Wang",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Now_You_Shake_CVPR_2018_paper.html": {
    "title": "Now You Shake Me: Towards Automatic 4D Cinema",
    "volume": "main",
    "abstract": "We are interested in enabling automatic 4D cinema by parsing physical and special effects from untrimmed movies. These include effects such as physical interactions, water splashing, light, and shaking, and are grounded to either a character in the scene or the camera. We collect a new dataset referred to as the Movie4D dataset which annotates over 9K effects in 63 movies. We propose a Conditional Random Field model atop a neural network that brings together visual and audio information, as well as semantics in the form of person tracks. Our model further exploits correlations of effects between different characters in the clip as well as across movie threads. We propose effect detection and classification as two tasks, and present results along with ablation studies on our dataset, paving the way towards 4D cinema in everyone's homes",
    "checked": true,
    "id": "ebb1a828444cf3009cb4c4918d9350ceb5f3d547",
    "semantic_title": "now you shake me: towards automatic 4d cinema",
    "citation_count": 17,
    "authors": [
      "Yuhao Zhou",
      "Makarand Tapaswi",
      "Sanja Fidler"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kanehira_Viewpoint-Aware_Video_Summarization_CVPR_2018_paper.html": {
    "title": "Viewpoint-Aware Video Summarization",
    "volume": "main",
    "abstract": "This paper introduces a novel variant of video summarization, namely building a summary that depends on the particular aspect of a video the viewer focuses on. We refer to this as viewpoint. To infer what the desired viewpoint may be, we assume that several other videos are available, especially groups of videos, e.g., as folders on a person's phone or laptop. The semantic similarity between videos in a group vs. the dissimilarity between groups is used to produce viewpoint-specific summaries. For considering similarity as well as avoiding redundancy, output summary should be (A) diverse, (B) representative of videos in the same group, and (C) discriminative against videos in the different groups. To satisfy these requirements (A)-(C) simultaneously, we proposed a novel video summarization method from multiple groups of videos. Inspired by Fisher's discriminant criteria, it selects summary by optimizing the combination of three terms (a) inner-summary, (b) inner-group, and (c) between-group variances defined on the feature representation of summary, which can simply represent (A)-(C). Moreover, we developed a novel dataset to investigate how well the generated summary reflects the underlying viewpoint. Quantitative and qualitative experiments conducted on the dataset demonstrate the effectiveness of proposed method",
    "checked": true,
    "id": "067abbecd4b98c2156d277c19bcdee0a6642cc71",
    "semantic_title": "viewpoint-aware video summarization",
    "citation_count": 35,
    "authors": [
      "Atsushi Kanehira",
      "Luc Van Gool",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fujimura_Photometric_Stereo_in_CVPR_2018_paper.html": {
    "title": "Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter",
    "volume": "main",
    "abstract": "Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media",
    "checked": true,
    "id": "da203ab6a84da9f31bc60b6ad52f4cf6ef887587",
    "semantic_title": "photometric stereo in participating media considering shape-dependent forward scatter",
    "citation_count": 15,
    "authors": [
      "Yuki Fujimura",
      "Masaaki Iiyama",
      "Atsushi Hashimoto",
      "Michihiko Minoh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Direction-Aware_Spatial_Context_CVPR_2018_paper.html": {
    "title": "Direction-Aware Spatial Context Features for Shadow Detection",
    "volume": "main",
    "abstract": "Shadow detection is a fundamental and challenging task, since it requires an understanding of global image semantics and there are various backgrounds around shadows. This paper presents a novel network for shadow detection by analyzing image context in a direction-aware manner. To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN. By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting shadows. This design is developed into the DSC module and embedded in a CNN to learn DSC features at different levels. Moreover, a weighted cross entropy loss is designed to make the training more effective. We employ two common shadow detection benchmark datasets and perform various experiments to evaluate our network. Experimental results show that our network outperforms state-of-the-art methods and achieves 97% accuracy and 38% reduction on balance error rate",
    "checked": true,
    "id": "a1153e3ebed473a518feeb9cece6a5bf22206fe4",
    "semantic_title": "direction-aware spatial context features for shadow detection",
    "citation_count": 156,
    "authors": [
      "Xiaowei Hu",
      "Lei Zhu",
      "Chi-Wing Fu",
      "Jing Qin",
      "Pheng-Ann Heng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Discriminative_Learning_of_CVPR_2018_paper.html": {
    "title": "Discriminative Learning of Latent Features for Zero-Shot Recognition",
    "volume": "main",
    "abstract": "Zero-shot learning (ZSL) aims to recognize unseen image categories by learning an embedding space between image and semantic representations. For years, among existing works, it has been the center task to learn the proper mapping matrices aligning the visual and semantic space, whilst the importance to learn discriminative representations for ZSL is ignored. In this work, we retrospect existing methods and demonstrate the necessity to learn discriminative representations for both visual and semantic instances of ZSL. We propose an end-to-end network that is capable of 1) automatically discovering discriminative regions by a zoom network; and 2) learning discriminative semantic representations in an augmented space introduced for both user-defined and latent attributes. Our proposed method is tested extensively on two challenging ZSL datasets, and the experiment results show that the proposed method significantly outperforms state-of-the-art methods",
    "checked": true,
    "id": "85ffbcf7ab725d4cf8e2e78456ddd68733165027",
    "semantic_title": "discriminative learning of latent features for zero-shot recognition",
    "citation_count": 72,
    "authors": [
      "Yan Li",
      "Junge Zhang",
      "Jianguo Zhang",
      "Kaiqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tsai_Learning_to_Adapt_CVPR_2018_paper.html": {
    "title": "Learning to Adapt Structured Output Space for Semantic Segmentation",
    "volume": "main",
    "abstract": "Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. To further improve our method, we utilize multi-level output adaptation based on feature maps at different levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality",
    "checked": true,
    "id": "193b518bc3025804c6d587c74cbc154d91478417",
    "semantic_title": "learning to adapt structured output space for semantic segmentation",
    "citation_count": 1301,
    "authors": [
      "Yi-Hsuan Tsai",
      "Wei-Chih Hung",
      "Samuel Schulter",
      "Kihyuk Sohn",
      "Ming-Hsuan Yang",
      "Manmohan Chandraker"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.html": {
    "title": "Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics",
    "volume": "main",
    "abstract": "Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task",
    "checked": true,
    "id": "f98788f32b0d33d200c9bc7d900d0ef39519c927",
    "semantic_title": "multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
    "citation_count": 2446,
    "authors": [
      "Alex Kendall",
      "Yarin Gal",
      "Roberto Cipolla"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Jointly_Localizing_and_CVPR_2018_paper.html": {
    "title": "Jointly Localizing and Describing Events for Dense Video Captioning",
    "volume": "main",
    "abstract": "Automatically describing a video with natural language is regarded as a fundamental challenge in computer vision. The problem nevertheless is not trivial especially when a video contains multiple events to be worthy of mention, which often happens in real videos. A valid question is how to temporally localize and then describe events, which is known as ``dense video captioning.\" In this paper, we present a novel framework for dense video captioning that unifies the localization of temporal event proposals and sentence generation of each proposal, by jointly training them in an end-to-end manner. To combine these two worlds, we integrate a new design, namely descriptiveness regression, into a single shot detection structure to infer the descriptive complexity of each detected proposal via sentence generation. This in turn adjusts the temporal locations of each event proposal. Our model differs from existing dense video captioning methods since we propose a joint and global optimization of detection and captioning, and the framework uniquely capitalizes on an attribute-augmented video captioning architecture. Extensive experiments are conducted on ActivityNet Captions dataset and our framework shows clear improvements when compared to the state-of-the-art techniques. More remarkably, we obtain a new record: METEOR of 12.96% on ActivityNet Captions official test set",
    "checked": true,
    "id": "19d7f83c3d7147f0eed1e1471438066eb4fe51fb",
    "semantic_title": "jointly localizing and describing events for dense video captioning",
    "citation_count": 148,
    "authors": [
      "Yehao Li",
      "Ting Yao",
      "Yingwei Pan",
      "Hongyang Chao",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gorji_Going_From_Image_CVPR_2018_paper.html": {
    "title": "Going From Image to Video Saliency: Augmenting Image Salience With Dynamic Attentional Push",
    "volume": "main",
    "abstract": "We present a novel method to incorporate the recent advent in static saliency models to predict the saliency in videos. Our model augments the static saliency models with the Attentional Push effect of the photographer and the scene actors in a shared attention setting. We demonstrate that not only it is imperative to use static Attentional Push cues, noticeable performance improvement is achievable by learning the time-varying nature of Attentional Push. We propose a multi-stream Convolutional Long Short-Term Memory network (ConvLSTM) structure which augments state-of-the-art in static saliency models with dynamic Attentional Push. Our network contains four pathways, a saliency pathway and three Attentional Push pathways. The multi-pathway structure is followed by an augmenting convnet that learns to combine the complementary and time-varying outputs of the ConvLSTMs by minimizing the relative entropy between the augmented saliency and viewers fixation patterns on videos. We evaluate our model by comparing the performance of several augmented static saliency models with state-of-the-art in spatiotemporal saliency on three largest dynamic eye tracking datasets, HOLLYWOOD2, UCF-Sport and DIEM. Experimental results illustrates that solid performance gain is achievable using the proposed methodology",
    "checked": true,
    "id": "9a2fae46c67189fb2aea33f12091772e635361f1",
    "semantic_title": "going from image to video saliency: augmenting image salience with dynamic attentional push",
    "citation_count": 54,
    "authors": [
      "Siavash Gorji",
      "James J. Clark"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_M3_Multimodal_Memory_CVPR_2018_paper.html": {
    "title": "M3: Multimodal Memory Modelling for Video Captioning",
    "volume": "main",
    "abstract": "Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, video captioning has made great progress. However, learning an effective mapping from the visual sequence space to the language space is still a challenging problem due to the long-term multimodal dependency modelling and semantic misalignment. Inspired by the facts that memory modelling poses potential advantages to long-term sequential problems [35] and working memory is the key factor of visual attention [33], we propose a Multimodal Memory Model (M3) to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide visual attention on described visual targets to solve visual-textual alignments. Specifically, similar to [10], the proposed M3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. To evaluate the proposed model, we perform experiments on two public datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms most of the state-of-the-art methods in terms of BLEU and METEOR",
    "checked": true,
    "id": "b910a6f687a4e56062dc326786cee297bd60e8c1",
    "semantic_title": "m3: multimodal memory modelling for video captioning",
    "citation_count": 135,
    "authors": [
      "Junbo Wang",
      "Wei Wang",
      "Yan Huang",
      "Liang Wang",
      "Tieniu Tan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_Emotional_Attention_A_CVPR_2018_paper.html": {
    "title": "Emotional Attention: A Study of Image Sentiment and Visual Attention",
    "volume": "main",
    "abstract": "Image sentiment influences visual perception. Emotion-eliciting stimuli such as happy faces and poisonous snakes are generally prioritized in human attention. However, little research has evaluated the interrelationships of image sentiment and visual saliency. In this paper, we present the first study to focus on the relation between emotional properties of an image and visual attention. We first create the EMOtional attention dataset (EMOd). It is a diverse set of emotion-eliciting images, and each image has (1) eye-tracking data collected from 16 subjects, (2) intensive image context labels including object contour, object sentiment, object semantic category, and high-level perceptual attributes such as image aesthetics and elicited emotions. We perform extensive analyses on EMOd to identify how image sentiment relates to human attention. We discover an emotion prioritization effect: for our images, emotion-eliciting content attracts human attention strongly, but such advantage diminishes dramatically after initial fixation. Aiming to model the human emotion prioritization computationally, we design a deep neural network for saliency prediction, which includes a novel subnetwork that learns the spatial and semantic context of the image scene. The proposed network outperforms the state-of-the-art on three benchmark datasets, by effectively capturing the relative importance of human attention within an image. The code, models, and dataset are available online at https://nus-sesame.top/emotionalattention/",
    "checked": true,
    "id": "c46d02e64fbd70080c2f6ab6cc96c9c36f3107f1",
    "semantic_title": "emotional attention: a study of image sentiment and visual attention",
    "citation_count": 123,
    "authors": [
      "Shaojing Fan",
      "Zhiqi Shen",
      "Ming Jiang",
      "Bryan L. Koenig",
      "Juan Xu",
      "Mohan S. Kankanhalli",
      "Qi Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Andreopoulos_A_Low_Power_CVPR_2018_paper.html": {
    "title": "A Low Power, High Throughput, Fully Event-Based Stereo System",
    "volume": "main",
    "abstract": "We introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-Neumann computation model, where no frames, arrays, or any other such data-structures are used. This is the first time that an end-to-end stereo pipeline from image acquisition and rectification, multi-scale spatio-temporal stereo correspondence, winner-take-all, to disparity regularization is implemented fully on event-based hardware. Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects. System evaluation on event-based sequences demonstrates a ~200X improvement in terms of power per pixel per disparity map compared to the closest state-of-the-art, and maximum latencies of up to 11ms from spike injection to disparity map ejection",
    "checked": true,
    "id": "92b078d24bdfb68d5f2006df883962caabd1e37c",
    "semantic_title": "a low power, high throughput, fully event-based stereo system",
    "citation_count": 52,
    "authors": [
      "Alexander Andreopoulos",
      "Hirak J. Kashyap",
      "Tapan K. Nayak",
      "Arnon Amir",
      "Myron D. Flickner"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Han_VITON_An_Image-Based_CVPR_2018_paper.html": {
    "title": "VITON: An Image-Based Virtual Try-On Network",
    "volume": "main",
    "abstract": "We present an image-based VIirtual Try-On Network (VITON) without using 3D information in any form, which seamlessly transfers a desired clothing item onto the corresponding region of a person using a coarse-to-fine strategy. Conditioned upon a new clothing-agnostic yet descriptive person representation, our framework first generates a coarse synthesized image with the target clothing item overlaid on that same person in the same pose. We further enhance the initial blurry clothing area with a refinement network. The network is trained to learn how much detail to utilize from the target clothing item, and where to apply to the person in order to synthesize a photo-realistic image in which the target item deforms naturally with clear visual patterns. Experiments on our newly collected Zalando dataset demonstrate its promise in the image-based virtual try-on task over state-of-the-art generative models",
    "checked": true,
    "id": "473993aad08d6e1967ad692513e7c539f07b780e",
    "semantic_title": "viton: an image-based virtual try-on network",
    "citation_count": 445,
    "authors": [
      "Xintong Han",
      "Zuxuan Wu",
      "Zhe Wu",
      "Ruichi Yu",
      "Larry S. Davis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lyu_Multi-Oriented_Scene_Text_CVPR_2018_paper.html": {
    "title": "Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation",
    "volume": "main",
    "abstract": "Previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories. The first category treats scene text as a type of general objects and follows general object detection paradigm to localize scene text by regressing the text box locations, but troubled by the arbitrary-orientation and large aspect ratios of scene text. The second one segments text regions directly, but mostly needs complex post processing. In this paper, we present a method that combines the ideas of the two types of methods while avoiding their shortcomings. We propose to detect scene text by localizing corner points of text bounding boxes and segmenting text regions in relative positions. In inference stage, candidate boxes are generated by sampling and grouping corner points, which are further scored by segmentation maps and suppressed by NMS. Compared with previous methods, our method can handle long oriented text naturally and doesn't need complex post processing. The experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text demonstrate that the proposed algorithm achieves better or comparable results in both accuracy and efficiency. Based on VGG16, it achieves an F-measure of 84:3% on ICDAR2015 and 81:5% on MSRA-TD500",
    "checked": true,
    "id": "d2f881259d016e6b45c41dd31b5b79b42a9410f0",
    "semantic_title": "multi-oriented scene text detection via corner localization and region segmentation",
    "citation_count": 302,
    "authors": [
      "Pengyuan Lyu",
      "Cong Yao",
      "Wenhao Wu",
      "Shuicheng Yan",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Azadi_Multi-Content_GAN_for_CVPR_2018_paper.html": {
    "title": "Multi-Content GAN for Few-Shot Font Style Transfer",
    "volume": "main",
    "abstract": "In this work, we focus on the challenge of taking partial observations of highly-stylized text and generalizing the observations to generate unobserved glyphs in the ornamented typeface. To generate a set of multi-content images following a consistent style from very few examples, we propose an end-to-end stacked conditional GAN model considering content along channels and style along network layers. Our proposed network transfers the style of given glyphs to the contents of unseen ones, capturing highly stylized fonts found in the real-world such as those on movie posters or infographics. We seek to transfer both the typographic stylization (ex. serifs and ears) as well as the textual stylization (ex. color gradients and effects.) We base our experiments on our collected data set including 10,000 fonts with different styles and demonstrate effective generalization from a very small number of observed glyphs",
    "checked": true,
    "id": "8faddb00bb5b99013ec359475ba7787ff7e05229",
    "semantic_title": "multi-content gan for few-shot font style transfer",
    "citation_count": 274,
    "authors": [
      "Samaneh Azadi",
      "Matthew Fisher",
      "Vladimir G. Kim",
      "Zhaowen Wang",
      "Eli Shechtman",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shlizerman_Audio_to_Body_CVPR_2018_paper.html": {
    "title": "Audio to Body Dynamics",
    "volume": "main",
    "abstract": "We present a method that gets as input an audio of violin or piano playing, and outputs a video of skeleton predictions which are further used to animate an avatar. The key idea is to create an animation of an avatar that moves their hands similarly to how a pianist or violinist would do, just from audio. Notably, it's not clear if body movement can be predicted from music at all and our aim in this work is to explore this possibility. In this paper, we present the first result that shows that natural body dynamics can be predicted. We built an LSTM network that is trained on violin and piano recital videos uploaded to the Internet. The predicted points are applied onto a rigged avatar to create the animation",
    "checked": true,
    "id": "c6d60aaad68fa78c914ee34c26bceab033a88622",
    "semantic_title": "audio to body dynamics",
    "citation_count": 130,
    "authors": [
      "Eli Shlizerman",
      "Lucio Dery",
      "Hayden Schoen",
      "Ira Kemelmacher-Shlizerman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Weakly_Supervised_Coupled_CVPR_2018_paper.html": {
    "title": "Weakly Supervised Coupled Networks for Visual Sentiment Analysis",
    "volume": "main",
    "abstract": "Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions on-line. In this paper, we solve the problem of visual sentiment analysis using the high-level abstraction in the recognition process. Existing methods based on convolutional neural networks learn sentiment representations from the holistic image appearance. However, different image regions can have a different influence on the intended expression. This paper presents a weakly supervised coupled convolutional network with two branches to leverage the localized information. The first branch detects a sentiment specific soft map by training a fully convolutional network with the cross spatial pooling strategy, which only requires image-level labels, thereby significantly reducing the annotation burden. The second branch utilizes both the holistic and localized information by coupling the sentiment map with deep features for robust classification. We integrate the sentiment detection and classification branches into a unified deep framework and optimize the network in an end-to-end manner. Extensive experiments on six benchmark datasets demonstrate that the proposed method performs favorably against the state-ofthe-art methods for visual sentiment analysis",
    "checked": true,
    "id": "5ec456cea7c6831eba87cebf9e2fa873b5aee4d2",
    "semantic_title": "weakly supervised coupled networks for visual sentiment analysis",
    "citation_count": 107,
    "authors": [
      "Jufeng Yang",
      "Dongyu She",
      "Yu-Kun Lai",
      "Paul L. Rosin",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yagi_Future_Person_Localization_CVPR_2018_paper.html": {
    "title": "Future Person Localization in First-Person Videos",
    "volume": "main",
    "abstract": "We present a new task that predicts future locations of people observed in first-person videos. Consider a first-person video stream continuously recorded by a wearable camera. Given a short clip of a person that is extracted from the complete stream, we aim to predict that person's location in future frames. To facilitate this future person localization ability, we make the following three key observations: a) First-person videos typically involve significant ego-motion which greatly affects the location of the target person in future frames; b) Scales of the target person act as a salient cue to estimate a perspective effect in first-person videos; c) First-person videos often capture people up-close, making it easier to leverage target poses (e.g., where they look) for predicting their future locations. We incorporate these three observations into a prediction framework with a multi-stream convolution-deconvolution architecture. Experimental results reveal our method to be effective on our new dataset as well as on a public social interaction dataset",
    "checked": true,
    "id": "0d1b8ac91ca95f5234d58602078aa13753f3c73b",
    "semantic_title": "future person localization in first-person videos",
    "citation_count": 145,
    "authors": [
      "Takuma Yagi",
      "Karttikeya Mangalam",
      "Ryo Yonetani",
      "Yoichi Sato"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Annadani_Preserving_Semantic_Relations_CVPR_2018_paper.html": {
    "title": "Preserving Semantic Relations for Zero-Shot Learning",
    "volume": "main",
    "abstract": "Zero-shot learning has gained popularity due to its potential to scale recognition models without requiring additional training data. This is usually achieved by associating categories with their semantic information like attributes. However, we believe that the potential offered by this paradigm is not yet fully exploited. In this work, we propose to utilize the structure of the space spanned by the attributes using a set of relations. We devise objective functions to preserve these relations in the embedding space, thereby inducing semanticity to the embedding space. Through extensive experimental evaluation on five benchmark datasets, we demonstrate that inducing semanticity to the embedding space is beneficial for zero-shot learning. The proposed approach outperforms the state-of-the-art on the standard zero-shot setting as well as the more realistic generalized zero-shot setting. We also demonstrate how the proposed approach can be useful for making approximate semantic inferences about an image belonging to a category for which attribute information is not available",
    "checked": true,
    "id": "f1e79edbd45049e26beb93775d998901da0af912",
    "semantic_title": "preserving semantic relations for zero-shot learning",
    "citation_count": 213,
    "authors": [
      "Yashas Annadani",
      "Soma Biswas"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ravi_Show_Me_a_CVPR_2018_paper.html": {
    "title": "Show Me a Story: Towards Coherent Neural Story Illustration",
    "volume": "main",
    "abstract": "We propose an end-to-end network for the visual illustration of a sequence of sentences forming a story. At the core of our model is the ability to model the inter-related nature of the sentences within a story, as well as the ability to learn coherence to support reference resolution. The framework takes the form of an encoder-decoder architecture, where sentences are encoded using a hierarchical two-level sentence-story GRU, combined with an encoding of coherence, and sequentially decoded using predicted feature representation into a consistent illustrative image sequence. We optimize all parameters of our network in an end-to-end fashion with respect to order embedding loss, encoding entailment between images and sentences. Experiments on the VIST storytelling dataset cite{vist} highlight the importance of our algorithmic choices and efficacy of our overall model",
    "checked": true,
    "id": "a50edc0c1b68634b33770bd0220fb8c3f27503f5",
    "semantic_title": "show me a story: towards coherent neural story illustration",
    "citation_count": 24,
    "authors": [
      "Hareesh Ravi",
      "Lezi Wang",
      "Carlos Muniz",
      "Leonid Sigal",
      "Dimitris Metaxas",
      "Mubbasir Kapadia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Reconstruction_Network_for_CVPR_2018_paper.html": {
    "title": "Reconstruction Network for Video Captioning",
    "volume": "main",
    "abstract": "In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) with a novel encoder-decoder-reconstructor architecture, which leverages both the forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder makes use of the forward flow to produce the sentence description based on the encoded video semantic features. Two types of reconstructors are customized to employ the backward flow and reproduce the video features based on the hidden state sequence generated by the decoder. The generation loss yielded by encoder-decoder and the reconstruction loss introduced by reconstructor are jointly drawn into training the proposed RecNet in an end-to-end fashion. Experimental results on benchmark datasets demonstrate that the proposed reconstructor could boost the encoder-decoder models and leads to significant gains on video caption accuracy",
    "checked": true,
    "id": "ba7405516e1408f0ee6e0d0a8c6d511ce33c0551",
    "semantic_title": "reconstruction network for video captioning",
    "citation_count": 278,
    "authors": [
      "Bairui Wang",
      "Lin Ma",
      "Wei Zhang",
      "Wei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Iscen_Fast_Spectral_Ranking_CVPR_2018_paper.html": {
    "title": "Fast Spectral Ranking for Similarity Search",
    "volume": "main",
    "abstract": "Despite the success of deep learning on representing images for particular object retrieval, recent studies show that the learned representations still lie on manifolds in a high dimensional space. This makes the Euclidean nearest neighbor search biased for this task. Exploring the manifolds online remains expensive even if a nearest neighbor graph has been computed offline. This work introduces an explicit embedding reducing manifold search to Euclidean search followed by dot product similarity search. This is equivalent to linear graph filtering of a sparse signal in the frequency domain. To speed up online search, we compute an approximate Fourier basis of the graph offline. We improve the state of art on particular object retrieval datasets including the challenging Instre dataset containing small objects. At a scale of 10^5 images, the offline cost is only a few hours, while query time is comparable to standard similarity search",
    "checked": true,
    "id": "dde940da5d588f16e232e3a39cf8e92fdebf87fb",
    "semantic_title": "fast spectral ranking for similarity search",
    "citation_count": 44,
    "authors": [
      "Ahmet Iscen",
      "Yannis Avrithis",
      "Giorgos Tolias",
      "Teddy Furon",
      "Ondřej Chum"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Iscen_Mining_on_Manifolds_CVPR_2018_paper.html": {
    "title": "Mining on Manifolds: Metric Learning Without Labels",
    "volume": "main",
    "abstract": "In this work we present a novel unsupervised framework for hard training example mining. The only input to the method is a collection of images relevant to the target application and a meaningful initial representation, provided e.g. by pre-trained CNN. Positive examples are distant points on a single manifold, while negative examples are nearby points on different manifolds. Both types of examples are revealed by disagreements between Euclidean and manifold similarities. The discovered examples can be used in training with any discriminative loss. The method is applied to unsupervised fine-tuning of pre-trained networks for fine-grained classification and particular object retrieval. Our models are on par or are outperforming prior models that are fully or partially supervised",
    "checked": true,
    "id": "8bf6a40787c76527351ac50d92ce9d0b98b8a542",
    "semantic_title": "mining on manifolds: metric learning without labels",
    "citation_count": 108,
    "authors": [
      "Ahmet Iscen",
      "Giorgos Tolias",
      "Yannis Avrithis",
      "Ondřej Chum"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_PIXOR_Real-Time_3D_CVPR_2018_paper.html": {
    "title": "PIXOR: Real-Time 3D Object Detection From Point Clouds",
    "volume": "main",
    "abstract": "We address the problem of real-time 3D object detection from point clouds in the context of autonomous driving. Speed is critical as detection is a necessary component for safety. Existing approaches are, however, expensive in computation due to high dimensionality of point clouds. We utilize the 3D data more efficiently by representing the scene from the Bird's Eye View (BEV), and propose PIXOR, a proposal-free, single-stage detector that outputs oriented 3D object estimates decoded from pixel-wise neural network predictions. The input representation, network architecture, and model optimization are specially designed to balance high accuracy and real-time efficiency. We validate PIXOR on two datasets: the KITTI BEV object detection benchmark, and a large-scale 3D vehicle detection benchmark. In both datasets we show that the proposed detector surpasses other state-of-the-art methods notably in terms of Average Precision (AP), while still runs at 10 FPS",
    "checked": true,
    "id": "2bcfce1e68e9adb5f1547307e66a7b23c16d319a",
    "semantic_title": "pixor: real-time 3d object detection from point clouds",
    "citation_count": 968,
    "authors": [
      "Bin Yang",
      "Wenjie Luo",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Leveraging_Unlabeled_Data_CVPR_2018_paper.html": {
    "title": "Leveraging Unlabeled Data for Crowd Counting by Learning to Rank",
    "volume": "main",
    "abstract": "We propose a novel crowd counting approach that leverages abundantly available unlabeled crowd imagery in a learning-to-rank framework. To induce a ranking of cropped images , we use the observation that any sub-image of a crowded scene image is guaranteed to contain the same number or fewer persons than the super-image. This allows us to address the problem of limited size of existing datasets for crowd counting. We collect two crowd scene datasets from Google using keyword searches and query-by-example image retrieval, respectively. We demonstrate how to efficiently learn from these unlabeled datasets by incorporating learning-to-rank in a multi-task network which simultaneously ranks images and estimates crowd density maps. Experiments on two of the most challenging crowd counting datasets show that our approach obtains state-of-the-art results",
    "checked": true,
    "id": "5ece646d13786df1d84ff8eb2bfbc736dd337378",
    "semantic_title": "leveraging unlabeled data for crowd counting by learning to rank",
    "citation_count": 265,
    "authors": [
      "Xialei Liu",
      "Joost van de Weijer",
      "Andrew D. Bagdanov"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Zero-Shot_Kernel_Learning_CVPR_2018_paper.html": {
    "title": "Zero-Shot Kernel Learning",
    "volume": "main",
    "abstract": "In this paper, we address an open problem of zero-shot learning. Its principle is based on learning a mapping that associates feature vectors extracted from i.e. images and attribute vectors that describe objects and/or scenes of interest. In turns, this allows classifying unseen object classes and/or scenes by matching feature vectors via mapping to a newly defined attribute vector describing a new class. Due to importance of such a learning task, there exist many methods that learn semantic, probabilistic, linear or piece-wise linear mappings. In contrast, we apply well-established kernel methods to learn a non-linear mapping between the feature and attribute spaces. We propose an easy learning objective with orthogonality constraints inspired by the Linear Discriminant Analysis, Kernel-Target Alignment and Kernel Polarization methods. We evaluate the performance of our algorithm on the Polynomial as well as shift-invariant Gaussian and Cauchy kernels. Despite simplicity of our approach, we obtain state-of-the-art results on several zero-shot learning datasets and benchmarks including very recent AWA2 dataset",
    "checked": true,
    "id": "04b559c0968751cc89980de3429c6bd10d6a1750",
    "semantic_title": "zero-shot kernel learning",
    "citation_count": 106,
    "authors": [
      "Hongguang Zhang",
      "Piotr Koniusz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Patro_Differential_Attention_for_CVPR_2018_paper.html": {
    "title": "Differential Attention for Visual Question Answering",
    "volume": "main",
    "abstract": "In this paper we aim to answer questions based on images when provided with a dataset of question-answer pairs for a number of images during training. A number of methods have focused on solving this problem by using image based attention. This is done by focusing on a specific part of the image while answering the question. Humans also do so when solving this problem. However, the regions that the previous systems focus on are not correlated with the regions that humans focus on. The accuracy is limited due to this drawback. In this paper, we propose to solve this problem by using an exemplar based method. We obtain one or more supporting and opposing exemplars to obtain a differential attention region. This differential attention is closer to human attention than other image based attention methods. It also helps in obtaining improved accuracy when answering questions. The method is evaluated on challenging benchmark datasets. We perform better than other image based attention methods and are competitive with other state of the art methods that focus on both image and questions",
    "checked": true,
    "id": "6ef1d2076f50940683e326b97cbf0d9e5d630116",
    "semantic_title": "differential attention for visual question answering",
    "citation_count": 71,
    "authors": [
      "Badri Patro",
      "Vinay P. Namboodiri"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Niu_Learning_From_Noisy_CVPR_2018_paper.html": {
    "title": "Learning From Noisy Web Data With Category-Level Supervision",
    "volume": "main",
    "abstract": "Learning from web data is increasingly popular due to abundant free web resources. However, the performance gap between webly supervised learning and traditional supervised learning is still very large, due to the label noise of web data. To fill this gap, most existing methods propose to purify or augment web data using instance-level supervision, which generally requires heavy annotation. Instead, we propose to address the label noise by using more accessible category-level supervision. In particular, we build our deep probabilistic framework upon variational autoencoder (VAE), in which classification network and VAE can jointly leverage category-level hybrid information. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "5b5c8d2feb75b79c37fa51487b9e611b9242642b",
    "semantic_title": "learning from noisy web data with category-level supervision",
    "citation_count": 30,
    "authors": [
      "Li Niu",
      "Qingtao Tang",
      "Ashok Veeraraghavan",
      "Ashutosh Sabharwal"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ramanishka_Toward_Driving_Scene_CVPR_2018_paper.html": {
    "title": "Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning",
    "volume": "main",
    "abstract": "Driving Scene understanding is a key ingredient for intelligent transportation systems. To achieve systems that can operate in a complex physical and social environment, they need to understand and learn how humans drive and interact with traffic scenes. We present the Honda Research Institute Driving Dataset (HDD), a challenging dataset to enable research on learning driver behavior in real-life environments. The dataset includes 104 hours of real human driving in the San Francisco Bay Area collected using an instrumented vehicle equipped with different sensors. We provide a detailed analysis of HDD with a comparison to other driving datasets. A novel annotation methodology is introduced to enable research on driver behavior understanding from untrimmed data sequences. As the first step, baseline algorithms for driver behavior detection are trained and tested to demonstrate the feasibility of the proposed task",
    "checked": true,
    "id": "7b5bd8535e49747b752c5eb8c0d80cd45a30078b",
    "semantic_title": "toward driving scene understanding: a dataset for learning driver behavior and causal reasoning",
    "citation_count": 225,
    "authors": [
      "Vasili Ramanishka",
      "Yi-Ting Chen",
      "Teruhisa Misu",
      "Kate Saenko"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ak_Learning_Attribute_Representations_CVPR_2018_paper.html": {
    "title": "Learning Attribute Representations With Localization for Flexible Fashion Search",
    "volume": "main",
    "abstract": "In this paper, we investigate ways of conducting a detailed fashion search using query images and attributes. A credible fashion search platform should be able to (1) find images that share the same attributes as the query image, (2) allow users to manipulate certain attributes, e.g. replace collar attribute from round to v-neck, and (3) handle region-specific attribute manipulations, e.g. replacing the color attribute of the sleeve region without changing the color attribute of other regions. A key challenge to be addressed is that fashion products have multiple attributes and it is important for each of these attributes to have representative features. To address these challenges, we propose the FashionSearchNet which uses a weakly supervised localization method to extract regions of attributes. By doing so, unrelated features can be ignored thus improving the similarity learning. Also, FashionSearchNet incorporates a new procedure that enables region awareness to be able to handle region-specific requests. FashionSearchNet outperforms the most recent fashion search techniques and is shown to be able to carry out different search scenarios using the dynamic queries",
    "checked": true,
    "id": "1c33e8577ef7122ba7aabf2b1e1f103e35b3577e",
    "semantic_title": "learning attribute representations with localization for flexible fashion search",
    "citation_count": 111,
    "authors": [
      "Kenan E. Ak",
      "Ashraf A. Kassim",
      "Joo Hwee Lim",
      "Jo Yew Tham"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper.html": {
    "title": "Bidirectional Retrieval Made Simple",
    "volume": "main",
    "abstract": "This paper provides a very simple yet effective character-level architecture for learning bidirectional retrieval models. Aligning multimodal content is particularly challenging considering the difficulty in finding semantic correspondence between images and descriptions. We introduce an efficient character-level inception module, designed to learn textual semantic embeddings by convolving raw characters in distinct granularity levels. Our approach is capable of explicitly encoding hierarchical information from distinct base-level representations (e.g., characters, words, and sentences) into a shared multimodal space, where it maps the semantic correspondence between images and descriptions via a contrastive pairwise loss function that minimizes order-violations. Models generated by our approach are far more robust to input noise than state-of-the-art strategies based on word-embeddings. Despite being conceptually much simpler and requiring fewer parameters, our models outperform the state-of-the-art approaches by 4.8% in the task of description retrieval and 2.7% (absolute R@1 values) in the task of image retrieval in the popular MS COCO retrieval dataset. Finally, we show that our models present solid performance for text classification as well, specially in multilingual and noisy domains",
    "checked": true,
    "id": "083551c35be43aa3f0cc45408c09ec46b7a239ea",
    "semantic_title": "bidirectional retrieval made simple",
    "citation_count": 36,
    "authors": [
      "Jônatas Wehrmann",
      "Rodrigo C. Barros"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Learning_Multi-Instance_Enriched_CVPR_2018_paper.html": {
    "title": "Learning Multi-Instance Enriched Image Representations via Non-Greedy Ratio Maximization of the l1-Norm Distances",
    "volume": "main",
    "abstract": "Multi-instance learning (MIL) has demonstrated its usefulness in many real-world image applications in recent years. However, two critical challenges prevent one from effectively using MIL in practice. First, existing MIL methods routinely model the predictive targets using the instances of input images, but rarely utilize an input image as a whole. As a result, the useful information conveyed by the holistic representation of an input image could be potentially lost. Second, the varied numbers of the instances of the input images in a data set make it infeasible to use traditional learning models that can only deal with single-vector inputs. To tackle these two challenges, in this paper we propose a novel image representation learning method that can integrate the local patches (the instances) of an input image (the bag) and its holistic representation into one single-vector representation. Our new method first learns a projection to preserve both global and local consistencies of the instances of an input image. It then projects the holistic representation of the same image into the learned subspace for information enrichment. Taking into account the content and characterization variations in natural scenes and photos, we develop an objective that maximizes the ratio of the summations of a number of L1-norm distances, which is difficult to solve in general. To solve our objective, we derive a new efficient non-greedy iterative algorithm and rigorously prove its convergence. Promising results in extensive experiments have demonstrated improved performances of our new method that validate its effectiveness",
    "checked": true,
    "id": "f05b2227cc20ffe1f99ea5c31ba9479030af36df",
    "semantic_title": "learning multi-instance enriched image representations via non-greedy ratio maximization of the l1-norm distances",
    "citation_count": 18,
    "authors": [
      "Kai Liu",
      "Hua Wang",
      "Feiping Nie",
      "Hao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Su_Learning_Visual_Knowledge_CVPR_2018_paper.html": {
    "title": "Learning Visual Knowledge Memory Networks for Visual Question Answering",
    "volume": "main",
    "abstract": "Visual question answering (VQA) requires joint comprehension of images and natural language questions, where many questions can't be directly or clearly answered from visual content but require reasoning from structured human knowledge with confirmation from visual content. This paper proposes visual knowledge memory network (VKMN) to address this issue, which seamlessly incorporates structured human knowledge and deep visual features into memory networks in an end-to-end learning framework. Comparing to existing methods for leveraging external knowledge for supporting VQA, this paper stresses more on two missing mechanisms. First is the mechanism for integrating visual contents with knowledge facts. VKMN handles this issue by embedding knowledge triples (subject, relation, target) and deep visual features jointly into the visual knowledge features. Second is the mechanism for handling multiple knowledge facts expanding from question and answer pairs. VKMN stores joint embedding using key-value pair structure in the memory networks so that it is easy to handle multiple facts. Experiments show that the proposed method achieves promising results on both VQA v1.0 and v2.0 benchmarks, while outperforms state-of-the-art methods on the knowledge-reasoning related questions",
    "checked": true,
    "id": "33f08157b959070ba802afbb135f4336c5a426fd",
    "semantic_title": "learning visual knowledge memory networks for visual question answering",
    "citation_count": 57,
    "authors": [
      "Zhou Su",
      "Chen Zhu",
      "Yinpeng Dong",
      "Dongqi Cai",
      "Yurong Chen",
      "Jianguo Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_Visual_Grounding_via_CVPR_2018_paper.html": {
    "title": "Visual Grounding via Accumulated Attention",
    "volume": "main",
    "abstract": "Visual Grounding (VG) aims to locate the most relevant object or region in an image, based on a natural language query. The query can be a phrase, a sentence or even a multi-round dialogue. There are three main challenges in VG: 1) what is the main focus in a query; 2) how to understand an image; 3) how to locate an object. Most existing methods combine all the information curtly, which may suffer from the problem of information redundancy (i.e. ambiguous query, complicated image and a large number of objects). In this paper, we formulate these challenges as three attention problems and propose an accumulated attention (A-ATT) mechanism to reason among them jointly. Our A-ATT mechanism can circularly accumulate the attention for useful information in image, query, and objects, while the noises are ignored gradually. We evaluate the performance of A-ATT on four popular datasets (namely ReferCOCO, ReferCOCO+, ReferCOCOg, and Guesswhat?!), and the experimental results show the superiority of the proposed method in term of accuracy",
    "checked": true,
    "id": "8d1c8dd0559642a3bdd5c7234d2ce4611e911e23",
    "semantic_title": "visual grounding via accumulated attention",
    "citation_count": 9,
    "authors": [
      "Chaorui Deng",
      "Qi Wu",
      "Qingyao Wu",
      "Fuyuan Hu",
      "Fan Lyu",
      "Mingkui Tan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Beyond_Trade-Off_Accelerate_CVPR_2018_paper.html": {
    "title": "Beyond Trade-Off: Accelerate FCN-Based Face Detector With Higher Accuracy",
    "volume": "main",
    "abstract": "Fully convolutional neural network (FCN) has been dominating the game of face detection task for a few years with its congenital capability of sliding-window-searching with shared kernels, which boiled down all the redundant calculation, and most recent state-of-the-art methods such as Faster-RCNN, SSD, YOLO and FPN use FCN as their backbone. So here comes one question: Can we find a universal strategy to further accelerate FCN with higher accuracy, so could accelerate all the recent FCN-based methods? To analyze this, we decompose the face searching space into two orthogonal directions, `scale' and `spatial'. Only a few coordinates in the space expanded by the two base vectors indicate foreground. So if FCN could ignore most of the other points, the searching space and false alarm should be significantly boiled down. Based on this philosophy, a novel method named scale estimation and spatial attention proposal (S^2AP) is proposed to pay attention to some specific scales in image pyramid and valid locations in each scales layer. Furthermore, we adopt a masked convolution operation based on the attention result to accelerate FCN calculation. Experiments show that FCN-based method RPN can be accelerated by about 4X with the help of S^2AP and masked-FCN and at the same time it can also achieve the state-of-the-art on FDDB, AFW and MALF face detection benchmarks as well",
    "checked": true,
    "id": "493f7f6db3160b3e011a96db8add618bcda39616",
    "semantic_title": "beyond trade-off: accelerate fcn-based face detector with higher accuracy",
    "citation_count": 34,
    "authors": [
      "Guanglu Song",
      "Yu Liu",
      "Ming Jiang",
      "Yujie Wang",
      "Junjie Yan",
      "Biao Leng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.html": {
    "title": "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning",
    "volume": "main",
    "abstract": "This paper presents a method for adding multiple tasks to a single deep neural network while avoiding catastrophic forgetting. Inspired by network pruning techniques, we exploit redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, we are able to sequentially ``pack'' multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. Unlike prior work that uses proxy losses to maintain accuracy on older tasks, we always optimize for the task at hand. We perform extensive experiments on a variety of network architectures and large-scale datasets, and observe much better robustness against catastrophic forgetting than prior work. In particular, we are able to add three fine-grained classification tasks to a single ImageNet-trained VGG-16 network and achieve accuracies close to those of separately trained networks for each task",
    "checked": true,
    "id": "47bc048efb90e7b8bae5c1fcc979a78b65763fe9",
    "semantic_title": "packnet: adding multiple tasks to a single network by iterative pruning",
    "citation_count": 964,
    "authors": [
      "Arun Mallya",
      "Svetlana Lazebnik"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.html": {
    "title": "Repulsion Loss: Detecting Pedestrians in a Crowd",
    "volume": "main",
    "abstract": "Detecting individual pedestrians in a crowd remains a challenging problem since the pedestrians often gather together and occlude each other in real-world scenarios. In this paper, we first explore how a state-of-the-art pedestrian detector is harmed by crowd occlusion via experimentation, providing insights into the crowd occlusion problem. Then, we propose a novel bounding box regression loss specifically designed for crowd scenes, termed repulsion loss. This loss is driven by two motivations: the attraction by target, and the repulsion by other surrounding objects. The repulsion term prevents the proposal from shifting to surrounding objects thus leading to more crowd-robust localization. Our detector trained by repulsion loss outperforms the state-of-the-art methods with a significant improvement in occlusion cases",
    "checked": true,
    "id": "a7dd621658a44d007f5029fb2909ebf3d3e3fc22",
    "semantic_title": "repulsion loss: detecting pedestrians in a crowd",
    "citation_count": 441,
    "authors": [
      "Xinlong Wang",
      "Tete Xiao",
      "Yuning Jiang",
      "Shuai Shao",
      "Jian Sun",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Camgoz_Neural_Sign_Language_CVPR_2018_paper.html": {
    "title": "Neural Sign Language Translation",
    "volume": "main",
    "abstract": "Sign Language Recognition (SLR) has been an active research field for the last two decades. However, most research to date has considered SLR as a naive gesture recognition problem. SLR seeks to recognize a sequence of continuous signs but neglects the underlying rich grammatical and linguistic structures of sign language that differ from spoken language. In contrast, we introduce the Sign Language Translation (SLT) problem. Here, the objective is to generate spoken language translations from sign language videos, taking into account the different word orders and grammar. We formalize SLT in the framework of Neural Machine Translation (NMT) for both end-to-end and pretrained settings (using expert knowledge). This allows us to jointly learn the spatial representations, the underlying language model, and the mapping between sign and spoken language. To evaluate the performance of Neural SLT, we collected the first publicly available Continuous SLT dataset, RWTH-PHOENIX-Weather 2014T. It provides spoken language translations and gloss level annotations for German Sign Language videos of weather broadcasts. Our dataset contains over .95M frames with >67K signs from a sign vocabulary of >1K and >99K words from a German vocabulary of >2.8K. We report quantitative and qualitative results for various SLT setups to underpin future research in this newly established field. The upper bound for translation performance is calculated at 19.26 BLEU-4, while our end-to-end frame-level and gloss-level tokenization networks were able to achieve 9.58 and 18.13 respectively",
    "checked": true,
    "id": "644602c65a5d8f30e62be027eb7b47f7c335191a",
    "semantic_title": "neural sign language translation",
    "citation_count": 398,
    "authors": [
      "Necati Cihan Camgoz",
      "Simon Hadfield",
      "Oscar Koller",
      "Hermann Ney",
      "Richard Bowden"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html": {
    "title": "Non-Local Neural Networks",
    "volume": "main",
    "abstract": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available",
    "checked": true,
    "id": "8899094797e82c5c185a0893896320ef77f60e64",
    "semantic_title": "non-local neural networks",
    "citation_count": 7615,
    "authors": [
      "Xiaolong Wang",
      "Ross Girshick",
      "Abhinav Gupta",
      "Kaiming He"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Baraldi_LAMV_Learning_to_CVPR_2018_paper.html": {
    "title": "LAMV: Learning to Align and Match Videos With Kernelized Temporal Layers",
    "volume": "main",
    "abstract": "This paper considers a learnable approach for comparing and aligning videos. Our architecture builds upon and revisits temporal match kernels within neural networks: we propose a new temporal layer that finds temporal alignments by maximizing the scores between two sequences of vectors, according to a time-sensitive similarity metric parametrized in the Fourier domain. We learn this layer with a temporal proposal strategy, in which we minimize a triplet loss that takes into account both the localization accuracy and the recognition rate. We evaluate our approach on video alignment, copy detection and event retrieval. Our approach outperforms the state on the art on temporal video alignment and video copy detection datasets in comparable setups. It also attains the best reported results for particular event search, while precisely aligning videos",
    "checked": true,
    "id": "f11acabdc1aa9fb8917431268f85746b88d88c32",
    "semantic_title": "lamv: learning to align and match videos with kernelized temporal layers",
    "citation_count": 40,
    "authors": [
      "Lorenzo Baraldi",
      "Matthijs Douze",
      "Rita Cucchiara",
      "Hervé Jégou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Optimizing_Video_Object_CVPR_2018_paper.html": {
    "title": "Optimizing Video Object Detection via a Scale-Time Lattice",
    "volume": "main",
    "abstract": "High-performance object detection relies on expensive convolutional networks to compute features, often leading to significant challenges in applications, e.g. those that re- quire detecting objects from video streams in real time. The key to this problem is to trade accuracy for efficiency in an effective way, i.e. reducing the computing cost while maintaining competitive performance. To seek a good balance, previous efforts usually focus on optimizing the model architectures. This paper explores an alternative approach, that is, to reallocate the computation over a scale-time space. The basic idea is to perform expensive detection sparsely and propagate the results across both scales and time with substantially cheaper networks, by exploiting the strong correlations among them. Specifically, we present a unified framework that integrates detection, temporal propagation, and across-scale refinement on a Scale-Time Lattice. On this framework, one can explore various strategies to balance performance and cost. Taking advantage of this flexibility, we further develop an adaptive scheme with the detector invoked on demand and thus obtain improved tradeoff. On ImageNet VID dataset, the proposed method can achieve a competitive mAP 79.6% at 20 fps, or 79.0% at 62 fps as a performance/speed tradeoff",
    "checked": true,
    "id": "ce7b5c17062f796cd5e964fa756c92476682cf07",
    "semantic_title": "optimizing video object detection via a scale-time lattice",
    "citation_count": 98,
    "authors": [
      "Kai Chen",
      "Jiaqi Wang",
      "Shuo Yang",
      "Xingcheng Zhang",
      "Yuanjun Xiong",
      "Chen Change Loy",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Su_Learning_Compressible_360deg_CVPR_2018_paper.html": {
    "title": "Learning Compressible 360° Video Isomers",
    "volume": "main",
    "abstract": "Standard video encoders developed for conventional narrow field-of-view video are widely applied to 360° video as well, with reasonable results. However, while this approach commits arbitrarily to a projection of the spherical frames, we observe that some orientations of a 360° video, once projected, are more compressible than others. We introduce an approach to predict the sphere rotation that will yield the maximal compression rate. Given video clips in their original encoding, a convolutional neural network learns the association between a clip's visual content and its compressibility at different rotations of a cubemap projection. Given a novel video, our learning-based approach efficiently infers the most compressible direction in one shot, without repeated rendering and compression of the source video. We validate our idea on thousands of video clips and multiple popular video codecs. The results show that this untapped dimension of 360° compression has substantial potential—\"good\" rotations are typically 8−10% more compressible than bad ones, and our learning approach can predict them reliably 82% of the time",
    "checked": true,
    "id": "bfda6c1f4023dd854645e5685829c21a581ebe8b",
    "semantic_title": "learning compressible 360° video isomers",
    "citation_count": 18,
    "authors": [
      "Yu-Chuan Su",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Long_Attention_Clusters_Purely_CVPR_2018_paper.html": {
    "title": "Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification",
    "volume": "main",
    "abstract": "Recently, substantial research effort has focused on how to apply CNNs or RNNs to better capture temporal patterns in videos, so as to improve the accuracy of video classification. In this paper, however, we show that temporal information, especially longer-term patterns, may not be necessary to achieve competitive results on common trimmed video classification datasets. We investigate the potential of a purely attention based local feature integration. Accounting for the characteristics of such features in video classification, we propose a local feature integration framework based on attention clusters, and introduce a shifting operation to capture more diverse signals. We carefully analyze and compare the effect of different attention mechanisms, cluster sizes, and the use of the shifting operation, and also investigate the combination of attention clusters for multimodal integration. We demonstrate the effectiveness of our framework on three real-world video classification datasets. Our model achieves competitive results across all of these. In particular, on the large-scale Kinetics dataset, our framework obtains an excellent single model accuracy of 79.4% in terms of the top-1 and 94.0% in terms of the top-5 accuracy on the validation set",
    "checked": true,
    "id": "5406fd98aa22bc2a0c1a8bc2a58ca3eb7a91155d",
    "semantic_title": "attention clusters: purely attention based local feature integration for video classification",
    "citation_count": 200,
    "authors": [
      "Xiang Long",
      "Chuang Gan",
      "Gerard de Melo",
      "Jiajun Wu",
      "Xiao Liu",
      "Shilei Wen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Feichtenhofer_What_Have_We_CVPR_2018_paper.html": {
    "title": "What Have We Learned From Deep Representations for Action Recognition?",
    "volume": "main",
    "abstract": "As the success of deep models has led to their deployment in all areas of computer vision, it is increasingly important to understand how these representations work and what they are capturing. In this paper, we shed light on deep spatiotemporal representations by visualizing what two-stream models have learned in order to recognize actions in video. We show that local detectors for appearance and motion objects arise to form distributed representations for recognizing human actions. Key observations include the following. First, cross-stream fusion enables the learning of true spatiotemporal features rather than simply separate appearance and motion features. Second, the networks can learn local representations that are highly class specific, but also generic representations that can serve a range of classes. Third, throughout the hierarchy of the network, features become more abstract and show increasing invariance to aspects of the data that are unimportant to desired distinctions (e.g. motion patterns across various speeds). Fourth, visualizations can be used not only to shed light on learned representations, but also to reveal idiosyncracies of training data and to explain failure cases of the system",
    "checked": true,
    "id": "93a7b595757f0e37e59a7b24c6d08508c4177405",
    "semantic_title": "what have we learned from deep representations for action recognition?",
    "citation_count": 46,
    "authors": [
      "Christoph Feichtenhofer",
      "Axel Pinz",
      "Richard P. Wildes",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hao_Controllable_Video_Generation_CVPR_2018_paper.html": {
    "title": "Controllable Video Generation With Sparse Trajectories",
    "volume": "main",
    "abstract": "Video generation and manipulation is an important yet challenging task in computer vision. Existing methods usually lack ways to explicitly control the synthesized motion. In this work, we present a conditional video generation model that allows detailed control over the motion of the generated video. Given the first frame and sparse motion trajectories specified by users, our model can synthesize a video with corresponding appearance and motion. We propose to combine the advantage of copying pixels from the given frame and hallucinating the lightness difference from scratch which help generate sharp video while keeping the model robust to occlusion and lightness change. We also propose a training paradigm that calculate trajectories from video clips, which eliminated the need of annotated training data. Experiments on several standard benchmarks demonstrate that our approach can generate realistic videos comparable to state-of-the-art video generation and video prediction methods while the motion of the generated videos can correspond well with user input",
    "checked": true,
    "id": "9d629ee73070e7c693ae6924aa52df129a127b33",
    "semantic_title": "controllable video generation with sparse trajectories",
    "citation_count": 82,
    "authors": [
      "Zekun Hao",
      "Xun Huang",
      "Serge Belongie"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Park_Representing_and_Learning_CVPR_2018_paper.html": {
    "title": "Representing and Learning High Dimensional Data With the Optimal Transport Map From a Probabilistic Viewpoint",
    "volume": "main",
    "abstract": "In this paper, we propose a generative model in the space of diffeomorphic deformation maps. More precisely, we utilize the Kantarovich-Wasserstein metric and accompanying geometry to represent an image as a deformation from templates. Moreover, we incorporate a probabilistic viewpoint by assuming that each image is locally generated from a reference image. We capture the local structure by modelling the tangent planes at reference images. %; we assume that each image is generated from one of finite number of tangent planes. % by an unobserved discrete random variable that indexes the tangent plane the image belongs to. Once basis vectors for each tangent plane are learned via probabilistic PCA, we can sample a local coordinate, that can be inverted back to image space exactly. With experiments using 4 different datasets, we show that the generative tangent plane model in the optimal transport (OT) manifold can be learned with small numbers of images and can be used to create infinitely many `unseen' images. In addition, the Bayesian classification accompanied with the probabilist modeling of the tangent planes shows improved accuracy over that done in the image space. Combining the results of our experiments supports our claim that certain datasets can be better represented with the Kantarovich-Wasserstein metric. We envision that the proposed method could be a practical solution to learning and representing data that is generated with templates in situatons where only limited numbers of data points are available",
    "checked": true,
    "id": "f395606853a78d6f64b1f13be8938293fdc114ef",
    "semantic_title": "representing and learning high dimensional data with the optimal transport map from a probabilistic viewpoint",
    "citation_count": 12,
    "authors": [
      "Serim Park",
      "Matthew Thorpe"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tung_CLIP-Q_Deep_Network_CVPR_2018_paper.html": {
    "title": "CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization",
    "volume": "main",
    "abstract": "Deep neural networks enable state-of-the-art accuracy on visual recognition tasks such as image classification and object detection. However, modern deep networks contain millions of learned weights; a more efficient utilization of computation resources would assist in a variety of deployment scenarios, from embedded platforms with resource constraints to computing clusters running ensembles of networks. In this paper, we combine network pruning and weight quantization in a single learning framework that performs pruning and quantization jointly, and in parallel with fine-tuning. This allows us to take advantage of the complementary nature of pruning and quantization and to recover from premature pruning errors, which is not possible with current two-stage approaches. Our proposed CLIP-Q method (Compression Learning by In-Parallel Pruning-Quantization) compresses AlexNet by 51-fold, GoogLeNet by 10-fold, and ResNet-50 by 15-fold, while preserving the uncompressed network accuracies on ImageNet",
    "checked": true,
    "id": "289b69ac9bc6b859ab05671f02b0a4f82280e88f",
    "semantic_title": "clip-q: deep network compression learning by in-parallel pruning-quantization",
    "citation_count": 182,
    "authors": [
      "Frederick Tung",
      "Greg Mori"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shanu_Inference_in_Higher_CVPR_2018_paper.html": {
    "title": "Inference in Higher Order MRF-MAP Problems With Small and Large Cliques",
    "volume": "main",
    "abstract": "Higher Order MRF-MAP formulation has been a popular technique for solving many problems in computer vision. Inference in a general MRF-MAP problem is NP Hard, but can be performed in polynomial time for the special case when potential functions are submodular. Two popular combinatorial approaches for solving such formulations are flow based and polyhedral approaches. Flow based approaches work well with small cliques and in that mode can handle problems with millions of variables. Polyhedral approaches can handle large cliques but in small numbers. We show in this paper that the variables in these seemingly disparate techniques can be mapped to each other. This allows us to combine the two styles in a joint framework exploiting the strength of both of them. Using the proposed joint framework, we are able to perform tractable inference in MRF-MAP problems with millions of variables and a mix of small and large cliques, a formulation which can not be solved by either of the two styles individually. We show applicability of this hybrid framework on object segmentation problem as an example of a situation where quality of results is significantly better than systems which are based only on the use of small or large cliques",
    "checked": true,
    "id": "cbc6193973419b61ee10fac69e9a6d77f68e12a7",
    "semantic_title": "inference in higher order mrf-map problems with small and large cliques",
    "citation_count": 5,
    "authors": [
      "Ishant Shanu",
      "Chetan Arora",
      "S.N. Maheshwari"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_ROAD_Reality_Oriented_CVPR_2018_paper.html": {
    "title": "ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes",
    "volume": "main",
    "abstract": "Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a significant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overfits to synthetic images, making the convolutional filters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose a new reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatial-aware adaptation scheme to effectively align the distribution of two domains. These two modules can be readily integrated with existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We evaluate the proposed method on Cityscapes dataset by adapting from GTAV and SYNTHIA datasets, where the results demonstrate the effectiveness of our method",
    "checked": true,
    "id": "0301963f73e8ace21653c0bed0e5feee128f140e",
    "semantic_title": "road: reality oriented adaptation for semantic segmentation of urban scenes",
    "citation_count": 273,
    "authors": [
      "Yuhua Chen",
      "Wen Li",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dolhansky_Eye_In-Painting_With_CVPR_2018_paper.html": {
    "title": "Eye In-Painting With Exemplar Generative Adversarial Networks",
    "volume": "main",
    "abstract": "This paper introduces a novel approach to in-painting where the identity of the object to remove or change is preserved and accounted for at inference time: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize exemplar information to produce high-quality, personalized in-painting results. We propose using exemplar information in the form of a reference image of the region to in-paint, or a perceptual code describing that object. Unlike previous conditional GAN formulations, this extra information can be inserted at multiple points within the adversarial network, thus increasing its descriptive power. We show that ExGANs can produce photo-realistic personalized in-painting results that are both perceptually and semantically plausible by applying them to the task of closed-to-open eye in-painting in natural pictures. A new benchmark dataset is also introduced for the task of eye in-painting for future comparisons",
    "checked": true,
    "id": "c4051f726e529543bd2f2b3039f61c1ba0889301",
    "semantic_title": "eye in-painting with exemplar generative adversarial networks",
    "citation_count": 102,
    "authors": [
      "Brian Dolhansky",
      "Cristian Canton Ferrer"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ClcNet_Improving_the_CVPR_2018_paper.html": {
    "title": "ClcNet: Improving the Efficiency of Convolutional Neural Network Using Channel Local Convolutions",
    "volume": "main",
    "abstract": "Depthwise convolution and grouped convolution has been successfully applied to improve the efficiency of convolutional neural network (CNN). We suggest that these models can be considered as special cases of a generalized convolution operation, named channel local convolution(CLC), where an output channel is computed using a subset of the input channels. This definition entails computation dependency relations between input and output channels, which can be represented by a channel dependency graph(CDG). By modifying the CDG of grouped convolution, a new CLC kernel named interlaced grouped convolution (IGC) is created. Stacking IGC and GC kernels results in a convolution block (named CLC Block) for approximating regular convolution. By resorting to the CDG as an analysis tool, we derive the rule for setting the meta-parameters of IGC and GC and the framework for minimizing the computational cost. A new CNN model named clcNet is then constructed using CLC blocks, which shows significantly higher computational efficiency and fewer parameters compared to state-of-the-art networks, when being tested using the ImageNet-1K dataset",
    "checked": true,
    "id": "8faad74c9373836cd2744056cd1e3f883d8b5bd8",
    "semantic_title": "clcnet: improving the efficiency of convolutional neural network using channel local convolutions",
    "citation_count": 10,
    "authors": [
      "Dong-Qing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhuang_Towards_Effective_Low-Bitwidth_CVPR_2018_paper.html": {
    "title": "Towards Effective Low-Bitwidth Convolutional Neural Networks",
    "volume": "main",
    "abstract": "This paper tackles the problem of training a deep convolutional neural network with both low-precision weights and low-bitwidth activations. Optimizing a low-precision network is very challenging since the training process can easily get trapped in a poor local minima, which results in substantial accuracy loss. To mitigate this problem, we propose three simple-yet-effective approaches to improve the network training. First, we propose to use a two-stage optimization strategy to progressively find good local minima. Specifically, we propose to first optimize a net with quantized weights and then quantized activations. This is in contrast to the traditional methods which optimize them simultaneously. Second, following a similar spirit of the first method, we propose another progressive optimization approach which progressively decreases the bit-width from high-precision to low-precision during the course of training. Third, we adopt a novel learning scheme to jointly train a full-precision model alongside the low-precision one. By doing so, the full-precision model provides hints to guide the low-precision model training. Extensive experiments on various datasets (ie, CIFAR-100 and ImageNet) show the effectiveness of the proposed methods. To highlight, using our methods to train a 4-bit precision network leads to no performance decrease in comparison with its full-precision counterpart with standard network architectures (ie, AlexNet and ResNet-50)",
    "checked": true,
    "id": "4d71873a40063178b3bf894ac87b371f835d658e",
    "semantic_title": "towards effective low-bitwidth convolutional neural networks",
    "citation_count": 211,
    "authors": [
      "Bohan Zhuang",
      "Chunhua Shen",
      "Mingkui Tan",
      "Lingqiao Liu",
      "Ian Reid"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kuen_Stochastic_Downsampling_for_CVPR_2018_paper.html": {
    "title": "Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks",
    "volume": "main",
    "abstract": "It is desirable to train convolutional networks (CNNs) to run more efficiently during inference. In many cases however, the computational budget that the system has for inference cannot be known beforehand during training, or the inference budget is dependent on the changing real-time resource availability. Thus, it is inadequate to train just inference-efficient CNNs, whose inference costs are not adjustable and cannot adapt to varied inference budgets. We propose a novel approach for cost-adjustable inference in CNNs - Stochastic Downsampling Point (SDPoint). During training, SDPoint applies feature map downsampling to a random point in the layer hierarchy, with a random downsampling ratio. The different stochastic downsampling configurations known as SDPoint instances (of the same model) have computational costs different from each other, while being trained to minimize the same prediction loss. Sharing network parameters across different instances provides significant regularization boost. During inference, one may handpick a SDPoint instance that best fits the inference budget. The effectiveness of SDPoint, as both a cost-adjustable inference approach and a regularizer, is validated through extensive experiments on image classification",
    "checked": true,
    "id": "9784bdcb1401301166869969f468dfbe45aad68d",
    "semantic_title": "stochastic downsampling for cost-adjustable inference and improved regularization in convolutional networks",
    "citation_count": 21,
    "authors": [
      "Jason Kuen",
      "Xiangfei Kong",
      "Zhe Lin",
      "Gang Wang",
      "Jianxiong Yin",
      "Simon See",
      "Yap-Peng Tan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Face_Aging_With_CVPR_2018_paper.html": {
    "title": "Face Aging With Identity-Preserved Conditional Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Face aging is of great importance for cross-age recognition and entertainment related applications. However, the lack of labeled faces of the same person across a long age range makes it challenging. Because of different aging speed of different persons, our face aging approach aims at synthesizing a face whose target age lies in some given age group instead of synthesizing a face with a certain age. By grouping faces with target age together, the objective of face aging is equivalent to transferring aging patterns of faces within the target age group to the face whose aged face is to be synthesized. Meanwhile, the synthesized face should have the same identity with the input face. Thus we propose an Identity-Preserved Conditional Generative Adversarial Networks (IPCGANs) framework, in which a Conditional Generative Adversarial Networks module functions as generating a face that looks realistic and is with the target age, an identity-preserved module preserves the identity information and an age classifier forces the generated face with the target age. Both qualitative and quantitative experiments show that our method can generate more realistic faces in terms of image quality, person identity and age consistency with human observations",
    "checked": true,
    "id": "e4e1787c34e0bdbfce4c27de1baafeb9c66c7bab",
    "semantic_title": "face aging with identity-preserved conditional generative adversarial networks",
    "citation_count": 168,
    "authors": [
      "Zongwei Wang",
      "Xu Tang",
      "Weixin Luo",
      "Shenghua Gao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lv_Unsupervised_Cross-Dataset_Person_CVPR_2018_paper.html": {
    "title": "Unsupervised Cross-Dataset Person Re-Identification by Transfer Learning of Spatial-Temporal Patterns",
    "volume": "main",
    "abstract": "Most of the proposed person re-identification algorithms conduct supervised training and testing on single labeled datasets with small size, so directly deploying these trained models to a large-scale real-world camera network may lead to poor performance due to underfitting. It is challenging to incrementally optimize the models by using the abundant unlabeled data collected from the target domain. To address this challenge, we propose an unsupervised incremental learning algorithm, TFusion, which is aided by the transfer learning of the pedestrians' spatio-temporal patterns in the target domain. Specifically, the algorithm firstly transfers the visual classifier trained from small labeled source dataset to the unlabeled target dataset so as to learn the pedestrians' spatial-temporal patterns. Secondly, a Bayesian fusion model is proposed to combine the learned spatio-temporal patterns with visual features to achieve a significantly improved classifier. Finally, we propose a learning-to-rank based mutual promotion procedure to incrementally optimize the classifiers based on the unlabeled data in the target domain. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm gains significant improvement compared with the state-of-art cross-dataset unsupervised person re-identification algorithms",
    "checked": true,
    "id": "4f6e1319c795e7ea1ec220a007c04e9cf37ff4e9",
    "semantic_title": "unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns",
    "citation_count": 168,
    "authors": [
      "Jianming Lv",
      "Weihang Chen",
      "Qing Li",
      "Can Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Feature_Quantization_for_CVPR_2018_paper.html": {
    "title": "Feature Quantization for Defending Against Distortion of Images",
    "volume": "main",
    "abstract": "In this work, we address the problem of improving robustness of convolutional neural networks (CNNs) to image distortion. We argue that higher moment statistics of feature distributions can be shifted due to image distortion, and the shift leads to performance decrease and cannot be reduced by ordinary normalization methods as observed in our experimental analyses. In order to mitigate this effect, we propose an approach base on feature quantization. To be specific, we propose to employ three different types of additional non-linearity in CNNs: i) a floor function with scalable resolution, ii) a power function with learnable exponents, and iii) a power function with data-dependent exponents. In the experiments, we observe that CNNs which employ the proposed methods obtain better performance in both generalization performance and robustness for various distortion types for large scale benchmark datasets. For instance, a ResNet-50 model equipped with the proposed method (+HPOW) obtains 6.95%, 5.26% and 5.61% better accuracy on the ILSVRC-12 classification tasks using images distorted with motion blur, salt and pepper and mixed distortions",
    "checked": true,
    "id": "51bd5966fc992498cb1147d34527e33656c696bb",
    "semantic_title": "feature quantization for defending against distortion of images",
    "citation_count": 23,
    "authors": [
      "Zhun Sun",
      "Mete Ozay",
      "Yan Zhang",
      "Xing Liu",
      "Takayuki Okatani"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Tagging_Like_Humans_CVPR_2018_paper.html": {
    "title": "Tagging Like Humans: Diverse and Distinct Image Annotation",
    "volume": "main",
    "abstract": "In this work we propose a new automatic image annotation model, dubbed diverse and distinct image annotation (D2IA). The generative model D2IA is inspired by the ensemble of human annotations, which create semantically relevant, yet distinct and diverse tags. In D2IA, we generate a relevant and distinct tag subset, in which the tags are relevant to the image contents and semantically distinct to each other, using sequential sampling from a determinantal point process (DPP) model. Multiple such tag subsets that cover diverse semantic aspects or diverse semantic levels of the image contents are generated by randomly perturbing the DPP sampling process. We leverage a generative adversarial network (GAN) model to train D2IA. We perform extensive experiments including quantitative and qualitative comparisons, as well as human subject studies, on two benchmark datasets to demonstrate that the proposed model can produce more diverse and distinct tags than the state-of-the-arts",
    "checked": true,
    "id": "4ba7fb86657a2ee18577af26ee407723e97353d8",
    "semantic_title": "tagging like humans: diverse and distinct image annotation",
    "citation_count": 45,
    "authors": [
      "Baoyuan Wu",
      "Weidong Chen",
      "Peng Sun",
      "Wei Liu",
      "Bernard Ghanem",
      "Siwei Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Re-Weighted_Adversarial_Adaptation_CVPR_2018_paper.html": {
    "title": "Re-Weighted Adversarial Adaptation Network for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to transfer domain knowledge from existing well-defined tasks to new ones where labels are unavailable. In the real-world applications, as the domain (task) discrepancies are usually uncontrollable, it is significantly motivated to match the feature distributions even if the domain discrepancies are disparate. Additionally, as no label is available in the target domain, how to successfully adapt the classifier from the source to the target domain still remains an open question. In this paper, we propose the Re-weighted Adversarial Adaptation Network (RAAN) to reduce the feature distribution divergence and adapt the classifier when domain discrepancies are disparate. Specifically, to alleviate the need of common supports in matching the feature distribution, we choose to minimize optimal transport (OT) based Earth-Mover (EM) distance and reformulate it to a minimax objective function. Utilizing this, RAAN can be trained in an end-to-end and adversarial manner. To further adapt the classifier, we propose to match the label distribution and embed it into the adversarial training. Finally, after extensive evaluation of our method using UDA datasets of varying difficulty, RAAN achieved the state-of-the-art results and outperformed other methods by a large margin when the domain shifts are disparate",
    "checked": true,
    "id": "dad542826a1a1a3cb8170ce2c1315b2c7995df34",
    "semantic_title": "re-weighted adversarial adaptation network for unsupervised domain adaptation",
    "citation_count": 114,
    "authors": [
      "Qingchao Chen",
      "Yang Liu",
      "Zhaowen Wang",
      "Ian Wassell",
      "Kevin Chetty"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hong_Inferring_Semantic_Layout_CVPR_2018_paper.html": {
    "title": "Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "We propose a novel hierarchical approach for text-to-image synthesis by inferring semantic layout. Instead of learning a direct mapping from text to image, our algorithm decomposes the generation process into multiple steps, in which it first constructs a semantic layout from the text by the layout generator and converts the layout to an image by the image generator. The proposed layout generator progressively constructs a semantic layout in a coarse-to-fine manner by generating object bounding boxes and refining each box by estimating object shapes inside the box. The image generator synthesizes an image conditioned on the inferred semantic layout, which provides a useful semantic structure of an image matching with the text description. Our model not only generates semantically more meaningful images, but also allows automatic annotation of generated images and user-controlled generation process by modifying the generated scene layout. We demonstrate the capability of the proposed model on challenging MS-COCO dataset and show that the model can substantially improve the image quality, interpretability of output and semantic alignment to input text over existing approaches",
    "checked": true,
    "id": "cc63a155021362b05c3c75bd5040373d72e0623e",
    "semantic_title": "inferring semantic layout for hierarchical text-to-image synthesis",
    "citation_count": 321,
    "authors": [
      "Seunghoon Hong",
      "Dingdong Yang",
      "Jongwook Choi",
      "Honglak Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Regularizing_RNNs_for_CVPR_2018_paper.html": {
    "title": "Regularizing RNNs for Caption Generation by Reconstructing the Past With the Present",
    "volume": "main",
    "abstract": "Recently, caption generation with an encoder-decoder framework has been extensively studied and applied in different domains, such as image captioning, code captioning, and so on. In this paper, we propose a novel architecture, namely Auto-Reconstructor Network (ARNet), which, coupling with the conventional encoder-decoder framework, works in an end-to-end fashion to generate captions. ARNet aims at reconstructing the previous hidden state with the present one, besides behaving as the input-dependent transition operator. Therefore, ARNet encourages the current hidden state to embed more information from the previous one, which can help regularize the transition dynamics of recurrent neural networks (RNNs). Extensive experimental results show that our proposed ARNet boosts the performance over the existing encoder-decoder models on both image captioning and source code captioning tasks. Additionally, ARNet remarkably reduces the discrepancy between training and inference processes for caption generation. Furthermore, the performance on permuted sequential MNIST demonstrates that ARNet can effectively regularize RNN, especially on modeling long-term dependencies. Our code is available at: https://github.com/chenxinpeng/ARNet",
    "checked": true,
    "id": "85e2b2c35b916b1ee4926c155065d01b21c80c60",
    "semantic_title": "regularizing rnns for caption generation by reconstructing the past with the present",
    "citation_count": 87,
    "authors": [
      "Xinpeng Chen",
      "Lin Ma",
      "Wenhao Jiang",
      "Jian Yao",
      "Wei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pinheiro_Unsupervised_Domain_Adaptation_CVPR_2018_paper.html": {
    "title": "Unsupervised Domain Adaptation With Similarity Learning",
    "volume": "main",
    "abstract": "The objective of unsupervised domain adaptation is to leverage features from a labeled source domain and learn a classifier for an unlabeled target domain, with a similar but different data distribution. Most deep learning approaches consist of two steps: (i) learn features that preserve a low risk on labeled samples (source domain) and (ii) make the features from both domains to be as indistinguishable as possible, so that a classifier trained on the source can also be applied on the target domain. In general, the classifiers in step (i) consist of fully-connected layers applied directly on the indistinguishable features learned in (ii). In this paper, we propose a different way to do the classification, using similarity learning. The proposed method learns a pairwise similarity function in which classification can be performed by computing distances between prototype representations of each category. The domain-invariant features and the categorical prototype representations are learned jointly and in an end-to-end fashion. At inference time, images from the target domain are compared to the prototypes and the label associated with the one that best matches the image is outputed. The approach is simple, scalable and effective. We show that our model achieves state-of-the-art performance in different large-scale unsupervised domain adaptation scenarios",
    "checked": true,
    "id": "152ad26e691a2e3389c31efdaba6ec3f74b9ef8a",
    "semantic_title": "unsupervised domain adaptation with similarity learning",
    "citation_count": 242,
    "authors": [
      "Pedro O. Pinheiro"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Muhammad_Learning_Deep_Sketch_CVPR_2018_paper.html": {
    "title": "Learning Deep Sketch Abstraction",
    "volume": "main",
    "abstract": "Human free-hand sketches have been studied in various contexts including sketch recognition, synthesis and fine-grained sketch-based image retrieval (FG-SBIR). A fundamental challenge for sketch analysis is to deal with drastically different human drawing styles, particularly in terms of abstraction level. In this work, we propose the first stroke-level sketch abstraction model based on the insight of sketch abstraction as a process of trading off between the recognizability of a sketch and the number of strokes used to draw it. Concretely, we train a model for abstract sketch generation through reinforcement learning of a stroke removal policy that learns to predict which strokes can be safely removed without affecting recognizability. We show that our abstraction model can be used for various sketch analysis tasks including: (1) modeling stroke saliency and understanding the decision of sketch recognition models, (2) synthesizing sketches of variable abstraction for a given category, or reference object instance in a photo, and (3) training a FG-SBIR model with photos only, bypassing the expensive photo-sketch pair collection step",
    "checked": true,
    "id": "4feb17463f78ac0365a6ca1ba1ba90efe7e4a768",
    "semantic_title": "learning deep sketch abstraction",
    "citation_count": 83,
    "authors": [
      "Umar Riaz Muhammad",
      "Yongxin Yang",
      "Yi-Zhe Song",
      "Tao Xiang",
      "Timothy M. Hospedales"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mattyus_Matching_Adversarial_Networks_CVPR_2018_paper.html": {
    "title": "Matching Adversarial Networks",
    "volume": "main",
    "abstract": "Generative Adversarial Nets (GANs) and Conditonal GANs (CGANs) show that using a trained network as loss function (discriminator) enables to synthesize highly structured outputs (e.g. natural images). However, applying a discriminator network as a universal loss function for common supervised tasks (e.g. semantic segmentation, line detection, depth estimation) is considerably less successful. We argue that the main difficulty of applying CGANs to supervised tasks is that the generator training consists of optimizing a loss function that does not depend directly on the ground truth labels. To overcome this, we propose to replace the discriminator with a matching network taking into account both the ground truth outputs as well as the generated examples. As a consequence, the generator loss function also depends on the targets of the training examples, thus facilitating learning. We demonstrate on three computer vision tasks that this approach can significantly outperform CGANs achieving comparable or superior results to task-specific solutions and results in stable training. Importantly, this is a general approach that does not require the use of task-specific loss functions",
    "checked": true,
    "id": "95549a8692f734b978f1177c76242e074d52e67a",
    "semantic_title": "matching adversarial networks",
    "citation_count": 22,
    "authors": [
      "Gellért Máttyus",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sznaier_SoS-RSC_A_Sum-of-Squares_CVPR_2018_paper.html": {
    "title": "SoS-RSC: A Sum-of-Squares Polynomial Approach to Robustifying Subspace Clustering Algorithms",
    "volume": "main",
    "abstract": "This paper addresses the problem of subspace clustering in the presence of outliers. Typically, this scenario is handled through a regularized optimization, whose computational complexity scales polynomially with the size of the data. Further, the regularization terms need to be manually tuned to achieve optimal performance. To circumvent these difficulties, in this paper we propose an outlier removal algorithm based on evaluating a suitable sum-ofsquares polynomial, computed directly from the data. This algorithm only requires performing two singular value decompositions of fixed size, and provides certificates on the probability of misclassifying outliers as inliers",
    "checked": true,
    "id": "f58ddc47e485bb1fe70e150a7e9d7c2ea1f83d8c",
    "semantic_title": "sos-rsc: a sum-of-squares polynomial approach to robustifying subspace clustering algorithms",
    "citation_count": 10,
    "authors": [
      "Mario Sznaier",
      "Octavia Camps"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Resource_Aware_Person_CVPR_2018_paper.html": {
    "title": "Resource Aware Person Re-Identification Across Multiple Resolutions",
    "volume": "main",
    "abstract": "Not all people are equally easy to identify: color statistics might be enough for some cases while others might require careful reasoning about high- and low-level details. However, prevailing person re-identification(re-ID) methods use one-size-fits-all high-level embeddings from deep convolutional networks for all cases. This might limit their accuracy on difficult examples or makes them needlessly expensive for the easy ones. To remedy this, we present a new person re-ID model that combines effective embeddings built on multiple convolutional network layers, trained with deep-supervision. On traditional re-ID benchmarks, our method improves substantially over the previous state-of-the-art results on all five datasets that we evaluate on. We then propose two new formulations of the person re-ID problem under resource-constraints, and show how our model can be used to effectively trade off accuracy and computation in the presence of resource constraints",
    "checked": true,
    "id": "c27ecf5b8943a643584424fab38a902f344165e0",
    "semantic_title": "resource aware person re-identification across multiple resolutions",
    "citation_count": 230,
    "authors": [
      "Yan Wang",
      "Lequn Wang",
      "Yurong You",
      "Xu Zou",
      "Vincent Chen",
      "Serena Li",
      "Gao Huang",
      "Bharath Hariharan",
      "Kilian Q. Weinberger"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Learning_and_Using_CVPR_2018_paper.html": {
    "title": "Learning and Using the Arrow of Time",
    "volume": "main",
    "abstract": "We seek to understand the arrow of time in videos -- what makes videos look like they are playing forwards or backwards? Can we visualize the cues? Can the arrow of time be a supervisory signal useful for activity analysis? To this end, we build three large-scale video datasets and apply a learning-based approach to these tasks. To learn the arrow of time efficiently and reliably, we design a ConvNet suitable for extended temporal footprints and for class activation visualization, and study the effect of artificial cues, such as cinematographic conventions, on learning. Our trained model achieves state-of-the-art performance on large-scale real-world video datasets. Through cluster analysis and localization of important regions for the prediction, we examine learned visual cues that are consistent among many samples and show when and where they occur. Lastly, we use the trained ConvNet for two applications: self-supervision for action recognition, and video forensics -- determining whether Hollywood film clips have been deliberately reversed in time, often used as special effects",
    "checked": true,
    "id": "9baf01eb53abda6a169110477f2c7a3492559368",
    "semantic_title": "learning and using the arrow of time",
    "citation_count": 314,
    "authors": [
      "Donglai Wei",
      "Joseph J. Lim",
      "Andrew Zisserman",
      "William T. Freeman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Neural_Style_Transfer_CVPR_2018_paper.html": {
    "title": "Neural Style Transfer via Meta Networks",
    "volume": "main",
    "abstract": "In this paper we propose a noval method to generate the specified network parameters through one feed-forward propagation in the meta networks for neural style transfer. Recent works on style transfer typically need to train image transformation networks for every new style, and the style is encoded in the network parameters by enormous iterations of stochastic gradient descent, which lacks the generalization ability to new style in the inference stage. To tackle these issues, we build a meta network which takes in the style image and generates a corresponding image transformation network directly. Compared with optimization-based methods for every style, our meta networks can handle an arbitrary new style within 19 milliseconds on one modern GPU card. The fast image transformation network generated by our meta network is only 449 KB, which is capable of real-time running on a mobile device. We also investigate the manifold of the style transfer networks by operating the hidden features from meta networks. Experiments have well validated the effectiveness of our method. Code and trained models will be released",
    "checked": true,
    "id": "bd68e6f668670f7651345670961a5f8eddeddd97",
    "semantic_title": "neural style transfer via meta networks",
    "citation_count": 110,
    "authors": [
      "Falong Shen",
      "Shuicheng Yan",
      "Gang Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Marsden_People_Penguins_and_CVPR_2018_paper.html": {
    "title": "People, Penguins and Petri Dishes: Adapting Object Counting Models to New Visual Domains and Object Types Without Forgetting",
    "volume": "main",
    "abstract": "In this paper we propose a technique to adapt a convolutional neural network (CNN) based object counter to additional visual domains and object types while still preserving the original counting function. Domain-specific normalisation and scaling operators are trained to allow the model to adjust to the statistical distributions of the various visual domains. The developed adaptation technique is used to produce a singular patch-based counting regressor capable of counting various object types including people, vehicles, cell nuclei and wildlife. As part of this study a challenging new cell counting dataset in the context of tissue culture and patient diagnosis is constructed. This new collection, referred to as the Dublin Cell Counting (DCC) dataset, is the first of its kind to be made available to the wider computer vision community. State-of-the-art object counting performance is achieved in both the Shanghaitech (parts A and B) and Penguins datasets while competitive performance is observed on the TRANCOS and Modified Bone Marrow (MBM) datasets, all using a shared counting model",
    "checked": true,
    "id": "139f8dea90ed4b535aea4ae03211ca948fe34acd",
    "semantic_title": "people, penguins and petri dishes: adapting object counting models to new visual domains and object types without forgetting",
    "citation_count": 72,
    "authors": [
      "Mark Marsden",
      "Kevin McGuinness",
      "Suzanne Little",
      "Ciara E. Keogh",
      "Noel E. O'Connor"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mullapudi_HydraNets_Specialized_Dynamic_CVPR_2018_paper.html": {
    "title": "HydraNets: Specialized Dynamic Architectures for Efficient Inference",
    "volume": "main",
    "abstract": "There is growing interest in improving the design of deep network architectures to be both accurate and low cost. This paper explores semantic specialization as a mechanism for improving the computational efficiency (accuracy-per-unit-cost) of inference in the context of image classification. Specifically, we propose a network architecture template called HydraNet, which enables state-of-the-art architectures for image classification to be transformed into dynamic architectures which exploit conditional execution for efficient inference. HydraNets are wide networks containing distinct components specialized to compute features for visually similar classes, but they retain efficiency by dynamically selecting only a small number of components to evaluate for any one input image. This design is made possible by a soft gating mechanism that encourages component specialization during training and accurately performs component selection during inference. We evaluate the HydraNet approach on both the CIFAR-100 and ImageNet classification tasks. On CIFAR, applying the HydraNet template to the ResNet and DenseNet family of models reduces inference cost by 2-4x while retaining the accuracy of the baseline architectures. On ImageNet, applying the HydraNet template improves accuracy up to 2.5% when compared to an efficient baseline architecture with similar inference cost",
    "checked": true,
    "id": "c421337717edf24ee0d40fc5d86e71488ca4b713",
    "semantic_title": "hydranets: specialized dynamic architectures for efficient inference",
    "citation_count": 117,
    "authors": [
      "Ravi Teja Mullapudi",
      "William R. Mark",
      "Noam Shazeer",
      "Kayvon Fatahalian"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_SketchMate_Deep_Hashing_CVPR_2018_paper.html": {
    "title": "SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval",
    "volume": "main",
    "abstract": "We propose a deep hashing framework for sketch retrieval that, for the first time, works on a multi-million scale human sketch dataset.Leveraging on this large dataset, we explore a few sketch-specific traits that were otherwise under-studied in prior literature. Instead of following the conventional sketch recognition task, we introduce the novel problem of sketch hashing retrieval which is not only more challenging, but also offers a better testbed for large-scale sketch analysis, since: (i) more fine-grained sketch feature learning is required to accommodate the large variations in style and abstraction, and (ii) a compact binary code needs to be learned at the same time to enable efficient retrieval.Key to our network design is the embedding of unique characteristics of human sketch, where (i) a two-branch CNN-RNN architecture is adapted to explore the temporal ordering of strokes, and (ii) a novel hashing loss is specifically designed to accommodate both the temporal and abstract traits of sketches. By working with a 3.8M sketch dataset,we show that state-of-the-art hashing models specifically engineered for static images fail to perform well on temporal sketch data. Our network on the other hand not only offers the best retrieval performance on various code sizes, but also yields the best generalization performance under a zero-shot setting and when re-purposed for sketch recognition.Such superior performances effectively demonstrate the benefit of our sketch-specific design",
    "checked": true,
    "id": "f7e3405e135e7e3ea4128014a16e2e9f810a0cb3",
    "semantic_title": "sketchmate: deep hashing for million-scale human sketch retrieval",
    "citation_count": 103,
    "authors": [
      "Peng Xu",
      "Yongye Huang",
      "Tongtong Yuan",
      "Kaiyue Pang",
      "Yi-Zhe Song",
      "Tao Xiang",
      "Timothy M. Hospedales",
      "Zhanyu Ma",
      "Jun Guo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Russo_From_Source_to_CVPR_2018_paper.html": {
    "title": "From Source to Target and Back: Symmetric Bi-Directional Adaptive GAN",
    "volume": "main",
    "abstract": "The effectiveness of GANs in producing images according to a specific visual domain has shown potential in unsupervised domain adaptation. Source labeled images have been modified to mimic target samples for training classifiers in the target domain, and inverse mappings from the target to the source domain have also been evaluated, without new image generation. In this paper we aim at getting the best of both worlds by introducing a symmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. We define a new class consistency loss that aligns the generators in the two directions, imposing to preserve the class identity of an image passing through both domain mappings. A detailed analysis of the reconstructed images, a thorough ablation study and extensive experiments on six different settings confirm the power of our approach",
    "checked": true,
    "id": "9ac9eca9f9ecde8f99573d3c51463bfef81b8014",
    "semantic_title": "from source to target and back: symmetric bi-directional adaptive gan",
    "citation_count": 232,
    "authors": [
      "Paolo Russo",
      "Fabio M. Carlucci",
      "Tatiana Tommasi",
      "Barbara Caputo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lezama_OLE_Orthogonal_Low-Rank_CVPR_2018_paper.html": {
    "title": "OLÉ: Orthogonal Low-Rank Embedding - A Plug and Play Geometric Loss for Deep Learning",
    "volume": "main",
    "abstract": "Deep neural networks trained using a softmax layer at the top and the cross-entropy loss are ubiquitous tools for image classification. Yet, this does not naturally enforce intra-class similarity nor inter-class margin of the learned deep representations. To simultaneously achieve these two goals, different solutions have been proposed in the literature, such as the pairwise or triplet losses. However, these carry the extra task of selecting pairs or triplets, and the extra computational burden of computing and learning for many combinations of them. In this paper, we propose a plug-and-play loss term for deep networks that explicitly reduces intra-class variance and enforces inter-class margin simultaneously, in a simple and elegant geometric manner. For each class, the deep features are collapsed into a learned linear subspace, or union of them, and inter-class subspaces are pushed to be as orthogonal as possible. Our proposed Orthogonal Low-rank Embedding (OLE) does not require carefully crafting pairs or triplets of samples for training, and works standalone as a classification loss, being the first reported deep metric learning framework of its kind. Because of the improved margin between features of different classes, the resulting deep networks generalize better, are more discriminative, and more robust. We demonstrate improved classification performance in general object recognition, plugging the proposed loss term into existing off-the-shelf architectures. In particular, we show the advantage of the proposed loss in the small data/model scenario, and we significantly advance the state-of-the-art on the Stanford STL-10 benchmark",
    "checked": true,
    "id": "d66d93fafe61e878b8da5a8d0ecee442d44b5be0",
    "semantic_title": "ole: orthogonal low-rank embedding, a plug and play geometric loss for deep learning",
    "citation_count": 60,
    "authors": [
      "José Lezama",
      "Qiang Qiu",
      "Pablo Musé",
      "Guillermo Sapiro"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Rebuffi_Efficient_Parametrization_of_CVPR_2018_paper.html": {
    "title": "Efficient Parametrization of Multi-Domain Deep Neural Networks",
    "volume": "main",
    "abstract": "A practical limitation of deep neural networks is their high degree of specialization to a single task and visual domain. In complex applications such as mobile platforms, this requires juggling several large models with detrimental effect on speed and battery life. Recently, inspired by the successes of transfer learning, several authors have proposed to learn instead universal, fixed feature extractors that, used as the first stage of any deep network, work well for all tasks and domains simultaneously. Nevertheless, such universal features are still somewhat inferior to specialized networks. To overcome this limitation, in this paper we propose to consider instead universal parametric families of neural networks, which still contain specialized problem-specific models, but that differ only by a small number of parameters. We study different designs for such parametrizations, including series and parallel residual adapters, regularization strategies, and parameter allocations, and empirically identify the ones that yield the highest compression. We show that, in order to maximize performance, it is necessary to adapt both shallow and deep layers of a deep network, but the required changes are very small. We also show that these universal parametrization are very effective for transfer learning, where they outperform traditional fine-tuning techniques",
    "checked": true,
    "id": "4081de7e0f94e7e0d7b645c298d7768698d05774",
    "semantic_title": "efficient parametrization of multi-domain deep neural networks",
    "citation_count": 308,
    "authors": [
      "Sylvestre-Alvise Rebuffi",
      "Hakan Bilen",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_Deep_Density_Clustering_CVPR_2018_paper.html": {
    "title": "Deep Density Clustering of Unconstrained Faces",
    "volume": "main",
    "abstract": "In this paper, we consider the problem of grouping a collection of unconstrained face images in which the number of subjects is not known. We propose an unsupervised clustering algorithm called Deep Density Clustering (DDC) which is based on measuring density affinities between local neighborhoods in the feature space. By learning the minimal covering sphere for each neighborhood, information about the underlying structure is encapsulated. The encapsulation is also capable of locating high-density region of the neighborhood, which aids in measuring the neighborhood similarity. We theoretically show that the encapsulation asymptotically converges to a Parzen window density estimator. Our experiments show that DDC is a superior candidate for clustering unconstrained faces when the number of subjects is unknown. Unlike conventional linkage and density-based methods that are sensitive to the selection operating points, DDC attains more consistent and improved performance. Furthermore, the density-aware property reduces the difficulty in finding appropriate operating points",
    "checked": true,
    "id": "b35ff9985aaee9371588330bcef0dfc88d1401d7",
    "semantic_title": "deep density clustering of unconstrained faces",
    "citation_count": 59,
    "authors": [
      "Wei-An Lin",
      "Jun-Cheng Chen",
      "Carlos D. Castillo",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Amayo_Geometric_Multi-Model_Fitting_CVPR_2018_paper.html": {
    "title": "Geometric Multi-Model Fitting With a Convex Relaxation Algorithm",
    "volume": "main",
    "abstract": "We propose a novel method for fitting multiple geometric models to multi-structural data via convex relaxation. Unlike greedy methods - which maximise the number of inliers - our approach efficiently searches for a soft assignment of points to geometric models by minimising the energy of the overall assignment. The inherently parallel nature of our approach, as compared to the sequential approach found in state-of-the-art energy minimisation techniques, allows for the elegant treatment of a scaling factor that occurs as the number of features in the data increases. This results in an energy minimisation that, per iteration, is as much as two orders of magnitude faster on comparable architectures thus bringing real-time, robust performance to a wider set of geometric multi-model fitting problems. We demonstrate the versatility of our approach on two canonical problems in estimating structure from images: plane extraction from RGB-D images and homography estimation from pairs of images. Our approach seamlessly adapts to the different metrics brought forth in these distinct problems. In both cases, we report results on publicly available data-sets that in most instances outperform the state-of-the-art while simultaneously presenting run-times that are as much as an order of magnitude faster",
    "checked": true,
    "id": "3b128d61a50a112247e8e6bf68a59ecf00a5d4c4",
    "semantic_title": "geometric multi-model fitting with a convex relaxation algorithm",
    "citation_count": 32,
    "authors": [
      "Paul Amayo",
      "Pedro Piniés",
      "Lina M. Paz",
      "Paul Newman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ikami_Fast_and_Robust_CVPR_2018_paper.html": {
    "title": "Fast and Robust Estimation for Unit-Norm Constrained Linear Fitting Problems",
    "volume": "main",
    "abstract": "M-estimator using iteratively reweighted least squares (IRLS) is one of the best-known methods for robust estimation. However, IRLS is ineffective for robust unit-norm constrained linear fitting (UCLF) problems, such as fundamental matrix estimation because of a poor initial solution. We overcome this problem by developing a novel objective function and its optimization, named iteratively reweighted eigenvalues minimization (IREM). IREM is guaranteed to decrease the objective function and achieves fast convergence and high robustness. In robust fundamental matrix estimation, IREM performs approximately 5-500 times faster than random sampling consensus (RANSAC) while preserving comparable or superior robustness",
    "checked": true,
    "id": "10367a8335d89b47e5e220a8c9181d4d5edf914d",
    "semantic_title": "fast and robust estimation for unit-norm constrained linear fitting problems",
    "citation_count": 9,
    "authors": [
      "Daiki Ikami",
      "Toshihiko Yamasaki",
      "Kiyoharu Aizawa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Importance_Weighted_Adversarial_CVPR_2018_paper.html": {
    "title": "Importance Weighted Adversarial Nets for Partial Domain Adaptation",
    "volume": "main",
    "abstract": "This paper proposes an importance weighted adversarial nets-based method for unsupervised domain adaptation, specific for partial domain adaptation where the target domain has less number of classes compared to the source domain. Previous domain adaptation methods generally assume the identical label spaces, such that reducing the distribution divergence leads to feasible knowledge transfer. However, such an assumption is no longer valid in a more realistic scenario that requires adaptation from a larger and more diverse source domain to a smaller target domain with less number of classes. This paper extends the adversarial nets-based domain adaptation and proposes a novel adversarial nets-based partial domain adaptation method to identify the source samples that are potentially from the outlier classes and, at the same time, reduce the shift of shared classes between domains",
    "checked": true,
    "id": "7b24d0001163440adfd26b9f7e91a8f32d51e81f",
    "semantic_title": "importance weighted adversarial nets for partial domain adaptation",
    "citation_count": 357,
    "authors": [
      "Jing Zhang",
      "Zewei Ding",
      "Wanqing Li",
      "Philip Ogunbona"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lui_Efficient_Subpixel_Refinement_CVPR_2018_paper.html": {
    "title": "Efficient Subpixel Refinement With Symbolic Linear Predictors",
    "volume": "main",
    "abstract": "We present an efficient subpixel refinement method using a learning-based approach called Linear Predictors. Firstly, we present a novel technique, called Symbolic Linear Predictors, which makes the learning step efficient for subpixel refinement. This makes our approach feasible for online applications without compromising accuracy, while taking advantage of the run-time efficiency of learning based approaches. Secondly, we show how Linear Predictors can be used to predict the expected alignment error, allowing us to use only the best keypoints in resource constrained applications. We show the efficiency and accuracy of our method through extensive experiments",
    "checked": true,
    "id": "1a1ffc3df319ae61bff50b879998bd30141851da",
    "semantic_title": "efficient subpixel refinement with symbolic linear predictors",
    "citation_count": 1,
    "authors": [
      "Vincent Lui",
      "Jonathon Geeves",
      "Winston Yii",
      "Tom Drummond"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tao_Scale-Recurrent_Network_for_CVPR_2018_paper.html": {
    "title": "Scale-Recurrent Network for Deep Image Deblurring",
    "volume": "main",
    "abstract": "In single image deblurring, the ``coarse-to-fine'' scheme, i.e. gradually restoring the sharp image on different resolutions in a pyramid, is very successful in both traditional optimization-based methods and recent neural-network-based approaches. In this paper, we investigate this strategy and propose a Scale-recurrent Network (SRN-DeblurNet) for this deblurring task. Compared with the many recent learning-based approaches, it has a simpler network structure, a smaller number of parameters and is easier to train. We evaluate our method on large-scale deblurring datasets with complex motion. Results show that our method can produce better quality results than state-of-the-arts, both quantitatively and qualitatively",
    "checked": true,
    "id": "cd9ce1db5ee59d582a3e95e32094fd2a1eec7410",
    "semantic_title": "scale-recurrent network for deep image deblurring",
    "citation_count": 916,
    "authors": [
      "Xin Tao",
      "Hongyun Gao",
      "Xiaoyong Shen",
      "Jue Wang",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kupyn_DeblurGAN_Blind_Motion_CVPR_2018_paper.html": {
    "title": "DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks",
    "volume": "main",
    "abstract": "We present DeblurGAN, an end-to-end learned method for motion deblurring. The learning is based on a conditional GAN and the content loss . DeblurGAN achieves state-of-the art performance both in the structural similarity measure and visual appearance. The quality of the deblurring model is also evaluated in a novel way on a real-world problem -- object detection on (de-)blurred images. The method is 5 times faster than the closest competitor -- DeepDeblur. We also introduce a novel method for generating synthetic motion blurred images from sharp ones, allowing realistic dataset augmentation. The model, code and the dataset are available at https://github.com/KupynOrest/DeblurGAN",
    "checked": true,
    "id": "9184da852d02f1a15bc465d9cceae613b0a03e51",
    "semantic_title": "deblurgan: blind motion deblurring using conditional adversarial networks",
    "citation_count": 1240,
    "authors": [
      "Orest Kupyn",
      "Volodymyr Budzan",
      "Mykola Mykhailych",
      "Dmytro Mishkin",
      "Jiří Matas"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_A2-RL_Aesthetics_Aware_CVPR_2018_paper.html": {
    "title": "A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping",
    "volume": "main",
    "abstract": "Image cropping aims at improving the aesthetic quality of images by adjusting their composition. Most weakly supervised cropping methods (without bounding box supervision) rely on the sliding window mechanism. The sliding window mechanism requires fixed aspect ratios and limits the cropping region with arbitrary size. Moreover, the sliding window method usually produces tens of thousands of windows on the input image which is very time-consuming. Motivated by these challenges, we firstly formulate the aesthetic image cropping as a sequential decision-making process and propose a weakly supervised Aesthetics Aware Reinforcement Learning (A2-RL) framework to address this problem. Particularly, the proposed method develops an aesthetics aware reward function which especially benefits image cropping. Similar to human's decision making, we use a comprehensive state representation including both the current observation and the historical experience. We train the agent using the actor-critic architecture in an end-to-end manner. The agent is evaluated on several popular unseen cropping datasets. Experiment results show that our method achieves the state-of-the-art performance with much fewer candidate windows and much less time compared with previous weakly supervised methods",
    "checked": true,
    "id": "b5ac3c258bcadc0766e29638aa95e14058f6adf0",
    "semantic_title": "a2-rl: aesthetics aware reinforcement learning for image cropping",
    "citation_count": 93,
    "authors": [
      "Debang Li",
      "Huikai Wu",
      "Junge Zhang",
      "Kaiqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Single_Image_Dehazing_CVPR_2018_paper.html": {
    "title": "Single Image Dehazing via Conditional Generative Adversarial Network",
    "volume": "main",
    "abstract": "In this paper, we present an algorithm to directly restore a clear image from a hazy image. This problem is highly ill-posed and most existing algorithms often use hand-crafted features, e.g., dark channel, color disparity, maximum contrast, to estimate transmission maps and then atmospheric lights. In contrast, we solve this problem based on a conditional generative adversarial network (cGAN), where the clear image is estimated by an end-to-end trainable neural network. Different from the generative network in basic cGAN, we propose an encoder and decoder architecture so that it can generate better results. To generate realistic clear images, we further modify the basic cGAN formulation by introducing the VGG features and a L_1-regularized gradient prior. We also synthesize a hazy dataset including indoor and outdoor scenes to train and evaluate the proposed algorithm. Extensive experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on both synthetic dataset and real world hazy images",
    "checked": true,
    "id": "6a77a59257442da152f4e90729f483018659b7ef",
    "semantic_title": "single image dehazing via conditional generative adversarial network",
    "citation_count": 317,
    "authors": [
      "Runde Li",
      "Jinshan Pan",
      "Zechao Li",
      "Jinhui Tang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Galdran_On_the_Duality_CVPR_2018_paper.html": {
    "title": "On the Duality Between Retinex and Image Dehazing",
    "volume": "main",
    "abstract": "Image dehazing deals with the removal of undesired loss of visibility in outdoor images due to the presence of fog. Retinex is a color vision model mimicking the ability of the Human Visual System to robustly discount varying illuminations when observing a scene under different spectral lighting conditions. Retinex has been widely explored in the computer vision literature for image enhancement and other related tasks. While these two problems are apparently unrelated, the goal of this work is to show that they can be connected by a simple linear relationship. Specifically, most Retinex-based algorithms have the characteristic feature of always increasing image brightness, which turns them into ideal candidates for effective image dehazing by directly applying Retinex to a hazy image whose intensities have been inverted. In this paper, we give theoretical proof that Retinex on inverted intensities is a solution to the image dehazing problem. Comprehensive qualitative and quantitative results indicate that several classical and modern implementations of Retinex can be transformed into competing image dehazing algorithms performing on pair with more complex fog removal methods, and can overcome some of the main challenges associated with this problem",
    "checked": true,
    "id": "5922af77cf107120043fbb1cf311bf19ab3dc72e",
    "semantic_title": "on the duality between retinex and image dehazing",
    "citation_count": 87,
    "authors": [
      "Adrian Galdran",
      "Aitor Alvarez-Gila",
      "Alessandro Bria",
      "Javier Vazquez-Corral",
      "Marcelo Bertalmío"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gu_Arbitrary_Style_Transfer_CVPR_2018_paper.html": {
    "title": "Arbitrary Style Transfer With Deep Feature Reshuffle",
    "volume": "main",
    "abstract": "This paper introduces a novel method by reshuffling deep features (i.e., permuting the spacial locations of a feature map) of the style image for arbitrary style transfer. We theoretically prove that our new style loss based on reshuffle connects both global and local style losses respectively used by most parametric and non-parametric neural style transfer methods. This simple idea can effectively address the challenging issues in existing style transfer methods. On one hand, it can avoid distortions in local style patterns, and allow semantic-level transfer, compared with neural parametric methods. On the other hand, it can preserve globally similar appearance to the style image, and avoid wash-out artifacts, compared with neural non-parametric methods. Based on the proposed loss, we also present a progressive feature-domain optimization approach. The experiments show that our method is widely applicable to various styles, and produces better quality than existing methods",
    "checked": true,
    "id": "42ce7ca66b0471c93fdb5863973abc494df540bb",
    "semantic_title": "arbitrary style transfer with deep feature reshuffle",
    "citation_count": 165,
    "authors": [
      "Shuyang Gu",
      "Congliang Chen",
      "Jing Liao",
      "Lu Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Nonlocal_Low-Rank_Tensor_CVPR_2018_paper.html": {
    "title": "Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration",
    "volume": "main",
    "abstract": "Low-rank signal modeling has been widely leveraged to capture non-local correlation in image processing applications. We propose a new method that employs low-rank tensor factor analysis for tensors generated by grouped image patches. The low-rank tensors are fed into the alternative direction multiplier method (ADMM) to further improve image reconstruction. The motivating application is compressive sensing (CS), and a deep convolutional architecture is adopted to approximate the expensive matrix inversion in CS applications. An iterative algorithm based on this low-rank tensor factorization strategy, called NLR-TFA, is presented in detail. Experimental results on noiseless and noisy CS measurements demonstrate the superiority of the proposed approach, especially at low CS sampling rates",
    "checked": true,
    "id": "b4f1937d4428064904b7e384770d61885023f053",
    "semantic_title": "nonlocal low-rank tensor factor analysis for image restoration",
    "citation_count": 27,
    "authors": [
      "Xinyuan Zhang",
      "Xin Yuan",
      "Lawrence Carin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper.html": {
    "title": "Avatar-Net: Multi-Scale Zero-Shot Style Transfer by Feature Decoration",
    "volume": "main",
    "abstract": "Zero-shot artistic style transfer is an important image synthesis problem aiming at transferring arbitrary style into content images. However, the trade-off between the generalization and efficiency in existing methods impedes a high quality zero-shot style transfer in real-time. In this paper, we resolve this dilemma and propose an efficient yet effective Avatar-Net that enables visually plausible multi-scale transfer for arbitrary style. The key ingredient of our method is a style decorator that makes up the content features by semantically aligned style features from an arbitrary style image, which does not only holistically match their feature distributions but also preserve detailed style patterns in the decorated features. By embedding this module into an image reconstruction network that fuses multi- scale style abstractions, the Avatar-Net renders multi-scale stylization for any style image in one feed-forward pass. We demonstrate the state-of-the-art effectiveness and efficiency of the proposed method in generating high-quality stylized images, with a series of successive applications include multiple style integration, video stylization and etc",
    "checked": true,
    "id": "e0df8fe29381154c06b6b488cf32b6f6627b394f",
    "semantic_title": "avatar-net: multi-scale zero-shot style transfer by feature decoration",
    "citation_count": 259,
    "authors": [
      "Lu Sheng",
      "Ziyi Lin",
      "Jing Shao",
      "Xiaogang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yokota_Missing_Slice_Recovery_CVPR_2018_paper.html": {
    "title": "Missing Slice Recovery for Tensors Using a Low-Rank Model in Embedded Space",
    "volume": "main",
    "abstract": "Let us consider a case where all of the elements in some continuous slices are missing in tensor data. In this case, the nuclear-norm and total variation regularization methods usually fail to recover the missing elements. The key problem is capturing some delay/shift-invariant structure. In this study, we consider a low-rank model in an embedded space of a tensor. For this purpose, we extend a delay embedding for a time series to a ``multi-way delay-embedding transform'' for a tensor, which takes a given incomplete tensor as the input and outputs a higher-order incomplete Hankel tensor. The higher-order tensor is then recovered by Tucker-based low-rank tensor factorization. Finally, an estimated tensor can be obtained by using the inverse multi-way delay embedding transform of the recovered higher-order tensor. Our experiments showed that the proposed method successfully recovered missing slices for some color images and functional magnetic resonance images",
    "checked": true,
    "id": "0eeec6e0948983a716cbd6e4cdebcc4820328fcf",
    "semantic_title": "missing slice recovery for tensors using a low-rank model in embedded space",
    "citation_count": 80,
    "authors": [
      "Tatsuya Yokota",
      "Burak Erem",
      "Seyhmus Guler",
      "Simon K. Warfield",
      "Hidekata Hontani"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Semantic_Face_CVPR_2018_paper.html": {
    "title": "Deep Semantic Face Deblurring",
    "volume": "main",
    "abstract": "In this paper, we present an effective and efficient face deblurring algorithm by exploiting semantic cues via deep convolutional neural networks (CNNs). As face images are highly structured and share several key semantic components (e.g., eyes and mouths), the semantic information of a face provides a strong prior for restoration. As such, we propose to incorporate global semantic priors as input and impose local structure losses to regularize the output within a multi-scale deep CNN. We train the network with perceptual and adversarial losses to generate photo-realistic results and develop an incremental training strategy to handle random blur kernels in the wild. Quantitative and qualitative evaluations demonstrate that the proposed face deblurring algorithm restores sharp images with more facial details and performs favorably against state-of-the-art methods in terms of restoration quality, face recognition and execution speed",
    "checked": true,
    "id": "8191f76d9df5c59186859c4834026c8e0fb42922",
    "semantic_title": "deep semantic face deblurring",
    "citation_count": 173,
    "authors": [
      "Ziyi Shen",
      "Wei-Sheng Lai",
      "Tingfa Xu",
      "Jan Kautz",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper.html": {
    "title": "GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "In this paper, we propose a GraphBit method to learn deep binary descriptors in a directed acyclic graph unsupervisedly, representing bitwise interactions as edges between the nodes of bits. Conventional binary representation learning methods enforce each element to be binarized into zero or one. However, there are elements lying in the boundary which suffer from doubtful binarization as ``ambiguous bits''. Ambiguous bits fail to collect effective information for confident binarization, which are unreliable and sensitive to noise. We argue that there are implicit inner relationships between bits in binary descriptors, where the related bits can provide extra instruction as prior knowledge for ambiguity elimination. Specifically, we design a deep reinforcement learning model to learn the structure of the graph for bitwise interaction mining, reducing the uncertainty of binary codes by maximizing the mutual information with inputs and related bits, so that the ambiguous bits receive additional instruction from the graph for confident binarization. Due to the reliability of the proposed binary codes with bitwise interaction, we obtain an average improvement of 9.64%, 8.84% and 3.22% on the CIFAR-10, Brown and HPatches datasets respectively compared with the state-of-the-art unsupervised binary descriptors",
    "checked": true,
    "id": "19075e854e29edce89a1cb39167eb1af06b86fe5",
    "semantic_title": "graphbit: bitwise interaction mining via deep reinforcement learning",
    "citation_count": 25,
    "authors": [
      "Yueqi Duan",
      "Ziwei Wang",
      "Jiwen Lu",
      "Xudong Lin",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Recurrent_Saliency_Transformation_CVPR_2018_paper.html": {
    "title": "Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation",
    "volume": "main",
    "abstract": "We aim at segmenting small organs (e.g., the pancreas) from abdominal CT scans. As the target often occupies a relatively small region in the input image, deep neural networks can be easily confused by the complex and variable background. To alleviate this, researchers proposed a coarse-to-fine approach, which used prediction from the first (coarse) stage to indicate a smaller input region for the second (fine) stage. Despite its effectiveness, this algorithm dealt with two stages individually, which lacked optimizing a global energy function, and limited its ability to incorporate multi-stage visual cues. Missing contextual information led to unsatisfying convergence in iterations, and that the fine stage sometimes produced even lower segmentation accuracy than the coarse stage. This paper presents a Recurrent Saliency Transformation Network. The key innovation is a saliency transformation module, which repeatedly converts the segmentation probability map from the previous iteration as spatial weights and applies these weights to the current iteration. This brings us two-fold benefits. In training, it allows joint optimization over the deep networks dealing with different input scales. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments in the NIH pancreas segmentation dataset demonstrate the state-of-the-art accuracy, which outperforms the previous best by an average of over 2%. Much higher accuracies are also reported on several small organs in a larger dataset collected by ourselves. In addition, our approach enjoys better convergence properties, making it more efficient and reliable in practice",
    "checked": true,
    "id": "0bffe8c90fda09f7181a97e1b1b705aab483755b",
    "semantic_title": "recurrent saliency transformation network: incorporating multi-stage visual cues for small organ segmentation",
    "citation_count": 189,
    "authors": [
      "Qihang Yu",
      "Lingxi Xie",
      "Yan Wang",
      "Yuyin Zhou",
      "Elliot K. Fishman",
      "Alan L. Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Thoracic_Disease_Identification_CVPR_2018_paper.html": {
    "title": "Thoracic Disease Identification and Localization With Limited Supervision",
    "volume": "main",
    "abstract": "Accurate identification and localization of abnormalities from radiology images play an integral part in clinical diagnosis and treatment planning. Building a highly accurate prediction model for these tasks usually requires a large number of images manually annotated with labels and finding sites of abnormalities. In reality, however, such annotated data are expensive to acquire, especially the ones with location annotations. We need methods that can work well with only a small amount of location annotations. To address this challenge, we present a unified approach that simultaneously performs disease identification and localization through the same underlying model for all images.We demonstrate that our approach can effectively leverage both class information as well as limited location annotation, and significantly outperforms the comparative reference baseline in both classification and localization tasks",
    "checked": true,
    "id": "8bdfb96e2865ad0d1e263f04fc0dab134052d64e",
    "semantic_title": "thoracic disease identification and localization with limited supervision",
    "citation_count": 306,
    "authors": [
      "Zhe Li",
      "Chong Wang",
      "Mei Han",
      "Yuan Xue",
      "Wei Wei",
      "Li-Jia Li",
      "Li Fei-Fei"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Quantization_of_Fully_CVPR_2018_paper.html": {
    "title": "Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation",
    "volume": "main",
    "abstract": "With pervasive applications of medical imaging in healthcare, biomedical image segmentation plays a central role in quantitative analysis, clinical diagnosis, and medical intervention. Since manual annotation suffers limited reproducibility, arduous efforts, and excessive time, automatic segmentation is desired to process increasingly larger scale histopathological data. Recently, deep neural networks (DNNs), particularly fully convolutional networks (FCNs), have been widely applied to biomedical image segmentation, attaining much improved performance. At the same time, quantization of DNNs has become an active research topic, which aims to represent weights with less memory (precision) to considerably reduce memory and computation requirements of DNNs while maintaining acceptable accuracy. In this paper, we apply quantization techniques to FCNs for accurate biomedical image segmentation. Unlike existing literature on quantization which primarily targets memory and computation complexity reduction, we apply quantization as a method to reduce overfitting in FCNs for better accuracy. Specifically, we focus on a state-of-the-art segmentation framework, suggestive annotation [22], which judiciously extracts representative annotation samples from the original training dataset, obtaining an effective small-sized balanced training dataset. We develop two new quantization processes for this framework: (1) suggestive annotation with quantization for highly representative training samples, and (2) network training with quantization for high accuracy. Extensive experiments on the MICCAI Gland dataset show that both quantization processes can improve the segmentation performance, and our proposed method exceeds the current state-of-the-art performance by up to 1%. In addition, our method have a reduction of up to 6.4x on memory usage",
    "checked": true,
    "id": "3fce10437f102ff2ba241e38011a6d52377a353e",
    "semantic_title": "quantization of fully convolutional networks for accurate biomedical image segmentation",
    "citation_count": 82,
    "authors": [
      "Xiaowei Xu",
      "Qing Lu",
      "Lin Yang",
      "Sharon Hu",
      "Danny Chen",
      "Yu Hu",
      "Yiyu Shi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Baumgartner_Visual_Feature_Attribution_CVPR_2018_paper.html": {
    "title": "Visual Feature Attribution Using Wasserstein GANs",
    "volume": "main",
    "abstract": "Attributing the pixels of an input image to a certain category is an important and well-studied problem in computer vision, with applications ranging from weakly supervised localisation to understanding hidden effects in the data. In recent years, approaches based on interpreting a previously trained neural network classifier have become the de facto state-of-the-art and are commonly used on medical as well as natural image datasets. In this paper, we discuss a limitation of these approaches which may lead to only a subset of the category specific features being detected. To address this problem we develop a novel feature attribution technique based on Wasserstein Generative Adversarial Networks (WGAN), which does not suffer from this limitation. We show that our proposed method performs substantially better than the state-of-the-art for visual attribution on a synthetic dataset and on real 3D neuroimaging data from patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD). For AD patients the method produces compellingly realistic disease effect maps which are very close to the observed effects",
    "checked": true,
    "id": "a1da80cca297f7f47a36dd2d68a3f16cbd1f6c36",
    "semantic_title": "visual feature attribution using wasserstein gans",
    "citation_count": 126,
    "authors": [
      "Christian F. Baumgartner",
      "Lisa M. Koch",
      "Kerem Can Tezcan",
      "Jia Xi Ang",
      "Ender Konukoglu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Joo_Total_Capture_A_CVPR_2018_paper.html": {
    "title": "Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies",
    "volume": "main",
    "abstract": "We present a unified deformation model for the markerless capture of multiple scales of human movement, including facial expressions, body motion, and hand gestures. An initial model is generated by locally stitching together models of the individual parts of the human body, which we refer to as the ``Frankenstein'' model. This model enables the full expression of part movements, including face and hands by a single seamless model. Using a large-scale capture of people wearing everyday clothes, we optimize the Frankenstein model to create ``Adam\". Adam is a model that shares the same skeleton hierarchy as the initial model, but can express hair and clothing geometry, making it directly usable for fitting people as they normally appear in everyday life. Finally, we demonstrate the use of these models for total motion tracking method, simultaneously capturing the large-scale body movements and the subtle face and hand motion of a social group of people",
    "checked": true,
    "id": "ae41d678ec8c6460e96f70972d6d8ae265c42f48",
    "semantic_title": "total capture: a 3d deformation model for tracking faces, hands, and bodies",
    "citation_count": 472,
    "authors": [
      "Hanbyul Joo",
      "Tomas Simon",
      "Yaser Sheikh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Baek_Augmented_Skeleton_Space_CVPR_2018_paper.html": {
    "title": "Augmented Skeleton Space Transfer for Depth-Based Hand Pose Estimation",
    "volume": "main",
    "abstract": "Crucial to the success of training a depth-based 3D hand pose estimator (HPE) is the availability of comprehensive datasets covering diverse camera perspectives, shapes, and pose variations. However, collecting such annotated datasets is challenging. We propose to complete existing databases by generating new database entries. The key idea is to synthesize data in the skeleton space (instead of doing so in the depth-map space) which enables an easy and intuitive way of manipulating data entries. Since the skeleton entries generated in this way do not have the corresponding depth map entries, we exploit them by training a separate hand pose generator (HPG) which synthesizes the depth map from the skeleton entries. By training the HPG and HPE in a single unified optimization framework enforcing that 1) the HPE agrees with the paired depth and skeleton entries; and 2) the HPG-HPE combination satisfies the cyclic consistency (both the input and the output of HPG-HPE are skeletons) observed via the newly generated unpaired skeletons, our algorithm constructs a HPE which is robust to variations that go beyond the coverage of the existing database. Our training algorithm adopts the generative adversarial networks (GAN) training process. As a by-product, we obtain a hand pose discriminator (HPD) that is capable of picking out realistic hand poses. Our algorithm exploits this capability to refine the initial skeleton estimates in testing, further improving the accuracy. We test our algorithm on four challenging benchmark datasets (ICVL, MSRA, NYU and Big Hand 2.2M datasets) and demonstrate that our approach outperforms or is on par with state-of-the-art methods quantitatively and qualitatively",
    "checked": true,
    "id": "245138dfd9229ec4f737313267f2619f1902142e",
    "semantic_title": "augmented skeleton space transfer for depth-based hand pose estimation",
    "citation_count": 75,
    "authors": [
      "Seungryul Baek",
      "Kwang In Kim",
      "Tae-Kyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Balakrishnan_Synthesizing_Images_of_CVPR_2018_paper.html": {
    "title": "Synthesizing Images of Humans in Unseen Poses",
    "volume": "main",
    "abstract": "We address the computational problem of novel human pose synthesis. Given an image of a person and a desired pose, we produce a depiction of that person in that pose, retaining the appearance of both the person and background. We present a modular generative neural network that synthesizes unseen poses using training pairs of images and poses taken from human action videos. Our network separates a scene into different body part and background layers, moves body parts to new locations and refines their appearances, and composites the new foreground with a hole-filled background. These subtasks, implemented with separate modules, are trained jointly using only a single target image as a supervised label. We use an adversarial discriminator to force our network to synthesize realistic details conditioned on pose. We demonstrate image synthesis results on three action classes: golf, yoga/workouts and tennis, and show that our method produces accurate results within action classes as well as across action classes. Given a sequence of desired poses, we also produce coherent videos of actions",
    "checked": true,
    "id": "1bd2ce1d6b4f675dfba3a12ca0254c1ba8f6689a",
    "semantic_title": "synthesizing images of humans in unseen poses",
    "citation_count": 296,
    "authors": [
      "Guha Balakrishnan",
      "Amy Zhao",
      "Adrian V. Dalca",
      "Frédo Durand",
      "John Guttag"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_SSNet_Scale_Selection_CVPR_2018_paper.html": {
    "title": "SSNet: Scale Selection Network for Online 3D Action Prediction",
    "volume": "main",
    "abstract": "In action prediction (early action recognition), the goal is to predict the class label of an ongoing action using its observed part so far. In this paper, we focus on online action prediction in streaming 3D skeleton sequences. A dilated convolutional network is introduced to model the motion dynamics in temporal dimension via a sliding window over the time axis. As there are significant temporal scale variations of the observed part of the ongoing action at different progress levels, we propose a novel window scale selection scheme to make our network focus on the performed part of the ongoing action and try to suppress the noise from the previous actions at each time step. Furthermore, an activation sharing scheme is proposed to deal with the overlapping computations among the adjacent steps, which allows our model to run more efficiently. The extensive experiments on two challenging datasets show the effectiveness of the proposed action prediction framework",
    "checked": true,
    "id": "5e2f163e8a94d7a8597cabe73e7503d6b3bd7bec",
    "semantic_title": "ssnet: scale selection network for online 3d action prediction",
    "citation_count": 52,
    "authors": [
      "Jun Liu",
      "Amir Shahroudy",
      "Gang Wang",
      "Ling-Yu Duan",
      "Alex C. Kot"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gkioxari_Detecting_and_Recognizing_CVPR_2018_paper.html": {
    "title": "Detecting and Recognizing Human-Object Interactions",
    "volume": "main",
    "abstract": "To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting (human, verb, object) triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person -- their pose, clothing, action -- is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results",
    "checked": true,
    "id": "3b41981da4f853ce72fda0f892592de33aeb0e55",
    "semantic_title": "detecting and recognizing human-object interactions",
    "citation_count": 489,
    "authors": [
      "Georgia Gkioxari",
      "Ross Girshick",
      "Piotr Dollár",
      "Kaiming He"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sener_Unsupervised_Learning_and_CVPR_2018_paper.html": {
    "title": "Unsupervised Learning and Segmentation of Complex Activities From Video",
    "volume": "main",
    "abstract": "This paper presents a new method for unsupervised segmentation of complex activities from video into multiple steps, or sub-activities, without any textual input. We propose an iterative discriminative-generative approach which alternates between discriminatively learning the appearance of sub-activities from the videos' visual features to sub-activity labels and generatively modelling the temporal structure of sub-activities using a Generalized Mallows Model. In addition, we introduce a model for background to account for frames unrelated to the actual activities. Our approach is validated on the challenging Breakfast Actions and Inria Instructional Videos datasets and outperforms both unsupervised and weakly-supervised state of the art",
    "checked": true,
    "id": "154695e22470bef2413c2b9388c7f1b575e3a2e1",
    "semantic_title": "unsupervised learning and segmentation of complex activities from video",
    "citation_count": 101,
    "authors": [
      "Fadime Sener",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Genova_Unsupervised_Training_for_CVPR_2018_paper.html": {
    "title": "Unsupervised Training for 3D Morphable Model Regression",
    "volume": "main",
    "abstract": "We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results",
    "checked": true,
    "id": "643d11703569766bed0a994941ae5f7b3e101659",
    "semantic_title": "unsupervised training for 3d morphable model regression",
    "citation_count": 285,
    "authors": [
      "Kyle Genova",
      "Forrester Cole",
      "Aaron Maschinot",
      "Aaron Sarna",
      "Daniel Vlasic",
      "William T. Freeman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Alldieck_Video_Based_Reconstruction_CVPR_2018_paper.html": {
    "title": "Video Based Reconstruction of 3D People Models",
    "volume": "main",
    "abstract": "This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people. Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. We present evaluation results for a number of test subjects and analyze overall performance. Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping",
    "checked": true,
    "id": "223185c0dea34ed7e72df12df264674d24e824ed",
    "semantic_title": "video based reconstruction of 3d people models",
    "citation_count": 386,
    "authors": [
      "Thiemo Alldieck",
      "Marcus Magnor",
      "Weipeng Xu",
      "Christian Theobalt",
      "Gerard Pons-Moll"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Pose-Guided_Photorealistic_Face_CVPR_2018_paper.html": {
    "title": "Pose-Guided Photorealistic Face Rotation",
    "volume": "main",
    "abstract": "Face rotation provides an effective and cheap way for data augmentation and representation learning of face recognition. It is a challenging generative learning problem due to the large pose discrepancy between two face images. This work focuses on flexible face rotation of arbitrary head poses, including extreme profile views. We propose a novel Couple-Agent Pose-Guided Generative Adversarial Network (CAPG-GAN) to generate both neutral and profile head pose face images. The head pose information is encoded by facial landmark heatmaps. It not only forms a mask image to guide the generator in learning process but also provides a flexible controllable condition during inference. A couple-agent discriminator is introduced to reinforce on the realism of synthetic arbitrary view faces. Besides the generator and conditional adversarial loss, CAPG-GAN further employs identity preserving loss and total variation regularization to preserve identity information and refine local textures respectively. Quantitative and qualitative experimental results on the Multi-PIE and LFW databases consistently show the superiority of our face rotation method over the state-of-the-art",
    "checked": true,
    "id": "20f87ed94a423b5d8599d85d1f2f80bab8902107",
    "semantic_title": "pose-guided photorealistic face rotation",
    "citation_count": 156,
    "authors": [
      "Yibo Hu",
      "Xiang Wu",
      "Bing Yu",
      "Ran He",
      "Zhenan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huynh_Mesoscopic_Facial_Geometry_CVPR_2018_paper.html": {
    "title": "Mesoscopic Facial Geometry Inference Using Deep Neural Networks",
    "volume": "main",
    "abstract": "We present a learning-based approach for synthesizing facial geometry at medium and fine scales from diffusely-lit facial texture maps. When applied to an image sequence, the synthesized detail is temporally coherent. Unlike current state-of-the-art methods, which assume \"dark is deep\", our model is trained with measured facial detail collected using polarized gradient illumination in a Light Stage. This enables us to produce plausible facial detail across the entire face, including where previous approaches may incorrectly interpret dark features as concavities such as at moles, hair stubble, and occluded pores. Instead of directly inferring 3D geometry, we propose to encode fine details in high-resolution displacement maps which are learned through a hybrid network adopting the state-of-the-art image-to-image translation network and super resolution network. To effectively capture geometric detail at both mid- and high frequencies, we factorize the learning into two separate sub-networks, enabling the full range of facial detail to be modeled. Results from our learning-based approach compare favorably with a high-quality active facial scanning technique, and require only a single passive lighting condition without a complex scanning setup",
    "checked": true,
    "id": "63eb1347911762557d5bd294c0f1d92b520ed74a",
    "semantic_title": "mesoscopic facial geometry inference using deep neural networks",
    "citation_count": 64,
    "authors": [
      "Loc Huynh",
      "Weikai Chen",
      "Shunsuke Saito",
      "Jun Xing",
      "Koki Nagano",
      "Andrew Jones",
      "Paul Debevec",
      "Hao Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ge_Hand_PointNet_3D_CVPR_2018_paper.html": {
    "title": "Hand PointNet: 3D Hand Pose Estimation Using Point Sets",
    "volume": "main",
    "abstract": "Convolutional Neural Network (CNN) has shown promising results for 3D hand pose estimation in depth images. Different from existing CNN-based hand pose estimation methods that take either 2D images or 3D volumes as the input, our proposed Hand PointNet directly processes the 3D point cloud that models the visible surface of the hand for pose regression. Taking the normalized point cloud as the input, our proposed hand pose regression network is able to capture complex hand structures and accurately regress a low dimensional representation of the 3D hand pose. In order to further improve the accuracy of fingertips, we design a fingertip refinement network that directly takes the neighboring points of the estimated fingertip location as input to refine the fingertip location. Experiments on three challenging hand pose datasets show that our proposed method outperforms state-of-the-art methods",
    "checked": true,
    "id": "084b57b2d973562f1e91711ced38c46f25188fcd",
    "semantic_title": "hand pointnet: 3d hand pose estimation using point sets",
    "citation_count": 218,
    "authors": [
      "Liuhao Ge",
      "Yujun Cai",
      "Junwu Weng",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Nagrani_Seeing_Voices_and_CVPR_2018_paper.html": {
    "title": "Seeing Voices and Hearing Faces: Cross-Modal Biometric Matching",
    "volume": "main",
    "abstract": "We introduce a seemingly impossible task: given only an audio clip of someone speaking, decide which of two face images is the speaker. In this paper we study this, and a number of related cross-modal tasks, aimed at answering the question: how much can we infer from the voice about the face and vice versa? We study this task \"in the wild\", employing the datasets that are now publicly available for face recognition from static images (VGGFace) and speaker identification from audio (VoxCeleb). These provide training and testing scenarios for both static and dynamic testing of cross-modal matching. We make the following contributions: (i) we introduce CNN architectures for both binary and multi-way cross-modal face and audio matching; (ii) we compare dynamic testing (where video information is available, but the audio is not from the same video) with static testing (where only a single still image is available); and (iii) we use hu- man testing as a baseline to calibrate the difficulty of the task. We show that a CNN can indeed be trained to solve this task in both the static and dynamic scenarios, and is even well above chance on 10-way classification of the face given the voice. The CNN matches human performance on easy examples (e.g. different gender across faces) but exceeds human performance on more challenging examples (e.g. faces with the same gender, age and nationality)",
    "checked": true,
    "id": "2c75658b080a9baaac20db39af86016ffa36f6f0",
    "semantic_title": "seeing voices and hearing faces: cross-modal biometric matching",
    "citation_count": 194,
    "authors": [
      "Arsha Nagrani",
      "Samuel Albanie",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Rhodin_Learning_Monocular_3D_CVPR_2018_paper.html": {
    "title": "Learning Monocular 3D Human Pose Estimation From Multi-View Images",
    "volume": "main",
    "abstract": "Accurate 3D human pose estimation from single images is possible with sophisticated deep-net architectures that have been trained on very large datasets. However, this still leaves open the problem of capturing motions for which no such database exists. Manual annotation is tedious, slow, and error-prone. In this paper, we propose to replace most of the annotations by the use of multiple views, at training time only. Specifically, we train the system to predict the same pose in all views. Such a consistency constraint is necessary but not sufficient to predict accurate poses. We therefore complement it with a supervised loss aiming to predict the correct pose in a small set of labeled images, and with a regularization term that penalizes drift from initial predictions. Furthermore, we propose a method to estimate camera pose jointly with human pose, which lets us utilize multi-view footage where calibration is difficult, e.g., for pan-tilt or moving handheld cameras. We demonstrate the effectiveness of our approach on established benchmarks, as well as on a new Ski dataset with rotating cameras and expert ski motion, for which annotations are truly hard to obtain",
    "checked": true,
    "id": "45dec5e3aca87cb70d8dafd355a046d5b2ec4c36",
    "semantic_title": "learning monocular 3d human pose estimation from multi-view images",
    "citation_count": 220,
    "authors": [
      "Helge Rhodin",
      "Jörg Spörri",
      "Isinsu Katircioglu",
      "Victor Constantin",
      "Frédéric Meyer",
      "Erich Müller",
      "Mathieu Salzmann",
      "Pascal Fua"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Separating_Style_and_CVPR_2018_paper.html": {
    "title": "Separating Style and Content for Generalized Style Transfer",
    "volume": "main",
    "abstract": "Neural style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here attempt to separate the representations for styles and contents, and propose a generalized style transfer network consisting of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content factors from the style reference images and content reference images, respectively. The mixer employs a bilinear model to integrate the above two factors and finally feeds it into a decoder to generate images with target style and content. To separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. During training, the encoder network learns to extract styles and contents from two sets of reference images in limited size, one with shared style and the other with shared content. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special `multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. For validation, we applied the proposed algorithm to the Chinese Typeface transfer problem. Extensive experiment results on character generation have demonstrated the effectiveness and robustness of our method",
    "checked": true,
    "id": "1b0ea9b19edaf99aff5e411020387a70c79867c8",
    "semantic_title": "separating style and content for generalized style transfer",
    "citation_count": 137,
    "authors": [
      "Yexun Zhang",
      "Ya Zhang",
      "Wenbin Cai"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xian_TextureGAN_Controlling_Deep_CVPR_2018_paper.html": {
    "title": "TextureGAN: Controlling Deep Image Synthesis With Texture Patches",
    "volume": "main",
    "abstract": "In this paper, we investigate deep image synthesis guided by sketch, color, and texture. Previous image synthesis methods can be controlled by sketch and color strokes but we are the first to examine texture control. We allow a user to place a texture patch on a sketch at arbitrary locations and scales to control the desired output texture. Our generative network learns to synthesize objects consistent with these texture suggestions. To achieve this, we develop a local texture loss in addition to adversarial and content loss to train the generative network. We conduct experiments using sketches generated from real images and textures sampled from a separate texture database and results show that our proposed algorithm is able to generate plausible images that are faithful to user controls. Ablation studies show that our proposed pipeline can generate more realistic images than adapting existing methods directly",
    "checked": true,
    "id": "ad527d3375ffd1d7f114011541bdcf76d7abcb47",
    "semantic_title": "texturegan: controlling deep image synthesis with texture patches",
    "citation_count": 259,
    "authors": [
      "Wenqi Xian",
      "Patsorn Sangkloy",
      "Varun Agrawal",
      "Amit Raj",
      "Jingwan Lu",
      "Chen Fang",
      "Fisher Yu",
      "James Hays"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Orekondy_Connecting_Pixels_to_CVPR_2018_paper.html": {
    "title": "Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images",
    "volume": "main",
    "abstract": "Images convey a broad spectrum of personal information. If such images are shared on social media platforms, this personal information is leaked which conflicts with the privacy of depicted persons. Therefore, we aim for automated approaches to redact such private information and thereby protect privacy of the individual. By conducting a user study we find that obfuscating the image regions related to the private information leads to privacy while retaining utility of the images. Moreover, by varying the size of the regions different privacy-utility trade-offs can be achieved. Our findings argue for a \"redaction by segmentation\" paradigm. Hence, we propose the first sizable dataset of private images \"in the wild\" annotated with pixel and instance level labels across a broad range of privacy classes. We present the first model for automatic redaction of diverse private information. It is effective at achieving various privacy-utility trade-offs within 83% of the performance of redactions based on ground-truth annotation",
    "checked": true,
    "id": "38cd95ed542040a22b24dee8ec94c60dfb7ad42e",
    "semantic_title": "connecting pixels to privacy and utility: automatic redaction of private information in images",
    "citation_count": 73,
    "authors": [
      "Tribhuvanesh Orekondy",
      "Mario Fritz",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Henriques_MapNet_An_Allocentric_CVPR_2018_paper.html": {
    "title": "MapNet: An Allocentric Spatial Memory for Mapping Environments",
    "volume": "main",
    "abstract": "Autonomous agents need to reason about the world beyond their instantaneous sensory input. Integrating information over time, however, requires switching from an egocentric representation of a scene to an allocentric one, expressed in the world reference frame. It must also be possible to update the representation dynamically, which requires localizing and registering the sensor with respect to it. In this paper, we develop a differentiable module that satisfies such requirements, while being robust, efficient, and suitable for integration in end-to-end deep networks. The module contains an allocentric spatial memory that can be accessed associatively by feeding to it the current sensory input, resulting in localization, and then updated using an LSTM or similar mechanism. We formulate efficient localization and registration of sensory information as a dual pair of convolution/deconvolution operators in memory space. The map itself is a 2.5D representation of an environment storing information that a deep neural network module learns to distill from RGBD input. The result is a map that contains multi-task information, different from classical approaches to mapping such as structure-from-motion. We present results using synthetic mazes, a dataset of hours of recorded gameplay of the classic game Doom, and the very recent Active Vision Dataset of real images captured from a robot",
    "checked": true,
    "id": "795783cf1b7f267fa59cb6901a238762fe3ee608",
    "semantic_title": "mapnet: an allocentric spatial memory for mapping environments",
    "citation_count": 140,
    "authors": [
      "João F. Henriques",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bhattacharyya_Accurate_and_Diverse_CVPR_2018_paper.html": {
    "title": "Accurate and Diverse Sampling of Sequences Based on a \"Best of Many\" Sample Objective",
    "volume": "main",
    "abstract": "For autonomous agents to successfully operate in the real world, anticipation of future events and states of their environment is a key competence. This problem has been formalized as a sequence extrapolation problem, where a number of observations are used to predict the sequence into the future. Real-world scenarios demand a model of uncertainty of such predictions, as predictions become increasingly uncertain -- in particular on long time horizons. While impressive results have been shown on point estimates, scenarios that induce multi-modal distributions over future sequences remain challenging. Our work addresses these challenges in a Gaussian Latent Variable model for sequence prediction. Our core contribution is a ``Best of Many'' sample objective that leads to more accurate and more diverse predictions that better capture the true variations in real-world sequence data. Beyond our analysis of improved model fit, our models also empirically outperform prior work on three diverse tasks ranging from traffic scenes to weather data",
    "checked": true,
    "id": "c731a3d8ef9121de7ee6a14dc70e0e774792982c",
    "semantic_title": "accurate and diverse sampling of sequences based on a \"best of many\" sample objective",
    "citation_count": 98,
    "authors": [
      "Apratim Bhattacharyya",
      "Bernt Schiele",
      "Mario Fritz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.html": {
    "title": "VirtualHome: Simulating Household Activities via Programs",
    "volume": "main",
    "abstract": "In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to \"drive'' an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language",
    "checked": true,
    "id": "7139a5f730652abbeabf9e140009907d2c7da3e5",
    "semantic_title": "virtualhome: simulating household activities via programs",
    "citation_count": 321,
    "authors": [
      "Xavier Puig",
      "Kevin Ra",
      "Marko Boben",
      "Jiaman Li",
      "Tingwu Wang",
      "Sanja Fidler",
      "Antonio Torralba"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.html": {
    "title": "Generate to Adapt: Aligning Domains Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Domain Adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network. This is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data. We demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and USPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain adaptation from synthetic to real data. Our method achieves state-of-the art performance in most experimental settings and by far the only GAN-based method that has been shown to work well across different datasets such as OFFICE and DIGITS",
    "checked": true,
    "id": "15168665f4b8eb11466086e69780ed98e5280059",
    "semantic_title": "generate to adapt: aligning domains using generative adversarial networks",
    "citation_count": 617,
    "authors": [
      "Swami Sankaranarayanan",
      "Yogesh Balaji",
      "Carlos D. Castillo",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ghosh_Multi-Agent_Diverse_Generative_CVPR_2018_paper.html": {
    "title": "Multi-Agent Diverse Generative Adversarial Networks",
    "volume": "main",
    "abstract": "We propose MAD-GAN, an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known problem of mode collapse. First, MAD-GAN is a multi-agent GAN architecture incorporating multiple generators and one discriminator. Second, to enforce that different generators capture diverse high probability modes, the discriminator of MAD-GAN is designed such that along with finding the real and fake samples, it is also required to identify the generator that generated the given fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for challenging tasks such as image-to-image translation and face generation. In addition, we also show that MAD-GAN is able to disentangle different modalities when trained using highly challenging diverse-class dataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the end, we show its efficacy on the unsupervised feature representation task",
    "checked": true,
    "id": "813f49eb47f1fedc115742287f6211cf90b4c710",
    "semantic_title": "multi-agent diverse generative adversarial networks",
    "citation_count": 283,
    "authors": [
      "Arnab Ghosh",
      "Viveka Kulharia",
      "Vinay P. Namboodiri",
      "Philip H.S. Torr",
      "Puneet K. Dokania"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/An_A_PID_Controller_CVPR_2018_paper.html": {
    "title": "A PID Controller Approach for Stochastic Optimization of Deep Networks",
    "volume": "main",
    "abstract": "Deep neural networks have demonstrated their power in many computer vision applications. State-of-the-art deep architectures such as VGG, ResNet, and DenseNet are mostly optimized by the SGD-Momentum algorithm, which updates the weights by considering their past and current gradients. Nonetheless, SGD-Momentum suffers from the overshoot problem, which hinders the convergence of network training. Inspired by the prominent success of proportional-integral-derivative (PID) controller in automatic control, we propose a PID approach for accelerating deep network optimization. We first reveal the intrinsic connections between SGD-Momentum and PID based controller, then present the optimization algorithm which exploits the past, current, and change of gradients to update the network parameters. The proposed PID method reduces much the overshoot phenomena of SGD-Momentum, and it achieves up to 50% acceleration on popular deep network architectures with competitive accuracy, as verified by our experiments on the benchmark datasets including CIFAR10, CIFAR100, and Tiny-ImageNet",
    "checked": true,
    "id": "0ec62495941ab2810eed2ec61b2d7cc23956d7ab",
    "semantic_title": "a pid controller approach for stochastic optimization of deep networks",
    "citation_count": 88,
    "authors": [
      "Wangpeng An",
      "Haoqian Wang",
      "Qingyun Sun",
      "Jun Xu",
      "Qionghai Dai",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Carreira-Perpinan_Learning-Compression_Algorithms_for_CVPR_2018_paper.html": {
    "title": "Learning-Compression\" Algorithms for Neural Net Pruning",
    "volume": "main",
    "abstract": "Pruning a neural net consists of removing weights without degrading its performance. This is an old problem of renewed interest because of the need to compress ever larger nets so they can run in mobile devices. Pruning has been traditionally done by ranking or penalizing weights according to some criterion (such as magnitude), removing low-ranked weights and retraining the remaining ones. We formulate pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition. We give a generic algorithm to solve this which alternates \"learning\" steps that optimize a regularized, data-dependent loss and \"compression\" steps that mark weights for pruning in a data-independent way. Magnitude thresholding arises naturally in the compression step, but unlike existing magnitude pruning approaches, our algorithm explores subsets of weights rather than committing irrevocably to a specific subset from the beginning. It is also able to learn automatically the best number of weights to prune in each layer of the net without incurring an exponentially costly model selection. Using a single pruning-level user parameter, we achieve state-of-the-art pruning in nets of various sizes",
    "checked": true,
    "id": "d719009bade1c245ac6e2fa9e4cd74eddd4f34b4",
    "semantic_title": "learning-compression\" algorithms for neural net pruning",
    "citation_count": 187,
    "authors": [
      "Miguel Á. Carreira-Perpiñán",
      "Yerlan Idelbayev"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qian_Large-Scale_Distance_Metric_CVPR_2018_paper.html": {
    "title": "Large-Scale Distance Metric Learning With Uncertainty",
    "volume": "main",
    "abstract": "Distance metric learning (DML) has been studied extensively in the past decades for its superior performance with distance-based algorithms. Most of the existing methods propose to learn a distance metric with pairwise or triplet constraints. However, the number of constraints is quadratic or even cubic in the number of the original examples, which makes it challenging for DML to handle the large-scale data set. Besides, the real-world data may contain various uncertainty, especially for the image data. The uncertainty can mislead the learning procedure and cause the performance degradation. By investigating the image data, we find that the original data can be observed from a small set of clean latent examples with different distortions. In this work, we propose the margin preserving metric learning framework to learn the distance metric and latent examples simultaneously. By leveraging the ideal properties of latent examples, the training efficiency can be improved significantly while the learned metric also becomes robust to the uncertainty in the original data. Furthermore, we can show that the metric is learned from latent examples only, but it can preserve the large margin property even for the original data. The empirical study on the benchmark image data sets demonstrates the efficacy and efficiency of the proposed method",
    "checked": true,
    "id": "7c1ac5aeac1d27a2dae85d46f0cb856ff56d6f8c",
    "semantic_title": "large-scale distance metric learning with uncertainty",
    "citation_count": 22,
    "authors": [
      "Qi Qian",
      "Jiasheng Tang",
      "Hao Li",
      "Shenghuo Zhu",
      "Rong Jin"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Rupprecht_Guide_Me_Interacting_CVPR_2018_paper.html": {
    "title": "Guide Me: Interacting With Deep Networks",
    "volume": "main",
    "abstract": "Interaction and collaboration between humans and intelligent machines has become increasingly important as machine learning methods move into real-world applications that involve end users. While much prior work lies at the intersection of natural language and vision, such as image captioning or image generation from text descriptions, less focus has been placed on the use of language to guide or improve the performance of a learned visual processing algorithm. In this paper, we explore methods to flexibly guide a trained convolutional neural network through user input to improve its performance during inference. We do so by inserting a layer that acts as a spatio-semantic guide into the network. This guide is trained to modify the network's activations, either directly via an energy minimization scheme or indirectly through a recurrent model that translates human language queries to interaction weights. Learning the verbal interaction is fully automatic and does not require manual text annotations. We evaluate the method on two datasets, showing that guiding a pre-trained network can improve performance, and provide extensive insights into the interaction between the guide and the CNN",
    "checked": true,
    "id": "a334442b493501bb60a53dc3e689fc569965ad81",
    "semantic_title": "guide me: interacting with deep networks",
    "citation_count": 36,
    "authors": [
      "Christian Rupprecht",
      "Iro Laina",
      "Nassir Navab",
      "Gregory D. Hager",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Khrulkov_Art_of_Singular_CVPR_2018_paper.html": {
    "title": "Art of Singular Vectors and Universal Adversarial Perturbations",
    "volume": "main",
    "abstract": "Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been attracting a lot of attention in recent studies. It has been shown that for many state of the art DNNs performing image classification there exist universal adversarial perturbations --- image-agnostic perturbations mere addition of which to natural images with high probability leads to their misclassification. In this work we propose a new algorithm for constructing such universal perturbations. Our approach is based on computing the so-called (p, q)-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns, and by using only 64 images we were able to construct universal perturbations with more than 60 % fooling rate on the dataset consisting of 50000 images. We also investigate a correlation between the maximal singular value of the Jacobian matrix and the fooling rate of the corresponding singular vector, and show that the constructed perturbations generalize across networks",
    "checked": true,
    "id": "dc777eb44207a3349c6bd0656687ffd376101610",
    "semantic_title": "art of singular vectors and universal adversarial perturbations",
    "citation_count": 117,
    "authors": [
      "Valentin Khrulkov",
      "Ivan Oseledets"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Prakash_Deflecting_Adversarial_Attacks_CVPR_2018_paper.html": {
    "title": "Deflecting Adversarial Attacks With Pixel Deflection",
    "volume": "main",
    "abstract": "CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classification accuracy is significantly preserved in the presence of such adversarial manipulations. Image classifiers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN",
    "checked": true,
    "id": "f5e602bf3d59d2ea1553f06a94e9e94880af065d",
    "semantic_title": "deflecting adversarial attacks with pixel deflection",
    "citation_count": 273,
    "authors": [
      "Aaditya Prakash",
      "Nick Moran",
      "Solomon Garber",
      "Antonella DiLillo",
      "James Storer"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Vicol_MovieGraphs_Towards_Understanding_CVPR_2018_paper.html": {
    "title": "MovieGraphs: Towards Understanding Human-Centric Situations From Videos",
    "volume": "main",
    "abstract": "There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to \"read\" people's emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called MovieGraphs which provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions. In addition, most interactions and many attributes are grounded in the video with time stamps. We provide a thorough analysis of our dataset, showing interesting common-sense correlations between different social aspects of scenes, as well as across scenes over time. We propose a method for querying videos and text with graphs, and show that: 1) our graphs contain rich and sufficient information to summarize and localize each scene; and 2) subgraphs allow us to describe situations at an abstract level and retrieve multiple semantically relevant situations. We also propose methods for interaction understanding via ordering, and reason understanding. MovieGraphs is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent AI agents",
    "checked": true,
    "id": "523574aca71d8981b4122cce8d132f22391ef26e",
    "semantic_title": "moviegraphs: towards understanding human-centric situations from videos",
    "citation_count": 114,
    "authors": [
      "Paul Vicol",
      "Makarand Tapaswi",
      "Lluís Castrejón",
      "Sanja Fidler"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mathews_SemStyle_Learning_to_CVPR_2018_paper.html": {
    "title": "SemStyle: Learning to Generate Stylised Image Captions Using Unaligned Text",
    "volume": "main",
    "abstract": "Linguistic style is an essential part of written communication, with the power to affect both clarity and attractiveness. With recent advances in vision and language, we can start to tackle the problem of generating image captions that are both visually grounded and appropriately styled. Existing approaches either require styled training captions aligned to images or generate captions with low relevance. We develop a model that learns to generate visually relevant styled captions from a large corpus of styled text without aligned images. The core idea of this model, called SemStyle, is to separate semantics and style. One key component is a novel and concise semantic term representation generated using natural language processing techniques and frame semantics. In addition, we develop a unified language model that decodes sentences with diverse word choices and syntax for different styles. Evaluations, both automatic and manual, show captions from SemStyle preserve image semantics, are descriptive, and are style shifted. More broadly, this work provides possibilities to learn richer image descriptions from the plethora of linguistic data available on the web",
    "checked": true,
    "id": "beeebd2af0d8f130dcf234231de4569d584cb7fd",
    "semantic_title": "semstyle: learning to generate stylised image captions using unaligned text",
    "citation_count": 103,
    "authors": [
      "Alexander Mathews",
      "Lexing Xie",
      "Xuming He"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sattler_Benchmarking_6DOF_Outdoor_CVPR_2018_paper.html": {
    "title": "Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions",
    "volume": "main",
    "abstract": "Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera pose estimates. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on 6DOF camera pose estimation accuracy through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions, showing that long-term localization is far from solved, and propose promising avenues for future work, including sequence-based localization approaches and the need for better local features. Our benchmark is available at visuallocalization.net",
    "checked": true,
    "id": "5b9b5b7e20afff4e2caa08e0d22cefe87007bf63",
    "semantic_title": "benchmarking 6dof outdoor visual localization in changing conditions",
    "citation_count": 539,
    "authors": [
      "Torsten Sattler",
      "Will Maddern",
      "Carl Toft",
      "Akihiko Torii",
      "Lars Hammarstrand",
      "Erik Stenborg",
      "Daniel Safari",
      "Masatoshi Okutomi",
      "Marc Pollefeys",
      "Josef Sivic",
      "Fredrik Kahl",
      "Tomas Pajdla"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_IVQA_Inverse_Visual_CVPR_2018_paper.html": {
    "title": "IVQA: Inverse Visual Question Answering",
    "volume": "main",
    "abstract": "We propose the inverse problem of Visual question answering (iVQA), and explore its suitability as a benchmark for visuo-linguistic understanding. The iVQA task is to generate a question that corresponds to a given image and answer pair. Since the answers are less informative than the questions, and the questions have less learnable bias, an iVQA model needs to better understand the image to be successful than a VQA model. We pose question generation as a multi-modal dynamic inference process and propose an iVQA model that can gradually adjust its focus of attention guided by both a partially generated question and the answer. For evaluation, apart from existing linguistic metrics, we propose a new ranking metric. This metric compares the ground truth question's rank among a list of distractors, which allows the drawbacks of different algorithms and sources of error to be studied. Experimental results show that our model can generate diverse, grammatically correct and content correlated questions that match the given answer",
    "checked": true,
    "id": "2125074798ad4110f7310ef9e9bfb172e60774cb",
    "semantic_title": "ivqa: inverse visual question answering",
    "citation_count": 33,
    "authors": [
      "Feng Liu",
      "Tao Xiang",
      "Timothy M. Hospedales",
      "Wankou Yang",
      "Changyin Sun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Pumarola_Unsupervised_Person_Image_CVPR_2018_paper.html": {
    "title": "Unsupervised Person Image Synthesis in Arbitrary Poses",
    "volume": "main",
    "abstract": "We present a novel approach for synthesizing photo-realistic images of people in arbitrary poses using generative adversarial learning. Given an input image of a person and a desired pose represented by a 2D skeleton, our model renders the image of the same person under the new pose, synthesizing novel views of the parts visible in the input image and hallucinating those that are not seen. This problem has recently been addressed in a supervised manner, i.e., during training the ground truth images under the new poses are given to the network. We go beyond these approaches by proposing a fully unsupervised strategy. We tackle this challenging scenario by splitting the problem into two principal subtasks. First, we consider a pose conditioned bidirectional generator that maps back the initially rendered image to the original pose, hence being directly comparable to the input image without the need to resort to any training image. Second, we devise a novel loss function that incorporates content and style terms, and aims at producing images of high perceptual quality. Extensive experiments conducted on the DeepFashion dataset demonstrate that the images rendered by our model are very close in appearance to those obtained by fully supervised approaches",
    "checked": true,
    "id": "41d3603bebd818b79590383d94b5b36349171289",
    "semantic_title": "unsupervised person image synthesis in arbitrary poses",
    "citation_count": 160,
    "authors": [
      "Albert Pumarola",
      "Antonio Agudo",
      "Alberto Sanfeliu",
      "Francesc Moreno-Noguer"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xie_Learning_Descriptor_Networks_CVPR_2018_paper.html": {
    "title": "Learning Descriptor Networks for 3D Shape Synthesis and Analysis",
    "volume": "main",
    "abstract": "This paper proposes a 3D shape descriptor network, which is a deep convolutional energy-based model, for modeling volumetric shape patterns. The maximum likelihood training of the model follows an \"analysis by synthesis\" scheme and can be interpreted as a mode seeking and mode shifting process. The model can synthesize 3D shape patterns by sampling from the probability distribution via MCMC such as Langevin dynamics. The model can be used to train a 3D generator network via MCMC teaching. The conditional version of the 3D shape descriptor net can be used for 3D object recovery and 3D object super-resolution. Experiments demonstrate that the proposed model can generate realistic 3D shape patterns and can be useful for 3D shape analysis",
    "checked": true,
    "id": "c10ad19c33025e05f51e16ba871b4f4a2c418fe6",
    "semantic_title": "learning descriptor networks for 3d shape synthesis and analysis",
    "citation_count": 134,
    "authors": [
      "Jianwen Xie",
      "Zilong Zheng",
      "Ruiqi Gao",
      "Wenguan Wang",
      "Song-Chun Zhu",
      "Ying Nian Wu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Villegas_Neural_Kinematic_Networks_CVPR_2018_paper.html": {
    "title": "Neural Kinematic Networks for Unsupervised Motion Retargetting",
    "volume": "main",
    "abstract": "We propose a recurrent neural network architecture with a Forward Kinematics layer and cycle consistency based adversarial training objective for unsupervised motion retargetting. Our network captures the high-level properties of an input motion by the forward kinematics layer, and adapts them to a target character with different skeleton bone lengths (e.g., shorter, longer arms etc.). Collecting paired motion training sequences from different characters is expensive. Instead, our network utilizes cycle consistency to learn to solve the Inverse Kinematics problem in an unsupervised manner. Our method works online, i.e., it adapts the motion sequence on-the-fly as new frames are received. In our experiments, we use the Mixamo animation data to test our method for a variety of motions and characters and achieve state-of-the-art results. We also demonstrate motion retargetting from monocular human videos to 3D characters using an off-the-shelf 3D pose estimator",
    "checked": true,
    "id": "c632af0d740a551a148e48091499ca4ddba45881",
    "semantic_title": "neural kinematic networks for unsupervised motion retargetting",
    "citation_count": 181,
    "authors": [
      "Ruben Villegas",
      "Jimei Yang",
      "Duygu Ceylan",
      "Honglak Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Group_Consistent_Similarity_CVPR_2018_paper.html": {
    "title": "Group Consistent Similarity Learning via Deep CRF for Person Re-Identification",
    "volume": "main",
    "abstract": "Person re-identification benefits greatly from deep neural networks (DNN) to learn accurate similarity metrics and robust feature embeddings. However, most of the current methods impose only local constraints for similarity learning. In this paper, we incorporate constraints on large image groups by combining the CRF with deep neural networks. The proposed method aims to learn the ``local similarity\" metrics for image pairs while taking into account the dependencies from all the images in a group, forming ``group similarities\". Our method involves multiple images to model the relationships among the local and global similarities in a unified CRF during training, while combines multi-scale local similarities as the predicted similarity in testing. We adopt an approximate inference scheme for estimating the group similarity, enabling end-to-end training. Extensive experiments demonstrate the effectiveness of our model that combines DNN and CRF for learning robust multi-scale local similarities. The overall results outperform those by state-of-the-arts with considerable margins on three widely-used benchmarks",
    "checked": true,
    "id": "308a13fd1d2847d98930a8e5542f773a9651a0ae",
    "semantic_title": "group consistent similarity learning via deep crf for person re-identification",
    "citation_count": 220,
    "authors": [
      "Dapeng Chen",
      "Dan Xu",
      "Hongsheng Li",
      "Nicu Sebe",
      "Xiaogang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gong_Learning_Compositional_Visual_CVPR_2018_paper.html": {
    "title": "Learning Compositional Visual Concepts With Mutual Consistency",
    "volume": "main",
    "abstract": "Compositionality of semantic concepts in image synthesis and analysis is appealing as it can help in decomposing known and generatively recomposing unknown data. For instance, we may learn concepts of changing illumination, geometry or albedo of a scene, and try to recombine them to generate physically meaningful, but unseen data for training and testing. In practice however we often do not have samples from the joint concept space available: We may have data on illumination change in one data set and on geometric change in another one without complete overlap. We pose the following question: How can we learn two or more concepts jointly from different data sets with mutual consistency where we do not have samples from the full joint space? We present a novel answer in this paper based on cyclic consistency over multiple concepts, represented individually by generative adversarial networks (GANs). Our method, ConceptGAN, can be understood as a drop in for data augmentation to improve resilience for real world applications. Qualitative and quantitative evaluations demonstrate its efficacy in generating semantically meaningful images, as well as one shot face verification as an example application",
    "checked": true,
    "id": "f026f2578658683d8a9e5ed98af6b5cc75a371cb",
    "semantic_title": "learning compositional visual concepts with mutual consistency",
    "citation_count": 8,
    "authors": [
      "Yunye Gong",
      "Srikrishna Karanam",
      "Ziyan Wu",
      "Kuan-Chuan Peng",
      "Jan Ernst",
      "Peter C. Doerschuk"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_NestedNet_Learning_Nested_CVPR_2018_paper.html": {
    "title": "NestedNet: Learning Nested Sparse Structures in Deep Neural Networks",
    "volume": "main",
    "abstract": "Recently, there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve the inference speed. While many recent works focus on reducing the redundancy by eliminating unneeded weight parameters, it is not possible to apply a single deep network for multiple devices with different resources. When a new device or circumstantial condition requires a new deep architecture, it is necessary to construct and train a new network from scratch. In this work, we propose a novel deep learning framework, called a nested sparse network, which exploits an n-in-1-type nested structure in a neural network. A nested sparse network consists of multiple levels of networks with a different sparsity ratio associated with each level, and higher level networks share parameters with lower level networks to enable stable nested learning. The proposed framework realizes a resource-aware versatile architecture as the same network can meet diverse resource requirements, i.e., anytime property. Moreover, the proposed nested network can learn different forms of knowledge in its internal networks at different levels, enabling multiple tasks using a single network, such as coarse-to-fine hierarchical classification. In order to train the proposed nested network, we propose efficient weight connection learning and channel and layer scheduling strategies. We evaluate our network in multiple tasks, including adaptive deep compression, knowledge distillation, and learning class hierarchy, and demonstrate that nested sparse networks perform competitively, but more efficiently, compared to existing methods",
    "checked": true,
    "id": "89ae26307de754638a49ab92c71e481df61a5a74",
    "semantic_title": "nestednet: learning nested sparse structures in deep neural networks",
    "citation_count": 52,
    "authors": [
      "Eunwoo Kim",
      "Chanho Ahn",
      "Songhwai Oh"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_Context_Embedding_Networks_CVPR_2018_paper.html": {
    "title": "Context Embedding Networks",
    "volume": "main",
    "abstract": "Low dimensional embeddings that capture the main variations of interest in collections of data are important for many applications. One way to construct these embeddings is to acquire estimates of similarity from the crowd. Similarity is a multi-dimensional concept that varies from individual to individual. However, existing models for learning crowd embeddings typically make simplifying assumptions such as all individuals estimate similarity using the same criteria, the list of criteria is known in advance, or that the crowd workers are not influenced by the data that they see. To overcome these limitations we introduce Context Embedding Networks (CENs). In addition to learning interpretable embeddings from images, CENs also model worker biases for different attributes along with the visual context i.e. the attributes highlighted by a set of images. Experiments on three noisy crowd annotated datasets show that modeling both worker bias and visual context results in more interpretable embeddings compared to existing approaches",
    "checked": true,
    "id": "6a20f975eb253503bba46a421275d46cf74ee1b9",
    "semantic_title": "context embedding networks",
    "citation_count": 8,
    "authors": [
      "Kun Ho Kim",
      "Oisin Mac Aodha",
      "Pietro Perona"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Iterative_Learning_With_CVPR_2018_paper.html": {
    "title": "Iterative Learning With Open-Set Noisy Labels",
    "volume": "main",
    "abstract": "Large-scale datasets possessing clean label annotations are crucial for training Convolutional Neural Networks (CNNs). However, labeling large-scale data can be very costly and error-prone, and even high-quality datasets are likely to contain noisy (incorrect) labels. Existing works usually employ a closed-set assumption, whereby the samples associated with noisy labels possess a true class contained within the set of known classes in the training data. However, such an assumption is too restrictive for many applications, since samples associated with noisy labels might in fact possess a true class that is not present in the training data. We refer to this more complex scenario as the open-set noisy label problem and show that it is nontrivial in order to make accurate predictions. To address this problem, we propose a novel iterative learning framework for training CNNs on datasets with open-set noisy labels. Our approach detects noisy labels and learns deep discriminative features in an iterative fashion. To benefit from the noisy label detection, we design a Siamese network to encourage clean labels and noisy labels to be dissimilar. A reweighting module is also applied to simultaneously emphasize the learning from clean labels and reduce the effect caused by noisy labels. Experiments on CIFAR-10, ImageNet and real-world noisy (web-search) datasets demonstrate that our proposed model can robustly train CNNs in the presence of a high proportion of open-set as well as closed-set noisy labels",
    "checked": true,
    "id": "0abea3322379506b017784309d9bee2e375e3e5d",
    "semantic_title": "iterative learning with open-set noisy labels",
    "citation_count": 285,
    "authors": [
      "Yisen Wang",
      "Weiyang Liu",
      "Xingjun Ma",
      "James Bailey",
      "Hongyuan Zha",
      "Le Song",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html": {
    "title": "Learning Transferable Architectures for Scalable Image Recognition",
    "volume": "main",
    "abstract": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the \"NASNet search space\"\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS -- a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset",
    "checked": true,
    "id": "d0611891b9e8a7c5731146097b6f201578f47b2f",
    "semantic_title": "learning transferable architectures for scalable image recognition",
    "citation_count": 4973,
    "authors": [
      "Barret Zoph",
      "Vijay Vasudevan",
      "Jonathon Shlens",
      "Quoc V. Le"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_SBNet_Sparse_Blocks_CVPR_2018_paper.html": {
    "title": "SBNet: Sparse Blocks Network for Fast Inference",
    "volume": "main",
    "abstract": "Conventional deep convolutional neural networks (CNNs) apply convolution operators uniformly in space across all feature maps for hundreds of layers - this incurs a high computational cost for real-time applications. For many problems such as object detection and semantic segmentation, we are able to obtain a low-cost computation mask, either from a priori problem knowledge, or from a low-resolution segmentation network. We show that such computation masks can be used to reduce computation in the high-resolution main network. Variants of sparse activation CNNs have previously been explored on small-scale tasks and showed no degradation in terms of object classification accuracy, but often measured gains in terms of theoretical FLOPs without realizing a practical speed-up when compared to highly optimized dense convolution implementations. In this work, we leverage the sparsity structure of computation masks and propose a novel tiling-based sparse convolution algorithm. We verified the effectiveness of our sparse CNN on LiDAR-based 3D object detection, and we report significant wall-clock speed-ups compared to dense convolution without noticeable loss of accuracy",
    "checked": true,
    "id": "3f3a1decfe07b61a63dafc61616c779ed67d9bed",
    "semantic_title": "sbnet: sparse blocks network for fast inference",
    "citation_count": 162,
    "authors": [
      "Mengye Ren",
      "Andrei Pokrovsky",
      "Bin Yang",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Language-Based_Image_Editing_CVPR_2018_paper.html": {
    "title": "Language-Based Image Editing With Recurrent Attentive Models",
    "volume": "main",
    "abstract": "We investigate the problem of Language-Based Image Editing (LBIE). Given a source image and a natural language description, we want to generate a target image by editing the source image based on the description. We propose a generic modeling framework for two sub-tasks of LBIE: language-based image segmentation and image colorization. The framework uses recurrent attentive models to fuse image and language features. Instead of using a fixed step size, we introduce for each region of the image a termination gate to dynamically determine after each inference step whether to continue extrapolating additional information from the textual description. The effectiveness of the framework is validated on three datasets. First, we introduce a synthetic dataset, called CoSaL, to evaluate the end-to-end performance of our LBIE system. Second, we show that the framework leads to state-of-the-art performance on image segmentation on the ReferIt dataset. Third, we present the first language-based colorization result on the Oxford-102 Flowers dataset",
    "checked": true,
    "id": "d852da3f126ba2faca380064675a7af3805ec7cb",
    "semantic_title": "language-based image editing with recurrent attentive models",
    "citation_count": 99,
    "authors": [
      "Jianbo Chen",
      "Yelong Shen",
      "Jianfeng Gao",
      "Jingjing Liu",
      "Xiaodong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fong_Net2Vec_Quantifying_and_CVPR_2018_paper.html": {
    "title": "Net2Vec: Quantifying and Explaining How Concepts Are Encoded by Filters in Deep Neural Networks",
    "volume": "main",
    "abstract": "In an effort to understand the meaning of the intermediate representations captured by deep networks, recent papers have tried to associate specific semantic concepts to individual neural network filter responses, where interesting correlations are often found, largely by focusing on extremal filter responses. In this paper, we show that this approach can favor easy-to-interpret cases that are not necessarily representative of the average behavior of a representation. A more realistic but harder-to-study hypothesis is that semantic representations are distributed, and thus filters must be studied in conjunction. In order to investigate this idea while enabling systematic visualization and quantification of multiple filter responses, we introduce the Net2Vec framework, in which semantic concepts are mapped to vectorial embeddings based on corresponding filter responses. By studying such embeddings, we are able to show that 1., in most cases, multiple filters are required to code for a concept, that 2., often filters are not concept specific and help encode multiple concepts, and that 3., compared to single filter activations, filter embeddings are able to better characterize the meaning of a representation and its relationship to other concepts",
    "checked": true,
    "id": "de99fbe728dfd3d337ab13ae27512ab028444c6a",
    "semantic_title": "net2vec: quantifying and explaining how concepts are encoded by filters in deep neural networks",
    "citation_count": 219,
    "authors": [
      "Ruth Fong",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.html": {
    "title": "End-to-End Dense Video Captioning With Masked Transformer",
    "volume": "main",
    "abstract": "Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively",
    "checked": true,
    "id": "35ed258aede3df17ee20a6635364cb5fd2461049",
    "semantic_title": "end-to-end dense video captioning with masked transformer",
    "citation_count": 461,
    "authors": [
      "Luowei Zhou",
      "Yingbo Zhou",
      "Jason J. Corso",
      "Richard Socher",
      "Caiming Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dogan_A_Neural_Multi-Sequence_CVPR_2018_paper.html": {
    "title": "A Neural Multi-Sequence Alignment TeCHnique (NeuMATCH)",
    "volume": "main",
    "abstract": "The alignment of heterogeneous sequential data (video to text) is an important and challenging problem. Standard techniques for this task, including Dynamic Time Warping (DTW) and Conditional Random Fields (CRFs), suffer from inherent drawbacks. Mainly, the Markov assumption implies that, given the immediate past, future alignment decisions are independent of further history. The separation between similarity computation and alignment decision also prevents end-to-end training. In this paper, we propose an end-to-end neural architecture where alignment actions are implemented as moving data between stacks of Long Short-term Memory (LSTM) blocks. This flexible architecture supports a large variety of alignment tasks, including one-to-one, one-to-many, skipping unmatched elements, and (with extensions) non-monotonic alignment. Extensive experiments on semi-synthetic and real datasets show that our algorithm outperforms state-of-the-art baselines",
    "checked": true,
    "id": "f8027791ca64f4270cd86e2deb830a3a7383dcff",
    "semantic_title": "a neural multi-sequence alignment technique (neumatch)",
    "citation_count": 16,
    "authors": [
      "Pelin Dogan",
      "Boyang Li",
      "Leonid Sigal",
      "Markus Gross"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Path_Aggregation_Network_CVPR_2018_paper.html": {
    "title": "Path Aggregation Network for Instance Segmentation",
    "volume": "main",
    "abstract": "The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Yet they are useful and make our PANet reach the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. PANet is also state-of-the-art on MVD and Cityscapes",
    "checked": true,
    "id": "5cc22f65bf4e5c0aa61f3139da832d3a946e15cf",
    "semantic_title": "path aggregation network for instance segmentation",
    "citation_count": 4019,
    "authors": [
      "Shu Liu",
      "Lu Qi",
      "Haifang Qin",
      "Jianping Shi",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Van_Horn_The_INaturalist_Species_CVPR_2018_paper.html": {
    "title": "The INaturalist Species Classification and Detection Dataset",
    "volume": "main",
    "abstract": "Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning",
    "checked": true,
    "id": "05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628",
    "semantic_title": "the inaturalist species classification and detection dataset",
    "citation_count": 1192,
    "authors": [
      "Grant Van Horn",
      "Oisin Mac Aodha",
      "Yang Song",
      "Yin Cui",
      "Chen Sun",
      "Alex Shepard",
      "Hartwig Adam",
      "Pietro Perona",
      "Serge Belongie"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Park_Multimodal_Explanations_Justifying_CVPR_2018_paper.html": {
    "title": "Multimodal Explanations: Justifying Decisions and Pointing to the Evidence",
    "volume": "main",
    "abstract": "Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justifications. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to define and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets define visual and textual justifications of a classification decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justification models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer significant benefits over unimodal approaches",
    "checked": true,
    "id": "ef153ece43ee50f8208f6197f0eaf3d324e4475b",
    "semantic_title": "multimodal explanations: justifying decisions and pointing to the evidence",
    "citation_count": 359,
    "authors": [
      "Dong Huk Park",
      "Lisa Anne Hendricks",
      "Zeynep Akata",
      "Anna Rohrbach",
      "Bernt Schiele",
      "Trevor Darrell",
      "Marcus Rohrbach"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.html": {
    "title": "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation",
    "volume": "main",
    "abstract": "Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks",
    "checked": true,
    "id": "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64",
    "semantic_title": "stargan: unified generative adversarial networks for multi-domain image-to-image translation",
    "citation_count": 3151,
    "authors": [
      "Yunjey Choi",
      "Minje Choi",
      "Munyoung Kim",
      "Jung-Woo Ha",
      "Sunghun Kim",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html": {
    "title": "High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs",
    "volume": "main",
    "abstract": "We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing",
    "checked": true,
    "id": "f0a0c0f0d6a7ff53abea40a8c0c678ed570bf851",
    "semantic_title": "high-resolution image synthesis and semantic manipulation with conditional gans",
    "citation_count": 3426,
    "authors": [
      "Ting-Chun Wang",
      "Ming-Yu Liu",
      "Jun-Yan Zhu",
      "Andrew Tao",
      "Jan Kautz",
      "Bryan Catanzaro"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper.html": {
    "title": "Semi-Parametric Image Synthesis",
    "volume": "main",
    "abstract": "We present a semi-parametric approach to photographic image synthesis from semantic layouts. The approach combines the complementary strengths of parametric and nonparametric techniques. The nonparametric component is a memory bank of image segments constructed from a training set of images. Given a novel semantic layout at test time, the memory bank is used to retrieve photographic references that are provided as source material to a deep network. The synthesis is performed by a deep network that draws on the provided photographic material. Experiments on multiple semantic segmentation datasets show that the presented approach yields considerably more realistic images than recent purely parametric techniques",
    "checked": true,
    "id": "b37b5faa0d32a88c3a31882abe9c0ec221917d48",
    "semantic_title": "semi-parametric image synthesis",
    "citation_count": 162,
    "authors": [
      "Xiaojuan Qi",
      "Qifeng Chen",
      "Jiaya Jia",
      "Vladlen Koltun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_BlockDrop_Dynamic_Inference_CVPR_2018_paper.html": {
    "title": "BlockDrop: Dynamic Inference Paths in Residual Networks",
    "volume": "main",
    "abstract": "Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20% on average, going as high as 36% for some images, while maintaining the same 76.4% top-1 accuracy on ImageNet",
    "checked": true,
    "id": "e16cfe727e27be27115d0f842375c46e7e3f384b",
    "semantic_title": "blockdrop: dynamic inference paths in residual networks",
    "citation_count": 420,
    "authors": [
      "Zuxuan Wu",
      "Tushar Nagarajan",
      "Abhishek Kumar",
      "Steven Rennie",
      "Larry S. Davis",
      "Kristen Grauman",
      "Rogerio Feris"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Interpretable_Convolutional_Neural_CVPR_2018_paper.html": {
    "title": "Interpretable Convolutional Neural Networks",
    "volume": "main",
    "abstract": "This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e., what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github.com/zqs1022/interpretableCNN",
    "checked": true,
    "id": "773ca5c76da50cf6f21553b0f8eee391ac65f9c8",
    "semantic_title": "interpretable convolutional neural networks",
    "citation_count": 693,
    "authors": [
      "Quanshi Zhang",
      "Ying Nian Wu",
      "Song-Chun Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Deep_Cross-Media_Knowledge_CVPR_2018_paper.html": {
    "title": "Deep Cross-Media Knowledge Transfer",
    "volume": "main",
    "abstract": "Cross-media retrieval is a research hotspot in multimedia area, which aims to perform retrieval across different media types such as image and text. The performance of existing methods usually relies on labeled data for model training. However, cross-media data is very labor consuming to collect and label, so how to transfer valuable knowledge in existing data to new data is a key problem towards application. For achieving the goal, this paper proposes deep cross-media knowledge transfer (DCKT) approach, which transfers knowledge from a large-scale cross-media dataset to promote the model training on another small-scale cross-media dataset. The main contributions of DCKT are: (1) Two-level transfer architecture is proposed to jointly minimize the media-level and correlation-level domain discrepancies, which allows two important and complementary aspects of knowledge to be transferred: intra-media semantic and inter-media correlation knowledge. It can enrich the training information and boost the retrieval accuracy. (2) Progressive transfer mechanism is proposed to iteratively select training samples with ascending transfer difficulties, via the metric of cross-media domain consistency with adaptive feedback. It can drive the transfer process to gradually reduce vast cross-media domain discrepancy, so as to enhance the robustness of model training. For verifying the effectiveness of DCKT, we take the large-scale dataset XMediaNet as source domain, and 3 widely-used datasets as target domain for cross-media retrieval. Experimental results show that DCKT achieves promising improvement on retrieval accuracy",
    "checked": true,
    "id": "8771a6e7245537c803e3e03b12beb0025a8790a5",
    "semantic_title": "deep cross-media knowledge transfer",
    "citation_count": 38,
    "authors": [
      "Xin Huang",
      "Yuxin Peng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xie_Interleaved_Structured_Sparse_CVPR_2018_paper.html": {
    "title": "Interleaved Structured Sparse Convolutional Neural Networks",
    "volume": "main",
    "abstract": "In this paper, we study the problem of designing efficient convolutional neural network architectures with the interest in eliminating the redundancy in convolution kernels. In addition to structured sparse kernels, low-rank kernels and the product of low-rank kernels,the product of structured sparse kernels, which is a framework for interpreting the recently-developed interleaved group convolutions (IGC) and its variants (e.g. , Xception), has been attracting increasing interests. Motivated by the observation that the convolutions contained in a group convolution in IGC can be further decomposed in the same manner, we present a modularized building block, {IGC-V2:}interleaved structured sparse convolutions. It generalizes interleaved group convolutions, which is composed of two structured sparse kernels, to the product of more structured sparse kernels, further eliminating the redundancy. We present the complementary condition and the balance condition to guide the design of structured sparse kernels, obtaining a balance between three aspects: model size and computation complexity and classification accuracy. Experimental results demonstrate the advantage on the balance between these three aspects compared to interleaved group convolutions and Xception and competitive performance with other state-of-the-art architecture design methods",
    "checked": true,
    "id": "2fef9c616bc2b54c732939cffed3d1789d5f499a",
    "semantic_title": "interleaved structured sparse convolutional neural networks",
    "citation_count": 96,
    "authors": [
      "Guotian Xie",
      "Jingdong Wang",
      "Ting Zhang",
      "Jianhuang Lai",
      "Richang Hong",
      "Guo-Jun Qi"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Esser_A_Variational_U-Net_CVPR_2018_paper.html": {
    "title": "A Variational U-Net for Conditional Appearance and Shape Generation",
    "volume": "main",
    "abstract": "Deep generative models have demonstrated great performance in image synthesis. However, results deteriorate in case of spatial deformations, since they generate images of objects directly, rather than modeling the intricate interplay of their inherent shape and appearance. We present a conditional U-Net for shape-guided image generation, conditioned on the output of a variational autoencoder for appearance. The approach is trained end-to-end on images, without requiring samples of the same object with varying pose or appearance. Experiments show that the model enables conditional image generation and transfer. Therefore, either shape or appearance can be retained from a query image, while freely altering the other. Moreover, appearance can be sampled due to its stochastic latent representation, while preserving shape. In quantitative and qualitative experiments on COCO, DeepFashion, shoes, Market-1501 and handbags, the approach demonstrates significant improvements over the state-of-the-art",
    "checked": true,
    "id": "5fdb3533152f9862e3e4c2282cd5f1400af18956",
    "semantic_title": "a variational u-net for conditional appearance and shape generation",
    "citation_count": 394,
    "authors": [
      "Patrick Esser",
      "Ekaterina Sutter",
      "Björn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Detach_and_Adapt_CVPR_2018_paper.html": {
    "title": "Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation",
    "volume": "main",
    "abstract": "While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods",
    "checked": true,
    "id": "486ee9d420fd1eba1006221d3d61931563bc4115",
    "semantic_title": "detach and adapt: learning cross-domain disentangled deep representation",
    "citation_count": 109,
    "authors": [
      "Yen-Cheng Liu",
      "Yu-Ying Yeh",
      "Tzu-Chien Fu",
      "Sheng-De Wang",
      "Wei-Chen Chiu",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Marcos_Learning_Deep_Structured_CVPR_2018_paper.html": {
    "title": "Learning Deep Structured Active Contours End-to-End",
    "volume": "main",
    "abstract": "The world is covered with millions of buildings, and precisely knowing each instance's position and extents is vital to a multitude of applications. Recently, automated building footprint segmentation models have shown superior detection accuracy thanks to the usage of Convolutional Neural Networks (CNN). However, even the latest evolutions struggle to precisely delineating borders, which often leads to geometric distortions and inadvertent fusion of adjacent building instances. We propose to overcome this issue by exploiting the distinct geometric properties of buildings. To this end, we present Deep Structured Active Contours (DSAC), a novel framework that integrates priors and constraints into the segmentation process, such as continuous boundaries, smooth edges, and sharp corners. To do so, DSAC employs Active Contour Models (ACM), a family of constraint- and prior-based polygonal models. We learn ACM parameterizations per instance using a CNN, and show how to incorporate all components in a structured output model, making DSAC trainable end-to-end. We evaluate DSAC on three challenging building instance segmentation datasets, where it compares favorably against state-of-the-art. Code will be made available",
    "checked": true,
    "id": "b6472a40ef83ca64330b9472e9704fabde00d109",
    "semantic_title": "learning deep structured active contours end-to-end",
    "citation_count": 133,
    "authors": [
      "Diego Marcos",
      "Devis Tuia",
      "Benjamin Kellenberger",
      "Lisa Zhang",
      "Min Bai",
      "Renjie Liao",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lambert_Deep_Learning_Under_CVPR_2018_paper.html": {
    "title": "Deep Learning Under Privileged Information Using Heteroscedastic Dropout",
    "volume": "main",
    "abstract": "Unlike machines, humans learn through rapid, abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (ie. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in sample efficiency by providing a generalization error bound decreasing with O(1/n), where n is the number of training examples, in an oracle case",
    "checked": true,
    "id": "4571aa4e5cc335fd71f4fdce13c9a80419b90aaa",
    "semantic_title": "deep learning under privileged information using heteroscedastic dropout",
    "citation_count": 78,
    "authors": [
      "John Lambert",
      "Ozan Sener",
      "Silvio Savarese"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_Smooth_Neighbors_on_CVPR_2018_paper.html": {
    "title": "Smooth Neighbors on Teacher Graphs for Semi-Supervised Learning",
    "volume": "main",
    "abstract": "The recently proposed self-ensembling methods have achieved promising results in deep semi-supervised learning, which penalize inconsistent predictions of unlabeled data under different perturbations. However, they only consider adding perturbations to each single data point, while ignoring the connections between data samples. In this paper, we propose a novel method, called Smooth Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on the predictions of the teacher model, i.e., the implicit self-ensemble of models. Then the graph serves as a similarity measure with respect to which the representations of \"similar\" neighboring points are learned to be smooth on the low-dimensional manifold. We achieve state-of-the-art results on semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular, the improvements are significant when the labels are fewer. For the non-augmented MNIST with only 20 labels, the error rate is reduced from previous 4.81% to 1.36%. Our method also shows robustness to noisy labels",
    "checked": true,
    "id": "90bdcd465ea88f6fe19039f142871cc9a3500c99",
    "semantic_title": "smooth neighbors on teacher graphs for semi-supervised learning",
    "citation_count": 223,
    "authors": [
      "Yucen Luo",
      "Jun Zhu",
      "Mengxi Li",
      "Yong Ren",
      "Bo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Interpret_Neural_Networks_CVPR_2018_paper.html": {
    "title": "Interpret Neural Networks by Identifying Critical Data Routing Paths",
    "volume": "main",
    "abstract": "Interpretability of a deep neural network aims to explain the rationale behind its decisions and enable the users to understand the intelligent agents, which has become an important issue due to its importance in practical applications. To address this issue, we develop a Distillation Guided Routing method, which is a flexible framework to interpret a deep neural network by identifying critical data routing paths and analyzing the functional processing behavior of the corresponding layers. Specifically, we propose to discover the critical nodes on the data routing paths during network inferring prediction for individual input samples by learning associated control gates for each layer's output channel. The routing paths can, therefore, be represented based on the responses of concatenated control gates from all the layers, which reflect the network's semantic selectivity regarding to the input patterns and more detailed functional process across different layer levels. Based on the discoveries, we propose an adversarial sample detection algorithm by learning a classifier to discriminate whether the critical data routing paths are from real or adversarial samples. Experiments demonstrate that our algorithm can effectively achieve high defense rate with minor training overhead",
    "checked": true,
    "id": "17b8e0e75c5cf180811522d822262411b60fc489",
    "semantic_title": "interpret neural networks by identifying critical data routing paths",
    "citation_count": 83,
    "authors": [
      "Yulong Wang",
      "Hang Su",
      "Bo Zhang",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chandra_Deep_Spatio-Temporal_Random_CVPR_2018_paper.html": {
    "title": "Deep Spatio-Temporal Random Fields for Efficient Video Segmentation",
    "volume": "main",
    "abstract": "In this work we introduce a time- and memory-efficient method for structured prediction that couples neuron decisions across both space at time. We show that we are able to perform exact and efficient inference on a densely connected spatio-temporal graph by capitalizing on recent advances on deep Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a) efficient, (b) has a unique global minimum, and (c) can be trained end-to-end alongside contemporary deep networks for video understanding. We experiment with multiple connectivity patterns in the temporal domain, and present empirical improvements over strong baselines on the tasks of both semantic and instance segmentation of videos. Our implementation is based on the Caffe2 framework and will be available at https://github.com/siddharthachandra/gcrf-v3.0",
    "checked": true,
    "id": "2f8d2fda28488d55fbd0a0ea4d613b3e5f20e3d9",
    "semantic_title": "deep spatio-temporal random fields for efficient video segmentation",
    "citation_count": 55,
    "authors": [
      "Siddhartha Chandra",
      "Camille Couprie",
      "Iasonas Kokkinos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Shin_Customized_Image_Narrative_CVPR_2018_paper.html": {
    "title": "Customized Image Narrative Generation via Interactive Visual Question Generation and Answering",
    "volume": "main",
    "abstract": "Image description task has been invariably examined in a static manner with qualitative presumptions held to be universally applicable, regardless of the scope or target of the description. In practice, however, different viewers may pay attention to different aspects of the image, and yield different descriptions or interpretations under various contexts. Such diversity in perspectives is difficult to derive with conventional image description techniques. In this paper, we propose a customized image narrative generation task, in which the users are interactively engaged in the generation process by providing answers to the questions. We further attempt to learn the user's interest via repeating such interactive stages, and to automatically reflect the interest in descriptions for new images. Experimental results demonstrate that our model can generate a variety of descriptions from single image that cover a wider range of topics than conventional models, while being customizable to the target user of interaction",
    "checked": true,
    "id": "09efde3bd0a380e8cbcd55a13694648276c2c166",
    "semantic_title": "customized image narrative generation via interactive visual question generation and answering",
    "citation_count": 6,
    "authors": [
      "Andrew Shin",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_PWC-Net_CNNs_for_CVPR_2018_paper.html": {
    "title": "PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",
    "volume": "main",
    "abstract": "We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the current optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on our project website",
    "checked": true,
    "id": "54c802e7dadac855de06725bf146cdf20319e46b",
    "semantic_title": "pwc-net: cnns for optical flow using pyramid, warping, and cost volume",
    "citation_count": 2062,
    "authors": [
      "Deqing Sun",
      "Xiaodong Yang",
      "Ming-Yu Liu",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_Revisiting_Deep_Intrinsic_CVPR_2018_paper.html": {
    "title": "Revisiting Deep Intrinsic Image Decompositions",
    "volume": "main",
    "abstract": "While invaluable for many computer vision applications, decomposing a natural image into intrinsic reflectance and shading layers represents a challenging, underdetermined inverse problem. As opposed to strict reliance on conventional optimization or filtering solutions with strong prior assumptions, deep learning based approaches have also been proposed to compute intrinsic image decompositions when granted access to sufficient labeled training data. The downside is that current data sources are quite limited, and broadly speaking fall into one of two categories: either dense fully-labeled images in synthetic/narrow settings, or weakly-labeled data from relatively diverse natural scenes. In contrast to many previous learning-based approaches, which are often tailored to the structure of a particular dataset (and may not work well on others), we adopt core network structures that universally reflect loose prior knowledge regarding the intrinsic image formation process and can be largely shared across datasets. We then apply flexibly supervised loss layers that are customized for each source of ground truth labels. The resulting deep architecture achieves state-of-the-art results on all of the major intrinsic image benchmarks, and runs considerably faster than most at test time",
    "checked": true,
    "id": "9700d39d92d57c49753ba3b15f12093e01480f8e",
    "semantic_title": "revisiting deep intrinsic image decompositions",
    "citation_count": 102,
    "authors": [
      "Qingnan Fan",
      "Jiaolong Yang",
      "Gang Hua",
      "Baoquan Chen",
      "David Wipf"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yellin_Multi-Cell_Detection_and_CVPR_2018_paper.html": {
    "title": "Multi-Cell Detection and Classification Using a Generative Convolutional Model",
    "volume": "main",
    "abstract": "Detecting, counting, and classifying various cell types in images of human blood is important in many biomedical applications. However, these tasks can be very difficult due to the wide range of biological variability and the resolution limitations of many imaging modalities. This paper proposes a new approach to detecting, counting and classifying white blood cell populations in holographic images, which capitalizes on the fact that the variability in a mixture of blood cells is constrained by physiology. The proposed approach is based on a probabilistic generative model that describes an image of a population of cells as the sum of atoms from a convolutional dictionary of cell templates. The class of each template is drawn from a prior distribution that captures statistical information about blood cell mixtures. The parameters of the prior distribution are learned from a database of complete blood count results obtained from patients, and the cell templates are learned from images of purified cells from a single cell class using an extension of convolutional dictionary learning. Cell detection, counting and classification is then done using an extension of convolutional sparse coding that accounts for class proportion priors. This method has been successfully used to detect, count and classify white blood cell populations in holographic images of lysed blood obtained from 20 normal blood donors and 12 abnormal clinical blood discard samples. The error from our method is under 6.8% for all class populations, compared to errors of over 28.6% for all other methods tested",
    "checked": true,
    "id": "6e5f34ab5189b5741c78347547e5e294c373f4ea",
    "semantic_title": "multi-cell detection and classification using a generative convolutional model",
    "citation_count": 4,
    "authors": [
      "Florence Yellin",
      "Benjamin D. Haeffele",
      "Sophie Roth",
      "René Vidal"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Learning_Spatial-Aware_Regressions_CVPR_2018_paper.html": {
    "title": "Learning Spatial-Aware Regressions for Visual Tracking",
    "volume": "main",
    "abstract": "In this paper, we analyze the spatial information of deep features, and propose two complementary regressions for robust visual tracking. First, we propose a kernelized ridge regression model wherein the kernel value is defined as the weighted sum of similarity scores of all pairs of patches between two samples. We show that this model can be formulated as a neural network and thus can be efficiently solved. Second, we propose a fully convolutional neural network with spatially regularized kernels, through which the filter kernel corresponding to each output channel is forced to focus on a specific region of the target. Distance transform pooling is further exploited to determine the effectiveness of each output channel of the convolution layer. The outputs from the kernelized ridge regression model and the fully convolutional neural network are combined to obtain the ultimate response. Experimental results on two benchmark datasets validate the effectiveness of the proposed method",
    "checked": true,
    "id": "73bfe7a27e63a1308d667a3b6c2e5ba3c2a7c5bf",
    "semantic_title": "learning spatial-aware regressions for visual tracking",
    "citation_count": 195,
    "authors": [
      "Chong Sun",
      "Dong Wang",
      "Huchuan Lu",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_High_Performance_Visual_CVPR_2018_paper.html": {
    "title": "High Performance Visual Tracking With Siamese Region Proposal Network",
    "volume": "main",
    "abstract": "Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. In the inference phase, the proposed framework is formulated as a local one-shot detection task. We can pre-compute the template branch of the Siamese subnetwork and formulate the correlation layers as trivial convolution layers to perform online tracking. Benefit from the proposal refinement, traditional multi-scale test and online fine-tuning can be discarded. The Siamese-RPN runs at 160 FPS while achieving leading performance in VOT2015, VOT2016 and VOT2017 real-time challenges",
    "checked": true,
    "id": "320d05db95ab42ade69294abe46cd1aca6aca602",
    "semantic_title": "high performance visual tracking with siamese region proposal network",
    "citation_count": 1861,
    "authors": [
      "Bo Li",
      "Junjie Yan",
      "Wei Wu",
      "Zheng Zhu",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper.html": {
    "title": "LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation",
    "volume": "main",
    "abstract": "FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that attains performance on par with FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet",
    "checked": true,
    "id": "3dd6a8a12ed9e8359c8e5a37db63c964646c8e99",
    "semantic_title": "supplementary material for liteflownet: a lightweight convolutional neural network for optical flow estimation",
    "citation_count": 417,
    "authors": [
      "Tak-Wai Hui",
      "Xiaoou Tang",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Song_VITAL_VIsual_Tracking_CVPR_2018_paper.html": {
    "title": "VITAL: VIsual Tracking via Adversarial Learning",
    "volume": "main",
    "abstract": "The tracking-by-detection framework consists of two stages, i.e., drawing samples around the target object in the first stage and classifying each sample as the target object or as background in the second stage. The performance of existing tracking-by-detection trackers using deep classification networks is limited by two aspects. First, the positive samples in each frame are highly spatially overlapped, and they fail to capture rich appearance variations. Second, there exists severe class imbalance between positive and negative samples. This paper presents the VITAL algorithm to address these two problems via adversarial learning. To augment positive samples, we use a generative network to randomly generate masks, which are applied to input features to capture a variety of appearance changes. With the use of adversarial learning, our network identifies the mask that maintains the most robust features of the target objects over a long temporal span. In addition, to handle the issue of class imbalance, we propose a high-order cost sensitive loss to decrease the effect of easy negative samples to facilitate training the classification network. Extensive experiments on benchmark datasets demonstrate that the proposed tracker performs favorably against state-of-the-art approaches",
    "checked": true,
    "id": "7cbad516a393ca808d6bb30e2b49cc82628af1e5",
    "semantic_title": "vital: visual tracking via adversarial learning",
    "citation_count": 476,
    "authors": [
      "Yibing Song",
      "Chao Ma",
      "Xiaohe Wu",
      "Lijun Gong",
      "Linchao Bao",
      "Wangmeng Zuo",
      "Chunhua Shen",
      "Rynson W.H. Lau",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jiang_Super_SloMo_High_CVPR_2018_paper.html": {
    "title": "Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation",
    "volume": "main",
    "abstract": "Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. To train our network, we use 1,132 240-fps video clips, containing 300K individual video frames. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods",
    "checked": true,
    "id": "baa1ae74fbf7ed6204f2f6364d51375ff81aabc1",
    "semantic_title": "super slomo: high quality estimation of multiple intermediate frames for video interpolation",
    "citation_count": 664,
    "authors": [
      "Huaizu Jiang",
      "Deqing Sun",
      "Varun Jampani",
      "Ming-Hsuan Yang",
      "Erik Learned-Miller",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Runia_Real-World_Repetition_Estimation_CVPR_2018_paper.html": {
    "title": "Real-World Repetition Estimation by Div, Grad and Curl",
    "volume": "main",
    "abstract": "We consider the problem of estimating repetition in video, such as performing push-ups, cutting a melon or playing violin. Existing work shows good results under the assumption of static and stationary periodicity. As realistic video is rarely perfectly static and stationary, the often preferred Fourier-based measurements is inapt. Instead, we adopt the wavelet transform to better handle non-static and non-stationary video dynamics. From the flow field and its differentials, we derive three fundamental motion types and three motion continuities of intrinsic periodicity in 3D. On top of this, the 2D perception of 3D periodicity considers two extreme viewpoints. What follows are 18 fundamental cases of recurrent perception in 2D. In practice, to deal with the variety of repetitive appearance, our theory implies measuring time-varying flow and its differentials (gradient, divergence and curl) over segmented foreground motion. For experiments, we introduce the new QUVA Repetition dataset, reflecting reality by including non-static and non-stationary videos. On the task of counting repetitions in video, we obtain favorable results compared to a deep learning alternative",
    "checked": true,
    "id": "34c0b795b0acd9874c9511d73515c83b82e8d357",
    "semantic_title": "real-world repetition estimation by div, grad and curl",
    "citation_count": 46,
    "authors": [
      "Tom F. H. Runia",
      "Cees G. M. Snoek",
      "Arnold W. M. Smeulders"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Kong_Recurrent_Pixel_Embedding_CVPR_2018_paper.html": {
    "title": "Recurrent Pixel Embedding for Instance Grouping",
    "volume": "main",
    "abstract": "We introduce a differentiable, end-to-end trainable framework for solving pixel-level grouping problems such as instance segmentation consisting of two novel components. First, we regress pixels into a hyper-spherical embedding space so that pixels from the same group have high cosine similarity while those from different groups have similarity below a specified margin. We analyze the choice of embedding dimension and margin, relating them to theoretical results on the problem of distributing points uniformly on the sphere. Second, to group instances, we utilize a variant of mean-shift clustering, implemented as a recurrent neural network parameterized by kernel bandwidth. This recurrent grouping module is differentiable, enjoys convergent dynamics and probabilistic interpretability. Backpropagating the group-weighted loss through this module allows learning to focus on correcting embedding errors that won't be resolved during subsequent clustering. Our framework, while conceptually simple and theoretically abundant, is also practically effective and computationally efficient. We demonstrate substantial improvements over state-of-the-art instance segmentation for object proposal generation, as well as demonstrating the benefits of grouping loss for classification tasks such as boundary detection and semantic segmentation",
    "checked": true,
    "id": "35502af359aa60ae8047df172e29503cfb29c3f9",
    "semantic_title": "recurrent pixel embedding for instance grouping",
    "citation_count": 162,
    "authors": [
      "Shu Kong",
      "Charless C. Fowlkes"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Unsupervised_Saliency_CVPR_2018_paper.html": {
    "title": "Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective",
    "volume": "main",
    "abstract": "The success of current deep saliency detection methods heavily depends on the availability of large-scale supervision in the form of per-pixel labeling. Such supervision, while labor-intensive and not always possible, tends to hinder the generalization ability of the learned models. By contrast, traditional handcrafted features based unsupervised saliency detection methods, even though have been surpassed by the deep supervised methods, are generally dataset-independent and could be applied in the wild. This raises a natural question that ``Is it possible to learn saliency maps without using labeled data while improving the generalization ability?''. To this end, we present a novel perspective to unsupervised saliency detection through learning from multiple noisy labeling generated by ``weak'' and ``noisy'' unsupervised handcrafted saliency methods. Our end-to-end deep learning framework for unsupervised saliency detection consists of a latent saliency prediction module and a noise modeling module that work collaboratively and are optimized jointly. Explicit noise modeling enables us to deal with noisy saliency maps in a probabilistic way. Extensive experimental results on various benchmarking datasets show that our model not only outperforms all the unsupervised saliency methods with a large margin but also achieves comparable performance with the recent state-of-the-art supervised deep saliency methods",
    "checked": true,
    "id": "9f295c2dd41c9dec1c0e866c1701b4cf48ff807d",
    "semantic_title": "deep unsupervised saliency detection: a multiple noisy labeling perspective",
    "citation_count": 166,
    "authors": [
      "Jing Zhang",
      "Tong Zhang",
      "Yuchao Dai",
      "Mehrtash Harandi",
      "Richard Hartley"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Learning_Intrinsic_Image_CVPR_2018_paper.html": {
    "title": "Learning Intrinsic Image Decomposition From Watching the World",
    "volume": "main",
    "abstract": "Single-view intrinsic image decomposition is a highly ill-posed problem, making learning from large amounts of data an attractive approach. However, it is difficult to collect ground truth training data at scale for intrinsic images. In this paper, we explore a different approach to learning intrinsic images: observing image sequences over time depicting the same scene under changing illumination, and learning single-view decompositions that are consistent with these changes. This approach allows us to learn without ground truth decompositions, and instead to exploit information available from multiple images. Our trained model can then be applied at test time to single views. We describe a new learning framework based on this idea, including new loss functions that can be efficiently evaluated over entire sequences. While prior learning-based intrinsic image methods achieve good performance on specific benchmarks, we show that our approach generalizes well to several diverse datasets, including MIT intrinsic images, Intrinsic Images in the Wild and Shading Annotations in the Wild",
    "checked": true,
    "id": "70cefe655a7f9dae0948b2102871661666dc1be8",
    "semantic_title": "learning intrinsic image decomposition from watching the world",
    "citation_count": 151,
    "authors": [
      "Zhengqi Li",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_TieNet_Text-Image_Embedding_CVPR_2018_paper.html": {
    "title": "TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-Rays",
    "volume": "main",
    "abstract": "Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reports can be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together. The classification results are significantly improved (6% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI)",
    "checked": true,
    "id": "b1b2402dcd85b81381fde40d0b971b510471ef23",
    "semantic_title": "tienet: text-image embedding network for common thorax disease classification and reporting in chest x-rays",
    "citation_count": 398,
    "authors": [
      "Xiaosong Wang",
      "Yifan Peng",
      "Le Lu",
      "Zhiyong Lu",
      "Ronald M. Summers"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Teixeira_Generating_Synthetic_X-Ray_CVPR_2018_paper.html": {
    "title": "Generating Synthetic X-Ray Images of a Person From the Surface Geometry",
    "volume": "main",
    "abstract": "We present a novel framework that learns to predict human anatomy from body surface. Specifically, our approach generates a synthetic X-ray image of a person only from the person's surface geometry. Furthermore, the synthetic X-ray image is parametrized and can be manipulated by adjusting a set of body markers which are also generated during the X-ray image prediction. With the proposed framework, multiple synthetic X-ray images can easily be generated by varying surface geometry. By perturbing the parameters, several additional synthetic X-ray images can be generated from the same surface geometry. As a result, our approach offers a potential to overcome the training data barrier in the medical domain. This capability is achieved by learning a pair of networks - one learns to generate the full image from the partial image and a set of parameters, and the other learns to estimate the parameters given the full image. During training, the two networks are trained iteratively such that they would converge to a solution where the predicted parameters and the full image are consistent with each other. In addition to medical data enrichment, our framework can also be used for image completion as well as anomaly detection",
    "checked": true,
    "id": "c23eb32c137f436ba294c16d67f01549f344e3f4",
    "semantic_title": "generating synthetic x-ray images of a person from the surface geometry",
    "citation_count": 18,
    "authors": [
      "Brian Teixeira",
      "Vivek Singh",
      "Terrence Chen",
      "Kai Ma",
      "Birgi Tamersoy",
      "Yifan Wu",
      "Elena Balashova",
      "Dorin Comaniciu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Xia_Gibson_Env_Real-World_CVPR_2018_paper.html": {
    "title": "Gibson Env: Real-World Perception for Embodied Agents",
    "volume": "main",
    "abstract": "Perception and being active (having a certain level of motion freedom) are closely tied. Learning active perception and sensorimotor control in the physical world is cumbersome as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning in simulation which consequently casts a question on transferring to real-world. In this paper, we investigate learning a real-world perception for active agents, propose Gibson virtual environment for this purpose, and showcase a set of learned complex locomotion abilities. The primary characteristics of the learning environments, which transfer into the trained agents, are I) being from the real-world and reflecting its semantic complexity, II) having a mechanism to ensure no need to further domain adaptation prior to deployment of results in real-world, III) embodiment of the agent and making it subject to constraints of space and physics",
    "checked": true,
    "id": "e89a4fe6e8286eccedd702216153f0f248adb151",
    "semantic_title": "gibson env: real-world perception for embodied agents",
    "citation_count": 660,
    "authors": [
      "Fei Xia",
      "Amir R. Zamir",
      "Zhiyang He",
      "Alexander Sax",
      "Jitendra Malik",
      "Silvio Savarese"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Han_Reinforcement_Cutting-Agent_Learning_CVPR_2018_paper.html": {
    "title": "Reinforcement Cutting-Agent Learning for Video Object Segmentation",
    "volume": "main",
    "abstract": "Video object segmentation is a fundamental yet challenging task in computer vision community. In this paper, we formulate this problem as a Markov Decision Process, where agents are learned to segment object regions under a deep reinforcement learning framework. Essentially, learning agents for segmentation is nontrivial as segmentation is a nearly continuous decision-making process, where the number of the involved agents (pixels or superpixels) and action steps from the seed (super)pixels to the whole object mask might be incredibly huge. To overcome this difficulty, this paper simplifies the learning of segmentation agents to the learning of a cutting-agent, which only has a limited number of action units and can converge in just a few action steps. The basic assumption is that object segmentation mainly relies on the interaction between object regions and their context. Thus, with an optimal object (box) region and context (box) region, we can obtain the desirable segmentation mask through further inference. Based on this assumption, we establish a novel reinforcement cutting-agent learning framework, where the cutting-agent consists of a cutting-policy network and a cutting-execution network. The former learns policies for deciding optimal object-context box pair, while the latter executing the cutting function based on the inferred object-context box pair. With the collaborative interaction between the two networks, our method can achieve the outperforming VOS performance on two public benchmarks, which demonstrates the rationality of our assumption as well as the effectiveness of the proposed learning framework",
    "checked": true,
    "id": "dd5878b1eab85dffc89cd31eb5fdc34710eef75f",
    "semantic_title": "reinforcement cutting-agent learning for video object segmentation",
    "citation_count": 80,
    "authors": [
      "Junwei Han",
      "Le Yang",
      "Dingwen Zhang",
      "Xiaojun Chang",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Feature_Space_Transfer_CVPR_2018_paper.html": {
    "title": "Feature Space Transfer for Data Augmentation",
    "volume": "main",
    "abstract": "The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder/decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one/few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods",
    "checked": true,
    "id": "f3ee8dcaaad5f47f347354fe5d740096097cbed5",
    "semantic_title": "feature space transfer for data augmentation",
    "citation_count": 81,
    "authors": [
      "Bo Liu",
      "Xudong Wang",
      "Mandar Dixit",
      "Roland Kwitt",
      "Nuno Vasconcelos"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bibi_Analytic_Expressions_for_CVPR_2018_paper.html": {
    "title": "Analytic Expressions for Probabilistic Moments of PL-DNN With Gaussian Input",
    "volume": "main",
    "abstract": "The outstanding performance of deep neural networks (DNNs), for the visual recognition task in particular, has been demonstrated on several large-scale benchmarks. This performance has immensely strengthened the line of re- search that aims to understand and analyze the driving reasons behind the effectiveness of these networks. One important aspect of this analysis has recently gained much attention, namely the reaction of a DNN to noisy input. This has spawned research on developing adversarial input attacks as well as training strategies that make DNNs more robust against these attacks. To this end, we derive in this pa- per exact analytic expressions for the first and second moments (mean and variance) of a small piecewise linear (PL) network (Affine, ReLU, Affine) subject to general Gaussian input. We experimentally show that these expressions are tight under simple linearizations of deeper PL-DNNs, especially popular architectures in the literature (e.g. LeNet and AlexNet). Extensive experiments on image classification show that these expressions can be used to study the behaviour of the output mean of the logits for each class, the interclass confusion and the pixel-level spatial noise sensitivity of the network. Moreover, we show how these expressions can be used to systematically construct targeted and non-targeted adversarial attacks",
    "checked": true,
    "id": "d7439e5cbce3574730f9f95558b29f6dac62830d",
    "semantic_title": "analytic expressions for probabilistic moments of pl-dnn with gaussian input",
    "citation_count": 31,
    "authors": [
      "Adel Bibi",
      "Modar Alfadly",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Saeedan_Detail-Preserving_Pooling_in_CVPR_2018_paper.html": {
    "title": "Detail-Preserving Pooling in Deep Networks",
    "volume": "main",
    "abstract": "Most convolutional neural networks use some method for gradually downscaling the size of the hidden layers. This is commonly referred to as pooling, and is applied to reduce the number of parameters, improve invariance to certain distortions, and increase the receptive field size. Since pooling by nature is a lossy process, it is crucial that each such layer maintains the portion of the activations that is most important for the network's discriminability. Yet, simple maximization or averaging over blocks, max or average pooling, or plain downsampling in the form of strided convolutions are the standard. In this paper, we aim to leverage recent results on image downscaling for the purposes of deep learning. Inspired by the human visual system, which focuses on local spatial changes, we propose detail-preserving pooling (DPP), an adaptive pooling method that magnifies spatial changes and preserves important structural detail. Importantly, its parameters can be learned jointly with the rest of the network. We analyze some of its theoretical properties and show its empirical benefits on several datasets and networks, where DPP consistently outperforms previous pooling approaches",
    "checked": true,
    "id": "a651aaf02be93392a628656e1ba3aa2da4ab32ce",
    "semantic_title": "detail-preserving pooling in deep networks",
    "citation_count": 86,
    "authors": [
      "Faraz Saeedan",
      "Nicolas Weber",
      "Michael Goesele",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wan_Rethinking_Feature_Distribution_CVPR_2018_paper.html": {
    "title": "Rethinking Feature Distribution for Loss Functions in Image Classification",
    "volume": "main",
    "abstract": "We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural networks in classification tasks. Different from the softmax cross-entropy loss, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. By involving a classification margin and a likelihood regularization, the L-GM loss facilitates both a high classification performance and an accurate modeling of the training feature distribution. As such, the L-GM loss is superior to the softmax loss and its major variants in the sense that besides classification, it can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on their features' likelihood to the training feature distribution. Extensive experiments on various recognition benchmarks like MNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate the effectiveness of our proposal",
    "checked": true,
    "id": "b6b24dfaf4c9e498ca9b9ee9f82d8d0c5bdb77e9",
    "semantic_title": "rethinking feature distribution for loss functions in image classification",
    "citation_count": 148,
    "authors": [
      "Weitao Wan",
      "Yuanyi Zhong",
      "Tianpeng Li",
      "Jiansheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Shift_A_Zero_CVPR_2018_paper.html": {
    "title": "Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions",
    "volume": "main",
    "abstract": "Neural networks rely on convolutions to aggregate spatial information. However, spatial convolutions are expensive in terms of model size and computation, both of which grow quadratically with respect to kernel size. In this paper, we present a parameter-free, FLOP-free \"shift\" operation as an alternative to spatial convolutions. We fuse shifts and point-wise convolutions to construct end-to-end trainable shift-based modules, with a hyperparameter characterizing the tradeoff between accuracy and efficiency. To demonstrate the operation's efficacy, we replace ResNet's 3x3 convolutions with shift-based modules for improved CIFAR-10 and CIFAR-100 accuracy using 60% fewer parameters; we additionally demonstrate the operation's resilience to parameter reduction on ImageNet, outperforming ResNet family members despite having millions fewer parameters. We further design a family of neural networks called ShiftNet, which achieve strong performance on classification, face verification and style transfer while demanding many fewer parameters",
    "checked": true,
    "id": "70c810ba62c5ee40d611e134b2ac2ca61c4de16b",
    "semantic_title": "shift: a zero flop, zero parameter alternative to spatial convolutions",
    "citation_count": 321,
    "authors": [
      "Bichen Wu",
      "Alvin Wan",
      "Xiangyu Yue",
      "Peter Jin",
      "Sicheng Zhao",
      "Noah Golmant",
      "Amir Gholaminejad",
      "Joseph Gonzalez",
      "Kurt Keutzer"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Sketch-a-Classifier_Sketch-Based_Photo_CVPR_2018_paper.html": {
    "title": "Sketch-a-Classifier: Sketch-Based Photo Classifier Generation",
    "volume": "main",
    "abstract": "Contemporary deep learning techniques have made image recognition a reasonably reliable technology. However training effective photo classifiers typically takes numerous examples which limits image recognition's scalability and applicability to scenarios where images may not be available. This has motivated investigation into zero-shot learning, which addresses the issue via knowledge transfer from other modalities such as text. In this paper we investigate an alternative approach of synthesizing image classifiers: almost directly from a user's imagination, via free-hand sketch. This approach doesn't require the category to be nameable or describable via attributes as per zero-shot learning. We achieve this via training a model regression network to map from free-hand sketch space to the space of photo classifiers. It turns out that this mapping can be learned in a category-agnostic way, allowing photo classifiers for new categories to be synthesized by user with no need for annotated training photos. We also demonstrate that this modality of classifier generation can also be used to enhance the granularity of an existing photo classifier, or as a complement to name-based zero-shot learning",
    "checked": true,
    "id": "b6922c5ef4760b92cae2bcb164927708e75a7b22",
    "semantic_title": "sketch-a-classifier: sketch-based photo classifier generation",
    "citation_count": 25,
    "authors": [
      "Conghui Hu",
      "Da Li",
      "Yi-Zhe Song",
      "Tao Xiang",
      "Timothy M. Hospedales"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Alperovich_Light_Field_Intrinsics_CVPR_2018_paper.html": {
    "title": "Light Field Intrinsics With a Deep Encoder-Decoder Network",
    "volume": "main",
    "abstract": "We present a fully convolutional autoencoder for light fields, which jointly encodes stacks of horizontal and vertical epipolar plane images through a deep network of residual layers. The complex structure of the light field is thus reduced to a comparatively low-dimensional representation, which can be decoded in a variety of ways. The different pathways of upconvolution we currently support are for disparity estimation and separation of the lightfield into diffuse and specular intrinsic components. The key idea is that we can jointly perform unsupervised training for the autoencoder path of the network, and supervised training for the other decoders. This way, we find features which are both tailored to the respective tasks and generalize well to datasets for which only example light fields are available. We provide an extensive evaluation on synthetic light field data, and show that the network yields good results on previously unseen real world data captured by a Lytro Illum camera and various gantries",
    "checked": true,
    "id": "161b529a9d76075ec0c8b8aa1fc3e601077ba800",
    "semantic_title": "light field intrinsics with a deep encoder-decoder network",
    "citation_count": 52,
    "authors": [
      "Anna Alperovich",
      "Ole Johannsen",
      "Michael Strecke",
      "Bastian Goldluecke"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Gao_Learning_Generative_ConvNets_CVPR_2018_paper.html": {
    "title": "Learning Generative ConvNets via Multi-Grid Modeling and Sampling",
    "volume": "main",
    "abstract": "This paper proposes a multi-grid method for learning energy-based generative ConvNet models of images. For each grid, we learn an energy-based probabilistic model where the energy function is defined by a bottom-up convolutional neural network (ConvNet or CNN). Learning such a model requires generating synthesized examples from the model. Within each iteration of our learning algorithm, for each observed training image, we generate synthesized images at multiple grids by initializing the finite-step MCMC sampling from a minimal 1 x 1 version of the training image. The synthesized image at each subsequent grid is obtained by a finite-step MCMC initialized from the synthesized image generated at the previous coarser grid. After obtaining the synthesized examples, the parameters of the models at multiple grids are updated separately and simultaneously based on the differences between synthesized and observed examples. We show that this multi-grid method can learn realistic energy-based generative ConvNet models, and it outperforms the original contrastive divergence (CD) and persistent CD",
    "checked": true,
    "id": "c62e11dee8c687a226cfb398285246e8f938391b",
    "semantic_title": "learning generative convnets via multi-grid modeling and sampling",
    "citation_count": 74,
    "authors": [
      "Ruiqi Gao",
      "Yang Lu",
      "Junpei Zhou",
      "Song-Chun Zhu",
      "Ying Nian Wu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mehr_Manifold_Learning_in_CVPR_2018_paper.html": {
    "title": "Manifold Learning in Quotient Spaces",
    "volume": "main",
    "abstract": "When learning 3D shapes we are usually interested in their intrinsic geometry rather than in their orientation. To deal with the orientation variations the usual trick consists in augmenting the data to exhibit all possible variability, and thus let the model learn both the geometry as well as the rotations. In this paper we introduce a new autoencoder model for encoding and synthesis of 3D shapes. To get rid of undesirable input variability our model learns a manifold in a quotient space of the input space. Typically, we propose to quotient the space of 3D models by the action of rotations. Thus, our quotient autoencoder allows to directly learn in the space of interest, ignoring side information. This is reflected in better performances on reconstruction and interpolation tasks, as our experiments show that our model outperforms a vanilla autoencoder on the well-known Shapenet dataset. Moreover, our model learns a rotation-invariant representation, leading to interesting results in shapes co-alignment. Finally, we extend our quotient autoencoder to quotient by non-rigid transformations",
    "checked": true,
    "id": "d14c1f0217903c82d3b53fb4a1ff957658adaf5a",
    "semantic_title": "manifold learning in quotient spaces",
    "citation_count": 14,
    "authors": [
      "Éloi Mehr",
      "André Lieutier",
      "Fernando Sanchez Bermudez",
      "Vincent Guitteny",
      "Nicolas Thome",
      "Matthieu Cord"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Konyushkova_Learning_Intelligent_Dialogs_CVPR_2018_paper.html": {
    "title": "Learning Intelligent Dialogs for Bounding Box Annotation",
    "volume": "main",
    "abstract": "We introduce Intelligent Annotation Dialogs for bounding box annotation. We train an agent to automatically choose a sequence of actions for a human annotator to produce a bounding box in a minimal amount of time. Specifically, we consider two actions: box verification, where the annotator verifies a box generated by an object detector, and manual box drawing. We explore two kinds of agents, one based on predicting the probability that a box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the image difficulty, the desired quality of the boxes, and the detector strength; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also outperforming any fixed combination of verification and drawing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger",
    "checked": true,
    "id": "4a31b3fcc346bf3235efe1c0fb310833570cef27",
    "semantic_title": "learning intelligent dialogs for bounding box annotation",
    "citation_count": 42,
    "authors": [
      "Ksenia Konyushkova",
      "Jasper Uijlings",
      "Christoph H. Lampert",
      "Vittorio Ferrari"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Boosting_Adversarial_Attacks_CVPR_2018_paper.html": {
    "title": "Boosting Adversarial Attacks With Momentum",
    "volume": "main",
    "abstract": "Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions",
    "checked": true,
    "id": "8e37a3b227b68953f8067215828dc8b8714cb21b",
    "semantic_title": "boosting adversarial attacks with momentum",
    "citation_count": 2090,
    "authors": [
      "Yinpeng Dong",
      "Fangzhou Liao",
      "Tianyu Pang",
      "Hang Su",
      "Jun Zhu",
      "Xiaolin Hu",
      "Jianguo Li"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_NISP_Pruning_Networks_CVPR_2018_paper.html": {
    "title": "NISP: Pruning Networks Using Neuron Importance Score Propagation",
    "volume": "main",
    "abstract": "To reduce the significant redundancy in deep Convolutional Neural Networks (CNNs), most existing methods prune neurons by only considering the statistics of an individual layer or two consecutive layers (e.g., prune one layer to minimize the reconstruction error of the next layer), ignoring the effect of error propagation in deep networks. In contrast, we argue that for a pruned network to retain its predictive power, it is essential to prune neurons in the entire neuron network jointly based on a unified goal: minimizing the reconstruction error of important responses in the ``final response layer\" (FRL), which is the second-to-last layer before classification. Specifically, we apply feature ranking techniques to measure the importance of each neuron in the FRL, formulate network pruning as a binary integer optimization problem, and derive a closed-form solution to it for pruning neurons in earlier layers. Based on our theoretical analysis, we propose the Neuron Importance Score Propagation (NISP) algorithm to propagate the importance scores of final responses to every neuron in the network. The CNN is pruned by removing neurons with least importance, and it is then fine-tuned to recover its predictive power. NISP is evaluated on several datasets with multiple CNN models and demonstrated to achieve significant acceleration and compression with negligible accuracy loss",
    "checked": true,
    "id": "af03709f0893a7ff1c2656b73249d60030bab996",
    "semantic_title": "nisp: pruning networks using neuron importance score propagation",
    "citation_count": 709,
    "authors": [
      "Ruichi Yu",
      "Ang Li",
      "Chun-Fu Chen",
      "Jui-Hsin Lai",
      "Vlad I. Morariu",
      "Xintong Han",
      "Mingfei Gao",
      "Ching-Yung Lin",
      "Larry S. Davis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Le_PointGrid_A_Deep_CVPR_2018_paper.html": {
    "title": "PointGrid: A Deep Network for 3D Shape Understanding",
    "volume": "main",
    "abstract": "This paper presents a new deep learning architecture called PointGrid that is designed for 3D model recognition from unorganized point clouds. The new architecture embeds the input point cloud into a 3D grid by a simple, yet effective, sampling strategy and directly learns transformations and features from their raw coordinates. The proposed method is an integration of point and grid, a hybrid model, that leverages the simplicity of grid-based approaches such as VoxelNet while avoid its information loss. PointGrid learns better global information compared with PointNet and is much simpler than PointNet++, Kd-Net, Oct-Net and O-CNN, yet provides comparable recognition accuracy. With experiments on popular shape recognition benchmarks, PointGrid demonstrates competitive performance over existing deep learning methods on both classification and segmentation",
    "checked": true,
    "id": "e81acd4f35eb2202bf753266ec83d3174645820f",
    "semantic_title": "pointgrid: a deep network for 3d shape understanding",
    "citation_count": 292,
    "authors": [
      "Truc Le",
      "Ye Duan"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Tell_Me_Where_CVPR_2018_paper.html": {
    "title": "Tell Me Where to Look: Guided Attention Inference Network",
    "volume": "main",
    "abstract": "Weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients. These attention maps are then available as priors for tasks such as object localization and semantic segmentation. In one common framework we address three shortcomings of previous approaches in modeling such attention maps: We (1) make attention maps an explicit and natural component of the end-to-end training for the first time, (2) provide self-guidance directly on these maps by exploring supervision from the network itself to improve them, and (3) seamlessly bridge the gap between using weak and extra supervision if available. Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods. We clearly surpass the state-of-the-art on PASCAL VOC 2012 test and val. sets. Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks. Under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance",
    "checked": true,
    "id": "2622d2467f19bc60427f8ea495515e7da82316c9",
    "semantic_title": "tell me where to look: guided attention inference network",
    "citation_count": 468,
    "authors": [
      "Kunpeng Li",
      "Ziyan Wu",
      "Kuan-Chuan Peng",
      "Jan Ernst",
      "Yun Fu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.html": {
    "title": "3D Semantic Segmentation With Submanifold Sparse Convolutional Networks",
    "volume": "main",
    "abstract": "Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard ``dense'' implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition",
    "checked": true,
    "id": "1d6a5d0299ed8458191e4e0407d4d513e6a7dd7e",
    "semantic_title": "3d semantic segmentation with submanifold sparse convolutional networks",
    "citation_count": 1213,
    "authors": [
      "Benjamin Graham",
      "Martin Engelcke",
      "Laurens van der Maaten"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_TOM-Net_Learning_Transparent_CVPR_2018_paper.html": {
    "title": "TOM-Net: Learning Transparent Object Matting From a Single Image",
    "volume": "main",
    "abstract": "This paper addresses the problem of transparent object matting. Existing image matting approaches for transparent objects often require tedious capturing procedures and long processing time, which limit their practical use. In this paper, we first formulate transparent object matting as a refractive flow estimation problem. We then propose a deep learning framework, called TOM-Net, for learning the refractive flow. Our framework comprises two parts, namely a multi-scale encoder-decoder network for producing a coarse prediction, and a residual network for refinement. At test time, TOM-Net takes a single image as input, and outputs a matte (consisting of an object mask, an attenuation mask and a refractive flow field) in a fast feed-forward pass. As no off-the-shelf dataset is available for transparent object matting, we create a large-scale synthetic dataset consisting of 178K images of transparent objects rendered in front of images sampled from the Microsoft COCO dataset. We also collect a real dataset consisting of 876 samples using 14 transparent objects and 60 background images. Promising experimental results have been achieved on both synthetic and real data, which clearly demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "6f754dfbe9201b16422e4e7971f57cfd12d204bf",
    "semantic_title": "tom-net: learning transparent object matting from a single image",
    "citation_count": 67,
    "authors": [
      "Guanying Chen",
      "Kai Han",
      "Kwan-Yee K. Wong"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Translating_and_Segmenting_CVPR_2018_paper.html": {
    "title": "Translating and Segmenting Multimodal Medical Volumes With Cycle- and Shape-Consistency Generative Adversarial Network",
    "volume": "main",
    "abstract": "Synthesized medical images have several important applications, e.g., as an intermedium in cross-modality image registration and as supplementary training samples to boost the generalization capability of a classifier. Especially, synthesized CT data can provide X-ray attenuation map for radiation therapy planning. In this work, we propose a generic cross-modality synthesis approach with the following targets: 1) synthesizing realistic looking 3D images using unpaired training data, 2) ensuring consistent anatomical structures, which could changed by geometric distortion in cross-modality synthesis and 3) improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss, which is supervised by segmentors, to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. With extensive experiments on a dataset including a total of 4,496 CT and MRI cardiovascular volumes, we show both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively",
    "checked": true,
    "id": "89fb34d44de8e63ec5bcaa736c2f52a363c90942",
    "semantic_title": "translating and segmenting multimodal medical volumes with cycle- and shape-consistency generative adversarial network",
    "citation_count": 340,
    "authors": [
      "Zizhao Zhang",
      "Lin Yang",
      "Yefeng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Balakrishnan_An_Unsupervised_Learning_CVPR_2018_paper.html": {
    "title": "An Unsupervised Learning Model for Deformable Medical Image Registration",
    "volume": "main",
    "abstract": "We present a fast learning-based algorithm for deformable, pairwise 3D medical image registration. Current registration methods optimize an objective function independently for each pair of images, which can be time-consuming for large data. We define registration as a parametric function, and optimize its parameters given a set of images from a collection of interest. Given a new pair of scans, we can quickly compute a registration field by directly evaluating the function using the learned parameters. We model this function using a CNN, and use a spatial transform layer to reconstruct one image from another while imposing smoothness constraints on the registration field. The proposed method does not require supervised information such as ground truth registration fields or anatomical landmarks. We demonstrate registration accuracy comparable to state-of-the-art 3D image registration, while operating orders of magnitude faster in practice. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is available at https://github.com/balakg/voxelmorph",
    "checked": true,
    "id": "a336727837d2c66e70d9d54a036fa8177cb4d0ed",
    "semantic_title": "an unsupervised learning model for deformable medical image registration",
    "citation_count": 541,
    "authors": [
      "Guha Balakrishnan",
      "Amy Zhao",
      "Mert R. Sabuncu",
      "John Guttag",
      "Adrian V. Dalca"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Yan_Deep_Lesion_Graphs_CVPR_2018_paper.html": {
    "title": "Deep Lesion Graphs in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-Scale Lesion Database",
    "volume": "main",
    "abstract": "Radiologists in their daily work routinely find and annotate significant abnormalities on a large number of radiology images. Such abnormalities, or lesions, have collected over years and stored in hospitals' picture archiving and communication systems. However, they are basically unsorted and lack semantic annotations like type and location. In this paper, we aim to organize and explore them by learning a deep feature representation for each lesion. A large-scale and comprehensive dataset, DeepLesion, is introduced for this task. DeepLesion contains bounding boxes and size measurements of over 32K lesions. To model their similarity relationship, we leverage multiple supervision information including types, self-supervised location coordinates, and sizes. They require little manual annotation effort but describe useful attributes of the lesions. Then, a triplet network is utilized to learn lesion embeddings with a sequential sampling strategy to depict their hierarchical similarity structure. Experiments show promising qualitative and quantitative results on lesion retrieval, clustering, and classification. The learned embeddings can be further employed to build a lesion graph for various clinically useful applications. An algorithm for intra-patient lesion matching is proposed and validated with experiments",
    "checked": true,
    "id": "c562e95b7906066be4210d00c4f6187475e6e13a",
    "semantic_title": "deep lesion graphs in the wild: relationship learning and organization of significant radiology image findings in a diverse large-scale lesion database",
    "citation_count": 125,
    "authors": [
      "Ke Yan",
      "Xiaosong Wang",
      "Le Lu",
      "Ling Zhang",
      "Adam P. Harrison",
      "Mohammadhadi Bagheri",
      "Ronald M. Summers"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Bone_Learning_Distributions_of_CVPR_2018_paper.html": {
    "title": "Learning Distributions of Shape Trajectories From Longitudinal Datasets: A Hierarchical Model on a Manifold of Diffeomorphisms",
    "volume": "main",
    "abstract": "We propose a method to learn a distribution of shape trajectories from longitudinal data, i.e. the collection of individual objects repeatedly observed at multiple time-points. The method allows to compute an average spatiotemporal trajectory of shape changes at the group level, and the individual variations of this trajectory both in terms of geometry and time dynamics. First, we formulate a non-linear mixed-effects statistical model as the combination of a generic statistical model for manifold-valued longitudinal data, a deformation model defining shape trajectories via the action of a finite-dimensional set of diffeomorphisms with a manifold structure, and an efficient numerical scheme to compute parallel transport on this manifold. Second, we introduce a MCMC-SAEM algorithm with a specific approach to shape sampling, an adaptive scheme for proposal variances, and a log-likelihood tempering strategy to estimate our model. Third, we validate our algorithm on 2D simulated data, and then estimate a scenario of alteration of the shape of the hippocampus 3D brain structure during the course of Alzheimer's disease. The method shows for instance that hippocampal atrophy progresses more quickly in female subjects, and occurs earlier in APOE4 mutation carriers. We finally illustrate the potential of our method for classifying pathological trajectories versus normal ageing",
    "checked": true,
    "id": "e9ccfc4e5598a88ffbcb7995b0fceb8b286b3866",
    "semantic_title": "learning distributions of shape trajectories from longitudinal datasets: a hierarchical model on a manifold of diffeomorphisms",
    "citation_count": 32,
    "authors": [
      "Alexandre Bône",
      "Olivier Colliot",
      "Stanley Durrleman"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Jiang_CNN_Driven_Sparse_CVPR_2018_paper.html": {
    "title": "CNN Driven Sparse Multi-Level B-Spline Image Registration",
    "volume": "main",
    "abstract": "Traditional single-grid and pyramidal B-spline parameterizations used in deformable image registration require users to specify control point spacing configurations capable of accurately capturing both global and complex local deformations. In many cases, such grid configurations are non-obvious and largely selected based on user experience. Recent regularization methods imposing sparsity upon the B-spline coefficients throughout simultaneous multi-grid optimization, however, have provided a promising means of determining suitable configurations automatically. Unfortunately, imposing sparsity on over-parameterized B-spline models is computationally expensive and introduces additional difficulties such as undesirable local minima in the B-spline coefficient optimization process. To overcome these difficulties in determining B-spline grid configurations, this paper investigates the use of convolutional neural networks (CNNs) to learn and infer expressive sparse multi-grid configurations prior to B-spline coefficient optimization. Experimental results show that multi-grid configurations produced in this fashion using our CNN based approach provide registration quality comparable to L1-norm constrained over-parameterizations in terms of exactness, while exhibiting significantly reduced computational requirements",
    "checked": true,
    "id": "a1d4db80899af4dee7bc210fac63e91ae15df87c",
    "semantic_title": "cnn driven sparse multi-level b-spline image registration",
    "citation_count": 11,
    "authors": [
      "Pingge Jiang",
      "James A. Shackleford"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Dalca_Anatomical_Priors_in_CVPR_2018_paper.html": {
    "title": "Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation",
    "volume": "main",
    "abstract": "We consider the problem of segmenting a biomedical image into anatomical regions of interest. We specifically address the frequent scenario where we have no paired training data that contains images and their manual segmentations. Instead, we employ unpaired segmentation images that we use to build an anatomical prior. Critically these segmentations can be derived from imaging data from a different dataset and imaging modality than the current task. We introduce a generative probabilistic model that employs the learned prior through a convolutional neural network to compute segmentations in an unsupervised setting. We conducted an empirical analysis of the proposed approach in the context of structural brain MRI segmentation, using a multi-study dataset of more than 14,000 scans. Our results show that an anatomical prior enables fast unsupervised segmentation which is typically not possible using standard convolutional networks. The integration of anatomical priors can facilitate CNN-based anatomical segmentation in a range of novel clinical problems, where few or no annotations are available and thus standard networks are not trainable. The code, model definitions and model weights are freely available at http://github.com/adalca/neuron",
    "checked": true,
    "id": "896a89bf385a3bea1531f8d012172d4fa4393c37",
    "semantic_title": "anatomical priors in convolutional networks for unsupervised biomedical segmentation",
    "citation_count": 132,
    "authors": [
      "Adrian V. Dalca",
      "John Guttag",
      "Mert R. Sabuncu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Raposo_3D_Registration_of_CVPR_2018_paper.html": {
    "title": "3D Registration of Curves and Surfaces Using Local Differential Information",
    "volume": "main",
    "abstract": "This article presents for the first time a global method for registering 3D curves with 3D surfaces without requiring an initialization. The algorithm works with 2-tuples point+vector that consist in pairs of points augmented with the information of their tangents or normals. A closed-form solution for determining the alignment transformation from a pair of matching 2-tuples is proposed. In addition, the set of necessary conditions for two 2-tuples to match is derived. This allows fast search of correspondences that are used in an hypothesise-and-test framework for accomplishing global registration. Comparative experiments demonstrate that the proposed algorithm is the first effective solution for curve vs surface registration, with the method achieving accurate alignment in situations of small overlap and large percentage of outliers in a fraction of a second. The proposed framework is extended to the cases of curve vs curve and surface vs surface registration, with the former being particularly relevant since it is also a largely unsolved problem",
    "checked": true,
    "id": "3a90ce3266adf2c49677f292d9dcc492284eff77",
    "semantic_title": "3d registration of curves and surfaces using local differential information",
    "citation_count": 11,
    "authors": [
      "Carolina Raposo",
      "João P. Barreto"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Caicedo_Weakly_Supervised_Learning_CVPR_2018_paper.html": {
    "title": "Weakly Supervised Learning of Single-Cell Feature Embeddings",
    "volume": "main",
    "abstract": "Many new applications in drug discovery and functional genomics require capturing the morphology of individual imaged cells as comprehensively as possible rather than measuring one particular feature. In these so-called profiling experiments, the goal is to compare populations of cells treated with different chemicals or genetic perturbations in order to identify biomedically important similarities. Deep convolutional neural networks (CNNs) often make excellent feature extractors but require ground truth for training; this is rarely available in biomedical profiling experiments. We therefore propose to train CNNs based on a weakly supervised approach, where the network aims to classify each treatment against all others. Using this network as a feature extractor performed comparably to a network trained on non-biological, natural images on a chemical screen benchmark task, and improved results significantly on a more challenging genetic benchmark presented for the first time",
    "checked": true,
    "id": "b802bb5ffeba07f05d6095672dbdb8b8f58f9497",
    "semantic_title": "weakly supervised learning of single-cell feature embeddings",
    "citation_count": 59,
    "authors": [
      "Juan C. Caicedo",
      "Claire McQuin",
      "Allen Goodman",
      "Shantanu Singh",
      "Anne E. Carpenter"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Haehn_Guided_Proofreading_of_CVPR_2018_paper.html": {
    "title": "Guided Proofreading of Automatic Segmentations for Connectomics",
    "volume": "main",
    "abstract": "Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets",
    "checked": true,
    "id": "1c074e80f80a16bd6fddf0e299fac36404731efb",
    "semantic_title": "guided proofreading of automatic segmentations for connectomics",
    "citation_count": 23,
    "authors": [
      "Daniel Haehn",
      "Verena Kaynig",
      "James Tompkin",
      "Jeff W. Lichtman",
      "Hanspeter Pfister"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Wide_Compression_Tensor_CVPR_2018_paper.html": {
    "title": "Wide Compression: Tensor Ring Nets",
    "volume": "main",
    "abstract": "Deep neural networks have demonstrated state-of-the-art performance in a variety of real-world applications. In order to obtain performance gains, these networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, and computation, thus limiting their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which significantly compress both the fully connected layers and the convolutional layers of deep networks. Our results show that our TR-Nets approach is able to compress LeNet-5 by 11x without losing accuracy, and can compress the state-of-the-art Wide ResNet by 243x with only 2.3% degradation in Cifar10 image classification. Overall, this compression scheme shows promise in scientific computing and deep learning, especially for emerging resource-constrained devices such as smartphones, wearables, and IoT devices",
    "checked": true,
    "id": "8efc2a0b99445d1a754212a6c82377984181e30e",
    "semantic_title": "wide compression: tensor ring nets",
    "citation_count": 142,
    "authors": [
      "Wenqi Wang",
      "Yifan Sun",
      "Brian Eriksson",
      "Wenlin Wang",
      "Vaneet Aggarwal"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Mundhenk_Improvements_to_Context_CVPR_2018_paper.html": {
    "title": "Improvements to Context Based Self-Supervised Learning",
    "volume": "main",
    "abstract": "We develop a set of methods to improve on the results of self-supervised learning using context. We start with a baseline of patch based arrangement context learning and go from there. Our methods address some overt problems such as chromatic aberration as well as other potential problems such as spatial skew and mid-level feature neglect. We prevent problems with testing generalization on common self-supervised benchmark tests by using different datasets during our development. The results of our methods combined yield top scores on all standard self-supervised benchmarks, including classification and detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and \"linear tests\" on the ImageNet and CSAIL Places datasets. We obtain an improvement over our baseline method of between 4.0 to 7.1 percentage points on transfer learning classification tests. We also show results on different standard network architectures to demonstrate generalization as well as portability. All data, models and programs are available at: https://gdo-datasci. llnl.gov/selfsupervised/",
    "checked": true,
    "id": "f0550daaf6b173759aa5a2e39a3a9d874d65e858",
    "semantic_title": "improvements to context based self-supervised learning",
    "citation_count": 115,
    "authors": [
      "T. Nathan Mundhenk",
      "Daniel Ho",
      "Barry Y. Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Keshari_Learning_Structure_and_CVPR_2018_paper.html": {
    "title": "Learning Structure and Strength of CNN Filters for Small Sample Size Training",
    "volume": "main",
    "abstract": "Convolutional Neural Networks have provided state-of-the-art results in several computer vision problems. However, due to a large number of parameters in CNNs, they require a large number of training samples which is a limiting factor for small sample size problems. To address this limitation, in this paper, we propose SSF-CNN which focuses on learning the \"structure\" and \"strength\" of filters. The structure of the filter is initialized using a dictionary based filter learning algorithm and the strength of the filter is learned using the small sample training data. The architecture provides the flexibility of training with both small and large training databases, and yields good accuracies even with small size training data. The effectiveness of the algorithm is demonstrated on MNIST, CIFAR10, NORB, Omniglot, and Newborn Face Image databases, with varying number of training samples. The results show that SSF-CNN significantly reduces the number of parameters required for training while providing high accuracies on the test database. On small problems such as newborn face recognition, the results demonstrate improvement in rank-1 identification accuracy by at least 10%",
    "checked": true,
    "id": "f4d25431e4022e60c465b3f93abd0dbb08864453",
    "semantic_title": "learning structure and strength of cnn filters for small sample size training",
    "citation_count": 72,
    "authors": [
      "Rohit Keshari",
      "Mayank Vatsa",
      "Richa Singh",
      "Afzel Noore"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Noroozi_Boosting_Self-Supervised_Learning_CVPR_2018_paper.html": {
    "title": "Boosting Self-Supervised Learning via Knowledge Transfer",
    "volume": "main",
    "abstract": "In self-supervised learning one trains a model to solve a so-called pretext task on a dataset without the need for human annotation. The main objective, however, is to transfer this model to a target domain and task. Currently, the most effective transfer strategy is fine-tuning, which restricts one to use the same model or parts thereof for both pretext and target tasks. In this paper, we present a novel framework for self-supervised learning that overcomes limitations in designing and comparing different tasks, models, and data domains. In particular, our framework decouples the structure of the self-supervised model from the final task-specific fine-tuned model. This allows us to: 1) quantitatively assess previously incompatible models including handcrafted features; 2) show that deeper neural network models can learn better representations from the same pretext task; 3) transfer knowledge learned with a deep model to a shallower one and thus boost its learning. We use this framework to design a novel self-supervised task, which achieves state-of-the-art performance on the common benchmarks in PASCAL VOC 2007, ILSVRC12 and Places by a significant margin. A surprising result is that our learned features shrink the mAP gap between models trained via self-supervised learning and supervised learning from $5.9$ to $2.6$ in object detection on PASCAL VOC 2007",
    "checked": true,
    "id": "2a1f38e4451e826e01c9874954ba7c6f32ff79f4",
    "semantic_title": "boosting self-supervised learning via knowledge transfer",
    "citation_count": 272,
    "authors": [
      "Mehdi Noroozi",
      "Ananth Vinjimoor",
      "Paolo Favaro",
      "Hamed Pirsiavash"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Beluch_The_Power_of_CVPR_2018_paper.html": {
    "title": "The Power of Ensembles for Active Learning in Image Classification",
    "volume": "main",
    "abstract": "Deep learning methods have become the de-facto standard for challenging image processing tasks such as image classification. One major hurdle of deep learning approaches is that large sets of labeled data are necessary, which can be prohibitively costly to obtain, particularly in medical image diagnosis applications. Active learning techniques can alleviate this labeling effort. In this paper we investigate some recently proposed methods for active learning with high-dimensional data and convolutional neural network classifiers. We compare ensemble-based methods against Monte-Carlo Dropout and geometric approaches. We find that ensembles perform better and lead to more calibrated predictive uncertainties, which are the basis for many active learning algorithms. To investigate why Monte-Carlo Dropout uncertainties perform worse, we explore potential differences in isolation in a series of experiments. We show results for MNIST and CIFAR-10, on which we achieve a test set accuracy of $90 %$ with roughly 12,200 labeled images, and initial results on ImageNet. Additionally, we show results on a large, highly class-imbalanced diabetic retinopathy dataset. We observe that the ensemble-based active learning effectively counteracts this imbalance during acquisition",
    "checked": true,
    "id": "e81c70bc8b81797645332e5db726add973a5633a",
    "semantic_title": "the power of ensembles for active learning in image classification",
    "citation_count": 544,
    "authors": [
      "William H. Beluch",
      "Tim Genewein",
      "Andreas Nürnberger",
      "Jan M. Köhler"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ye_Learning_Compact_Recurrent_CVPR_2018_paper.html": {
    "title": "Learning Compact Recurrent Neural Networks With Block-Term Tensor Decomposition",
    "volume": "main",
    "abstract": "Recurrent Neural Networks (RNNs) are powerful sequence modeling tools. However, when dealing with high dimensional inputs, the training of RNNs becomes computational expensive due to the large number of model parameters. This hinders RNNs from solving many important computer vision tasks, such as Action Recognition in Videos and Image Captioning. To overcome this problem, we propose a compact and flexible structure, namely Block-Term tensor decomposition, which greatly reduces the parameters of RNNs and improves their training efficiency. Compared with alternative low-rank approximations, such as tensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only more concise (when using the same rank), but also able to attain a better approximation to the original RNNs with much fewer parameters. On three challenging tasks, including Action Recognition in Videos, Image Captioning and Image Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of both prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes 17,388 times fewer parameters than the standard LSTM to achieve an accuracy improvement over 15.6% in the Action Recognition task on the UCF11 dataset",
    "checked": true,
    "id": "d76900b0a0b0ad381db20efd0f0cfff54d338d08",
    "semantic_title": "learning compact recurrent neural networks with block-term tensor decomposition",
    "citation_count": 121,
    "authors": [
      "Jinmian Ye",
      "Linnan Wang",
      "Guangxi Li",
      "Di Chen",
      "Shandian Zhe",
      "Xinqi Chu",
      "Zenglin Xu"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Tabernik_Spatially-Adaptive_Filter_Units_CVPR_2018_paper.html": {
    "title": "Spatially-Adaptive Filter Units for Deep Neural Networks",
    "volume": "main",
    "abstract": "Classical deep convolutional networks increase receptive field size by either gradual resolution reduction or application of hand-crafted dilated convolutions to prevent increase in the number of parameters. In this paper we propose a novel displaced aggregation unit (DAU) that does not require hand-crafting. In contrast to classical filters with units (pixels) placed on a fixed regular grid, the displacement of the DAUs are learned, which enables filters to spatially-adapt their receptive field to a given problem. We extensively demonstrate the strength of DAUs on a classification and semantic segmentation tasks. Compared to ConvNets with regular filter, ConvNets with DAUs achieve comparable performance at faster convergence and up to 3-times reduction in parameters. Furthermore, DAUs allow us to study deep networks from novel perspectives. We study spatial distributions of DAU filters and analyze the number of parameters allocated for spatial coverage in a filter",
    "checked": true,
    "id": "902480dbac5091925d0b69a04775809964a6b832",
    "semantic_title": "spatially-adaptive filter units for deep neural networks",
    "citation_count": 10,
    "authors": [
      "Domen Tabernik",
      "Matej Kristan",
      "Aleš Leonardis"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.html": {
    "title": "SO-Net: Self-Organizing Network for Point Cloud Analysis",
    "volume": "main",
    "abstract": "This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website",
    "checked": true,
    "id": "49b0b43e57dc8efe3a92f06ddcb5a7f39da4790d",
    "semantic_title": "so-net: self-organizing network for point cloud analysis",
    "citation_count": 823,
    "authors": [
      "Jiaxin Li",
      "Ben M. Chen",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chavdarova_SGAN_An_Alternative_CVPR_2018_paper.html": {
    "title": "SGAN: An Alternative Training of Generative Adversarial Networks",
    "volume": "main",
    "abstract": "The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them. We consider an alternative training process, named SGAN, in which several adversarial \"local\" pairs of networks are trained independently so that a \"global\" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones. The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well",
    "checked": true,
    "id": "fc62dad5dc03f7b2a6d9b8e7d3934108f4b511f8",
    "semantic_title": "sgan: an alternative training of generative adversarial networks",
    "citation_count": 48,
    "authors": [
      "Tatjana Chavdarova",
      "François Fleuret"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_SketchyGAN_Towards_Diverse_CVPR_2018_paper.html": {
    "title": "SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis",
    "volume": "main",
    "abstract": "Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores",
    "checked": true,
    "id": "bbe3d39adcb41ad2824204c0b0d299d77c2d8363",
    "semantic_title": "sketchygan: towards diverse and realistic sketch to image synthesis",
    "citation_count": 268,
    "authors": [
      "Wengling Chen",
      "James Hays"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Explicit_Loss-Error-Aware_Quantization_CVPR_2018_paper.html": {
    "title": "Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks",
    "volume": "main",
    "abstract": "Benefiting from tens of millions of hierarchically stacked learnable parameters, Deep Neural Networks (DNNs) have demonstrated overwhelming accuracy on a variety of artificial intelligence tasks. However reversely, the large size of DNN models lays a heavy burden on storage, computation and power consumption, which prohibits their deployments on the embedded and mobile systems. In this paper, we propose Explicit Loss-error-aware Quantization (ELQ), a new method that can train DNN models with very low-bit parameter values such as ternary and binary ones to approximate 32-bit floating-point counterparts without noticeable loss of predication accuracy. Unlike existing methods that usually pose the problem as a straightforward approximation of the layer-wise weights or outputs of the original full-precision model (specifically, minimizing the error of the layer-wise weights or inner products of the weights and the inputs between the original and respective quantized models), our ELQ elaborately bridges the loss perturbation from the weight quantization and an incremental quantization strategy to address DNN quantization. Through explicitly regularizing the loss perturbation and the weight approximation error in an incremental way, we show that such a new optimization method is theoretically reasonable and practically effective. As validated with two mainstream convolutional neural network families (i.e., fully convolutional and non-fully convolutional), our ELQ shows better results than the state-of-the-art quantization methods on the large scale ImageNet classification dataset. Code will be made publicly available",
    "checked": true,
    "id": "c9de6315d55c55a465eb646eec76d66d435a7800",
    "semantic_title": "explicit loss-error-aware quantization for low-bit deep neural networks",
    "citation_count": 83,
    "authors": [
      "Aojun Zhou",
      "Anbang Yao",
      "Kuan Wang",
      "Yurong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Towards_Universal_Representation_CVPR_2018_paper.html": {
    "title": "Towards Universal Representation for Unseen Action Recognition",
    "volume": "main",
    "abstract": "Unseen Action Recognition (UAR) aims to recognise novel action categories without training examples. While previous methods focus on inner-dataset seen/unseen splits, this paper proposes a pipeline using a large-scale training source to achieve a Universal Representation (UR) that can generalise to a more realistic Cross-Dataset UAR (CD-UAR) scenario. We first address UAR as a Generalised Multiple-Instance Learning (GMIL) problem and discover \"building-blocks\" from the large-scale ActivityNet dataset using distribution kernels. Essential visual and semantic components are preserved in a shared space to achieve the UR that can efficiently generalise to new datasets. Predicted UR exemplars can be improved by a simple semantic adaptation, and then an unseen action can be directly recognised using UR during the test. Without further training, extensive experiments manifest significant improvements over the UCF101 and HMDB51 benchmarks",
    "checked": true,
    "id": "30ca009c3988d96c4ef7671692f709e8100967f5",
    "semantic_title": "towards universal representation for unseen action recognition",
    "citation_count": 93,
    "authors": [
      "Yi Zhu",
      "Yang Long",
      "Yu Guan",
      "Shawn Newsam",
      "Ling Shao"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.html": {
    "title": "Deep Image Prior",
    "volume": "main",
    "abstract": "Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity",
    "checked": true,
    "id": "faa98e73eeee551c40923c896817ab640925ce20",
    "semantic_title": "deep image prior",
    "citation_count": 2545,
    "authors": [
      "Dmitry Ulyanov",
      "Andrea Vedaldi",
      "Victor Lempitsky"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper.html": {
    "title": "ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing",
    "volume": "main",
    "abstract": "We address the problem of finding realistic geometric corrections to a foreground object such that it appears natural when composited into a background image. To achieve this, we propose a novel Generative Adversarial Network (GAN) architecture that utilizes Spatial Transformer Networks (STNs) as the generator, which we call Spatial Transformer GANs (ST-GANs). ST-GANs seek image realism by operating in the geometric warp parameter space. In particular, we exploit an iterative STN warping scheme and propose a sequential training strategy that achieves better results compared to naive training of a single generator. One of the key advantages of ST-GAN is its applicability to high-resolution images indirectly since the predicted warp parameters are transferable between reference frames. We demonstrate our approach in two applications: (1) visualizing how indoor furniture (e.g. from product images) might be perceived in a room, (2) hallucinating how accessories like glasses would look when matched with real portraits",
    "checked": true,
    "id": "9e7e54896e7ee201d7e8c7f973818f58371e1fd5",
    "semantic_title": "st-gan: spatial transformer generative adversarial networks for image compositing",
    "citation_count": 195,
    "authors": [
      "Chen-Hsuan Lin",
      "Ersin Yumer",
      "Oliver Wang",
      "Eli Shechtman",
      "Simon Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.html": {
    "title": "CartoonGAN: Generative Adversarial Networks for Photo Cartoonization",
    "volume": "main",
    "abstract": "In this paper, we propose a solution to transforming photos of real-world scenes into cartoon style images, which is valuable and challenging in computer vision and computer graphics. Our solution belongs to learning based methods, which have recently become popular to stylize images in artistic forms such as painting. However, existing methods do not produce satisfactory results for cartoonization, due to the fact that (1) cartoon styles have unique characteristics with high level simplification and abstraction, and (2) cartoon images tend to have clear edges, smooth color shading and relatively simple textures, which exhibit significant challenges for texture-descriptor-based loss functions used in existing methods. In this paper, we propose CartoonGAN, a generative adversarial network (GAN) framework for cartoon stylization. Our method takes unpaired photos and cartoon images for training, which is easy to use. Two novel losses suitable for cartoonization are proposed: (1) a semantic content loss, which is formulated as a sparse regularization in the high-level feature maps of the VGG network to cope with substantial style variation between photos and cartoons, and (2) an edge-promoting adversarial loss for preserving clear edges. We further introduce an initialization phase, to improve the convergence of the network to the target manifold. Our method is also much more efficient to train than existing methods. Experimental results show that our method is able to generate high-quality cartoon images from real-world photos (i.e., following specific artists' styles and with clear edges and smooth shading) and outperforms state-of-the-art methods",
    "checked": true,
    "id": "6394a5b15281180591bdf7e6fbdf704bccc899ed",
    "semantic_title": "cartoongan: generative adversarial networks for photo cartoonization",
    "citation_count": 287,
    "authors": [
      "Yang Chen",
      "Yu-Kun Lai",
      "Yong-Jin Liu"
    ]
  }
}