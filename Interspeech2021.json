{
  "https://www.isca-speech.org/archive/interspeech_2021/pucher21_interspeech.html": {
    "title": "Conversion of Airborne to Bone-Conducted Speech with Deep Neural Networks",
    "volume": "main",
    "abstract": "It is a common experience of most speakers that the playback of one's own voice sounds strange. This can be mainly attributed to the missing bone-conducted speech signal that is not present in the playback signal. It was also shown that some phonemes have a high bone-conducted relative to air-conducted sound transmission, which means that the bone-conduction filter is phone-dependent. To achieve such a phone-dependent modeling we train different speaker dependent and speaker adaptive speech conversion systems using airborne and bone-conducted speech data from 8 speakers (5 male, 3 female), which allow for the conversion of airborne speech to bone-conducted speech. The systems are based on Long Short-Term Memory (LSTM) deep neural networks, where the speaker adaptive versions with speaker embedding can be used without bone-conduction signals from the target speaker. Additionally we also used models that apply a global filtering. The different models are then evaluated by an objective error metric and a subjective listening experiment, which show that the LSTM based models outperform the global filters",
    "checked": true,
    "id": "fa04146b59a5a17c00b28c223b415b4df45f0991",
    "semantic_title": "conversion of airborne to bone-conducted speech with deep neural networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rezackova21_interspeech.html": {
    "title": "T5G2P: Using Text-to-Text Transfer Transformer for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Despite the increasing popularity of end-to-end text-to-speech (TTS) systems, the correct grapheme-to-phoneme (G2P) module is still a crucial part of those relying on a phonetic input. In this paper, we, therefore, introduce a T5G2P model, a Text-to-Text Transfer Transformer (T5) neural network model which is able to convert an input text sentence into a phoneme sequence with a high accuracy. The evaluation of our trained T5 model is carried out on English and Czech, since there are different specific properties of G2P, including homograph disambiguation, cross-word assimilation and irregular pronunciation of loanwords. The paper also contains an analysis of a homographs issue in English and offers another approach to Czech phonetic transcription using the detection of pronunciation exceptions",
    "checked": true,
    "id": "8ca9702be6bb1a9af323b778ad96894db284313e",
    "semantic_title": "t5g2p: using text-to-text transfer transformer for grapheme-to-phoneme conversion",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/perrotin21_interspeech.html": {
    "title": "Evaluating the Extrapolation Capabilities of Neural Vocoders to Extreme Pitch Values",
    "volume": "main",
    "abstract": "Neural vocoders are systematically evaluated on homogeneous train and test databases. This kind of evaluation is efficient to compare neural vocoders in their \"comfort zone\", yet it hardly reveals their limits towards unseen data during training. To compare their extrapolation capabilities, we introduce a methodology that aims at quantifying the robustness of neural vocoders in synthesising unseen data, by precisely controlling the ranges of seen/unseen data in the training database. By focusing in this study on the pitch (F ) parameter, our methodology involves a careful splitting of a dataset to control which F values are seen/unseen during training, followed by both global (utterance) and local (frame) evaluation of vocoders. Comparison of four types of vocoders (autoregressive, sourcefilter, flows, GAN) displays a wide range of behaviour towards unseen input pitch values, including excellent extrapolation (WaveGlow); widely-spread F errors (WaveRNN); and systematic generation of the training set median F (LPCNet, Parallel WaveGAN). In contrast, fewer differences between vocoders were observed when using homogeneous train and test sets, thus demonstrating the potential and need for such evaluation to better discriminate the neural vocoders abilities to generate out-of-training-range data",
    "checked": true,
    "id": "2a1d555165bbde06837b0b8eca013f59f54c7ed7",
    "semantic_title": "evaluating the extrapolation capabilities of neural vocoders to extreme pitch values",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/do21_interspeech.html": {
    "title": "A Systematic Review and Analysis of Multilingual Data Strategies in Text-to-Speech for Low-Resource Languages",
    "volume": "main",
    "abstract": "We provide a systematic review of past studies that use multilingual data for text-to-speech (TTS) of low-resource languages (LRLs). We focus on the strategies used by these studies for incorporating multilingual data and how they affect output speech quality. To investigate the difference in output quality between corresponding monolingual and multilingual models, we propose a novel measure to compare this difference across the included studies and their various evaluation metrics. This measure, called the Multilingual Model Effect (MLME), is found to be affected by: acoustic model architecture, the difference ratio of target language data between corresponding multilingual and monolingual experiments, the balance ratio of target language data to total data, and the amount of target language data used. These findings can act as reference for data strategies in future experiments with multilingual TTS models for LRLs. Language family classification, despite being widely used, is not found to be an effective criterion for selecting source languages",
    "checked": true,
    "id": "016064c068dfe3dd3ae8b2a8c8940c2a4e2db866",
    "semantic_title": "a systematic review and analysis of multilingual data strategies in text-to-speech for low-resource languages",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/talkar21_interspeech.html": {
    "title": "Acoustic Indicators of Speech Motor Coordination in Adults With and Without Traumatic Brain Injury",
    "volume": "main",
    "abstract": "A traumatic brain injury (TBI) can lead to various long-term effects on memory, attention, and mood, as well as the occurrence of headaches, speech, and hearing problems. There is a need to better understand the long-term effects of a TBI for objective tracking of an individual's recovery, which could be used to determine intervention trajectories. This study utilizes acoustic features derived from recordings of speech tasks completed by active-duty service members and veterans (SMVs) enrolled in the Defense and Veterans Brain Injury (DVBIC)/Traumatic Brain Injury Center of Excellence (TBICoE) 15-Year Longitudinal TBI Study. We hypothesize that the individuals diagnosed with moderate to severe TBI would demonstrate motor speech impairments through decreased coordination of the speech production subsystems as compared to individuals with no history of TBI. Speech motor coordination is measured through correlations of acoustic feature time series representing speech subsystems. Eigenspectra derived from these correlations are utilized in machine learning models to discriminate between the two groups. The fusion of correlation features derived from the recordings achieves an AUC of 0.78. This suggests that residual motor impairments from moderate to severe TBI could be detectable through objective measures of speech motor coordination",
    "checked": true,
    "id": "29a7bc914934229c5c190d4e7e5cb2b17aa9cbc8",
    "semantic_title": "acoustic indicators of speech motor coordination in adults with and without traumatic brain injury",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vasquezcorrea21_interspeech.html": {
    "title": "On Modeling Glottal Source Information for Phonation Assessment in Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's disease produces several motor symptoms, including different speech impairments that are known as hypokinetic dysarthria. Symptoms associated to dysarthria affect different dimensions of speech such as phonation, articulation, prosody, and intelligibility. Studies in the literature have mainly focused on the analysis of articulation and prosody because they seem to be the most prominent symptoms associated to dysarthria severity. However, phonation impairments also play a significant role to evaluate the global speech severity of Parkinson's patients. This paper proposes an extensive comparison of different methods to automatically evaluate the severity of specific phonation impairments in Parkinson's patients. The considered models include the computation of perturbation and glottal-based features, in addition to features extracted from a zero frequency filtered signals. We consider as well end-to-end models based on 1D CNNs, which are trained to learn features from the raw speech waveform, reconstructed glottal signals, and zero-frequency filtered signals. The results indicate that it is possible to automatically classify between speakers with low versus high phonation severity due to the presence of dysarthria and at the same time to evaluate the severity of the phonation impairments on a continuous scale, posed as a regression problem",
    "checked": true,
    "id": "7dfa3283092c0c780cff9228153196b2cb84d7d6",
    "semantic_title": "on modeling glottal source information for phonation assessment in parkinson's disease",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/daoudi21_interspeech.html": {
    "title": "Distortion of Voiced Obstruents for Differential Diagnosis Between Parkinson's Disease and Multiple System Atrophy",
    "volume": "main",
    "abstract": "Parkinson's disease (PD) and the parkinsonian variant of Multiple System Atrophy (MSA-P) are two neurodegenerative diseases which share similar clinical features, particularly in early disease stages. The differential diagnosis can be thus very challenging. Dysarthria is known to be a frequent and early clinical feature of PD and MSA. It can be thus used as a vehicle to provide a vocal biomarker which could help in the differential diagnosis. In particular, distortion of consonants is known to be a frequent impairment in these diseases. The aim of this study is to investigate distinctive patterns in the distortion of voiced obstruents (plosives and fricatives). It is the first study which attempts to examine such distortions in the French language for the purpose of the differential diagnosis between PD and MSA-P (and among the very few studies if we consider all languages). We carry out a perceptual and objective analysis of voiced obstruents extracted from isolated pseudo-words initials. We first show that devoicing is a significant impairment which predominates in MSA-P. We then show that voice onset time (VOT) of voiced plosives (prevoicing duration) can be a complementary feature to improve the accuracy in discrimination between PD and MSA-P",
    "checked": true,
    "id": "07b9da3899e66c7dbd79de23a294811f8b1c34dc",
    "semantic_title": "distortion of voiced obstruents for differential diagnosis between parkinson's disease and multiple system atrophy",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21_interspeech.html": {
    "title": "A Study into Pre-Training Strategies for Spoken Language Understanding on Dysarthric Speech",
    "volume": "main",
    "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems avoid an intermediate textual representation by mapping speech directly into intents with slot values. This approach requires considerable domain-specific training data. In low-resource scenarios this is a major concern, e.g., in the present study dealing with SLU for dysarthric speech. Pretraining part of the SLU model for automatic speech recognition targets helps but no research has shown to which extent SLU on dysarthric speech benefits from knowledge transferred from other dysarthric speech tasks. This paper investigates the efficiency of pre-training strategies for SLU tasks on dysarthric speech. The designed SLU system consists of a TDNN acoustic model for feature encoding and a capsule network for intent and slot decoding. The acoustic model is pre-trained in two stages: initialization with a corpus of normal speech and finetuning on a mixture of dysarthric and normal speech. By introducing the intelligibility score as a metric of the impairment severity, this paper quantitatively analyzes the relation between generalization and pathology severity for dysarthric speech",
    "checked": true,
    "id": "c19084b61d05c32320a6caaa739eb68c4eb9d07a",
    "semantic_title": "a study into pre-training strategies for spoken language understanding on dysarthric speech",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/turrisi21_interspeech.html": {
    "title": "EasyCall Corpus: A Dysarthric Speech Dataset",
    "volume": "main",
    "abstract": "This paper introduces a new dysarthric speech command dataset in Italian, called EasyCall corpus. The dataset consists of 21386 audio recordings from 24 healthy and 31 dysarthric speakers, whose individual degree of speech impairment was assessed by neurologists through the Therapy Outcome Measure. The corpus aims at providing a resource for the development of ASR-based assistive technologies for patients with dysarthria. In particular, it may be exploited to develop a voice-controlled contact application for commercial smartphones, aiming at improving dysarthric patients' ability to communicate with their family and caregivers. Before recording the dataset, participants were administered a survey to evaluate which commands are more likely to be employed by dysarthric individuals in a voice-controlled contact application. In addition, the dataset includes a list of non-commands (i.e., words near/inside commands or phonetically close to commands) that can be leveraged to build a more robust command recognition system. At present commercial ASR systems perform poorly on the EasyCall Corpus as we report in this paper. This result corroborates the need for dysarthric speech corpora for developing effective assistive technologies. To the best of our knowledge, this database represents the richest corpus of dysarthric speech to date",
    "checked": true,
    "id": "6524f8f7692c43e000bea2f68677a0b3dae69015",
    "semantic_title": "easycall corpus: a dysarthric speech dataset",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bie21_interspeech.html": {
    "title": "A Benchmark of Dynamical Variational Autoencoders Applied to Speech Spectrogram Modeling",
    "volume": "main",
    "abstract": "The Variational Autoencoder (VAE) is a powerful deep generative model that is now extensively used to represent high-dimensional complex data via a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, input data vectors are processed independently. In recent years, a series of papers have presented different extensions of the VAE to process sequential data, that not only model the latent space, but also model the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks. We recently performed a comprehensive review of those models and unified them into a general class called Dynamical Variational Autoencoders (DVAEs). In the present paper, we present the results of an experimental benchmark comparing six of those DVAE models on the speech analysis-resynthesis task, as an illustration of the high potential of DVAEs for speech modeling",
    "checked": true,
    "id": "5ace2ad34b19f668232a45806429ab0bfbd2c387",
    "semantic_title": "a benchmark of dynamical variational autoencoders applied to speech spectrogram modeling",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yurt21_interspeech.html": {
    "title": "Fricative Phoneme Detection Using Deep Neural Networks and its Comparison to Traditional Methods",
    "volume": "main",
    "abstract": "Accurate phoneme detection and processing can enhance speech intelligibility in hearing aids and audio & speech codecs. As fricative phonemes have an important part of their energy concentrated in high frequency bands, frequency lowering algorithms are used in hearing aids to improve fricative intelligibility for people with high-frequency hearing loss. In traditional audio codecs, while processing speech in blocks, spectral smearing around fricative phoneme borders results in pre and post echo artifacts. Hence, detecting the fricative borders and adapting the processing accordingly could enhance the quality of speech. Until recently, phoneme detection and analysis were mostly done by extracting features specific to the class of phonemes. In this paper, we present a deep learning based fricative phoneme detection algorithm that exceeds the state-of-the-art fricative phoneme detection accuracy on the TIMIT speech corpus. Moreover, we compare our method to other approaches that employ classical signal processing for fricative detection and also evaluate it on the TIMIT files coded with AAC codec followed by bandwidth limitation. Reported results of our deep learning approach on original TIMIT files are reproducible and come with an easy to use code that could serve as a baseline for any future research on this topic",
    "checked": true,
    "id": "e8bd30368ce8d8103b4404bbc5bb1f619743c541",
    "semantic_title": "fricative phoneme detection using deep neural networks and its comparison to traditional methods",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/prasad21_interspeech.html": {
    "title": "Identification of F1 and F2 in Speech Using Modified Zero Frequency Filtering",
    "volume": "main",
    "abstract": "Formants are major resonances in the vocal tract system. Identification of formants is important for study of speech. In the literature, formants are typically identified by first deriving formant frequency candidates (e.g., using linear prediction) and then applying a tracking mechanism. In this paper, we propose a simple tracking-free formant identification approach based on zero frequency filtering. More precisely, formants F1-F2 are identified by modifying the trend removal operation in zero frequency filtering and picking simply the dominant peak in the short-term discrete Fourier transform spectra. We demonstrate the potential of the approach by comparing it against state-of-the-art formant identification approaches on a typical speech data set (TIMIT-VTR) and an atypical speech data set (PC-GITA)",
    "checked": true,
    "id": "418936459718355b4c59a3b0c7698882428639c3",
    "semantic_title": "identification of f1 and f2 in speech using modified zero frequency filtering",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/teytaut21_interspeech.html": {
    "title": "Phoneme-to-Audio Alignment with Recurrent Neural Networks for Speaking and Singing Voice",
    "volume": "main",
    "abstract": "Phoneme-to-audio alignment is the task of synchronizing voice recordings and their related phonetic transcripts. In this work, we introduce a new system to forced phonetic alignment with Recurrent Neural Networks (RNN). With the Connectionist Temporal Classification (CTC) loss as training objective, and an additional reconstruction cost, we learn to infer relevant per-frame phoneme probabilities from which alignment is derived. The core of the neural architecture is a context-aware attention mechanism between mel-spectrograms and side information. We investigate two contexts given by either phoneme sequences (model PhAtt) or spectrograms themselves (model SpAtt). Evaluations show that these models produce precise alignments for both speaking and singing voice. Best results are obtained with the model PhAtt, which outperforms baseline reference with an average imprecision of 16.3ms and 29.8ms on speech and singing, respectively. The model SpAtt also appears as an interesting alternative, capable of aligning longer audio files without requiring phoneme sequences on small audio segments",
    "checked": true,
    "id": "bfba22e6b5774780e18fb348db365df211280f2a",
    "semantic_title": "phoneme-to-audio alignment with recurrent neural networks for speaking and singing voice",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21_interspeech.html": {
    "title": "Adaptive Convolutional Neural Network for Text-Independent Speaker Recognition",
    "volume": "main",
    "abstract": "In text-independent speaker recognition, each speech is composed of different phonemes depending on spoken text. The conventional neural networks for speaker recognition are static models, so they do not reflect this phoneme-varying characteristic well. To tackle this limitation, we propose an adaptive convolutional neural network (ACNN) for text-independent speaker recognition. The utterance is divided along the time axis into short segments with small fluctuating phonemes. Frame-level features are extracted by applying input-dependent kernels adaptive to each segment. By applying time average pooling and linear layers, utterance-level embeddings extraction and speaker recognition are performed. Adaptive VGG-M using 0.356 seconds segmentation shows better speaker recognition performance than baseline models, with a Top-1 of 86.51% and an EER of 5.68%. It extracts more accurate frame-level embeddings for vowel and nasal phonemes compared to the conventional method without overfitting and large parameters. This framework for text-independent speaker recognition effectively utilizes phonemes and text-varying characteristic of speech",
    "checked": true,
    "id": "4c653bd510d5c1bbf2e5690e7075c1b5b6ce104f",
    "semantic_title": "adaptive convolutional neural network for text-independent speaker recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qi21_interspeech.html": {
    "title": "Bidirectional Multiscale Feature Aggregation for Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we propose a novel bidirectional multiscale feature aggregation (BMFA) network with attentional fusion modules for text-independent speaker verification. The feature maps from different stages of the backbone network are iteratively combined and refined in both a bottom-up and top-down manner. Furthermore, instead of simple concatenation or elementwise addition of feature maps from different stages, an attentional fusion module is designed to compute the fusion weights. Experiments are conducted on the NIST SRE16 and VoxCeleb1 datasets. The experimental results demonstrate the effectiveness of the bidirectional aggregation strategy and show that the proposed attentional fusion module can further improve the performance",
    "checked": true,
    "id": "588856b77b4e8709746dda0db3ed5e626bf87aba",
    "semantic_title": "bidirectional multiscale feature aggregation for speaker verification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21_interspeech.html": {
    "title": "Improving Time Delay Neural Network Based Speaker Recognition with Convolutional Block and Feature Aggregation Methods",
    "volume": "main",
    "abstract": "In this paper, we develop a system that integrates multiple ideas and techniques inspired by the convolutional block and feature aggregation methods. We begin with the state-of-the-art speaker-embedding model for speaker recognition, namely the model of Emphasized Channel Attention, Propagation, and Aggregation in Time Delay Neural Network, and then gradually experiment with the proposed network modules, including bottleneck residual blocks, attention mechanisms, and feature aggregation methods. In our final model, we replace the Res2Block with SC-Block and we use a hierarchical architecture for feature aggregation. We evaluate the performance of our model on the VoxCeleb1 test set and the 2020 VoxCeleb Speaker Recognition Challenge (VoxSRC20) validation set. The relative improvement of the proposed models over ECAPA-TDNN is 22.8% on VoxCeleb1 and 18.2% on VoxSRC20",
    "checked": true,
    "id": "1b4de223d5393bb2abac0ecb6e46de0a020ad18e",
    "semantic_title": "improving time delay neural network based speaker recognition with convolutional block and feature aggregation methods",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21_interspeech.html": {
    "title": "Improving Deep CNN Architectures with Variable-Length Training Samples for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Deep Convolutional Neural Network (CNN) based speaker embeddings, such as r-vectors, have shown great success in text-independent speaker verification (TI-SV) task. However, previous deep CNN models usually use fixed-length samples for training and employ variable-length utterances for speaker embeddings, which generates a mismatch between training and embedding. To address this issue, we investigate the effect of employing variable-length training samples on CNN-based TI-SV systems and explore two approaches to improve the performance of deep CNN architectures on TI-SV through capturing variable-term contexts. Firstly, we present an improved selective kernel convolution which allows the networks to adaptively switch between short-term and long-term contexts based on variable-length utterances. Secondly, we propose a multi-scale statistics pooling method to aggregate multiple time-scale features from different layers of the networks. We build a novel ResNet34 based architecture with two proposed approaches. Experiments are conducted on the VoxCeleb datasets. The results demonstrate that the effect of using variable-length samples is diverse in different networks and the architecture with two proposed approaches achieves significant improvement over r-vectors baseline system",
    "checked": true,
    "id": "899172bf6b54aeb4359fcf6c18d50b63358f34b6",
    "semantic_title": "improving deep cnn architectures with variable-length training samples for text-independent speaker verification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21_interspeech.html": {
    "title": "Binary Neural Network for Speaker Verification",
    "volume": "main",
    "abstract": "Although deep neural networks are successful for many tasks in the speech domain, the high computational and memory costs of deep neural networks make it difficult to directly deploy high-performance Neural Network systems on low-resource embedded devices. There are several mechanisms to reduce the size of the neural networks i.e. parameter pruning, parameter quantization, etc. This paper focuses on how to apply binary neural networks to the task of speaker verification. The proposed binarization of training parameters can largely maintain the performance while significantly reducing storage space requirements and computational costs. Experiment results show that, after binarizing the Convolutional Neural Network, the ResNet34-based network achieves an EER of around 5% on the Voxceleb1 testing dataset and even outperforms the traditional real number network on the text-dependent dataset: Xiaole while having a 32Ã— memory saving",
    "checked": true,
    "id": "08b1a85ca48ca7f723b0b5e331c89561158d7b2e",
    "semantic_title": "binary neural network for speaker verification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tu21_interspeech.html": {
    "title": "Mutual Information Enhanced Training for Speaker Embedding",
    "volume": "main",
    "abstract": "Mutual information (MI) is useful in unsupervised and self-supervised learning. Maximizing the MI between the low-level features and the learned embeddings can preserve meaningful information in the embeddings, which can contribute to performance gains. This strategy is called deep InfoMax (DIM) in representation learning. In this paper, we follow the DIM framework so that the speaker embeddings can capture more information from the frame-level features. However, a straightforward implementation of DIM may pose a dimensionality imbalance problem because the dimensionality of the frame-level features is much larger than that of the speaker embeddings. This problem can lead to unreliable MI estimation and can even cause detrimental effects on speaker verification. To overcome this problem, we propose to squeeze the frame-level features before MI estimation through some global pooling methods. We call the proposed method squeeze-DIM. Although the squeeze operation inevitably introduces some information loss, we empirically show that the squeeze-DIM can achieve performance gains on both Voxceleb1 and VOiCES-19 tasks. This suggests that the squeeze operation facilitates the MI estimation and maximization in a balanced dimensional space, which helps learn more informative speaker embeddings",
    "checked": true,
    "id": "289c4486a6f54644b2c6285d6640bb30ba71fbbe",
    "semantic_title": "mutual information enhanced training for speaker embedding",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21b_interspeech.html": {
    "title": "Y-Vector: Multiscale Waveform Encoder for Speaker Embedding",
    "volume": "main",
    "abstract": "State-of-the-art text-independent speaker verification systems typically use cepstral features or filter bank energies as speech features. Recent studies attempted to extract speaker embeddings directly from raw waveforms and have shown competitive results. In this paper, we propose a novel multi-scale waveform encoder that uses three convolution branches with different time scales to compute speech features from the waveform. These features are then processed by squeeze-and-excitation blocks, a multi-level feature aggregator, and a time delayed neural network (TDNN) to compute speaker embedding. We show that the proposed embeddings outperform existing raw-waveform-based speaker embeddings on speaker verification by a large margin. A further analysis of the learned filters shows that the multi-scale encoder attends to different frequency bands at its different scales while resulting in a more flat overall frequency response than any of the single-scale counterparts",
    "checked": true,
    "id": "165c4715fdf6a7a0229f2810f2d32a526444b216",
    "semantic_title": "y-vector: multiscale waveform encoder for speaker embedding",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21_interspeech.html": {
    "title": "Phoneme-Aware and Channel-Wise Attentive Learning for Text Dependent Speaker Verification",
    "volume": "main",
    "abstract": "This paper proposes a multi-task learning network with phoneme-aware and channel-wise attentive learning strategies for text-dependent Speaker Verification (SV). In the proposed structure, the frame-level multi-task learning along with the segment-level adversarial learning is adopted for speaker embedding extraction. The phoneme-aware attentive pooling is exploited on frame-level features in the main network for speaker classifier, with the corresponding posterior probability for the phoneme distribution in the auxiliary subnet. Further, the introduction of Squeeze and Excitation (SE-block) performs dynamic channel-wise feature recalibration, which improves the representational ability. The proposed method exploits speaker idiosyncrasies associated with pass-phrases, and is further improved by the phoneme-aware attentive pooling and SE-block from temporal and channel-wise aspects, respectively. The experiments conducted on RSR2015 Part 1 database confirm that the proposed system achieves outstanding results for text-dependent SV",
    "checked": false,
    "id": "923a03112032a81c77ed3e8f4498e60225263442",
    "semantic_title": "phoneme-aware and channel-wise attentive learning for text dependentspeaker verification",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21c_interspeech.html": {
    "title": "Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding",
    "volume": "main",
    "abstract": "This paper proposes a serialized multi-layer multi-head attention for neural speaker embedding in text-independent speaker verification. In prior works, frame-level features from one layer are aggregated to form an utterance-level representation. Inspired by the Transformer network, our proposed method utilizes the hierarchical architecture of stacked self-attention mechanisms to derive refined features that are more correlated with speakers. Serialized attention mechanism contains a stack of self-attention modules to create fixed-dimensional representations of speakers. Instead of utilizing multi-head attention in parallel, the proposed serialized multi-layer multi-head attention is designed to aggregate and propagate attentive statistics from one layer to the next in a serialized manner. In addition, we employ an input-aware query for each utterance with the statistics pooling. With more layers stacked, the neural network can learn more discriminative speaker embeddings. Experiment results on VoxCeleb1 dataset and SITW dataset show that our proposed method outperforms other baseline methods, including x-vectors and other x-vectors + conventional attentive pooling approaches by 9.7% in EER and 8.1% in DCF10 ",
    "checked": true,
    "id": "c3241cacfbb0ef38b87444634652a9202ac3da4e",
    "semantic_title": "serialized multi-layer multi-head attention for neural speaker embedding",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21_interspeech.html": {
    "title": "TacoLPCNet: Fast and Stable TTS by Conditioning LPCNet on Mel Spectrogram Predictions",
    "volume": "main",
    "abstract": "The combination of the recently proposed LPCNet vocoder and a seq-to-seq acoustic model, i.e., Tacotron, has successfully achieved lightweight speech synthesis systems. However, the quality of synthesized speech is often unstable because the precision of the pitch parameters predicted by acoustic models is insufficient, especially for some tonal languages like Chinese and Japanese. In this paper, we propose an end-to-end speech synthesis system, TacoLPCNet, by conditioning LPCNet on Mel spectrogram predictions. First, we extend LPCNet for the Mel spectrogram instead of using explicit pitch information and pitch-related network. Furthermore, we optimize the system by model pruning, multi-frame inference, and increasing frame length, to enable it to meet the conditions required for real-time applications. The objective and subjective evaluation results for various languages show that the proposed system is more stable for tonal languages within the proposed optimization strategies. The experimental results also verify that our model improves synthesis runtime by 3.12 times than that of the baseline on a standard CPU while maintaining naturalness",
    "checked": true,
    "id": "c84bec97770c7851af0516f18bf1121d780897d1",
    "semantic_title": "tacolpcnet: fast and stable tts by conditioning lpcnet on mel spectrogram predictions",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bak21_interspeech.html": {
    "title": "FastPitchFormant: Source-Filter Based Decomposed Modeling for Speech Synthesis",
    "volume": "main",
    "abstract": "Methods for modeling and controlling prosody with acoustic features have been proposed for neural text-to-speech (TTS) models. Prosodic speech can be generated by conditioning acoustic features. However, synthesized speech with a large pitch-shift scale suffers from audio quality degradation, and speaker characteristics deformation. To address this problem, we propose a feed-forward Transformer based TTS model that is designed based on the source-filter theory. This model, called , has a unique structure that handles text and acoustic features in parallel. With modeling each feature separately, the tendency that the model learns the relationship between two features can be mitigated. Owing to its structural characteristics, FastPitchFormant is robust and accurate for pitch control and generates prosodic speech preserving speaker characteristics. The experimental results show that proposed model outperforms the baseline FastPitch",
    "checked": true,
    "id": "034869f2f55b01f240b30923983ea197ff9fc32c",
    "semantic_title": "fastpitchformant: source-filter based decomposed modeling for speech synthesis",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nakamura21_interspeech.html": {
    "title": "Sequence-to-Sequence Learning for Deep Gaussian Process Based Speech Synthesis Using Self-Attention GP Layer",
    "volume": "main",
    "abstract": "This paper presents a speech synthesis method based on deep Gaussian process (DGP) and sequence-to-sequence (Seq2Seq) learning toward high-quality end-to-end speech synthesis. Feed-forward and recurrent models using DGP are known to produce more natural synthetic speech than deep neural networks (DNNs) because of Bayesian learning and kernel regression. However, such DGP models consist of a pipeline architecture of independent models, acoustic and duration models, and require a high level of expertise in text processing. The proposed model is based on Seq2Seq learning, which enables a unified training of acoustic and duration models. The encoder and decoder layers are represented by Gaussian process regressions (GPRs) and the parameters are trained as a Bayesian model. We also propose a self-attention mechanism with Gaussian processes to effectively model character-level input in the encoder. The subjective evaluation results show that the proposed Seq2Seq-SA-DGP can synthesize more natural speech than DNNs with self-attention and recurrent structures. Besides, Seq2Seq-SA-DGP reduces the smoothing problems of recurrent structures and is effective when a simple input for an end-to-end system is given",
    "checked": true,
    "id": "9758aae98703469691e454ee622e5201ebfc7ae3",
    "semantic_title": "sequence-to-sequence learning for deep gaussian process based speech synthesis using self-attention gp layer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kakegawa21_interspeech.html": {
    "title": "Phonetic and Prosodic Information Estimation from Texts for Genuine Japanese End-to-End Text-to-Speech",
    "volume": "main",
    "abstract": "The biggest obstacle to develop end-to-end Japanese text-to-speech (TTS) systems is to estimate phonetic and prosodic information (PPI) from Japanese texts. The following are the reasons: (1) the Kanji characters of the Japanese writing system have multiple corresponding pronunciations, (2) there is no separation mark between words, and (3) an accent nucleus must be assigned at appropriate positions. In this paper, we propose to solve the problems by neural machine translation (NMT) on the basis of encoder-decoder models, and compare NMT models of recurrent neural networks and the Transformer architecture. The proposed model handles texts on token (character) basis, although conventional systems handle them on word basis. To ensure the potential of the proposed approach, NMT models are trained using pairs of sentences and their PPIs that are generated by a conventional Japanese TTS system from 5 million sentences. Evaluation experiments were performed using PPIs that are manually annotated for 5,142 sentences. The experimental results showed that the Transformer architecture has the best performance, with 98.0% accuracy for phonetic information estimation and 95.0% accuracy for PPI estimation. Judging from the results, NMT models are promising toward end-to-end Japanese TTS",
    "checked": true,
    "id": "8fb7bf026379d6e2a51ee95b6dc3aa09ca53b4bc",
    "semantic_title": "phonetic and prosodic information estimation from texts for genuine japanese end-to-end text-to-speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dai21_interspeech.html": {
    "title": "Information Sieve: Content Leakage Reduction in End-to-End Prosody Transfer for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "Expressive neural text-to-speech (TTS) systems incorporate a style encoder to learn a latent embedding as the style information. However, this embedding process may encode redundant textual information. This phenomenon is called content leakage. Researchers have attempted to resolve this problem by adding an ASR or other auxiliary supervision loss functions. In this study, we propose an unsupervised method called the \"information sieve\" to reduce the effect of content leakage in prosody transfer. The rationale of this approach is that the style encoder can be forced to focus on style information rather than on textual information contained in the reference speech by a well-designed downsample-upsample filter, i.e., the extracted style embeddings can be downsampled at a certain interval and then upsampled by duplication. Furthermore, we used instance normalization in convolution layers to help the system learn a better latent style space. Objective metrics such as the significantly lower word error rate (WER) demonstrate the effectiveness of this model in mitigating content leakage. Listening tests indicate that the model retains its prosody transferability compared with the baseline models such as the original GST-Tacotron and ASR-guided Tacotron",
    "checked": false,
    "id": "155e69f30da7726a584bb293c1b8c9ff227d11fe",
    "semantic_title": "information sieve: content leakage reduction in end-to-end prosody for expressive speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dou21_interspeech.html": {
    "title": "Deliberation-Based Multi-Pass Speech Synthesis",
    "volume": "main",
    "abstract": "Sequence-to-sequence (seq2seq) models have achieved state-of-the-art performance in a wide range of tasks including Neural Machine Translation (NMT) and Text-To-Speech (TTS). These models are usually trained with teacher forcing, where the reference back-history is used to predict the next token. This makes training efficient, but limits performance, because during inference the free-running back-history must be used. To address this problem, deliberation-based multi-pass seq2seq has been used in NMT. Here the output sequence is generated in multiple passes, each one conditioned on the initial input and the free-running output of the previous pass. This paper investigates, and compares, deliberation-based multi-pass seq2seq for TTS and NMT. For NMT the simplest form of multi-pass approaches, where the free-running first-pass output is combined with the initial input, improves performance. However, applying this scheme to TTS is challenging: the multi-pass model tends to converge to the standard single-pass model, ignoring the previous output. To tackle this issue, a guided attention loss is added, enabling the system to make more extensive use of the free-running output. Experimental results confirm the above analysis and demonstrate that the proposed TTS model outperforms a strong baseline",
    "checked": true,
    "id": "b8233798ff4bd3b98c884a61b84e1fa02fd1aefa",
    "semantic_title": "deliberation-based multi-pass speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elias21_interspeech.html": {
    "title": "Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling",
    "volume": "main",
    "abstract": "This paper introduces , a non-autoregressive neural text-to-speech model with a fully differentiable duration model which does not require supervised duration signals. The duration model is based on a novel attention mechanism and an iterative reconstruction loss based on Soft Dynamic TimeWarping, this model can learn token-frame alignments as well as token durations automatically. Experimental results show that Parallel Tacotron 2 outperforms baselines in subjective naturalness in several diverse multi speaker evaluations",
    "checked": true,
    "id": "d500d4147509749e10d388fd4900372d01a7c5df",
    "semantic_title": "parallel tacotron 2: a non-autoregressive neural tts model with differentiable duration modeling",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21b_interspeech.html": {
    "title": "Transformer-Based Acoustic Modeling for Streaming Speech Synthesis",
    "volume": "main",
    "abstract": "Transformer models have shown promising results in neural speech synthesis due to their superior ability to model long-term dependencies compared to recurrent networks. The computation complexity of transformers increases quadratically with sequence length, making it impractical for many real-time applications. To address the complexity issue in speech synthesis domain, this paper proposes an efficient transformer-based acoustic model that is constant-speed regardless of input sequence length, making it ideal for streaming speech synthesis applications. The proposed model uses a transformer network that predicts the prosody features at phone rate and then an Emformer network to predict the frame-rate spectral features in a streaming manner. Both the transformer and Emformer in the proposed architecture use a self-attention mechanism that involves explicit long-term information, thus providing improved speech naturalness for long utterances. In our experiments, we use a WaveRNN neural vocoder that takes in the predicted spectral features and generates the final audio. The overall architecture achieves human-like speech quality both on short and long utterances while maintaining a low latency and low real-time factor. Our mean opinion score (MOS) evaluation shows that for short utterances, the proposed model achieves a MOS of 4.213 compared to ground-truth with MOS of 4.307; and for long utterances, it also produces high-quality speech with a MOS of 4.201 compared to ground-truth with MOS of 4.360",
    "checked": true,
    "id": "3b0e51ad9b89f87538f6256cda0b7a22dd71a0b8",
    "semantic_title": "transformer-based acoustic modeling for streaming speech synthesis",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jia21_interspeech.html": {
    "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS",
    "volume": "main",
    "abstract": "This paper introduces , a new encoder model for neural TTS. This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them. It can be pre-trained on a large text corpus in a self-supervised manner, and fine-tuned in a TTS task. Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training. Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers",
    "checked": true,
    "id": "c437b1f260f294af628483d77a49239fa613aa89",
    "semantic_title": "png bert: augmented bert on phonemes and graphemes for neural tts",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ge21_interspeech.html": {
    "title": "Speed up Training with Variable Length Inputs by Efficient Batching Strategies",
    "volume": "main",
    "abstract": "In the model training with neural networks, although the model performance is always the first priority to optimize, training efficiency also plays an important role in model deployment. There are many ways to speed up training with minimal performance loss, such as training with more GPUs, or with mixed precisions, optimizing training parameters, or making features more compact but more representable. Since mini-batch training is now the go-to approach for many machine learning tasks, minimizing the zero-padding to incorporate samples of different lengths into one batch, is an alternative approach to save training time. Here we propose a batching strategy based on semi-sorted samples, with dynamic batch sizes and batch randomization. By replacing the random batching with the proposed batching strategies, it saves more than 40% training time without compromising performance in training seq2seq neural text-to-speech models based on the Tacotron framework. We also compare it with two other batching strategies and show it performs similarly in terms of saving time and maintaining performance, but with a simpler concept and a smoother tuning parameter to balance between zero-padding and randomness level",
    "checked": true,
    "id": "e8aea5f6152a76f6c552789343c248873a204246",
    "semantic_title": "speed up training with variable length inputs by efficient batching strategies",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sun21_interspeech.html": {
    "title": "Funnel Deep Complex U-Net for Phase-Aware Speech Enhancement",
    "volume": "main",
    "abstract": "The emergence of deep neural networks has made speech enhancement well developed. Most of the early models focused on estimating the magnitude of spectrum while ignoring the phase, this gives the evaluation result a certain upper limit. Some recent researches proposed deep complex network, which can handle complex inputs, and realize joint estimation of magnitude spectrum and phase spectrum by outputting real and imaginary parts respectively. The encoder-decoder structure in Deep Complex U-net (DCU) has been proven to be effective for complex-valued data. To further improve the performance, in this paper, we design a new network called Funnel Deep Complex U-net (FDCU), which could process magnitude information and phase information separately through one-encoder-two-decoders structure. Moreover, in order to achieve better training effect, we define negative stretched-SI-SNR as the loss function to avoid errors caused by the negative vector angle. Experimental results show that our FDCU model outperforms state-of-the-art approaches in all evaluation metrics",
    "checked": true,
    "id": "c9ca35b78e5f19eb955d9173ce019adba9045854",
    "semantic_title": "funnel deep complex u-net for phase-aware speech enhancement",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21b_interspeech.html": {
    "title": "Temporal Convolutional Network with Frequency Dimension Adaptive Attention for Speech Enhancement",
    "volume": "main",
    "abstract": "Despite much progress, most temporal convolutional networks (TCN) based speech enhancement models are mainly focused on modeling the long-term temporal contextual dependencies of speech frames, without taking into account the distribution information of speech signal in frequency dimension. In this study, we propose a frequency dimension adaptive attention (FAA) mechanism to improve TCNs, which guides the model selectively emphasize the frequency-wise features with important speech information and also improves the representation capability of network. Our extensive experimental investigation demonstrates that the proposed FAA mechanism is able to consistently provide significant improvements in terms of speech quality (PESQ), intelligibility (STOI) and three other composite metrics. More promisingly, it has better generalization ability to real-world noisy environment",
    "checked": true,
    "id": "30e79104624988e9a4da0c58044ffd59e53f4ab9",
    "semantic_title": "temporal convolutional network with frequency dimension adaptive attention for speech enhancement",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21_interspeech.html": {
    "title": "Perceptual Contributions of Vowels and Consonant-Vowel Transitions in Understanding Time-Compressed Mandarin Sentences",
    "volume": "main",
    "abstract": "Many early studies reported the importance of vowels and vowel-consonant transitions to speech intelligibility. The present work assessed their perceptual impacts to the understanding of time-compressed sentences, which could be used to measure the temporal acuity during speech understanding. Mandarin sentences were edited to selectively preserve vowel centers or vowel-consonant transitional segments, and compress the rest regions with equipment time compression rates (TCRs) up to 3, including conditions only preserving vowel centers or vowel-consonant transitions. The processed stimuli were presented to normal-hearing listeners to recognize. Results showed that, consistent with the segmental contributions in understanding uncompressed speech, the vowel-only time-compressed stimuli were highly intelligible (i.e., intelligibility score >85%) at a TCR around 3, and vowel-consonant transitions carried important intelligibility information in understanding time-compressed sentences. The time-compression conditions in the present work provided higher intelligibility scores than their counterparties in understanding the PSOLA-processed time-compressed sentences with TCRs around 3. The findings in this work suggested that the design of time compression processing could be guided towards selectively preserving perceptually important speech segments (e.g., vowels) in the future",
    "checked": true,
    "id": "3fc7c3e0b439ff6ddf30ea0168f02311e323d1ce",
    "semantic_title": "perceptual contributions of vowels and consonant-vowel transitions in understanding time-compressed mandarin sentences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/biswas21_interspeech.html": {
    "title": "Transfer Learning for Speech Intelligibility Improvement in Noisy Environments",
    "volume": "main",
    "abstract": "In a recent work [1], a novel Delta Function-based Formant Shifting approach was proposed for speech intelligibility improvement. The underlying principle is to dynamically relocate the formants based on their occurrence in the spectrum away from the region of noise. The manner in which the formants are shifted is decided by the parameters of the Delta Function, the optimal values of which are evaluated using Comprehensive Learning Particle Swarm Optimization (CLPSO). Although effective, CLPSO is computationally expensive to the extent that it overshadows its merits in intelligibility improvement. As a solution to this, the current work aims to improve the Short-Time Objective Intelligibility (STOI) of (target) speech using a Delta Function that has been generated using a different (source) language. This transfer learning is based upon the relative positioning of the formant frequencies and pitch values of the source & target language datasets. The proposed approach is demonstrated and validated by subjecting it to experimentation with three different languages under variable noisy conditions",
    "checked": true,
    "id": "89a3c322ce4acf30e2d3298759fc47d2f6e5627b",
    "semantic_title": "transfer learning for speech intelligibility improvement in noisy environments",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yamamoto21_interspeech.html": {
    "title": "Comparison of Remote Experiments Using Crowdsourcing and Laboratory Experiments on Speech Intelligibility",
    "volume": "main",
    "abstract": "Many subjective experiments have been performed to develop objective speech intelligibility measures, but the novel coronavirus outbreak has made it difficult to conduct experiments in a laboratory. One solution is to perform remote testing using crowdsourcing; however, because we cannot control the listening conditions, it is unclear whether the results are entirely reliable. In this study, we compared the speech intelligibility scores obtained from remote and laboratory experiments. The results showed that the mean and standard deviation (SD) of the remote experiments' speech reception threshold (SRT) were higher than those of the laboratory experiments. However, the variance in the SRTs across the speech-enhancement conditions revealed similarities, implying that remote testing results may be as useful as laboratory experiments to develop an objective measure. We also show that practice session scores are correlated with SRT values. This is a priori information before performing the main tests and would be useful for data screening to reduce the variability of the SRT distribution",
    "checked": true,
    "id": "6318311b8dad29622cf6fda76af9c2f249b62bfc",
    "semantic_title": "comparison of remote experiments using crowdsourcing and laboratory experiments on speech intelligibility",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21b_interspeech.html": {
    "title": "Know Your Enemy, Know Yourself: A Unified Two-Stage Framework for Speech Enhancement",
    "volume": "main",
    "abstract": "Traditional spectral subtraction-type single channel speech enhancement (SE) algorithms often need to estimate interference components including noise and/or reverberation before subtracting them while deep neural network-based SE methods often aim to realize the end-to-end target mapping. In this paper, we show that both denoising and dereverberation can be unified into a common problem by introducing a two-stage paradigm, namely for interference components estimation and speech recovery. In the first stage, we propose to explicitly extract the magnitude of interference components, which serves as the prior information. In the second stage, with the guidance of this estimated magnitude prior, we can expect to better recover the target speech. In addition, we propose a transform module to facilitate the interaction between interference components and the desired speech modalities. Meanwhile, a temporal fusion module is designed to model long-term dependencies without ignoring short-term details. We conduct the experiments on the WSJ0-SI84 corpus and the results on both denoising and dereverberation tasks show that our approach outperforms previous advanced systems and achieves state-of-the-art performance in terms of many objective metrics",
    "checked": true,
    "id": "7caad4b855165f3f19501c105b7302a4401893a4",
    "semantic_title": "know your enemy, know yourself: a unified two-stage framework for speech enhancement",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kong21_interspeech.html": {
    "title": "Speech Enhancement with Weakly Labelled Data from AudioSet",
    "volume": "main",
    "abstract": "Speech enhancement is a task to improve the intelligibility and perceptual quality of degraded speech signals. Recently, neural network-based methods have been applied to speech enhancement. However, many neural network-based methods require users to collect clean speech and background noise for training, which can be time-consuming. In addition, speech enhancement systems trained on particular types of background noise may not generalize well to a wide range of noise. To tackle those problems, we propose a speech enhancement framework trained on weakly labelled data. We first apply a pretrained sound event detection system to detect anchor segments that contain sound events in audio clips. Then, we randomly mix two detected anchor segments as a mixture. We build a conditional source separation network using the mixture and a conditional vector as input. The conditional vector is obtained from the audio tagging predictions on the anchor segments. In inference, we input a noisy speech signal with the one-hot encoding of \"Speech\" as a condition to the trained system to predict enhanced speech. Our system achieves a PESQ of 2.28 and an SSNR of 8.75 dB on the VoiceBank-DEMAND dataset, outperforming the previous SEGAN system of 2.16 and 7.73 dB respectively",
    "checked": true,
    "id": "89db5d6c26219fbc09ab4b1ac1853a514281f848",
    "semantic_title": "speech enhancement with weakly labelled data from audioset",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hsieh21_interspeech.html": {
    "title": "Improving Perceptual Quality by Phone-Fortified Perceptual Loss Using Wasserstein Distance for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement (SE) aims to improve speech quality and intelligibility, which are both related to a smooth transition in speech segments that may carry linguistic information, e.g. phones and syllables. In this study, we propose a novel phone-fortified perceptual loss (PFPL) that takes phonetic information into account for training SE models. To effectively incorporate the phonetic information, the PFPL is computed based on latent representations of the model, a powerful self-supervised encoder that renders rich phonetic information. To more accurately measure the distribution distances of the latent representations, the PFPL adopts the Wasserstein distance as the distance measure. Our experimental results first reveal that the PFPL is more correlated with the perceptual evaluation metrics, as compared to signal-level losses. Moreover, the results showed that the PFPL can enable a deep complex U-Net SE model to achieve highly competitive performance in terms of standardized quality and intelligibility evaluations on the Voice Bankâ€“DEMAND dataset",
    "checked": true,
    "id": "27fe7a76098c9344024fa5c70c8f53e9df637040",
    "semantic_title": "improving perceptual quality by phone-fortified perceptual loss using wasserstein distance for speech enhancement",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fu21_interspeech.html": {
    "title": "MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement",
    "volume": "main",
    "abstract": "The discrepancy between the cost function used for training a speech enhancement model and human auditory perception usually makes the quality of enhanced speech unsatisfactory. Objective evaluation metrics which consider human perception can hence serve as a bridge to reduce the gap. Our previously proposed MetricGAN was designed to optimize objective metrics by connecting the metric with a discriminator. Because only the scores of the target evaluation functions are needed during training, the metrics can even be non-differentiable. In this study, we propose a MetricGAN+ in which three training techniques incorporating domain-knowledge of speech processing are proposed. With these techniques, experimental results on the VoiceBank-DEMAND dataset show that MetricGAN+ can increase PESQ score by 0.3 compared to the previous MetricGAN and achieve state-of-the-art results (PESQ score = 3.15)",
    "checked": true,
    "id": "686a302e00af17433aa10d8c38725b58668ad52e",
    "semantic_title": "metricgan+: an improved version of metricgan for speech enhancement",
    "citation_count": 106
  },
  "https://www.isca-speech.org/archive/interspeech_2021/edraki21_interspeech.html": {
    "title": "A Spectro-Temporal Glimpsing Index (STGI) for Speech Intelligibility Prediction",
    "volume": "main",
    "abstract": "We propose a monaural intrusive speech intelligibility prediction (SIP) algorithm called STGI based on detecting in short-time segments in a spectro-temporal modulation decomposition of the input speech signals. Unlike existing glimpse-based SIP methods, the application of STGI is not limited to additive uncorrelated noise; STGI can be employed in a broad range of degradation conditions. Our results show that STGI performs consistently well across 15 datasets covering degradation conditions including modulated noise, noise reduction processing, reverberation, near-end listening enhancement, checkerboard noise, and gated noise",
    "checked": true,
    "id": "88ac79710f0ff8696c8e98d7a28adcf8c183cbd9",
    "semantic_title": "a spectro-temporal glimpsing index (stgi) for speech intelligibility prediction",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiu21_interspeech.html": {
    "title": "Self-Supervised Learning Based Phone-Fortified Speech Enhancement",
    "volume": "main",
    "abstract": "For speech enhancement, deep complex network based methods have shown promising performance due to their effectiveness in dealing with complex-valued spectrums. Recent speech enhancement methods focus on further optimization of network structures and hyperparameters, however, ignore inherent speech characteristics (e.g., phonetic characteristics), which are important for networks to learn and reconstruct speech information. In this paper, we propose a novel self-supervised learning based phone-fortified (SSPF) method for speech enhancement. Our method explicitly imports phonetic characteristics into a deep complex convolutional network via a Contrastive Predictive Coding (CPC) model pre-trained with self-supervised learning. This operation can greatly improve speech representation learning and speech enhancement performance. Moreover, we also apply the self-attention mechanism to our model for learning long-range dependencies of a speech sequence, which further improves the performance of speech enhancement. The experimental results demonstrate that our SSPF method outperforms existing methods and achieves state-of-the-art performance in terms of speech quality and intelligibility",
    "checked": true,
    "id": "b71441f85b95b0274675159c3cc1022f91bb75d3",
    "semantic_title": "self-supervised learning based phone-fortified speech enhancement",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nayem21_interspeech.html": {
    "title": "Incorporating Embedding Vectors from a Human Mean-Opinion Score Prediction Model for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Objective measures of success, such as the perceptual evaluation of speech quality (PESQ), signal-to-distortion ratio (SDR), and short-time objective intelligibility (STOI), have recently been used to optimize deep-learning based speech enhancement algorithms, in an effort to incorporate perceptual constraints into the learning process. Optimizing with these measures, however, may be sub-optimal, since the objective scores do not always strongly correlate with a listener's evaluation. This motivates the need for approaches that either are optimized with scores that are strongly correlated with human assessments or that use alternative strategies for incorporating perceptual constraints. In this work, we propose an attention-based approach that uses learned speech embedding vectors from a mean-opinion score (MOS) prediction model and a speech enhancement module to jointly enhance noisy speech. Our loss function is jointly optimized with signal approximation and MOS prediction loss terms. We train the model using real-world noisy speech data that has been captured in everyday environments. The results show that our proposed model significantly outperforms other approaches that are optimized with objective measures",
    "checked": true,
    "id": "c1078dcb98cee7e6c5b67ff0adda5f62a4644398",
    "semantic_title": "incorporating embedding vectors from a human mean-opinion score prediction model for monaural speech enhancement",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21c_interspeech.html": {
    "title": "Restoring Degraded Speech via a Modified Diffusion Model",
    "volume": "main",
    "abstract": "There are many deterministic mathematical operations (e.g. compression, clipping, downsampling) that degrade speech quality considerably. In this paper we introduce a neural network architecture, based on a modification of the DiffWave model, that aims to restore the original speech signal. DiffWave, a recently published diffusion-based vocoder, has shown state-of-the-art synthesized speech quality and relatively shorter waveform generation times, with only a small set of parameters. We replace the mel-spectrum upsampler in DiffWave with a deep CNN upsampler, which is trained to alter the degraded speech mel-spectrum to match that of the original speech. The model is trained using the original speech waveform, but conditioned on the degraded speech mel-spectrum. Post-training, only the degraded mel-spectrum is used as input and the model generates an estimate of the original speech. Our model results in improved speech quality (original DiffWave model as baseline) on several different experiments. These include improving the quality of speech degraded by LPC-10 compression, AMR-NB compression, and signal clipping. Compared to the original DiffWave architecture, our scheme achieves better performance on several objective perceptual metrics and in subjective comparisons. Improvements over baseline are further amplified in a out-of-corpus evaluation setting",
    "checked": true,
    "id": "28db8fd711ac13599c9921db08cd586235d303ba",
    "semantic_title": "restoring degraded speech via a modified diffusion model",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21_interspeech.html": {
    "title": "User-Initiated Repetition-Based Recovery in Multi-Utterance Dialogue Systems",
    "volume": "main",
    "abstract": "Recognition errors are common in human communication. Similar errors often lead to unwanted behaviour in dialogue systems or virtual assistants. In human communication, we can recover from them by repeating misrecognized words or phrases; however in human-machine communication this recovery mechanism is not available. In this paper, we attempt to bridge this gap and present a system that allows a user to correct speech recognition errors in a virtual assistant by repeating misunderstood words. When a user repeats part of the phrase the system rewrites the original query to incorporate the correction. This rewrite allows the virtual assistant to understand the original query successfully. We present an end-to-end 2-step attention pointer network that can generate the the rewritten query by merging together the incorrectly understood utterance with the correction follow-up. We evaluate the model on data collected for this task and compare the proposed model to a rule-based baseline and a standard pointer network. We show that rewriting the original query is an effective way to handle repetition-based recovery and that the proposed model outperforms the rule based baseline, reducing Word Error Rate by 19% relative at 2% False Alarm Rate on annotated data",
    "checked": true,
    "id": "3722c06603b144ecdb8bb9f594751934f52b214d",
    "semantic_title": "user-initiated repetition-based recovery in multi-utterance dialogue systems",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21_interspeech.html": {
    "title": "Self-Supervised Dialogue Learning for Spoken Conversational Question Answering",
    "volume": "main",
    "abstract": "In spoken conversational question answering (SCQA), the answer to the corresponding question is generated by retrieving and then analyzing a fixed spoken document, including multi-part conversations. Most SCQA systems have considered only retrieving information from ordered utterances. However, the sequential order of dialogue is important to build a robust spoken conversational question answering system, and the changes of utterances order may severely result in low-quality and incoherent corpora. To this end, we introduce a self-supervised learning approach, including , and , to explicitly capture the coreference resolution and dialogue coherence among spoken documents. Specifically, we design a joint learning framework where the auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards more coherent and meaningful spoken dialogue learning. We also utilize the proposed self-supervised learning tasks to capture intra-sentence coherence. Experimental results demonstrate that our proposed method provides more coherent, meaningful, and appropriate responses, yielding superior performance gains compared to the original pre-trained language models. Our method achieves state-of-the-art results on the Spoken-CoQA dataset",
    "checked": true,
    "id": "904afb6696575bb7d74f5e4e74d9cfced480a645",
    "semantic_title": "self-supervised dialogue learning for spoken conversational question answering",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2021/su21_interspeech.html": {
    "title": "Act-Aware Slot-Value Predicting in Multi-Domain Dialogue State Tracking",
    "volume": "main",
    "abstract": "As an essential component in task-oriented dialogue systems, dialogue state tracking (DST) aims to track human-machine interactions and generate state representations for managing the dialogue. Representations of dialogue states are dependent on the domain ontology and the user's goals. In several task-oriented dialogues with a limited scope of objectives, dialogue states can be represented as a set of slot-value pairs. As the capabilities of dialogue systems expand to support increasing naturalness in communication, incorporating dialogue act processing into dialogue model design becomes essential. The lack of such consideration limits the scalability of dialogue state tracking models for dialogues having specific objectives and ontology. To address this issue, we formulate and incorporate dialogue acts, and leverage recent advances in machine reading comprehension to predict both categorical and non-categorical types of slots for multi-domain dialogue state tracking. Experimental results show that our models can improve the overall accuracy of dialogue state tracking on the MultiWOZ 2.1 dataset, and demonstrate that incorporating dialogue acts can guide dialogue state design for future task-oriented dialogue systems",
    "checked": true,
    "id": "8fe6ac053bc961093f46d8592e743215ad33bbad",
    "semantic_title": "act-aware slot-value predicting in multi-domain dialogue state tracking",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chiba21_interspeech.html": {
    "title": "Dialogue Situation Recognition for Everyday Conversation Using Multimodal Information",
    "volume": "main",
    "abstract": "In recent years, dialogue systems have been applied to daily living. Such systems should be able to associate conversations with dialogue situations, such as a place where a dialogue occurs and the relationship between participants. In this study, we propose a dialogue situation recognition method that understands the perspective of dialogue scenes. The target dialogue situations contain dialogue styles, places, activities, and relations between participants. We used the Corpus of Everyday Japanese Conversation (CEJC), which records natural everyday conversations in various situations for experiments. We experimentally verified the effectiveness of our proposed method using multimodal information for situation recognition",
    "checked": true,
    "id": "c5be70f553d46a6f6413fb48c044f6b8093bbefc",
    "semantic_title": "dialogue situation recognition for everyday conversation using multimodal information",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yamazaki21_interspeech.html": {
    "title": "Neural Spoken-Response Generation Using Prosodic and Linguistic Context for Conversational Systems",
    "volume": "main",
    "abstract": "Spoken dialogue systems have become widely used in daily life. Such a system must interact with the user socially to truly operate as a partner with humans. In studies of recent dialogue systems, neural response generation led to natural response generation. However, these studies have not considered the acoustic aspects of conversational phenomena, such as the adaptation of prosody. We propose a spoken-response generation model that extends a neural conversational model to deal with pitch control signals. Our proposed model is trained using multimodal dialogue between humans. The generated pitch control signals are input to a speech synthesis system to control the pitch of synthesized speech. Our experiment shows that the proposed system can generate synthesized speech with an appropriate F0 contour as an utterance in context compared to the output of a system without pitch control, although language generation remains an issue",
    "checked": true,
    "id": "202c9b9e772fe9a8f26ea886268420e96376fcc3",
    "semantic_title": "neural spoken-response generation using prosodic and linguistic context for conversational systems",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21_interspeech.html": {
    "title": "Semantic Transportation Prototypical Network for Few-Shot Intent Detection",
    "volume": "main",
    "abstract": "Few-shot intent detection is a problem that only a few annotated examples are available for unseen intents, and deep models could suffer from the overfitting problem because of scarce data. Existing state-of-the-art few-shot model, Prototypical Network (PN), mainly focus on computing the similarity between examples in a metric space by leveraging sentence-level instance representations. However, sentence-level representations may incorporate highly noisy signals from unrelated words which leads to performance degradation. In this paper, we propose Semantic Transportation Prototypical Network (STPN) to alleviate this issue. Different from the original PN, our approach takes word-level representation as input and uses a new distance metric to obtain better sample matching result. And we reformulate the few-shot classification task into an instance of optimal matching, in which the key word semantic information between examples are expected to be matched and the matching cost is treated as similarity. Specifically, we design Mutual-Semantic mechanism to generate word semantic information, which could reduce the unrelated word noise and enrich key word information. Then, Earth Mover's Distance (EMD) is applied to find an optimal matching solution. Comprehensive experiments on two benchmark datasets are conducted to validate the effectiveness and generalization of our proposed model",
    "checked": true,
    "id": "8decba8a86d69c19aa87edf53e06b31298ab22c8",
    "semantic_title": "semantic transportation prototypical network for few-shot intent detection",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tang21_interspeech.html": {
    "title": "Domain-Specific Multi-Agent Dialog Policy Learning in Multi-Domain Task-Oriented Scenarios",
    "volume": "main",
    "abstract": "Traditional dialog policy learning methods train a generic dialog agent to address all situations. However, when the dialog agent encounters a complicated task that involves more than one domain, it becomes difficult to perform concordant actions due to the hybrid information in the multi-domain ontology. Inspired by a real-life scenario at a bank, there are always several specialized departments that deal with different businesses. In this paper, we propose Domain-Specific Multi-Agent Dialog Policy Learning (DSMADPL), in which the dialog system is composed of a set of agents where each agent represents a specialized skill in a particular domain. Every domain-specific agent is first pretrained with supervised learning using a dialog corpus, and then they are jointly improved with multi-agent reinforcement learning. When the dialog system interacts with the user, in each turn the system action is decided by the actions of relevant agents. Experiments conducted on the commonly used MultiWOZ dataset prove the effectiveness of the proposed method, in which dialog success rate increases from 55.0% for the traditional method to 67.2% for our method in multi-domain scenarios",
    "checked": true,
    "id": "e0a9a3e463bbc7b65fd562c5c9a8dbc6217218c9",
    "semantic_title": "domain-specific multi-agent dialog policy learning in multi-domain task-oriented scenarios",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21b_interspeech.html": {
    "title": "Leveraging ASR N-Best in Deep Entity Retrieval",
    "volume": "main",
    "abstract": "Entity Retrieval (ER) in spoken dialog systems is a task that retrieves entities in a catalog for the entity mentions in user utterances. ER systems are susceptible to upstream errors, with Automatic Speech Recognition (ASR) errors being particularly troublesome. In this work, we propose a robust deep learning based ER system by leveraging ASR N-best hypotheses. Specifically, we evaluate different neural architectures to infuse ASR N-best through an attention mechanism. On 750 hours of audio data taken from live traffic, our best model achieves 11.07% relative error reduction while maintaining the same performance on rejecting out-of-domain ER requests",
    "checked": true,
    "id": "e90aa6debe3bdc089f174bb8f6671e8f47798592",
    "semantic_title": "leveraging asr n-best in deep entity retrieval",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21d_interspeech.html": {
    "title": "End-to-End Spelling Correction Conditioned on Acoustic Feature for Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "In this work, we propose a new end-to-end (E2E) spelling correction method for post-processing of code-switching automatic speech recognition (ASR). Existing E2E spelling correction models take the hypotheses of ASR as inputs and annotated text as the targets. Due to the powerful modeling capabilities of the E2E model, the training of the correction system is extremely prone to over-fitting. It usually requires sufficient data diversity for reliable training. Therefore, it is difficult to apply the E2E correction models to the code-switching ASR task because of the data shortage. In this paper, we introduce the acoustic features into the spelling correction model. Our method can alleviate the problem of over-fitting and has better performance. Meanwhile, because the acoustic features are encode-free, our proposed model can be applied to the ASR model without significantly increasing the computational cost. The experimental results on ASRU 2019 Mandarin-English Code-switching Challenge data set show that the proposed method achieves 11.14% relative error rate reduction compared with baseline",
    "checked": true,
    "id": "832344b4e8dcdebca7a78e4d1dab1b73cbc08b9b",
    "semantic_title": "end-to-end spelling correction conditioned on acoustic feature for code-switching speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/siminyu21_interspeech.html": {
    "title": "Phoneme Recognition Through Fine Tuning of Phonetic Representations: A Case Study on Luhya Language Varieties",
    "volume": "main",
    "abstract": "Models pre-trained on multiple languages have shown significant promise for improving speech recognition, particularly for low-resource languages. In this work, we focus on phoneme recognition using Allosaurus, a method for multilingual recognition based on phonetic annotation, which incorporates phonological knowledge through a language-dependent allophone layer that associates a universal narrow phone-set with the phonemes that appear in each language. To evaluate in a challenging real-world scenario, we curate phone recognition datasets for Bukusu and Saamia, two varieties of the Luhya language cluster of western Kenya and eastern Uganda. To our knowledge, these datasets are the first of their kind. We carry out similar experiments on the dataset of an endangered Tangkhulic language, East Tusom, a Tibeto-Burman language variety spoken mostly in India. We explore both zero-shot and few-shot recognition by fine-tuning using datasets of varying sizes (10 to 1000 utterances). We find that fine-tuning of Allosaurus, even with just 100 utterances, leads to significant improvements in phone error rates",
    "checked": true,
    "id": "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713",
    "semantic_title": "phoneme recognition through fine tuning of phonetic representations: a case study on luhya language varieties",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/loweimi21_interspeech.html": {
    "title": "Speech Acoustic Modelling Using Raw Source and Filter Components",
    "volume": "main",
    "abstract": "Source-filter modelling is among the fundamental techniques in speech processing with a wide range of applications. In acoustic modelling, features such as MFCC and PLP which parametrise the filter component are widely employed. In this paper, we investigate the efficacy of building acoustic models from the raw filter and source components. The raw magnitude spectrum, as the primary information stream, is decomposed into the excitation and vocal tract information streams via cepstral liftering. Then, acoustic models are built via multi-head CNNs which, among others, allow for processing each individual stream via a sequence of bespoke transforms and fusing them at an optimal level of abstraction. We discuss the possible advantages of such information factorisation and recombination, investigate the dynamics of these models and explore the optimal fusion level. Furthermore, we illustrate the CNN's learned filters and provide some interpretation for the captured patterns. The proposed approach with optimal fusion scheme results in up to 14% and 7% relative WER reduction in WSJ and Aurora-4 tasks",
    "checked": true,
    "id": "956404c3e5ad68fcf7e416792889630f0f5c76f9",
    "semantic_title": "speech acoustic modelling using raw source and filter components",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujimoto21_interspeech.html": {
    "title": "Noise Robust Acoustic Modeling for Single-Channel Speech Recognition Based on a Stream-Wise Transformer Architecture",
    "volume": "main",
    "abstract": "This paper addresses a noise-robust automatic speech recognition (ASR) method under the constraints of real-time, one-pass, and single-channel processing. Under such strong constraints, single-channel speech enhancement becomes a key technology because methods with multiple-passes or batch processing, such as acoustic model adaptation, are not suitable for use. However, single-channel speech enhancement often degrades ASR performance due to speech distortion. To overcome this problem, we propose a noise robust acoustic modeling method based on the stream-wise transformer model. The proposed method accepts multi-stream features obtained by multiple single-channel speech enhancement methods as input and selectively uses an appropriate feature stream according to the noise environment by paying attention to the noteworthy stream on the basis of multi-head attention. The proposed method considers the attention for the stream direction instead of the time series direction, and it is thus capable of real-time and low-latency processing. Comparative evaluations reveal that the proposed method successfully improves the accuracy of ASR in noisy environments and reduces the number of model parameters even under strong constraints",
    "checked": true,
    "id": "59dfc48ad807150a8db4a0b8b7e1eb8f8ac33103",
    "semantic_title": "noise robust acoustic modeling for single-channel speech recognition based on a stream-wise transformer architecture",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ratnarajah21_interspeech.html": {
    "title": "IR-GAN: Room Impulse Response Generator for Far-Field Speech Recognition",
    "volume": "main",
    "abstract": "We present a Generative Adversarial Network (GAN) based room impulse response generator (IR-GAN) for generating realistic synthetic room impulse responses (RIRs). IR-GAN extracts acoustic parameters from captured real-world RIRs and uses these parameters to generate new synthetic RIRs. We use these generated synthetic RIRs to improve far-field automatic speech recognition in new environments that are different from the ones used in training datasets. In particular, we augment the far-field speech training set by convolving our synthesized RIRs with a clean LibriSpeech dataset [1]. We evaluate the quality of our synthetic RIRs on the far-field LibriSpeech test set created using real-world RIRs from the BUT ReverbDB [2] and AIR [3] datasets. Our IR-GAN reports up to an 8.95% lower error rate than Geometric Acoustic Simulator (GAS) in far-field speech recognition benchmarks. We further improve the performance when we combine our synthetic RIRs with synthetic impulse responses generated using GAS. This combination can reduce the word error rate by up to 14.3% in far-field speech recognition benchmarks",
    "checked": true,
    "id": "23e208eb3251fd3521c14bcbd3ee2d97a369ed89",
    "semantic_title": "ir-gan: room impulse response generator for far-field speech recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21b_interspeech.html": {
    "title": "Scaling Sparsemax Based Channel Selection for Speech Recognition with ad-hoc Microphone Arrays",
    "volume": "main",
    "abstract": "Recently, speech recognition with ad-hoc microphone arrays has received much attention. It is known that channel selection is an important problem of ad-hoc microphone arrays, however, this topic seems far from explored in speech recognition yet, particularly with a large-scale ad-hoc microphone array. To address this problem, we propose a algorithm for the channel selection problem of the speech recognition with large-scale ad-hoc microphone arrays. Specifically, we first replace the conventional Softmax operator in the stream attention mechanism of a multichannel end-to-end speech recognition system with Sparsemax, which conducts channel selection by forcing the channel weights of noisy channels to zero. Because Sparsemax punishes the weights of many channels to zero harshly, we propose Scaling Sparsemax which punishes the channels mildly by setting the weights of very noisy channels to zero only. Experimental results with ad-hoc microphone arrays of over 30 channels under the conformer speech recognition architecture show that the proposed Scaling Sparsemax yields a word error rate of over 30% lower than Softmax on simulation data sets, and over 20% lower on semi-real data sets, in test scenarios with both matched and mismatched channel numbers",
    "checked": true,
    "id": "d359eee45dc476c98324aff01240ad1163818a97",
    "semantic_title": "scaling sparsemax based channel selection for speech recognition with ad-hoc microphone arrays",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chang21_interspeech.html": {
    "title": "Multi-Channel Transformer Transducer for Speech Recognition",
    "volume": "main",
    "abstract": "Multi-channel inputs offer several advantages over single-channel, to improve the robustness of on-device speech recognition systems. Recent work on multi-channel transformer, has proposed a way to incorporate such inputs into end-to-end ASR for improved accuracy. However, this approach is characterized by a high computational complexity, which prevents it from being deployed in on-device systems. In this paper, we present a novel speech recognition model, , which features end-to-end multi-channel training, low computation cost, and low latency so that it is suitable for streaming decoding in on-device speech recognition. In a far-field in-house dataset, our MCTT outperforms stagewise multi-channel models with transformer-transducer up to 6.01% relative WER improvement (WERR). In addition, MCTT outperforms the multi-channel transformer up to 11.62% WERR, and is 15.8 times faster in terms of inference speed. We further show that we can improve the computational cost of MCTT by constraining the future and previous context in attention computations",
    "checked": true,
    "id": "f35e8a7a2bafa8ccb12ae165294e165dfdb986d8",
    "semantic_title": "multi-channel transformer transducer for speech recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tsunoo21_interspeech.html": {
    "title": "Data Augmentation Methods for End-to-End Speech Recognition on Distant-Talk Scenarios",
    "volume": "main",
    "abstract": "Although end-to-end automatic speech recognition (E2E ASR) has achieved great performance in tasks that have numerous paired data, it is still challenging to make E2E ASR robust against noisy and low-resource conditions. In this study, we investigated data augmentation methods for E2E ASR in distant-talk scenarios. E2E ASR models are trained on the series of CHiME challenge datasets, which are suitable tasks for studying robustness against noisy and spontaneous speech. We propose to use three augmentation methods and their combinations: 1) data augmentation using text-to-speech (TTS) data, 2) cycle-consistent generative adversarial network (Cycle-GAN) augmentation trained to map two different audio characteristics, the one of clean speech and of noisy recordings, to match the testing condition, and 3) pseudo-label augmentation provided by the pretrained ASR module for smoothing label distributions. Experimental results using the CHiME-6/CHiME-4 datasets show that each augmentation method individually improves the accuracy on top of the conventional SpecAugment; further improvements are obtained by combining these approaches. We achieved 4.3% word error rate (WER) reduction, which was more significant than that of the SpecAugment, when we combine all three augmentations for the CHiME-6 task",
    "checked": true,
    "id": "77cd3ae8a0b9ef6865d5324a4d62280e6f7a1053",
    "semantic_title": "data augmentation methods for end-to-end speech recognition on distant-talk scenarios",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21_interspeech.html": {
    "title": "Leveraging Phone Mask Training for Phonetic-Reduction-Robust E2E Uyghur Speech Recognition",
    "volume": "main",
    "abstract": "In Uyghur speech, consonant and vowel reduction are often encountered, especially in spontaneous speech with high speech rate, which will cause a degradation of speech recognition performance. To solve this problem, we propose an effective phone mask training method for Conformer-based Uyghur end-to-end (E2E) speech recognition. The idea is to randomly mask off a certain percentage features of phones during model training, which simulates the above verbal phenomena and facilitates E2E model to learn more contextual information. According to experiments, the above issues can be greatly alleviated. In addition, deep investigations are carried out into different units in masking, which shows the effectiveness of our proposed masking unit. We also further study the masking method and optimize filling strategy of phone mask. Finally, compared with Conformer-based E2E baseline without mask training, our model demonstrates about 5.51% relative Word Error Rate (WER) reduction on reading speech and 12.92% on spontaneous speech, respectively. The above approach has also been verified on test-set of open-source data THUYG-20, which shows 20% relative improvements",
    "checked": true,
    "id": "a090394ade7c1d094b268282e0872d5547e16c91",
    "semantic_title": "leveraging phone mask training for phonetic-reduction-robust e2e uyghur speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/likhomanenko21_interspeech.html": {
    "title": "Rethinking Evaluation in ASR: Are Our Models Robust Enough?",
    "volume": "main",
    "abstract": "Is pushing numbers on a single benchmark valuable in automatic speech recognition? Research results in acoustic modeling are typically evaluated based on performance on a single dataset. While the research community has coalesced around various benchmarks, we set out to understand generalization performance in acoustic modeling across datasets â€” in particular, if models trained on a single dataset transfer to other (possibly out-of-domain) datasets. Further, we demonstrate that when a large enough set of benchmarks is used, average word error rate (WER) performance over them provides a good proxy for performance on real-world data. Finally, we show that training a single acoustic model on the most widely-used datasets â€” combined â€” reaches competitive performance on both research and real-world benchmarks",
    "checked": true,
    "id": "62ce4d65335c32844247e0ecf3be6de1ccb924b2",
    "semantic_title": "rethinking evaluation in asr: are our models robust enough?",
    "citation_count": 65
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lam21_interspeech.html": {
    "title": "Raw Waveform Encoder with Multi-Scale Globally Attentive Locally Recurrent Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end speech recognition generally uses hand-engineered acoustic features as input and excludes the feature extraction module from its joint optimization. To extract learnable and adaptive features and mitigate information loss, we propose a new encoder that adopts globally attentive locally recurrent (GALR) networks and directly takes raw waveform as input. We observe improved ASR performance and robustness by applying GALR on different window lengths to aggregate fine-grain temporal information into multi-scale acoustic features. Experiments are conducted on a benchmark dataset and two large-scale Mandarin speech corpus of 5,000 hours and 21,000 hours. With faster speed and comparable model size, our proposed multi-scale GALR waveform encoder achieved consistent character error rate reductions (CERRs) from 7.9% to 28.1% relative over strong baselines, including Conformer and TDNN-Conformer. In particular, our approach demonstrated notable robustness than the traditional handcrafted features and outperformed the baseline MFCC-based TDNN-Conformer model by a 15.2% CERR on a music-mixed real-world speech test set",
    "checked": true,
    "id": "0610eab8e6f454486006ff16b27aed975af9dccf",
    "semantic_title": "raw waveform encoder with multi-scale globally attentive locally recurrent networks for end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hou21_interspeech.html": {
    "title": "Attention-Based Cross-Modal Fusion for Audio-Visual Voice Activity Detection in Musical Video Streams",
    "volume": "main",
    "abstract": "Many previous audio-visual voice-related works focus on speech, ignoring the singing voice in the growing number of musical video streams on the Internet. For processing diverse musical video data, voice activity detection is a necessary step. This paper attempts to detect the speech and singing voices of target performers in musical video streams using audio-visual information. To integrate information of audio and visual modalities, a multi-branch network is proposed to learn audio and image representations, and the representations are fused by attention based on semantic similarity to shape the acoustic representations through the probability of anchor vocalization. Experiments show the proposed audio-visual multi-branch network far outperforms the audio-only model in challenging acoustic environments, indicating the cross-modal information fusion based on semantic correlation is sensible and successful",
    "checked": true,
    "id": "508d61701998e2d22cf2ed53c1ca8b28f2cf39a0",
    "semantic_title": "attention-based cross-modal fusion for audio-visual voice activity detection in musical video streams",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21b_interspeech.html": {
    "title": "Noise-Tolerant Self-Supervised Learning for Audio-Visual Voice Activity Detection",
    "volume": "main",
    "abstract": "Recent audio-visual voice activity detectors based on supervised learning require large amounts of labeled training data with manual mouth-region cropping in videos, and the performance is sensitive to a mismatch between the training and testing noise conditions. This paper introduces contrastive self-supervised learning for audio-visual voice activity detection as a possible solution to such problems. In addition, a novel self-supervised learning framework is proposed to improve overall training efficiency and testing performance on noise-corrupted datasets, as in real-world scenarios. This framework includes a branched audio encoder and a noise-tolerant loss function to cope with the uncertainty of speech and noise feature separation in a self-supervised manner. Experimental results, particularly under mismatched noise conditions, demonstrate the improved performance compared with a self-supervised learning baseline and a supervised learning framework",
    "checked": true,
    "id": "bdfeba36c6cac3b3033e72ec3345d619e8b1d1fd",
    "semantic_title": "noise-tolerant self-supervised learning for audio-visual voice activity detection",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/park21_interspeech.html": {
    "title": "Noisy Student-Teacher Training for Robust Keyword Spotting",
    "volume": "main",
    "abstract": "We propose self-training with noisy student-teacher approach for streaming keyword spotting, that can utilize large-scale unlabeled data and aggressive data augmentation. The proposed method applies aggressive data augmentation (spectral augmentation) on the input of both student and teacher and utilize unlabeled data at scale, which significantly boosts the accuracy of student against challenging conditions. Such aggressive augmentation usually degrades model performance when used with supervised training with hard-labeled data. Experiments show that aggressive spec augmentation on baseline supervised training method degrades accuracy, while the proposed self-training with noisy student-teacher training improves accuracy of some difficult-conditioned test sets by as much as 60%",
    "checked": true,
    "id": "d81bfd0b57ab77a51aa2108e20861f1394629e96",
    "semantic_title": "noisy student-teacher training for robust keyword spotting",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ichikawa21_interspeech.html": {
    "title": "Multi-Channel VAD for Transcription of Group Discussion",
    "volume": "main",
    "abstract": "Attempts are being made to visualize the learning process by attaching microphones to students participating in group works conducted in classrooms, and subsequently, their speech using an automatic speech recognition (ASR) system. However, the voices of nearby students frequently become mixed with the output speech data, even when using close-talk microphones with noise robustness. To resolve this challenge, in this paper, we propose using multi-channel voice activity detection (VAD) to determine the speech segments of a target speaker while also referencing the output speech from the microphones attached to the other speakers in the group. The conducted evaluation experiments using the actual speech of middle school students during group work lessons showed that our proposed method significantly improves the frame error rate (38.7%) compared to that of the conventional technology, single-channel VAD (49.5%). In our view, conventional approaches, such as distributed microphone arrays and deep learning, are somewhat dependent on the temporal stationarity of the speakers' positions. However, the proposed method is essentially a VAD process and thus works robustly. It is the practical and proven solution in a real classroom environment",
    "checked": true,
    "id": "6b1c9a7316501307e88c04af959e726af09c8ef9",
    "semantic_title": "multi-channel vad for transcription of group discussion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21_interspeech.html": {
    "title": "Audio-Visual Information Fusion Using Cross-Modal Teacher-Student Learning for Voice Activity Detection in Realistic Environments",
    "volume": "main",
    "abstract": "We propose an information fusion approach to audio-visual voice activity detection (AV-VAD) based on cross-modal teacher-student learning leveraging on factorized bilinear pooling (FBP) and Kullback-Leibler (KL) regularization. First, we design an audio-visual network by using FBP fusion to fully utilize the interaction between audio and video modalities. Next, to transfer the rich information in audio-based VAD (A-VAD) model trained with a massive audio-only dataset to AV-VAD model built with relatively limited multi-modal data, a cross-modal teacher-student learning framework is then proposed based on cross entropy with regulated KL-divergence. Finally, evaluated on an in-house dataset recorded in realistic conditions using standard VAD metrics, the proposed approach yields consistent and significant improvements over other state-of-the-art techniques. Moreover, by applying our AV-VAD technique to an audio-visual Chinese speech recognition task, the character error rate is reduced by 24.15% and 8.66% from A-VAD and the baseline AV-VAD systems, respectively",
    "checked": true,
    "id": "36878855de3551d0dce4db978e3d87fb7483296f",
    "semantic_title": "audio-visual information fusion using cross-modal teacher-student learning for voice activity detection in realistic environments",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/makishima21_interspeech.html": {
    "title": "Enrollment-Less Training for Personalized Voice Activity Detection",
    "volume": "main",
    "abstract": "We present a novel personalized voice activity detection (PVAD) learning method that does not require enrollment data during training. PVAD is a task to detect the speech segments of a specific target speaker at the frame level using enrollment speech of the target speaker. Since PVAD must learn speakers' speech variations to clarify the boundary between speakers, studies on PVAD used large-scale datasets that contain many utterances for each speaker. However, the datasets to train a PVAD model are often limited because substantial cost is needed to prepare such a dataset. In addition, we cannot utilize the datasets used to train the standard VAD because they often lack speaker labels. To solve these problems, our key idea is to use one utterance as both a kind of enrollment speech and an input to the PVAD during training, which enables PVAD training without enrollment speech. In our proposed method, called enrollment-less training, we augment one utterance so as to create variability between the input and the enrollment speech while keeping the speaker identity, which avoids the mismatch between training and inference. Our experimental results demonstrate the efficacy of the method",
    "checked": true,
    "id": "d6c44c2faa653eef801ffbd51292bcfa23135072",
    "semantic_title": "enrollment-less training for personalized voice activity detection",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nonaka21_interspeech.html": {
    "title": "Voice Activity Detection for Live Speech of Baseball Game Based on Tandem Connection with Speech/Noise Separation Model",
    "volume": "main",
    "abstract": "When applying voice activity detection (VAD) to a noisy sound, in general, noise reduction (speech separation) and VAD are performed separately. In this case, the noise reduction may suppress the speech, and the VAD may not work well for the speech after the noise reduction. This study proposes a VAD model through the tandem connection of neural network-based noise separation and a VAD model. By training the two models simultaneously, the noise separation model is expected to be trained to consider the VAD results, and thus effective noise separation can be achieved. Moreover, the improved speech/noise separation model will improve the accuracy of the VAD model. In this research, we deal with real-live speeches from baseball games, which have a very poor signal-to-noise ratio. The VAD experiments showed that the VAD performance at the frame level achieved 4.2 points improvement in F1-score by tandemly connecting the speech/noise separation model and the VAD model",
    "checked": true,
    "id": "10dbd2366917c7ef826fb8255412ee8747163560",
    "semantic_title": "voice activity detection for live speech of baseball game based on tandem connection with speech/noise separation model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kwon21_interspeech.html": {
    "title": "FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications",
    "volume": "main",
    "abstract": "Various incremental learning (IL) approaches have been proposed to help deep learning models learn new tasks/classes continuously without forgetting what was learned previously (i.e., avoid catastrophic forgetting). With the growing number of deployed audio sensing applications that need to dynamically incorporate new tasks and changing input distribution from users, the ability of IL on-device becomes essential for both efficiency and user privacy However, prior works suffer from high computational costs and storage demands which hinders the deployment of IL on-device. In this work, to overcome these limitations, we develop an end-to-end and on-device IL framework, FastICARL, that incorporates an exemplar-based IL and quantization in the context of audio-based applications. We first employ k-nearest-neighbor to reduce the latency of IL. Then, we jointly utilize a quantization technique to decrease the storage requirements of IL. We implement FastICARL on two types of mobile devices and demonstrate that FastICARL remarkably decreases the IL time up to 78â€“92% and the storage requirements by 2â€“4 times without sacrificing its performance. FastICARL enables complete on-device IL, ensuring user privacy as the user data does not need to leave the device",
    "checked": true,
    "id": "92faea491cfb376477073aa4a070fa0973bba032",
    "semantic_title": "fasticarl: fast incremental classifier and representation learning with efficient budget allocation in audio sensing applications",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wei21_interspeech.html": {
    "title": "End-to-End Transformer-Based Open-Vocabulary Keyword Spotting with Location-Guided Local Attention",
    "volume": "main",
    "abstract": "Open-vocabulary keyword spotting (KWS) aims to detect arbitrary keywords from continuous speech, which allows users to define their personal keywords. In this paper, we propose a novel location guided end-to-end (E2E) keyword spotting system. Firstly, we predict endpoints of keyword in the entire speech based on attention mechanism. Secondly, we calculate the existence probability of keyword by fusing the located keyword speech segment and text with local attention. The results on Librispeech dataset and Google speech commands dataset show our proposed method significantly outperforms the baseline method and the latest small-footprint E2E KWS method",
    "checked": true,
    "id": "ce14a649fcf10aeaf9eca0ed7d2dcfff1383e979",
    "semantic_title": "end-to-end transformer-based open-vocabulary keyword spotting with location-guided local attention",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bhati21_interspeech.html": {
    "title": "Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation",
    "volume": "main",
    "abstract": "Automatic detection of phoneme or word-like units is one of the core objectives in zero-resource speech processing. Recent attempts employ self-supervised training methods, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework that can model the signal structure at a higher level e.g. at the phoneme level. In this framework, a convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Typically, phoneme and word segmentation are treated as separate tasks. We unify them and experimentally show that our single model outperforms existing phoneme and word segmentation methods on TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and when is the right time to include the segmental loss in the learning process",
    "checked": true,
    "id": "642dab29e680f516eb25949d616a24e0ad147a19",
    "semantic_title": "segmental contrastive predictive coding for unsupervised word segmentation",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21b_interspeech.html": {
    "title": "A Lightweight Framework for Online Voice Activity Detection in the Wild",
    "volume": "main",
    "abstract": "Voice activity detection (VAD) is an essential pre-processing component for speech-related tasks such as automatic speech recognition (ASR). Traditional VAD systems require strong frame-level supervision for training, inhibiting their performance in real-world test scenarios. Previously, the general-purpose VAD (GPVAD) framework has been proposed to enhance noise robustness significantly. However, GPVAD models are comparatively large and only work for offline evaluation. This work proposes the use of a knowledge distillation framework, where a (large, offline) teacher model provides frame-level supervision to a (light, online) student model. Our experiments verify that our proposed lightweight student models outperform GPVAD on all test sets, including clean, synthetic and real-world scenarios. Our smallest student model only uses 2.2% of the parameters and 15.9% duration cost of our teacher model for inference when evaluated on a Raspberry Pi",
    "checked": true,
    "id": "a4557f8c519bdc765c00124cc86736e9c359eb77",
    "semantic_title": "a lightweight framework for online voice activity detection in the wild",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chlebowski21_interspeech.html": {
    "title": "See what I mean, huh?\" Evaluating Visual Inspection of F",
    "volume": "main",
    "abstract": "This paper proposes to evaluate the method used in ChlÃ©bowski and Ballier [1] for the annotation of F variations in nasal grunts. We discuss and test issues raised by this kind of approach exclusively based on visual inspection of the F tracking in [2]. Results tend to show that consistency in the annotation depends on acoustic features intrinsic to the grunts such as F slope and duration that are sensitive to display settings. We nonetheless acknowledge the potential benefits of such a method for automation and implementation in IA and in this respect, we introduce [3] as an alternative material-maker",
    "checked": false,
    "id": "50b077c4c2e04dd22acd939a16266c0dc10b348f",
    "semantic_title": "see what i mean, huh?\" evaluating visual inspection of f0 tracking in nasal grunts",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21c_interspeech.html": {
    "title": "System Performance as a Function of Calibration Methods, Sample Size and Sampling Variability in Likelihood Ratio-Based Forensic Voice Comparison",
    "volume": "main",
    "abstract": "In data-driven forensic voice comparison, sample size is an issue which can have substantial effects on system output. Numerous calibration methods have been developed and some have been proposed as solutions to sample size issues. In this paper, we test four calibration methods (i.e. logistic regression, regularised logistic regression, Bayesian model, ELUB) under different conditions of sampling variability and sample size. Training and test scores were simulated from skewed distributions derived from real experiments, increasing sample sizes from 20 to 100 speakers for both the training and test sets. For each sample size, the experiments were replicated 100 times to test the susceptibility of different calibration methods to sampling variability. The C mean and range across replications were used for evaluation. The Bayesian model and regularized logistic regression produced the most stable C values when the sample size is small (i.e. 20 speakers), although mean C is consistently lowest using logistic regression. The ELUB calibration method generally is the least preferred as it is the most sensitive to sample size and sampling variability (mean = 0.66, range = 0.21â€“0.59)",
    "checked": true,
    "id": "51bee70b4a53baa635f8b4fdd523e62410ce924c",
    "semantic_title": "system performance as a function of calibration methods, sample size and sampling variability in likelihood ratio-based forensic voice comparison",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bonneau21_interspeech.html": {
    "title": "Voicing Assimilations by French Speakers of German in Stop-Fricative Sequences",
    "volume": "main",
    "abstract": "Voicing assimilations inside groups of obstruents occur in opposite directions in French and German, where they are respectively regressive and progressive. The aim of the study is to investigate (1) whether non native speakers (here French learners of German) are apt to acquire subtle L2 specificities like assimilation direction, although they are not aware of their very existence, or (2) whether their productions depend essentially upon other factors, in particular consonant place of articulation. To that purpose, a corpus made up of groups of obstruents (/t/ followed by /z/, /v/ or /f/) embedded into sentences has been recorded by 16 French learners of German (beginners and advanced speakers). The consonants are separated by a word or a syllable boundary. Results, derived from the analysis of consonant periodicity and duration, do not stand for an acquisition of progressive assimilation, even by advanced speakers, and do not show differences between the productions of advanced speakers and beginners. On the contrary the boundary type and the consonant place of articulation play an important role in the presence or absence of voicing inside obstruent groups. The role of phonetic, universal mechanisms against linguistic specific rules is discussed to interpret the data",
    "checked": true,
    "id": "6d4c67542f20ff95ce6aa41ff9c71f6bbab903d5",
    "semantic_title": "voicing assimilations by french speakers of german in stop-fricative sequences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chakraborty21_interspeech.html": {
    "title": "The Four-Way Classification of Stops with Voicing and Aspiration for Non-Native Speech Evaluation",
    "volume": "main",
    "abstract": "The four-way distinction of plosives in terms of voicing and aspiration is rare in the world's languages, but is an important characteristic of the Indo-Aryan language family. Both perception and production pose challenges to the language learner whose native tongue does not afford the specific distinctions. A study of the acoustic-phonetics of the sounds and their possible dependence on speaker characteristics, such as gender or native tongue, can inform methods for accurate feedback on the quality of the phones produced by a non-native learner. We present a system for the four-way classification of stops building on features previously proposed for aspiration detection in unvoiced and voiced plosives. Trained on an available dataset of Hindi speech by native speakers, the system works reliably on production data comprising Bangla words uttered by native Bangla and non-native (American English L1) speakers. The latter display a variety of articulation patterns for the given target contrasts, providing useful insights related to L1 influence on the voicing-aspiration production in word-initial CV contexts",
    "checked": true,
    "id": "b6f2a10ed88619aef50ae19dc486cbc3058f5b8f",
    "semantic_title": "the four-way classification of stops with voicing and aspiration for non-native speech evaluation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/urooj21_interspeech.html": {
    "title": "Acoustic and Prosodic Correlates of Emotions in Urdu Speech",
    "volume": "main",
    "abstract": "Emotional speech corpora exhibit differences in duration, intensity and fundamental frequency. We investigated acoustic as well as prosodic correlates of emotional speech in Urdu. We recorded a corpus of 23 sentences from four speakers of Urdu covering four emotional states. Main results show that: a) sadness exhibits lowest utterance rate, lowest intensity and narrow pitch range, b) anger exhibits highest utterance rate, highest intensity and wider pitch range, and c) happiness exhibits higher utterance rate and wider pitch range as compared to neutral and sadness; but no significant differences are found between the intensity and pitch range of anger and happiness. The analysis also shows differences in terms of pitch or phrase accents and boundary tones",
    "checked": true,
    "id": "634c84a5806bd95455bb11938238a07628c895aa",
    "semantic_title": "acoustic and prosodic correlates of emotions in urdu speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tamim21_interspeech.html": {
    "title": "Voicing Contrasts in the Singleton Stops of Palestinian Arabic: Production and Perception",
    "volume": "main",
    "abstract": "This study investigates the stop voicing contrast in Palestinian Arabic (PA) by examining Voice Onset Time (VOT) in both production and perception. An acoustic analysis of the recordings of 8 speakers showed that word-initial voiced stops in sentence context have an average VOT of -93 msec, and word-initial voiceless stops one of 29 msec. PA thus belongs, like most dialects of Arabic, to true voicing languages, i.e., languages with a contrast between voicing lead and short lag VOT We furthermore tested whether the phoneme /b/, without voiceless counterpart /p/ in PA, has similar VOT values to /d, d /, which have voiceless counterparts /t, t /. Similarly, we compared /k/, without counterpart /g/ in the PA dialect we investigated, to /t, t /. For /b/ we found very similar VOT values to /d, d /, while for /k/ we found a difference to /t, t /, attributable to a general tendency of velars to have longer VOT than denti-alveolars. We thus found no evidence for a less contrastive realization of unpaired plosives in PA In a categorization experiment of the denti-alveolar phoneme pairs with the same 8 speakers, VOT proved sufficient as a perceptual cue, though f0 of the following vowel also influenced the categorization",
    "checked": true,
    "id": "e1f3460fc7dc182432405a26643ecc96603cfdc4",
    "semantic_title": "voicing contrasts in the singleton stops of palestinian arabic: production and perception",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/coy21_interspeech.html": {
    "title": "A Comparison of the Accuracy of Dissen and Keshet's (2016) DeepFormants and Traditional LPC Methods for Semi-Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "There is a growing trend in the field of forensic speech science towards integrating the vanguard of speech technology with traditional linguistic methods in pursuit of both scalable (i.e. automatable) and accurate evidential methods. To this end, this paper investigates DeepFormants, a DNN formant estimator which its creators, Dissen and Keshet [1], claim constitutes an accurate tool ready for use by linguists. In the present paper, DeepFormants is integrated into semi-automatic speaker recognition systems using long-term formant distributions and compared against systems using traditional linear predictive coding. The readiness of the tool is assessed on overall speaker recognition performance, measured using equal error rates (EER) and the log LR cost functions (C ). In high-quality conditions, DeepFormants outperforms the best performing LPC systems. Much poorer overall performance is found in channel mismatch conditions for DeepFormants, suggesting it is not adaptable to conditions it was not originally trained on. However, this is also true of LPC methods, raising questions over the validity of using formant analysis at all in such cases. A major benefit of DeepFormants over LPC is that the analyst does not need to specify settings. We discuss the implications of this with regard to results for individual speakers",
    "checked": true,
    "id": "03415b1cfdb19bff136be84559da5e807187a305",
    "semantic_title": "a comparison of the accuracy of dissen and keshet's (2016) deepformants and traditional lpc methods for semi-automatic speaker recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jessen21_interspeech.html": {
    "title": "MAP Adaptation Characteristics in Forensic Long-Term Formant Analysis",
    "volume": "main",
    "abstract": "Forensic data from long-term formant analysis were used as input to the GMM-UBM approach, which is a way of deriving Likelihood Ratios. Tests were performed running 22 same-speaker comparisons and 462 different-speaker comparisons from a corpus of anonymized casework data involving telephone-intercepted speech. In a first series of tests, the number of Gaussian modules for GMM-modeling was increased from 1 to 32. In a second series of tests the duration of formant input in the compared files was reduced from 10 seconds to 5 and then to 2.5. All tests were performed both without and with the use of MAP adaptation. Results were evaluated in terms of overall performance characteristics EER and Cllr and in terms of score distributions visualized as Tippett plots. The main goal of the study was to compare the use and non-use of MAP and to look at the practical forensic implications of the difference. Results show that in terms of overall performance characteristics there is little difference between the selection and de-selection of MAP. Tippett plot patterns however reveal strong differences. Application of MAP allows for more symmetric same- and different-speaker distributions and shows more robustness against duration reductions, both of which are forensically important",
    "checked": true,
    "id": "2f2a4f02f9c5fe54ad2f99dfec14028b3f13b473",
    "semantic_title": "map adaptation characteristics in forensic long-term formant analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lo21_interspeech.html": {
    "title": "Cross-Linguistic Speaker Individuality of Long-Term Formant Distributions: Phonetic and Forensic Perspectives",
    "volume": "main",
    "abstract": "This study considers issues of language- and speaker-specificity in long-term formant distributions (LTFDs) from phonetic and forensic perspectives and examines their potential value in cases of cross-language forensic voice comparison. Acoustic analysis of 60 male Englishâ€“French bilinguals revealed systematic differences in LTFDs between the two languages, with higher LTF2â€“4 in French than in English. Cross-linguistic differences in the shapes of LTFDs were also found. These differences are argued to reflect not only vowel inventories of each language but also language-specific phonetic settings. At the same time, a high degree of within-speaker consistency was found across languages. Likelihood ratio based testing was carried out to examine the effect of language mismatch on the utility of LTFDs as speaker discriminants. Results showed that while the performance of LTFDs was worse in cross-language comparisons than in same-language comparisons, they were still capable of providing speaker-specific information. These findings demonstrate that, in spite of deteriorated performance, LTFDs are still potentially useful speaker discriminants in cases of language mismatch. These findings thus call for further empirical investigation into the use of linguistic-phonetic features in cross-language comparisons",
    "checked": true,
    "id": "7f2ddbb9a0184e81b9252d5b81538bc41c015040",
    "semantic_title": "cross-linguistic speaker individuality of long-term formant distributions: phonetic and forensic perspectives",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/soo21_interspeech.html": {
    "title": "Sound Change in Spontaneous Bilingual Speech: A Corpus Study on the Cantonese n-l Merger in Cantonese-English Bilinguals",
    "volume": "main",
    "abstract": "In Cantonese and several other Chinese languages, /n/ is merging with /l/. The Cantonese merger appears categorical, with /n/ becoming /l/ word-initially. This project aims to describe the status of /n/ and /l/ in bilingual Cantonese and English speech to better understand individual differences at the interface of crosslinguistic influence and sound change. We examine bilingual speech using the SpiCE corpus, composed of speech from 34 early Cantonese-English bilinguals. Acoustic measures were collected on pre-vocalic nasal and lateral onsets in both languages. If bilinguals maintain separate representations for corresponding segments across languages, smaller differences between /n/ and /l/ are predicted in Cantonese compared to English. Measures of mid-frequency spectral tilt suggest that the /n/ and /l/ contrast is robustly maintained in English, but not Cantonese. The spacing of F2-F1 suggests small differences between Cantonese /n/ and /l/, and robust differences in English. While cross-language categories appear independent, substantial individual differences exist in the data. These data contribute to the understanding of the /n/ and /l/ merger in Cantonese and other Chinese languages, in addition to providing empirical and theoretical insights into crosslinguistic influence in early bilinguals",
    "checked": true,
    "id": "33e65d11709fee658352706e51b3f91efde51710",
    "semantic_title": "sound change in spontaneous bilingual speech: a corpus study on the cantonese n-l merger in cantonese-english bilinguals",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lalhminghlui21_interspeech.html": {
    "title": "Characterizing Voiced and Voiceless Nasals in Mizo",
    "volume": "main",
    "abstract": "Mizo has voicing contrasts in nasals. This study investigates the acoustic properties of Mizo voiced and voiceless nasals using nasometric measurements. The dual channel data obtained for Mizo nasals is separated into oral and nasal channels and nasalance is calculated at every 10% of the duration of the nasals. Apart from that, the amount of voicing and duration of the nasals are also measured. The results show that nasalance is affected by the place of articulation of the nasals. Additionally, the voiceless nasals are found to be significantly longer than the voiced nasals",
    "checked": true,
    "id": "63040a44e70d2c4ecbd62ca15567ece42c26dbbe",
    "semantic_title": "characterizing voiced and voiceless nasals in mizo",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schuller21_interspeech.html": {
    "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation & Primates",
    "volume": "main",
    "abstract": "The INTERSPEECH 2021 Computational Paralinguistics Challenge addresses four different problems for the first time in a research competition under well-defined conditions: In the and Sub-Challenges, a binary classification on COVID-19 infection has to be made based on coughing sounds and speech; in the Sub-Challenge, a three-way assessment of the level of escalation in a dialogue is featured; and in the Sub-Challenge, four species vs background need to be classified. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the â€˜usual' ComParE and BoAW features as well as deep unsupervised representation learning using the auDeep toolkit, and deep feature extraction from pre-trained CNNs using the Deep Spectrum toolkit; in addition, we add deep end-to-end sequential modelling, and partially linguistic analysis",
    "checked": true,
    "id": "12267f4ee5fafc807b793dc09dd8a5b8cee8115e",
    "semantic_title": "the interspeech 2021 computational paralinguistics challenge: covid-19 cough, covid-19 speech, escalation & primates",
    "citation_count": 90
  },
  "https://www.isca-speech.org/archive/interspeech_2021/soleraurena21_interspeech.html": {
    "title": "Transfer Learning-Based Cough Representations for Automatic Detection of COVID-19",
    "volume": "main",
    "abstract": "In the last months, there has been an increasing interest in developing reliable, cost-effective, immediate and easy to use machine learning based tools that can help health care operators, institutions, companies, etc. to optimize their screening campaigns. In this line, several initiatives emerged aimed at the automatic detection of COVID-19 from speech, breathing and coughs, with inconclusive preliminary results. The ComParE 2021 COVID-19 Cough Sub-challenge provides researchers from all over the world a suitable test-bed for the evaluation and comparison of their work. In this paper, we present the INESC-ID contribution to the ComParE 2021 COVID-19 Cough Sub-challenge. We leverage transfer learning to develop a set of three expert classifiers based on deep cough representation extractors. A calibrated decision-level fusion system provides the final classification of coughs recordings as either COVID-19 positive or negative. Results show unweighted average recalls of 72.3% and 69.3% in the development and test sets, respectively. Overall, the experimental assessment shows the potential of this approach although much more research on extended respiratory sounds datasets is needed",
    "checked": true,
    "id": "287547a8cf4c372b975535222caabf03202bcc32",
    "semantic_title": "transfer learning-based cough representations for automatic detection of covid-19",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/klumpp21_interspeech.html": {
    "title": "The Phonetic Footprint of Covid-19?",
    "volume": "main",
    "abstract": "Against the background of the ongoing pandemic, this year's Computational Paralinguistics Challenge featured a classification problem to detect Covid-19 from speech recordings. The presented approach is based on a phonetic analysis of speech samples, thus it enabled us not only to discriminate between Covid and non-Covid samples, but also to better understand how the condition influenced an individual's speech signal Our deep acoustic model was trained with datasets collected exclusively from healthy speakers. It served as a tool for segmentation and feature extraction on the samples from the challenge dataset. Distinct patterns were found in the embeddings of phonetic classes that have their place of articulation deep inside the vocal tract. We observed profound differences in classification results for development and test splits, similar to the baseline method We concluded that, based on our phonetic findings, it was safe to assume that our classifier was able to reliably detect a pathological condition located in the respiratory tract. However, we found no evidence to claim that the system was able to discriminate between Covid-19 and other respiratory diseases",
    "checked": true,
    "id": "608d76a77413cf7e1e47a926f9838b6fa5cba32d",
    "semantic_title": "the phonetic footprint of covid-19?",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/casanova21_interspeech.html": {
    "title": "Transfer Learning and Data Augmentation Techniques to the COVID-19 Identification Tasks in ComParE 2021",
    "volume": "main",
    "abstract": "In this work, we propose several techniques to address data scarceness in ComParE 2021 COVID-19 identification tasks for the application of deep models such as Convolutional Neural Networks. Data is initially preprocessed into spectrogram or MFCC-gram formats. After preprocessing, we combine three different data augmentation techniques to be applied in model training. Then we employ transfer learning techniques from pretrained audio neural networks. Those techniques are applied to several distinct neural architectures. For COVID-19 identification in speech segments, we obtained competitive results. On the other hand, in the identification task based on cough data, we succeeded in producing a noticeable improvement on existing baselines, reaching 75.9% unweighted average recall (UAR)",
    "checked": true,
    "id": "9a4d3eaf07bd3b90a055f7ca53a07de6683d8291",
    "semantic_title": "transfer learning and data augmentation techniques to the covid-19 identification tasks in compare 2021",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/illium21_interspeech.html": {
    "title": "Visual Transformers for Primates Classification and Covid Detection",
    "volume": "main",
    "abstract": "We apply the vision transformer, a deep machine learning model build around the attention mechanism, on mel-spectrogram representations of raw audio recordings. When adding mel-based data augmentation techniques and sample-weighting, we achieve comparable performance on both (PRS and CCS challenge) tasks of ComParE21, outperforming most single model baselines. We further introduce overlapping vertical patching and evaluate the influence of parameter configurations",
    "checked": true,
    "id": "65fe63a40f54f3c4d231f10f351001a27c6151c5",
    "semantic_title": "visual transformers for primates classification and covid detection",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pellegrini21_interspeech.html": {
    "title": "Deep-Learning-Based Central African Primate Species Classification with MixUp and SpecAugment",
    "volume": "main",
    "abstract": "In this paper, we report experiments in which we aim to automatically classify primate vocalizations according to four primate species of interest, plus a background category with forest sound events. We compare several standard deep neural networks architectures: standard deep convolutional neural networks (CNNs), MobileNets and ResNets. To tackle the small size of the training dataset, less than seven thousand audio files, the data augmentation techniques SpecAugment and MixUp proved to be very useful. Against the very unbalanced classes of the dataset, we used a balanced data sampler that showed to be efficient. An exponential moving average of the model weights allowed to get slight further gains. The best model was a standard 10-layer CNN, comprised of about five million parameters. It achieved a 93.6% Unweighted Average Recall (UAR) on the development set, and generalized well on the test set with a 92.5% UAR, outperforming an official baseline of 86.6%. We quantify the performance gains brought by the augmentations and training tricks, and report fusion and classification experiments based on embeddings that did not bring better results",
    "checked": true,
    "id": "e343a2d6792e3739a9c0de9857e34b7eb59b72cc",
    "semantic_title": "deep-learning-based central african primate species classification with mixup and specaugment",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muller21_interspeech.html": {
    "title": "A Deep and Recurrent Architecture for Primate Vocalization Classification",
    "volume": "main",
    "abstract": "Wildlife monitoring is an essential part of most conservation efforts where one of the many building blocks is acoustic monitoring. Acoustic monitoring has the advantage of being non-invasive and applicable in areas of high vegetation. In this work, we present a deep and recurrent architecture for the classification of primate vocalizations that is based upon well proven modules such as bidirectional Long Short-Term Memory neural networks, pooling, normalized softmax and focal loss. Additionally, we apply Bayesian optimization to obtain a suitable set of hyperparameters. We test our approach on a recently published dataset of primate vocalizations that were recorded in an African wildlife sanctuary. Using an ensemble of the best five models found during hyperparameter optimization on the development set, we achieve a Unweighted Average Recall of 89.3% on the test set. Our approach outperforms the best baseline, an ensemble of various deep and shallow classifiers, which achieves a UAR of 87.5%",
    "checked": true,
    "id": "d4008aeffed6e875397c94433cbee47595e0ae75",
    "semantic_title": "a deep and recurrent architecture for primate vocalization classification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zwerts21_interspeech.html": {
    "title": "Introducing a Central African Primate Vocalisation Dataset for Automated Species Classification",
    "volume": "main",
    "abstract": "Automated classification of animal vocalisations is a potentially powerful wildlife monitoring tool. Training robust classifiers requires sizable annotated datasets, which are not easily recorded in the wild. To circumvent this problem, we recorded four primate species under semi-natural conditions in a wildlife sanctuary in Cameroon with the objective to train a classifier capable of detecting species in the wild. Here, we introduce the collected dataset, describe our approach and initial results of classifier development. To increase the efficiency of the annotation process, we condensed the recordings with an energy/change based automatic vocalisation detection. Segmenting the annotated chunks into training, validation and test sets, initial results reveal up to 82% unweighted average recall test set performance in four-class primate species classification",
    "checked": true,
    "id": "98a1034c5c0d30f7c83d3a637aa36de02c58e64f",
    "semantic_title": "introducing a central african primate vocalisation dataset for automated species classification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rizos21_interspeech.html": {
    "title": "Multi-Attentive Detection of the Spider Monkey Whinny in the (Actual) Wild",
    "volume": "main",
    "abstract": "We study deep bioacoustic event detection through multi-head attention based pooling, exemplified by wildlife monitoring. In the multiple instance learning framework, a core deep neural network learns a projection of the input acoustic signal into a sequence of embeddings, each representing a segment of the input. Sequence pooling is then required to aggregate the information present in the sequence such that we have a single clip-wise representation. We propose an improvement based on Squeeze-and-Excitation mechanisms upon a recently proposed audio tagging ResNet, and show that it performs significantly better than the baseline, as well as a collection of other recent audio models. We then further enhance our model, by performing an extensive comparative study of recent sequence pooling mechanisms, and achieve our best result using multi-head self-attention followed by concatenation of the head-specific pooled embeddings â€” better than prediction pooling methods, as well as compared to other recent sequence pooling tricks. We perform these experiments on a novel dataset of spider monkey whinny calls we introduce here, recorded in a rainforest in the South-Pacific coast of Costa Rica, with a promising outlook pertaining to minimally invasive wildlife monitoring",
    "checked": true,
    "id": "e472f18d522b9c49f006859ac0caf51072ad1d11",
    "semantic_title": "multi-attentive detection of the spider monkey whinny in the (actual) wild",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/egaslopez21_interspeech.html": {
    "title": "Identifying Conflict Escalation and Primates by Using Ensemble X-Vectors and Fisher Vector Features",
    "volume": "main",
    "abstract": "Computational paralinguistics is concerned with the automatic identification of non-verbal information in human speech. The Interspeech ComParE challenge features new paralinguistic tasks each year; this time, among others, a cross-corpus conflict escalation task and the identification of primates based solely on audio are the actual problems set. In our entry to ComParE 2021, we utilize x-vectors and Fisher vectors as features. To improve the robustness of the predictions, we also experiment with building an ensemble of classifiers from the x-vectors. Lastly, we exploit the fact that the Escalation Sub-Challenge is a conflict detection task, and incorporate the SSPNet Conflict Corpus in our training workflow. Using these approaches, at the time of writing, we had already surpassed the official Challenge baselines on both tasks, which demonstrates the efficiency of the employed techniques",
    "checked": true,
    "id": "f65af2bb8200779b9c09bf7be5567d713b756803",
    "semantic_title": "identifying conflict escalation and primates by using ensemble x-vectors and fisher vector features",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/verkholyak21_interspeech.html": {
    "title": "Ensemble-Within-Ensemble Classification for Escalation Prediction from Speech",
    "volume": "main",
    "abstract": "Conflict situations arise frequently in our daily life and often require timely response to resolve the issues. In order to automatically classify conflict (also referred to as escalation) speech utterances we propose ensemble learning as it improves prediction performance by combining several heterogeneous models that compensate for each other's weaknesses. However, the effectiveness of the classification ensemble greatly depends on its constituents and their fusion strategy. This paper provides experimental evidence for effectiveness of different prediction-level fusion strategies and demonstrates the performance of each proposed ensemble on the Escalation Sub-Challenge (ESS) in the framework of the Computational Paralinguistics Challenge (ComParE-2021). The ensembles comprise various machine learning approaches based on acoustic and linguistic characteristics of speech. The training strategy is specifically designed to increase the generalization performance on the unseen data, while the diverse nature of ensemble candidates ensures high prediction power and accurate classification",
    "checked": true,
    "id": "9dc936b03cae158437d844e152d01fde3641c0d5",
    "semantic_title": "ensemble-within-ensemble classification for escalation prediction from speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schiller21_interspeech.html": {
    "title": "Analysis by Synthesis: Using an Expressive TTS Model as Feature Extractor for Paralinguistic Speech Classification",
    "volume": "main",
    "abstract": "Modeling adequate features of speech prosody is one key factor to good performance in affective speech classification. However, the distinction between the prosody that is induced by â€˜how' something is said (i.e., affective prosody) and the prosody that is induced by â€˜what' is being said (i.e., linguistic prosody) is neglected in state-of-the-art feature extraction systems. This results in high variability of the calculated feature values for different sentences that are spoken with the same affective intent, which might negatively impact the performance of the classification. While this distinction between different prosody types is mostly neglected in affective speech recognition, it is explicitly modeled in expressive speech synthesis to create controlled prosodic variation. In this work, we use the expressive Text-To-Speech model Global Style Token Tacotron to extract features for a speech analysis task. We show that the learned prosodic representations outperform state-of-the-art feature extraction systems in the exemplary use case of Escalation Level Classification",
    "checked": true,
    "id": "3dd293025a3b66ae924067bb29ebee319dda55cc",
    "semantic_title": "analysis by synthesis: using an expressive tts model as feature extractor for paralinguistic speech classification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/christensen21_interspeech.html": {
    "title": "Towards Automatic Speech Recognition for People with Atypical Speech",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "34b94c4c71f0c2de77bfbaab5d9ecf2ba261f85b",
    "semantic_title": "towards automatic speech recognition for people with atypical speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luu21_interspeech.html": {
    "title": "Leveraging Speaker Attribute Information Using Multi Task Learning for Speaker Verification and Diarization",
    "volume": "main",
    "abstract": "Deep speaker embeddings have become the leading method for encoding speaker identity in speaker recognition tasks. The embedding space should ideally capture the variations between all possible speakers, encoding the multiple acoustic aspects that make up a speaker's identity, whilst being robust to non-speaker acoustic variation. Deep speaker embeddings are normally trained discriminatively, predicting speaker identity labels on the training data. We hypothesise that additionally predicting speaker-related auxiliary variables â€” such as age and nationality â€” may yield representations that are better able to generalise to unseen speakers. We propose a framework for making use of auxiliary label information, even when it is only available for speech corpora mismatched to the target application. On a test set of US Supreme Court recordings, we show that by leveraging two additional forms of speaker attribute information derived respectively from the matched training data, and VoxCeleb corpus, we improve the performance of our deep speaker embeddings for both verification and diarization tasks, achieving a relative improvement of 26.2% in DER and 6.7% in EER compared to baselines using speaker labels only. This improvement is obtained despite the auxiliary labels having been scraped from the web and being potentially noisy",
    "checked": true,
    "id": "3843ef594c733b8c8d854535c674d0fbff292eb6",
    "semantic_title": "leveraging speaker attribute information using multi task learning for speaker verification and diarization",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rybicka21_interspeech.html": {
    "title": "Spine2Net: SpineNet with Res2Net and Time-Squeeze-and-Excitation Blocks for Speaker Recognition",
    "volume": "main",
    "abstract": "Modeling speaker embeddings using deep neural networks is currently state-of-the-art in speaker recognition. Recently, ResNet-based structures have gained a broader interest, slowly becoming the baseline along with the deep-rooted Time Delay Neural Network based models. However, the scale-decreased design of the ResNet models may not preserve all of the speaker information. In this paper, we investigate the SpineNet structure with scale-permuted design to tackle this problem, in which feature size either increases or decreases depending on the processing stage in the network. Apart from the presented adjustments of the SpineNet model for the speaker recognition task, we also incorporate popular modules dedicated to the residual-like structures, namely the Res2Net and Squeeze-and-Excitation blocks, and modify them to work effectively in the presented neural network architectures. The final proposed model, i.e., the SpineNet architecture with Res2Net and Time-Squeeze-and-Excitation blocks, achieves remarkable Equal Error Rates of 0.99 and 0.92 for the Extended and Original trial lists of the well-known VoxCeleb1 dataset",
    "checked": true,
    "id": "62a007787bdf51bb58668d2a88df18850c4e9e28",
    "semantic_title": "spine2net: spinenet with res2net and time-squeeze-and-excitation blocks for speaker recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stafylakis21_interspeech.html": {
    "title": "Speaker Embeddings by Modeling Channel-Wise Correlations",
    "volume": "main",
    "abstract": "Speaker embeddings extracted with deep 2D convolutional neural networks are typically modeled as projections of first and second order statistics of channel-frequency pairs onto a linear layer, using either average or attentive pooling along the time axis. In this paper we examine an alternative pooling method, where pairwise correlations between channels for given frequencies are used as statistics. The method is inspired by style-transfer methods in computer vision, where the style of an image, modeled by the matrix of channel-wise correlations, is transferred to another image, in order to produce a new image having the style of the first and the content of the second. By drawing analogies between image style and speaker characteristics, and between image content and phonetic sequence, we explore the use of such channel-wise correlations features to train a ResNet architecture in an end-to-end fashion. Our experiments on VoxCeleb demonstrate the effectiveness of the proposed pooling method in speaker recognition",
    "checked": true,
    "id": "87382405a9f9abb7f0eabce17799b626f2e20e4b",
    "semantic_title": "speaker embeddings by modeling channel-wise correlations",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/he21_interspeech.html": {
    "title": "Multi-Task Neural Network for Robust Multiple Speaker Embedding Extraction",
    "volume": "main",
    "abstract": "This paper introduces a novel approach for extracting speaker embeddings from audio mixtures of multiple overlapping voices. This approach is based on a multi-task neural network. The network first extracts a latent feature for each direction. This feature is used for detecting sound sources as well as identifying speakers. In contrast to traditional approaches, the proposed method does not rely on explicit sound source separation. The neural network model learns from data to extract the most suitable features of the sounds at different directions. The experiments using audio recordings of overlapping sound sources show that the proposed approach outperforms a beamforming-based traditional method",
    "checked": true,
    "id": "e8a9987893824dadd9c72692a2b0448e3951a5a6",
    "semantic_title": "multi-task neural network for robust multiple speaker embedding extraction",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21_interspeech.html": {
    "title": "ICSpk: Interpretable Complex Speaker Embedding Extractor from Raw Waveform",
    "volume": "main",
    "abstract": "Recently, extracting speaker embedding directly from raw waveform has drawn increasing attention in the field of speaker verification. Parametric real-valued filters in the first convolutional layer are learned to transform the waveform into time-frequency representations. However, these methods only focus on the magnitude spectrum and the poor interpretability of the learned filters limits the performance. In this paper, we propose a complex speaker embedding extractor, named ICSpk, with higher interpretability and fewer parameters. Specifically, at first, to quantify the speaker-related frequency response of waveform, we modify the original short-term Fourier transform filters into a family of complex exponential filters, named interpretable complex (IC) filters. Each IC filter is confined by a complex exponential filter parameterized by frequency. Then, a deep complex-valued speaker embedding extractor is designed to operate on the complex-valued output of IC filters. The proposed ICSpk is evaluated on VoxCeleb and CNCeleb databases. Experimental results demonstrate the IC filters-based system exhibits a significant improvement over the complex spectrogram based systems. Furthermore, the proposed ICSpk outperforms existing raw waveform based systems by a large margin",
    "checked": true,
    "id": "4cd567a2dd9b55247179ac136cd236e4941aa4a4",
    "semantic_title": "icspk: interpretable complex speaker embedding extractor from raw waveform",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xiao21_interspeech.html": {
    "title": "Prosodic Disambiguation Using Chironomic Stylization of Intonation with Native and Non-Native Speakers",
    "volume": "main",
    "abstract": "This paper introduces an interface that enables the real-time gestural control of intonation in phrases produced by a vocal synthesizer. The melody and timing of a target phrase can be modified by tracing melodic contours on the touch-screen of a mobile tablet. Envisioning this interface as a means for non-native speakers to practice the intonation of a foreign language, we present a pilot study where native and non-native speakers imitated the pronunciation of French phrases using their voice and the interface, with a visual guide and without. Comparison of resulting F0 curves against the reference contour and a preliminary perceptual assessment of synthesized utterances suggest that for both non-native and native speakers, imitation with the help of a visual guide is comparable in accuracy to vocal imitation, and that timing control was a source of difficulty",
    "checked": true,
    "id": "f97cd9369c07403dfe622fbd74b33ae35622516b",
    "semantic_title": "prosodic disambiguation using chironomic stylization of intonation with native and non-native speakers",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/block21_interspeech.html": {
    "title": "Variation in Perceptual Sensitivity and Compensation for Coarticulation Across Adult and Child Naturally-Produced and TTS Voices",
    "volume": "main",
    "abstract": "The current study explores whether perception of coarticulatory vowel nasalization differs by speaker age (adult vs. child) and type of voice (naturally produced vs. synthetic speech). Listeners completed a 4IAX discrimination task between pairs containing acoustically identical (both nasal or oral) vowels and acoustically distinct (one oral, one nasal) vowels. Vowels occurred in either the same consonant contexts or different contexts across pairs. Listeners completed the experiment with either naturally produced speech or text-to-speech (TTS). For same-context trials, listeners were better at discriminating between oral and nasal vowels for child speech in the synthetic voices but adult speech in the natural voices. Meanwhile, in different-context trials, listeners were less able to discriminate, indicating more perceptual compensation for synthetic voices. There was no difference in different-context discrimination across talker ages, indicating that listeners did not compensate differently if the speaker was a child or adult. Findings are relevant for models of compensation, computer personification theories, and speaker-indexical perception accounts",
    "checked": true,
    "id": "9156bfc6f44686cf3391398e58989978cb87bb4e",
    "semantic_title": "variation in perceptual sensitivity and compensation for coarticulation across adult and child naturally-produced and tts voices",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/monesi21_interspeech.html": {
    "title": "Extracting Different Levels of Speech Information from EEG Using an LSTM-Based Model",
    "volume": "main",
    "abstract": "Decoding the speech signal that a person is listening to from the human brain via electroencephalography (EEG) can help us understand how our auditory system works. Linear models have been used to reconstruct the EEG from speech or vice versa. Recently, Artificial Neural Networks (ANNs) such as Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based architectures have outperformed linear models in modeling the relation between EEG and speech. Before attempting to use these models in real-world applications such as hearing tests or (second) language comprehension assessment we need to know what level of speech information is being utilized by these models. In this study, we aim to analyze the performance of an LSTM-based model using different levels of speech features. The task of the model is to determine which of two given speech segments is matched with the recorded EEG. We used low- and high-level speech features including: envelope, mel spectrogram, voice activity, phoneme identity, and word embedding. Our results suggest that the model exploits information about silences, intensity, and broad phonetic classes from the EEG. Furthermore, the mel spectrogram, which contains all this information, yields the highest accuracy (84%) among all the features",
    "checked": true,
    "id": "e7b5f3cd4ccced32cc691a47344e66fca1c52f88",
    "semantic_title": "extracting different levels of speech information from eeg using an lstm-based model",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bosch21_interspeech.html": {
    "title": "Word Competition: An Entropy-Based Approach in the DIANA Model of Human Word Comprehension",
    "volume": "main",
    "abstract": "We discuss the role of entropy of the set of unfolding word candidates in the context of DIANA, a computational model of human auditory speech comprehension. DIANA consists of three major interacting components: Activation, Decision and Execution. The Activation component computes activations of word candidates that change over time as a function of the unfolding audio input. The resulting set of word candidate activations can be associated with an entropy that is related to difficulty of the decision when one of these candidates must be selected at time T. The paper presents the close relation between entropy measures and the between-word competition during the unfolding of the auditory stimuli, and at the end of the stimuli if no decision could be made before stimulus offset. We present a way for computing the entropy that takes into account linguistic-phonetic constraints that play a role in speech comprehension and in lexical decision experiments. Using the BALDEY data set and linear mixed effects regression models for RT, we show that entropy measures explain differences between RTs of words with different morphological structure",
    "checked": true,
    "id": "912d256af6fbb78cd79b5f323aa69ceb9d5234ae",
    "semantic_title": "word competition: an entropy-based approach in the diana model of human word comprehension",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bosch21b_interspeech.html": {
    "title": "Time-to-Event Models for Analyzing Reaction Time Sequences",
    "volume": "main",
    "abstract": "We investigate reaction time (RT) sequences obtained from lexical decision experiments by applying Time-to-Event modelling (Survival Analysis). This is a branch of statistics for analyzing the expected duration until one or more events happen, associated with a set of potential â€˜causes' (in our case the decision for a â€˜word' judgment as a function of conventional predictors such as lexical frequency, stimulus duration, reduction, etc.). In this analysis, RTs are considered a by-product of an (unobservable) cumulative incidence function that results in a decision when it exceeds a certain threshold We show that Survival Analysis can be effectively used to narrow the gap between data-oriented models and process-oriented models for RT data from lexical decision experiments. Results of this analysis technique are presented for two different RT data sets. The analysis reveals time-varying patterns of predictors that reflect the differences in cognitive processes during the presentation of auditory stimuli",
    "checked": true,
    "id": "5f6773faef5ee141af529d58862c7de1efe8e574",
    "semantic_title": "time-to-event models for analyzing reaction time sequences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/brand21_interspeech.html": {
    "title": "Models of Reaction Times in Auditory Lexical Decision: RTonset versus RToffset",
    "volume": "main",
    "abstract": "We investigate how the role of predictors in models of reaction times in auditory lexical decision experiments depends on the operational definition of RT: whether the time is measured from stimulus onset or from stimulus offset. In a large body of literature, RTs are measured from the onset of the stimulus to the start of the response (often a button press or an oral response). The rationale behind this choice is that information about the stimulus becomes available to the listener starting at onset. Alternatively, the RT from offset is less dependent on stimulus duration and is assumed to focus on those cognitive processes that play a role late(r) in the word and after word offset, when all information is available The paper presents RT-onset and RT-offset-based linear mixed effects models for three different lexical decision-based data sets and explains the significant differences between these models, showing to what extent both definitions of reaction time reveal different roles for predictors and how early and later contributions to the overall RT can be differentiated",
    "checked": true,
    "id": "f5fbf5e3d6fa5e4f556c5b0853fc09b72f6716d4",
    "semantic_title": "models of reaction times in auditory lexical decision: rtonset versus rtoffset",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21c_interspeech.html": {
    "title": "SpecMix : A Mixed Sample Data Augmentation Method for Training with Time-Frequency Domain Features",
    "volume": "main",
    "abstract": "A mixed sample data augmentation strategy is proposed to enhance the performance of models on audio scene classification, sound event classification, and speech enhancement tasks. While there have been several augmentation methods shown to be effective in improving image classification performance, their efficacy toward time-frequency domain features of audio is not assured. We propose a novel audio data augmentation approach named \"Specmix\" specifically designed for dealing with time-frequency domain features. The augmentation method consists of mixing two different data samples by applying time-frequency masks effective in preserving the spectral correlation of each audio sample. Our experiments on acoustic scene classification, sound event classification, and speech enhancement tasks show that the proposed Specmix improves the performance of various neural network architectures by a maximum of 2.7%",
    "checked": false,
    "id": "c514b8b084f5e210228f44c95dd5a9ac99d914b0",
    "semantic_title": "specmix : a mixed sample data augmentation method for training withtime-frequency domain features",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21d_interspeech.html": {
    "title": "SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "In this paper, we present SpecAugment++, a novel data augmentation method for deep neural networks based acoustic scene classification (ASC). Different from other popular data augmentation methods such as SpecAugment and mixup that only work on the input space, SpecAugment++ is applied to both the input space and the hidden space of the deep neural networks to enhance the input and the intermediate feature representations. For an intermediate hidden state, the augmentation techniques consist of masking blocks of frequency channels and masking blocks of time frames, which improve generalization by enabling a model to attend not only to the most discriminative parts of the feature, but also the entire parts. Apart from using zeros for masking, we also examine two approaches for masking based on the use of other samples within the mini-batch, which helps introduce noises to the networks to make them more discriminative for classification. The experimental results on the DCASE 2018 Task1 dataset and DCASE 2019 Task1 dataset show that our proposed method can obtain 3.6% and 4.7% accuracy gains over a strong baseline without augmentation (i.e. CP-ResNet) respectively, and outperforms other previous data augmentation methods",
    "checked": true,
    "id": "254a094e5269e76cb92f7169c160fd9c67a057bd",
    "semantic_title": "specaugment++: a hidden space data augmentation method for acoustic scene classification",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zheng21_interspeech.html": {
    "title": "An Effective Mutual Mean Teaching Based Domain Adaptation Method for Sound Event Detection",
    "volume": "main",
    "abstract": "In this paper, we present a novel mutual mean teaching based domain adaptation (MMT-DA) method for sound event detection (SED) task, which can effectively exploit synthetic data to improve the SED performance. Existing methods simply treat the synthetic data as strongly-labeled data in semi-supervised learning (SSL) framework. Benefiting from the strong labels of synthetic data, superior SED performance can be achieved. However, a distribution mismatch between synthetic and real data raises an evident challenge for domain adaptation (DA). In MMT-DA, convolutional recurrent neural networks (CRNN) learned from different datasets (i.e :real+synthetic, and ) are exploited for DA. Specifically, mean teacher method using CRNN is employed for utilizing the unlabeled real data. To compensate the domain diversity, an additional domain classifier with gradient reverse layer(GRL) is used for training a mean teacher for The student CRNNs are mutually taught using the soft predictions of unlabeled data obtained from different teachers. Furthermore, a strip pooling based attention module is exploited to model the inter-dependencies between channels and time-frequency dimensions to exploit the structure information. Experimental results on Task4 of DCASE2020 demonstrate the ability of the proposed method, achieving 52.0% F1-score on the validation dataset, which outperforms the winning system's 50.6%",
    "checked": true,
    "id": "831d0bf67a37506dec36c5cfddd8bd62a2a519b5",
    "semantic_title": "an effective mutual mean teaching based domain adaptation method for sound event detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nandi21_interspeech.html": {
    "title": "Acoustic Scene Classification Using Kervolution-Based SubSpectralNet",
    "volume": "main",
    "abstract": "In this paper, a Kervolution-based SubSpectralNet model is proposed for Acoustic Scene Classification (ASC). SubSpectralNet is a competitive model which divides the mel spectrogram into horizontal slices termed as sub-spectrograms that are considered as input to the Convolutional Neural Network (CNN). In this work, the linear convolutional operation of SubSpectralNet is replaced with a non-linear operation using the kernel trick. This is also known as kervolution (kernel convolution)-based SubSpectralNet. The performance of the proposed methodology is evaluated on the DCASE (Detection and Classification of Acoustic Scenes and Events) 2018 development dataset. The proposed method achieves 73.52% and 75.76% accuracy with Polynomial and Gaussian Kernels respectively",
    "checked": true,
    "id": "7a5da2afd67169d3df47d4bff55b41878e1b7443",
    "semantic_title": "acoustic scene classification using kervolution-based subspectralnet",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sundar21_interspeech.html": {
    "title": "Event Specific Attention for Polyphonic Sound Event Detection",
    "volume": "main",
    "abstract": "The concept of multi-headed self attention (MHSA) introduced as a critical building block of a Transformer Encoder/Decoder Module has made a significant impact in the areas of natural language processing (NLP), automatic speech recognition (ASR) and recently in the area of sound event detection (SED). The current state-of-the-art approaches to SED employ a shared attention mechanism achieved through a stack of MHSA blocks to detect multiple sound events. Consequently, in a multi-label SED task, a common attention mechanism would be responsible for generating relevant feature representations for each of the events to be detected. In this paper, we show through empirical evaluation that having more MHSA blocks dedicated specifically for individual events, rather than having a stack of shared MHSA blocks, improves the overall detection performance. Interestingly, this improvement in performance comes about because the event-specific attention blocks help in resolving confusions in the case of co-occurring events. The proposed \"Event-specific Attention Network\" (ESA-Net) can be trained in an end-to-end manner. On the DCASE 2020 Task 4 data set, we show that with ESA-Net, the best single model achieves an event-based F1 score of 52.1% on the public validation data set improving over the existing state of the art result",
    "checked": true,
    "id": "6fc0c8b8beaebf9b76769f64eff495d0ef8bf88e",
    "semantic_title": "event specific attention for polyphonic sound event detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21b_interspeech.html": {
    "title": "AST: Audio Spectrogram Transformer",
    "volume": "main",
    "abstract": "In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2",
    "checked": true,
    "id": "0e2d8b8d81092037f9866c1ceddcebb87318e38b",
    "semantic_title": "ast: audio spectrogram transformer",
    "citation_count": 300
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seo21_interspeech.html": {
    "title": "Shallow Convolution-Augmented Transformer with Differentiable Neural Computer for Low-Complexity Classification of Variable-Length Acoustic Scene",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNNs) exhibit good performance in low-complexity classification with fixed-length acoustic scenes. However, previous studies have not considered variable-length acoustic scenes in which performance degradation is prevalent. In this regard, we investigate two novel architectures â€” convolution-augmented transformer (Conformer) and differentiable neural computer (DNC). Both the models show desirable performance for variable-length data but require a large amount of data. In other words, small amounts of data, such as those from acoustic scenes, lead to overfitting in these models. In this paper, we propose a shallow convolution-augmented Transformer with a differentiable neural computer (shallow Conformer-DNC) for the low-complexity classification of variable-length acoustic scenes. The shallow Conformer-DNC is enabled to converge with small amounts of data. Short-term and long-term contexts of variable-length acoustic scenes are trained by using the shallow Conformer and shallow DNC, respectively. The experiments were conducted for variable-length conditions using the TAU Urban Acoustic Scenes 2020 Mobile dataset. As a result, a peak accuracy of 61.25% was confirmed for shallow Conformer-DNC with a model parameter of 34 K. It is comparable performance to state-of-the-art CNNs",
    "checked": true,
    "id": "100bb70815cd72ab469d885d1eae43a0e0b1de29",
    "semantic_title": "shallow convolution-augmented transformer with differentiable neural computer for low-complexity classification of variable-length acoustic scene",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bear21_interspeech.html": {
    "title": "An Evaluation of Data Augmentation Methods for Sound Scene Geotagging",
    "volume": "main",
    "abstract": "Sound scene geotagging is a new topic of research which has evolved from acoustic scene classification. It is motivated by the idea of audio surveillance. Not content with only describing a scene in a recording, a machine which can locate where the recording was captured would be of use to many. In this paper we explore a series of common audio data augmentation methods to evaluate which best improves the accuracy of audio geotagging classifiers Our work improves on the state-of-the-art city geotagging method by 23% in terms of classification accuracy",
    "checked": true,
    "id": "cb767ecb7f2c1b5b33b7b09a82a3ccd9d8aa2749",
    "semantic_title": "an evaluation of data augmentation methods for sound scene geotagging",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hori21_interspeech.html": {
    "title": "Optimizing Latency for Online Video Captioning Using Audio-Visual Transformers",
    "volume": "main",
    "abstract": "Video captioning is an essential technology to understand scenes and describe events in natural language. To apply it to real-time monitoring, a system needs not only to describe events accurately but also to produce the captions as soon as possible. Low-latency captioning is needed to realize such functionality, but this research area for online video captioning has not been pursued yet. This paper proposes a novel approach to optimize each caption's output timing based on a trade-off between latency and caption quality. An audio-visual Transformer is trained to generate ground-truth captions using only a small portion of all video frames, and to mimic outputs of a pre-trained Transformer to which all the frames are given. A CNN-based timing detector is also trained to detect a proper output timing, where the captions generated by the two Transformers become sufficiently close to each other. With the jointly trained Transformer and timing detector, a caption can be generated in the early stages of an event-triggered video clip, as soon as an event happens or when it can be forecasted. Experiments with the ActivityNet Captions dataset show that our approach achieves 94% of the caption quality of the upper bound given by the pre-trained Transformer using the entire video clips, using only 28% of frames from the beginning",
    "checked": false,
    "id": "343da36b7e02087342e5e29d538fef6cc24d97c1",
    "semantic_title": "optimizing latency for online video captioningusing audio-visual transformers",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/si21_interspeech.html": {
    "title": "Variational Information Bottleneck for Effective Low-Resource Audio Classification",
    "volume": "main",
    "abstract": "Large-scale deep neural networks (DNNs) such as convolutional neural networks (CNNs) have achieved impressive performance in audio classification for their powerful capacity and strong generalization ability. However, when training a DNN model on low-resource tasks, it is usually prone to overfitting the small data and learning too much redundant information. To address this issue, we propose to use variational information bottleneck (VIB) to mitigate overfitting and suppress irrelevant information. In this work, we conduct experiments on a 4-layer CNN. However, the VIB framework is ready-to-use and could be easily utilized with many other state-of-the-art network architectures. Evaluation on a few audio datasets shows that our approach significantly outperforms baseline methods, yielding â‰¥ 5.0% improvement in terms of classification accuracy in some low-source settings",
    "checked": true,
    "id": "a384336e61895d1b05e136ab0dcef9addf8f7548",
    "semantic_title": "variational information bottleneck for effective low-resource audio classification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deshmukh21_interspeech.html": {
    "title": "Improving Weakly Supervised Sound Event Detection with Self-Supervised Auxiliary Tasks",
    "volume": "main",
    "abstract": "While multitask and transfer learning has shown to improve the performance of neural networks in limited data settings, they require pretraining of the model on large datasets beforehand. In this paper, we focus on improving the performance of weakly supervised sound event detection in low data and noisy settings simultaneously without requiring any pretraining task. To that extent, we propose a shared encoder architecture with sound event detection as a primary task and an additional secondary decoder for a self-supervised auxiliary task. We empirically evaluate the proposed framework for weakly supervised sound event detection on a remix dataset of the DCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20 dB SNR. To ensure we retain the localisation information of multiple sound events, we propose a two-step attention pooling mechanism that provides a time-frequency localisation of multiple audio events in the clip. The proposed framework with two-step attention outperforms existing benchmark models by 22.3%, 12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an ablation study to determine the contribution of the auxiliary task and two-step attention pooling to the SED performance improvement",
    "checked": true,
    "id": "ba3a4ce1e04276ddde8ba7cdb05192c24cae10ea",
    "semantic_title": "improving weakly supervised sound event detection with self-supervised auxiliary tasks",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/komatsu21_interspeech.html": {
    "title": "Acoustic Event Detection with Classifier Chains",
    "volume": "main",
    "abstract": "This paper proposes acoustic event detection (AED) with classifier chains, a new classifier based on the probabilistic chain rule. The proposed AED with classifier chains consists of a gated recurrent unit and performs iterative binary detection of each event one by one. In each iteration, the event's activity is estimated and used to condition the next output based on the probabilistic chain rule to form classifier chains. Therefore, the proposed method can handle the interdependence among events upon classification, while the conventional AED methods with multiple binary classifiers with a linear layer and sigmoid function have placed an assumption of conditional independence. In the experiments with a real-recording dataset, the proposed method demonstrates its superior AED performance to a relative 14.80% improvement compared to a convolutional recurrent neural network baseline system with the multiple binary classifiers",
    "checked": true,
    "id": "3f5b7fcb6fc50ba80318ab959f3d63253cd0ef6b",
    "semantic_title": "acoustic event detection with classifier chains",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tseng21_interspeech.html": {
    "title": "Segment and Tone Production in Continuous Speech of Hearing and Hearing-Impaired Children",
    "volume": "main",
    "abstract": "Verbal communication in daily use is conducted in the form of continuous speech that theoretically is the ideal data format for assessing oral language ability in educational and clinical domains. But as phonetic reduction and particularly lexical tones in Chinese are greatly affected by discourse context, it is a challenging task for automatic systems to evaluate continuous speech only by acoustic features. This study analyzed repetitive and storytelling speech produced by selected Chinese-speaking hearing and hearing-impaired children with distinctively high and low speech intelligibility levels. Word-based reduction types are derived by phonological properties that characterize contraction degrees of automatically generated surface forms of disyllabic words. F0-based tonal contours are visualized using the centroid-nearest data points in the major clusters computed for tonal syllables. Our results show that primary speech characteristics across different groups of children can be differentiated by means of reduction type and tone production",
    "checked": true,
    "id": "3a79c74a5f7b527b94cb93c8401fd3c37433d90d",
    "semantic_title": "segment and tone production in continuous speech of hearing and hearing-impaired children",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21e_interspeech.html": {
    "title": "Effect of Carrier Bandwidth on Understanding Mandarin Sentences in Simulated Electric-Acoustic Hearing",
    "volume": "main",
    "abstract": "For patients suffering with high-frequency hearing loss and preserving low-frequency hearing, combined electric-acoustic stimulation (EAS) may significantly improve their speech perception compared with cochlear implants (CIs). In combined EAS, a hearing aid provides low-frequency information via acoustic (A) stimulation and a CI evokes high-frequency sound sensation via electrical (E) stimulation. The present work investigated the EAS advantage when only a small number (i.e., 1 or 2) of channels were provided for electrical stimulation in a CI, and the effect of carrier bandwidth on understanding Mandarin sentences in a simulation of combined EAS experiment. The A-portion was extracted via low-pass filtering processing and the E-portion was generated with a vocoder model preserving multi-channel temporal envelope waveforms, whereas a noise-vocoder and a tone-vocoder were used to simulate the effect of carrier bandwidth. The synthesized stimuli were presented to normal-hearing listeners to recognize. Experimental results showed that while low-pass filtered Mandarin speech was not very intelligible, adding one or two E channels could significantly improve the intelligibility score to above 86.0%. Under the condition with one E channel, using a large carrier bandwidth in noise-vocoder processing provided a better intelligibility performance than using a narrow carrier bandwidth in tone-vocoder processing",
    "checked": true,
    "id": "0bd7227e80a1eb8b7b8148ba44c2447b0f83f366",
    "semantic_title": "effect of carrier bandwidth on understanding mandarin sentences in simulated electric-acoustic hearing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sharma21_interspeech.html": {
    "title": "A Comparative Study of Different EMG Features for Acoustics-to-EMG Mapping",
    "volume": "main",
    "abstract": "Electromyography (EMG) signals have been extensively used to capture facial muscle movements while speaking since they are one of the most closely related bio-signals generated during speech production. In this work, we focus on speech acoustics to EMG prediction. We present a comparative study of ten different EMG signal-based features including Time Domain (TD) features existing in the literature to examine their effectiveness in speech acoustics to EMG inverse (AEI) mapping. We propose a novel feature based on the Hilbert envelope of the filtered EMG signal. The raw EMG signal is reconstructed from these features as well. For the AEI mapping, we use a bi-directional long short-term memory (BLSTM) network in a session-dependent manner. To estimate the raw EMG signal from the EMG features, we use a CNN-BLSTM model comprising of a convolution neural network (CNN) followed by BLSTM layers. AEI mapping performance using the BLSTM network reveals that the Hilbert envelope based feature is predicted from speech with the highest accuracy, among all the features. Therefore, it could be the most representative feature of the underlying muscle activation during speech production. The proposed Hilbert envelope feature, when used together with the existing TD features, improves the raw EMG signal reconstruction performance compared to using the TD features alone",
    "checked": true,
    "id": "cd51e410d8fa732adf89f03186bada63022273a4",
    "semantic_title": "a comparative study of different emg features for acoustics-to-emg mapping",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/abraham21_interspeech.html": {
    "title": "Image-Based Assessment of Jaw Parameters and Jaw Kinematics for Articulatory Simulation: Preliminary Results",
    "volume": "main",
    "abstract": "Correcting the deficits in jaw movements have often been ignored in assessment and treatment of speech disorders. A robotic simulation is being developed to facilitate Speech Language Pathologists to demonstrate the movement of jaw, tongue and teeth during production of speech sounds, as a part of a larger study. Profiling of jaw movement is an important aspect of articulatory simulation. The present study attempts to develop a simple and efficient technique for deriving the jaw parameters and using them to simulate jaw movements through inverse kinematics Three Kannada speaking male participants in the age range of 26 to 33 years were instructed to produce selected speech sounds. The image of the final position of the jaw during production of each speech sound was recorded through CT scan and video camera. Angle of ramus and angle of body of mandible were simulated through inverse kinematics using RoboAnalyzer software. The variables for inverse kinematics were derived through kinematic analysis. The Denavit-Hartenberg (D-H) parameters required for kinematic analysis were obtained from still image. Angles simulated were compared with the angles obtained from CT scan images. No significant difference was observed",
    "checked": true,
    "id": "efd6b477a05a0a2df158a67868150486351cf4ba",
    "semantic_title": "image-based assessment of jaw parameters and jaw kinematics for articulatory simulation: preliminary results",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21f_interspeech.html": {
    "title": "An Attention Self-Supervised Contrastive Learning Based Three-Stage Model for Hand Shape Feature Representation in Cued Speech",
    "volume": "main",
    "abstract": "Cued Speech (CS) is a communication system for deaf people or hearing impaired people, in which a speaker uses it to aid a lipreader in phonetic level by clarifying potentially ambiguous mouth movements with hand shape and positions. Feature extraction of multi-modal CS is a key step in CS recognition. Recent supervised deep learning based methods suffer from noisy CS data annotations especially for hand shape modality. In this work, we first propose a self-supervised contrastive learning method to learn the feature representation of image without using labels. Secondly, a small amount of manually annotated CS data are used to fine-tune the first module. Thirdly, we present a module, which combines Bi-LSTM and self-attention networks to further learn sequential features with temporal and contextual information. Besides, to enlarge the volume and the diversity of the current limited CS datasets, we build a new British English dataset containing 5 native CS speakers. Evaluation results on both French and British English datasets show that our model achieves over 90% accuracy in hand shape recognition. Significant improvements of 8.75% (for French) and 10.09% (for British English) are achieved in CS phoneme recognition correctness compared with the state-of-the-art",
    "checked": true,
    "id": "c0aa75df231ecbe77364c0eed1a9a4a97ce01626",
    "semantic_title": "an attention self-supervised contrastive learning based three-stage model for hand shape feature representation in cued speech",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dineley21_interspeech.html": {
    "title": "Remote Smartphone-Based Speech Collection: Acceptance and Barriers in Individuals with Major Depressive Disorder",
    "volume": "main",
    "abstract": "The ease of in-the-wild speech recording using smartphones has sparked considerable interest in the combined application of speech, remote measurement technology (RMT) and advanced analytics as a research and healthcare tool. For this to be realised, the acceptability of remote speech collection to the user must be established, in addition to feasibility from an analytical perspective. To understand the acceptance, facilitators, and barriers of smartphone-based speech recording, we invited 384 individuals with major depressive disorder (MDD) from the Remote Assessment of Disease and Relapse â€” Central Nervous System (RADAR-CNS) research programme in Spain and the UK to complete a survey on their experiences recording their speech. In this analysis, we demonstrate that study participants were more comfortable completing a scripted speech task than a free speech task. For both speech tasks, we found depression severity and country to be significant predictors of comfort. Not seeing smartphone notifications of the scheduled speech tasks, low mood and forgetfulness were the most commonly reported obstacles to providing speech recordings",
    "checked": true,
    "id": "2c9cec59469b8aaefa9ab7e1e4b03d4310d71447",
    "semantic_title": "remote smartphone-based speech collection: acceptance and barriers in individuals with major depressive disorder",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21_interspeech.html": {
    "title": "An Automatic, Simple Ultrasound Biofeedback Parameter for Distinguishing Accurate and Misarticulated Rhotic Syllables",
    "volume": "main",
    "abstract": "Characterizing accurate vs. misarticulated patterns of tongue movement using ultrasound can be challenging in real time because of the fast, independent movement of tongue regions. The usefulness of ultrasound for biofeedback speech therapy is limited because speakers must mentally track and compare differences between their tongue movement and available models. It is desirable to automate this interpretive task using a single parameter representing deviation from known accurate tongue movements. In this study, displacements recorded automatically by ultrasound image tracking were transformed into a single biofeedback parameter (time-dependent difference between blade and dorsum displacements). Receiver operating characteristic (ROC) curve analysis was used to evaluate this parameter as a predictor of production accuracy over a range of different vowel contexts with initial and final /r/ in American English. Areas under ROC curves were 0.8 or above, indicating that this simple parameter may provide useful real-time biofeedback on /r/ accuracy within a range of rhotic contexts",
    "checked": true,
    "id": "35e23d8a22399dfbfaeee78e92d918d89f666812",
    "semantic_title": "an automatic, simple ultrasound biofeedback parameter for distinguishing accurate and misarticulated rhotic syllables",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ribeiro21_interspeech.html": {
    "title": "Silent versus Modal Multi-Speaker Speech Recognition from Ultrasound and Video",
    "volume": "main",
    "abstract": "We investigate multi-speaker speech recognition from ultrasound images of the tongue and video images of the lips. We train our systems on imaging data from modal speech, and evaluate on matched test sets of two speaking modes: silent and modal speech. We observe that silent speech recognition from imaging data underperforms compared to modal speech recognition, likely due to a speaking-mode mismatch between training and testing. We improve silent speech recognition performance using techniques that address the domain mismatch, such as fMLLR and unsupervised model adaptation. We also analyse the properties of silent and modal speech in terms of utterance duration and the size of the articulatory space. To estimate the articulatory space, we compute the convex hull of tongue splines, extracted from ultrasound tongue images. Overall, we observe that the duration of silent speech is longer than that of modal speech, and that silent speech covers a smaller articulatory space than modal speech. Although these two properties are statistically significant across speaking modes, they do not directly correlate with word error rates from speech recognition",
    "checked": true,
    "id": "6db077cf8ad1ebd68b5cd37b6dc533c5d89c3a78",
    "semantic_title": "silent versus modal multi-speaker speech recognition from ultrasound and video",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ferreira21_interspeech.html": {
    "title": "RaSSpeR: Radar-Based Silent Speech Recognition",
    "volume": "main",
    "abstract": "Speech is our most natural and efficient way of communication and offers a strong potential to improve how we interact with machines. However, speech communication can sometimes be limited by environmental (e.g., ambient noise), contextual (e.g., need for privacy in a public place), or health conditions (e.g., laryngectomy), hindering the consideration of audible speech. In this regard, silent speech interfaces (SSI) have been proposed (e.g., considering video, electromyography), however, many technologies still face limitations regarding their everyday use, e.g., the need to place equipment in contact with the speaker (e.g., electrodes/ultrasound probe), and raise technical (e.g., lighting conditions for video) or privacy concerns. In this context, the consideration of technologies that can help tackle these issues, e.g, by being contactless and/or placed in the environment, can foster the widespread use of SSI. In this article, continuous-wave radar is explored to assess its potential for SSI. To this end, a corpus of 13 words was acquired, for 3 speakers, and different classifiers were tested on the resulting data. The best results, obtained using Bagging classifier, trained for each speaker, with 5-fold cross-validation, yielded an average accuracy of 0.826, an encouraging result that establishes promising grounds for further exploration of this technology for silent speech recognition",
    "checked": true,
    "id": "519ba8a37856e929f87d9baf817235000dcc9ed2",
    "semantic_title": "rassper: radar-based silent speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cao21_interspeech.html": {
    "title": "Investigating Speech Reconstruction for Laryngectomees for Silent Speech Interfaces",
    "volume": "main",
    "abstract": "Silent speech interfaces (SSIs) are devices that convert non-audio bio-signals to speech, which hold the potential of recovering quality speech for laryngectomees (people who have undergone laryngectomy). Although significant progress has been made, most of the recent SSI works focused on data collected from healthy speakers. SSIs for laryngectomees have rarely been investigated. In this study, we investigated the reconstruction of speech for two laryngectomees who either use tracheoesophageal puncture (TEP) or electro-larynx (EL) speech as their post-surgery communication mode. We reconstructed their speech using two SSI designs (1) real-time recognition-and-synthesis and (2) directly articulation-to-speech synthesis (ATS). The reconstructed speech samples were measured in subjective evaluation by 20 listeners in terms of naturalness and intelligibility. The results indicated that both designs increased the naturalness of alaryngeal speech. The real-time recognition-and-synthesis design obtained higher intelligibility in electrolarynx speech as well, while the ATS did not. These preliminary results suggest the real-time recognition-and-synthesis design may have a better potential for clinical applications (for laryngectomees) than ATS",
    "checked": true,
    "id": "62825b598df8e6a6cfbace2d89e086c7d1534e78",
    "semantic_title": "investigating speech reconstruction for laryngectomees for silent speech interfaces",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schroter21_interspeech.html": {
    "title": "LACOPE: Latency-Constrained Pitch Estimation for Speech Enhancement",
    "volume": "main",
    "abstract": "Fundamental frequency (f ) estimation, also known as pitch tracking, has been a long-standing research topic in the speech and signal processing community. Many pitch estimation algorithms, however, fail in noisy conditions or introduce large delays due to their frame size or Viterbi decoding In this study, we propose a deep learning-based pitch estimation algorithm, LACOPE, which was trained in a joint pitch estimation and speech enhancement framework. In contrast to previous work, this algorithm allows for a configurable latency down to an algorithmic delay of 0. This is achieved by exploiting the smoothness properties of the pitch trajectory. That is, a recurrent neural network compensates delay introduced by the feature computation by predicting the pitch for a desired point, allowing a trade-off between pitch accuracy and latency We integrate the pitch estimation in a speech enhancement framework for hearing aids. For this application, we allow a delay on the analysis side of approx. 5ms. The pitch estimate is then used for constructing a comb filter in frequency domain as post-processing step to remove intra-harmonic noise Our pitch estimation performance is on par with SOTA algorithms like PYIN or CREPE for spoken speech in all noise conditions while introducing minimal latency",
    "checked": true,
    "id": "c72d19ac039f20dd7ce57922af4ab1d080c5b695",
    "semantic_title": "lacope: latency-constrained pitch estimation for speech enhancement",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fontaine21_interspeech.html": {
    "title": "Alpha-Stable Autoregressive Fast Multichannel Nonnegative Matrix Factorization for Joint Speech Enhancement and Dereverberation",
    "volume": "main",
    "abstract": "This paper proposes Î±-stable autoregressive fast multichannel nonnegative matrix factorization (Î±-AR-FastMNMF), a robust joint blind speech enhancement and dereverberation method for improved automatic speech recognition in a realistic adverse environment. The state-of-the-art versatile blind source separation method called FastMNMF that assumes the short-time Fourier transform (STFT) coefficients of a direct sound to follow a circular complex Gaussian distribution with jointly-diagonalizable full-rank spatial covariance matrices was extended to AR-FastMNMF with an autoregressive reverberation model. Instead of the light-tailed Gaussian distribution, we use the heavy-tailed Î±-stable distribution, which also has the reproductive property useful for the additive source modeling, to better deal with the large dynamic range of the direct sound. The experimental results demonstrate that the proposed Î±-AR-FastMNMF works well as a front-end of an automatic speech recognition system. It outperforms Î±-AR-ILRMA, which is a special case of Î±-AR-FastMNMF, and their Gaussian counterparts, i.e., AR-FastMNMF and AR-ILRMA, in terms of the speech signal quality metrics and word error rate",
    "checked": true,
    "id": "44ad09ff5a972f6e556c36d54b124805f337badd",
    "semantic_title": "alpha-stable autoregressive fast multichannel nonnegative matrix factorization for joint speech enhancement and dereverberation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21e_interspeech.html": {
    "title": "Microphone Array Generalization for Multichannel Narrowband Deep Speech Enhancement",
    "volume": "main",
    "abstract": "This paper addresses the problem of microphone array generalization for deep-learning-based end-to-end multichannel speech enhancement. We aim to train a unique deep neural network (DNN) potentially performing well on unseen microphone arrays. The microphone array geometry shapes the network's parameters when training on a fixed microphone array, and thus restricts the generalization of the trained network to another microphone array. To resolve this problem, a single network is trained using data recorded by various microphone arrays of different geometries. We design three variants of our recently proposed narrowband network to cope with the agnostic number of microphones. Overall, the goal is to make the network learn the universal information for speech enhancement that is available for any array geometry, rather than learn the one-array-dedicated characteristics. The experiments on both simulated and real room impulse responses (RIR) demonstrate the excellent across-array generalization capability of the proposed networks, in the sense that their performance measures are very close to, or even exceed the network trained with test arrays. Moreover, they notably outperform various beamforming methods and other advanced deep-learning-based methods",
    "checked": true,
    "id": "99da88791b9ee4ec1855d6017e27d9d19e63b731",
    "semantic_title": "microphone array generalization for multichannel narrowband deep speech enhancement",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/song21_interspeech.html": {
    "title": "Multiple Sound Source Localization Based on Interchannel Phase Differences in All Frequencies with Spectral Masks",
    "volume": "main",
    "abstract": "One of the most widely used cues for sound source localization is the interchannel phase differences (IPDs) in the frequency domain. However, the spatial aliasing makes the utilization of the IPDs in the high frequencies difficult, especially when the distance between the microphones is high. Recently, the phase replication method which considers the direction-of-arrival (DoA) candidates corresponding to all the possible unwrapped phase differences in all frequency bins was proposed. However, high frequency bins with possible spatial aliasing contribute more when constructing initial DoA histograms compared with low frequency bins, which may not be desirable for source localization. In this paper, we propose to utilize the IPDs in all the frequency bins with equal weights regardless of maximum number of phase wrapping in that frequency for dual microphone sound source localization. We applied spectral masks based on local signal-to-noise ratios and coherences between microphone signals to exclude time-frequency bins without directional audio signal from the DoA histogram construction. Experimental results show that the proposed method results in more distinct peaks in the DoA histogram and outperforms the conventional method in various noisy and reverberant environments",
    "checked": true,
    "id": "5de77bee7b188d1090495c1d3ee4c9df5321764c",
    "semantic_title": "multiple sound source localization based on interchannel phase differences in all frequencies with spectral masks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zarazaga21_interspeech.html": {
    "title": "Cancellation of Local Competing Speaker with Near-Field Localization for Distributed ad-hoc Sensor Network",
    "volume": "main",
    "abstract": "In scenarios such as remote work, open offices and call centers, multiple people may simultaneously have independent spoken interactions with their devices in the same room. The speech of competing speakers will however be picked up by all microphones, both reducing the quality of audio and exposing speakers to breaches in privacy. We propose a cooperative cross-talk cancellation solution breaking the single active speaker assumption employed by most telecommunication systems. The proposed method applies source separation on the microphone signals of independent devices, to extract the dominant speaker in each device. It is realized using a localization estimator based on a deep neural network, followed by a time-frequency mask to separate the target speech from the interfering one at each time-frequency unit referring to its orientation. By experimental evaluation, we confirm that the proposed method effectively reduces crosstalk and exceeds the baseline expectation maximization method by 10 dB in terms of interference rejection. This performance makes the proposed method a viable solution for cross-talk cancellation in near-field conditions, thus protecting the privacy of external speakers in the same acoustic space",
    "checked": true,
    "id": "9a06d45c23c7501f3e95c34c3e2365706e9c7d09",
    "semantic_title": "cancellation of local competing speaker with near-field localization for distributed ad-hoc sensor network",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21f_interspeech.html": {
    "title": "A Deep Learning Method to Multi-Channel Active Noise Control",
    "volume": "main",
    "abstract": "This paper addresses multi-channel active noise control (MCANC) on the basis of deep ANC, which performs active noise control by employing deep learning to encode the optimal control parameters corresponding to different noises and environments. The proposed method trains a convolutional recurrent network (CRN) to estimate the real and imaginary spectrograms of all the canceling signals simultaneously from the reference signals so that the corresponding anti-noises cancel or attenuate the primary noises in an MCANC system. We evaluate the proposed method under multiple MCANC setups and investigate the impact of the number of canceling loudspeakers and error microphones on the overall performance. Experimental results show that deep ANC is effective for MCANC in various scenarios. Moreover, the proposed method is robust against untrained noises and works well in the presence of loudspeaker nonlinearity",
    "checked": true,
    "id": "4f849a816bbf37faa265b2e79ce5f5a7a9039ffa",
    "semantic_title": "a deep learning method to multi-channel active noise control",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/graetzer21_interspeech.html": {
    "title": "Clarity-2021 Challenges: Machine Learning Challenges for Advancing Hearing Aid Processing",
    "volume": "main",
    "abstract": "In recent years, rapid advances in speech technology have been made possible by machine learning challenges such as CHiME, REVERB, Blizzard, and Hurricane. In the Clarity project, the machine learning approach is applied to the problem of hearing aid processing of speech-in-noise, where current technology in enhancing the speech signal for the hearing aid wearer is often ineffective. The scenario is a (simulated) cuboid-shaped living room in which there is a single listener, a single target speaker and a single interferer, which is either a competing talker or domestic noise. All sources are static, the target is always within Â±30Â° azimuth of the listener and at the same elevation, and the interferer is an omnidirectional point source at the same elevation. The target speech comes from an open source 40-speaker British English speech database collected for this purpose. This paper provides a baseline description of the round one Clarity challenges for both enhancement (CEC1) and prediction (CPC1). To the authors' knowledge, these are the first machine learning challenges to consider the problem of hearing aid speech signal processing",
    "checked": true,
    "id": "dcd58c352561634afc3573a4ddcb05e993b736fd",
    "semantic_title": "clarity-2021 challenges: machine learning challenges for advancing hearing aid processing",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tu21b_interspeech.html": {
    "title": "Optimising Hearing Aid Fittings for Speech in Noise with a Differentiable Hearing Loss Model",
    "volume": "main",
    "abstract": "Current hearing aids normally provide amplification based on a general prescriptive fitting, and the benefits provided by the hearing aids vary among different listening environments despite the inclusion of noise suppression feature. Motivated by this fact, this paper proposes a data-driven machine learning technique to develop hearing aid fittings that are customised to speech in different noisy environments. A differentiable hearing loss model is proposed and used to optimise fittings with back-propagation. The customisation is reflected on the data of speech in different noise with also the consideration of noise suppression. The objective evaluation shows the advantages of optimised custom fittings over general prescriptive fittings",
    "checked": true,
    "id": "24fdd8f9fcd5c050b9c4620c5212519538691ad2",
    "semantic_title": "optimising hearing aid fittings for speech in noise with a differentiable hearing loss model",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sivasankaran21_interspeech.html": {
    "title": "Explaining Deep Learning Models for Speech Enhancement",
    "volume": "main",
    "abstract": "We consider the problem of explaining the robustness of neural networks used to compute time-frequency masks for speech enhancement to mismatched noise conditions. We employ the Deep SHapley Additive exPlanations (DeepSHAP) feature attribution method to quantify the contribution of every time-frequency bin in the input noisy speech signal to every time-frequency bin in the output time-frequency mask. We define an objective metric â€” referred to as the speech relevance score â€” that summarizes the obtained SHAP values and show that it correlates with the enhancement performance, as measured by the word error rate on the CHiME-4 real evaluation dataset. We use the speech relevance score to explain the generalization ability of three speech enhancement models trained using synthetically generated speech-shaped noise, noise from a professional sound effects library, or real CHiME-4 noise. To the best of our knowledge, this is the first study on neural network explainability in the context of speech enhancement",
    "checked": true,
    "id": "f5c070b5ef48030379a4abcfa0de6f369db5effd",
    "semantic_title": "explaining deep learning models for speech enhancement",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21_interspeech.html": {
    "title": "Minimum-Norm Differential Beamforming for Linear Array with Directional Microphones",
    "volume": "main",
    "abstract": "Among different differential beamforming approaches, the minimum-norm one has received much attention as it maximizes the white noise gain(WNG). WNG measures the robustness of beamformer. But in practice, the conventional minimum-norm differential beamforming with omnidirectional elements still suffers in low white-noise-gain at the low frequencies. The major contributions of this paper are as follows: First, we extend the existing work by presenting a new solution with the use of the directional microphone elements, and show clearly the connection between the conventional beamforming and the proposed beamforming. Second, through the derivation as well as simulations, we show the proposed solution brings noticeable improvement in WNG at the low frequencies when the null positions of the directional elements coincide with the null-constraints of minimum norm solution",
    "checked": true,
    "id": "80eafc4a3d2d83fad181c5be66634dcee466cb32",
    "semantic_title": "minimum-norm differential beamforming for linear array with directional microphones",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cao21b_interspeech.html": {
    "title": "Improving Streaming Transformer Based ASR Under a Framework of Self-Supervised Learning",
    "volume": "main",
    "abstract": "Recently self-supervised learning has emerged as an effective approach to improve the performance of automatic speech recognition (ASR). Under such a framework, the neural network is usually pre-trained with massive unlabeled data and then fine-tuned with limited labeled data. However, the non-streaming architecture like bidirectional transformer is usually adopted by the neural network to achieve competitive results, which cannot be used in streaming scenarios. In this paper, we mainly focus on improving the performance of streaming transformer under the self-supervised learning framework. Specifically, we propose a novel two-stage training method during fine-tuning, which combines knowledge distilling and self-training. The proposed training method achieves 16.3% relative word error rate (WER) reduction on Librispeech noisy test set. Finally, by only using the 100h clean subset of Librispeech as the labeled data and the rest (860h) as the unlabeled data, our streaming transformer based model obtains competitive WERs 3.5/8.7 on Librispeech clean/noisy test sets",
    "checked": true,
    "id": "aa98a02b83ad8920a9909fae187ce0a96532a95a",
    "semantic_title": "improving streaming transformer based asr under a framework of self-supervised learning",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sadhu21_interspeech.html": {
    "title": "wav2vec-C: A Self-Supervised Model for Speech Representation Learning",
    "volume": "main",
    "abstract": "wav2vec-C introduces a novel representation learning technique combining elements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized representations from partially masked speech encoding using a contrastive loss in a way similar to wav2vec 2.0. However, the quantization process is regularized by an additional consistency network that learns to reconstruct the input features to the wav2vec 2.0 network from the quantized representations in a way similar to a VQ-VAE model. The proposed self-supervised model is trained on 10k hours of unlabeled data and subsequently used as the speech encoder in a RNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one of the very few studies of self-supervised learning on speech tasks with a large volume of real far-field labeled data. The wav2vec-C encoded representations achieve, on average, twice the error reduction over baseline and a higher codebook utilization in comparison to wav2vec 2.0",
    "checked": true,
    "id": "253c6c9e6b976d0b89052a21249ff23146a81a4b",
    "semantic_title": "wav2vec-c: a self-supervised model for speech representation learning",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wallington21_interspeech.html": {
    "title": "On the Learning Dynamics of Semi-Supervised Training for ASR",
    "volume": "main",
    "abstract": "The use of semi-supervised training (SST) has become an increasingly popular way of increasing the performance of ASR acoustic models without the need for further transcribed speech data. However, the performance of the technique can be very sensitive to the quality of the initial ASR system. This paper undertakes a comprehensive study of the improvements gained with respect to variation in the initial systems, the quantity of untranscribed data used, and the learning schedules. We postulate that the reason SST can be effective even when the initial model is poor is because it enables utterance-level information to be propagated to the frame level, and hence hypothesise that the quality of the language model plays a much larger role than the quality of the acoustic model. In experiments on Tagalog data from the IARPA MATERIAL programme, we find that indeed this is the case, and show that with an appropriately chosen recipe it is possible to achieve over 50% relative WER reductions from SST, even when the WER of the initial system is more than 80%",
    "checked": true,
    "id": "00b4536b83475504ca8e57e290235c2cc4d3fc90",
    "semantic_title": "on the learning dynamics of semi-supervised training for asr",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hsu21_interspeech.html": {
    "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
    "volume": "main",
    "abstract": "Self-supervised learning of speech representations has been a very active research area but most work is focused on a single domain such as read audio books for which there exist large quantities of labeled and unlabeled data. In this paper, we explore more general setups where the domain of the unlabeled data for pre-training data differs from the domain of the labeled data for fine-tuning, which in turn may differ from the test data domain. Our experiments show that using target domain data during pre-training leads to large performance improvements across a variety of setups. With no access to in-domain labeled data, pre-training on unlabeled in-domain data closes 66â€“73% of the performance gap between the ideal setting of in-domain labeled data and a competitive supervised out-of-domain model. This has obvious practical implications since it is much easier to obtain unlabeled target domain data than labeled data. Moreover, we find that pre-training on multiple domains improves generalization performance on domains not seen during training. We will release pre-trained models",
    "checked": true,
    "id": "5f769c5df8de29d0a2cd9c020f78047013a87b34",
    "semantic_title": "robust wav2vec 2.0: analyzing domain shift in self-supervised pre-training",
    "citation_count": 136
  },
  "https://www.isca-speech.org/archive/interspeech_2021/higuchi21_interspeech.html": {
    "title": "Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition",
    "volume": "main",
    "abstract": "Pseudo-labeling (PL) has been shown to be effective in semi-supervised automatic speech recognition (ASR), where a base model is self-trained with pseudo-labels generated from unlabeled data. While PL can be further improved by iteratively updating pseudo-labels as the model evolves, most of the previous approaches involve inefficient retraining of the model or intricate control of the label update. We present (MPL), a simple yet effective strategy for semi-supervised ASR. MPL consists of a pair of and models that interact and learn from each other, inspired by the mean teacher method. The online model is trained to predict pseudo-labels generated on the fly by the offline model. The offline model maintains a momentum-based moving average of the online model. MPL is performed in a single training process and the interaction between the two models effectively helps them reinforce each other to improve the ASR performance. We apply MPL to an end-to-end ASR model based on the connectionist temporal classification. The experimental results demonstrate that MPL effectively improves over the base model and is scalable to different semi-supervised scenarios with varying amounts of data or domain mismatch",
    "checked": true,
    "id": "60013b38457658c186197d3320209b8abd46531f",
    "semantic_title": "momentum pseudo-labeling for semi-supervised speech recognition",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2021/misra21_interspeech.html": {
    "title": "A Comparison of Supervised and Unsupervised Pre-Training of End-to-End Models",
    "volume": "main",
    "abstract": "In the absence of large-scale in-domain supervised training data, ASR models can achieve reasonable performance through pre-training on additional data that is unlabeled, mismatched or both. Given such data constraints, we compare pre-training end-to-end models on matched but unlabeled data (unsupervised) and on labeled but mismatched data (supervised), where the labeled data is mismatched in either domain or language. Across encoder architectures, pre-training methods and languages, our experiments indicate that both types of pre-training improve performance, with relative WER reductions of 15â€“30% in the domain mismatch case and up to 15% in the language mismatch condition. We further find that the advantage from unsupervised pre-training is most prominent when there is no matched and labeled fine-tuning data, provided that a sufficient amount of mismatched data is still available for supervised fine-tuning",
    "checked": true,
    "id": "9b8c447d4fcea02ca3372e3cdaa0285c6b7f0cdb",
    "semantic_title": "a comparison of supervised and unsupervised pre-training of end-to-end models",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21c_interspeech.html": {
    "title": "Semi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation",
    "volume": "main",
    "abstract": "Semi and self-supervised training techniques have the potential to improve performance of speech recognition systems without additional transcribed speech data. In this work, we demonstrate the efficacy of two approaches to semi-supervision for automated speech recognition. The two approaches leverage vast amounts of available unspoken text and untranscribed audio. First, we present to improve data augmentation on unspoken text. Next, we propose the algorithm with to learn from untranscribed speech. The algorithm is built on top of our online implementation of Noisy Student Training. We demonstrate the compatibility of these techniques yielding an overall relative reduction of word error rate of up to 14.4% on the voice search tasks on 4 Indic languages",
    "checked": true,
    "id": "a9f2e5b7933c46409456fd14b457eb3d603ffdc1",
    "semantic_title": "semi-supervision in asr: sequential mixmatch and factorized tts-based augmentation",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2021/likhomanenko21b_interspeech.html": {
    "title": "slimIPL: Language-Model-Free Iterative Pseudo-Labeling",
    "volume": "main",
    "abstract": "Recent results in end-to-end automatic speech recognition have demonstrated the efficacy of pseudo-labeling for semi-supervised models trained both with Connectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq) losses. Iterative Pseudo-Labeling (IPL), which continuously trains a single model using pseudo-labels iteratively re-generated as the model learns, has been shown to further improve performance in ASR. We improve upon the IPL algorithm: as the model learns, we propose to iteratively re-generate transcriptions with hard labels (the most probable tokens), that is, a language model. We call this approach Language-Model-Free IPL (slimIPL) and give a resultant training setup for low-resource settings with CTC-based models. slimIPL features a dynamic cache for pseudo-labels which reduces sensitivity to changes in relabeling hyperparameters and results in improved training stability. slimIPL is also highly-efficient and requires 3.5â€“4Ã— fewer computational resources to converge than other state-of-the-art semi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL is competitive with self-supervised approaches, and is state-of-the-art with 100 hours of labeled audio without the use of a language model both at test time and during pseudo-label generation",
    "checked": true,
    "id": "7f0c7c324675179f0e32c160d99c7066c7ab30ae",
    "semantic_title": "slimipl: language-model-free iterative pseudo-labeling",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yue21_interspeech.html": {
    "title": "Phonetically Motivated Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Self-supervised representation learning has seen remarkable success in encoding high-level semantic information from unlabelled speech data. The studies have been focused on exploring new pretext tasks to improve the learned speech representation and various masking schemes with reference to speech frames. We consider effective latent speech representation should be phonetically informed. In this work, we propose a novel phonetically motivated masking scheme. Specifically, we select the masked speech frames according to the phonetic segmentation in an utterance. The phonetically motivated self-supervised representation learns the speech representation that benefits downstream speech processing tasks. We evaluate the proposed learning algorithm on phoneme classification, speech recognition, and speaker recognition, and show that it consistently outperforms competitive baselines",
    "checked": true,
    "id": "076a3c66db625224f30f8cdf27fde0ea4116cc49",
    "semantic_title": "phonetically motivated self-supervised speech representation learning",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21_interspeech.html": {
    "title": "Improving RNN-T for Domain Scaling Using Semi-Supervised Training with Neural TTS",
    "volume": "main",
    "abstract": "Recurrent neural network transducer (RNN-T) has shown to be comparable with conventional hybrid model for speech recognition. However, there is still a challenge in out-of-domain scenarios with context or words different from training data. In this paper, we explore the semi-supervised training which optimizes RNN-T jointly with neural text-to-speech (TTS) to better generalize to new domains using domain-specific text data. We apply the method to two tasks: one with out-of-domain context and the other with significant out-of-vocabulary (OOV) words. The results show that the proposed method significantly improves the recognition accuracy in both tasks, resulting in 61.4% and 53.8% relative word error rate (WER) reductions respectively, from a well-trained RNN-T with 65 thousand hours of training data. We do further study on the semi-supervised training methodology: 1) which modules of RNN-T model to be updated; 2) the impact of using different neural TTS models; 3) the performance of using text with different relevancy to target domain. Finally, we compare several RNN-T customization methods, and conclude that semi-supervised training with neural TTS is comparable and complementary with Internal Language Model Estimation (ILME) or biasing",
    "checked": true,
    "id": "9e8b8d25c94aa67025a441d8fb838749d4c79dba",
    "semantic_title": "improving rnn-t for domain scaling using semi-supervised training with neural tts",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seyfarth21_interspeech.html": {
    "title": "Speaker-Conversation Factorial Designs for Diarization Error Analysis",
    "volume": "main",
    "abstract": "Speaker diarization accuracy can be affected by both acoustics and conversation characteristics. Determining the cause of diarization errors is difficult because speaker voice acoustics and conversation structure co-vary, and the interactions between acoustics, conversational structure, and diarization accuracy are complex. This paper proposes a methodology that can distinguish independent marginal effects of acoustic and conversation characteristics on diarization accuracy by remixing conversations in a factorial design. As an illustration, this approach is used to investigate gender-related and language-related accuracy differences with three diarization systems: a baseline system using subsegment x-vector clustering, a variant of it with shorter subsegments, and a third system based on a Bayesian hidden Markov model. Our analysis shows large accuracy disparities for the baseline system primarily due to conversational structure, which are partially mitigated in the other two systems. The illustration thus demonstrates how the methodology can be used to identify and guide diarization model improvements",
    "checked": true,
    "id": "19e485b3a2e95dd7445ec8daf316ddf00e5873a8",
    "semantic_title": "speaker-conversation factorial designs for diarization error analysis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mcgowan21_interspeech.html": {
    "title": "SmallER: Scaling Neural Entity Resolution for Edge Devices",
    "volume": "main",
    "abstract": "In this paper we introduce SmallER, a scalable neural entity resolution system capable of running directly on edge devices. SmallER addresses constraints imposed by the on-device setting such as bounded memory consumption for both model and catalog storage, limited compute resources, and related latency challenges introduced by those restrictions. Our model includes distinct modules to learn syntactic and semantic information and is trained to handle multiple domains within one compact architecture. We use compressed tries to reduce the space required to store catalogs and a novel implementation of spatial partitioning trees to strike a balance between reducing runtime latency and preserving recall relative to full catalog search. Our final model consumes only 3MB of memory at inference time with classification accuracy surpassing that of previously established, domain-specific baseline models on live customer utterances. For the largest catalogs we consider (300 or more entries), our proxy metric for runtime latency is reduced by more than 90%",
    "checked": true,
    "id": "03305747804b812345adce3ce2b6725d752013c7",
    "semantic_title": "smaller: scaling neural entity resolution for edge devices",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rocholl21_interspeech.html": {
    "title": "Disfluency Detection with Unlabeled Data and Small BERT Models",
    "volume": "main",
    "abstract": "Disfluency detection models now approach high accuracy on English text. However, little exploration has been done in improving the size and inference time of the model. At the same time, Automatic Speech Recognition (ASR) models are moving from server-side inference to local, on-device inference. Supporting models in the transcription pipeline (like disfluency detection) must follow suit. In this work we concentrate on the disfluency detection task, focusing on small, fast, on-device models based on the BERT architecture. We demonstrate it is possible to train disfluency detection models as small as 1.3 MiB, while retaining high performance. We build on previous work that showed the benefit of data augmentation approaches such as self-training. Then, we evaluate the effect of domain mismatch between conversational and written text on model performance. We find that domain adaptation and data augmentation strategies have a more pronounced effect on these smaller models, as compared to conventional BERT models",
    "checked": true,
    "id": "63ca82b7e846468e1aa9eacec3b1a26b7ceb94c0",
    "semantic_title": "disfluency detection with unlabeled data and small bert models",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21d_interspeech.html": {
    "title": "Discriminative Self-Training for Punctuation Prediction",
    "volume": "main",
    "abstract": "Punctuation prediction for automatic speech recognition (ASR) output transcripts plays a crucial role for improving the readability of the ASR transcripts and for improving the performance of downstream natural language processing applications. However, achieving good performance on punctuation prediction often requires large amounts of labeled speech transcripts, which is expensive and laborious. In this paper, we propose a Discriminative Self-Training approach with weighted loss and discriminative label smoothing to exploit unlabeled speech transcripts. Experimental results on the English IWSLT2011 benchmark test set and an internal Chinese spoken language dataset demonstrate that the proposed approach achieves significant improvement on punctuation prediction accuracy over strong baselines including BERT, RoBERTa, and ELECTRA models. The proposed Discriminative Self-Training approach outperforms the vanilla self-training approach. We establish a new state-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current SOTA model by 1.3% absolute gain on F ",
    "checked": true,
    "id": "178808258cb73831f5a39781842ce01419f04883",
    "semantic_title": "discriminative self-training for punctuation prediction",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ihori21_interspeech.html": {
    "title": "Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks Using Switching Tokens",
    "volume": "main",
    "abstract": "In this paper, we propose a novel spoken-text-style conversion method that can simultaneously execute multiple style conversion modules such as punctuation restoration and disfluency deletion without preparing matched datasets. In practice, transcriptions generated by automatic speech recognition systems are not highly readable because they often include many disfluencies and do not include punctuation marks. To improve their readability, multiple spoken-text-style conversion modules that individually model a single conversion task are cascaded because matched datasets that simultaneously handle multiple conversion tasks are often unavailable. However, the cascading is unstable against the order of tasks because of the chain of conversion errors. Besides, the computation cost of the cascading must be higher than the single conversion. To execute multiple conversion tasks simultaneously without preparing matched datasets, our key idea is to distinguish individual conversion tasks using the In our proposed zero-shot joint modeling, we switch the individual tasks using multiple switching tokens, enabling us to utilize a zero-shot learning approach to executing simultaneous conversions. Our experiments on joint modeling of disfluency deletion and punctuation restoration demonstrate the effectiveness of our method",
    "checked": true,
    "id": "733a84c6fae83d584c4e338d8496bc165671d6a7",
    "semantic_title": "zero-shot joint modeling of multiple spoken-text-style conversion tasks using switching tokens",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21_interspeech.html": {
    "title": "A Noise Robust Method for Word-Level Pronunciation Assessment",
    "volume": "main",
    "abstract": "The common approach for pronunciation evaluation is based on Goodness of pronunciation (GOP). It has been found that GOP may perform worse under noise conditions. Traditional methods compensate pronunciation features to improve the performance of pronunciation assessment in noise situations. This paper proposed a noise robust model for word-level pronunciation assessment based on a domain adversarial training (DAT) method. We treat the pronunciation assessment in the clean and noise situations as the source and target domains. The network is optimized by incorporating both the pronunciation assessment and noise domain discrimination. The domain labels are generated from unsupervised methods to adapt to various noise situations. We evaluate the model performance based on English words recorded by Chinese English learners and labeled by three experts. Experimental results show on average the proposed model outperforms the baseline by 3% in Pearson correlation coefficients (PCC) and 4% in accuracy under different noise conditions",
    "checked": true,
    "id": "73be6722a76b57f2855a65cbb5f796b18619c09b",
    "semantic_title": "a noise robust method for word-level pronunciation assessment",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wintrode21_interspeech.html": {
    "title": "Targeted Keyword Filtering for Accelerated Spoken Topic Identification",
    "volume": "main",
    "abstract": "We present a novel framework for spoken topic identification that simultaneously learns both topic-specific keywords and acoustic keyword filters from only document-level topic labels. At inference time, only audio segments likely to contain topic-salient keywords are fully decoded, reducing the system's overall computation cost. We show that this filtering allows for effective topic classification while decoding only 50% of ASR output word lattices, and achieves error rates within 1.2% and precision within 2.6% of an unfiltered baseline system",
    "checked": true,
    "id": "7b04775139acf18a4ad498077dddaaf49130f8cd",
    "semantic_title": "targeted keyword filtering for accelerated spoken topic identification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/palaskar21_interspeech.html": {
    "title": "Multimodal Speech Summarization Through Semantic Concept Learning",
    "volume": "main",
    "abstract": "We propose a cascaded multimodal abstractive speech summarization model that generates semantic concepts as an intermediate step towards summarization. We describe a method to leverage existing multimodal dataset annotations to curate groundtruth labels for such intermediate concept modeling. In addition to cascaded training, the concept labels also provide an interpretable intermediate output level that helps improve performance on the downstream summarization task. On the open-domain How2 data, we conduct utterance-level and video-level experiments for two granularities of concepts: Specific and Abstract. We compare various multimodal fusion models for concept generation based on the respective input modalities. We observe consistent improvements in concept modeling by using multimodal adaptation models over unimodal models. Using the cascaded multimodal speech summarization model, we see a significant improvement of 7.5 METEOR points and 5.1 ROUGE-L points compared to previous methods of speech summarization. Finally, we show the benefits of scalability of the proposed approaches on 2000 h of video data",
    "checked": true,
    "id": "297b515fa8e5a129d757dd48065e51656dae55ba",
    "semantic_title": "multimodal speech summarization through semantic concept learning",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21_interspeech.html": {
    "title": "Enhancing Semantic Understanding with Self-Supervised Methods for Abstractive Dialogue Summarization",
    "volume": "main",
    "abstract": "Contextualized word embeddings can lead to state-of-the-art performances in natural language understanding. Recently, a pre-trained deep contextualized text encoder such as BERT has shown its potential in improving natural language tasks including abstractive summarization. Existing approaches in dialogue summarization focus on incorporating a large language model into summarization task trained on large-scale corpora consisting of news articles rather than dialogues of multiple speakers. In this paper, we introduce self-supervised methods to compensate shortcomings to train a dialogue summarization model. Our principle is to detect incoherent information flows using pretext dialogue text to enhance BERT's ability to contextualize the dialogue text representations. We build and fine-tune an abstractive dialogue summarization model on a shared encoder-decoder architecture using the enhanced BERT. We empirically evaluate our abstractive dialogue summarizer with the SAMSum corpus, a recently introduced dataset with abstractive dialogue summaries. All of our methods have contributed improvements to abstractive summary measured in ROUGE scores. Through an extensive ablation study, we also present a sensitivity analysis to critical model hyperparameters, probabilities of switching utterances and masking interlocutors",
    "checked": true,
    "id": "e661fabf7e3408afc8bbe47b8ea096867494afd6",
    "semantic_title": "enhancing semantic understanding with self-supervised methods for abstractive dialogue summarization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wodarczak21_interspeech.html": {
    "title": "Speaker Transition Patterns in Three-Party Conversation: Evidence from English, Estonian and Swedish",
    "volume": "main",
    "abstract": "During conversation, speakers hold and relinquish the floor, resulting in turn yield and retention. We examine these phenomena in three-party conversations in English, Swedish, and Estonian. We define within- and between-speaker transitions in terms of shorter intervals of speech, silence and overlap bounded by stretches of one-party speech longer than 1 second by the same or different speakers. This method gives us insights into how turn change and retention proceed, revealing that the majority of speaker transitions are more complex and involve more intermediate activity than a single silence or overlap. We examine the composition of within and between transitions in terms of number of speakers involved, incidence and proportion of solo speech, silence and overlap. We derive the most common within- and between-speaker transitions in the three languages, finding evidence of striking commonalities in how the floor is managed. Our findings suggest that current models of turn-taking used in dialogue technology could be extended using these results to more accurately reflect the realities of human-human dialogue",
    "checked": true,
    "id": "849e1128dff8ac41315eef1962743ffc158c5bc3",
    "semantic_title": "speaker transition patterns in three-party conversation: evidence from english, estonian and swedish",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/broughton21_interspeech.html": {
    "title": "Investigating Deep Neural Structures and their Interpretability in the Domain of Voice Conversion",
    "volume": "main",
    "abstract": "Generative Adversarial Networks (GANs) are machine learning networks based around creating synthetic data. Voice Conversion (VC) is a subset of voice translation that involves translating the paralinguistic features of a source speaker to a target speaker while preserving the linguistic information. The aim of non-parallel conditional GANs for VC is to translate an acoustic speech feature sequence from one domain to another without the use of paired data. In the study reported here, we investigated the interpretability of state-of-the-art implementations of non-parallel GANs in the domain of VC. We show that the learned representations in the repeating layers of a particular GAN architecture remain close to their original random initialised parameters, demonstrating that it is the number of repeating layers that is more responsible for the quality of the output. We also analysed the learned representations of a model trained on one particular dataset when used during transfer learning on another dataset. This also showed high levels of similarity in the repeating layers. Together, these results provide new insight into how the learned representations of deep generative networks change during learning and the importance of the number of layers, which would help build better GAN-based speech conversion models",
    "checked": true,
    "id": "971818b7c6ceea6ee06ba73b7d44ee92e1afdc88",
    "semantic_title": "investigating deep neural structures and their interpretability in the domain of voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21b_interspeech.html": {
    "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training",
    "volume": "main",
    "abstract": "Emotional voice conversion (EVC) aims to change the emotional state of an utterance while preserving the linguistic content and speaker identity. In this paper, we propose a novel 2-stage training strategy for sequence-to-sequence emotional voice conversion with a limited amount of emotional speech data. We note that the proposed EVC framework leverages text-to-speech (TTS) as they share a common goal that is to generate high-quality expressive voice. In stage 1, we perform style initialization with a multi-speaker TTS corpus, to disentangle speaking style and linguistic content. In stage 2, we perform emotion training with a limited amount of emotional speech data, to learn how to disentangle emotional style and linguistic information from the speech. The proposed framework can perform both spectrum and prosody conversion and achieves significant improvement over the state-of-the-art baselines in both objective and subjective evaluation",
    "checked": true,
    "id": "21df371e20d80039a90fddd98f06eb545cf41ceb",
    "semantic_title": "limited data emotional voice conversion leveraging text-to-speech: two-stage sequence-to-sequence training",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ding21_interspeech.html": {
    "title": "Adversarial Voice Conversion Against Neural Spoofing Detectors",
    "volume": "main",
    "abstract": "The naturalness and similarity of voice conversion have been significantly improved in recent years with the development of deep-learning-based conversion models and neural vocoders. Accordingly, the task of detecting spoofing speech also attracts research attention. In the latest ASVspoof 2019 challenge, the best spoofing detection model can distinguish most artificial utterances from natural ones. Inspired by recent progress of adversarial example generation, this paper proposes an adversarial post-processing network (APN) which generates adversarial examples against a neural-network-based spoofing detector by white-box attack. The APN model post-processes the speech waveforms generated by a baseline voice conversion system. An adversarial loss derived from the spoofing detector together with two regularization losses are applied to optimize the parameters of APN. In our experiments, using the logical access (LA) dataset of ASVspoof 2019, results show that our proposed method can improve the adversarial ability of converted speech against the spoofing detectors based on light convolution neural networks (LCNNs) effectively without degrading its subjective quality",
    "checked": true,
    "id": "df1ee465fea51bec203a36f09774d558df37c00d",
    "semantic_title": "adversarial voice conversion against neural spoofing detectors",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/he21b_interspeech.html": {
    "title": "An Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation",
    "volume": "main",
    "abstract": "Emotional Voice Conversion (EVC) aims to convert the emotional style of a source speech signal to a target style while preserving its content and speaker identity information. Previous emotional conversion studies do not disentangle emotional information from emotion-independent information that should be preserved, thus transforming it all in a monolithic manner and generating audio of low quality, with linguistic distortions. To address this distortion problem, we propose a novel StarGAN framework along with a two-stage training process that separates emotional features from those independent of emotion by using an autoencoder with two encoders as the generator of the Generative Adversarial Network (GAN). The proposed model achieves favourable results in both the objective evaluation and the subjective evaluation in terms of distortion, which reveals that the proposed model can effectively reduce distortion. Furthermore, in data augmentation experiments for end-to-end speech emotion recognition, the proposed StarGAN model achieves an increase of 2% in Micro-F1 and 5% in Macro-F1 compared to the baseline StarGAN model, which indicates that the proposed model is more valuable for data augmentation",
    "checked": true,
    "id": "8afc1ea6ee94a212f5a8439a57dc6adf733f32da",
    "semantic_title": "an improved stargan for emotional voice conversion: enhancing voice quality and data augmentation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21e_interspeech.html": {
    "title": "TVQVC: Transformer Based Vector Quantized Variational Autoencoder with CTC Loss for Voice Conversion",
    "volume": "main",
    "abstract": "Techniques of voice conversion (VC) aim to modify the speaker identity and style of an utterance while preserving the linguistic content. Although there are lots of VC methods, the state of the art of VC is still cascading automatic speech recognition (ASR) and text-to-speech (TTS). This paper presents a new structure of vector-quantized autoencoder based on transformer with CTC loss for non-parallel VC, which inspired by cascading ASR and TTS VC method. Our proposed method combines CTC loss and vector quantization to get high-level linguistic information without speaker information. Objective and subjective evaluations on the mandarin datasets show that the converted speech of our proposed model is better than baselines on naturalness, rhythm and speaker similarity",
    "checked": true,
    "id": "32e4f594da0eb1d1ca72f01a52ab63dff7fd5189",
    "semantic_title": "tvqvc: transformer based vector quantized variational autoencoder with ctc loss for voice conversion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21g_interspeech.html": {
    "title": "Enriching Source Style Transfer in Recognition-Synthesis Based Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "Current voice conversion (VC) methods can successfully convert timbre of the audio. As modeling source audio's prosody effectively is a challenging task, there are still limitations of transferring source style to the converted speech. This study proposes a source style transfer method based on recognition-synthesis framework. Previously in speech generation task, prosody can be modeled explicitly with prosodic features or implicitly with a latent prosody extractor. In this paper, taking advantages of both, we model the prosody in a hybrid manner, which effectively combines explicit and implicit methods in a proposed prosody module. Specifically, prosodic features are used to explicit model prosody, while VAE and reference encoder are used to implicitly model prosody, which take Mel spectrum and bottleneck feature as input respectively. Furthermore, adversarial training is introduced to remove speaker-related information from the VAE outputs, avoiding leaking source speaker information while transferring style. Finally, we use a modified self-attention based encoder to extract sentential context from bottleneck features, which also implicitly aggregates the prosodic aspects of source speech from the layered representations. Experiments show that our approach is superior to the baseline and a competitive system in terms of style transfer; meanwhile, the speech quality and speaker similarity are well maintained",
    "checked": true,
    "id": "682482dc466a96637771f8d37da50ed3c82e487f",
    "semantic_title": "enriching source style transfer in recognition-synthesis based non-parallel voice conversion",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21b_interspeech.html": {
    "title": "S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations",
    "volume": "main",
    "abstract": "Any-to-any voice conversion (VC) aims to convert the timbre of utterances from and to any speakers seen or unseen during training. Various any-to-any VC approaches have been proposed like AutoVC, AdaINVC, and FragmentVC. AutoVC, and AdaINVC utilize source and target encoders to disentangle the content and speaker information of the features. FragmentVC utilizes two encoders to encode source and target information and adopts cross attention to align the source and target features with similar phonetic content. Moreover, pretrained features are adopted. AutoVC used d-vector to extract speaker information, and self-supervised learning (SSL) features like wav2vec 2.0 is used in FragmentVC to extract the phonetic content information. Different from previous works, we proposed S2VC that utilizes Self-Supervised features as both source and target features for the VC model. Supervised phoneme posteriorgram (PPG), which is believed to be speaker-independent and widely used in VC to extract content information, is chosen as a strong baseline for SSL features. The objective evaluation and subjective evaluation both show models taking SSL feature CPC as both source and target features outperforms that taking PPG as source feature, suggesting that SSL features have great potential in improving VC",
    "checked": true,
    "id": "6fcaafc1f8ae649899e978a7a9baf682755dc7de",
    "semantic_title": "s2vc: a framework for any-to-any voice conversion with self-supervised pretrained representations",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liberatore21_interspeech.html": {
    "title": "An Exemplar Selection Algorithm for Native-Nonnative Voice Conversion",
    "volume": "main",
    "abstract": "We present an algorithm for selecting exemplars for native-to-nonnative voice conversion (VC) using a Sparse, Anchor-Based Representation of speech (SABR). The algorithm uses phoneme labels and clustering to learn optimal exemplars when source and target speakers are affected by poor time alignment, as is common in in native-to-nonnative voice conversion. We evaluate the method on speech from the ARCTIC and L2-ARCTIC corpora and compare it to a baseline exemplar-based VC algorithm. The proposed algorithm significantly improves synthesis quality and more than doubles that of a baseline exemplar-based VC system while using two orders of magnitude fewer atoms. Additionally, the proposed algorithm significantly reduces the VC error and improves the synthesis quality as compared to unoptimized SABR models. We discuss the implications of both optimization algorithms for SABR and broader exemplar-based VC systems.Index terms should be included as shown below",
    "checked": true,
    "id": "99e695a12b396db6b75536d26304db2de52b40b4",
    "semantic_title": "an exemplar selection algorithm for native-nonnative voice conversion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21h_interspeech.html": {
    "title": "Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion",
    "volume": "main",
    "abstract": "Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC). Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors. State-of-the-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement. To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning. Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT. The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others. Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58",
    "checked": true,
    "id": "607c18c160aa66c13314b9da1e89639b79f67bca",
    "semantic_title": "adversarially learning disentangled speech representations for robust multi-factor voice conversion",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luong21_interspeech.html": {
    "title": "Many-to-Many Voice Conversion Based Feature Disentanglement Using Variational Autoencoder",
    "volume": "main",
    "abstract": "Voice conversion is a challenging task which transforms the voice characteristics of a source speaker to a target speaker without changing linguistic content. Recently, there have been many works on many-to-many Voice Conversion (VC) based on Variational Autoencoder (VAEs) achieving good results, however, these methods lack the ability to disentangle speaker identity and linguistic content to achieve good performance on unseen speaker's scenarios. In this paper, we propose a new method based on feature disentanglement to tackle many-to-many voice conversion. The method has the capability to disentangle speaker identity and linguistic content from utterances, it can convert from many source speakers to many target speakers with a single autoencoder network. Moreover, it naturally deals with the unseen target speaker's scenarios. We perform both objective and subjective evaluations to show the competitive performance of our proposed method compared with other state-of-the-art models in terms of naturalness and target speaker similarity",
    "checked": true,
    "id": "9cf89ded1cdd420f1d3acd09a843741e1367f6aa",
    "semantic_title": "many-to-many voice conversion based feature disentanglement using variational autoencoder",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chouchane21_interspeech.html": {
    "title": "Privacy-Preserving Voice Anti-Spoofing Using Secure Multi-Party Computation",
    "volume": "main",
    "abstract": "In recent years the automatic speaker verification (ASV) community has grappled with vulnerabilities to spoofing attacks whereby fraudsters masquerade as enrolled subjects to provoke illegitimate accepts. Countermeasures have hence been developed to protect ASV systems from such attacks. Given that recordings of speech contain potentially sensitive information, any system operating upon them, including spoofing countermeasures, must have provisions for privacy preservation. While privacy enhancing technologies such as Homomorphic Encryption or Secure Multi-Party Computation (MPC) are effective in preserving privacy, these tend to impact upon computational capacity and computational precision, while no available spoofing countermeasures preserve privacy. This paper reports the first solution based upon the combination of shallow neural networks with secure MPC. Experiments performed using the ASVspoof 2019 logical access database show that the proposed solution is not only computationally efficient, but that it also improves upon the performance of the ASVspoof baseline countermeasure, all while preserving privacy",
    "checked": true,
    "id": "b419209b1b34847b42ed80b52b7ce50ba3d08713",
    "semantic_title": "privacy-preserving voice anti-spoofing using secure multi-party computation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/aloufi21_interspeech.html": {
    "title": "Configurable Privacy-Preserving Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Voice assistive technologies have given rise to far-reaching privacy and security concerns. In this paper we investigate whether modular automatic speech recognition (ASR) can improve privacy in voice assistive systems by combining independently trained separation, recognition, and discretization modules to design configurable privacy-preserving ASR systems. We evaluate privacy concerns and the effects of applying various state-of-the-art techniques at each stage of the system, and report results using task-specific metrics (i.e., WER, ABX, and accuracy). We show that overlapping speech inputs to ASR systems present further privacy concerns, and how these may be mitigated using speech separation and optimization techniques. Our discretization module is shown to minimize paralinguistics privacy leakage from ASR acoustic models to levels commensurate with random guessing. We show that voice privacy can be , and argue this presents new opportunities for privacy-preserving applications incorporating ASR",
    "checked": true,
    "id": "6a9cc72a0b18ea375eebdc352f7244d33a8f762f",
    "semantic_title": "configurable privacy-preserving automatic speech recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/novotney21_interspeech.html": {
    "title": "Adjunct-Emeritus Distillation for Semi-Supervised Language Model Adaptation",
    "volume": "main",
    "abstract": "To improve customer privacy, commercial speech applications are reducing human transcription of customer data. This has a negative impact on language model training due to a smaller amount of in-domain transcripts. Prior work demonstrated that training on automated transcripts alone provides modest gains due to reinforcement of recognition errors. We consider a new condition, where a model trained on historical human transcripts, but not the transcripts themselves, are available to us. To overcome temporal drift in vocabulary and topics, we propose a novel extension of knowledge distillation, where two imperfect teachers jointly train a student model. We conduct experiments on an English voice assistant domain and simulate a one year gap in human transcription. Unlike fine-tuning, our approach is architecture agnostic and achieves a 14% relative reduction in perplexity over the baseline approach of freezing model development and improves over the baseline of knowledge distillation",
    "checked": true,
    "id": "aea05c410fd750d90f43aabc1fd92c3625fc20fd",
    "semantic_title": "adjunct-emeritus distillation for semi-supervised language model adaptation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ro21_interspeech.html": {
    "title": "Communication-Efficient Agnostic Federated Averaging",
    "volume": "main",
    "abstract": "In distributed learning settings such as federated learning, the training algorithm can be potentially biased towards different clients. [1] proposed a domain-agnostic learning algorithm, where the model is optimized for any target distribution formed by a mixture of the client distributions in order to overcome this bias. They further proposed an algorithm for the cross-silo federated learning setting, where the number of clients is small. We consider this problem in the cross-device setting, where the number of clients is much larger. We propose a communication-efficient distributed algorithm called Agnostic Federated Averaging (or AgnosticFedAvg) to minimize the domain-agnostic objective proposed in [1], which is amenable to other private mechanisms such as secure aggregation. We highlight two types of naturally occurring domains in federated learning and argue that AgnosticFedAvg performs well on both. To demonstrate the practical effectiveness of AgnosticFedAvg, we report positive results for large-scale language modeling tasks in both simulation and live experiments, where the latter involves training language models for Spanish virtual keyboard for millions of user devices",
    "checked": true,
    "id": "25c6e17a64548994befaca422748b6fe7478af0a",
    "semantic_title": "communication-efficient agnostic federated averaging",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/koppelmann21_interspeech.html": {
    "title": "Privacy-Preserving Feature Extraction for Cloud-Based Wake Word Verification",
    "volume": "main",
    "abstract": "Wake word detection and verification systems often involve a local, on-device wake word detector and a cloud-based verification node. In such systems, the audio representation sent to the cloud-based server may exhibit sensitive information that might be intercepted by an eavesdropper. To improve privacy of cloud-based wake word verification (WWV) systems, we propose to use a privacy-preserving feature representation that minimizes the automatic speech recognition (ASR) capability of a potential attacker. The proposed approach employs an adversarial training schedule that aims to minimize an attacker's word error rate (WER) while maintaining a high WWV performance. To this end, we apply an adaptive weighting factor in the combined loss function to control the balance between minimizing the WWV loss and maximizing the ASR loss. We show that the proposed training method significantly reduces possible privacy risks while maintaining a strong WWV performance",
    "checked": true,
    "id": "6b772b81e93deed6b35fbb2cffc778fb93a6b52f",
    "semantic_title": "privacy-preserving feature extraction for cloud-based wake word verification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21_interspeech.html": {
    "title": "PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification",
    "volume": "main",
    "abstract": "We propose using an adversarial autoencoder (AAE) to replace generative adversarial network (GAN) in private aggregation of teacher ensembles (PATE), a solution for ensuring differential privacy in speech applications. The AAE architecture allows us to obtain good synthetic speech leveraging upon a discriminative training of latent vectors. Such synthetic speech is used to build a privacy-preserving classifier when non-sensitive data is not sufficiently available in the public domain. This classifier follows the PATE scheme that uses an ensemble of noisy outputs to label the synthetic samples and guarantee Îµ-differential privacy (DP) on its derived classifiers. Our proposed framework thus consists of an AAE-based generator and a PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands Dataset Version II, the proposed PATE-AAE improves the average classification accuracy by +2.11% and +6.60%, respectively, when compared with alternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while maintaining a strong level of privacy target at Îµ=0.01 with a fixed Î´=10 ",
    "checked": true,
    "id": "2bbfd3671198bc23a96cb7f992e3faca62721ee6",
    "semantic_title": "pate-aae: incorporating adversarial autoencoder into private aggregation of teacher ensembles for spoken command classification",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21b_interspeech.html": {
    "title": "Continual Learning for Fake Audio Detection",
    "volume": "main",
    "abstract": "Fake audio attack becomes a major threat to the speaker verification system. Although current detection approaches have achieved promising results on dataset-specific scenarios, they encounter difficulties on unseen spoofing data. Fine-tuning and retraining from scratch have been applied to incorporate new data. However, fine-tuning leads to performance degradation on previous data. Retraining takes a lot of time and computation resources. Besides, previous data are unavailable due to privacy in some situations. To solve the above problems, this paper proposes detecting fake without forgetting, a continual-learning-based method, to make the model learn new spoofing attacks incrementally. A knowledge distillation loss is introduced to loss function to preserve the memory of original model. Supposing the distribution of genuine voice is consistent among different scenarios, an extra embedding similarity loss is used as another constraint to further do a positive sample alignment. Experiments are conducted on the ASVspoof2019 dataset. The results show that our proposed method outperforms fine-tuning by the relative reduction of average equal error rate up to 81.62%",
    "checked": true,
    "id": "8c632eb06fe6e70f20705429f612a6aeccb0a4fd",
    "semantic_title": "continual learning for fake audio detection",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shah21_interspeech.html": {
    "title": "Evaluating the Vulnerability of End-to-End Automatic Speech Recognition Models to Membership Inference Attacks",
    "volume": "main",
    "abstract": "Recent studies have shown that it may be possible to determine if a machine learning model was trained on a given data sample, using Membership Inference Attacks (MIA). In this paper we evaluate the vulnerability of state-of-the-art speech recognition models to MIA under black-box access. Using models trained with standard methods and public datasets, we demonstrate that without any knowledge of the target model's parameters or training data a MIA can successfully infer membership with precision and recall more than 60%. Furthermore, for utterances from about 39% of the speakers the precision is more than 75%, indicating that training data membership can be inferred more precisely for some speakers than others. While strong regularization reduces the overall accuracy of MIA to almost 50%, the attacker can still infer membership for utterances from 25% of the speakers with high precision. These results indicate that (1) speaker-level MIA success should be reported, along with overall accuracy, to provide a holistic view of the model's vulnerability and (2) conventional regularization is an inadequate defense against MIA.We believe that the insights gleaned from this study can direct future work towards more effective defenses",
    "checked": true,
    "id": "c91b8cb2129654ab91f9374e10c90756f64ee880",
    "semantic_title": "evaluating the vulnerability of end-to-end automatic speech recognition models to membership inference attacks",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fazel21_interspeech.html": {
    "title": "SynthASR: Unlocking Synthetic Data for Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have recently demonstrated superior performance over the traditional hybrid ASR models. Training an E2E ASR model requires a large amount of data which is not only expensive but may also raise dependency on production data. At the same time, synthetic speech generated by the state-of-the-art text-to-speech (TTS) engines has advanced to near-human naturalness. In this work, we propose to utilize synthetic speech for ASR training (SynthASR) in applications where data is sparse or hard to get for ASR model training. In addition, we apply continual learning with a novel multi-stage training strategy to address catastrophic forgetting, achieved by a mix of weighted multi-style training, data augmentation, encoder freezing, and parameter regularization. In our experiments conducted on in-house datasets for a new application of recognizing medication names, training ASR RNN-T models with synthetic audio via the proposed multi-stage training improved the recognition performance on new application by more than 65% relative, without degradation on existing general applications. Our observations show that SynthASR holds great promise in training the state-of-the-art large-scale E2E ASR models for new applications while reducing the costs and dependency on production data",
    "checked": true,
    "id": "2b7f54a8bbda05d07871f1662d5397160bde8de7",
    "semantic_title": "synthasr: unlocking synthetic data for speech recognition",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muguli21_interspeech.html": {
    "title": "DiCOVA Challenge: Dataset, Task, and Baseline System for COVID-19 Diagnosis Using Acoustics",
    "volume": "main",
    "abstract": "The DiCOVA challenge aims at accelerating research in diagnosing COVID-19 using acoustics (DiCOVA), a topic at the intersection of speech and audio processing, respiratory health diagnosis, and machine learning. This challenge is an open call for researchers to analyze a dataset of sound recordings, collected from COVID-19 infected and non-COVID-19 individuals, for a two-class classification. These recordings were collected via crowdsourcing from multiple countries, through a website application. The challenge features two tracks, one focusing on cough sounds, and the other on using a collection of breath, sustained vowel phonation, and number counting speech recordings. In this paper, we introduce the challenge and provide a detailed description of the task, and present a baseline system for the task",
    "checked": true,
    "id": "02093d2179a4eef9fa5bf0d59498f60e1a6c78b6",
    "semantic_title": "dicova challenge: dataset, task, and baseline system for covid-19 diagnosis using acoustics",
    "citation_count": 71
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kamble21_interspeech.html": {
    "title": "PANACEA Cough Sound-Based Diagnosis of COVID-19 for the DiCOVA 2021 Challenge",
    "volume": "main",
    "abstract": "The COVID-19 pandemic has led to the saturation of public health services worldwide. In this scenario, the early diagnosis of SARS-Cov-2 infections can help to stop or slow the spread of the virus and to manage the demand upon health services. This is especially important when resources are also being stretched by heightened demand linked to other seasonal diseases, such as the flu. In this context, the organisers of the DiCOVA 2021 challenge have collected a database with the aim of diagnosing COVID-19 through the use of coughing audio samples. This work presents the details of the automatic system for COVID-19 detection from cough recordings presented by team PANACEA. This team consists of researchers from two European academic institutions and one company: EURECOM (France), University of Granada (Spain), and Biometric Vox S.L. (Spain). We developed several systems based on established signal processing and machine learning methods. Our best system employs a Teager energy operator cepstral coefficients (TECCs) based front-end and Light gradient boosting machine (LightGBM) back-end. The AUC obtained by this system on the test set is 76.31% which corresponds to a 10% improvement over the official baseline",
    "checked": true,
    "id": "d095e8160ad109057369c515d5e8ffc4c607454d",
    "semantic_title": "panacea cough sound-based diagnosis of covid-19 for the dicova 2021 challenge",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karas21_interspeech.html": {
    "title": "Recognising Covid-19 from Coughing Using Ensembles of SVMs and LSTMs with Handcrafted and Deep Audio Features",
    "volume": "main",
    "abstract": "As the Covid-19 pandemic continues, digital health solutions can provide valuable insights and assist in diagnosis and prevention. Since the disease affects the respiratory system, it is hypothesised that sound formation is changed, and thus, an infection can be automatically recognised through audio analysis. We present an ensemble learning approach used in our entry to Track 1 of the DiCOVA 2021 Challenge, which aims at binary classification of Covid-19 infection on a crowd-sourced dataset of 1 040 cough sounds. Our system is based on a combination of handcrafted features for paralinguistics with deep feature extraction from spectrograms using pre-trained CNNs. We extract features both at segment level and with a sliding window approach, and process them with SVMs and LSTMs, respectively. We then perform least-squares weighted late fusion of our classifiers. Our system surpasses the challenge baseline, with a ROC-AUC on the test set of 78.18%",
    "checked": true,
    "id": "5cf0da43f1a2e92c58185235e8f3d4e319db31c0",
    "semantic_title": "recognising covid-19 from coughing using ensembles of svms and lstms with handcrafted and deep audio features",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sodergren21_interspeech.html": {
    "title": "Detecting COVID-19 from Audio Recording of Coughs Using Random Forests and Support Vector Machines",
    "volume": "main",
    "abstract": "The detection of COVID-19 is and will remain in the foreseeable future a crucial challenge, making the development of tools for the task important. One possible approach, on the confines of speech and audio processing, is detecting potential COVID-19 cases based on cough sounds. We propose a simple, yet robust method based on the well-known ComParE 2016 feature set, and two classical machine learning models, namely Random Forests, and Support Vector Machines (SVMs). Furthermore, we combine the two methods, by calculating the weighted average of their predictions. Our results in the DiCOVA challenge show that this simple approach leads to a robust solution while producing competitive results. Based on the Area Under the Receiver Operating Characteristic Curve (AUC ROC) score, both classical machine learning methods we applied markedly outperform the baseline provided by the challenge organisers. Moreover, their combination attains an AUC ROC score of 85.21, positioning us at fourth place on the leaderboard (where the second team attained a similar, 85.43 score). Here, we would describe this system in more detail, and analyse the resulting models, drawing conclusions, and determining future work directions",
    "checked": true,
    "id": "24a23e0509771e58bfcd5cb5a6d0953a1e3da5dc",
    "semantic_title": "detecting covid-19 from audio recording of coughs using random forests and support vector machines",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/das21_interspeech.html": {
    "title": "Diagnosis of COVID-19 Using Auditory Acoustic Cues",
    "volume": "main",
    "abstract": "COVID-19 can be pre-screened based on symptoms and confirmed using other laboratory tests. The cough or speech from patients are also studied in the recent time for detection of COVID-19 as they are indicators of change in anatomy and physiology of the respiratory system. Along this direction, the diagnosis of COVID-19 using acoustics (DiCOVA) challenge aims to promote such research by releasing publicly available cough/speech corpus. We participated in the Track-1 of the challenge, which deals with COVID-19 detection using cough sounds from individuals. In this challenge, we use a few novel auditory acoustic cues based on long-term transform, equivalent rectangular bandwidth spectrum and gammatone filterbank. We evaluate these representations using logistic regression, random forest and multilayer perceptron classifiers for detection of COVID-19. On the blind test set, we obtain an area under the ROC curve (AUC) of 83.49% for the best system submitted to the challenge. It is worth noting that the submitted system ranked among the top few systems on the leaderboard and outperformed the challenge baseline by a large margin",
    "checked": true,
    "id": "8140fb53adf948abdf0c00a59220a35e9c3f31db",
    "semantic_title": "diagnosis of covid-19 using auditory acoustic cues",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/harvill21_interspeech.html": {
    "title": "Classification of COVID-19 from Cough Using Autoregressive Predictive Coding Pretraining and Spectral Data Augmentation",
    "volume": "main",
    "abstract": "Serum and saliva-based testing methods have been crucial to slowing the COVID-19 pandemic, yet have been limited by slow throughput and cost. A system able to determine COVID-19 status from cough sounds alone would provide a low cost, rapid, and remote alternative to current testing methods. We explore the applicability of recent techniques such as pre-training and spectral augmentation in improving the performance of a neural cough classification system. We use Autoregressive Predictive Coding (APC) to pre-train a unidirectional LSTM on the COUGHVID dataset. We then generate our final model by fine-tuning added BLSTM layers on the DiCOVA challenge dataset. We perform various ablation studies to see how each component impacts performance and improves generalization with a small dataset. Our final system achieves an AUC of 85.35 and places third out of 29 entries in the DiCOVA challenge",
    "checked": true,
    "id": "536615589a2196967330642c8fc8ad860b069d10",
    "semantic_title": "classification of covid-19 from cough using autoregressive predictive coding pretraining and spectral data augmentation",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deshpande21_interspeech.html": {
    "title": "The DiCOVA 2021 Challenge â€” An Encoder-Decoder Approach for COVID-19 Recognition from Coughing Audio",
    "volume": "main",
    "abstract": "This paper presents the automatic recognition of COVID-19 from coughing. In particular, it describes our contribution to the DiCOVA challenge â€” Track 1, which addresses such cough sound analysis for COVID-19 detection. Pathologically, the effects of a COVID-19 infection on the respiratory system and on breathing patterns are known. We demonstrate the use of breathing patterns of the cough audio signal in identifying the COVID-19 status. Breathing patterns of the cough audio signal are derived using a model trained with the subset of the UCL Speech Breath Monitoring (UCL-SBM) database. This database provides speech recordings of the participants while their breathing values are captured by a respiratory belt. We use an encoder-decoder architecture. The encoder encodes the audio signal into breathing patterns and the decoder decodes the COVID-19 status for the corresponding breathing patterns using an attention mechanism. The encoder uses a pre-trained model which predicts breathing patterns from the speech signal, and transfers the learned patterns to cough audio signals With this architecture, we achieve an AUC of 64.42% on the evaluation set of Track 1",
    "checked": false,
    "id": "dd7de0d1fd182ec5f3bdee0705a116816f07bf9d",
    "semantic_title": "the dicova 2021 challenge - an encoder-decoder approach for covid-19 recognition from coughing audio",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ritwik21_interspeech.html": {
    "title": "COVID-19 Detection from Spectral Features on the DiCOVA Dataset",
    "volume": "main",
    "abstract": "In this paper we investigate the cues of COVID-19 on sustained phonation of Vowel-/i/, deep breathing and number counting data of the DiCOVA dataset. We use an ensemble of classifiers trained on different features, namely, super-vectors, formants, harmonics and MFCC features. We fit a two-class Weighted SVM classifier to separate the COVID-19 audio from Non-COVID-19 audio. Weighted penalties help mitigate the challenge of class imbalance in the dataset. The results are reported on the stationary (breathing, Vowel-/i/) and non-stationary (counting data) data using individual and combination of features on each type of utterance. We find that the Formant information plays a crucial role in classification. The proposed system resulted in an AUC score of 0.734 for cross validation, and 0.717 for evaluation dataset",
    "checked": true,
    "id": "021b2b919eec8add2e2a9c7a168177cc88e6ea32",
    "semantic_title": "covid-19 detection from spectral features on the dicova dataset",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mallolragolta21_interspeech.html": {
    "title": "Cough-Based COVID-19 Detection with Contextual Attention Convolutional Neural Networks and Gender Information",
    "volume": "main",
    "abstract": "The aim of this contribution is to automatically detect COVID-19 patients by analysing the acoustic information embedded in coughs. COVID-19 affects the respiratory system, and, consequently, respiratory-related signals have the potential to contain salient information for the task at hand. We focus on analysing the spectrogram representations of cough samples with the aim to investigate whether COVID-19 alters the frequency content of these signals. Furthermore, this work also assesses the impact of gender in the automatic detection of COVID-19. To extract deep-learnt representations of the spectrograms, we compare the performance of a cough-specific, and a Resnet18 pre-trained Convolutional Neural Network (CNN). Additionally, our approach explores the use of contextual attention, so the model can learn to highlight the most relevant deep-learnt features extracted by the CNN. We conduct our experiments on the dataset released for the Cough Sound Track of the DICOVA 2021 Challenge. The best performance on the test set is obtained using the Resnet18 pre-trained CNN with contextual attention, which scored an Area Under the Curve (AUC) of 70.91% at 80% sensitivity",
    "checked": true,
    "id": "8857a1de6e8a3ba32c0a321958ad21eaff1aa622",
    "semantic_title": "cough-based covid-19 detection with contextual attention convolutional neural networks and gender information",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bhosale21_interspeech.html": {
    "title": "Contrastive Learning of Cough Descriptors for Automatic COVID-19 Preliminary Diagnosis",
    "volume": "main",
    "abstract": "Cough sounds as a descriptor have been used for detecting various respiratory ailments based on its intensity, duration of intermediate phase between two cough sounds, repetitions, dryness etc. However, COVID-19 diagnosis using only cough sounds is challenging because of cough being a common symptom among many non COVID-19 health diseases and inherent data imbalance within the available datasets. As one of the approach in this direction, we explore the robustness of multi-domain representation by performing the early fusion over a wide set of temporal, spectral and tempo-spectral handcrafted features, followed by training a Support Vector Machine (SVM) classifier. In our second approach, using a contrastive loss function we learn a latent space from Mel Filter Cepstral Coefficients (MFCCs) where representations belonging to samples having similar cough characteristics are closer. This helps learn representations for the highly varied COVID-negative class (healthy and symptomatic COVID-negative), by learning multiple smaller clusters. Using only the DiCOVA data, multi-domain features yields an absolute improvement of 0.74% and 1.07%, whereas our second approach shows an improvement of 2.09% and 3.98%, over the blind test and validation set, respectively, when compared with challenge baseline",
    "checked": true,
    "id": "c6fc8a0b000404d62961c121301bfda0fe105ad4",
    "semantic_title": "contrastive learning of cough descriptors for automatic covid-19 preliminary diagnosis",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/avila21_interspeech.html": {
    "title": "Investigating Feature Selection and Explainability for COVID-19 Diagnostics from Cough Sounds",
    "volume": "main",
    "abstract": "In this paper, we propose an approach to automatically classify COVID-19 and non-COVID-19 cough samples based on the combination of both feature engineering and deep learning models. In the feature engineering approach, we develop a support vector machine classifier over high dimensional (6373D) space of acoustic features. In the deep learning-based approach, on the other hand, we apply a convolutional neural network trained on the log-mel spectrograms. These two methodologically diverse models are then combined by fusing the probability scores of the models. The proposed system, which ranked 9 on the 2021 Diagnosing COVID-19 using Acoustics (DiCOVA) challenge leaderboard, obtained an area under the receiver operating characteristic curve (AUC) of 0.81 on the blind test data set, which is a 10.9% absolute improvement compared to the baseline. Moreover, we analyze the explainability of the deep learning-based model when detecting COVID-19 from cough signals",
    "checked": true,
    "id": "db0e656171e98c44b24b124c67f0d7df623cae29",
    "semantic_title": "investigating feature selection and explainability for covid-19 diagnostics from cough sounds",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kiss21_interspeech.html": {
    "title": "Application for Detecting Depression, Parkinson's Disease and Dysphonic Speech",
    "volume": "main",
    "abstract": "In this Show&Tell presentation we demonstrate an application that is able to assess a voice sample according to three different voice disorders: depression, Parkinson's disease and dysphonic speech. Affection probability of each disorder is analyzed along with their severity estimation. Although the acoustic models (support vector machine and regression models) are trained on Hungarian voice samples, English samples can also be utilized for assessment. The results are displayed by as pie chart for probabilities and separate severity scores. The input of the application is a read text with a fixed linguistic content. It is possible to load a pre-recorded voice sample or create a live recording. The developed system could evaluate a speaker's voice sample, assisting medical staff",
    "checked": true,
    "id": "a8d8176fc5f695df8103998bd533482d2458dfc2",
    "semantic_title": "application for detecting depression, parkinson's disease and dysphonic speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/weingartova21_interspeech.html": {
    "title": "Beey: More Than a Speech-to-Text Editor",
    "volume": "main",
    "abstract": "We present Beey, a newly developed web-based multimedia platform for producing Automatic Speech Recognition (ASR) and editing its output. In addition to ASR, Beey employs modules for speaker diarization and identification, text formatting, automatic punctuation insertion, subtitling, automatic translation, transcription of stream and more The platform and its development are focused on user experience and fast document creation. Our aim is to transfer research results in the field of speech recognition and signal processing into practice and enable Beey's users to make their production processes faster and cheaper by minimizing human effort and costs",
    "checked": true,
    "id": "35592cfd5da2a2dd721302b37c3916bccf36dc52",
    "semantic_title": "beey: more than a speech-to-text editor",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/arai21_interspeech.html": {
    "title": "Downsizing of Vocal-Tract Models to Line up Variations and Reduce Manufacturing Costs",
    "volume": "main",
    "abstract": "Demonstrating vowel production with physical models of the human vocal tract is a part of intuitive education in speech science. The adult male vocal tract was most often used as a model in the past because of the limited availability of physical models, but discussions on different vocal tract sizes were ongoing. Therefore, we focused on downsizing the vocal-tract models in this study, especially the straight models. We reduced the cross-sectional area function for the sliding three-tube model (including the total length) to female adult and child sizes. Furthermore, we created fixed straight models of similar dimensions for the five Japanese vowels. We found that the intelligibility of each model was preserved as long as the ratios of the cross-sectional areas were maintained even if the cross-sections were less than the average human sizes. This indicates that we can reduce the cost of manufacturing the models, as cost is typically a barrier when the models are used for pedagogical purposes",
    "checked": true,
    "id": "e85d8a3618832fd00d3512fea2b41f0a514eab8d",
    "semantic_title": "downsizing of vocal-tract models to line up variations and reduce manufacturing costs",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fabien21_interspeech.html": {
    "title": "ROXANNE Research Platform: Automate Criminal Investigations",
    "volume": "main",
    "abstract": "Criminal investigations require manual intervention of several investigators and translators. However, the amount and the diversity of the data collected raises many challenges, and cross-border investigations against organized crime can quickly impossible to handle. We developed ROXANNE Research platform, an all-in-one platform which processes intercepted phone calls, runs state-of-the-art components such as speaker identification, automatic speech recognition or named entity detection, and builds a knowledge graph of the extracted information. Our aim for this work is to do a first step in the direction of an open research platform combining speech, text, and video processing algorithms with criminal network analysis for combating organized crime",
    "checked": true,
    "id": "f9220a2365e4859f306dadf73741a963765c1d3d",
    "semantic_title": "roxanne research platform: automate criminal investigations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/flucha21_interspeech.html": {
    "title": "The LIUM Human Active Correction Platform for Speaker Diarization",
    "volume": "main",
    "abstract": "We developed a human assisted speaker diarization platform that enables a human annotator to correct the output of any speaker diarization system by providing a graphical view of the diarization segmentation and clustering steps while guiding the human annotator to optimize the correction process and easily improve the resulting diarization",
    "checked": true,
    "id": "4c1a33cd483f7608dbc9c14a000c9fa74d83a91f",
    "semantic_title": "the lium human active correction platform for speaker diarization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oh21_interspeech.html": {
    "title": "On-Device Streaming Transformer-Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "This work is the first attempt to run streaming Transformer-based end-to-end speech recognition on embedded scale IoT systems. Recently there are many researches on online Transformer-based speech recognition such as a contextual block encoder [1] and a block-wise synchronous beam search [2]. Based on them we designed a novel fully-streaming end-to-end speech recognition method using Transformer. By efficiently utilizing a connectionist temporal classification network to detect symbol and sentence boundaries, we make decoder in streaming manner. Moreover, by using the optimized model structure, the proposed method could be deployed on a low-power edge device such as Raspberry Pi 4B with the high accuracy and the small latency. With the experiments with Librispeech corpus, the methods achieved word error rates of 3.76% and 9.25% respectively. Also the recognition speed is measured in two aspects; the real-time factor and the user perceived latency. The system is evaluated to have 0.84 xRT and the average latency of 0.75Â±0.62 seconds on Raspberry Pi 4B",
    "checked": true,
    "id": "7b93d0d442d3fcc0dde20208509d35cd08b606eb",
    "semantic_title": "on-device streaming transformer-based end-to-end speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cmejla21_interspeech.html": {
    "title": "Advanced Semi-Blind Speaker Extraction and Tracking Implemented in Experimental Device with Revolving Dense Microphone Array",
    "volume": "main",
    "abstract": "We present a new device for speaker extraction and physical tracking and demonstrate its use in real conditions. The device is equipped with a dense planar array consisting of 64 microphones mounted on a rotating platform. State-of-the-art blind source extraction algorithms controlled by x-vector piloting are used to extract the desired speaker, which is being tracked by the rotating microphone array. The audience will experience the functionality of the device and the potential of the blind algorithms to extract the speaker from multi-source noisy recordings in a live situation",
    "checked": true,
    "id": "69cdda7308c708a515b9af9aa63a37f75f4325d4",
    "semantic_title": "advanced semi-blind speaker extraction and tracking implemented in experimental device with revolving dense microphone array",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ney21_interspeech.html": {
    "title": "Forty Years of Speech and Language Processing: From Bayes Decision Rule to Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ee6871b3ed3fc7bed4016f959d82b885e7eebc18",
    "semantic_title": "forty years of speech and language processing: from bayes decision rule to deep learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chorowski21_interspeech.html": {
    "title": "Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw",
    "volume": "main",
    "abstract": "We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval",
    "checked": true,
    "id": "77ba536360ba5e46d3072424264c02ce9d9223ab",
    "semantic_title": "information retrieval for zerospeech 2021: the submission by university of wroclaw",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chorowski21b_interspeech.html": {
    "title": "Aligned Contrastive Predictive Coding",
    "volume": "main",
    "abstract": "We investigate the possibility of forcing a self-supervised model trained using a contrastive predictive loss, to extract slowly varying latent representations. Rather than producing individual predictions for each of the future representations, the model emits a sequence of predictions shorter than the sequence of upcoming representations to which they will be aligned. In this way, the prediction network solves a simpler task of predicting the next symbols, but not their exact timing, while the encoding network is trained to produce piece-wise constant latent codes. We evaluate the model on a speech coding task and demonstrate that the proposed Aligned Contrastive Predictive Coding (ACPC) leads to higher linear phone prediction accuracy and lower ABX error rates, while being slightly faster to train due to the reduced number of prediction heads",
    "checked": true,
    "id": "1a03d08a8ce390b8f99fd919c95fb2c2f4c0d567",
    "semantic_title": "aligned contrastive predictive coding",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/suter21_interspeech.html": {
    "title": "Neural Text Denormalization for Speech Transcripts",
    "volume": "main",
    "abstract": "This paper presents a simple sequence-to-sequence approach to restore standard orthography in raw, normalized speech transcripts, including insertion of punctuation marks, prediction of capitalization, restoration of numeric forms, formatting of dates and times, and other, fully data-driven adjustments. We further describe our method to generate synthetic parallel training data, and explore suitable performance metrics, which we align with human judgment through subjective MOS-like evaluations Our models for English, Russian, and German have a word error rate of 6.36%, 4.88%, and 5.23%, respectively. We focus on simplicity and reproducibility, make our framework available under a BSD license, and share our base models for English and Russian",
    "checked": true,
    "id": "cf532e151c34eb2bb1c668e9f6ae1d38764a9379",
    "semantic_title": "neural text denormalization for speech transcripts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/joglekar21_interspeech.html": {
    "title": "Fearless Steps Challenge Phase-3 (FSC P3): Advancing SLT for Unseen Channel and Mission Data Across NASA Apollo Audio",
    "volume": "main",
    "abstract": "The Fearless Steps Challenge (FSC) initiative was designed to host a series of progressively complex tasks to promote advanced speech research across naturalistic \"Big Data\" corpora. The Center for Robust Speech Systems at UT-Dallas in collaboration with the National Institute of Standards and Technology (NIST) and Linguistic Data Consortium (LDC) conducted Phase-3 of the FSC series (FSC P3), with a focus on motivating speech and language technology (SLT) system generalizability across channel and mission diversity under the same training conditions as in Phase-2. The FSC P3 introduced 10 hours of previously unseen channel audio from Apollo-11 and 5 hours of novel audio from Apollo-13 to be evaluated over both previously established and newly introduced SLT tasks with streamlined tracks. This paper presents an overview of the newly introduced conversational analysis tracks, Apollo-13 data, and analysis of system performance for matched and mismatched challenge conditions. We also discuss the Phase-3 challenge results, evolution of system performance across the three Phases, and next steps in the Challenge Series",
    "checked": true,
    "id": "e896215b18529d671f88c0b25f5799109721b94d",
    "semantic_title": "fearless steps challenge phase-3 (fsc p3): advancing slt for unseen channel and mission data across nasa apollo audio",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leykum21_interspeech.html": {
    "title": "Voice Quality in Verbal Irony: Electroglottographic Analyses of Ironic Utterances in Standard Austrian German",
    "volume": "main",
    "abstract": "When using verbal irony in interpersonal communication, paraverbal cues can reduce the risk of misunderstandings. Besides fundamental frequency, intensity and duration, speakers could use voice quality parameters to disambiguate between ironic and literal utterances. How these paraverbal cues are used to mark irony appears to be language- and/or culture-specific. Since the role of voice quality in ironic utterances has not yet been investigated in Austrian German, the present study addresses this issue. In addition to the acoustic signal, the vocal fold vibration is recorded via electroglottography (EGG). The detailed analysis of the EGG data as well as the acoustic data, provides insight into voice quality characteristics of ironic and literal realisations of short utterances. The analyses reveal that, in Standard Austrian German, some differences in voice quality exist between ironic and literal realisations of utterances: When being ironic, speakers' voices tend to be breathier, creakier or rougher. Differences are more pronounced in the older age group and in male speakers",
    "checked": true,
    "id": "3c6e426889a2a66e3198bc5b8800e302bc6a21ae",
    "semantic_title": "voice quality in verbal irony: electroglottographic analyses of ironic utterances in standard austrian german",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hutin21_interspeech.html": {
    "title": "Synchronic Fortition in Five Romance Languages? A Large Corpus-Based Study of Word-Initial Devoicing",
    "volume": "main",
    "abstract": "Devoicing is a process whereby a voiced consonant such as /bdg/ is realized as voiceless [ptk]. Some theorists [1,2] propose that this phenomenon is an instance of fortition, or consonant strengthening, especially when it occurs word-initially. This study proposes an in-depth exploration of voicing alternations in word-initial position in five Romance languages (Portuguese, Spanish, French, Italian, Romanian) using large corpora (ca. 1000h of speech) and automatic alignment. Our results show that (i) there is initial devoicing in all languages, and (ii) this devoicing is conditioned by the preceding context. This allows the languages to be divided into those displaying (a) only phrase-initial fortition (Spanish), (b) phrase-initial and post-obstruent fortition (French, Romanian and possibly Italian) and (c) generalized word-initial fortition (Portuguese)",
    "checked": true,
    "id": "a2bc2547710fa9d52870f0f043eda73ca5f10e8a",
    "semantic_title": "synchronic fortition in five romance languages? a large corpus-based study of word-initial devoicing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kraljevski21_interspeech.html": {
    "title": "Glottal Stops in Upper Sorbian: A Data-Driven Approach",
    "volume": "main",
    "abstract": "We present a data-driven approach for the quantitative analysis of glottal stops before word-initial vowels in Upper Sorbian, a West Slavic minority language spoken in Germany. Glottal stops are word-boundary markers and their detection can improve the performance of automatic speech recognition and speech synthesis systems We employed cross-language transfer using an acoustic model in German to develop a forced-alignment method for the phonetic segmentation of a read-speech corpus in Upper Sorbian. The missing phonemic units were created by combining the existing phoneme models. In the forced-alignment procedure, the glottal stops were considered optional in front of word-initial vowels To investigate the influence of speaker type (males, females, and children) and vowel on the occurrence of glottal stops, binomial regression analysis with a generalized linear mixed model was performed. Results show that children glottalize word-initial vowels more frequently than adults, and that glottal stop occurrences are influenced by vowel quality",
    "checked": true,
    "id": "f39b4141333b0d6a1966c733d758c8e0cb20bf3a",
    "semantic_title": "glottal stops in upper sorbian: a data-driven approach",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ludusan21_interspeech.html": {
    "title": "Cue Interaction in the Perception of Prosodic Prominence: The Role of Voice Quality",
    "volume": "main",
    "abstract": "Voice quality is an important dimension in human communication, used to mark a variety of phenomena in speech, including prosodic prominence. Even though numerous studies have shown that speakers modify their voice quality parameters for marking prosodic prominence, the impact of these modifications on perceived prominence is less studied. Our investigation looks at the effect of a well-known measure of voice quality, cepstral peak prominence (CPP), on syllabic prominence ratings given by both naive and expert listeners. Employing read speech materials in German, we quantify the role of CPP alone and in combination with other acoustic cues marking prominence, namely intensity, duration and fundamental frequency. While CPP, by itself, had a significant effect on the perceived prominence for most of the listeners, when used in conjunction with the other cues, its impact was reduced. Moreover, when assessing the importance of each of these four cues for determining the perceived prominence score we found important individual variation, as well as differences between naive and expert listeners",
    "checked": true,
    "id": "427170c3bb548edecc824bf622b83faf81d08ce3",
    "semantic_title": "cue interaction in the perception of prosodic prominence: the role of voice quality",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rodriguez21_interspeech.html": {
    "title": "Glottal Sounds in Korebaju",
    "volume": "main",
    "abstract": "Korebaju (ISO639-3: coe) [ÌkÃ²rÃ¨Î²Ã hÃ­Ìµ] is a tonal language spoken in the foothills of the Colombian Amazon. Three field surveys carried out between 2017 and 2019 with six native speakers (3 females and 3 males) from the same village provide a set of glottal productions at both phonetic and phonological levels. This study focuses on the four types of glottal units we have found in this language: A set of vowels /a /, /e /, /o /, [i ] and [É¨ ] including 3 phonemes; the glottal stop [Ê”] and the consonant [*] transcribed and described as a by [1]. Both consonants occurred in intervocalic contexts and can be analyzed as a suprasegmental feature [constricted glottis] which marks the syllable onset. Finally, we have also found a clear and systematic burst which accompanies the release of the nasal consonants [m , n , É² ]. No change was found in the EGG signal for these consonants suggesting an abrupt release of the aeroacoustic pressure",
    "checked": true,
    "id": "02a7fa355efd1db9970dba5803d0f04fe0af3a69",
    "semantic_title": "glottal sounds in korebaju",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chanclu21_interspeech.html": {
    "title": "Automatic Classification of Phonation Types in Spontaneous Speech: Towards a New Workflow for the Characterization of Speakers' Voice Quality",
    "volume": "main",
    "abstract": "Voice quality is known to be an important factor for the characterization of a speaker's voice, both in terms of physiological features (mainly laryngeal and supralaryngeal) and of the speaker's habits (sociolinguistic factors). This paper is devoted to one of the main components of voice quality: phonation type. It proposes neural representations of speech followed by a cascade of two binary neural network-based classifiers, one dedicated to the detection of modal and nonmodal vowels, and one for the classification of nonmodal vowels into creaky and breathy types. This approach is evaluated on the spontaneous part of the PTSVOX database, following an expert manual labelling of the data by phonation type. The results of the proposed classifiers reaches on average 85%accuracy at the frame-level and up to 95% accuracy at the segment-level. Further research is planned to generalize the classifiers on more contexts and speakers, and thus pave the way for a new workflow aimed at characterizing phonation types",
    "checked": true,
    "id": "12b0265127906da817e4d762f6628727d5a920b6",
    "semantic_title": "automatic classification of phonation types in spontaneous speech: towards a new workflow for the characterization of speakers' voice quality",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/son21_interspeech.html": {
    "title": "Measuring Voice Quality Parameters After Speaker Pseudonymization",
    "volume": "main",
    "abstract": "Collecting and sharing speech resources is important for progress in speech science and technology. Often, speech resources cannot be shared because of concerns over the privacy of the speakers, e.g., minors or people with medical conditions. Current technologies for pseudonymizing speech have only been tested on \"standard\" speech for which pseudonymization methods are evaluated on speaker identification risk, intelligibility, and naturalness. For many applications, the important characteristics are para-linguistic aspects of the speech, e.g., voice quality, emotion, or disease progression. Little information is available about the extent to which speaker pseudonymization methods preserve such paralinguistic information. The current study investigates how well voice quality parameters are preserved by an example speech pseudonymization application. Correlations prove to be high between original and pseudonymized recordings for seven acoustic parameters and a composite measure of dysphonia, the Root mean square errors for these parameters were reasonably small. A linear mixed effect model shows a link between the difference between source and target speaker and the size of the absolute difference in the It is argued that new measures of quality are needed for pseudonymized non-standard speech before wide-spread application of pseudonymized speech can be considered in research and clinical practise",
    "checked": true,
    "id": "8e98709e1cd7fca3a15e2775f9acbb78b379db79",
    "semantic_title": "measuring voice quality parameters after speaker pseudonymization",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/steinert21_interspeech.html": {
    "title": "Audio-Visual Recognition of Emotional Engagement of People with Dementia",
    "volume": "main",
    "abstract": "Dementia places an immeasurable burden on affected individuals and caregivers. In addition to general cognitive decline, dementia has a negative impact on communication. Technical activation systems are thus in high demand, as cognitive activation may help to moderate the decline. However, effective activation requires sustained engagement â€” which, in turn, first needs to be reliably recognized. In this study, we examine emotional engagement recognition for People with Dementia (PwD) using non-intrusive biosignals resulting from speech communication and facial expressions. PwD suffering from mild to severe dementia used a tablet-based activation system over multiple sessions. We demonstrate that they retained their ability to verbally express emotional engagement even at severe stages of the disease. For recognition of emotional engagement, we propose an architecture of Bidirectional Long-Short-Term-Memory Networks that combines video information with up to three speech-based feature sets (eGeMAPS, ComParE'13, DeepSpectrum). Using data of 24 PwD, we show that adding speech improves recognition performance significantly compared to a video-only model. Interestingly, disease-progression did not appear to have a substantial impact on recognition performance in this sample. We further discuss the opportunities and challenges of detecting emotional engagement from speech in PwD",
    "checked": true,
    "id": "2cb5bd1f22bcd83e878171d635c207139e3e6711",
    "semantic_title": "audio-visual recognition of emotional engagement of people with dementia",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hecker21_interspeech.html": {
    "title": "Speaking Corona? Human and Machine Recognition of COVID-19 from Voice",
    "volume": "main",
    "abstract": "With the COVID-19 pandemic, several research teams have reported successful advances in automated recognition of COVID-19 by voice. Resulting voice-based screening tools for COVID-19 could support large-scale testing efforts. While capabilities of machines on this task are progressing, we approach the so far unexplored aspect whether human raters can distinguish COVID-19 positive and negative tested speakers from voice samples, and compare their performance to a machine learning baseline. To account for the challenging symptom similarity between COVID-19 and other respiratory diseases, we use a carefully balanced dataset of voice samples, in which COVID-19 positive and negative tested speakers are matched by their symptoms alongside COVID-19 negative speakers without symptoms. Both human raters and the machine struggle to reliably identify COVID-19 positive speakers in our dataset. These results indicate that particular attention should be paid to the distribution of symptoms across all speakers of a dataset when assessing the capabilities of existing systems. The identification of acoustic aspects of COVID-19-related symptom manifestations might be the key for a reliable voice-based COVID-19 detection in the future by both trained human raters and machine learning models",
    "checked": true,
    "id": "983c6877511b509ed42322604c15e3492114a56c",
    "semantic_title": "speaking corona? human and machine recognition of covid-19 from voice",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21b_interspeech.html": {
    "title": "Acoustic-Prosodic, Lexical and Demographic Cues to Persuasiveness in Competitive Debate Speeches",
    "volume": "main",
    "abstract": "We analyze the acoustic-prosodic and lexical correlates of persuasiveness, taking into account speaker, judge and debate characteristics in a novel data set of 674 audio profiles, transcripts, evaluation scores and demographic data from professional debate tournament speeches. By conducting 10-fold cross validation experiments with linear, LASSO and random forest regression, we predict how different feature combinations contribute toward speech scores (i.e. persuasiveness) between men and women. Overall, lexical features, i.e. word complexity, nouns, fillers and hedges, are the most predictive features of speech evaluation scores; in addition to the gender composition of judge panels and opponents. In a combined lexical and demographic feature model, we achieve an R of 0.40. Different lexical features predict speech evaluation scores for male vs. female speakers, and further investigation is necessary to understand whether differential evaluation standards applied across genders. This work contributes a larger-scale debate data set in a democratically relevant, competitive format with high external relevance to persuasive speech education in other competitive settings",
    "checked": true,
    "id": "da08ac507456cd1e794ac63e36ed737c75f13cbe",
    "semantic_title": "acoustic-prosodic, lexical and demographic cues to persuasiveness in competitive debate speeches",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/borgstrom21_interspeech.html": {
    "title": "Unsupervised Bayesian Adaptation of PLDA for Speaker Verification",
    "volume": "main",
    "abstract": "This paper presents a Bayesian framework for unsupervised domain adaptation of Probabilistic Linear Discriminant Analysis (PLDA). By interpreting class labels as latent random variables, Variational Bayes (VB) is used to derive a maximum (MAP) solution of the adapted PLDA model when labels are missing, referred to as VB-MAP. The VB solution iteratively infers class labels and updates PLDA hyperparameters, offering a systematic framework for dealing with unlabeled data. While presented as a general solution, this paper includes experimental results for domain adaptation in speaker verification. VB-MAP estimation is applied to the 2016 and 2018 NIST Speaker Recognition Evaluations (SREs), both of which included small and unlabeled in-domain data sets, and is shown to provide performance improvements over a variety of state-of-the-art domain adaptation methods. Additionally, VB-MAP estimation is used to train a fully unsupervised PLDA model, suffering only minor performance degradation relative to conventional supervised training, offering promise for training PLDA models when no relevant labeled data exists",
    "checked": true,
    "id": "e70e19c203f302014d166ea0696d8e80fc96d71f",
    "semantic_title": "unsupervised bayesian adaptation of plda for speaker verification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21i_interspeech.html": {
    "title": "The DKU-Duke-Lenovo System Description for the Fearless Steps Challenge Phase III",
    "volume": "main",
    "abstract": "This paper describes the systems developed by the DKU-Duke-Lenovo team for the Fearless Steps Challenge Phase III. For the speech activity detection (SAD) task, we employ the U-Net-based model which has not been used for SAD before, observing a DCF of 1.915% on the eval set. For the speaker identification (SID) task, we adopt the ResNet-SE and ECAPA-TDNN model, and we obtain a Top-5 accuracy of 86.21%. For the speaker diarization (SD) task, we employ several different clustering methods. Besides, domain adaptation, system fusion, and Target-Speaker Voice Activity Detection (TS-VAD) significantly improve the SD performance. We obtain a DER of 12.32% on track 2, and the major contribution is from our ResNet-based TS-VAD model. We finally achieve a first-place ranking for SD and SID and a second-place for SAD in the challenge",
    "checked": true,
    "id": "243bc87ec4be8f1b2f8032b5d7899ae1d89d010c",
    "semantic_title": "the dku-duke-lenovo system description for the fearless steps challenge phase iii",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21f_interspeech.html": {
    "title": "Improved Meta-Learning Training for Speaker Verification",
    "volume": "main",
    "abstract": "Meta-learning (ML) has recently become a research hotspot in speaker verification (SV). We introduce two methods to improve the meta-learning training for SV in this paper. For the first method, a backbone embedding network is first jointly trained with the conventional cross entropy loss and prototypical networks (PN) loss. Then, inspired by speaker adaptive training in speech recognition, additional transformation coefficients are trained with only the PN loss. The transformation coefficients are used to modify the original backbone embedding network in the x-vector extraction process. Furthermore, the random erasing (RE) data augmentation technique is applied to all support samples in each episode to construct positive pairs, and a contrastive loss between the augmented and the original support samples is added to the objective in model training. Experiments are carried out on the Speaker in the Wild (SITW) and VOiCES databases. Both of the methods can obtain consistent improvements over existing meta-learning training frameworks. By combining these two methods, we can observe further improvements on these two databases",
    "checked": true,
    "id": "a2c4227eb8073e4238237efc667a10576b65ce89",
    "semantic_title": "improved meta-learning training for speaker verification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21j_interspeech.html": {
    "title": "Variational Information Bottleneck Based Regularization for Speaker Recognition",
    "volume": "main",
    "abstract": "Speaker recognition (SR) is inevitably affected by noise in real-life scenarios, resulting in decreased recognition accuracy. In this paper, we introduce a novel regularization method, variable information bottleneck (VIB), in speaker recognition to extract robust speaker embeddings. VIB prompts the neural network to ignore as much speaker-identity irrelevant information as possible. We also propose a more effective network, VovNet with an ultra-lightweight subspace attention module (ULSAM), as a feature extractor. ULSAM infers different attention maps for each feature map subspace, enabling efficient learning of cross-channel information along with multi-scale and multi-frequency feature representation. The experimental results demonstrate that our proposed framework outperforms the ResNet-based baseline by 11.4% in terms of equal error rate (EER). The VIB regularization method gives a further performance boost with an 18.9% EER decrease",
    "checked": true,
    "id": "43ea4f86b12fc01b8ae29756806131f9ccd995bd",
    "semantic_title": "variational information bottleneck based regularization for speaker recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/brummer21_interspeech.html": {
    "title": "Out of a Hundred Trials, How Many Errors Does Your Speaker Verifier Make?",
    "volume": "main",
    "abstract": "Out of a hundred trials, how many errors does your speaker verifier make? For the user this is an important, practical question, but researchers and vendors typically sidestep it and supply instead the conditional error-rates that are given by the ROC/DET curve. We posit that the user's question is answered by the Bayes error-rate. We present a tutorial to show how to compute the error-rate that results when making Bayes decisions with calibrated likelihood ratios, supplied by the verifier, and an hypothesis prior, supplied by the user. For perfect calibration, the Bayes error-rate is upper bounded by min(EER,P,1-P), where EER is the equal-error-rate and P, 1-P are the prior probabilities of the competing hypotheses. The EER represents the accuracy of the verifier, while min(P,1-P) represents the hardness of the classification problem. We further show how the Bayes error-rate can be computed also for non-perfect calibration and how to generalize from error-rate to expected cost. We offer some criticism of decisions made by direct score thresholding. Finally, we demonstrate by analyzing error-rates of the recently published DCA-PLDA speaker verifier",
    "checked": true,
    "id": "6e0fc506853f73cf2b9e867938bd7750da9493c5",
    "semantic_title": "out of a hundred trials, how many errors does your speaker verifier make?",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chojnacka21_interspeech.html": {
    "title": "SpeakerStew: Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System",
    "volume": "main",
    "abstract": "In this paper, we describe â€” a hybrid system to perform speaker verification on 46 languages. Two core ideas were explored in this system: (1) Pooling training data of different languages together for multilingual generalization and reducing development cycles; (2) A novel triage mechanism between text-dependent and text-independent models to reduce runtime cost and expected latency. To the best of our knowledge, this is the first study of speaker verification systems at the scale of 46 languages. The problem is framed from the perspective of using a smart speaker device with interactions consisting of a wake-up keyword (text-dependent) followed by a speech query (text-independent). Experimental evidence suggests that training on multiple languages can generalize to unseen varieties while maintaining performance on seen varieties. We also found that it can reduce computational requirements for training models by an order of magnitude. Furthermore, during model inference on English data, we observe that leveraging a triage framework can reduce the number of calls to the more computationally expensive text-independent system by 73% (and reduce latency by 59%) while maintaining an EER no worse than the text-independent setup",
    "checked": true,
    "id": "638dd69688b38b808b90ab7a3992b90e543c9142",
    "semantic_title": "speakerstew: scaling to many languages with a triaged multilingual text-dependent and text-independent speaker verification system",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21k_interspeech.html": {
    "title": "AntVoice Neural Speaker Embedding System for FFSVC 2020",
    "volume": "main",
    "abstract": "This paper presents a comprehensive description of the AntVoice system for the first two tracks of far-field speaker verification from single microphone array in FFSVC 2020 [1]. The system is based on neural speaker embeddings from deep neural network-based encoder networks. These encoder networks for acoustic modeling include 2D convolutional residual-like networks that are shown to be effective on the tasks. Specifically, we apply the Squeeze-and-Excitation residual network (SE-ResNet) [2] to model cross-channel inter-dependency information. On short utterances, we observe that SE-ResNet outperforms alternative methods in the text-dependent verification task. The system adopts a joint loss function that combines the additive cosine margin softmax loss [3] with the equidistant triplet-based loss[4]. This loss function results in performance gains with more discriminative speaker embeddings from enhanced intra-class similarity and increased inter-class variances. We also apply speech enhancement and data augmentation to improve data quality and diversity. Even without using model ensembles, the proposed system significantly outperforms the baselines [1] in both tracks of the speaker verification challenge. With fusion of several encoder neural networks, this system is able to achieve further performance improvements consistently. In the end, the AntVoice system achieves the third place in the text-dependent verification task",
    "checked": true,
    "id": "6a12130f7269bd348eec6b7ee8f0ff58cd90f0ac",
    "semantic_title": "antvoice neural speaker embedding system for ffsvc 2020",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21b_interspeech.html": {
    "title": "Gradient Regularization for Noise-Robust Speaker Verification",
    "volume": "main",
    "abstract": "Noise robustness is a challenge for speaker recognition systems. To solve this problem, one of the most common approaches is to joint-train a model by using both clean and noisy utterances. However, the gradients calculated on noisy utterances generally contain speaker-irrelevant noisy components, resulting in overfitting for the seen noisy data and poor generalization for the unseen noisy environments. To alleviate this problem, we propose the gradient regularization method to reduce the speaker-irrelevant noisy components by aligning the gradients among the noisy utterances and their clean counterparts. Specifically, the gradients on noisy utterances are forced to follow the directions of the gradients calculated on their clean counterparts, and the gradients across different types of noisy utterances are also aligned to point in similar directions. Since the noise-related components of the gradients can be reduced by the above alignment, the speaker model can be prevented from encoding irrelevant noisy information. To achieve the gradient regularization goals, a novel sequential inner training strategy is also proposed. Experiments on the VoxCeleb1 dataset indicate that our method achieves the best performance in seen and unseen noisy environments",
    "checked": true,
    "id": "37f9f829bd3383524d336125ede257bbbab4fcd8",
    "semantic_title": "gradient regularization for noise-robust speaker verification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kataria21_interspeech.html": {
    "title": "Deep Feature CycleGANs: Speaker Identity Preserving Non-Parallel Microphone-Telephone Domain Adaptation for Speaker Verification",
    "volume": "main",
    "abstract": "With the increase in the availability of speech from varied domains, it is imperative to use such out-of-domain data to improve existing speech systems. Domain adaptation is a prominent pre-processing approach for this. We investigate it to adapt microphone speech to the telephone domain. Specifically, we explore CycleGAN-based unpaired translation of microphone data to improve the x-vector/speaker embedding network for Telephony Speaker Verification. We first demonstrate the efficacy of this on real challenging data and then, to improve further, we modify the CycleGAN formulation to make the adaptation We modify CycleGAN's identity loss, cycle-consistency loss, and adversarial loss to operate in the space of a signal are extracted from an auxiliary (speaker embedding) network and, hence, preserves speaker identity. Our 3D convolution-based Deep Feature Discriminators (DFD) show relative improvements of 5â€“10% in terms of equal error rate. To dive deeper, we study a challenging scenario of pooling (adapted) microphone and telephone data with data augmentations and telephone codecs. Finally, we highlight the sensitivity of CycleGAN hyper-parameters and introduce a parameter called ",
    "checked": true,
    "id": "c3bb7ff3eba44535c9b704ee52041f91bde7bcd0",
    "semantic_title": "deep feature cyclegans: speaker identity preserving non-parallel microphone-telephone domain adaptation for speaker verification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pu21_interspeech.html": {
    "title": "Scaling Effect of Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "The success of modern deep learning systems is built on two cornerstones, massive amount of annotated training data and advanced computational infrastructure to support large-scale computation. In recent years, the model size of state-of-the-art deep learning systems has rapidly increased and sometimes reached to billions of parameters. Herein we take a close look into this phenomenon and present an empirical study on the scaling effect of model size for self-supervised speech models. In particular, we investigate the quantitative relationship between the model size and the loss/accuracy performance on speech tasks. First, the power-law scaling property between the number of parameters and the L self-supervised loss is verified for speech models. Then the advantage of large speech models in learning effective speech representations is demonstrated in two downstream tasks: i) speaker recognition and ii) phoneme classification. Moreover, it has been shown that the model size of self-supervised speech networks is able to compensate the lack of annotation when there is insufficient training data",
    "checked": true,
    "id": "752985595ce02327b76bb11e0afafb9d31522215",
    "semantic_title": "scaling effect of self-supervised speech models",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21c_interspeech.html": {
    "title": "Joint Feature Enhancement and Speaker Recognition with Multi-Objective Task-Oriented Network",
    "volume": "main",
    "abstract": "Recently, increasing attention has been paid to the joint training of upstream and downstream tasks, and to address the challenge of how to synchronize various loss functions in a multi-objective scenario. In this paper, to address the competing gradient directions between the speaker classification loss and the feature enhancement loss, we propose an asynchronous subregion optimization approach for the joint training of feature enhancement and speaker embedding neural networks. For the asynchronous subregion optimization, the squeeze and excitation (SE) method is introduced in the enhancement network to adaptively select important channels for speaker embedding. Furthermore, channel-wise feature concatenation is applied between the input feature and the enhanced feature to address the distortion of speaker information that is caused by enhancement loss. By using the proposed joint training network with asynchronous subregion optimization and channel-wise feature concatenation, we obtained relative gains of 11.95% and 6.43% in equal error rate on a noisy version of Voxceleb1 and VOiCES corpus, respectively",
    "checked": true,
    "id": "701d9058db1539b3f0c84c04d8e4a17c801eab05",
    "semantic_title": "joint feature enhancement and speaker recognition with multi-objective task-oriented network",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21g_interspeech.html": {
    "title": "Multi-Level Transfer Learning from Near-Field to Far-Field Speaker Verification",
    "volume": "main",
    "abstract": "In far-field speaker verification, the performance of speaker embeddings is susceptible to degradation when there is a mismatch between the conditions of enrollment and test speech. To solve this problem, we propose the feature-level and instance-level transfer learning in the teacher-student framework to learn a domain-invariant embedding space. For the feature-level knowledge transfer, we develop the contrastive loss to transfer knowledge from teacher model to student model, which not only decrease the intra-class distance, but also enlarge the inter-class distance. Moreover, we propose the instance-level pairwise distance transfer method to force the student model to preserve pairwise instances distance from the well optimized embedding space of the teacher model. On FFSVC 2020 evaluation set, our EER on Full-eval trials is relatively reduced by 13.9% compared with the fusion system result on Partial-eval trials of Task2. On Task1, compared with the winner's DenseNet result on Partial-eval trials, our minDCF on Full-eval trials is relatively reduced by 6.3%. On Task3, the EER and minDCF of our proposed method on Full-eval trials are very close to the result of the fusion system on Partial-eval trials. Our results also outperform other competitive domain adaptation methods",
    "checked": true,
    "id": "00b6cac57da8fa92c4a145b4556220c8b185de67",
    "semantic_title": "multi-level transfer learning from near-field to far-field speaker verification",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/patino21_interspeech.html": {
    "title": "Speaker Anonymisation Using the McAdams Coefficient",
    "volume": "main",
    "abstract": "Anonymisation has the goal of manipulating speech signals in order to degrade the reliability of automatic approaches to speaker recognition, while preserving other aspects of speech, such as those relating to intelligibility and naturalness. This paper reports an approach to anonymisation that, unlike other current approaches, requires no training data, is based upon well-known signal processing techniques and is both efficient and effective. The proposed solution uses the McAdams coefficient to transform the spectral envelope of speech signals. Results derived using common VoicePrivacy 2020 databases and protocols show that random, optimised transformations can outperform competing solutions in terms of anonymisation while causing only modest, additional degradations to intelligibility, even in the case of a semi-informed privacy adversary",
    "checked": true,
    "id": "857ef52c30dddce03ecd2ffba2697b9dd2f9cb21",
    "semantic_title": "speaker anonymisation using the mcadams coefficient",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21_interspeech.html": {
    "title": "Multi-Stream Gated and Pyramidal Temporal Convolutional Neural Networks for Audio-Visual Speech Separation in Multi-Talker Environments",
    "volume": "main",
    "abstract": "Speech separation is the task of extracting target speech from noisy mixture. In applications like video telephones or video conferencing, lip movements of the target speaker are accessible, which can be leveraged for speech separation. This paper proposes a time-domain audio-visual speech separation model under multi-talker environments. The model receives audio-visual inputs including noisy mixture and speaker lip embedding, and reconstructs clean speech waveform for the target speaker. Once trained, the model can be flexibly applied to unknown number of total speakers. This paper introduces and investigates the multi-stream gating mechanism and pyramidal convolution in temporal convolutional neural networks for audio-visual speech separation task. Speaker- and noise-independent multi-talker separation experiments are conducted on GRID benchmark dataset. The experimental results demonstrate the proposed method achieves 3.9 dB and 1.0 dB SI-SNRi improvement when compared with audio-only and audio-visual baselines respectively, showing effectiveness of the proposed method",
    "checked": true,
    "id": "d11d97bc016932b35b0e20dfde21b5cb81c29e42",
    "semantic_title": "multi-stream gated and pyramidal temporal convolutional neural networks for audio-visual speech separation in multi-talker environments",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21l_interspeech.html": {
    "title": "TeCANet: Temporal-Contextual Attention Network for Environment-Aware Speech Dereverberation",
    "volume": "main",
    "abstract": "In this paper, we exploit the effective way to leverage contextual information to improve the speech dereverberation performance in real-world reverberant environments. We propose a temporal-contextual attention approach on the deep neural network (DNN) for environment-aware speech dereverberation, which can adaptively attend to the contextual information. More specifically, a FullBand based Temporal Attention approach (FTA) is proposed, which models the correlations between the fullband information of the context frames. In addition, considering the difference between the attenuation of high frequency bands and low frequency bands (high frequency bands attenuate faster than low frequency bands) in the room impulse response (RIR), we also propose a SubBand based Temporal Attention approach (STA). In order to guide the network to be more aware of the reverberant environments, we jointly optimize the dereverberation network and the reverberation time (RT60) estimator in a multi-task manner. Our experimental results indicate that the proposed method outperforms our previously proposed reverberation-time-aware DNN and the learned attention weights are fully physical consistent. We also report a preliminary yet promising dereverberation and recognition experiment on real test data",
    "checked": true,
    "id": "c891e58c4705ff73dedc3bca703580d7b2c33adb",
    "semantic_title": "tecanet: temporal-contextual attention network for environment-aware speech dereverberation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gu21_interspeech.html": {
    "title": "Residual Echo and Noise Cancellation with Feature Attention Module and Multi-Domain Loss Function",
    "volume": "main",
    "abstract": "For real-time acoustic echo cancellation in noisy environments, the classical linear adaptive filters (LAFs) can only remove the linear components of acoustic echo. To further attenuate the non-linear echo components and background noise, this paper proposes a deep learning-based residual echo and noise cancellation (RENC) model, where multiple inputs are utilized and weighted by a feature attention module. More specifically, input features extracted from the far-end reference and the echo estimated by the LAF are scaled with time-frequency attention weights, depending on their correlation with the residual interference in LAF's output. Moreover, a scale-independent mean square error and perceptual loss function are further suggested for training the RENC model. Experimental results validate the efficacy of the proposed feature attention module and multi-domain loss function, which achieve an 8.4%, 14.9% and 29.5% improvement in perceptual evaluation of speech quality (PESQ), scale-invariant signal-to-distortion ratio (SI-SDR) and echo return loss enhancement (ERLE), respectively",
    "checked": true,
    "id": "4364b77c1a756bfe2a6256f417f088edc6353496",
    "semantic_title": "residual echo and noise cancellation with feature attention module and multi-domain loss function",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21c_interspeech.html": {
    "title": "MIMO Self-Attentive RNN Beamformer for Multi-Speaker Speech Separation",
    "volume": "main",
    "abstract": "Recently, our proposed recurrent neural network (RNN) based all deep learning minimum variance distortionless response (ADL-MVDR) beamformer method yielded superior performance over the conventional MVDR by replacing the matrix inversion and eigenvalue decomposition with two RNNs. In this work, we present a self-attentive RNN beamformer to further improve our previous RNN-based beamformer by leveraging on the powerful modeling capability of self-attention. Temporal-spatial self-attention module is proposed to better learn the beamforming weights from the speech and noise spatial covariance matrices. The temporal self-attention module could help RNN to learn global statistics of covariance matrices. The spatial self-attention module is designed to attend on the cross-channel correlation in the covariance matrices. Furthermore, a multi-channel input with multi-speaker directional features and multi-speaker speech separation outputs (MIMO) model is developed to improve the inference efficiency. The evaluations demonstrate that our proposed MIMO self-attentive RNN beamformer improves both the automatic speech recognition (ASR) accuracy and the perceptual estimation of speech quality (PESQ) against prior arts",
    "checked": true,
    "id": "1abd71fb70b8c3639af1d087cd179eaef8c718b0",
    "semantic_title": "mimo self-attentive rnn beamformer for multi-speaker speech separation",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/giri21_interspeech.html": {
    "title": "Personalized PercepNet: Real-Time, Low-Complexity Target Voice Separation and Enhancement",
    "volume": "main",
    "abstract": "The presence of multiple talkers in the surrounding environment poses a difficult challenge for real-time speech communication systems considering the constraints on network size and complexity. In this paper, we present Personalized PercepNet, a real-time speech enhancement model that separates a target speaker from a noisy multi-talker mixture without compromising on complexity of the recently proposed PercepNet. To enable speaker-dependent speech enhancement, we first show how we can train a perceptually motivated speaker embedder network to produce a representative embedding vector for the given speaker. Personalized PercepNet uses the target speaker embedding as additional information to pick out and enhance only the target speaker while suppressing all other competing sounds. Our experiments show that the proposed model significantly outperforms PercepNet and other baselines, both in terms of objective speech enhancement metrics and human opinion scores",
    "checked": true,
    "id": "9d41cfe2c1ef1691d0d5e256ac7d14a43327437e",
    "semantic_title": "personalized percepnet: real-time, low-complexity target voice separation and enhancement",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yemini21_interspeech.html": {
    "title": "Scene-Agnostic Multi-Microphone Speech Dereverberation",
    "volume": "main",
    "abstract": "Neural networks (NNs) have been widely applied in speech processing tasks, and, in particular, those employing microphone arrays. Nevertheless, most existing NN architectures can only deal with fixed and position-specific microphone arrays. In this paper, we present an NN architecture that can cope with microphone arrays whose number and positions of the microphones are unknown, and demonstrate its applicability in the speech dereverberation task. To this end, our approach harnesses recent advances in deep learning on set-structured data to design an architecture that enhances the reverberant log-spectrum. We use noisy and noiseless versions of a simulated reverberant dataset to test the proposed architecture. Our experiments on the noisy data show that the proposed scene-agnostic setup outperforms a powerful scene-aware framework, sometimes even with fewer microphones. With the noiseless dataset we show that, in most cases, our method outperforms the position-aware network as well as the state-of-the-art weighted linear prediction error (WPE) algorithm",
    "checked": true,
    "id": "4c49623df1f10a51408ebcc07558b74778f69aca",
    "semantic_title": "scene-agnostic multi-microphone speech dereverberation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html": {
    "title": "Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding Vectors Based on Regular Simplex",
    "volume": "main",
    "abstract": "This paper presents a new deep clustering (DC) method called manifold-aware DC (M-DC) that can enhance hyperspace utilization more effectively than the original DC. The original DC has a limitation in that a pair of two speakers has to be embedded having an orthogonal relationship due to its use of the one-hot vector-based loss function, while our method derives a unique loss function aimed at maximizing the target angle in the hyperspace based on the nature of a regular simplex. Our proposed loss imposes a higher penalty than the original DC when the speaker is assigned incorrectly. The change from DC to M-DC can be easily achieved by rewriting just one term in the loss function of DC, without any other modifications to the network architecture or model parameters. As such, our method has high practicability because it does not affect the original inference part. The experimental results show that the proposed method improves the performances of the original DC and its expansion method",
    "checked": true,
    "id": "7c8f7c7c4ac148d6f87c9f11ed75b52409750b54",
    "semantic_title": "manifold-aware deep clustering: maximizing angles between embedding vectors based on regular simplex",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21h_interspeech.html": {
    "title": "A Deep Learning Approach to Multi-Channel and Multi-Microphone Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Building on deep learning based acoustic echo cancellation (AEC) in the single-loudspeaker (single-channel) and single-microphone setup, this paper investigates multi-channel (multi-loudspeaker) AEC (MCAEC) and multi-microphone AEC (MMAEC). A convolutional recurrent network (CRN) is trained to predict the near-end speech from microphone signals with far-end signals used as additional information. We find that the deep learning based MCAEC approach avoids the non-uniqueness problem in traditional MCAEC algorithms. For the AEC setup with multiple microphones, rather than employing AEC for each microphone, we propose to train a single network to achieve echo removal for all microphones. Combining deep learning based AEC with supervised beamforming further improves the system performance. Experimental results show the effectiveness of deep learning approach to MCAEC and MMAEC. Furthermore, deep learning based methods are capable of removing echo and noise simultaneously and work well in the presence of nonlinear distortions",
    "checked": true,
    "id": "deb4812c80ac3e0a508e531770fd27cb3dad7ac2",
    "semantic_title": "a deep learning approach to multi-channel and multi-microphone acoustic echo cancellation",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/na21_interspeech.html": {
    "title": "Joint Online Multichannel Acoustic Echo Cancellation, Speech Dereverberation and Source Separation",
    "volume": "main",
    "abstract": "This paper presents a joint source separation algorithm that simultaneously reduces acoustic echo, reverberation and interfering sources. Target speeches are separated from the mixture by maximizing independence with respect to the other sources. It is shown that the separation process can be decomposed into cascading sub-processes that separately relate to acoustic echo cancellation, speech dereverberation and source separation, all of which are solved using the auxiliary function based independent component/vector analysis techniques, and their solving orders are exchangeable. The cascaded solution not only leads to lower computational complexity but also better separation performance than the vanilla joint algorithm",
    "checked": true,
    "id": "0629a5e5ddf82a2bc8f9e272f7ae220e35ce6427",
    "semantic_title": "joint online multichannel acoustic echo cancellation, speech dereverberation and source separation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sato21_interspeech.html": {
    "title": "Should We Always Separate?: Switching Between Enhanced and Observed Signals for Overlapping Speech Recognition",
    "volume": "main",
    "abstract": "Although recent advances in deep learning technology improved automatic speech recognition (ASR), it remains difficult to recognize speech when it overlaps other people's voices. Speech separation or extraction is often used as a front-end to ASR to handle such overlapping speech. However, deep neural network-based speech enhancement can generate â€˜processing artifacts' as a side effect of the enhancement, which degrades ASR performance. For example, it is well known that single-channel noise reduction for non-speech noise (non-overlapping speech) often does not improve ASR. Likewise, the processing artifacts may also be detrimental to ASR in some conditions when processing overlapping speech with a separation/extraction method, although it is usually believed that separation/extraction improves ASR. In order to answer the question â€˜Do we always have to separate/extract speech from mixtures?', we analyze ASR performance on observed and enhanced speech at various noise and interference conditions, and show that speech enhancement degrades ASR under some conditions even for overlapping speech. Based on these findings, we propose a simple switching algorithm between observed and enhanced speech based on the estimated signal-to-interference ratio and signal-to-noise ratio. We demonstrated experimentally that such a simple switching mechanism can improve recognition performance when processing artifacts are detrimental to ASR",
    "checked": true,
    "id": "f5cfb670174746894bad943f364063c09468ae93",
    "semantic_title": "should we always separate?: switching between enhanced and observed signals for overlapping speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/udupa21_interspeech.html": {
    "title": "Estimating Articulatory Movements in Speech Production with Transformer Networks",
    "volume": "main",
    "abstract": "We estimate articulatory movements in speech production from different modalities - acoustics and phonemes. Acoustic-to-articulatory inversion (AAI) is a sequence-to-sequence task. On the other hand, phoneme to articulatory (PTA) motion estimation faces a key challenge in reliably aligning the text and the articulatory movements. To address this challenge, we explore the use of a transformer architecture â€” FastSpeech, with explicit duration modelling to learn hard alignments between the phonemes and articulatory movements. We also train a transformer model on AAI. We use correlation coefficient (CC) and root mean squared error (rMSE) to assess the estimation performance in comparison to existing methods on both tasks. We observe 154%, 11.8% & 4.8% relative improvement in CC with subject-dependent, pooled and fine-tuning strategies, respectively, for PTA estimation. Additionally, on the AAI task, we obtain 1.5%, 3% and 3.1% relative gain in CC on the same setups compared to the state-of-the-art baseline. We further present the computational benefits of having transformer architecture as representation blocks",
    "checked": true,
    "id": "19be83b770b96905288fa47dd882665a4c64bb45",
    "semantic_title": "estimating articulatory movements in speech production with transformer networks",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21b_interspeech.html": {
    "title": "Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "It is well known that the mismatch between training (source) and test (target) data distribution will significantly decrease the performance of acoustic scene classification (ASC) systems. To address this issue, domain adaptation (DA) is one solution and many unsupervised DA methods have been proposed. These methods focus on a scenario of single source domain to single target domain. However, we will face such problem that test data comes from multiple target domains. This problem can be addressed by producing one model per target domain, but this solution is too costly. In this paper, we propose a novel unsupervised multi-target domain adaption (MTDA) method for ASC, which can adapt to multiple target domains simultaneously and make use of the underlying relation among multiple domains. Specifically, our approach combines traditional adversarial adaptation with two novel discriminator tasks that learns a common subspace shared by all domains. Furthermore, we propose to divide the target domain into the easy-to-adapt and hard-to-adapt domain, which enables the system to pay more attention to hard-to-adapt domain in training. The experimental results on the DCASE 2020 Task 1-A dataset and the DCASE 2019 Task 1-B dataset show that our proposed method significantly outperforms the previous unsupervised DA methods",
    "checked": true,
    "id": "dfcb28fd454ade5de3773a995d15abb5c39083d5",
    "semantic_title": "unsupervised multi-target domain adaptation for acoustic scene classification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jaramillo21_interspeech.html": {
    "title": "Speech Decomposition Based on a Hybrid Speech Model and Optimal Segmentation",
    "volume": "main",
    "abstract": "In a hybrid speech model, both voiced and unvoiced components can coexist in a segment. Often, the voiced speech is regarded as the deterministic component, and the unvoiced speech and additive noise are the stochastic components. Typically, the speech signal is considered stationary within fixed segments of 20â€“40 ms, but the degree of stationarity varies over time. For decomposing noisy speech into its voiced and unvoiced components, a fixed segmentation may be too crude, and we here propose to adapt the segment length according to the signal local characteristics. The segmentation relies on parameter estimates of a hybrid speech model and the maximum a posteriori (MAP) and log-likelihood criteria as rules for model selection among the possible segment lengths, for voiced and unvoiced speech, respectively. Given the optimal segmentation markers and the estimated statistics, both components are estimated using linear filtering. A codebook-based approach differentiates between unvoiced speech and noise. A better extraction of the components is possible by taking into account the adaptive segmentation, compared to a fixed one. Also, a lower distortion for voiced speech and higher segSNR for both components is possible, as compared to other decomposition methods",
    "checked": true,
    "id": "d93c6ae0fa63530cf1470c366a5a2898c9887645",
    "semantic_title": "speech decomposition based on a hybrid speech model and optimal segmentation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21b_interspeech.html": {
    "title": "Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation",
    "volume": "main",
    "abstract": "Predicting the altered acoustic frames is an effective way of self-supervised learning for speech representation. However, it is challenging to prevent the pretrained model from overfitting. In this paper, we proposed to introduce two dropout regularization methods into the pretraining of transformer encoder: (1) attention dropout, (2) layer dropout. Both of the two dropout methods encourage the model to utilize global speech information, and avoid just copying local spectrum features when reconstructing the masked frames. We evaluated the proposed methods on phoneme classification and speaker recognition tasks. The experiments demonstrate that our dropout approaches achieve competitive results, and improve the performance of classification accuracy on downstream tasks",
    "checked": true,
    "id": "d918c11715bf8e24a81b4988916e8478c970deee",
    "semantic_title": "dropout regularization for self-supervised learning of transformer encoder speech representation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yarra21_interspeech.html": {
    "title": "Noise Robust Pitch Stylization Using Minimum Mean Absolute Error Criterion",
    "volume": "main",
    "abstract": "We propose a pitch stylization technique in the presence of pitch halving and doubling errors. The technique uses an optimization criterion based on a minimum mean absolute error to make the stylization robust to such pitch estimation errors, particularly under noisy conditions. We obtain segments for the stylization automatically using dynamic programming. Experiments are performed at the frame level and the syllable level. At the frame level, the closeness of stylized pitch is analyzed with the ground truth pitch, which is obtained using a laryngograph signal, considering root mean square error (RMSE) measure. At the syllable level, the effectiveness of perceptual relevant embeddings in the stylized pitch is analyzed by estimating syllabic tones and comparing those with manual tone markings using the Levenshtein distance measure. The proposed approach performs better than a minimum mean squared error criterion based pitch stylization scheme at the frame level and a knowledge-based tone estimation scheme at the syllable level under clean and 20dB, 10dB and 0dB SNR conditions with five noises and four pitch estimation techniques. Among all the combinations of SNR, noise and pitch estimation techniques, the highest absolute RMSE and mean distance improvements are found to be 6.49Hz and 0.23, respectively",
    "checked": true,
    "id": "9b5193f7fb14813d42b4572fa298325143ec4b7c",
    "semantic_title": "noise robust pitch stylization using minimum mean absolute error criterion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21b_interspeech.html": {
    "title": "An Attribute-Aligned Strategy for Learning Speech Representation",
    "volume": "main",
    "abstract": "Advancement in speech technology has brought convenience to our life. However, the concern is on the rise as speech signal contains multiple personal attributes, which would lead to either sensitive information leakage or bias toward decision. In this work, we propose an attribute-aligned learning strategy to derive speech representation that can flexibly address these issues by attribute-selection mechanism. Specifically, we propose a layered-representation variational autoencoder (LR-VAE), which factorizes speech representation into attribute-sensitive nodes, to derive an identity-free representation for speech emotion recognition (SER), and an emotionless representation for speaker verification (SV). Our proposed method achieves competitive performances on identity-free SER and a better performance on emotionless SV, comparing to the current state-of-the-art method of using adversarial learning applied on a large emotion corpora, the MSP-Podcast. Also, our proposed learning strategy reduces the model and training process needed to achieve multiple privacy-preserving tasks",
    "checked": true,
    "id": "54cd75e17690e051fcac3ee0503ea35be406efa5",
    "semantic_title": "an attribute-aligned strategy for learning speech representation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shahrebabaki21_interspeech.html": {
    "title": "Raw Speech-to-Articulatory Inversion by Temporal Filtering and Decimation",
    "volume": "main",
    "abstract": "We propose a novel sequence-to-sequence acoustic-to-articulatory inversion (AAI) neural architecture in the temporal waveform domain. In contrast to traditional AAI approaches that leverage hand-crafted short-time spectral features obtained from the windowed signal, such as LSFs, or MFCCs, our solution directly process the input speech signal in the time domain, avoiding any intermediate signal transformation, using a cascade of 1D convolutional filters in a deep model. The time-rate synchronization between raw speech signal and the articulatory signal is obtained through a decimation process that acts upon each convolution step. Decimation in time thus avoids degradation phenomena observed in the conventional AAI procedure, caused by the need of framing the speech signal to produce a feature sequence that perfectly matches the articulatory data rate. Experimental evidence on the \"Haskins Production Rate Comparison\" corpus demonstrates the effectiveness of the proposed solution, which outperforms a conventional state-of-the-art AAI system leveraging MFCCs with an 20% relative improvement in terms of Pearson correlation coefficient (PCC) in mismatched speaking rate conditions. Finally, the proposed approach attains the same accuracy as the conventional AAI solution in the typical matched speaking rate condition",
    "checked": true,
    "id": "e43b0f4bce2b66f3b01b3441ff1187400ed58b80",
    "semantic_title": "raw speech-to-articulatory inversion by temporal filtering and decimation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lilley21_interspeech.html": {
    "title": "Unsupervised Training of a DNN-Based Formant Tracker",
    "volume": "main",
    "abstract": "Phonetic analysis often requires reliable estimation of formants, but estimates provided by popular programs can be unreliable. Recently, Dissen et al. [1] described DNN-based formant trackers that produced more accurate frequency estimates than several others, but require manually-corrected formant data for training. Here we describe a novel unsupervised training method for corpus-based DNN formant parameter estimation and tracking with accuracy similar to [1]. Frame-wise spectral envelopes serve as the input. The output is estimates of the frequencies and bandwidths plus amplitude adjustments for a prespecified number of poles and zeros, hereafter referred to as \"formant parameters.\" A custom loss measure based on the difference between the input envelope and one generated from the estimated formant parameters is calculated and back-propagated through the network to establish the gradients with respect to the formant parameters. The approach is similar to that of autoencoders, in that the model is trained to reproduce its input in order to discover latent features, in this case, the formant parameters. Our results demonstrate that a reliable formant tracker can be constructed for a speech corpus without the need for hand-corrected training data",
    "checked": true,
    "id": "cd4b4e29143f293c8855ed8408bb8992811413ac",
    "semantic_title": "unsupervised training of a dnn-based formant tracker",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21c_interspeech.html": {
    "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a on large volumes of unlabeled data and achieves state-of-the-art (SOTA) However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL for its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized prediction heads on top of the model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing",
    "checked": true,
    "id": "7e386158f474a395618c5e065ac55844b507007c",
    "semantic_title": "superb: speech processing universal performance benchmark",
    "citation_count": 409
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21i_interspeech.html": {
    "title": "Synchronising Speech Segments with Musical Beats in Mandarin and English Singing",
    "volume": "main",
    "abstract": "Generating synthesised singing voice with models trained on speech data has many advantages due to the models' flexibility and controllability. However, since the information about the temporal relationship between segments and beats are lacking in speech training data, the synthesised singing may sound off-beat at times. Therefore, the availability of the information on the temporal relationship between speech segments and music beats is crucial. The current study investigated the segment-beat synchronisation in singing data, with hypotheses formed based on the linguistics theories of P-centre and sonority hierarchy. A Mandarin corpus and an English corpus of professional singing data were manually annotated and analysed. The results showed that the presence of musical beats was more dependent on segment duration than sonority. However, the sonority hierarchy and the P-centre theory were highly related to the location of beats. Mandarin and English demonstrated cross-linguistic variations despite exhibiting common patterns",
    "checked": true,
    "id": "ef7fae7c75735cee3cd6db7f7f19d7fb7f962d25",
    "semantic_title": "synchronising speech segments with musical beats in mandarin and english singing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peplinski21_interspeech.html": {
    "title": "FRILL: A Non-Semantic Speech Embedding for Mobile Devices",
    "volume": "main",
    "abstract": "Learned speech representations can drastically improve performance on tasks with limited labeled data. However, due to their size and complexity, learned representations have limited utility in mobile settings where run-time performance can be a significant bottleneck. In this work, we propose a class of lightweight non-semantic speech embedding models that run efficiently on mobile devices based on the recently proposed TRILL speech embedding. We combine novel architectural modifications with existing speed-up techniques to create embedding models that are fast enough to run in real-time on a mobile device and exhibit minimal performance degradation on a benchmark of non-semantic speech tasks. One such model (FRILL) is 32Ã— faster on a Pixel 1 smartphone and 40% the size of TRILL, with an average decrease in accuracy of only 2%. To our knowledge, FRILL is the highest-quality non-semantic embedding designed for use on mobile devices. Furthermore, we demonstrate that these representations are useful for mobile health tasks such as non-speech human sounds detection and face-masked speech detection. Our models and code are publicly available",
    "checked": true,
    "id": "32b31ee920fb2b3487ba1a019e9035fff3b29c2f",
    "semantic_title": "frill: a non-semantic speech embedding for mobile devices",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mori21_interspeech.html": {
    "title": "Pitch Contour Separation from Overlapping Speech",
    "volume": "main",
    "abstract": "In everyday conversation, speakers' utterances often overlap. For conversation corpora that are recorded in diverse environments, results of pitch extraction in the overlapping parts may be incorrect. The goal of this study is to establish the technique of separating each speaker's pitch contour from an overlapping speech in conversation. The proposed method estimates statistically most plausible f contour from the spectrogram of overlapping speech, along with the information of the speaker to extract. Visual inspection of the separation results showed that the proposed model was able to extract accurate f contours from overlapping speeches of specified speakers. By applying this method, voicing decision errors and gross pitch errors were reduced by 63% compared to simple pitch extraction for overlapping speech",
    "checked": true,
    "id": "354cdc8b1c315871765316d9d27d2811c7ff9324",
    "semantic_title": "pitch contour separation from overlapping speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21_interspeech.html": {
    "title": "Do Sound Event Representations Generalize to Other Audio Tasks? A Case Study in Audio Transfer Learning",
    "volume": "main",
    "abstract": "Transfer learning is critical for efficient information transfer across multiple related learning problems. A simple, yet effective transfer learning approach utilizes deep neural networks trained on a large-scale task for feature extraction. Such representations are then used to learn related downstream tasks. In this paper, we investigate transfer learning capacity of audio representations obtained from neural networks trained on a large-scale sound event detection dataset. We build and evaluate these representations across a wide range of other audio tasks, via a simple linear classifier transfer mechanism. We show that such simple linear transfer is already powerful enough to achieve high performance on the downstream tasks. We also provide insights into the attributes of sound event representations that enable such efficient information transfer",
    "checked": true,
    "id": "0ce8c803608c06e8dc6eb065b7b3b8afa48476b6",
    "semantic_title": "do sound event representations generalize to other audio tasks? a case study in audio transfer learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21b_interspeech.html": {
    "title": "Data Augmentation for Spoken Language Understanding via Pretrained Language Models",
    "volume": "main",
    "abstract": "The training of spoken language understanding (SLU) models often faces the problem of data scarcity. In this paper, we put forward a data augmentation method using pretrained language models to boost the variability and accuracy of generated utterances. Furthermore, we investigate and propose solutions to two previously overlooked semi-supervised learning scenarios of data scarcity in SLU: i) : ontology information with numerous valid dialogue acts is given; ii) : a large number of unlabelled utterances are available. Empirical results show that our method can produce synthetic training data that boosts the performance of language understanding models in various scenarios",
    "checked": true,
    "id": "a8ec7942f0a77bfb61a8c534ad951e66e2f94188",
    "semantic_title": "data augmentation for spoken language understanding via pretrained language models",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/radfar21_interspeech.html": {
    "title": "FANS: Fusing ASR and NLU for On-Device SLU",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) systems translate voice input commands to semantics which are encoded as an intent and pairs of slot tags and values. Most current SLU systems deploy a cascade of two neural models where the first one maps the input audio to a transcript (ASR) and the second predicts the intent and slots from the transcript (NLU). In this paper, we introduce FANS, a new end-to-end SLU model that fuses an ASR audio encoder to a multi-task NLU decoder to infer the intent, slot tags, and slot values directly from a given input audio, obviating the need for transcription. FANS consists of a shared audio encoder and three decoders, two of which are seq-to-seq decoders that predict non null slot tags and slot values in parallel and in an auto-regressive manner. FANS neural encoder and decoders architectures are flexible which allows us to leverage different combinations of LSTM, self-attention, and attenders. Our experiments show compared to the state-of-the-art end-to-end SLU models, FANS reduces ICER and IRER errors relatively by 30% and 7%, respectively, when tested on an in-house SLU dataset and by 0.86% and 2% absolute when tested on a public SLU dataset",
    "checked": true,
    "id": "a2e20c8691cfa357fcfd6eb853f3a2df0350ff13",
    "semantic_title": "fans: fusing asr and nlu for on-device slu",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cao21c_interspeech.html": {
    "title": "Sequential End-to-End Intent and Slot Label Classification and Localization",
    "volume": "main",
    "abstract": "Human-computer interaction (HCI) is significantly impacted by delayed responses from a spoken dialogue system. Hence, end-to-end (e2e) spoken language understanding (SLU) solutions have recently been proposed to decrease latency. Such approaches allow for the extraction of semantic information directly from the speech signal, thus bypassing the need for a transcript from an automatic speech recognition (ASR) system. In this paper, we propose a compact e2e SLU architecture for streaming scenarios, where chunks of the speech signal are processed continuously to predict intent and slot values. Our model is based on a 3D convolutional neural network (3D-CNN) and a unidirectional long short-term memory (LSTM). We compare the performance of two alignment-free losses: the connectionist temporal classification (CTC) method and its adapted version, namely connectionist temporal localization (CTL). The latter performs not only the classification but also localization of sequential audio events. The proposed solution is evaluated on the Fluent Speech Command dataset and results show our model ability to process incoming speech signal, reaching accuracy as high as 98.97% for CTC and 98.78% for CTL on single-label classification, and as high as 95.69% for CTC and 95.28% for CTL on two-label prediction",
    "checked": true,
    "id": "8c0f2b475fdf87257cd08e01cbee939cddf9ad46",
    "semantic_title": "sequential end-to-end intent and slot label classification and localization",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muralidharan21_interspeech.html": {
    "title": "DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants",
    "volume": "main",
    "abstract": "Named entity recognition (NER) is usually developed and tested on text from well-written sources. However, in intelligent voice assistants, where NER is an important component, input to NER may be noisy because of user or speech recognition error. In applications, entity labels may change frequently, and non-textual properties like topicality or popularity may be needed to choose among alternatives We describe a NER system intended to address these problems. We test and train this system on a proprietary user-derived dataset. We compare with a baseline text-only NER system; the baseline enhanced with external gazetteers; and the baseline enhanced with the search and indirect labelling techniques we describe below. The final configuration gives around 6% reduction in NER error rate. We also show that this technique improves related tasks, such as semantic parsing, with an improvement of up to 5% in error rate",
    "checked": true,
    "id": "5701a84ff58ca1ac1326408b00c605ebbeafb4bd",
    "semantic_title": "dexter: deep encoding of external knowledge for named entity recognition in virtual assistants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21d_interspeech.html": {
    "title": "A Context-Aware Hierarchical BERT Fusion Network for Multi-Turn Dialog Act Detection",
    "volume": "main",
    "abstract": "The success of interactive dialog systems is usually associated with the quality of the spoken language understanding (SLU) task, which mainly identifies the corresponding dialog acts and slot values in each turn. By treating utterances in isolation, most SLU systems often overlook the semantic context in which a dialog act is expected. The act dependency between turns is nontrivial and yet critical to the identification of the correct semantic representations. Previous works with limited context awareness have exposed the inadequacy of dealing with complexity in multiproned user intents, which are subject to spontaneous change during turn transitions. In this work, we propose to enhance SLU in multi-turn dialogs, employing a context-aware hierarchical BERT fusion Network (CaBERT-SLU) to not only discern context information within a dialog but also jointly identify multiple dialog acts and slots in each utterance. Experimental results show that our approach reaches new state-of-the-art (SOTA) performances in two complicated multi-turn dialogue datasets with considerable improvements compared with previous methods, which only consider single utterances for multiple intents and slot filling",
    "checked": true,
    "id": "077ca9d42ccaa8dd5c888fb7c1c413322cc6cb6f",
    "semantic_title": "a context-aware hierarchical bert fusion network for multi-turn dialog act detection",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21g_interspeech.html": {
    "title": "Pre-Training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning",
    "volume": "main",
    "abstract": "In the traditional cascading architecture for spoken language understanding (SLU), it has been observed that automatic speech recognition errors could be detrimental to the performance of natural language understanding. End-to-end (E2E) SLU models have been proposed to directly map speech input to desired semantic frame with a single model, hence mitigating ASR error propagation. Recently, pre-training technologies have been explored for these E2E models. In this paper, we propose a novel joint textual-phonetic pre-training approach for learning spoken language representations, aiming at exploring the full potentials of phonetic information to improve SLU robustness to ASR errors. We explore phoneme labels as high-level speech features, and design and compare pre-training tasks based on conditional masked language model objectives and inter-sentence relation objectives. We also investigate the efficacy of combining textual and phonetic information during fine-tuning. Experimental results on spoken language understanding benchmarks, Fluent Speech Commands and SNIPS, show that the proposed approach significantly outperforms strong baseline models and improves robustness of spoken language understanding to ASR errors",
    "checked": true,
    "id": "75c4a2682efaf830729885f835f623c257085d21",
    "semantic_title": "pre-training for spoken language understanding with joint textual and phonetic representation learning",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/do21b_interspeech.html": {
    "title": "Predicting Temporal Performance Drop of Deployed Production Spoken Language Understanding Models",
    "volume": "main",
    "abstract": "In deployed real-world spoken language understanding (SLU) applications, data continuously flows into the system. This leads to distributional differences between training and application data that can deteriorate model performance. While regularly retraining the deployed model with new data helps mitigating this problem, it implies significant computational and human costs. In this paper, we develop a method, which can help guiding decisions on whether a model is safe to keep in production without notable performance loss or needs to be retrained. Towards this goal, we build a performance drop regression model for an SLU model that was trained offline to detect a potential model drift in the production phase. We present a wide range of experiments on multiple real-world datasets, indicating that our method is useful for guiding decisions in the SLU model development cycle and to reduce costs for model retraining",
    "checked": true,
    "id": "f997aa03d561583512a02495349174febd218a86",
    "semantic_title": "predicting temporal performance drop of deployed production spoken language understanding models",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ganhotra21_interspeech.html": {
    "title": "Integrating Dialog History into End-to-End Spoken Language Understanding Systems",
    "volume": "main",
    "abstract": "End-to-end spoken language understanding (SLU) systems that process human-human or human-computer interactions are often context independent and process each turn of a conversation independently. Spoken conversations on the other hand, are very much context dependent, and dialog history contains useful information that can improve the processing of each conversational turn. In this paper, we investigate the importance of dialog history and how it can be effectively integrated into end-to-end SLU systems. While processing a spoken utterance, our proposed RNN transducer (RNN-T) based SLU model has access to its dialog history in the form of decoded transcripts and SLU labels of previous turns. We encode the dialog history as BERT embeddings, and use them as an additional input to the SLU model along with the speech features for the current utterance. We evaluate our approach on a recently released spoken dialog data set, the HarperValleyBank corpus. We observe significant improvements: 8% for dialog action and 30% for caller intent recognition tasks, in comparison to a competitive context independent end-to-end baseline system",
    "checked": true,
    "id": "c4f5d9b7ff6ac2f5b015363f30b0c8d940a6f852",
    "semantic_title": "integrating dialog history into end-to-end spoken language understanding systems",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21_interspeech.html": {
    "title": "Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State Tracking",
    "volume": "main",
    "abstract": "Dialogue State Tracking (DST), which is the process of inferring user goals by estimating belief states given the dialogue history, plays a critical role in task-oriented dialogue systems. A coreference phenomenon observed in multi-turn conversations is not addressed by existing DST models, leading to suboptimal performances. In this paper, we propose Coreference Dialogue State Tracker (CDST) that explicitly models the coreference feature. In particular, at each turn, the proposed model jointly predicts the coreferred domain-slot pair and extracts the coreference values from the dialogue context. Experimental results on MultiWOZ 2.1 dataset show that the proposed model achieves the state-of-the-art joint goal accuracy of 56.47%",
    "checked": true,
    "id": "467717080eaf85fae09e5c6b937bc9777a4566d2",
    "semantic_title": "coreference augmentation for multi-domain task-oriented dialogue state tracking",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/arora21_interspeech.html": {
    "title": "Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding",
    "volume": "main",
    "abstract": "Decomposable tasks are complex and comprise of a hierarchy of sub-tasks. Spoken intent prediction, for example, combines automatic speech recognition and natural language understanding. Existing benchmarks, however, typically hold out examples for only the surface-level sub-task. As a result, models with similar performance on these benchmarks may have unobserved performance differences on the other sub-tasks. To allow insightful comparisons between competitive end-to-end architectures, we propose a framework to construct robust test sets using coordinate ascent over sub-task specific utility functions. Given a dataset for a decomposable task, our method optimally creates a test set for each sub-task to individually assess sub-components of the end-to-end model. Using spoken language understanding as a case study, we generate new splits for the Fluent Speech Commands and Snips SmartLights datasets. Each split has two test sets: one with held-out utterances assessing natural language understanding abilities, and one with held-out speakers to test speech processing skills. Our splits identify performance gaps up to 10% between end-to-end systems that were within 1% of each other on the original test sets. These performance gaps allow more realistic and actionable comparisons between different architectures, driving future model development. We release our splits and tools for the community",
    "checked": true,
    "id": "4cc07c367e4a1f932e159678ef711e1802edf49f",
    "semantic_title": "rethinking end-to-end evaluation of decomposable tasks: a case study on spoken language understanding",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sun21b_interspeech.html": {
    "title": "Semantic Data Augmentation for End-to-End Mandarin Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end models have gradually become the preferred option for automatic speech recognition (ASR) applications. During the training of end-to-end ASR, data augmentation is a quite effective technique for regularizing the neural networks. This paper proposes a novel data augmentation technique based on semantic transposition of the transcriptions via syntax rules for end-to-end Mandarin ASR. Specifically, we first segment the transcriptions based on part-of-speech tags. Then transposition strategies, such as placing the object in front of the subject or swapping the subject and the object, are applied on the segmented sentences. Finally, the acoustic features corresponding to the transposed transcription are reassembled based on the audio-to-text forced-alignment produced by a pre-trained ASR system. The combination of original data and augmented one is used for training a new ASR system. The experiments are conducted on the Transformer[2] and Conformer[3] based ASR. The results show that the proposed method can give consistent performance gain to the system. Augmentation related issues, such as comparison of different strategies and ratios for data combination are also investigated",
    "checked": true,
    "id": "e5ea412a2dcc7e2c64334f817a56b81eb93b5c74",
    "semantic_title": "semantic data augmentation for end-to-end mandarin speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21c_interspeech.html": {
    "title": "Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition",
    "volume": "main",
    "abstract": "Accent variability has posed a huge challenge to automatic speech recognition (ASR) modeling. Although one-hot accent vector based adaptation systems are commonly used, they require prior knowledge about the target accent and cannot handle unseen accents. Furthermore, simply concatenating accent embeddings does not make good use of accent knowledge, which has limited improvements. In this work, we aim to tackle these problems with a novel layer-wise adaptation structure injected into the E2E ASR model encoder. The adapter layer encodes an arbitrary accent in the accent space and assists the ASR model in recognizing accented speech. Given an utterance, the adaptation structure extracts the corresponding accent information and transforms the input acoustic feature into an accent-related feature through the linear combination of all accent bases. We further explore the injection position of the adaptation layer, the number of accent bases, and different types of accent bases to achieve better accent adaptation. Experimental results show that the proposed adaptation structure brings 12% and 10% relative word error rate (WER) reduction on the AESRC2020 accent dataset and the Librispeech dataset, respectively, compared to the baseline",
    "checked": true,
    "id": "5ba5b6f5dcef625e6697021465d29a82a9b7fffb",
    "semantic_title": "layer-wise fast adaptation for end-to-end multi-accent speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21m_interspeech.html": {
    "title": "Low Resource German ASR with Untranscribed Data Spoken by Non-Native Children â€” INTERSPEECH 2021 Shared Task SPAPL System",
    "volume": "main",
    "abstract": "This paper describes the SPAPL system for the INTERSPEECH 2021 Challenge: Shared Task on Automatic Speech Recognition for Non-Native Children's Speech in German. ~5 hours of transcribed data and ~60 hours of untranscribed data are provided to develop a German ASR system for children. For the training of the transcribed data, we propose a non-speech state discriminative loss (NSDL) to mitigate the influence of long-duration non-speech segments within speech utterances. In order to explore the use of the untranscribed data, various approaches are implemented and combined together to incrementally improve the system performance. First, bidirectional autoregressive predictive coding (Bi-APC) is used to learn initial parameters for acoustic modelling using the provided untranscribed data. Second, incremental semi-supervised learning is further used to iteratively generate pseudo-transcribed data. Third, different data augmentation schemes are used at different training stages to increase the variability and size of the training data. Finally, a recurrent neural network language model (RNNLM) is used for rescoring. Our system achieves a word error rate (WER) of 39.68% on the evaluation data, an approximately 12% relative improvement over the official baseline (45.21%)",
    "checked": false,
    "id": "1c01a697307fcec11bfa6e6a6e5f018394385601",
    "semantic_title": "low resource german asr with untranscribed data spoken by non-native children - interspeech 2021 shared task spapl system",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sim21_interspeech.html": {
    "title": "Robust Continuous On-Device Personalization for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "On-device personalization of an all-neural automatic speech recognition (ASR) model can be achieved efficiently by fine-tuning the last few layers of the model. This approach has been shown to be effective for adapting the model to recognize rare named entities using only a small amount of data. To reliably perform continuous on-device learning, it is important for the training process to be completely autonomous without manual intervention. Our simulation studies show that training over many rounds may eventually lead to a significant model drift if the personalized model is indiscriminately accepted at the end of each training round. It is important to have appropriate acceptance criteria in place to guard the model against drifting. Moreover, for storage efficiency, it is desirable to persist the model weights in quantized form. We found that quantizing and dequantizing the model weights in between training rounds can prevent the model from learning effectively. This issue can be circumvented by adding noise to the quantized weights at the start of each training round",
    "checked": true,
    "id": "8b8b7b20108a650df42feddf2be181ab9d69d8b8",
    "semantic_title": "robust continuous on-device personalization for automatic speech recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21b_interspeech.html": {
    "title": "Speaker Normalization Using Joint Variational Autoencoder",
    "volume": "main",
    "abstract": "Speaker adaptation is known to provide significant improvement in speech recognition accuracy. However, in practical scenario, only a few seconds of audio is available due to which it may be infeasible to apply speaker adaptation methods such as i-vector and fMLLR robustly. Also, decoding with fMLLR transformation happens in two-passes which is impractical for real-time applications. In recent past, mapping speech features from speaker independent (SI) space to fMLLR normalized space using denoising autoencoder (DA) has been explored. To the best of our knowledge, such mapping generally does not yield consistent improvement. In this paper, we show that our proposed joint VAE based mapping achieves a large improvements over ASR models trained using filterbank SI features. We also show that joint VAE outperforms DA by a large margin. We observe a relative improvement of 17% in word error rate (WER) compared to ASR model trained using filterbank features with i-vectors and 23% without i-vectors",
    "checked": true,
    "id": "6dfcee22813c749870d20d19fc17296dabc5cd74",
    "semantic_title": "speaker normalization using joint variational autoencoder",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21c_interspeech.html": {
    "title": "The TAL System for the INTERSPEECH2021 Shared Task on Automatic Speech Recognition for Non-Native Childrens Speech",
    "volume": "main",
    "abstract": "This paper describes TAL's system for the INTERSPEECH 2021 shared task on Automatic Speech Recognition (ASR) for non-native children's speech. In this work, we attempt to apply the self-supervised approach to non-native German children's ASR. First, we conduct some baseline experiments to indicate that self-supervised learning can capture more acoustic information on non-native children's speech. Then, we apply the 11-fold data augmentation and combine it with data clean-up to supplement to the limited training data. Moreover, an in-domain semi-supervised VAD model is utilized to segment untranscribed audio. These strategies can significantly improve the system performance. Furthermore, we use two types of language models to further improve performance, i.e., a 4-gram LM with CTC beam-search and a Transformer LM for 2-pass rescoring. Our ASR system reduces the Word Error Rate (WER) by about 48% relatively in comparison with the baseline, achieving 1st in the evaluation period with the WER of 23.5%",
    "checked": true,
    "id": "eab196bf9bdaf7b6042ad2c75cbc9c3563284b9b",
    "semantic_title": "the tal system for the interspeech2021 shared task on automatic speech recognition for non-native childrens speech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lam21b_interspeech.html": {
    "title": "On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR",
    "volume": "main",
    "abstract": "We propose an on-the-fly data augmentation method for automatic speech recognition (ASR) that uses alignment information to generate effective training samples. Our method, called Aligned Data Augmentation (ADA) for ASR, replaces transcribed tokens and the speech representations in an aligned manner to generate previously unseen training pairs. The speech representations are sampled from an audio dictionary that has been extracted from the training corpus and inject speaker variations into the training examples. The transcribed tokens are either predicted by a language model such that the augmented data pairs are semantically close to the original data, or randomly sampled. Both strategies result in training pairs that improve robustness in ASR training. Our experiments on a Seq-to-Seq architecture show that ADA can be applied on top of SpecAugment, and achieves about 9â€“23% and 4â€“15% relative improvements in WER over SpecAugment alone on LibriSpeech 100h and LibriSpeech 960h test datasets, respectively",
    "checked": true,
    "id": "1f814fb7aef27f74bbb164a0dbbc97a6d5837b6a",
    "semantic_title": "on-the-fly aligned data augmentation for sequence-to-sequence asr",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21_interspeech.html": {
    "title": "Zero-Shot Cross-Lingual Phonetic Recognition with External Language Embedding",
    "volume": "main",
    "abstract": "Many existing languages are too sparsely resourced for monolingual deep learning networks to achieve high accuracy. Multilingual phonetic recognition systems mitigate data sparsity issues by training models on data from multiple languages and learning a speech-to-phone or speech-to-text model universal to all languages. However, despite their good performance on the seen training languages, multilingual systems have poor performance on unseen languages. This paper argues that in the real world, even an unseen language has metadata: linguists can tell us the language name, its language family and, usually, its phoneme inventory. Even with no transcribed speech, it is possible to train a language embedding using only data from language typologies (phylogenetic node and phoneme inventory) that reduces ASR error rates. Experiments on a 20-language corpus show that our methods achieve phonetic token error rate (PTER) reduction on all the unseen test languages. An ablation study shows that using the wrong language embedding usually harms PTER if the two languages are from different language families. However, even the wrong language embedding often improves PTER if the language embedding belongs to another member of the same language family",
    "checked": true,
    "id": "7f2f850e4f4cca313d984d8c71557b96c8bd12fc",
    "semantic_title": "zero-shot cross-lingual phonetic recognition with external language embedding",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21c_interspeech.html": {
    "title": "Rapid Speaker Adaptation for Conformer Transducer: Attention and Bias Are All You Need",
    "volume": "main",
    "abstract": "Conformer transducer achieves new state-of-the-art end-to-end (E2E) system performance and has become increasingly appealing for production. In this paper, we study how to effectively perform rapid speaker adaptation in a conformer transducer and how it compares with the RNN transducer. We hierarchically decompose the conformer transducer and compare adapting each component through fine-tuning. Among various interesting observations, there are three distinct findings: First, adapting the self-attention can achieve more than 80% gain of the full network adaptation. When the adaptation data is extremely scarce, attention is all you need to adapt. Second, within the self-attention, adapting the value projection outperforms adapting the key or the query projection. Lastly, bias adaptation, despite of its compact parameter space, is surprisingly effective. We conduct experiments on a state-of-the-art conformer transducer for an email dictation task. With 3 to 5 min source speech and 200 minute personalized TTS speech, the best performing encoder and joint network adaptation yields 38.37% and 19.90% relative word error rate (WER) reduction. Combining the attention and bias adaptation can achieve 90% of the gain with significantly smaller footprint. Further comparison with the RNN-T suggests the new state-of-the-art conformer transducer can benefit as much as if not more from personalization",
    "checked": true,
    "id": "39108c8127b90392a5338d64c8cd5d179a3f1723",
    "semantic_title": "rapid speaker adaptation for conformer transducer: attention and bias are all you need",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/das21b_interspeech.html": {
    "title": "Best of Both Worlds: Robust Accented Speech Recognition with Adversarial Transfer Learning",
    "volume": "main",
    "abstract": "Training deep neural networks for automatic speech recognition (ASR) requires large amounts of transcribed speech. This becomes a bottleneck for training robust models for speech which typically contains high variability in pronunciation and other semantics, since obtaining large amounts of annotated accented data is both tedious and costly. Often, we only have access to large amounts of speech from different accents. In this work, we leverage this unannotated data to provide semantic regularization to an ASR model that has been trained only on one accent, to improve its performance for multiple accents. We propose Accent Pre-Training (Acc-PT), a semi-supervised training strategy that combines transfer learning and adversarial training. Our approach improves the performance of a state-of-the-art ASR model by 33% on average over the baseline across multiple accents, training only on annotated samples from one standard accent, and as little as 105 minutes of speech from a target accent",
    "checked": true,
    "id": "5807b5abac23b658b003583f9b02eec4d4cf5daf",
    "semantic_title": "best of both worlds: robust accented speech recognition with adversarial transfer learning",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chu21_interspeech.html": {
    "title": "Extending Pronunciation Dictionary with Automatically Detected Word Mispronunciations to Improve PAII's System for Interspeech 2021 Non-Native Child English Close Track ASR Challenge",
    "volume": "main",
    "abstract": "This paper proposed to automatically detect mispronounced words over the regions that have low Goodness-of-Pronunciation scores through a constrained phone decoder, then add these word mispronunciations into the orthodox lexicon without colliding with existing pronunciations, finally use the expanded lexicon for decoding non-native speech. The constrained phone decoder is compiled by using a phone-level automatically generated one-edit-distance network to eliminate the need of extended recognition networks designed by phonologists. Results and analysis have shown that the pronunciation dictionary extension is effective in improving WER performance for non-native speech recognition. This paper also described the details of PAII's single-pass fusion-free hybrid system for this Interspeech 2021 non-native children English close track ASR challenge, especially showed the effective use of non-speech segments in the training set as noise sources to perform noise augmentation on the training data, and also conducted a comparison of acoustic models with different neural network architectures with analysis. Final WERs of 12.10%/28.25% are obtained compared to a well-optimized baseline with WERs of 13.37%/33.51% on development/evaluation set, respectively",
    "checked": true,
    "id": "b85f49e35890ad7022524c22d844cb0a8973613c",
    "semantic_title": "extending pronunciation dictionary with automatically detected word mispronunciations to improve paii's system for interspeech 2021 non-native child english close track asr challenge",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21d_interspeech.html": {
    "title": "CVC: Contrastive Learning for Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "Cycle consistent generative adversarial network (CycleGAN) and variational autoencoder (VAE) based models have gained popularity in non-parallel voice conversion recently. However, they often suffer from difficult training process and unsatisfactory results. In this paper, we propose a contrastive learning-based adversarial approach for voice conversion, namely contrastive voice conversion (CVC). Compared to previous CycleGAN-based methods, CVC only requires an efficient one-way GAN training by taking the advantage of contrastive learning. When it comes to non-parallel one-to-one voice conversion, CVC is on par or better than CycleGAN and VAE while effectively reducing training time. CVC further demonstrates superior performance in many-to-one voice conversion, enabling the conversion from unseen speakers",
    "checked": true,
    "id": "bda26246400f9e8a63ead1b8272051391570fc21",
    "semantic_title": "cvc: contrastive learning for non-parallel voice conversion",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21d_interspeech.html": {
    "title": "A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion",
    "volume": "main",
    "abstract": "We propose a new paradigm for maintaining speaker identity in dysarthric voice conversion (DVC). The poor quality of dysarthric speech can be greatly improved by statistical VC, but as the normal speech utterances of a dysarthria patient are nearly impossible to collect, previous work failed to recover the individuality of the patient. In light of this, we suggest a novel, two-stage approach for DVC, which is highly flexible in that no normal speech of the patient is required. First, a powerful parallel sequence-to-sequence model converts the input dysarthric speech into a normal speech of a reference speaker as an intermediate product, and a nonparallel, frame-wise VC model realized with a variational autoencoder then converts the speaker identity of the reference speech back to that of the patient while assumed to be capable of preserving the enhanced quality. We investigate several design options. Experimental evaluation results demonstrate the potential of our approach to improving the quality of the dysarthric speech while maintaining the speaker identity",
    "checked": true,
    "id": "28140c67f5d01a825c709e2c7fe13eccbae24d6c",
    "semantic_title": "a preliminary study of a two-stage paradigm for preserving speaker identity in dysarthric voice conversion",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eskimez21_interspeech.html": {
    "title": "One-Shot Voice Conversion with Speaker-Agnostic StarGAN",
    "volume": "main",
    "abstract": "In this work, we propose a variant of STARGAN for many-to-many voice conversion (VC) conditioned on the d-vectors for short-duration (2â€“15 seconds) speech. We make several modifications to the STARGAN training and employ new network architectures. We employ a transformer encoder in the discriminator network, and we apply the discriminator loss to the cycle consistency and identity samples in addition to the generated (fake) samples. Instead of classifying the samples as either real or fake, our discriminator tries to predict the categorical speaker class, where a fake class is added for the generated samples. Furthermore, we employ a reverse gradient layer after the generator's encoder and use an auxiliary classifier to remove the speaker's information from the encoded representation. We show that our method yields better results than the baseline method in objective and subjective evaluations in terms of voice conversion quality. Moreover, we provide an ablation study and show each component's influence on speaker similarity",
    "checked": true,
    "id": "6d4dae597c4bc7d546880f81c7c9d672297892b6",
    "semantic_title": "one-shot voice conversion with speaker-agnostic stargan",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/koshizuka21_interspeech.html": {
    "title": "Fine-Tuning Pre-Trained Voice Conversion Model for Adding New Target Speakers with Limited Data",
    "volume": "main",
    "abstract": "Voice conversion (VC) is a technique that converts speaker-dependent non-linguistic information into that of another speaker, while retaining the linguistic information of the input speech. A typical VC system comprises two modules: an encoder module that removes speaker individuality from the input speech and a decoder module that incorporates another speaker's individuality in synthesized speech. This paper proposes a training method for a vocoder-free any-to-many encoder-decoder VC model with limited data. Various pre-training techniques have been proposed to solve problems training to limited training data; some of these techniques employ the text-to-speech (TTS) task for pre-training. We pre-train the decoder module in the voice conversion task for growing our pre-training technique into continuously adding target speakers to the VC system. The experimental results show that good conversion performance can be achieved by conducting VC-based pre-training. We also confirmed that the rehearsal and pseudo-rehearsal methods can effectively fine-tune the model without degrading the conversion performance of the pre-trained target speakers",
    "checked": true,
    "id": "3bb29f61118abbeed3419a01e88d1426d6f599b4",
    "semantic_title": "fine-tuning pre-trained voice conversion model for adding new target speakers with limited data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21n_interspeech.html": {
    "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion",
    "volume": "main",
    "abstract": "One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement. Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance. To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner. Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics. In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems. Our code, pre-trained models and demo are publicly available",
    "checked": true,
    "id": "2f01cabbee57e1083f3d4499f112bb220dda69a4",
    "semantic_title": "vqmivc: vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion",
    "citation_count": 57
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21e_interspeech.html": {
    "title": "StarGANv2-VC: A Diverse, Unsupervised, Non-Parallel Framework for Natural-Sounding Voice Conversion",
    "volume": "main",
    "abstract": "We present an unsupervised non-parallel many-to-many voice conversion (VC) method using a generative adversarial network (GAN) called StarGAN v2. Using a combination of adversarial source classifier loss and perceptual loss, our model significantly outperforms previous VC models. Although our model is trained only with 20 English speakers, it generalizes to a variety of voice conversion tasks, such as any-to-many, cross-lingual, and singing conversion. Using a style encoder, our framework can also convert plain reading speech into stylistic speech, such as emotional and falsetto speech. Subjective and objective evaluation experiments on a non-parallel many-to-many voice conversion task revealed that our model produces natural sounding voices, close to the sound quality of state-of-the-art text-to-speech (TTS) based voice conversion methods without the need for text labels. Moreover, our model is completely convolutional and with a faster-than-real-time vocoder such as Parallel WaveGAN can perform real-time voice conversion",
    "checked": true,
    "id": "a58b237fe398f81fa3385c53bd341c0f4d05f3fe",
    "semantic_title": "starganv2-vc: a diverse, unsupervised, non-parallel framework for natural-sounding voice conversion",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21c_interspeech.html": {
    "title": "Normalization Driven Zero-Shot Multi-Speaker Speech Synthesis",
    "volume": "main",
    "abstract": "In this paper, we present a novel zero-shot multi-speaker speech synthesis approach (ZSM-SS) that leverages the normalization architecture and speaker encoder with non-autoregressive multi-head attention driven encoder-decoder architecture. Given an input text and a reference speech sample of an unseen person, ZSM-SS can generate speech in that person's style in a zero-shot manner. Additionally, we demonstrate how the affine parameters of normalization help in capturing the prosodic features such as energy and fundamental frequency in a disentangled fashion and can be used to generate morphed speech output. We demonstrate the efficacy of our proposed architecture on multi-speaker VCTK[1] and LibriTTS [2] datasets, using multiple quantitative metrics that measure generated speech distortion and MOS, along with speaker embedding analysis of the proposed speaker encoder model",
    "checked": true,
    "id": "87d7dedb6475f879b82d940652eff32eb0844bb7",
    "semantic_title": "normalization driven zero-shot multi-speaker speech synthesis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sakamoto21_interspeech.html": {
    "title": "StarGAN-VC+ASR: StarGAN-Based Non-Parallel Voice Conversion Regularized by Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Preserving the linguistic content of input speech is essential during voice conversion (VC). The star generative adversarial network-based VC method (StarGAN-VC) is a recently developed method that allows non-parallel many-to-many VC. Although this method is powerful, it can fail to preserve the linguistic content of input speech when the number of available training samples is extremely small. To overcome this problem, we propose the use of automatic speech recognition to assist model training, to improve StarGAN-VC, especially in low-resource scenarios. Experimental results show that using our proposed method, StarGAN-VC can retain more linguistic information than vanilla StarGAN-VC",
    "checked": true,
    "id": "fcac9eb041a58246edd7cdeb4014cd156c47fe1f",
    "semantic_title": "stargan-vc+asr: stargan-based non-parallel voice conversion regularized by automatic speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21d_interspeech.html": {
    "title": "Two-Pathway Style Embedding for Arbitrary Voice Conversion",
    "volume": "main",
    "abstract": "Arbitrary voice conversion, also referred to as zero-shot voice conversion, has recently attracted increased attention in the literature. Although disentangling the linguistic and style representations for acoustic features is an effective way to achieve zero-shot voice conversion, the problem of how to convert to a natural speaker style is challenging because of the intrinsic variabilities of speech and the difficulties of completely decoupling them. For this reason, in this paper, we propose a Two-Pathway Style Embedding Voice Conversion framework (TPSE-VC) for realistic and natural speech conversion. The novel feature of this method is to simultaneously embed sentence-level and phoneme-level style information. A novel attention mechanism is proposed to implement the implicit alignment for timbre style and phoneme content, further embedding a phoneme-level style representation. In addition, we consider embedding the complete set of time steps of audio style into a fixed-length vector to obtain the sentence-level style representation. Moreover, TPSEVC does not require any pre-trained models, and is only trained with non-parallel speech data. Experimental results demonstrate that the proposed TPSE-VC outperforms the state-of-the-art results on zero-shot voice conversion",
    "checked": true,
    "id": "d82ee934406903ee206a2c184587cbc124b4e68d",
    "semantic_title": "two-pathway style embedding for arbitrary voice conversion",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21c_interspeech.html": {
    "title": "Non-Parallel Any-to-Many Voice Conversion by Replacing Speaker Statistics",
    "volume": "main",
    "abstract": "This paper proposes a non-parallel any-to-many voice conversion (VC) approach with a novel statistics replacement layer. Non-parallel VC is usually achieved by firstly disentangling linguistic and speaker representations, and then concatenating the linguistic content with the learned target speaker's embedding at the conversion stage. While such a concatenation-based approach could introduce speaker-specific characteristics into the network, it is not very effective as it entirely relies on the network to learn to combine the linguistic content and the speaker characteristics. Inspired by X-vectors, where the statistics of hidden representation such as means and standard deviations are used for speaker differentiation, we propose a statistics replacement layer in VC systems to directly modify the hidden states to have the target speaker's statistics. The speaker-specific statistics of hidden states are learned for each target speaker during training and are used as guidance for the statistics replacement layer during inference. Moreover, to better concentrate the speaker information into the statistics of hidden representation, a multitask training with X-vector based speaker classification is also performed. Experimental results with Librispeech and VCTK datasets show that the proposed method can effectively improve the converted speech's naturalness and similarity",
    "checked": true,
    "id": "2cb242c2cfe1e3b18f8f25bc324d3867a0364adc",
    "semantic_title": "non-parallel any-to-many voice conversion by replacing speaker statistics",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21c_interspeech.html": {
    "title": "Cross-Lingual Voice Conversion with a Cycle Consistency Loss on Linguistic Representation",
    "volume": "main",
    "abstract": "Cross-Lingual Voice Conversion (XVC) aims to modify a source speaker identity towards a target while preserving the source linguistic content. This paper introduces a cycle consistency loss on linguistic representation to ensure the speech content unchanged after conversion. The proposed XVC model consists of two loss functions during optimization: a spectral reconstruction loss and a linguistic cycle consistency loss. The cycle consistency loss seeks to maintain the source speech's linguistic content. Specifically, we utilize Phonetic PosteriorGram (PPG) to represent the linguistic content. XVC experiments were conducted between English and Mandarin. Both objective and subjective evaluations demonstrated that with the proposed cycle consistency loss, converted speech is more intelligible",
    "checked": true,
    "id": "e7e5e4f34731286e9bc34ad14bcfb6b82472e81f",
    "semantic_title": "cross-lingual voice conversion with a cycle consistency loss on linguistic representation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/du21_interspeech.html": {
    "title": "Improving Robustness of One-Shot Voice Conversion with Deep Discriminative Speaker Encoder",
    "volume": "main",
    "abstract": "One-shot voice conversion has received significant attention since only one utterance from source speaker and target speaker respectively is required. Moreover, source speaker and target speaker do not need to be seen during training. However, available one-shot voice conversion approaches are not stable for unseen speakers as the speaker embedding extracted from one utterance of an unseen speaker is not reliable. In this paper, we propose a deep discriminative speaker encoder to extract speaker embedding from one utterance more effectively. Specifically, the speaker encoder first integrates residual network and squeeze-and-excitation network to extract discriminative speaker information in frame level by modeling frame-wise and channel-wise interdependence in features. Then attention mechanism is introduced to further emphasize speaker related information via assigning different weights to frame level speaker information. Finally a statistic pooling layer is used to aggregate weighted frame level speaker information to form utterance level speaker embedding. The experimental results demonstrate that our proposed speaker encoder can improve the robustness of one-shot voice conversion for unseen speakers and outperforms baseline systems in terms of speech quality and speaker similarity",
    "checked": true,
    "id": "2bdabff0c7c94d8e17324626391640c66b1c307a",
    "semantic_title": "improving robustness of one-shot voice conversion with deep discriminative speaker encoder",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/white21_interspeech.html": {
    "title": "Optimizing an Automatic Creaky Voice Detection Method for Australian English Speaking Females",
    "volume": "main",
    "abstract": "Creaky voice is a nonmodal phonation type that has various linguistic and sociolinguistic functions. Manually annotating creaky voice for phonetic analysis is time-consuming and labor-intensive. In recent years, automatic tools for detecting creaky voice have been proposed, which present the possibility for easier, faster and more consistent creak identification. One of these proposed tools is a Creak Detector algorithm that uses an automatic neural network taking its input from several acoustic cues to identify creaky voice. Previous work has suggested that the creak probability threshold at which this tool determines an instance to be creaky may vary depending on the speaker population. The present study investigates the optimal creak detection threshold for female Australian English speakers Results show further support for the practice of first finding the optimal threshold when using the Creak Detection algorithm on new data sets. Additionally, results show that accuracy of creaky voice detection using the Creak Detection algorithm can be significantly improved by excluding non-sonorant data",
    "checked": true,
    "id": "ba1dbe846f1cd196f994f383ea4a94ec1a0b5b51",
    "semantic_title": "optimizing an automatic creaky voice detection method for australian english speaking females",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/penney21_interspeech.html": {
    "title": "A Comparison of Acoustic Correlates of Voice Quality Across Different Recording Devices: A Cautionary Tale",
    "volume": "main",
    "abstract": "There has been a recent increase in speech research utilizing data recorded with participants' personal devices, particularly in light of the COVID-19 pandemic and restrictions on face-to-face interactions. This raises important questions about whether these recordings are comparable to those made in traditional lab-based settings. Some previous studies have compared the viability of recordings made with personal devices for the clinical evaluation of voice quality. However, these studies rely on simple statistical analyses and do not examine acoustic correlates of voice quality typically examined in the (socio-) phonetic literature (e.g. H1-H2). In this study, we compare recordings from a set of smartphones/laptops and a solid-state recorder to assess the reliability of a range of acoustic correlates of voice quality. The results show significant differences for many acoustic measures of voice quality across devices. Further exploratory analyses demonstrate that these differences are not simple offsets, but rather that their magnitude depends on the value of the measurement of interest. We therefore urge researchers to exercise caution when examining voice quality based on recordings made with participants' devices, particularly when interested in small effect sizes. We also call on the speech research community to investigate these issues more thoroughly",
    "checked": true,
    "id": "9ab8e9d363f5a978f0f3f62578f43e6196c81fc9",
    "semantic_title": "a comparison of acoustic correlates of voice quality across different recording devices: a cautionary tale",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sfakianaki21_interspeech.html": {
    "title": "Investigating Voice Function Characteristics of Greek Speakers with Hearing Loss Using Automatic Glottal Source Feature Extraction",
    "volume": "main",
    "abstract": "The current study investigates voice quality characteristics of Greek adults with normal hearing and hearing loss, automatically obtained from glottal inverse filtering analysis using the Aalto Aparat toolkit. Aalto Aparat has been employed in glottal flow analysis of disordered speech, but to the best of the authors' knowledge, not as yet in hearing impaired voice analysis and assessment. Five speakers, three women and two men, with normal hearing (NH) and five speakers with prelingual profound hearing impairment (HI), matched for age and sex, produced symmetrical /ËˆpVpV/ disyllables, where V=/i, a, u/. A state-of-the-art method named quasi-closed phase analysis (QCP) is offered in Aparat and it is used to estimate the glottal source signal. Glottal source features were obtained using time- and frequency-domain parametrization methods and analysed statistically. The interpretation of the results attempts to shed light on potential differences between HI and NH phonation strategies, while advantages and limitations of inverse filtering methods in HI voice assessment are discussed",
    "checked": true,
    "id": "7bf515d839e495fdc91c04093ec2efe8de84b30f",
    "semantic_title": "investigating voice function characteristics of greek speakers with hearing loss using automatic glottal source feature extraction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huckvale21_interspeech.html": {
    "title": "Automated Detection of Voice Disorder in the SaarbrÃ¼cken Voice Database: Effects of Pathology Subset and Audio Materials",
    "volume": "main",
    "abstract": "The SaarbrÃ¼cken Voice Database contains speech and simultaneous electroglottography recordings of 1002 speakers exhibiting a wide range of voice disorders, together with recordings of 851 controls. Previous studies have used this database to build systems for automated detection of voice disorders and for differential diagnosis. These studies have varied considerably in the subset of pathologies tested, the audio materials analyzed, the cross-validation method used and the performance metric reported. This variation has made it hard to determine the most promising approaches to the problem of detecting voice disorders. In this study we re-implement three recently published systems that have been trained to detect pathology using the SVD and compare their performance on the same pathologies with the same audio materials using a common cross-validation protocol and performance metric. We show that under this approach, there is much less difference in performance across systems than in their original publication. We also show that voice disorder detection on the basis of a short phrase gives similar performance to that based on a sequence of vowels of different pitch. Our evaluation protocol may be useful for future studies on voice disorder detection with the SVD",
    "checked": true,
    "id": "b3b5b914d7914a140c60b2850fd47039cba7bfc8",
    "semantic_title": "automated detection of voice disorder in the saarbrÃ¼cken voice database: effects of pathology subset and audio materials",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lulich21_interspeech.html": {
    "title": "Accelerometer-Based Measurements of Voice Quality in Children During Semi-Occluded Vocal Tract Exercise with a Narrow Straw in Air",
    "volume": "main",
    "abstract": "Non-invasive measures of voice quality, such as H1-H2, rely on oral flow signals, inverse filtered speech signals, or corrections for the effects of formants. Voice quality measures play especially important roles in the assessment of voice disorders and the evaluation of treatment efficacy. One type of treatment that is increasingly common in voice therapy, as well as in voice training for singers and actors, is semi-occluded vocal tract exercises (SOVTEs). The goal of SOVTEs is to change patterns of vocal fold vibration and thereby improve voice quality and vocal efficiency. Accelerometers applied to the skin of the neck have been used to investigate subglottal acoustics, to inverse-filter speech signals, and to obtain voice quality metrics. This paper explores the application of neck-skin accelerometers to measure voice quality without oral flow, inverse filtering, or formant correction. Accelerometer-based measures (uncorrected K1-K2 and corrected K1*-K2*, analogous to microphone-based H1-H2 and H1*-H2*) were obtained from typically developing children with healthy voice, before and during SOVTEs. Traditional microphone-based H1-H2 measures (corrected and uncorrected) were also obtained. Results showed that K1-K2 and K1*-K2* were not substantially affected by vocal tract acoustic changes in formant frequencies",
    "checked": true,
    "id": "93a2ec748931748a5adbc56289d32acf2214a23f",
    "semantic_title": "accelerometer-based measurements of voice quality in children during semi-occluded vocal tract exercise with a narrow straw in air",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/perez21_interspeech.html": {
    "title": "Articulatory Coordination for Speech Motor Tracking in Huntington Disease",
    "volume": "main",
    "abstract": "Huntington Disease (HD) is a progressive disorder which often manifests in motor impairment. Motor severity (captured via motor score) is a key component in assessing overall HD severity. However, motor score evaluation involves in-clinic visits with a trained medical professional, which are expensive and not always accessible. Speech analysis provides an attractive avenue for tracking HD severity because speech is easy to collect remotely and provides insight into motor changes. HD speech is typically characterized as having irregular articulation. With this in mind, acoustic features that can capture vocal tract movement and articulatory coordination are particularly promising for characterizing motor symptom progression in HD. In this paper, we present an experiment that uses Vocal Tract Coordination (VTC) features extracted from read speech to estimate a motor score. When using an elastic-net regression model, we find that VTC features significantly outperform other acoustic features across varied-length audio segments, which highlights the effectiveness of these features for both short- and long-form reading tasks. Lastly, we analyze the F-value scores of VTC features to visualize which channels are most related to motor score. This work enables future research efforts to consider VTC features for acoustic analyses which target HD motor symptomatology tracking",
    "checked": true,
    "id": "8c7156e57b36b09416590a6e1c94d1b46fa7e6e7",
    "semantic_title": "articulatory coordination for speech motor tracking in huntington disease",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ferrer21_interspeech.html": {
    "title": "Modeling Dysphonia Severity as a Function of Roughness and Breathiness Ratings in the GRBAS Scale",
    "volume": "main",
    "abstract": "Dysphonia comprises many perceptually deviating aspects of voice, and its overall severity perception is made by the listener according to methods of aggregating the single dimensions which are personally conceived and not well studied. Roughness and breathiness are constituent dimensions in most devised rating scales in clinical use. In this paper, we evaluate several ways to model the mapping of the overall severity as a function of the particular ratings of roughness and breathiness. The models include the simple linear averaging as well as several non-linear variants suggested elsewhere, and some minor adjustments. The models are evaluated on four datasets from different countries, allowing a more global evaluation of how the mapping is conceived Results show the limitations of the most widely assumed linear approach, while also hinting at a need for a more uniform coverage of the sample space in voice pathology datasets. The models explored in this paper can be expanded to higher-dimensional scales",
    "checked": true,
    "id": "bf399a85062282b9bc9d6eb3a91d31258463fdab",
    "semantic_title": "modeling dysphonia severity as a function of roughness and breathiness ratings in the grbas scale",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karpov21_interspeech.html": {
    "title": "Golos: Russian Dataset for Speech Research",
    "volume": "main",
    "abstract": "This paper introduces a novel Russian speech dataset called Golos, a large corpus suitable for speech research. The dataset mainly consists of recorded audio files manually annotated on the crowd-sourcing platform. The total duration of the audio is about 1240 hours. We have made the corpus freely available to download, along with the acoustic model with CTC loss prepared on this corpus. Additionally, transfer learning was applied to improve the performance of the acoustic model. In order to evaluate the quality of the dataset with the beam-search algorithm, we have built a 3-gram language model on the open Common Crawl dataset. The total word error rate (WER) metrics turned out to be about 3.3% and 11.5%",
    "checked": true,
    "id": "39dd1c778673976fdc7e9b8158068e1ac5b6edcc",
    "semantic_title": "golos: russian dataset for speech research",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sadhu21b_interspeech.html": {
    "title": "Radically Old Way of Computing Spectra: Applications in End-to-End ASR",
    "volume": "main",
    "abstract": "We propose a technique to compute spectrograms using Frequency Domain Linear Prediction (FDLP) that uses all-pole models to fit the squared Hilbert envelope of speech in different frequency sub-bands. The spectrogram of a complete speech utterance is computed by overlap-add of contiguous all-pole model responses. A long context window of 1.5 seconds allows us to capture the low frequency temporal modulations of speech in the spectrogram. For an end-to-end automatic speech recognition task, the FDLP spectrogram performs on par with the standard mel spectrogram features for clean read speech training and test data. For more realistic speech data with train-test domain mismatches or reverberations, FDLP spectrogram shows up to 25% and 22% relative WER improvements over mel spectrogram respectively",
    "checked": true,
    "id": "22895f07cbcbde21d115ebb744edf230ee7a7e18",
    "semantic_title": "radically old way of computing spectra: applications in end-to-end asr",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alghezi21_interspeech.html": {
    "title": "Self-Supervised End-to-End ASR for Low Resource L2 Swedish",
    "volume": "main",
    "abstract": "Unlike traditional (hybrid) Automatic Speech Recognition (ASR), end-to-end ASR systems simplify the training procedure by directly mapping acoustic features to sequences of graphemes or characters, thereby eliminating the need for specialized acoustic, language, or pronunciation models. However, one drawback of end-to-end ASR systems is that they require more training data than conventional ASR systems to achieve similar word error rate (WER). This makes it difficult to develop ASR systems for tasks where transcribed target data is limited such as developing ASR for Second Language (L2) speakers of Swedish. Nonetheless, recent advancements in self-supervised acoustic learning, manifested in wav2vec models [1, 2, 3], leverage the available untranscribed speech data to provide compact acoustic representation that can achieve low WER when incorporated in end-to-end systems. To this end, we experiment with several monolingual and cross-lingual self-supervised acoustic models to develop end-to-end ASR system for L2 Swedish. Even though our test is very small, it indicates that these systems are competitive in performance with traditional ASR pipeline. Our best model seems to reduce the WER by 7% relative to our traditional ASR baseline trained on the same target data",
    "checked": true,
    "id": "ee2bb24439ee90a58b0ff28d95b5568903d7ee96",
    "semantic_title": "self-supervised end-to-end asr for low resource l2 swedish",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oneill21_interspeech.html": {
    "title": "SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "In the English speech-to-text (STT) machine learning task, acoustic models are conventionally trained on uncased Latin characters, and any necessary orthography (such as capitalization, punctuation, and denormalization of non-standard words) is imputed by separate post-processing models. This adds complexity and limits performance, as many formatting tasks benefit from semantic information present in the acoustic signal but absent in transcription. Here we propose a new STT task: end-to-end neural transcription with fully formatted text for target labels. We present baseline Conformer-based models trained on a corpus of 5,000 hours of professionally transcribed earnings calls, achieving a CER of 1.7. As a contribution to the STT research community, we release the corpus free for non-commercial use",
    "checked": false,
    "id": "ca201db9980e49647feedf39eb30b19f074bf68a",
    "semantic_title": "spgispeech: 5, 000 hours of transcribed financial audio for fully formatted end-to-end speech recognition",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2021/evain21_interspeech.html": {
    "title": "",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) using huge unlabeled data has been successfully explored for image and natural language processing. Recent works also investigated SSL from speech. They were notably successful to improve performance on downstream tasks such as automatic speech recognition (ASR). While these works suggest it is possible to reduce dependence on labeled data for building efficient speech systems, their evaluation was mostly made on ASR and using multiple and heterogeneous experimental settings (most of them for English). This questions the objective comparison of SSL approaches and the evaluation of their impact on building speech systems. In this paper, we propose : a reproducible framework for assessing SSL from speech. It not only includes ASR (high and low resource) tasks but also spoken language understanding, speech translation and emotion recognition. We also focus on speech technologies in a language different than English: French. SSL models of different sizes are trained from carefully sourced and documented datasets. Experiments show that SSL is beneficial for most but not all tasks which confirms the need for exhaustive and reliable benchmarks to evaluate its real impact is shared with the scientific community for reproducible research in SSL from speech",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sturm21_interspeech.html": {
    "title": "Prosodic Accommodation in Face-to-Face and Telephone Dialogues",
    "volume": "main",
    "abstract": "The study of phonetic accommodation in various communicative situations is still relatively limited. This paper examines accommodation in spontaneous conversations of eight pairs of Czech young male speakers in two communicative conditions: unconstrained face-to-face conversation and goal-oriented interaction via mobile telephone. Articulation rate and measures of f0 level, range and variability were measured in 40 prosodic phrases per speaker in each condition. Analyses of LME models did not reveal a significant global effect of time throughout the interaction on the distance between speakers (convergence) in any of the examined parameters, or that of preceding phrase value on the subsequent turn-initial value (synchrony). However, more consistent patterns were observed when speaker pairs were examined separately, revealing substantial individual variation on the one hand and non-linear effects on the other. This shows that aggregate analyses can be misleading in the study of phonetic accommodation and that speakers dynamically employ different strategies throughout natural conversations",
    "checked": true,
    "id": "e40dcddbfc5d832a6a40b3b4b94e43aeae978542",
    "semantic_title": "prosodic accommodation in face-to-face and telephone dialogues",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/riverincoutlee21_interspeech.html": {
    "title": "Dialect Features in Heterogeneous and Homogeneous Gheg Speaking Communities",
    "volume": "main",
    "abstract": "This apparent and real time study analyses how dialect features in the speech of children and adults are differently affected depending on whether they live in homogeneous or heterogeneous speech communities. The general hypotheses are that speakers in such high contact settings as heterogeneous urban centers are more prone to innovation than speakers in homogeneous tightly-knit communities, and that children accelerate leveling, especially through schooling and socialization. This study is of Gheg Albanian, a dialect spoken in and around the capital Tirana. Two features were investigated: rounding of /a/ and vowel length contrasts. Two groups of adults and children were compared: one from Tirana and one from a nearby village. Additionally, the children were recorded twice over a period of 12 months and were compared longitudinally. The results showed that length contrasts were still present in both communities and age groups. Rounding of /a/ was lost in the city, but undergoing change in the village, with differences measured in apparent time, but also in child speech within the 12-month span. Our study further raises the issue of combining both apparent and real time data within the same design",
    "checked": true,
    "id": "4a3c29dd913bc82d23e1ccfc8ef45cfddf19bb3c",
    "semantic_title": "dialect features in heterogeneous and homogeneous gheg speaking communities",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zellers21_interspeech.html": {
    "title": "An Exploration of the Acoustic Space of Rhotics and Laterals in Ruruuli",
    "volume": "main",
    "abstract": "Liquid consonants â€” rhotics and laterals â€” have been shown to demonstrate unique distributional patterns cross-linguistically. It is also claimed that rhotics are more difficult to distinguish from one another phonetically than laterals, and that rhotics are less flexible than laterals when it comes to participation in consonant clusters and coarticulatory patterns. We investigate the phonetic realization of the rhotic and lateral phonemes in a Bantu language, Ruruuli. The acoustic space used for rhotics and laterals in this language is extremely similar, although the density peaks in terms of formant values are different. Formant values as well as formant ratios can be reliably used to distinguish between rhotics and laterals. In common with many other languages, an asymmetry between laterals and rhotics is found in Ruruuli, with laterals being more positionally constrained than rhotics. The overlap in acoustic space between rhotics and laterals may cast doubt on the status or stability of the phonological contrast between rhotics and laterals in this language",
    "checked": true,
    "id": "11da444cbc3dbee5a8399f61d4bc8f0a130f6051",
    "semantic_title": "an exploration of the acoustic space of rhotics and laterals in ruruuli",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bodur21_interspeech.html": {
    "title": "Domain-Initial Strengthening in Turkish: Acoustic Cues to Prosodic Hierarchy in Stop Consonants",
    "volume": "main",
    "abstract": "Studies have shown that cross-linguistically, consonants at the left edge of higher-level prosodic boundaries tend to be more forcefully articulated than those at lower-level boundaries, a phenomenon known as This study tests whether similar effects occur in Turkish, using the Autosegmental-Metrical model proposed by Ipek & Jun [1, 2] as the basis for assessing boundary strength. Productions of /t/ and /d/ were elicited in four domain-initial prosodic positions corresponding to progressively higher-level boundaries: syllable, word, intermediate phrase, and Intonational Phrase. A fifth position, nuclear word, was included in order to better situate it within the prosodic hierarchy. Acoustic correlates of articulatory strength were measured, including closure duration for /d/ and /t/, as well as voice onset time and burst energy for /t/. Our results show that closure duration increases cumulatively from syllable to intermediate phrase, while voice onset time and burst energy are not influenced by boundary strength. These findings provide corroborating evidence for Ipek & Jun's model, particularly for the distinction between word and intermediate phrase boundaries. Additionally, articulatory strength at the left edge of the nuclear word patterned closely with word-initial position, supporting the view that the nuclear word is not associated with a distinct phrasing domain",
    "checked": true,
    "id": "430cb5fe510e2597b337975f86414b013c972ad3",
    "semantic_title": "domain-initial strengthening in turkish: acoustic cues to prosodic hierarchy in stop consonants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zmolikova21_interspeech.html": {
    "title": "Auxiliary Loss Function for Target Speech Extraction and Recognition with Weak Supervision Based on Speaker Characteristics",
    "volume": "main",
    "abstract": "Automatic speech recognition systems deteriorate in presence of overlapped speech. A popular approach to alleviate this is target speech extraction. The extraction system is usually trained with a loss function measuring the discrepancy between the estimated and the reference target speech. This often leads to distortions to the target signal which is detrimental to the recognition accuracy. Additionally, it is necessary to have the strong supervision provided by parallel data consisting of speech mixtures and single-speaker signals. We propose an auxiliary loss function for retraining the target speech extraction. It is composed of two parts: first, a speaker identity loss, forcing the estimated speech to have correct speaker characteristics, and second, a mixture consistency loss, making the extracted sources sum back to the original mixture. The only supervision required for the proposed loss is speaker characteristics obtained from several segments spoken by the target speaker. Such weak supervision makes the loss suitable for adapting the system directly on real recordings. We show that the proposed loss yields signals more suitable for speech recognition and further, we can gain additional improvements by adaptation to target data. Overall, we can reduce the word error rate on LibriCSS dataset from 27.4% to 24.0%",
    "checked": true,
    "id": "c994372b3c33bbc1ad6b504c5efb5afd515a5009",
    "semantic_title": "auxiliary loss function for target speech extraction and recognition with weak supervision based on speaker characteristics",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/borsdorf21_interspeech.html": {
    "title": "Universal Speaker Extraction in the Presence and Absence of Target Speakers for Speech of One and Two Talkers",
    "volume": "main",
    "abstract": "Speaker extraction has been studied mostly for the scenarios where a target speaker is present in a two or more talkers mixture. Such scenarios do not adequately reflect everyday conversations. For example, a target speaker can be the only active talker, be quiet for a while, or leave the conversation, that means the target speaker is absent from the mixture. Traditional speaker extraction models fail in these scenarios. We propose a novel speaker extraction approach to handle speech mixtures with one or two talkers in which the target speaker can either be present or absent. First, we formulate four speaker extraction conditions to cover the typical scenarios of everyday conversations with one and two talkers. Second, we introduce a joint training scheme with one unified loss function that works for all four conditions. We show that only a small amount of data is required to adapt the model to work well in the four conditions",
    "checked": true,
    "id": "a83c9b63f54b5f8b644f9cac37575a80e972ef21",
    "semantic_title": "universal speaker extraction in the presence and absence of target speakers for speech of one and two talkers",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mateju21_interspeech.html": {
    "title": "Using X-Vectors for Speech Activity Detection in Broadcast Streams",
    "volume": "main",
    "abstract": "A new approach to speech activity detection (SAD) is presented in this work. It allows us to reduce the complexity and computation demands, namely in services that process streaming speech, where a SAD module usually forms the first block of the data pipeline (e.g., in a platform for 24/7 broadcast transcription). Our approach utilizes x-vectors as input features so that, within the subsequent pipeline stages, these embedding instances can also directly be employed for speaker diarization and recognition. The x-vectors are extracted by feed-forward sequential memory network (FSMN), allowing for modeling long-time dependencies; they thus form an input into a computationally undemanding binary classifier, whose output is smoothed by a decoder. Evaluation is performed on the standardized QUT-NOISE-TIMIT dataset as well as on broadcast data with large portions of music and background noise. The former data allows for comparison with other existing approaches. The latter shows the performance in terms of word error rate (WER) and reduction in real-time factor (RTF) of the transcription process",
    "checked": true,
    "id": "c1360ea0059b055a23a18476f99d8c43b9aaf78e",
    "semantic_title": "using x-vectors for speech activity detection in broadcast streams",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/salvati21_interspeech.html": {
    "title": "Time Delay Estimation for Speaker Localization Using CNN-Based Parametrized GCC-PHAT Features",
    "volume": "main",
    "abstract": "We propose a time delay estimation (TDE) method for speaker localization based on parametrized generalized cross-correlation phase transform (PGCC-PHAT) functions and convolutional neural networks (CNNs). The PGCC-PHAT is used to build a feature matrix, which gives TDE information of two microphone signals with different normalization levels in the cross-correlation functions. The feature matrix is processed by a CNN, composed by several convolutional layers and fully connected layers and by a regression output for the directly estimation of the time difference of arrival (TDOA). Simulations in noisy and reverberant adverse conditions show that the proposed method improves the TDOA estimation performance if compared to the GCC-PHAT",
    "checked": true,
    "id": "b60084f373ee49e5da24e3fde7171087337f3d89",
    "semantic_title": "time delay estimation for speaker localization using cnn-based parametrized gcc-phat features",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yousefi21_interspeech.html": {
    "title": "Real-Time Speaker Counting in a Cocktail Party Scenario Using Attention-Guided Convolutional Neural Network",
    "volume": "main",
    "abstract": "Most current speech technology systems are designed to operate well even in the presence of multiple active speakers. However, most solutions assume that the number of co-current speakers is known. Unfortunately, this information might not always be available in real-world applications. In this study, we propose a real-time, single-channel attention-guided Convolutional Neural Network (CNN) to estimate the number of active speakers in overlapping speech. The proposed system extracts higher-level information from the speech spectral content using a CNN model. Next, the attention mechanism summarizes the extracted information into a compact feature vector without losing critical information. Finally, the active speakers are classified using a fully connected network. Experiments on simulated overlapping speech using WSJ corpus show that the attention solution is shown to improve the performance by almost 3% absolute over conventional temporal average pooling. The proposed Attention-guided CNN achieves 76.15% for both Weighted Accuracy and average Recall, and 75.80% Precision on speech segments as short as 20 frames (i.e., 200 ms). All the classification metrics exceed 92% for the attention-guided model in offline scenarios where the input signal is more than 100 frames long (i.e., 1s)",
    "checked": true,
    "id": "9c71e80624ad66b9c7905301f1a2194fd4a1b2b8",
    "semantic_title": "real-time speaker counting in a cocktail party scenario using attention-guided convolutional neural network",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21d_interspeech.html": {
    "title": "End-to-End Language Diarization for Bilingual Code-Switching Speech",
    "volume": "main",
    "abstract": "We propose two end-to-end neural configurations for language diarization on bilingual code-switching speech. The first, a BLSTM-E2E architecture, includes a set of stacked bidirectional LSTMs to compute embeddings and incorporates the deep clustering loss to enforce grouping of languages belonging to the same class. The second, an XSA-E2E architecture, is based on an x-vector model followed by a self-attention encoder. The former encodes frame-level features into segment-level embeddings while the latter considers all those embeddings to generate a sequence of segment-level language labels. We evaluated the proposed methods on the dataset obtained from the shared task B in WSTCSMC 2020 and our handcrafted simulated data from the SEAME dataset. Experimental results show that our proposed XSA-E2E architecture achieved a relative improvement of 12.1% in equal error rate and a 7.4% relative improvement on accuracy compared with the baseline algorithm in the WSTCSMC 2020 dataset. Our proposed XSA-E2E architecture achieved an accuracy of 89.84% with a baseline of 85.60% on the simulated data derived from the SEAME dataset",
    "checked": true,
    "id": "9f08e77a8c072dd3994879f450d9de730b6cfe43",
    "semantic_title": "end-to-end language diarization for bilingual code-switching speech",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2021/duroselle21_interspeech.html": {
    "title": "Modeling and Training Strategies for Language Recognition Systems",
    "volume": "main",
    "abstract": "Automatic speech recognition is complementary to language recognition. The language recognition systems exploit this complementarity by using frame-level bottleneck features extracted from neural networks trained with a phone recognition task. Recent methods apply frame-level bottleneck features extracted from an end-to-end sequence-to-sequence speech recognition model. In this work, we study an integrated approach of the training of the speech recognition feature extractor and language recognition modules. We show that for both classical phone recognition and end-to-end sequence-to-sequence features, sequential training of the two modules is not the optimal strategy. The feature extractor can be improved by supervision with the language identification loss, either in a fine-tuning step or in a multi-task training framework. Besides, we notice that end-to-end sequence-to-sequence bottleneck features are on par with classical phone recognition bottleneck features without requiring a forced alignment of the signal with target tokens. However, for sequence-to-sequence, the architecture of the model seems to play an important role; the Conformer architectures leads to much better results than the conventional stacked DNNs approach; and can even be trained directly with the LID module in an end-to-end approach",
    "checked": true,
    "id": "a06ff7e5c112532b413f3065432235830fef9c08",
    "semantic_title": "modeling and training strategies for language recognition systems",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21o_interspeech.html": {
    "title": "A Weight Moving Average Based Alternate Decoupled Learning Algorithm for Long-Tailed Language Identification",
    "volume": "main",
    "abstract": "Language identification (LID) research has made tremendous progress in recent years, especially with the introduction of deep learning techniques. However, for real-world applications where the distribution of different language data is highly imbalanced, the performance of existing LID systems is still far from satisfactory. This raises the challenge of In this paper, we propose an effective weight moving average (WMA) based alternate decoupled learning algorithm, termed WADCL, for long-tailed LID. The system is divided into two components, a frontend feature extractor and a backend classifier. These are then alternately learned in an end-to-end manner using different sampling schemes to alleviate the distribution mismatch between training and test datasets. Furthermore, our WMA method aims to mitigate the side-effects of re-sampling schemes, by fusing the model parameters learned along the trajectory of stochastic gradient descent (SGD) optimization. To validate the effectiveness of the proposed WADCL algorithm, we evaluate and compare several systems over a language dataset constructed to match a long-tailed distribution based on real world application [1]. The experimental results from the long-tailed language dataset demonstrate that the proposed algorithm is able to achieve significant performance gains over existing state-of-the-art x-vector based LID methods",
    "checked": true,
    "id": "1d3eb72c47df9e19823a74e58d65c6776ef598df",
    "semantic_title": "a weight moving average based alternate decoupled learning algorithm for long-tailed language identification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21b_interspeech.html": {
    "title": "Improving Accent Identification and Accented Speech Recognition Under a Framework of Self-Supervised Learning",
    "volume": "main",
    "abstract": "Recently, self-supervised pre-training has gained success in automatic speech recognition (ASR). However, considering the difference between speech accents in real scenarios, how to identify accents and use accent features to improve ASR is still challenging. In this paper, we employ the self-supervised pre-training method for both accent identification and accented speech recognition tasks. For the former task, a standard deviation constraint loss (SDC-loss) based end-to-end (E2E) architecture is proposed to identify accents under the same language. As for accented speech recognition task, we design an accent-dependent ASR system, which can utilize additional accent input features. Furthermore, we propose a frame-level accent feature, which is extracted based on the proposed accent identification model and can be dynamically adjusted. We pre-train our models using 960 hours unlabeled LibriSpeech dataset and fine-tune them on AESRC2020 speech dataset. The experimental results show that our proposed accent-dependent ASR system is significantly ahead of the AESRC2020 baseline and achieves 6.5% relative word error rate (WER) reduction compared with our accent-independent ASR system",
    "checked": true,
    "id": "a5db43d09f18c8c59487dc81321ff4fd358468d4",
    "semantic_title": "improving accent identification and accented speech recognition under a framework of self-supervised learning",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fan21_interspeech.html": {
    "title": "Exploring wav2vec 2.0 on Speaker Verification and Language Identification",
    "volume": "main",
    "abstract": "wav2vec 2.0 is a recently proposed self-supervised framework for speech representation learning. It follows a two-stage training process of pre-training and fine-tuning, and performs well in speech recognition tasks especially ultra-low resource cases. In this work, we attempt to extend the self-supervised framework to speaker verification and language identification. First, we use some preliminary experiments to indicate that wav2vec 2.0 can capture the information about the speaker and language. Then we demonstrate the effectiveness of wav2vec 2.0 on the two tasks respectively. For speaker verification, we obtain a competitive result with the Equal Error Rate (EER) of 3.61% on the VoxCeleb1 dataset. For language identification, we obtain an EER of 12.02% on the 1 second condition and an EER of 3.47% on the full-length condition of the AP17-OLR dataset. Finally, we utilize one model to achieve the unified modeling by the multi-task learning for the two tasks",
    "checked": false,
    "id": "ffc09d24725d08956f6966ef79f994fa1d72d401",
    "semantic_title": "wav 2 vec 2 . 0 on speaker verification and language identification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ramesh21_interspeech.html": {
    "title": "Self-Supervised Phonotactic Representations for Language Identification",
    "volume": "main",
    "abstract": "Phonotactic constraints characterize the sequence of permissible phoneme structures in a language and hence form an important cue for language identification (LID) task. As phonotactic constraints span across multiple phonemes, the short-term spectral analysis (20â€“30 ms) alone is not sufficient to capture them. The speech signal has to be analyzed over longer contexts (100s of milliseconds) in order to extract features representing the phonotactic constraints. The supervised senone classifiers, aimed at modeling triphone context, have been used for extracting language-specific features for the LID task. However, it is difficult to get large amounts of manually labeled data to train the supervised models. In this work, we explore a self-supervised approach to extract long-term contextual features for the LID task. We have used wav2vec architecture to extract contextualized representations from multiple frames of the speech signal. The contextualized representations extracted from the pre-trained wav2vec model are used for the LID task. The performance of the proposed features is evaluated on a dataset containing 7 Indian languages. The proposed self-supervised embeddings achieved 23% absolute improvement over the acoustic features and 3% absolute improvement over their supervised counterparts",
    "checked": true,
    "id": "faa8fcc1b5343cc85881ff16e818c31596987f02",
    "semantic_title": "self-supervised phonotactic representations for language identification",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21j_interspeech.html": {
    "title": "E2E-Based Multi-Task Learning Approach to Joint Speech and Accent Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a single multi-task learning framework to perform End-to-End (E2E) speech recognition (ASR) and accent recognition (AR) simultaneously. The proposed framework is not only more compact but can also yield comparable or even better results than standalone systems. Specifically, we found that the overall performance is predominantly determined by the ASR task, and the E2E-based ASR pretraining is essential to achieve improved performance, particularly for the AR task. Additionally, we conduct several analyses of the proposed method. First, though the objective loss for the AR task is much smaller compared with its counterpart of ASR task, a smaller weighting factor with the AR task in the joint objective function is necessary to yield better results for each task. Second, we found that sharing only a few layers of the encoder yields better AR results than sharing the overall encoder. Experimentally, the proposed method produces WER results close to the best standalone E2E ASR ones, while it achieves 7.7% and 4.2% relative improvement over standalone and single-task-based joint recognition methods on test set for accent recognition respectively",
    "checked": true,
    "id": "1117dd82eec9f024ff6d3a3b72a909e194217822",
    "semantic_title": "e2e-based multi-task learning approach to joint speech and accent recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tzudir21_interspeech.html": {
    "title": "Excitation Source Feature Based Dialect Identification in Ao â€” A Low Resource Language",
    "volume": "main",
    "abstract": "Ao is an under-resourced Tibeto-Burman tonal language spoken in Nagaland, India. There are three distinct dialects of the language, namely, Chungli, Mongsen and Changki. The objective of dialect identification is to identify one dialect from the other within the same language family. The goal of this study is to ascertain the potential of excitation source features for automatic dialect identification in Ao. In this direction, Integrated Linear Prediction Residual (ILPR), an approximate representation of source signal, is explored. The log Mel spectrogram of ILPR ( ) signal is used to exploit the time-frequency characteristics of the excitation source. This work proposes attention based CNN-BiGRU architecture for automatic dialect identification tasks. Additionally, log Mel spectrogram ( ), extracted from the pre-emphasized speech signal, is used as a baseline method. The ( ) contains the vocal-tract characteristics of the speech signal. A significant performance improvement of (nearly) 6% accuracy is observed when the excitation source feature ( ) is combined with the vocal tract representation ( ). To analyse the effect of segment duration, dialect identification performance is reported for three different durations, viz., 1 sec, 3 sec and 6 sec. The effect of gender in dialect identification task for Ao is also studied in this work",
    "checked": false,
    "id": "9266103ac27c490009e60caaf2fee1aadaadb3bc",
    "semantic_title": "excitation source feature based dialect identification in ao - a low resource language",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/khare21_interspeech.html": {
    "title": "Low Resource ASR: The Surprising Effectiveness of High Resource Transliteration",
    "volume": "main",
    "abstract": "Cross-lingual transfer of knowledge from high-resource languages to low-resource languages is an important research problem in automatic speech recognition (ASR). We propose a new strategy of transfer learning by pretraining using large amounts of speech in the high-resource language but with its text transliterated to the target low-resource language. This simple mapping of scripts explicitly encourages increased sharing between the output spaces of both languages and is surprisingly effective even when the high-resource and low-resource languages are from unrelated language families. The utility of our proposed technique is more evident in very low-resource scenarios, where better initializations are more beneficial. We evaluate our technique on a transformer ASR architecture and the state-of-the-art wav2vec2.0 ASR architecture, with English as the high-resource language and six languages as low-resource targets. With access to 1 hour of target speech, we obtain relative WER reductions of up to 8.2% compared to existing transfer-learning approaches",
    "checked": true,
    "id": "40115c0e996d5a71f589407bd2deaee4b58e0ee2",
    "semantic_title": "low resource asr: the surprising effectiveness of high resource transliteration",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2021/feng21_interspeech.html": {
    "title": "Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation",
    "volume": "main",
    "abstract": "This paper tackles automatically discovering phone-like acoustic units (AUD) from unlabeled speech data. Past studies usually proposed single-step approaches. We propose a two-stage approach: the first stage learns a subword-discriminative feature representation, and the second stage applies clustering to the learned representation and obtains phone-like clusters as the discovered acoustic units. In the first stage, a recently proposed method in the task of unsupervised subword modeling is improved by replacing a monolingual out-of-domain (OOD) ASR system with a multilingual one to create a subword-discriminative representation that is more language-independent. In the second stage, segment-level k-means is adopted, and two methods to represent the variable-length speech segments as fixed-dimension feature vectors are compared. Experiments on a very low-resource Mboshi language corpus show that our approach outperforms state-of-the-art AUD in both normalized mutual information (NMI) and F-score. The multilingual ASR improved upon the monolingual ASR in providing OOD phone labels and in estimating the phone boundaries. A comparison of our systems with and without knowing the ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that the current approach can significantly benefit from improved phone boundary estimation",
    "checked": true,
    "id": "992bfc020bb91005efdfd25455f9de4872ef6c72",
    "semantic_title": "unsupervised acoustic unit discovery by leveraging a language-independent subword discriminative feature representation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kamper21_interspeech.html": {
    "title": "Towards Unsupervised Phone and Word Segmentation Using Self-Supervised Vector-Quantized Neural Networks",
    "volume": "main",
    "abstract": "We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized (VQ) neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized dynamic programming method generally performs best. While performance on individual tasks is only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate",
    "checked": true,
    "id": "0e8a0d7606ceff40a1111a26732d8bd97ce0b66a",
    "semantic_title": "towards unsupervised phone and word segmentation using self-supervised vector-quantized neural networks",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jiang21_interspeech.html": {
    "title": "Speech SimCLR: Combining Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Self-supervised visual pretraining has shown significant progress recently. Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semi-supervised learning on ImageNet. The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning. In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning. During training, Speech SimCLR applies augmentation on raw speech and its spectrogram. Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation. The proposed method achieved competitive results on speech emotion recognition and speech recognition",
    "checked": true,
    "id": "6f5fcdceec0f86b54ace93e30cdc497b5568b714",
    "semantic_title": "speech simclr: combining contrastive and reconstruction objective for self-supervised speech representation learning",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jacobs21_interspeech.html": {
    "title": "Multilingual Transfer of Acoustic Word Embeddings Improves When Training on Languages Related to the Target Zero-Resource Language",
    "volume": "main",
    "abstract": "Acoustic word embedding models map variable duration speech segments to fixed dimensional vectors, enabling efficient speech search and discovery. Previous work explored how embeddings can be obtained in zero-resource settings where no labelled data is available in the target language. The current best approach uses transfer learning: a single supervised multilingual model is trained using labelled data from multiple well-resourced languages and then applied to a target zero-resource language (without fine-tuning). However, it is still unclear how the specific choice of training languages affect downstream performance. Concretely, here we ask whether it is beneficial to use training languages related to the target. Using data from eleven languages spoken in Southern Africa, we experiment with adding data from different language families while controlling for the amount of data per language. In word discrimination and query-by-example search evaluations, we show that training on languages from the same family gives large improvements. Through finer-grained analysis, we show that training on even just a single related language gives the largest gain. We also find that adding data from unrelated languages generally doesn't hurt performance",
    "checked": true,
    "id": "5669e41490b0854a3dd7a1f74414bf1de382235d",
    "semantic_title": "multilingual transfer of acoustic word embeddings improves when training on languages related to the target zero-resource language",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/niekerk21_interspeech.html": {
    "title": "Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/takahashi21_interspeech.html": {
    "title": "Unsupervised Neural-Based Graph Clustering for Variable-Length Speech Representation Discovery of Zero-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maekaku21_interspeech.html": {
    "title": "Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cui21_interspeech.html": {
    "title": "Identifying Indicators of Vulnerability from Short Speech Segments Using Acoustic and Textual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dunbar21_interspeech.html": {
    "title": "The Zero Resource Speech Challenge 2021: Spoken Language Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gudur21_interspeech.html": {
    "title": "Zero-Shot Federated Learning with New Classes for Audio Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rouditchenko21_interspeech.html": {
    "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21b_interspeech.html": {
    "title": "N-Singer: A Non-Autoregressive Korean Singing Voice Synthesis System for Pronunciation Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maniati21_interspeech.html": {
    "title": "Cross-Lingual Low Resource Speaker Adaptation Using Phonological Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhan21_interspeech.html": {
    "title": "Improve Cross-Lingual Text-To-Speech Synthesis on Monolingual Corpora with Pitch Contour Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21d_interspeech.html": {
    "title": "Cross-Lingual Voice Conversion with Disentangled Universal Linguistic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21e_interspeech.html": {
    "title": "EfficientSing: A Chinese Singing Voice Synthesis System Using Duration-Free Acoustic Model and HiFi-GAN Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xin21_interspeech.html": {
    "title": "Cross-Lingual Speaker Adaptation Using Domain Adaptation and Speaker Consistency Loss for Text-To-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shang21_interspeech.html": {
    "title": "Incorporating Cross-Speaker Style Transfer for Multi-Language Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kesim21_interspeech.html": {
    "title": "Investigating Contributions of Speech and Facial Landmarks for Talking Head Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/si21b_interspeech.html": {
    "title": "Speech2Video: Cross-Modal Distillation for Speech to Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21c_interspeech.html": {
    "title": "NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21c_interspeech.html": {
    "title": "QISTA-Net-Audio: Audio Super-Resolution via Non-Convex â„“_q-Norm Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wen21_interspeech.html": {
    "title": "X-net: A Joint Scale Down and Scale Up Method for Voice Call",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21k_interspeech.html": {
    "title": "WSRGlow: A Glow-Based Waveform Generative Model for Audio Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yi21_interspeech.html": {
    "title": "Half-Truth: A Partially Fake Audio Detection Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chettri21_interspeech.html": {
    "title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheon21_interspeech.html": {
    "title": "Coded Speech Enhancement Using Neural Network-Based Vector-Quantized Residual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/drude21_interspeech.html": {
    "title": "Multi-Channel Opus Compression for Far-Field Automatic Speech Recognition with a Fixed Bitrate Budget",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/siegert21_interspeech.html": {
    "title": "Effects of Prosodic Variations on Accidental Triggers of a Commercial Voice Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gabrys21_interspeech.html": {
    "title": "Improving the Expressiveness of Neural Vocoding with Non-Affine Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/prajapati21_interspeech.html": {
    "title": "Voice Privacy Through x-Vector and CycleGAN-Based Anonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21d_interspeech.html": {
    "title": "A Two-Stage Approach to Speech Bandwidth Extension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/byun21_interspeech.html": {
    "title": "Development of a Psychoacoustic Loss Function for the Deep Neural Network (DNN)-Based Speech Coder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stoidis21_interspeech.html": {
    "title": "Protecting Gender and Identity with Disentangled Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/aldholmi21_interspeech.html": {
    "title": "Perception of Standard Arabic Synthetic Speech Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kishiyama21_interspeech.html": {
    "title": "The Influence of Parallel Processing on Illusory Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chingacham21_interspeech.html": {
    "title": "Exploring the Potential of Lexical Paraphrases for Mitigating Noise-Induced Comprehension Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/simantiraki21_interspeech.html": {
    "title": "SpeechAdjuster: A Tool for Investigating Listener Preferences and Speech Intelligibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saito21_interspeech.html": {
    "title": "VocalTurk: Exploring Feasibility of Crowdsourced Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21e_interspeech.html": {
    "title": "Effects of Aging and Age-Related Hearing Loss on Talker Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21l_interspeech.html": {
    "title": "Relationships Between Perceptual Distinctiveness, Articulatory Complexity and Functional Load in Speech Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/terblanche21_interspeech.html": {
    "title": "Human Spoofing Detection Performance on Degraded Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/einfeldt21_interspeech.html": {
    "title": "Reliable Estimates of Interpretable Cue Effects with Active Learning in Psycholinguistic Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21d_interspeech.html": {
    "title": "Towards the Explainability of Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zeng21_interspeech.html": {
    "title": "Primacy of Mouth over Eyes: Eye Movement Evidence from Audiovisual Mandarin Lexical Tones and Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ashihara21_interspeech.html": {
    "title": "Investigating the Impact of Spectral and Temporal Degradation on End-to-End Automatic Speech Recognition Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21c_interspeech.html": {
    "title": "Super-Human Performance in Online Low-Latency Recognition of Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/joshi21_interspeech.html": {
    "title": "Multiple Softmax Architecture for Streaming Multilingual End-to-End ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/le21_interspeech.html": {
    "title": "Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sainath21_interspeech.html": {
    "title": "An Efficient Streaming Non-Recurrent On-Device End-to-End Model with Improvements to Rare-Word Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21_interspeech.html": {
    "title": "Streaming Multi-Talker Speech Recognition with Joint Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/moriya21_interspeech.html": {
    "title": "Streaming End-to-End Speech Recognition for Hybrid RNN-T/Attention Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schwarz21_interspeech.html": {
    "title": "Improving RNN-T ASR Accuracy Using Context Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21e_interspeech.html": {
    "title": "HMM-Free Encoder Pre-Training for Streaming RNN Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cui21b_interspeech.html": {
    "title": "Reducing Exposure Bias in Training Recurrent Neural Network Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/doutre21_interspeech.html": {
    "title": "Bridging the Gap Between Streaming and Non-Streaming ASR Systems by Distilling Ensembles of CTC and RNN-T Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/audhkhasi21_interspeech.html": {
    "title": "Mixture Model Attention: Flexible Streaming and Non-Streaming Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/inaguma21_interspeech.html": {
    "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/moritz21_interspeech.html": {
    "title": "Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21d_interspeech.html": {
    "title": "Multi-Mode Transformer Transducer with Stochastic Future Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ren21_interspeech.html": {
    "title": "A Causal U-Net Based Neural Beamforming Network for Real-Time Multi-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21d_interspeech.html": {
    "title": "A Partitioned-Block Frequency-Domain Adaptive Kalman Filter for Stereophonic Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21p_interspeech.html": {
    "title": "Real-Time Independent Vector Analysis Using Semi-Supervised Nonnegative Matrix Factorization as a Source Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21b_interspeech.html": {
    "title": "Improving Channel Decorrelation for Multi-Channel Target Speech Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21f_interspeech.html": {
    "title": "Inplace Gated Convolutional Recurrent Neural Network for Dual-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/raj21_interspeech.html": {
    "title": "SRIB-LEAP Submission to Far-Field Multi-Channel Speech Enhancement Challenge for Video Conferencing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21_interspeech.html": {
    "title": "Real-Time Multi-Channel Speech Enhancement Based on Neural Network Masking with Attention Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ganapathy21_interspeech.html": {
    "title": "Uncovering the Acoustic Cues of COVID-19 Infection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fung21_interspeech.html": {
    "title": "Ethical and Technological Challenges of Conversational AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fohr21_interspeech.html": {
    "title": "BERT-Based Semantic Model for Rescoring N-Best Speech Recognition List",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/benes21_interspeech.html": {
    "title": "Text Augmentation for Language Models in High Error Recognition Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21b_interspeech.html": {
    "title": "On Sampling-Based Training Criteria for Neural Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pylkkonen21_interspeech.html": {
    "title": "Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cieri21_interspeech.html": {
    "title": "Using Games to Augment Corpora for Language Recognition and Confusability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fenu21_interspeech.html": {
    "title": "Fair Voice Biometrics: Impact of Demographic Imbalance on Group Fairness in Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21m_interspeech.html": {
    "title": "Knowledge Distillation from Multi-Modality to Single-Modality for Person Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/noe21_interspeech.html": {
    "title": "Adversarial Disentanglement of Speaker Representation for Attribute-Driven Privacy Preservation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/romana21_interspeech.html": {
    "title": "Automatically Detecting Errors and Disfluencies in Read Speech to Predict Cognitive Impairment in People with Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vaysse21_interspeech.html": {
    "title": "Automatic Extraction of Speech Rhythm Descriptors for Speech Intelligibility Assessment in the Context of Head and Neck Cancers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qi21b_interspeech.html": {
    "title": "Speech Disorder Classification Using Extended Factorized Hierarchical Variational Auto-Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mathad21_interspeech.html": {
    "title": "The Impact of Forced-Alignment Errors on Automatic Pronunciation Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/villatorotello21_interspeech.html": {
    "title": "Late Fusion of the Available Lexicon and Raw Waveform-Based Acoustic Modeling for Depression and Dementia Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shandiz21_interspeech.html": {
    "title": "Neural Speaker Embeddings for Ultrasound-Based Silent Speech Interfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lamba21_interspeech.html": {
    "title": "Cross-Modal Learning for Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cook21_interspeech.html": {
    "title": "A Psychology-Driven Computational Analysis of Political Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/santoso21_interspeech.html": {
    "title": "Speech Emotion Recognition Based on Attention Weight Correction Using Word-Level Confidence Measure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/silpachai21_interspeech.html": {
    "title": "Effects of Voice Type and Task on L2 Learners' Awareness of Pronunciation Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/menshikova21_interspeech.html": {
    "title": "Lexical Entrainment and Intra-Speaker Variability in Cooperative Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nasreen21_interspeech.html": {
    "title": "Detecting Alzheimer's Disease Using Interactional and Acoustic Features from Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kothare21_interspeech.html": {
    "title": "Investigating the Interplay Between Affective, Phonatory and Motoric Subsystems in Autism Spectrum Disorder Using a Multimodal Dialogue Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ishi21_interspeech.html": {
    "title": "Analysis of Eye Gaze Reasons and Gaze Aversions During Three-Party Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21e_interspeech.html": {
    "title": "Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21q_interspeech.html": {
    "title": "A Light-Weight Contextual Spelling Correction Model for Customizing Transducer-Based Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21_interspeech.html": {
    "title": "Incorporating External POS Tagger for Punctuation Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/papadourakis21_interspeech.html": {
    "title": "Phonetically Induced Subwords for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mansfield21_interspeech.html": {
    "title": "Revisiting Parity of Human vs. Machine Conversational Speech Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21f_interspeech.html": {
    "title": "Lookup-Table Recurrent Language Models for Long Tail Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/andresferrer21_interspeech.html": {
    "title": "Contextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21g_interspeech.html": {
    "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhao21_interspeech.html": {
    "title": "BART Based Semantic Correction for Mandarin Automatic Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dai21b_interspeech.html": {
    "title": "Class-Based Neural Network Language Model for Second-Pass Rescoring in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kurata21_interspeech.html": {
    "title": "Improving Customization of Neural Transducers by Mitigating Acoustic Mismatch of Synthesized Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saebi21_interspeech.html": {
    "title": "A Discriminative Entity-Aware Language Model for Virtual Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/namazifar21_interspeech.html": {
    "title": "Correcting Automated and Manual Speech Transcription Errors Using Warped Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21b_interspeech.html": {
    "title": "Dynamic Encoder Transducer: A Flexible Solution for Trading Off Accuracy for Latency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21n_interspeech.html": {
    "title": "Domain-Aware Self-Attention for Multi-Domain Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zeyer21_interspeech.html": {
    "title": "Librispeech Transducer Model with Internal Language Model Prior Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mavandadi21_interspeech.html": {
    "title": "A Deliberation-Based Joint Acoustic and Text Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tuske21_interspeech.html": {
    "title": "On the Limit of English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/an21_interspeech.html": {
    "title": "Deformable TDNN with Adaptive Receptive Fields for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/you21_interspeech.html": {
    "title": "SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leong21_interspeech.html": {
    "title": "Online Compressive Transformer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21e_interspeech.html": {
    "title": "End to End Transformer-Based Contextual Speech Recognition Based on Pointer Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karita21_interspeech.html": {
    "title": "A Comparative Study on Neural Architectures and Training Methods for Japanese Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hori21b_interspeech.html": {
    "title": "Advanced Long-Context End-to-End Speech Recognition Using Context-Expanded Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/haidar21_interspeech.html": {
    "title": "Transformer-Based ASR Incorporating Time-Reduction Layer and Fine-Tuning with Self-Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mahadeokar21_interspeech.html": {
    "title": "Flexi-Transducer: Optimizing Latency, Accuracy and Compute for Multi-Domain On-Device Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/falkowskigilski21_interspeech.html": {
    "title": "Difference in Perceived Speech Signal Quality Assessment Among Monolingual and Bilingual Teenage Students",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schymura21_interspeech.html": {
    "title": "PILOT: Introducing Transformers for Probabilistic Sound Event Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/togami21_interspeech.html": {
    "title": "Sound Source Localization with Majorization Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mittag21_interspeech.html": {
    "title": "NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/naderi21_interspeech.html": {
    "title": "Subjective Evaluation of Noise Suppression Algorithms in Crowdsourcing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/geng21_interspeech.html": {
    "title": "Reliable Intensity Vector Selection for Multi-Source Direction-of-Arrival Estimation Using a Single Acoustic Vector Sensor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yu21_interspeech.html": {
    "title": "MetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/toma21_interspeech.html": {
    "title": "CNN-Based Processing of Acoustic and Radio Frequency Signals for Speaker Localization from MAVs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/itoyama21_interspeech.html": {
    "title": "Assessment of von Mises-Bernoulli Deep Neural Network in Sound Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21g_interspeech.html": {
    "title": "Feature Fusion by Attention Networks for Robust DOA Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21f_interspeech.html": {
    "title": "Far-Field Speaker Localization and Adaptive GLMB Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/narayanaswamy21_interspeech.html": {
    "title": "On the Design of Deep Priors for Unsupervised Audio Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21h_interspeech.html": {
    "title": "CramÃ©r-Rao Lower Bound for DOA Estimation with an Array of Directional Microphones in Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/you21b_interspeech.html": {
    "title": "GAN Vocoder: Multi-Resolution Discriminator Is All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cong21_interspeech.html": {
    "title": "Glow-WaveGAN: Learning Speech Representations from GAN-Based Variational Auto-Encoder for High Fidelity Flow-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yoneyama21_interspeech.html": {
    "title": "Unified Source-Filter GAN: Unified Source-Filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mizuta21_interspeech.html": {
    "title": "Harmonic WaveGAN: GAN-Based Speech Waveform Generation Model with Harmonic Structure Discriminator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21f_interspeech.html": {
    "title": "Fre-GAN: Adversarial Frequency-Consistent Audio Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21e_interspeech.html": {
    "title": "GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jang21_interspeech.html": {
    "title": "UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alradhi21_interspeech.html": {
    "title": "Continuous Wavelet Vocoder-Based Decomposition of Parametric Speech Waveform Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tobing21_interspeech.html": {
    "title": "High-Fidelity and Low-Latency Universal Neural Vocoder Based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21h_interspeech.html": {
    "title": "Basis-MelGAN: Efficient Neural Vocoder Based on Audio Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hwang21_interspeech.html": {
    "title": "High-Fidelity Parallel WaveGAN with Multi-Band Harmonic-Plus-Noise Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21i_interspeech.html": {
    "title": "SpecRec: An Alternative Solution for Improving End-to-End Speech-to-Text Translation via Spectrogram Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cherry21_interspeech.html": {
    "title": "Subtitle Translation as Markup Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21r_interspeech.html": {
    "title": "Large-Scale Self- and Semi-Supervised Learning for Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21s_interspeech.html": {
    "title": "CoVoST 2 and Massively Multilingual Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheng21_interspeech.html": {
    "title": "AlloST: Low-Resource Speech Translation Without Source Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/effendi21_interspeech.html": {
    "title": "Weakly-Supervised Speech-to-Text Mapping with Visually Connected Non-Parallel Speech-Text Data Using Cyclic Partially-Aligned Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tokuyama21_interspeech.html": {
    "title": "Transcribing Paralinguistic Acoustic Cues to Target Language Text in Transformer-Based Speech-to-Text Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ye21_interspeech.html": {
    "title": "End-to-End Speech Translation via Cross-Modal Progressive Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ko21_interspeech.html": {
    "title": "ASR Posterior-Based Loss for Multi-Task End-to-End Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/perezgonzalezdemartos21_interspeech.html": {
    "title": "Towards Simultaneous Machine Interpretation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/martucci21_interspeech.html": {
    "title": "Lexical Modeling of ASR Errors for Robust Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vyas21_interspeech.html": {
    "title": "Optimally Encoding Inductive Biases into the Transformer Improves End-to-End Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ananthanarayana21_interspeech.html": {
    "title": "Effects of Feature Scaling and Fusion on Sign Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alenin21_interspeech.html": {
    "title": "The ID R&D System Description for Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/thienpondt21_interspeech.html": {
    "title": "Integrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gusev21_interspeech.html": {
    "title": "SdSVC Challenge 2021: Tips and Tricks to Boost the Short-Duration Speaker Verification System Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kang21_interspeech.html": {
    "title": "Team02 Text-Independent Speaker Verification System for SdSV Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qin21_interspeech.html": {
    "title": "Our Learned Lessons from Cross-Lingual Speaker Verification: The CRMI-DKU System Description for the Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21o_interspeech.html": {
    "title": "Investigation of IMU&Elevoc Submission for the Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21_interspeech.html": {
    "title": "The Sogou System for Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21c_interspeech.html": {
    "title": "The SJTU System for Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cho21_interspeech.html": {
    "title": "Multi-Speaker Emotional Text-to-Speech Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/prazak21_interspeech.html": {
    "title": "Live TV Subtitling Through Respeaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fragner21_interspeech.html": {
    "title": "Autonomous Robot for Measuring Room Impulse Responses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/beskow21_interspeech.html": {
    "title": "Expressive Robot Performance Based on Facial Motion Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dominguez21_interspeech.html": {
    "title": "ThemePro 2.0: Showcasing the Role of Thematic Progression in Engaging Human-Computer Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guruju21_interspeech.html": {
    "title": "Addressing Compliance in Call Centers with Entity Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gogineni21_interspeech.html": {
    "title": "Audio Segmentation Based Conversational Silence Detection for Contact Center Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/raj21b_interspeech.html": {
    "title": "Reformulating DOVER-Lap Label Mapping as a Graph Partitioning Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tak21_interspeech.html": {
    "title": "Graph Attention Networks for Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mingote21_interspeech.html": {
    "title": "Log-Likelihood-Ratio Cost Function as Objective Loss for Speaker Verification Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21c_interspeech.html": {
    "title": "Effective Phase Encoding for End-To-End Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21d_interspeech.html": {
    "title": "Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/machacek21_interspeech.html": {
    "title": "Lost in Interpreting: Speech Translation from Source or Interpreter?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pouthier21_interspeech.html": {
    "title": "Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-Based Multimodal Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wallbridge21_interspeech.html": {
    "title": "It's Not What You Said, it's How You Said it: Discriminative Perception of Speech as a Multichannel Communication System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/michael21_interspeech.html": {
    "title": "Extending the Fullband E-Model Towards Background Noise, Bursty Packet Loss, and Conversational Degradations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bergler21_interspeech.html": {
    "title": "ORCA-SLANG: An Automatic Multi-Stage Semi-Supervised Deep Learning Framework for Large-Scale Killer Whale Call Type Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/boes21_interspeech.html": {
    "title": "Audiovisual Transfer Learning for Audio Tagging and Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nessler21_interspeech.html": {
    "title": "Non-Intrusive Speech Quality Assessment with Transfer Learning and Subject-Specific Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oncescu21_interspeech.html": {
    "title": "Audio Retrieval with Natural Language Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/giollo21_interspeech.html": {
    "title": "Bootstrap an End-to-End ASR System by Multilingual Training, Transfer Learning, Text-to-Text Mapping and Synthetic Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pham21_interspeech.html": {
    "title": "Efficient Weight Factorization for Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/conneau21_interspeech.html": {
    "title": "Unsupervised Cross-Lingual Representation Learning for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hayakawa21_interspeech.html": {
    "title": "Language and Speaker-Independent Feature Transformation for End-to-End Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/n21_interspeech.html": {
    "title": "Using Large Self-Supervised Models for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21e_interspeech.html": {
    "title": "Dual Script E2E Framework for Multilingual and Code-Switching ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/diwan21_interspeech.html": {
    "title": "MUCS 2021: Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/winata21_interspeech.html": {
    "title": "Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sailor21_interspeech.html": {
    "title": "SRI-B End-to-End System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21f_interspeech.html": {
    "title": "Hierarchical Phone Recognition with Compositional Phonetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chowdhury21_interspeech.html": {
    "title": "Towards One Model to Rule All: Multilingual Strategy for Dialectal Code-Switching Arabic ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21b_interspeech.html": {
    "title": "Differentiable Allophone Graphs for Language-Universal Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/martin21_interspeech.html": {
    "title": "Automatic Speech Recognition Systems Errors for Objective Sleepiness Detection Through Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gillick21_interspeech.html": {
    "title": "Robust Laughter Detection in Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nagano21_interspeech.html": {
    "title": "Impact of Emotional State on Estimation of Willingness to Buy from Advertising Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alsofyani21_interspeech.html": {
    "title": "Stacked Recurrent Neural Networks for Speech-Based Inference of Attachment Condition in School Age Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/aloshban21_interspeech.html": {
    "title": "Language or Paralanguage, This is the Problem: Comparing Depressed and Non-Depressed Speakers Through the Analysis of Gated Multimodal Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tammewar21_interspeech.html": {
    "title": "Emotion Carrier Recognition from Personal Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/condron21_interspeech.html": {
    "title": "Non-Verbal Vocalisation and Laughter Detection Using Sequence-to-Sequence Models and Multi-Label Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cai21_interspeech.html": {
    "title": "TDCA-Net: Time-Domain Channel Attention Network for Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/botelho21_interspeech.html": {
    "title": "Visual Speech for Obstructive Sleep Apnea Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maruri21_interspeech.html": {
    "title": "Analysis of Contextual Voice Changes in Remote Meetings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seneviratne21_interspeech.html": {
    "title": "Speech Based Depression Severity Level Classification Using a Multi-Stage Dilated CNN-LSTM Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21g_interspeech.html": {
    "title": "Multi-Domain Knowledge Distillation via Uncertainty-Matching for End-to-End ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/macoskey21_interspeech.html": {
    "title": "Learning a Neural Diff for Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21p_interspeech.html": {
    "title": "Stochastic Attention Head Removal: A Simple and Effective Method for Improving Transformer Based ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21b_interspeech.html": {
    "title": "Model-Agnostic Fast Adaptive Multi-Objective Balancing Algorithm for Multilingual Automatic Speech Recognition Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chang21b_interspeech.html": {
    "title": "Towards Lifelong Learning of End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leal21_interspeech.html": {
    "title": "Self-Adaptive Distillation for Multilingual Speech Recognition: Leveraging Student Independence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21f_interspeech.html": {
    "title": "Regularizing Word Segmentation by Creating Misspellings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21t_interspeech.html": {
    "title": "Multitask Training with Text Data for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21j_interspeech.html": {
    "title": "Emitting Word Timings with HMM-Free End-to-End System in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/droppo21_interspeech.html": {
    "title": "Scaling Laws for Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/billa21_interspeech.html": {
    "title": "Leveraging Non-Target Language Resources to Improve ASR Performance in a Target Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fasoli21_interspeech.html": {
    "title": "4-Bit Quantization of LSTM-Based Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/masumura21_interspeech.html": {
    "title": "Unified Autoregressive Modeling for Joint End-to-End Multi-Talker Overlapped Speech Recognition and Speaker Attribute Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/meng21_interspeech.html": {
    "title": "Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jiang21b_interspeech.html": {
    "title": "Variable Frame Rate Acoustic Models Using Minimum Error Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kaland21_interspeech.html": {
    "title": "How f0 and Phrase Position Affect Papuan Malay Word Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jespersen21_interspeech.html": {
    "title": "On the Feasibility of the Danish Model of Intonational Transcription: Phonetic Evidence from Jutlandic Danish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/meli21_interspeech.html": {
    "title": "An Experiment in Paratone Detection in a Prosodically Annotated EAP Spoken Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gerazov21_interspeech.html": {
    "title": "ProsoBeast Prosody Annotation Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tran21_interspeech.html": {
    "title": "Assessing the Use of Prosody in Constituency Parsing of Imperfect Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21i_interspeech.html": {
    "title": "Targeted and Targetless Neutral Tones in Taiwanese Southern Min",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gosy21_interspeech.html": {
    "title": "The Interaction of Word Complexity and Word Duration in an Agglutinative Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21b_interspeech.html": {
    "title": "Taiwan Min Nan (Taiwanese) Checked Tones Sound Change",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jakob21_interspeech.html": {
    "title": "In-Group Advantage in the Perception of Emotions: Evidence from Three Varieties of German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gobl21_interspeech.html": {
    "title": "The LF Model in the Frequency Domain for Glottal Airflow Modelling Without Aliasing Distortion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wagner21_interspeech.html": {
    "title": "Parsing Speech for Grouping and Prominence, and the Typology of Rhythm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mumtaz21_interspeech.html": {
    "title": "Prosody of Case Markers in Urdu",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stefansdottir21_interspeech.html": {
    "title": "Articulatory Characteristics of Icelandic Voiced Fricative Lenition: Gradience, Categoricity, and Speaker/Gesture-Specific Effects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/johnson21_interspeech.html": {
    "title": "Leveraging the Uniformity Framework to Examine Crosslinguistic Similarity for Long-Lag Stops in Spontaneous Cantonese-English Bilingual Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sivaraman21_interspeech.html": {
    "title": "Personalized Speech Enhancement Through Self-Supervised Data Augmentation and Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saddler21_interspeech.html": {
    "title": "Speech Denoising with Auditory Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eskimez21b_interspeech.html": {
    "title": "Human Listening and Live Captioning: Multi-Task Training for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21g_interspeech.html": {
    "title": "Multi-Stage Progressive Speech Enhancement Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chang21c_interspeech.html": {
    "title": "Single-Channel Speech Enhancement Using Learnable Loss Mixup",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21q_interspeech.html": {
    "title": "A Maximum Likelihood Approach to SNR-Progressive Learning Using Generalized Gaussian Distribution for LSTM-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/agrawal21_interspeech.html": {
    "title": "Whisper Speech Enhancement Using Joint Variational Autoencoder for Improved Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21d_interspeech.html": {
    "title": "DEMUCS-Mobile : On-Device Lightweight Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kashyap21_interspeech.html": {
    "title": "Speech Denoising Without Clean Training Data: A Noise2Noise Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dang21_interspeech.html": {
    "title": "Improved Speech Enhancement Using a Complex-Domain GAN with Fused Time-Domain and Time-Frequency Domain Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21r_interspeech.html": {
    "title": "Speech Enhancement with Topology-Enhanced Generative Adversarial Networks (GANs)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bu21_interspeech.html": {
    "title": "Learning Speech Structure to Improve Time-Frequency Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21h_interspeech.html": {
    "title": "SE-Conformer: Time-Domain Speech Enhancement Using Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kongthaworn21_interspeech.html": {
    "title": "Spectral and Latent Speech Representation Distortion for TTS Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/valentinibotinhao21_interspeech.html": {
    "title": "Detection and Analysis of Attention Errors in Sequence-to-Sequence Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zandie21_interspeech.html": {
    "title": "RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21c_interspeech.html": {
    "title": "AISHELL-3: A Multi-Speaker Mandarin TTS Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eng21_interspeech.html": {
    "title": "Comparing Speech Enhancement Techniques for Voice Adaptation-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cui21c_interspeech.html": {
    "title": "EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rallabandi21_interspeech.html": {
    "title": "Perception of Social Speaker Characteristics in Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bakhturina21_interspeech.html": {
    "title": "Hi-Fi Multi-Speaker English TTS Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tseng21b_interspeech.html": {
    "title": "Utilizing Self-Supervised Representations for MOS Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mussakhojayeva21_interspeech.html": {
    "title": "KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/taylor21_interspeech.html": {
    "title": "Confidence Intervals for ASR-Based TTS Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/reddy21_interspeech.html": {
    "title": "INTERSPEECH 2021 Deep Noise Suppression Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21g_interspeech.html": {
    "title": "A Simultaneous Denoising and Dereverberation Framework with Target Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21h_interspeech.html": {
    "title": "Deep Noise Suppression with Non-Intrusive PESQNet Supervision Enabling the Use of Real Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/le21b_interspeech.html": {
    "title": "DPCRN: Dual-Path Convolution Recurrent Network for Single Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lv21_interspeech.html": {
    "title": "DCCRN+: Channel-Wise Subband DCCRN with SNR Estimation for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21s_interspeech.html": {
    "title": "DBNet: A Dual-Branch Network Architecture Processing on Spectrum and Waveform for Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21t_interspeech.html": {
    "title": "Low-Delay Speech Enhancement Using Perceptually Motivated Target and Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oostermeijer21_interspeech.html": {
    "title": "Lightweight Causal Transformer with Local Self-Attention for Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ristea21_interspeech.html": {
    "title": "Self-Paced Ensemble Learning for Speech and Audio Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kojima21_interspeech.html": {
    "title": "Knowledge Distillation for Streaming Transformerâ€“Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lohrenz21_interspeech.html": {
    "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zaiem21_interspeech.html": {
    "title": "Conditional Independence for Pretext Task Selection in Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zeineldeen21_interspeech.html": {
    "title": "Investigating Methods to Improve Language Model Integration for Attention-Based Encoder-Decoder ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vyas21b_interspeech.html": {
    "title": "Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/moine21_interspeech.html": {
    "title": "Speaker Attentive Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leem21_interspeech.html": {
    "title": "Separation of Emotional and Reconstruction Embeddings on Ladder Network to Improve Speech Emotion Recognition Robustness in Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/georgiou21_interspeech.html": {
    "title": "M",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/klejch21_interspeech.html": {
    "title": "The CSTR System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21d_interspeech.html": {
    "title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21e_interspeech.html": {
    "title": "Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/khosravani21_interspeech.html": {
    "title": "Modeling Dialectal Variation for Swiss German Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/egorova21_interspeech.html": {
    "title": "Out-of-Vocabulary Words Detection with Attention and CTC Alignments in an End-to-End ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wiesner21_interspeech.html": {
    "title": "Training Hybrid Models on Noisy Transliterated Transcripts for Code-Switched Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21c_interspeech.html": {
    "title": "Speech Intelligibility of Dysarthric Speech: Human Scores and Acoustic-Phonetic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21i_interspeech.html": {
    "title": "Analyzing Short Term Dynamic Speech Features for Understanding Behavioral Traits of Children with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jesko21_interspeech.html": {
    "title": "Vocalization Recognition of People with Profound Intellectual and Multiple Disabilities (PIMD) Using Machine Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fivela21_interspeech.html": {
    "title": "Phonetic Complexity, Speech Accuracy and Intelligibility Assessment of Italian Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ng21_interspeech.html": {
    "title": "Detection of Consonant Errors in Disordered Speech Based on Consonant-Vowel Segment Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hair21_interspeech.html": {
    "title": "Assessing Posterior-Based Mispronunciation Detection on Field-Collected Recordings from Child Speech Therapy Sessions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mirheidari21_interspeech.html": {
    "title": "Identifying Cognitive Impairment Using Sentence Representation Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yue21b_interspeech.html": {
    "title": "Parental Spoken Scaffolding and Narrative Skills in Crowd-Sourced Storytelling Samples of Young Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xia21_interspeech.html": {
    "title": "Uncertainty-Aware COVID-19 Detection from Imbalanced Sound Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21u_interspeech.html": {
    "title": "Unsupervised Domain Adaptation for Dysarthric Speech Detection via Domain Adversarial Training and Mutual Information Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bhattacharjee21_interspeech.html": {
    "title": "Source and Vocal Tract Cues for Speech-Based Classification of Patients with Parkinson's Disease and Healthy Subjects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/haulcy21_interspeech.html": {
    "title": "CLAC: A Speech Corpus of Healthy English Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nortje21_interspeech.html": {
    "title": "Direct Multimodal Few-Shot Learning of Speech and Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sanabria21_interspeech.html": {
    "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhao21b_interspeech.html": {
    "title": "A Fast Discrete Two-Step Learning Hashing for Scalable Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21v_interspeech.html": {
    "title": "Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/olaleye21_interspeech.html": {
    "title": "Attention-Based Keyword Localisation in Speech Using Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/khorrami21_interspeech.html": {
    "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21k_interspeech.html": {
    "title": "Automatic Lip-Reading with Hierarchical Pyramidal Convolution and Self-Attention for Image Sequences with No Word Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rouditchenko21b_interspeech.html": {
    "title": "Cascaded Multilingual Audio-Visual Learning from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21c_interspeech.html": {
    "title": "LiRA: Learning Visual Speech Representations from Audio Through Self-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rose21_interspeech.html": {
    "title": "End-to-End Audio-Visual Speech Recognition for Overlapping Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21e_interspeech.html": {
    "title": "Audio-Visual Multi-Talker Speech Recognition in a Cocktail Party",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21l_interspeech.html": {
    "title": "Ultra Fast Speech Separation Model with Teacher Student Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ali21_interspeech.html": {
    "title": "Group Delay Based Re-Weighted Sparse Recovery Algorithms for Robust and High-Resolution Source Separation in DOA Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21d_interspeech.html": {
    "title": "Continuous Speech Separation Using Speaker Inventory for Long Recording",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yuan21_interspeech.html": {
    "title": "Crossfire Conditional Generative Adversarial Networks for Singing Voice Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21w_interspeech.html": {
    "title": "End-to-End Speech Separation Using Orthogonal Representation in Complex and Real Time-Frequency Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nakagome21_interspeech.html": {
    "title": "Efficient and Stable Adversarial Learning Using Unpaired Data for Unsupervised Multichannel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21h_interspeech.html": {
    "title": "Stabilizing Label Assignment for Speech Separation by Self-Supervised Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21x_interspeech.html": {
    "title": "Dual-Path Filter Network: Speaker-Aware Modeling for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21f_interspeech.html": {
    "title": "Investigation of Practical Aspects of Single Channel Speech Separation for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21c_interspeech.html": {
    "title": "Implicit Filter-and-Sum Network for End-to-End Multi-Channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21i_interspeech.html": {
    "title": "Generalized Spatio-Temporal RNN Beamformer for Target Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21j_interspeech.html": {
    "title": "End-to-End Neural Diarization: From Transformer to Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jung21_interspeech.html": {
    "title": "Three-Class Overlapped Speech Detection Using a Convolutional Recurrent Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wan21_interspeech.html": {
    "title": "Online Speaker Diarization Equipped with Discriminative Modeling and Guided Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/takashima21_interspeech.html": {
    "title": "Semi-Supervised Training with Pseudo-Labeling for End-To-End Neural Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kwon21b_interspeech.html": {
    "title": "Adapting Speaker Embeddings for Speaker Diarisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21y_interspeech.html": {
    "title": "Scenario-Dependent Speaker Diarization for DIHARD-III Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bredin21_interspeech.html": {
    "title": "End-To-End Speaker Segmentation for Overlap-Aware Resegmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21d_interspeech.html": {
    "title": "Online Streaming End-to-End Neural Diarization Handling Overlapping Speech and Flexible Numbers of Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/anidjar21_interspeech.html": {
    "title": "A Thousand Words are Worth More Than One Recording:",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/futamata21_interspeech.html": {
    "title": "Phrase Break Prediction with Bidirectional Encoder Representations in Japanese Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vallesperez21_interspeech.html": {
    "title": "Improving Multi-Speaker TTS Prosody Variance with a Residual Encoder and Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/du21b_interspeech.html": {
    "title": "Rich Prosody Diversity Modelling with Phone-Level Mixture Density Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujita21_interspeech.html": {
    "title": "Phoneme Duration Modeling Using Speech Rhythm-Based Speaker Embeddings for Multi-Speaker Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zou21_interspeech.html": {
    "title": "Fine-Grained Prosody Modeling in Neural Speech Synthesis Using ToBI Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sharma21b_interspeech.html": {
    "title": "Intra-Sentential Speaking Rate Control in Neural Text-To-Speech for Automatic Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21u_interspeech.html": {
    "title": "Applying the Information Bottleneck Principle to Prosodic Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/baird21_interspeech.html": {
    "title": "A Prototypical Network Approach for Evaluating Generated Emotional Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yoshinaga21_interspeech.html": {
    "title": "A Simplified Model for the Vocal Tract of [s] with Inclined Incisors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/arai21b_interspeech.html": {
    "title": "Vocal-Tract Models to Visualize the Airstream of Human Breath and Droplets While Producing Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanji21_interspeech.html": {
    "title": "Using Transposed Convolution for Articulatory-to-Acoustic Conversion from Real-Time MRI Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/inaam21_interspeech.html": {
    "title": "Comparison Between Lumped-Mass Modeling and Flow Simulation of the Reed-Type Artificial Vocal Fold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/werner21_interspeech.html": {
    "title": "Inhalations in Speech: Acoustic and Physiological Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21j_interspeech.html": {
    "title": "Model-Based Exploration of Linking Between Vowel Articulatory Space and Acoustic Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elmers21_interspeech.html": {
    "title": "Take a Breath: Respiratory Sounds Improve Recollection in Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21m_interspeech.html": {
    "title": "Modeling Sensorimotor Adaptation in Speech Through Alterations to Forward and Inverse Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kawahara21_interspeech.html": {
    "title": "Mixture of Orthogonal Sequences Made from Extended Time-Stretched Pulses Enables Measurement of Involuntary Voice Fundamental Frequency Response to Pitch Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/you21c_interspeech.html": {
    "title": "Contextualized Attention-Based Knowledge Transfer for Spoken Conversational Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/duan21_interspeech.html": {
    "title": "Injecting Descriptive Meta-Information into Pre-Trained Language Models with Hypernetworks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rohmatillah21_interspeech.html": {
    "title": "Causal Confusion Reduction for Robust Multi-Domain Dialogue Policy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujie21_interspeech.html": {
    "title": "Timing Generating Networks: Neural Network Based Precise Turn-Taking Timing Prediction in Multiparty Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21n_interspeech.html": {
    "title": "Human-to-Human Conversation Dataset for Learning Fine-Grained Turn-Taking Action",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sundararaman21_interspeech.html": {
    "title": "PhonemeBERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21d_interspeech.html": {
    "title": "Joint Retrieval-Extraction Training for Evidence-Aware Dialog Response Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shenoy21_interspeech.html": {
    "title": "Adapting Long Context NLM for ASR Rescoring in Conversational Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21h_interspeech.html": {
    "title": "Oriental Language Recognition (OLR) 2020: Summary and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/duroselle21b_interspeech.html": {
    "title": "Language Recognition on Unknown Conditions: The LORIA-Inria-MULTISPEECH System for AP20-OLR Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kong21b_interspeech.html": {
    "title": "Dynamic Multi-Scale Convolution for Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21z_interspeech.html": {
    "title": "An End-to-End Dialect Identification System with Transfer Learning from a Multilingual Automatic Speech Recognition Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yu21b_interspeech.html": {
    "title": "Language Recognition Based on Unsupervised Pretrained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21i_interspeech.html": {
    "title": "Additive Phoneme-Aware Margin Softmax Loss for Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jahchan21_interspeech.html": {
    "title": "Towards an Accent-Robust Approach for ATC Communications Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/szoke21_interspeech.html": {
    "title": "Detecting English Speech in the Air Traffic Control Voice Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ohneiser21_interspeech.html": {
    "title": "Robust Command Recognition for Lithuanian Air Traffic Control Tower Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zuluagagomez21_interspeech.html": {
    "title": "Contextual Semi-Supervised Learning: An Approach to Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kocour21_interspeech.html": {
    "title": "Boosting of Contextual Information in ASR for Air-Traffic Call-Sign Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elie21_interspeech.html": {
    "title": "Modeling the Effect of Military Oxygen Masks on Speech Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/milde21_interspeech.html": {
    "title": "MoM: Minutes of Meeting Bot",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wilbrandt21_interspeech.html": {
    "title": "Articulatory Data Recorder: A Framework for Real-Time Articulatory Data Recording",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/codinafilba21_interspeech.html": {
    "title": "The INGENIOUS Multilingual Operations App",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rownicka21_interspeech.html": {
    "title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/geislinger21_interspeech.html": {
    "title": "Live Subtitling for BigBlueButton with Open-Source Software",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nicmanis21_interspeech.html": {
    "title": "Expressive Latvian Speech Synthesis for Dialog Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kachare21_interspeech.html": {
    "title": "ViSTAFAE: A Visual Speech-Training Aid with Feedback of Articulatory Efforts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/livescu21_interspeech.html": {
    "title": "Learning Speech Models from Multi-Modal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elhilali21_interspeech.html": {
    "title": "Adaptive Listening to Everyday Soundscapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ribeiro21b_interspeech.html": {
    "title": "Towards the Prediction of the Vocal Tract Shape from the Sequence of Phonemes to be Articulated",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/blandin21_interspeech.html": {
    "title": "Comparison of the Finite Element Method, the Multimodal Method and the Transmission-Line Model for the Computation of Vocal Tract Transfer Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wagner21b_interspeech.html": {
    "title": "Effects of Time Pressure and Spontaneity on Phonotactic Innovations in German Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/medina21_interspeech.html": {
    "title": "Importance of Parasagittal Sensor Information in Tongue Motion Capture Through a Diphonic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/georges21_interspeech.html": {
    "title": "Learning Robust Speech Representation with an Articulatory-Regularized Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/weston21_interspeech.html": {
    "title": "Changes in Glottal Source Parameter Values with Light to Moderate Physical Load",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vali21_interspeech.html": {
    "title": "End-to-End Optimized Multi-Stage Vector Quantization of Spectral Envelopes for Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nareddula21_interspeech.html": {
    "title": "Fusion-Net: Time-Frequency Information Fusion Y-Network for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/marcinek21_interspeech.html": {
    "title": "N-MTTL SI Model: Non-Intrusive Multi-Task Transfer Learning-Based Speech Intelligibility Prediction Model with Scenery Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xia21b_interspeech.html": {
    "title": "Temporal Context in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21j_interspeech.html": {
    "title": "Learning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vaaras21_interspeech.html": {
    "title": "Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qian21_interspeech.html": {
    "title": "Multimodal Sentiment Analysis with Temporal Modality Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/t21_interspeech.html": {
    "title": "Stochastic Process Regression for Cross-Cultural Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21k_interspeech.html": {
    "title": "Acted vs. Improvised: Domain Adaptation for Elicitation Approaches in Audio-Visual Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pepino21_interspeech.html": {
    "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21k_interspeech.html": {
    "title": "Graph Isomorphism Network for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumawat21_interspeech.html": {
    "title": "Applying TDNN Architectures for Analyzing Duration Dependencies on Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/keesing21_interspeech.html": {
    "title": "Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shon21_interspeech.html": {
    "title": "Leveraging Pre-Trained Language Model for Speech Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hou21b_interspeech.html": {
    "title": "Cross-Domain Speech Recognition with Unsupervised Character-Level Distribution Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kanda21_interspeech.html": {
    "title": "Large-Scale Pre-Training of End-to-End Multi-Talker ASR for Meeting Transcription with Single Distant Microphone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21b_interspeech.html": {
    "title": "On Minimum Word Error Rate Training of the Hybrid Autoregressive Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21j_interspeech.html": {
    "title": "Reducing Streaming ASR Model Delay with Self Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/diwan21b_interspeech.html": {
    "title": "Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fukuda21_interspeech.html": {
    "title": "Knowledge Distillation Based Training of Universal ASR Source Models for Cross-Lingual Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ray21_interspeech.html": {
    "title": "Listen with Intent: Improving Speech Recognition with Audio-to-Intent Front-End",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21c_interspeech.html": {
    "title": "Exploring Targeted Universal Adversarial Perturbations to End-to-End ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/delrio21_interspeech.html": {
    "title": "Earnings-21: A Practical Benchmark for ASR in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sun21c_interspeech.html": {
    "title": "Improving Multilingual Transformer Transducer Models by Reducing Language Confusions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ali21b_interspeech.html": {
    "title": "Arabic Code-Switching Speech Recognition Using Monolingual Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eisenberg21_interspeech.html": {
    "title": "Online Blind Audio Source Separation Using Recursive Expectation-Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21e_interspeech.html": {
    "title": "Empirical Analysis of Generalized Iterative Speech Separation Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/neumann21_interspeech.html": {
    "title": "Graph-PIT: Generalized Permutation Invariant Training for Continuous Separation of Arbitrary Numbers of Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21v_interspeech.html": {
    "title": "Teacher-Student MixIT for Unsupervised and Semi-Supervised Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/delcroix21_interspeech.html": {
    "title": "Few-Shot Learning of New Sound Classes for Target Sound Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21e_interspeech.html": {
    "title": "Binaural Speech Separation of Moving Speakers With Preserved Spatial Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hu21_interspeech.html": {
    "title": "AvaTr: One-Shot Speaker Extraction with Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sarkar21_interspeech.html": {
    "title": "Vocal Harmony Separation Using Time-Domain Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maciejewski21_interspeech.html": {
    "title": "Speaker Verification-Based Evaluation of Single-Channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lan21_interspeech.html": {
    "title": "Improved Speech Separation with Time-and-Frequency Cross-Domain Feature Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21c_interspeech.html": {
    "title": "Robust Speaker Extraction Network Based on Iterative Refined Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21aa_interspeech.html": {
    "title": "Neural Speaker Extraction with Speaker-Speech Cross-Attention Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rigal21_interspeech.html": {
    "title": "Deep Audio-Visual Speech Separation Based on Facial Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/singh21_interspeech.html": {
    "title": "LEAP Submission for the Third DIHARD Diarization Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21w_interspeech.html": {
    "title": "Investigation of Spatial-Acoustic Features for Overlapping Speech Detection in Multiparty Meetings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/he21c_interspeech.html": {
    "title": "Target-Speaker Voice Activity Detection with Improved i-Vector Estimation for Unknown Number of Speaker",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dawalatabad21_interspeech.html": {
    "title": "ECAPA-TDNN Embeddings for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kinoshita21_interspeech.html": {
    "title": "Advances in Integration of End-to-End Neural and Clustering-Based Diarization for Real Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ryant21_interspeech.html": {
    "title": "The Third DIHARD Diarization Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leung21_interspeech.html": {
    "title": "Robust End-to-End Speaker Diarization with Conformer and Additive Margin Penalty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/obrien21_interspeech.html": {
    "title": "Anonymous Speaker Clusters: Making Distinctions Between Anonymised Speech Recordings with Clustering Interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karra21_interspeech.html": {
    "title": "Speaker Diarization Using Two-Pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hong21_interspeech.html": {
    "title": "Federated Learning with Dynamic Transformer for Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21e_interspeech.html": {
    "title": "LiteTTS: A Lightweight Mel-Spectrogram-Free Text-to-Wave Synthesizer Based on Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tang21b_interspeech.html": {
    "title": "Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jeong21_interspeech.html": {
    "title": "Diff-TTS: A Denoising Diffusion Model for Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bae21_interspeech.html": {
    "title": "Hierarchical Context-Aware Transformers for Non-Autoregressive Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/polyak21_interspeech.html": {
    "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karanasou21_interspeech.html": {
    "title": "A Learned Conditional Prior for the VAE Acoustic Space of a TTS System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/paul21_interspeech.html": {
    "title": "A Universal Multi-Speaker Multi-Style Text-to-Speech via Disentangled Representation Learning Based on RÃ©nyi Divergence Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21g_interspeech.html": {
    "title": "Relational Data Selection for Data Augmentation of Speaker-Dependent Multi-Band MelGAN Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chung21_interspeech.html": {
    "title": "Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21g_interspeech.html": {
    "title": "Triple M: A Practical Text-to-Speech Synthesis System with Multi-Guidance Attention and Multi-Band Multi-Time LPCNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/casanova21b_interspeech.html": {
    "title": "SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker Text-To-Speech Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/palmer21_interspeech.html": {
    "title": "Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/salesky21_interspeech.html": {
    "title": "The Multilingual TEDx Corpus for Speech Recognition and Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mortensen21_interspeech.html": {
    "title": "Tusom2021: A Phonetically Transcribed Speech Dataset from an Endangered Language for Universal Phone Recognition Experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fu21b_interspeech.html": {
    "title": "AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21o_interspeech.html": {
    "title": "GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21k_interspeech.html": {
    "title": "Look Who's Talking: Active Speaker Detection in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ahmed21_interspeech.html": {
    "title": "AusKidTalk: An Auditory-Visual Corpus of 3- to 12-Year-Old Australian Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fallgren21_interspeech.html": {
    "title": "Human-in-the-Loop Efficiency Analysis for Binary Classification in Edyson",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ryumina21_interspeech.html": {
    "title": "Annotation Confidence vs. Training Sample Size: Trade-Off Solution for Partially-Continuous Categorical Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/garcesdiazmunio21_interspeech.html": {
    "title": "Europarl-ASR: A Large Corpus of Parliamentary Debates for Streaming ASR Benchmarking and Speech Data Filtering/Verbatimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kapoor21_interspeech.html": {
    "title": "Towards Automatic Speech to Sign Language Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cho21b_interspeech.html": {
    "title": "kosp2e: Korean Speech to English Translation Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21x_interspeech.html": {
    "title": "speechocean762: An Open-Source Non-Native English Speech Corpus for Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fan21b_interspeech.html": {
    "title": "An Improved Single Step Non-Autoregressive Transformer for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guo21_interspeech.html": {
    "title": "Multi-Speaker ASR Combining Non-Autoregressive Conformer CTC and Conditional Speaker Chain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ng21b_interspeech.html": {
    "title": "Pushing the Limits of Non-Autoregressive Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21l_interspeech.html": {
    "title": "Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nozaki21_interspeech.html": {
    "title": "Relaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujita21b_interspeech.html": {
    "title": "Toward Streaming ASR with Non-Autoregressive Insertion-Based Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21e_interspeech.html": {
    "title": "Layer Pruning on Demand with Intermediate CTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21l_interspeech.html": {
    "title": "Real-Time End-to-End Monaural Multi-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ba_interspeech.html": {
    "title": "Streaming End-to-End ASR Based on Blockwise Non-Autoregressive Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/beliaev21_interspeech.html": {
    "title": "TalkNet: Non-Autoregressive Depth-Wise Separable Convolutional Model for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21p_interspeech.html": {
    "title": "WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21q_interspeech.html": {
    "title": "Align-Denoise: Single-Pass Non-Autoregressive Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21d_interspeech.html": {
    "title": "VAENAR-TTS: Variational Auto-Encoder Based Non-AutoRegressive Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luz21_interspeech.html": {
    "title": "Detecting Cognitive Decline Using Speech Only: The ADReSSo Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pereztoro21_interspeech.html": {
    "title": "Influence of the Interviewer on the Automatic Assessment of Alzheimer's Disease in the Context of the ADReSSo Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21e_interspeech.html": {
    "title": "WavBERT: Exploiting Semantic and Non-Semantic Speech Using Wav2vec and BERT for Dementia Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gauder21_interspeech.html": {
    "title": "Alzheimer Disease Recognition Using Speech-Based Embeddings From Pre-Trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/balagopalan21_interspeech.html": {
    "title": "Comparing Acoustic-Based Approaches for Alzheimer's Disease Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiao21_interspeech.html": {
    "title": "Alzheimer's Disease Detection from Spontaneous Speech Through Combining Linguistic Complexity and (Dis)Fluency Features with Pretrained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21c_interspeech.html": {
    "title": "Using the Outputs of Different Automatic Speech Recognition Paradigms for Acoustic- and BERT-Based Alzheimer's Dementia Detection Through Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/syed21_interspeech.html": {
    "title": "Tackling the ADRESSO Challenge 2021: The MUET-RMIT System for Alzheimer's Dementia Recognition from Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rohanian21_interspeech.html": {
    "title": "Alzheimer's Dementia Recognition Using Acoustic, Lexical, Disfluency and Speech Pause Features Robust to Noisy Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pappagari21_interspeech.html": {
    "title": "Automatic Detection and Assessment of Alzheimer Disease Using Speech and Language Technologies in Low-Resource Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21r_interspeech.html": {
    "title": "Automatic Detection of Alzheimer's Disease Using Spontaneous Speech Only",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ca_interspeech.html": {
    "title": "Modular Multi-Modal Attention Network for Alzheimer's Disease Detection Using Patient Audio and Language Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21d_interspeech.html": {
    "title": "Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-Field Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gretter21_interspeech.html": {
    "title": "ETLT 2021: Shared Task on Automatic Speech Recognition for Non-Native Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rumberg21_interspeech.html": {
    "title": "Age-Invariant Training for End-to-End Child Speech Recognition Using Adversarial Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cornell21_interspeech.html": {
    "title": "Learning to Rank Microphones for Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gelin21_interspeech.html": {
    "title": "Simulating Reading Mistakes for Child Speech Transformer-Based Phone Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stephenson21_interspeech.html": {
    "title": "Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rijn21_interspeech.html": {
    "title": "Exploring Emotional Prototypes in a High Dimensional TTS Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mohan21_interspeech.html": {
    "title": "Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/torresquintero21_interspeech.html": {
    "title": "ADEPT: A Dataset for Evaluating Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/trang21_interspeech.html": {
    "title": "Prosodic Boundary Prediction Model for Vietnamese Text-To-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dovrat21_interspeech.html": {
    "title": "Many-Speakers Single Channel Speech Separation with Optimal Permutation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fras21_interspeech.html": {
    "title": "Combating Reverberation in NTF-Based Speech Separation Using a Sub-Source Weighted Multichannel Wiener Filter and Linear Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/strauss21_interspeech.html": {
    "title": "A Hands-On Comparison of DNNs for Dialog Separation Using Transfer Learning from Music Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/borsdorf21b_interspeech.html": {
    "title": "GlobalPhone Mix-To-Separate Out of 2: A Multilingual 2000 Speakers Mixtures Database for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tsukada21_interspeech.html": {
    "title": "Cross-Linguistic Perception of the Japanese Singleton/Geminate Contrast: Korean, Mandarin and Mongolian Compared",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/korzekwa21_interspeech.html": {
    "title": "Detection of Lexical Stress Errors in Non-Native (L2) English with Data Augmentation and Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/braun21_interspeech.html": {
    "title": "Testing Acoustic Voice Quality Classification Across Languages and Speech Styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21y_interspeech.html": {
    "title": "Acquisition of Prosodic Focus Marking by Three- to Six-Year-Old Children Learning Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mirzaei21_interspeech.html": {
    "title": "Adaptive Listening Difficulty Detection for L2 Learners Through Moderating ASR Resources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ding21b_interspeech.html": {
    "title": "F",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21h_interspeech.html": {
    "title": "A Neural Network-Based Noise Compensation Method for Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kudera21_interspeech.html": {
    "title": "Phonetic Distance and Surprisal in Multilingual Priming: Evidence from Slavic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21z_interspeech.html": {
    "title": "A Preliminary Study on Discourse Prosody Encoding in L1 and L2 English Spontaneous Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21h_interspeech.html": {
    "title": "Transformer Based End-to-End Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/graham21_interspeech.html": {
    "title": "L1 Identification from L2 Speech Using Neural Spectrogram Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oh21b_interspeech.html": {
    "title": "Leveraging Real-Time MRI for Illuminating Linguistic Velum Action",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21m_interspeech.html": {
    "title": "Segmental Alignment of English Syllables with Singleton and Cluster Onsets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hejna21_interspeech.html": {
    "title": "Exploration of Welsh English Pre-Aspiration: How Wide-Spread is it?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muhlack21_interspeech.html": {
    "title": "Revisiting Recall Effects of Filler Particles in German and English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ge21b_interspeech.html": {
    "title": "How Reliable Are Phonetic Data Collected Remotely? Comparison of Recording Devices and Environments on Acoustic Measurements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21i_interspeech.html": {
    "title": "A Cross-Dialectal Comparison of Apical Vowels in Beijing Mandarin, Northeastern Mandarin and Southwestern Mandarin: An EMA and Ultrasound Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gibson21_interspeech.html": {
    "title": "Dissecting the Aero-Acoustic Parameters of Open Articulatory Transitions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gully21_interspeech.html": {
    "title": "Quantifying Vocal Tract Shape Variation and its Acoustic Impact: A Geometric Morphometric Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guevararukoz21_interspeech.html": {
    "title": "Speech Perception and Loanword Adaptations: The Case of Copy-Vowel Epenthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guo21b_interspeech.html": {
    "title": "Speakers Coarticulate Less When Facing Real and Imagined Communicative Difficulties: An Analysis of Read and Spontaneous Speech from the LUCID Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/meister21_interspeech.html": {
    "title": "Developmental Changes of Vowel Acoustics in Adolescents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dapolito21_interspeech.html": {
    "title": "Context and Co-Text Influence on the Accuracy Production of Italian L2 Non-Native Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/heeringa21_interspeech.html": {
    "title": "A New Vowel Normalization for Sociophonetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/billington21_interspeech.html": {
    "title": "The Pacific Expansion: Optimizing Phonetic Transcription of Archival Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tian21_interspeech.html": {
    "title": "FSR: Accelerating the Inference Process of Transducer-Based Models by Applying Fast-Skip Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mitrofanov21_interspeech.html": {
    "title": "LT-LM: A Novel Non-Autoregressive Language Model for Single-Shot Lattice Rescoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/allauzen21_interspeech.html": {
    "title": "A Hybrid Seq-2-Seq ASR Design for On-Device and Server Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/inaguma21b_interspeech.html": {
    "title": "VAD-Free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yao21_interspeech.html": {
    "title": "WeNet: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanaka21b_interspeech.html": {
    "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21f_interspeech.html": {
    "title": "Deep Neural Network Calibration for E2E Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21m_interspeech.html": {
    "title": "Residual Energy-Based Models for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiu21b_interspeech.html": {
    "title": "Multi-Task Learning for End-to-End ASR Word and Utterance Confidence with Deletion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ollerenshaw21_interspeech.html": {
    "title": "Insights on Neural Representations for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/afshan21_interspeech.html": {
    "title": "Sequence-Level Confidence Classifier for ASR Utterance Accuracy and Application to Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tjandra21_interspeech.html": {
    "title": "Unsupervised Learning of Disentangled Speech Content and Style Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/choi21_interspeech.html": {
    "title": "Label Embedding for Chinese Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21aa_interspeech.html": {
    "title": "PDF: Polyphone Disambiguation in Chinese by Using FLAT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21n_interspeech.html": {
    "title": "Improving Polyphone Disambiguation for Mandarin Chinese by Combining Mix-Pooling Strategy and Window-Based Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21d_interspeech.html": {
    "title": "Polyphone Disambiguation in Mandarin Chinese with Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21s_interspeech.html": {
    "title": "A Neural-Network-Based Approach to Identifying Speakers in Novels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21f_interspeech.html": {
    "title": "UnitNet-Based Hybrid Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/novitasari21_interspeech.html": {
    "title": "Dynamically Adaptive Machine Speech Chain Inference for TTS in Noisy Environment: Listen and Speak Louder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ba_interspeech.html": {
    "title": "LinearSpeech: Parallel Text-to-Speech with Linear Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mansbach21_interspeech.html": {
    "title": "An Agent for Competing with Humans in a Deceptive Game Based on Vocal Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fakhry21_interspeech.html": {
    "title": "A Multi-Branch Deep Learning Network for Automated Detection of COVID-19",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21d_interspeech.html": {
    "title": "RW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dhamyal21_interspeech.html": {
    "title": "Fake Audio Detection in Resource-Constrained Settings Using Microfeatures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21c_interspeech.html": {
    "title": "Coughing-Based Recognition of Covid-19 with Spatial Attentive ConvLSTM Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/paul21b_interspeech.html": {
    "title": "Knowledge Distillation for Singing Voice Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/takeda21_interspeech.html": {
    "title": "Age Estimation with Speech-Age Model for Heterogeneous Speech Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/teh21_interspeech.html": {
    "title": "Open-Set Audio Classification with Limited Training Resources Based on Augmentation Enhanced Variational Auto-Encoder GAN with Detection-Classification Joint Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fukumori21_interspeech.html": {
    "title": "Deep Spectral-Cepstral Fusion for Shouted and Normal Speech Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/baghel21_interspeech.html": {
    "title": "Automatic Detection of Shouted Speech Segments in Indian News Debates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21c_interspeech.html": {
    "title": "Generalized Spoofing Detection Inspired from Audio Generation Artifacts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21t_interspeech.html": {
    "title": "Overlapped Speech Detection Based on Spectral and Spatial Feature Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/abdullah21_interspeech.html": {
    "title": "Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21d_interspeech.html": {
    "title": "Paraphrase Label Alignment for Voice Application Retrieval in Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rikhye21_interspeech.html": {
    "title": "Personalized Keyphrase Detection Using Speaker and Environment Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/garg21_interspeech.html": {
    "title": "Streaming Transformer for Hardware Efficient Voice Trigger Detection and False Trigger Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mazumder21_interspeech.html": {
    "title": "Few-Shot Keyword Spotting in Any Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21da_interspeech.html": {
    "title": "Text Anchor Based Metric Learning for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21u_interspeech.html": {
    "title": "A Meta-Learning Approach for User-Defined Spoken Term Classification with Varying Classes and Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21g_interspeech.html": {
    "title": "Auxiliary Sequence Labeling Tasks for Disfluency Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21g_interspeech.html": {
    "title": "Energy-Friendly Keyword Spotting System Using Add-Based Convolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jia21b_interspeech.html": {
    "title": "The 2020 Personalized Voice Trigger Challenge: Open Datasets, Evaluation Metrics, Baseline System and Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ea_interspeech.html": {
    "title": "Auto-KWS 2021 Challenge: Task, Datasets, and Baselines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/berg21_interspeech.html": {
    "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/awasthi21_interspeech.html": {
    "title": "Teaching Keyword Spotters to Spot New Keywords with Limited Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21fa_interspeech.html": {
    "title": "A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ca_interspeech.html": {
    "title": "An Initial Investigation for Detecting Partially Spoofed Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xie21_interspeech.html": {
    "title": "Siamese Network with wav2vec Feature for Spoofing Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheng21b_interspeech.html": {
    "title": "Cross-Database Replay Detection in Terminal-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21da_interspeech.html": {
    "title": "The Effect of Silence and Dual-Band Fusion in Anti-Spoofing System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21d_interspeech.html": {
    "title": "Pairing Weak with Strong: Twin Models for Defending Against Adversarial Attack on Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ling21_interspeech.html": {
    "title": "Attention-Based Convolutional Neural Network for ASV Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21i_interspeech.html": {
    "title": "Voting for the Right Answer: Adversarial Defense for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kinnunen21_interspeech.html": {
    "title": "Visualizing Classifier Adjacency Relations: A Case Study in Speaker Verification and Voice Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/villalba21_interspeech.html": {
    "title": "Representation Learning to Classify and Detect Adversarial Attacks Against Speaker and Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ea_interspeech.html": {
    "title": "An Empirical Study on Channel Effects for Synthetic Voice Spoofing Countermeasure Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21o_interspeech.html": {
    "title": "Channel-Wise Gated Res2Net: Towards Robust Detection of Synthetic Speech Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ge21c_interspeech.html": {
    "title": "Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peterson21_interspeech.html": {
    "title": "OpenASR20: An Open Challenge for Automatic Speech Recognition of Conversational Telephone Speech in Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/madikeri21_interspeech.html": {
    "title": "Multitask Adaptation with Lattice-Free MMI for Multi-Genre Speech Recognition of Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21f_interspeech.html": {
    "title": "An Improved Wav2Vec 2.0 Pre-Training Approach Using Enhanced Local Dependency Modeling for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21i_interspeech.html": {
    "title": "Systems for Low-Resource Speech Recognition Tasks in Open Automatic Speech Recognition and Formosa Speech Recognition Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhao21c_interspeech.html": {
    "title": "The TNT Team System Descriptions of Cantonese and Mongolian for IARPA OpenASR20",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alumae21_interspeech.html": {
    "title": "Combining Hybrid and End-to-End Approaches for the OpenASR20 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/morris21_interspeech.html": {
    "title": "One Size Does Not Fit All in Resource-Constrained ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cristia21_interspeech.html": {
    "title": "Child Language Acquisition Studied with Wearables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mikolov21_interspeech.html": {
    "title": "Language Modeling and Artificial Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gimeno21_interspeech.html": {
    "title": "Unsupervised Representation Learning for Speech Activity Detection in the Fearless Steps Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vuong21_interspeech.html": {
    "title": "The Application of Learnable STRF Kernels to the 2021 Fearless Steps Phase-03 SAD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sarfjoo21_interspeech.html": {
    "title": "Speech Activity Detection Based on Multilingual Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luckenbaugh21_interspeech.html": {
    "title": "Voice Activity Detection with Teacher-Student Domain Emulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ghahabi21_interspeech.html": {
    "title": "EML Online Speech Activity Detection for the Fearless Steps Challenge Phase-III",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/opatka21_interspeech.html": {
    "title": "Device Playback Augmentation with Echo Cancellation for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yusuf21_interspeech.html": {
    "title": "End-to-End Open Vocabulary Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/merkx21_interspeech.html": {
    "title": "Semantic Sentence Similarity: Size does not Always Matter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/svec21_interspeech.html": {
    "title": "Spoken Term Detection and Relevance Score Estimation Using Dot-Product of Pronunciation Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/buet21_interspeech.html": {
    "title": "Toward Genre Adapted Closed Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/korzekwa21b_interspeech.html": {
    "title": "Weakly-Supervised Word-Level Pronunciation Error Detection in Non-Native English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kanda21b_interspeech.html": {
    "title": "End-to-End Speaker-Attributed ASR with Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/soltau21_interspeech.html": {
    "title": "Understanding Medical Conversations: Rich Transcription, Confidence Scores & Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vidal21_interspeech.html": {
    "title": "Phone-Level Pronunciation Scoring for Spanish Speakers Learning English Using a GOP-DNN System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21k_interspeech.html": {
    "title": "Explore wav2vec 2.0 for Mispronunciation Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ando21_interspeech.html": {
    "title": "Lexical Density Analysis of Word Productions in Japanese English Using Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21j_interspeech.html": {
    "title": "Deep Feature Transfer Learning for Automatic Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21fa_interspeech.html": {
    "title": "Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21e_interspeech.html": {
    "title": "A Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiao21b_interspeech.html": {
    "title": "The Impact of ASR on the Automatic Analysis of Linguistic Complexity and Sophistication in Spontaneous L2 Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanaka21c_interspeech.html": {
    "title": "End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cumbal21_interspeech.html": {
    "title": "You don't understand me!\": Comparing ASR Results for L1 and L2 Speakers of Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ga_interspeech.html": {
    "title": "NeMo Inverse Text Normalization: From Development to Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/naijo21_interspeech.html": {
    "title": "Improvement of Automatic English Pronunciation Assessment with Small Number of Utterances Using Sentence Speakability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/haider21_interspeech.html": {
    "title": "Affect Recognition Through Scalogram and Multi-Resolution Cochleagram Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21n_interspeech.html": {
    "title": "A Speech Emotion Recognition Framework for Better Discrimination of Confusions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21p_interspeech.html": {
    "title": "Speech Emotion Recognition via Multi-Level Cross-Modal Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ito21_interspeech.html": {
    "title": "Audio-Visual Speech Emotion Recognition by Disentangling Emotion and Identity Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bose21_interspeech.html": {
    "title": "Parametric Distributions to Model Numerical Emotion Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21e_interspeech.html": {
    "title": "Metric Learning Based Feature Representation with Gated Fusion Model for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cai21b_interspeech.html": {
    "title": "Speech Emotion Recognition with Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seneviratne21b_interspeech.html": {
    "title": "Generalized Dilated CNN Models for Depression Detection Using Inverted Vocal Tract Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ga_interspeech.html": {
    "title": "Learning Mutual Correlation in Multimodal Transformer for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21o_interspeech.html": {
    "title": "Time-Frequency Representation Learning with Graph Convolutional Network for Dialogue-Level Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mordido21_interspeech.html": {
    "title": "Compressing 1D Time-Channel Separable Convolutions Using Sparse Random Ternary Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheng21c_interspeech.html": {
    "title": "Weakly Supervised Construction of ASR Systems from Massive Video Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21l_interspeech.html": {
    "title": "Broadcasted Residual Learning for Efficient Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/swaminathan21_interspeech.html": {
    "title": "CoDERT: Distilling Encoder Representations with Co-Learning for Transducer-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21f_interspeech.html": {
    "title": "Extremely Low Footprint End-to-End ASR System for Smart Device",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shangguan21_interspeech.html": {
    "title": "Dissecting User-Perceived Latency of On-Device E2E Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/macoskey21b_interspeech.html": {
    "title": "Amortized Neural Networks for Low-Latency Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/botros21_interspeech.html": {
    "title": "Tied & Reduced RNN-T Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21m_interspeech.html": {
    "title": "PQK: Model Compression via Pruning, Quantization, and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nagaraja21_interspeech.html": {
    "title": "Collaborative Training of Acoustic Encoders for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ha_interspeech.html": {
    "title": "Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/parcollet21_interspeech.html": {
    "title": "The Energy and Carbon Footprint of Training End-to-End Speech Recognizers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21v_interspeech.html": {
    "title": "Graph-Based Label Propagation for Semi-Supervised Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21q_interspeech.html": {
    "title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cumani21_interspeech.html": {
    "title": "A Generative Model for Duration-Dependent Score Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pelecanos21_interspeech.html": {
    "title": "Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kataria21b_interspeech.html": {
    "title": "Multi-Channel Speaker Verification for Single and Multi-Talker Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/padfield21_interspeech.html": {
    "title": "Chronological Self-Training for Real-Time Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xiao21b_interspeech.html": {
    "title": "Adaptive Margin Circle Loss for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/obrien21b_interspeech.html": {
    "title": "Presentation Matters: Evaluating Speaker Identification Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tong21_interspeech.html": {
    "title": "Automatic Error Correction for Speaker Embedding Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liao21_interspeech.html": {
    "title": "An Integrated Framework for Two-Pass Personalized Voice Trigger",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lian21_interspeech.html": {
    "title": "Masked Proxy Loss for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21h_interspeech.html": {
    "title": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21p_interspeech.html": {
    "title": "Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sivaprasad21_interspeech.html": {
    "title": "Emotional Prosody Control for Speech Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cong21b_interspeech.html": {
    "title": "Controllable Context-Aware Conversational Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21n_interspeech.html": {
    "title": "Expressive Text-to-Speech Using Style Tag",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21d_interspeech.html": {
    "title": "Adaptive Text to Speech for Spontaneous Style",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21r_interspeech.html": {
    "title": "Towards Multi-Scale Style Control for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21d_interspeech.html": {
    "title": "Cross-Speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tan21_interspeech.html": {
    "title": "Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/an21b_interspeech.html": {
    "title": "Improving Performance of Seen and Unseen Speech Style Transfer in End-to-End Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shechtman21_interspeech.html": {
    "title": "Synthesis of Expressive Speaking Styles with Limited Training Data in a Multi-Speaker, Prosody-Controllable Sequence-to-Sequence Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dao21_interspeech.html": {
    "title": "Intent Detection and Slot Filling for Vietnamese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21k_interspeech.html": {
    "title": "Augmenting Slot Values and Contexts for Spoken Language Understanding with Pretrained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gaspers21_interspeech.html": {
    "title": "The Impact of Intent Distribution Mismatch on Semi-Supervised Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jiang21c_interspeech.html": {
    "title": "Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ia_interspeech.html": {
    "title": "Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-Trained DNN-HMM-Based Acoustic-Phonetic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cha21_interspeech.html": {
    "title": "Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ha_interspeech.html": {
    "title": "End-to-End Cross-Lingual Spoken Language Understanding Model with Multilingual Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saghir21_interspeech.html": {
    "title": "Factorization-Aware Training of Transformers for Natural Language Understanding on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saxon21_interspeech.html": {
    "title": "End-to-End Spoken Language Understanding for Generalized Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21f_interspeech.html": {
    "title": "Bi-Directional Joint Neural Networks for Intent Classification and Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cutler21_interspeech.html": {
    "title": "INTERSPEECH 2021 Acoustic Echo Cancellation Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pfeifenberger21_interspeech.html": {
    "title": "Acoustic Echo Cancellation with Cross-Domain Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ia_interspeech.html": {
    "title": "F-T-LSTM Based Complex Network for Joint Acoustic Echo Cancellation and Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seidel21_interspeech.html": {
    "title": "Y",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21f_interspeech.html": {
    "title": "Acoustic Echo Cancellation Using Deep Complex Neural Network with Nonlinear Magnitude Compression and Phase Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ivry21_interspeech.html": {
    "title": "Nonlinear Acoustic Echo Cancellation with Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/green21_interspeech.html": {
    "title": "Automatic Speech Recognition of Disordered Speech: Personalized Models Outperforming Human Listeners on Short Phrases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/neumann21b_interspeech.html": {
    "title": "Investigating the Utility of Multimodal Conversational Technology and Audiovisual Analytic Measures for the Assessment and Monitoring of Amyotrophic Lateral Sclerosis at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hermann21_interspeech.html": {
    "title": "Handling Acoustic Variation in Dysarthric Speech Recognition Systems Through Model Combination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/geng21b_interspeech.html": {
    "title": "Spectro-Temporal Deep Features for Disordered Speech Assessment and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gutz21_interspeech.html": {
    "title": "Speaking with a KN95 Face Mask: ASR Performance and Speaker Compensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jin21_interspeech.html": {
    "title": "Adversarial Data Augmentation for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xie21b_interspeech.html": {
    "title": "Variational Auto-Encoder Based Variability Encoding for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ja_interspeech.html": {
    "title": "Learning Explicit Prosody Models and Deep Speaker Embeddings for Atypical Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21d_interspeech.html": {
    "title": "Bayesian Parametric and Architectural Domain Adaptation of LF-MMI Trained TDNNs for Elderly and Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cai21c_interspeech.html": {
    "title": "A Voice-Activated Switch for Persons with Motor and Speech Impairments: Isolated-Vowel Spotting Using Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21w_interspeech.html": {
    "title": "Conformer Parrotron: A Faster and Stronger End-to-End Speech Conversion and Recognition Model for Atypical Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/macdonald21_interspeech.html": {
    "title": "Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project Euphonia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yeo21_interspeech.html": {
    "title": "Automatic Severity Classification of Korean Dysarthric Speech Using Phoneme-Level Pronunciation Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/venugopalan21_interspeech.html": {
    "title": "Comparing Supervised Models and Learned Speech Representations for Classifying Intelligibility of Disordered Speech on Selected Phrases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mitra21_interspeech.html": {
    "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kawahara21b_interspeech.html": {
    "title": "Interactive and Real-Time Acoustic Measurement Tools for Speech Data Acquisition and Presentation: Application of an Extended Member of Time Stretched Pulses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tihelka21_interspeech.html": {
    "title": "Save Your Voice: Voice Banking and TTS for Anyone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ja_interspeech.html": {
    "title": "NeMo (Inverse) Text Normalization: From Development to Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hembise21_interspeech.html": {
    "title": "Lalilo: A Reading Assistant for Children Featuring Speech Recognition-Based Reading Mistake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21f_interspeech.html": {
    "title": "Automatic Radiology Report Editing Through Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21e_interspeech.html": {
    "title": "WittyKiddy: Multilingual Spoken Language Learning for Kids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jin21b_interspeech.html": {
    "title": "Duplex Conversation in Outbound Agent System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/udupa21b_interspeech.html": {
    "title": "Web Interface for Estimating Articulatory Movements in Speech Production from Acoustics and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}