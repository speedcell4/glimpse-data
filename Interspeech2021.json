{
  "https://www.isca-speech.org/archive/interspeech_2021/pucher21_interspeech.html": {
    "title": "Conversion of Airborne to Bone-Conducted Speech with Deep Neural Networks",
    "volume": "main",
    "abstract": "It is a common experience of most speakers that the playback of one's own voice sounds strange. This can be mainly attributed to the missing bone-conducted speech signal that is not present in the playback signal. It was also shown that some phonemes have a high bone-conducted relative to air-conducted sound transmission, which means that the bone-conduction filter is phone-dependent. To achieve such a phone-dependent modeling we train different speaker dependent and speaker adaptive speech conversion systems using airborne and bone-conducted speech data from 8 speakers (5 male, 3 female), which allow for the conversion of airborne speech to bone-conducted speech. The systems are based on Long Short-Term Memory (LSTM) deep neural networks, where the speaker adaptive versions with speaker embedding can be used without bone-conduction signals from the target speaker. Additionally we also used models that apply a global filtering. The different models are then evaluated by an objective error metric and a subjective listening experiment, which show that the LSTM based models outperform the global filters",
    "checked": true,
    "id": "fa04146b59a5a17c00b28c223b415b4df45f0991",
    "semantic_title": "conversion of airborne to bone-conducted speech with deep neural networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rezackova21_interspeech.html": {
    "title": "T5G2P: Using Text-to-Text Transfer Transformer for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Despite the increasing popularity of end-to-end text-to-speech (TTS) systems, the correct grapheme-to-phoneme (G2P) module is still a crucial part of those relying on a phonetic input. In this paper, we, therefore, introduce a T5G2P model, a Text-to-Text Transfer Transformer (T5) neural network model which is able to convert an input text sentence into a phoneme sequence with a high accuracy. The evaluation of our trained T5 model is carried out on English and Czech, since there are different specific properties of G2P, including homograph disambiguation, cross-word assimilation and irregular pronunciation of loanwords. The paper also contains an analysis of a homographs issue in English and offers another approach to Czech phonetic transcription using the detection of pronunciation exceptions",
    "checked": true,
    "id": "8ca9702be6bb1a9af323b778ad96894db284313e",
    "semantic_title": "t5g2p: using text-to-text transfer transformer for grapheme-to-phoneme conversion",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/perrotin21_interspeech.html": {
    "title": "Evaluating the Extrapolation Capabilities of Neural Vocoders to Extreme Pitch Values",
    "volume": "main",
    "abstract": "Neural vocoders are systematically evaluated on homogeneous train and test databases. This kind of evaluation is efficient to compare neural vocoders in their \"comfort zone\", yet it hardly reveals their limits towards unseen data during training. To compare their extrapolation capabilities, we introduce a methodology that aims at quantifying the robustness of neural vocoders in synthesising unseen data, by precisely controlling the ranges of seen/unseen data in the training database. By focusing in this study on the pitch (F ) parameter, our methodology involves a careful splitting of a dataset to control which F values are seen/unseen during training, followed by both global (utterance) and local (frame) evaluation of vocoders. Comparison of four types of vocoders (autoregressive, sourcefilter, flows, GAN) displays a wide range of behaviour towards unseen input pitch values, including excellent extrapolation (WaveGlow); widely-spread F errors (WaveRNN); and systematic generation of the training set median F (LPCNet, Parallel WaveGAN). In contrast, fewer differences between vocoders were observed when using homogeneous train and test sets, thus demonstrating the potential and need for such evaluation to better discriminate the neural vocoders abilities to generate out-of-training-range data",
    "checked": true,
    "id": "2a1d555165bbde06837b0b8eca013f59f54c7ed7",
    "semantic_title": "evaluating the extrapolation capabilities of neural vocoders to extreme pitch values",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/do21_interspeech.html": {
    "title": "A Systematic Review and Analysis of Multilingual Data Strategies in Text-to-Speech for Low-Resource Languages",
    "volume": "main",
    "abstract": "We provide a systematic review of past studies that use multilingual data for text-to-speech (TTS) of low-resource languages (LRLs). We focus on the strategies used by these studies for incorporating multilingual data and how they affect output speech quality. To investigate the difference in output quality between corresponding monolingual and multilingual models, we propose a novel measure to compare this difference across the included studies and their various evaluation metrics. This measure, called the Multilingual Model Effect (MLME), is found to be affected by: acoustic model architecture, the difference ratio of target language data between corresponding multilingual and monolingual experiments, the balance ratio of target language data to total data, and the amount of target language data used. These findings can act as reference for data strategies in future experiments with multilingual TTS models for LRLs. Language family classification, despite being widely used, is not found to be an effective criterion for selecting source languages",
    "checked": true,
    "id": "016064c068dfe3dd3ae8b2a8c8940c2a4e2db866",
    "semantic_title": "a systematic review and analysis of multilingual data strategies in text-to-speech for low-resource languages",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/talkar21_interspeech.html": {
    "title": "Acoustic Indicators of Speech Motor Coordination in Adults With and Without Traumatic Brain Injury",
    "volume": "main",
    "abstract": "A traumatic brain injury (TBI) can lead to various long-term effects on memory, attention, and mood, as well as the occurrence of headaches, speech, and hearing problems. There is a need to better understand the long-term effects of a TBI for objective tracking of an individual's recovery, which could be used to determine intervention trajectories. This study utilizes acoustic features derived from recordings of speech tasks completed by active-duty service members and veterans (SMVs) enrolled in the Defense and Veterans Brain Injury (DVBIC)/Traumatic Brain Injury Center of Excellence (TBICoE) 15-Year Longitudinal TBI Study. We hypothesize that the individuals diagnosed with moderate to severe TBI would demonstrate motor speech impairments through decreased coordination of the speech production subsystems as compared to individuals with no history of TBI. Speech motor coordination is measured through correlations of acoustic feature time series representing speech subsystems. Eigenspectra derived from these correlations are utilized in machine learning models to discriminate between the two groups. The fusion of correlation features derived from the recordings achieves an AUC of 0.78. This suggests that residual motor impairments from moderate to severe TBI could be detectable through objective measures of speech motor coordination",
    "checked": true,
    "id": "29a7bc914934229c5c190d4e7e5cb2b17aa9cbc8",
    "semantic_title": "acoustic indicators of speech motor coordination in adults with and without traumatic brain injury",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vasquezcorrea21_interspeech.html": {
    "title": "On Modeling Glottal Source Information for Phonation Assessment in Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's disease produces several motor symptoms, including different speech impairments that are known as hypokinetic dysarthria. Symptoms associated to dysarthria affect different dimensions of speech such as phonation, articulation, prosody, and intelligibility. Studies in the literature have mainly focused on the analysis of articulation and prosody because they seem to be the most prominent symptoms associated to dysarthria severity. However, phonation impairments also play a significant role to evaluate the global speech severity of Parkinson's patients. This paper proposes an extensive comparison of different methods to automatically evaluate the severity of specific phonation impairments in Parkinson's patients. The considered models include the computation of perturbation and glottal-based features, in addition to features extracted from a zero frequency filtered signals. We consider as well end-to-end models based on 1D CNNs, which are trained to learn features from the raw speech waveform, reconstructed glottal signals, and zero-frequency filtered signals. The results indicate that it is possible to automatically classify between speakers with low versus high phonation severity due to the presence of dysarthria and at the same time to evaluate the severity of the phonation impairments on a continuous scale, posed as a regression problem",
    "checked": true,
    "id": "7dfa3283092c0c780cff9228153196b2cb84d7d6",
    "semantic_title": "on modeling glottal source information for phonation assessment in parkinson's disease",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/daoudi21_interspeech.html": {
    "title": "Distortion of Voiced Obstruents for Differential Diagnosis Between Parkinson's Disease and Multiple System Atrophy",
    "volume": "main",
    "abstract": "Parkinson's disease (PD) and the parkinsonian variant of Multiple System Atrophy (MSA-P) are two neurodegenerative diseases which share similar clinical features, particularly in early disease stages. The differential diagnosis can be thus very challenging. Dysarthria is known to be a frequent and early clinical feature of PD and MSA. It can be thus used as a vehicle to provide a vocal biomarker which could help in the differential diagnosis. In particular, distortion of consonants is known to be a frequent impairment in these diseases. The aim of this study is to investigate distinctive patterns in the distortion of voiced obstruents (plosives and fricatives). It is the first study which attempts to examine such distortions in the French language for the purpose of the differential diagnosis between PD and MSA-P (and among the very few studies if we consider all languages). We carry out a perceptual and objective analysis of voiced obstruents extracted from isolated pseudo-words initials. We first show that devoicing is a significant impairment which predominates in MSA-P. We then show that voice onset time (VOT) of voiced plosives (prevoicing duration) can be a complementary feature to improve the accuracy in discrimination between PD and MSA-P",
    "checked": true,
    "id": "07b9da3899e66c7dbd79de23a294811f8b1c34dc",
    "semantic_title": "distortion of voiced obstruents for differential diagnosis between parkinson's disease and multiple system atrophy",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21_interspeech.html": {
    "title": "A Study into Pre-Training Strategies for Spoken Language Understanding on Dysarthric Speech",
    "volume": "main",
    "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems avoid an intermediate textual representation by mapping speech directly into intents with slot values. This approach requires considerable domain-specific training data. In low-resource scenarios this is a major concern, e.g., in the present study dealing with SLU for dysarthric speech. Pretraining part of the SLU model for automatic speech recognition targets helps but no research has shown to which extent SLU on dysarthric speech benefits from knowledge transferred from other dysarthric speech tasks. This paper investigates the efficiency of pre-training strategies for SLU tasks on dysarthric speech. The designed SLU system consists of a TDNN acoustic model for feature encoding and a capsule network for intent and slot decoding. The acoustic model is pre-trained in two stages: initialization with a corpus of normal speech and finetuning on a mixture of dysarthric and normal speech. By introducing the intelligibility score as a metric of the impairment severity, this paper quantitatively analyzes the relation between generalization and pathology severity for dysarthric speech",
    "checked": true,
    "id": "c19084b61d05c32320a6caaa739eb68c4eb9d07a",
    "semantic_title": "a study into pre-training strategies for spoken language understanding on dysarthric speech",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/turrisi21_interspeech.html": {
    "title": "EasyCall Corpus: A Dysarthric Speech Dataset",
    "volume": "main",
    "abstract": "This paper introduces a new dysarthric speech command dataset in Italian, called EasyCall corpus. The dataset consists of 21386 audio recordings from 24 healthy and 31 dysarthric speakers, whose individual degree of speech impairment was assessed by neurologists through the Therapy Outcome Measure. The corpus aims at providing a resource for the development of ASR-based assistive technologies for patients with dysarthria. In particular, it may be exploited to develop a voice-controlled contact application for commercial smartphones, aiming at improving dysarthric patients' ability to communicate with their family and caregivers. Before recording the dataset, participants were administered a survey to evaluate which commands are more likely to be employed by dysarthric individuals in a voice-controlled contact application. In addition, the dataset includes a list of non-commands (i.e., words near/inside commands or phonetically close to commands) that can be leveraged to build a more robust command recognition system. At present commercial ASR systems perform poorly on the EasyCall Corpus as we report in this paper. This result corroborates the need for dysarthric speech corpora for developing effective assistive technologies. To the best of our knowledge, this database represents the richest corpus of dysarthric speech to date",
    "checked": true,
    "id": "6524f8f7692c43e000bea2f68677a0b3dae69015",
    "semantic_title": "easycall corpus: a dysarthric speech dataset",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bie21_interspeech.html": {
    "title": "A Benchmark of Dynamical Variational Autoencoders Applied to Speech Spectrogram Modeling",
    "volume": "main",
    "abstract": "The Variational Autoencoder (VAE) is a powerful deep generative model that is now extensively used to represent high-dimensional complex data via a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, input data vectors are processed independently. In recent years, a series of papers have presented different extensions of the VAE to process sequential data, that not only model the latent space, but also model the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks. We recently performed a comprehensive review of those models and unified them into a general class called Dynamical Variational Autoencoders (DVAEs). In the present paper, we present the results of an experimental benchmark comparing six of those DVAE models on the speech analysis-resynthesis task, as an illustration of the high potential of DVAEs for speech modeling",
    "checked": true,
    "id": "5ace2ad34b19f668232a45806429ab0bfbd2c387",
    "semantic_title": "a benchmark of dynamical variational autoencoders applied to speech spectrogram modeling",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yurt21_interspeech.html": {
    "title": "Fricative Phoneme Detection Using Deep Neural Networks and its Comparison to Traditional Methods",
    "volume": "main",
    "abstract": "Accurate phoneme detection and processing can enhance speech intelligibility in hearing aids and audio & speech codecs. As fricative phonemes have an important part of their energy concentrated in high frequency bands, frequency lowering algorithms are used in hearing aids to improve fricative intelligibility for people with high-frequency hearing loss. In traditional audio codecs, while processing speech in blocks, spectral smearing around fricative phoneme borders results in pre and post echo artifacts. Hence, detecting the fricative borders and adapting the processing accordingly could enhance the quality of speech. Until recently, phoneme detection and analysis were mostly done by extracting features specific to the class of phonemes. In this paper, we present a deep learning based fricative phoneme detection algorithm that exceeds the state-of-the-art fricative phoneme detection accuracy on the TIMIT speech corpus. Moreover, we compare our method to other approaches that employ classical signal processing for fricative detection and also evaluate it on the TIMIT files coded with AAC codec followed by bandwidth limitation. Reported results of our deep learning approach on original TIMIT files are reproducible and come with an easy to use code that could serve as a baseline for any future research on this topic",
    "checked": true,
    "id": "e8bd30368ce8d8103b4404bbc5bb1f619743c541",
    "semantic_title": "fricative phoneme detection using deep neural networks and its comparison to traditional methods",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/prasad21_interspeech.html": {
    "title": "Identification of F1 and F2 in Speech Using Modified Zero Frequency Filtering",
    "volume": "main",
    "abstract": "Formants are major resonances in the vocal tract system. Identification of formants is important for study of speech. In the literature, formants are typically identified by first deriving formant frequency candidates (e.g., using linear prediction) and then applying a tracking mechanism. In this paper, we propose a simple tracking-free formant identification approach based on zero frequency filtering. More precisely, formants F1-F2 are identified by modifying the trend removal operation in zero frequency filtering and picking simply the dominant peak in the short-term discrete Fourier transform spectra. We demonstrate the potential of the approach by comparing it against state-of-the-art formant identification approaches on a typical speech data set (TIMIT-VTR) and an atypical speech data set (PC-GITA)",
    "checked": true,
    "id": "418936459718355b4c59a3b0c7698882428639c3",
    "semantic_title": "identification of f1 and f2 in speech using modified zero frequency filtering",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/teytaut21_interspeech.html": {
    "title": "Phoneme-to-Audio Alignment with Recurrent Neural Networks for Speaking and Singing Voice",
    "volume": "main",
    "abstract": "Phoneme-to-audio alignment is the task of synchronizing voice recordings and their related phonetic transcripts. In this work, we introduce a new system to forced phonetic alignment with Recurrent Neural Networks (RNN). With the Connectionist Temporal Classification (CTC) loss as training objective, and an additional reconstruction cost, we learn to infer relevant per-frame phoneme probabilities from which alignment is derived. The core of the neural architecture is a context-aware attention mechanism between mel-spectrograms and side information. We investigate two contexts given by either phoneme sequences (model PhAtt) or spectrograms themselves (model SpAtt). Evaluations show that these models produce precise alignments for both speaking and singing voice. Best results are obtained with the model PhAtt, which outperforms baseline reference with an average imprecision of 16.3ms and 29.8ms on speech and singing, respectively. The model SpAtt also appears as an interesting alternative, capable of aligning longer audio files without requiring phoneme sequences on small audio segments",
    "checked": true,
    "id": "bfba22e6b5774780e18fb348db365df211280f2a",
    "semantic_title": "phoneme-to-audio alignment with recurrent neural networks for speaking and singing voice",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21_interspeech.html": {
    "title": "Adaptive Convolutional Neural Network for Text-Independent Speaker Recognition",
    "volume": "main",
    "abstract": "In text-independent speaker recognition, each speech is composed of different phonemes depending on spoken text. The conventional neural networks for speaker recognition are static models, so they do not reflect this phoneme-varying characteristic well. To tackle this limitation, we propose an adaptive convolutional neural network (ACNN) for text-independent speaker recognition. The utterance is divided along the time axis into short segments with small fluctuating phonemes. Frame-level features are extracted by applying input-dependent kernels adaptive to each segment. By applying time average pooling and linear layers, utterance-level embeddings extraction and speaker recognition are performed. Adaptive VGG-M using 0.356 seconds segmentation shows better speaker recognition performance than baseline models, with a Top-1 of 86.51% and an EER of 5.68%. It extracts more accurate frame-level embeddings for vowel and nasal phonemes compared to the conventional method without overfitting and large parameters. This framework for text-independent speaker recognition effectively utilizes phonemes and text-varying characteristic of speech",
    "checked": true,
    "id": "4c653bd510d5c1bbf2e5690e7075c1b5b6ce104f",
    "semantic_title": "adaptive convolutional neural network for text-independent speaker recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qi21_interspeech.html": {
    "title": "Bidirectional Multiscale Feature Aggregation for Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we propose a novel bidirectional multiscale feature aggregation (BMFA) network with attentional fusion modules for text-independent speaker verification. The feature maps from different stages of the backbone network are iteratively combined and refined in both a bottom-up and top-down manner. Furthermore, instead of simple concatenation or elementwise addition of feature maps from different stages, an attentional fusion module is designed to compute the fusion weights. Experiments are conducted on the NIST SRE16 and VoxCeleb1 datasets. The experimental results demonstrate the effectiveness of the bidirectional aggregation strategy and show that the proposed attentional fusion module can further improve the performance",
    "checked": true,
    "id": "588856b77b4e8709746dda0db3ed5e626bf87aba",
    "semantic_title": "bidirectional multiscale feature aggregation for speaker verification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21_interspeech.html": {
    "title": "Improving Time Delay Neural Network Based Speaker Recognition with Convolutional Block and Feature Aggregation Methods",
    "volume": "main",
    "abstract": "In this paper, we develop a system that integrates multiple ideas and techniques inspired by the convolutional block and feature aggregation methods. We begin with the state-of-the-art speaker-embedding model for speaker recognition, namely the model of Emphasized Channel Attention, Propagation, and Aggregation in Time Delay Neural Network, and then gradually experiment with the proposed network modules, including bottleneck residual blocks, attention mechanisms, and feature aggregation methods. In our final model, we replace the Res2Block with SC-Block and we use a hierarchical architecture for feature aggregation. We evaluate the performance of our model on the VoxCeleb1 test set and the 2020 VoxCeleb Speaker Recognition Challenge (VoxSRC20) validation set. The relative improvement of the proposed models over ECAPA-TDNN is 22.8% on VoxCeleb1 and 18.2% on VoxSRC20",
    "checked": true,
    "id": "1b4de223d5393bb2abac0ecb6e46de0a020ad18e",
    "semantic_title": "improving time delay neural network based speaker recognition with convolutional block and feature aggregation methods",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21_interspeech.html": {
    "title": "Improving Deep CNN Architectures with Variable-Length Training Samples for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Deep Convolutional Neural Network (CNN) based speaker embeddings, such as r-vectors, have shown great success in text-independent speaker verification (TI-SV) task. However, previous deep CNN models usually use fixed-length samples for training and employ variable-length utterances for speaker embeddings, which generates a mismatch between training and embedding. To address this issue, we investigate the effect of employing variable-length training samples on CNN-based TI-SV systems and explore two approaches to improve the performance of deep CNN architectures on TI-SV through capturing variable-term contexts. Firstly, we present an improved selective kernel convolution which allows the networks to adaptively switch between short-term and long-term contexts based on variable-length utterances. Secondly, we propose a multi-scale statistics pooling method to aggregate multiple time-scale features from different layers of the networks. We build a novel ResNet34 based architecture with two proposed approaches. Experiments are conducted on the VoxCeleb datasets. The results demonstrate that the effect of using variable-length samples is diverse in different networks and the architecture with two proposed approaches achieves significant improvement over r-vectors baseline system",
    "checked": true,
    "id": "899172bf6b54aeb4359fcf6c18d50b63358f34b6",
    "semantic_title": "improving deep cnn architectures with variable-length training samples for text-independent speaker verification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21_interspeech.html": {
    "title": "Binary Neural Network for Speaker Verification",
    "volume": "main",
    "abstract": "Although deep neural networks are successful for many tasks in the speech domain, the high computational and memory costs of deep neural networks make it difficult to directly deploy high-performance Neural Network systems on low-resource embedded devices. There are several mechanisms to reduce the size of the neural networks i.e. parameter pruning, parameter quantization, etc. This paper focuses on how to apply binary neural networks to the task of speaker verification. The proposed binarization of training parameters can largely maintain the performance while significantly reducing storage space requirements and computational costs. Experiment results show that, after binarizing the Convolutional Neural Network, the ResNet34-based network achieves an EER of around 5% on the Voxceleb1 testing dataset and even outperforms the traditional real number network on the text-dependent dataset: Xiaole while having a 32Ã— memory saving",
    "checked": true,
    "id": "08b1a85ca48ca7f723b0b5e331c89561158d7b2e",
    "semantic_title": "binary neural network for speaker verification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tu21_interspeech.html": {
    "title": "Mutual Information Enhanced Training for Speaker Embedding",
    "volume": "main",
    "abstract": "Mutual information (MI) is useful in unsupervised and self-supervised learning. Maximizing the MI between the low-level features and the learned embeddings can preserve meaningful information in the embeddings, which can contribute to performance gains. This strategy is called deep InfoMax (DIM) in representation learning. In this paper, we follow the DIM framework so that the speaker embeddings can capture more information from the frame-level features. However, a straightforward implementation of DIM may pose a dimensionality imbalance problem because the dimensionality of the frame-level features is much larger than that of the speaker embeddings. This problem can lead to unreliable MI estimation and can even cause detrimental effects on speaker verification. To overcome this problem, we propose to squeeze the frame-level features before MI estimation through some global pooling methods. We call the proposed method squeeze-DIM. Although the squeeze operation inevitably introduces some information loss, we empirically show that the squeeze-DIM can achieve performance gains on both Voxceleb1 and VOiCES-19 tasks. This suggests that the squeeze operation facilitates the MI estimation and maximization in a balanced dimensional space, which helps learn more informative speaker embeddings",
    "checked": true,
    "id": "289c4486a6f54644b2c6285d6640bb30ba71fbbe",
    "semantic_title": "mutual information enhanced training for speaker embedding",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21b_interspeech.html": {
    "title": "Y-Vector: Multiscale Waveform Encoder for Speaker Embedding",
    "volume": "main",
    "abstract": "State-of-the-art text-independent speaker verification systems typically use cepstral features or filter bank energies as speech features. Recent studies attempted to extract speaker embeddings directly from raw waveforms and have shown competitive results. In this paper, we propose a novel multi-scale waveform encoder that uses three convolution branches with different time scales to compute speech features from the waveform. These features are then processed by squeeze-and-excitation blocks, a multi-level feature aggregator, and a time delayed neural network (TDNN) to compute speaker embedding. We show that the proposed embeddings outperform existing raw-waveform-based speaker embeddings on speaker verification by a large margin. A further analysis of the learned filters shows that the multi-scale encoder attends to different frequency bands at its different scales while resulting in a more flat overall frequency response than any of the single-scale counterparts",
    "checked": true,
    "id": "165c4715fdf6a7a0229f2810f2d32a526444b216",
    "semantic_title": "y-vector: multiscale waveform encoder for speaker embedding",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21_interspeech.html": {
    "title": "Phoneme-Aware and Channel-Wise Attentive Learning for Text Dependent Speaker Verification",
    "volume": "main",
    "abstract": "This paper proposes a multi-task learning network with phoneme-aware and channel-wise attentive learning strategies for text-dependent Speaker Verification (SV). In the proposed structure, the frame-level multi-task learning along with the segment-level adversarial learning is adopted for speaker embedding extraction. The phoneme-aware attentive pooling is exploited on frame-level features in the main network for speaker classifier, with the corresponding posterior probability for the phoneme distribution in the auxiliary subnet. Further, the introduction of Squeeze and Excitation (SE-block) performs dynamic channel-wise feature recalibration, which improves the representational ability. The proposed method exploits speaker idiosyncrasies associated with pass-phrases, and is further improved by the phoneme-aware attentive pooling and SE-block from temporal and channel-wise aspects, respectively. The experiments conducted on RSR2015 Part 1 database confirm that the proposed system achieves outstanding results for text-dependent SV",
    "checked": false,
    "id": "923a03112032a81c77ed3e8f4498e60225263442",
    "semantic_title": "phoneme-aware and channel-wise attentive learning for text dependentspeaker verification",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21c_interspeech.html": {
    "title": "Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding",
    "volume": "main",
    "abstract": "This paper proposes a serialized multi-layer multi-head attention for neural speaker embedding in text-independent speaker verification. In prior works, frame-level features from one layer are aggregated to form an utterance-level representation. Inspired by the Transformer network, our proposed method utilizes the hierarchical architecture of stacked self-attention mechanisms to derive refined features that are more correlated with speakers. Serialized attention mechanism contains a stack of self-attention modules to create fixed-dimensional representations of speakers. Instead of utilizing multi-head attention in parallel, the proposed serialized multi-layer multi-head attention is designed to aggregate and propagate attentive statistics from one layer to the next in a serialized manner. In addition, we employ an input-aware query for each utterance with the statistics pooling. With more layers stacked, the neural network can learn more discriminative speaker embeddings. Experiment results on VoxCeleb1 dataset and SITW dataset show that our proposed method outperforms other baseline methods, including x-vectors and other x-vectors + conventional attentive pooling approaches by 9.7% in EER and 8.1% in DCF10 ",
    "checked": true,
    "id": "c3241cacfbb0ef38b87444634652a9202ac3da4e",
    "semantic_title": "serialized multi-layer multi-head attention for neural speaker embedding",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21_interspeech.html": {
    "title": "TacoLPCNet: Fast and Stable TTS by Conditioning LPCNet on Mel Spectrogram Predictions",
    "volume": "main",
    "abstract": "The combination of the recently proposed LPCNet vocoder and a seq-to-seq acoustic model, i.e., Tacotron, has successfully achieved lightweight speech synthesis systems. However, the quality of synthesized speech is often unstable because the precision of the pitch parameters predicted by acoustic models is insufficient, especially for some tonal languages like Chinese and Japanese. In this paper, we propose an end-to-end speech synthesis system, TacoLPCNet, by conditioning LPCNet on Mel spectrogram predictions. First, we extend LPCNet for the Mel spectrogram instead of using explicit pitch information and pitch-related network. Furthermore, we optimize the system by model pruning, multi-frame inference, and increasing frame length, to enable it to meet the conditions required for real-time applications. The objective and subjective evaluation results for various languages show that the proposed system is more stable for tonal languages within the proposed optimization strategies. The experimental results also verify that our model improves synthesis runtime by 3.12 times than that of the baseline on a standard CPU while maintaining naturalness",
    "checked": true,
    "id": "c84bec97770c7851af0516f18bf1121d780897d1",
    "semantic_title": "tacolpcnet: fast and stable tts by conditioning lpcnet on mel spectrogram predictions",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bak21_interspeech.html": {
    "title": "FastPitchFormant: Source-Filter Based Decomposed Modeling for Speech Synthesis",
    "volume": "main",
    "abstract": "Methods for modeling and controlling prosody with acoustic features have been proposed for neural text-to-speech (TTS) models. Prosodic speech can be generated by conditioning acoustic features. However, synthesized speech with a large pitch-shift scale suffers from audio quality degradation, and speaker characteristics deformation. To address this problem, we propose a feed-forward Transformer based TTS model that is designed based on the source-filter theory. This model, called , has a unique structure that handles text and acoustic features in parallel. With modeling each feature separately, the tendency that the model learns the relationship between two features can be mitigated. Owing to its structural characteristics, FastPitchFormant is robust and accurate for pitch control and generates prosodic speech preserving speaker characteristics. The experimental results show that proposed model outperforms the baseline FastPitch",
    "checked": true,
    "id": "034869f2f55b01f240b30923983ea197ff9fc32c",
    "semantic_title": "fastpitchformant: source-filter based decomposed modeling for speech synthesis",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nakamura21_interspeech.html": {
    "title": "Sequence-to-Sequence Learning for Deep Gaussian Process Based Speech Synthesis Using Self-Attention GP Layer",
    "volume": "main",
    "abstract": "This paper presents a speech synthesis method based on deep Gaussian process (DGP) and sequence-to-sequence (Seq2Seq) learning toward high-quality end-to-end speech synthesis. Feed-forward and recurrent models using DGP are known to produce more natural synthetic speech than deep neural networks (DNNs) because of Bayesian learning and kernel regression. However, such DGP models consist of a pipeline architecture of independent models, acoustic and duration models, and require a high level of expertise in text processing. The proposed model is based on Seq2Seq learning, which enables a unified training of acoustic and duration models. The encoder and decoder layers are represented by Gaussian process regressions (GPRs) and the parameters are trained as a Bayesian model. We also propose a self-attention mechanism with Gaussian processes to effectively model character-level input in the encoder. The subjective evaluation results show that the proposed Seq2Seq-SA-DGP can synthesize more natural speech than DNNs with self-attention and recurrent structures. Besides, Seq2Seq-SA-DGP reduces the smoothing problems of recurrent structures and is effective when a simple input for an end-to-end system is given",
    "checked": true,
    "id": "9758aae98703469691e454ee622e5201ebfc7ae3",
    "semantic_title": "sequence-to-sequence learning for deep gaussian process based speech synthesis using self-attention gp layer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kakegawa21_interspeech.html": {
    "title": "Phonetic and Prosodic Information Estimation from Texts for Genuine Japanese End-to-End Text-to-Speech",
    "volume": "main",
    "abstract": "The biggest obstacle to develop end-to-end Japanese text-to-speech (TTS) systems is to estimate phonetic and prosodic information (PPI) from Japanese texts. The following are the reasons: (1) the Kanji characters of the Japanese writing system have multiple corresponding pronunciations, (2) there is no separation mark between words, and (3) an accent nucleus must be assigned at appropriate positions. In this paper, we propose to solve the problems by neural machine translation (NMT) on the basis of encoder-decoder models, and compare NMT models of recurrent neural networks and the Transformer architecture. The proposed model handles texts on token (character) basis, although conventional systems handle them on word basis. To ensure the potential of the proposed approach, NMT models are trained using pairs of sentences and their PPIs that are generated by a conventional Japanese TTS system from 5 million sentences. Evaluation experiments were performed using PPIs that are manually annotated for 5,142 sentences. The experimental results showed that the Transformer architecture has the best performance, with 98.0% accuracy for phonetic information estimation and 95.0% accuracy for PPI estimation. Judging from the results, NMT models are promising toward end-to-end Japanese TTS",
    "checked": true,
    "id": "8fb7bf026379d6e2a51ee95b6dc3aa09ca53b4bc",
    "semantic_title": "phonetic and prosodic information estimation from texts for genuine japanese end-to-end text-to-speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dai21_interspeech.html": {
    "title": "Information Sieve: Content Leakage Reduction in End-to-End Prosody Transfer for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "Expressive neural text-to-speech (TTS) systems incorporate a style encoder to learn a latent embedding as the style information. However, this embedding process may encode redundant textual information. This phenomenon is called content leakage. Researchers have attempted to resolve this problem by adding an ASR or other auxiliary supervision loss functions. In this study, we propose an unsupervised method called the \"information sieve\" to reduce the effect of content leakage in prosody transfer. The rationale of this approach is that the style encoder can be forced to focus on style information rather than on textual information contained in the reference speech by a well-designed downsample-upsample filter, i.e., the extracted style embeddings can be downsampled at a certain interval and then upsampled by duplication. Furthermore, we used instance normalization in convolution layers to help the system learn a better latent style space. Objective metrics such as the significantly lower word error rate (WER) demonstrate the effectiveness of this model in mitigating content leakage. Listening tests indicate that the model retains its prosody transferability compared with the baseline models such as the original GST-Tacotron and ASR-guided Tacotron",
    "checked": false,
    "id": "155e69f30da7726a584bb293c1b8c9ff227d11fe",
    "semantic_title": "information sieve: content leakage reduction in end-to-end prosody for expressive speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dou21_interspeech.html": {
    "title": "Deliberation-Based Multi-Pass Speech Synthesis",
    "volume": "main",
    "abstract": "Sequence-to-sequence (seq2seq) models have achieved state-of-the-art performance in a wide range of tasks including Neural Machine Translation (NMT) and Text-To-Speech (TTS). These models are usually trained with teacher forcing, where the reference back-history is used to predict the next token. This makes training efficient, but limits performance, because during inference the free-running back-history must be used. To address this problem, deliberation-based multi-pass seq2seq has been used in NMT. Here the output sequence is generated in multiple passes, each one conditioned on the initial input and the free-running output of the previous pass. This paper investigates, and compares, deliberation-based multi-pass seq2seq for TTS and NMT. For NMT the simplest form of multi-pass approaches, where the free-running first-pass output is combined with the initial input, improves performance. However, applying this scheme to TTS is challenging: the multi-pass model tends to converge to the standard single-pass model, ignoring the previous output. To tackle this issue, a guided attention loss is added, enabling the system to make more extensive use of the free-running output. Experimental results confirm the above analysis and demonstrate that the proposed TTS model outperforms a strong baseline",
    "checked": true,
    "id": "b8233798ff4bd3b98c884a61b84e1fa02fd1aefa",
    "semantic_title": "deliberation-based multi-pass speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elias21_interspeech.html": {
    "title": "Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling",
    "volume": "main",
    "abstract": "This paper introduces , a non-autoregressive neural text-to-speech model with a fully differentiable duration model which does not require supervised duration signals. The duration model is based on a novel attention mechanism and an iterative reconstruction loss based on Soft Dynamic TimeWarping, this model can learn token-frame alignments as well as token durations automatically. Experimental results show that Parallel Tacotron 2 outperforms baselines in subjective naturalness in several diverse multi speaker evaluations",
    "checked": true,
    "id": "d500d4147509749e10d388fd4900372d01a7c5df",
    "semantic_title": "parallel tacotron 2: a non-autoregressive neural tts model with differentiable duration modeling",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21b_interspeech.html": {
    "title": "Transformer-Based Acoustic Modeling for Streaming Speech Synthesis",
    "volume": "main",
    "abstract": "Transformer models have shown promising results in neural speech synthesis due to their superior ability to model long-term dependencies compared to recurrent networks. The computation complexity of transformers increases quadratically with sequence length, making it impractical for many real-time applications. To address the complexity issue in speech synthesis domain, this paper proposes an efficient transformer-based acoustic model that is constant-speed regardless of input sequence length, making it ideal for streaming speech synthesis applications. The proposed model uses a transformer network that predicts the prosody features at phone rate and then an Emformer network to predict the frame-rate spectral features in a streaming manner. Both the transformer and Emformer in the proposed architecture use a self-attention mechanism that involves explicit long-term information, thus providing improved speech naturalness for long utterances. In our experiments, we use a WaveRNN neural vocoder that takes in the predicted spectral features and generates the final audio. The overall architecture achieves human-like speech quality both on short and long utterances while maintaining a low latency and low real-time factor. Our mean opinion score (MOS) evaluation shows that for short utterances, the proposed model achieves a MOS of 4.213 compared to ground-truth with MOS of 4.307; and for long utterances, it also produces high-quality speech with a MOS of 4.201 compared to ground-truth with MOS of 4.360",
    "checked": true,
    "id": "3b0e51ad9b89f87538f6256cda0b7a22dd71a0b8",
    "semantic_title": "transformer-based acoustic modeling for streaming speech synthesis",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jia21_interspeech.html": {
    "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS",
    "volume": "main",
    "abstract": "This paper introduces , a new encoder model for neural TTS. This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them. It can be pre-trained on a large text corpus in a self-supervised manner, and fine-tuned in a TTS task. Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training. Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers",
    "checked": true,
    "id": "c437b1f260f294af628483d77a49239fa613aa89",
    "semantic_title": "png bert: augmented bert on phonemes and graphemes for neural tts",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ge21_interspeech.html": {
    "title": "Speed up Training with Variable Length Inputs by Efficient Batching Strategies",
    "volume": "main",
    "abstract": "In the model training with neural networks, although the model performance is always the first priority to optimize, training efficiency also plays an important role in model deployment. There are many ways to speed up training with minimal performance loss, such as training with more GPUs, or with mixed precisions, optimizing training parameters, or making features more compact but more representable. Since mini-batch training is now the go-to approach for many machine learning tasks, minimizing the zero-padding to incorporate samples of different lengths into one batch, is an alternative approach to save training time. Here we propose a batching strategy based on semi-sorted samples, with dynamic batch sizes and batch randomization. By replacing the random batching with the proposed batching strategies, it saves more than 40% training time without compromising performance in training seq2seq neural text-to-speech models based on the Tacotron framework. We also compare it with two other batching strategies and show it performs similarly in terms of saving time and maintaining performance, but with a simpler concept and a smoother tuning parameter to balance between zero-padding and randomness level",
    "checked": true,
    "id": "e8aea5f6152a76f6c552789343c248873a204246",
    "semantic_title": "speed up training with variable length inputs by efficient batching strategies",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sun21_interspeech.html": {
    "title": "Funnel Deep Complex U-Net for Phase-Aware Speech Enhancement",
    "volume": "main",
    "abstract": "The emergence of deep neural networks has made speech enhancement well developed. Most of the early models focused on estimating the magnitude of spectrum while ignoring the phase, this gives the evaluation result a certain upper limit. Some recent researches proposed deep complex network, which can handle complex inputs, and realize joint estimation of magnitude spectrum and phase spectrum by outputting real and imaginary parts respectively. The encoder-decoder structure in Deep Complex U-net (DCU) has been proven to be effective for complex-valued data. To further improve the performance, in this paper, we design a new network called Funnel Deep Complex U-net (FDCU), which could process magnitude information and phase information separately through one-encoder-two-decoders structure. Moreover, in order to achieve better training effect, we define negative stretched-SI-SNR as the loss function to avoid errors caused by the negative vector angle. Experimental results show that our FDCU model outperforms state-of-the-art approaches in all evaluation metrics",
    "checked": true,
    "id": "c9ca35b78e5f19eb955d9173ce019adba9045854",
    "semantic_title": "funnel deep complex u-net for phase-aware speech enhancement",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21b_interspeech.html": {
    "title": "Temporal Convolutional Network with Frequency Dimension Adaptive Attention for Speech Enhancement",
    "volume": "main",
    "abstract": "Despite much progress, most temporal convolutional networks (TCN) based speech enhancement models are mainly focused on modeling the long-term temporal contextual dependencies of speech frames, without taking into account the distribution information of speech signal in frequency dimension. In this study, we propose a frequency dimension adaptive attention (FAA) mechanism to improve TCNs, which guides the model selectively emphasize the frequency-wise features with important speech information and also improves the representation capability of network. Our extensive experimental investigation demonstrates that the proposed FAA mechanism is able to consistently provide significant improvements in terms of speech quality (PESQ), intelligibility (STOI) and three other composite metrics. More promisingly, it has better generalization ability to real-world noisy environment",
    "checked": true,
    "id": "30e79104624988e9a4da0c58044ffd59e53f4ab9",
    "semantic_title": "temporal convolutional network with frequency dimension adaptive attention for speech enhancement",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21_interspeech.html": {
    "title": "Perceptual Contributions of Vowels and Consonant-Vowel Transitions in Understanding Time-Compressed Mandarin Sentences",
    "volume": "main",
    "abstract": "Many early studies reported the importance of vowels and vowel-consonant transitions to speech intelligibility. The present work assessed their perceptual impacts to the understanding of time-compressed sentences, which could be used to measure the temporal acuity during speech understanding. Mandarin sentences were edited to selectively preserve vowel centers or vowel-consonant transitional segments, and compress the rest regions with equipment time compression rates (TCRs) up to 3, including conditions only preserving vowel centers or vowel-consonant transitions. The processed stimuli were presented to normal-hearing listeners to recognize. Results showed that, consistent with the segmental contributions in understanding uncompressed speech, the vowel-only time-compressed stimuli were highly intelligible (i.e., intelligibility score >85%) at a TCR around 3, and vowel-consonant transitions carried important intelligibility information in understanding time-compressed sentences. The time-compression conditions in the present work provided higher intelligibility scores than their counterparties in understanding the PSOLA-processed time-compressed sentences with TCRs around 3. The findings in this work suggested that the design of time compression processing could be guided towards selectively preserving perceptually important speech segments (e.g., vowels) in the future",
    "checked": true,
    "id": "3fc7c3e0b439ff6ddf30ea0168f02311e323d1ce",
    "semantic_title": "perceptual contributions of vowels and consonant-vowel transitions in understanding time-compressed mandarin sentences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/biswas21_interspeech.html": {
    "title": "Transfer Learning for Speech Intelligibility Improvement in Noisy Environments",
    "volume": "main",
    "abstract": "In a recent work [1], a novel Delta Function-based Formant Shifting approach was proposed for speech intelligibility improvement. The underlying principle is to dynamically relocate the formants based on their occurrence in the spectrum away from the region of noise. The manner in which the formants are shifted is decided by the parameters of the Delta Function, the optimal values of which are evaluated using Comprehensive Learning Particle Swarm Optimization (CLPSO). Although effective, CLPSO is computationally expensive to the extent that it overshadows its merits in intelligibility improvement. As a solution to this, the current work aims to improve the Short-Time Objective Intelligibility (STOI) of (target) speech using a Delta Function that has been generated using a different (source) language. This transfer learning is based upon the relative positioning of the formant frequencies and pitch values of the source & target language datasets. The proposed approach is demonstrated and validated by subjecting it to experimentation with three different languages under variable noisy conditions",
    "checked": true,
    "id": "89a3c322ce4acf30e2d3298759fc47d2f6e5627b",
    "semantic_title": "transfer learning for speech intelligibility improvement in noisy environments",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yamamoto21_interspeech.html": {
    "title": "Comparison of Remote Experiments Using Crowdsourcing and Laboratory Experiments on Speech Intelligibility",
    "volume": "main",
    "abstract": "Many subjective experiments have been performed to develop objective speech intelligibility measures, but the novel coronavirus outbreak has made it difficult to conduct experiments in a laboratory. One solution is to perform remote testing using crowdsourcing; however, because we cannot control the listening conditions, it is unclear whether the results are entirely reliable. In this study, we compared the speech intelligibility scores obtained from remote and laboratory experiments. The results showed that the mean and standard deviation (SD) of the remote experiments' speech reception threshold (SRT) were higher than those of the laboratory experiments. However, the variance in the SRTs across the speech-enhancement conditions revealed similarities, implying that remote testing results may be as useful as laboratory experiments to develop an objective measure. We also show that practice session scores are correlated with SRT values. This is a priori information before performing the main tests and would be useful for data screening to reduce the variability of the SRT distribution",
    "checked": true,
    "id": "6318311b8dad29622cf6fda76af9c2f249b62bfc",
    "semantic_title": "comparison of remote experiments using crowdsourcing and laboratory experiments on speech intelligibility",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21b_interspeech.html": {
    "title": "Know Your Enemy, Know Yourself: A Unified Two-Stage Framework for Speech Enhancement",
    "volume": "main",
    "abstract": "Traditional spectral subtraction-type single channel speech enhancement (SE) algorithms often need to estimate interference components including noise and/or reverberation before subtracting them while deep neural network-based SE methods often aim to realize the end-to-end target mapping. In this paper, we show that both denoising and dereverberation can be unified into a common problem by introducing a two-stage paradigm, namely for interference components estimation and speech recovery. In the first stage, we propose to explicitly extract the magnitude of interference components, which serves as the prior information. In the second stage, with the guidance of this estimated magnitude prior, we can expect to better recover the target speech. In addition, we propose a transform module to facilitate the interaction between interference components and the desired speech modalities. Meanwhile, a temporal fusion module is designed to model long-term dependencies without ignoring short-term details. We conduct the experiments on the WSJ0-SI84 corpus and the results on both denoising and dereverberation tasks show that our approach outperforms previous advanced systems and achieves state-of-the-art performance in terms of many objective metrics",
    "checked": true,
    "id": "7caad4b855165f3f19501c105b7302a4401893a4",
    "semantic_title": "know your enemy, know yourself: a unified two-stage framework for speech enhancement",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kong21_interspeech.html": {
    "title": "Speech Enhancement with Weakly Labelled Data from AudioSet",
    "volume": "main",
    "abstract": "Speech enhancement is a task to improve the intelligibility and perceptual quality of degraded speech signals. Recently, neural network-based methods have been applied to speech enhancement. However, many neural network-based methods require users to collect clean speech and background noise for training, which can be time-consuming. In addition, speech enhancement systems trained on particular types of background noise may not generalize well to a wide range of noise. To tackle those problems, we propose a speech enhancement framework trained on weakly labelled data. We first apply a pretrained sound event detection system to detect anchor segments that contain sound events in audio clips. Then, we randomly mix two detected anchor segments as a mixture. We build a conditional source separation network using the mixture and a conditional vector as input. The conditional vector is obtained from the audio tagging predictions on the anchor segments. In inference, we input a noisy speech signal with the one-hot encoding of \"Speech\" as a condition to the trained system to predict enhanced speech. Our system achieves a PESQ of 2.28 and an SSNR of 8.75 dB on the VoiceBank-DEMAND dataset, outperforming the previous SEGAN system of 2.16 and 7.73 dB respectively",
    "checked": true,
    "id": "89db5d6c26219fbc09ab4b1ac1853a514281f848",
    "semantic_title": "speech enhancement with weakly labelled data from audioset",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hsieh21_interspeech.html": {
    "title": "Improving Perceptual Quality by Phone-Fortified Perceptual Loss Using Wasserstein Distance for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement (SE) aims to improve speech quality and intelligibility, which are both related to a smooth transition in speech segments that may carry linguistic information, e.g. phones and syllables. In this study, we propose a novel phone-fortified perceptual loss (PFPL) that takes phonetic information into account for training SE models. To effectively incorporate the phonetic information, the PFPL is computed based on latent representations of the model, a powerful self-supervised encoder that renders rich phonetic information. To more accurately measure the distribution distances of the latent representations, the PFPL adopts the Wasserstein distance as the distance measure. Our experimental results first reveal that the PFPL is more correlated with the perceptual evaluation metrics, as compared to signal-level losses. Moreover, the results showed that the PFPL can enable a deep complex U-Net SE model to achieve highly competitive performance in terms of standardized quality and intelligibility evaluations on the Voice Bankâ€“DEMAND dataset",
    "checked": true,
    "id": "27fe7a76098c9344024fa5c70c8f53e9df637040",
    "semantic_title": "improving perceptual quality by phone-fortified perceptual loss using wasserstein distance for speech enhancement",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fu21_interspeech.html": {
    "title": "MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement",
    "volume": "main",
    "abstract": "The discrepancy between the cost function used for training a speech enhancement model and human auditory perception usually makes the quality of enhanced speech unsatisfactory. Objective evaluation metrics which consider human perception can hence serve as a bridge to reduce the gap. Our previously proposed MetricGAN was designed to optimize objective metrics by connecting the metric with a discriminator. Because only the scores of the target evaluation functions are needed during training, the metrics can even be non-differentiable. In this study, we propose a MetricGAN+ in which three training techniques incorporating domain-knowledge of speech processing are proposed. With these techniques, experimental results on the VoiceBank-DEMAND dataset show that MetricGAN+ can increase PESQ score by 0.3 compared to the previous MetricGAN and achieve state-of-the-art results (PESQ score = 3.15)",
    "checked": true,
    "id": "686a302e00af17433aa10d8c38725b58668ad52e",
    "semantic_title": "metricgan+: an improved version of metricgan for speech enhancement",
    "citation_count": 106
  },
  "https://www.isca-speech.org/archive/interspeech_2021/edraki21_interspeech.html": {
    "title": "A Spectro-Temporal Glimpsing Index (STGI) for Speech Intelligibility Prediction",
    "volume": "main",
    "abstract": "We propose a monaural intrusive speech intelligibility prediction (SIP) algorithm called STGI based on detecting in short-time segments in a spectro-temporal modulation decomposition of the input speech signals. Unlike existing glimpse-based SIP methods, the application of STGI is not limited to additive uncorrelated noise; STGI can be employed in a broad range of degradation conditions. Our results show that STGI performs consistently well across 15 datasets covering degradation conditions including modulated noise, noise reduction processing, reverberation, near-end listening enhancement, checkerboard noise, and gated noise",
    "checked": true,
    "id": "88ac79710f0ff8696c8e98d7a28adcf8c183cbd9",
    "semantic_title": "a spectro-temporal glimpsing index (stgi) for speech intelligibility prediction",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiu21_interspeech.html": {
    "title": "Self-Supervised Learning Based Phone-Fortified Speech Enhancement",
    "volume": "main",
    "abstract": "For speech enhancement, deep complex network based methods have shown promising performance due to their effectiveness in dealing with complex-valued spectrums. Recent speech enhancement methods focus on further optimization of network structures and hyperparameters, however, ignore inherent speech characteristics (e.g., phonetic characteristics), which are important for networks to learn and reconstruct speech information. In this paper, we propose a novel self-supervised learning based phone-fortified (SSPF) method for speech enhancement. Our method explicitly imports phonetic characteristics into a deep complex convolutional network via a Contrastive Predictive Coding (CPC) model pre-trained with self-supervised learning. This operation can greatly improve speech representation learning and speech enhancement performance. Moreover, we also apply the self-attention mechanism to our model for learning long-range dependencies of a speech sequence, which further improves the performance of speech enhancement. The experimental results demonstrate that our SSPF method outperforms existing methods and achieves state-of-the-art performance in terms of speech quality and intelligibility",
    "checked": true,
    "id": "b71441f85b95b0274675159c3cc1022f91bb75d3",
    "semantic_title": "self-supervised learning based phone-fortified speech enhancement",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nayem21_interspeech.html": {
    "title": "Incorporating Embedding Vectors from a Human Mean-Opinion Score Prediction Model for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Objective measures of success, such as the perceptual evaluation of speech quality (PESQ), signal-to-distortion ratio (SDR), and short-time objective intelligibility (STOI), have recently been used to optimize deep-learning based speech enhancement algorithms, in an effort to incorporate perceptual constraints into the learning process. Optimizing with these measures, however, may be sub-optimal, since the objective scores do not always strongly correlate with a listener's evaluation. This motivates the need for approaches that either are optimized with scores that are strongly correlated with human assessments or that use alternative strategies for incorporating perceptual constraints. In this work, we propose an attention-based approach that uses learned speech embedding vectors from a mean-opinion score (MOS) prediction model and a speech enhancement module to jointly enhance noisy speech. Our loss function is jointly optimized with signal approximation and MOS prediction loss terms. We train the model using real-world noisy speech data that has been captured in everyday environments. The results show that our proposed model significantly outperforms other approaches that are optimized with objective measures",
    "checked": true,
    "id": "c1078dcb98cee7e6c5b67ff0adda5f62a4644398",
    "semantic_title": "incorporating embedding vectors from a human mean-opinion score prediction model for monaural speech enhancement",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21c_interspeech.html": {
    "title": "Restoring Degraded Speech via a Modified Diffusion Model",
    "volume": "main",
    "abstract": "There are many deterministic mathematical operations (e.g. compression, clipping, downsampling) that degrade speech quality considerably. In this paper we introduce a neural network architecture, based on a modification of the DiffWave model, that aims to restore the original speech signal. DiffWave, a recently published diffusion-based vocoder, has shown state-of-the-art synthesized speech quality and relatively shorter waveform generation times, with only a small set of parameters. We replace the mel-spectrum upsampler in DiffWave with a deep CNN upsampler, which is trained to alter the degraded speech mel-spectrum to match that of the original speech. The model is trained using the original speech waveform, but conditioned on the degraded speech mel-spectrum. Post-training, only the degraded mel-spectrum is used as input and the model generates an estimate of the original speech. Our model results in improved speech quality (original DiffWave model as baseline) on several different experiments. These include improving the quality of speech degraded by LPC-10 compression, AMR-NB compression, and signal clipping. Compared to the original DiffWave architecture, our scheme achieves better performance on several objective perceptual metrics and in subjective comparisons. Improvements over baseline are further amplified in a out-of-corpus evaluation setting",
    "checked": true,
    "id": "28db8fd711ac13599c9921db08cd586235d303ba",
    "semantic_title": "restoring degraded speech via a modified diffusion model",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21_interspeech.html": {
    "title": "User-Initiated Repetition-Based Recovery in Multi-Utterance Dialogue Systems",
    "volume": "main",
    "abstract": "Recognition errors are common in human communication. Similar errors often lead to unwanted behaviour in dialogue systems or virtual assistants. In human communication, we can recover from them by repeating misrecognized words or phrases; however in human-machine communication this recovery mechanism is not available. In this paper, we attempt to bridge this gap and present a system that allows a user to correct speech recognition errors in a virtual assistant by repeating misunderstood words. When a user repeats part of the phrase the system rewrites the original query to incorporate the correction. This rewrite allows the virtual assistant to understand the original query successfully. We present an end-to-end 2-step attention pointer network that can generate the the rewritten query by merging together the incorrectly understood utterance with the correction follow-up. We evaluate the model on data collected for this task and compare the proposed model to a rule-based baseline and a standard pointer network. We show that rewriting the original query is an effective way to handle repetition-based recovery and that the proposed model outperforms the rule based baseline, reducing Word Error Rate by 19% relative at 2% False Alarm Rate on annotated data",
    "checked": true,
    "id": "3722c06603b144ecdb8bb9f594751934f52b214d",
    "semantic_title": "user-initiated repetition-based recovery in multi-utterance dialogue systems",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21_interspeech.html": {
    "title": "Self-Supervised Dialogue Learning for Spoken Conversational Question Answering",
    "volume": "main",
    "abstract": "In spoken conversational question answering (SCQA), the answer to the corresponding question is generated by retrieving and then analyzing a fixed spoken document, including multi-part conversations. Most SCQA systems have considered only retrieving information from ordered utterances. However, the sequential order of dialogue is important to build a robust spoken conversational question answering system, and the changes of utterances order may severely result in low-quality and incoherent corpora. To this end, we introduce a self-supervised learning approach, including , and , to explicitly capture the coreference resolution and dialogue coherence among spoken documents. Specifically, we design a joint learning framework where the auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards more coherent and meaningful spoken dialogue learning. We also utilize the proposed self-supervised learning tasks to capture intra-sentence coherence. Experimental results demonstrate that our proposed method provides more coherent, meaningful, and appropriate responses, yielding superior performance gains compared to the original pre-trained language models. Our method achieves state-of-the-art results on the Spoken-CoQA dataset",
    "checked": true,
    "id": "904afb6696575bb7d74f5e4e74d9cfced480a645",
    "semantic_title": "self-supervised dialogue learning for spoken conversational question answering",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2021/su21_interspeech.html": {
    "title": "Act-Aware Slot-Value Predicting in Multi-Domain Dialogue State Tracking",
    "volume": "main",
    "abstract": "As an essential component in task-oriented dialogue systems, dialogue state tracking (DST) aims to track human-machine interactions and generate state representations for managing the dialogue. Representations of dialogue states are dependent on the domain ontology and the user's goals. In several task-oriented dialogues with a limited scope of objectives, dialogue states can be represented as a set of slot-value pairs. As the capabilities of dialogue systems expand to support increasing naturalness in communication, incorporating dialogue act processing into dialogue model design becomes essential. The lack of such consideration limits the scalability of dialogue state tracking models for dialogues having specific objectives and ontology. To address this issue, we formulate and incorporate dialogue acts, and leverage recent advances in machine reading comprehension to predict both categorical and non-categorical types of slots for multi-domain dialogue state tracking. Experimental results show that our models can improve the overall accuracy of dialogue state tracking on the MultiWOZ 2.1 dataset, and demonstrate that incorporating dialogue acts can guide dialogue state design for future task-oriented dialogue systems",
    "checked": true,
    "id": "8fe6ac053bc961093f46d8592e743215ad33bbad",
    "semantic_title": "act-aware slot-value predicting in multi-domain dialogue state tracking",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chiba21_interspeech.html": {
    "title": "Dialogue Situation Recognition for Everyday Conversation Using Multimodal Information",
    "volume": "main",
    "abstract": "In recent years, dialogue systems have been applied to daily living. Such systems should be able to associate conversations with dialogue situations, such as a place where a dialogue occurs and the relationship between participants. In this study, we propose a dialogue situation recognition method that understands the perspective of dialogue scenes. The target dialogue situations contain dialogue styles, places, activities, and relations between participants. We used the Corpus of Everyday Japanese Conversation (CEJC), which records natural everyday conversations in various situations for experiments. We experimentally verified the effectiveness of our proposed method using multimodal information for situation recognition",
    "checked": true,
    "id": "c5be70f553d46a6f6413fb48c044f6b8093bbefc",
    "semantic_title": "dialogue situation recognition for everyday conversation using multimodal information",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yamazaki21_interspeech.html": {
    "title": "Neural Spoken-Response Generation Using Prosodic and Linguistic Context for Conversational Systems",
    "volume": "main",
    "abstract": "Spoken dialogue systems have become widely used in daily life. Such a system must interact with the user socially to truly operate as a partner with humans. In studies of recent dialogue systems, neural response generation led to natural response generation. However, these studies have not considered the acoustic aspects of conversational phenomena, such as the adaptation of prosody. We propose a spoken-response generation model that extends a neural conversational model to deal with pitch control signals. Our proposed model is trained using multimodal dialogue between humans. The generated pitch control signals are input to a speech synthesis system to control the pitch of synthesized speech. Our experiment shows that the proposed system can generate synthesized speech with an appropriate F0 contour as an utterance in context compared to the output of a system without pitch control, although language generation remains an issue",
    "checked": true,
    "id": "202c9b9e772fe9a8f26ea886268420e96376fcc3",
    "semantic_title": "neural spoken-response generation using prosodic and linguistic context for conversational systems",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21_interspeech.html": {
    "title": "Semantic Transportation Prototypical Network for Few-Shot Intent Detection",
    "volume": "main",
    "abstract": "Few-shot intent detection is a problem that only a few annotated examples are available for unseen intents, and deep models could suffer from the overfitting problem because of scarce data. Existing state-of-the-art few-shot model, Prototypical Network (PN), mainly focus on computing the similarity between examples in a metric space by leveraging sentence-level instance representations. However, sentence-level representations may incorporate highly noisy signals from unrelated words which leads to performance degradation. In this paper, we propose Semantic Transportation Prototypical Network (STPN) to alleviate this issue. Different from the original PN, our approach takes word-level representation as input and uses a new distance metric to obtain better sample matching result. And we reformulate the few-shot classification task into an instance of optimal matching, in which the key word semantic information between examples are expected to be matched and the matching cost is treated as similarity. Specifically, we design Mutual-Semantic mechanism to generate word semantic information, which could reduce the unrelated word noise and enrich key word information. Then, Earth Mover's Distance (EMD) is applied to find an optimal matching solution. Comprehensive experiments on two benchmark datasets are conducted to validate the effectiveness and generalization of our proposed model",
    "checked": true,
    "id": "8decba8a86d69c19aa87edf53e06b31298ab22c8",
    "semantic_title": "semantic transportation prototypical network for few-shot intent detection",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tang21_interspeech.html": {
    "title": "Domain-Specific Multi-Agent Dialog Policy Learning in Multi-Domain Task-Oriented Scenarios",
    "volume": "main",
    "abstract": "Traditional dialog policy learning methods train a generic dialog agent to address all situations. However, when the dialog agent encounters a complicated task that involves more than one domain, it becomes difficult to perform concordant actions due to the hybrid information in the multi-domain ontology. Inspired by a real-life scenario at a bank, there are always several specialized departments that deal with different businesses. In this paper, we propose Domain-Specific Multi-Agent Dialog Policy Learning (DSMADPL), in which the dialog system is composed of a set of agents where each agent represents a specialized skill in a particular domain. Every domain-specific agent is first pretrained with supervised learning using a dialog corpus, and then they are jointly improved with multi-agent reinforcement learning. When the dialog system interacts with the user, in each turn the system action is decided by the actions of relevant agents. Experiments conducted on the commonly used MultiWOZ dataset prove the effectiveness of the proposed method, in which dialog success rate increases from 55.0% for the traditional method to 67.2% for our method in multi-domain scenarios",
    "checked": true,
    "id": "e0a9a3e463bbc7b65fd562c5c9a8dbc6217218c9",
    "semantic_title": "domain-specific multi-agent dialog policy learning in multi-domain task-oriented scenarios",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21b_interspeech.html": {
    "title": "Leveraging ASR N-Best in Deep Entity Retrieval",
    "volume": "main",
    "abstract": "Entity Retrieval (ER) in spoken dialog systems is a task that retrieves entities in a catalog for the entity mentions in user utterances. ER systems are susceptible to upstream errors, with Automatic Speech Recognition (ASR) errors being particularly troublesome. In this work, we propose a robust deep learning based ER system by leveraging ASR N-best hypotheses. Specifically, we evaluate different neural architectures to infuse ASR N-best through an attention mechanism. On 750 hours of audio data taken from live traffic, our best model achieves 11.07% relative error reduction while maintaining the same performance on rejecting out-of-domain ER requests",
    "checked": true,
    "id": "e90aa6debe3bdc089f174bb8f6671e8f47798592",
    "semantic_title": "leveraging asr n-best in deep entity retrieval",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21d_interspeech.html": {
    "title": "End-to-End Spelling Correction Conditioned on Acoustic Feature for Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "In this work, we propose a new end-to-end (E2E) spelling correction method for post-processing of code-switching automatic speech recognition (ASR). Existing E2E spelling correction models take the hypotheses of ASR as inputs and annotated text as the targets. Due to the powerful modeling capabilities of the E2E model, the training of the correction system is extremely prone to over-fitting. It usually requires sufficient data diversity for reliable training. Therefore, it is difficult to apply the E2E correction models to the code-switching ASR task because of the data shortage. In this paper, we introduce the acoustic features into the spelling correction model. Our method can alleviate the problem of over-fitting and has better performance. Meanwhile, because the acoustic features are encode-free, our proposed model can be applied to the ASR model without significantly increasing the computational cost. The experimental results on ASRU 2019 Mandarin-English Code-switching Challenge data set show that the proposed method achieves 11.14% relative error rate reduction compared with baseline",
    "checked": true,
    "id": "832344b4e8dcdebca7a78e4d1dab1b73cbc08b9b",
    "semantic_title": "end-to-end spelling correction conditioned on acoustic feature for code-switching speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/siminyu21_interspeech.html": {
    "title": "Phoneme Recognition Through Fine Tuning of Phonetic Representations: A Case Study on Luhya Language Varieties",
    "volume": "main",
    "abstract": "Models pre-trained on multiple languages have shown significant promise for improving speech recognition, particularly for low-resource languages. In this work, we focus on phoneme recognition using Allosaurus, a method for multilingual recognition based on phonetic annotation, which incorporates phonological knowledge through a language-dependent allophone layer that associates a universal narrow phone-set with the phonemes that appear in each language. To evaluate in a challenging real-world scenario, we curate phone recognition datasets for Bukusu and Saamia, two varieties of the Luhya language cluster of western Kenya and eastern Uganda. To our knowledge, these datasets are the first of their kind. We carry out similar experiments on the dataset of an endangered Tangkhulic language, East Tusom, a Tibeto-Burman language variety spoken mostly in India. We explore both zero-shot and few-shot recognition by fine-tuning using datasets of varying sizes (10 to 1000 utterances). We find that fine-tuning of Allosaurus, even with just 100 utterances, leads to significant improvements in phone error rates",
    "checked": true,
    "id": "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713",
    "semantic_title": "phoneme recognition through fine tuning of phonetic representations: a case study on luhya language varieties",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/loweimi21_interspeech.html": {
    "title": "Speech Acoustic Modelling Using Raw Source and Filter Components",
    "volume": "main",
    "abstract": "Source-filter modelling is among the fundamental techniques in speech processing with a wide range of applications. In acoustic modelling, features such as MFCC and PLP which parametrise the filter component are widely employed. In this paper, we investigate the efficacy of building acoustic models from the raw filter and source components. The raw magnitude spectrum, as the primary information stream, is decomposed into the excitation and vocal tract information streams via cepstral liftering. Then, acoustic models are built via multi-head CNNs which, among others, allow for processing each individual stream via a sequence of bespoke transforms and fusing them at an optimal level of abstraction. We discuss the possible advantages of such information factorisation and recombination, investigate the dynamics of these models and explore the optimal fusion level. Furthermore, we illustrate the CNN's learned filters and provide some interpretation for the captured patterns. The proposed approach with optimal fusion scheme results in up to 14% and 7% relative WER reduction in WSJ and Aurora-4 tasks",
    "checked": true,
    "id": "956404c3e5ad68fcf7e416792889630f0f5c76f9",
    "semantic_title": "speech acoustic modelling using raw source and filter components",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujimoto21_interspeech.html": {
    "title": "Noise Robust Acoustic Modeling for Single-Channel Speech Recognition Based on a Stream-Wise Transformer Architecture",
    "volume": "main",
    "abstract": "This paper addresses a noise-robust automatic speech recognition (ASR) method under the constraints of real-time, one-pass, and single-channel processing. Under such strong constraints, single-channel speech enhancement becomes a key technology because methods with multiple-passes or batch processing, such as acoustic model adaptation, are not suitable for use. However, single-channel speech enhancement often degrades ASR performance due to speech distortion. To overcome this problem, we propose a noise robust acoustic modeling method based on the stream-wise transformer model. The proposed method accepts multi-stream features obtained by multiple single-channel speech enhancement methods as input and selectively uses an appropriate feature stream according to the noise environment by paying attention to the noteworthy stream on the basis of multi-head attention. The proposed method considers the attention for the stream direction instead of the time series direction, and it is thus capable of real-time and low-latency processing. Comparative evaluations reveal that the proposed method successfully improves the accuracy of ASR in noisy environments and reduces the number of model parameters even under strong constraints",
    "checked": true,
    "id": "59dfc48ad807150a8db4a0b8b7e1eb8f8ac33103",
    "semantic_title": "noise robust acoustic modeling for single-channel speech recognition based on a stream-wise transformer architecture",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ratnarajah21_interspeech.html": {
    "title": "IR-GAN: Room Impulse Response Generator for Far-Field Speech Recognition",
    "volume": "main",
    "abstract": "We present a Generative Adversarial Network (GAN) based room impulse response generator (IR-GAN) for generating realistic synthetic room impulse responses (RIRs). IR-GAN extracts acoustic parameters from captured real-world RIRs and uses these parameters to generate new synthetic RIRs. We use these generated synthetic RIRs to improve far-field automatic speech recognition in new environments that are different from the ones used in training datasets. In particular, we augment the far-field speech training set by convolving our synthesized RIRs with a clean LibriSpeech dataset [1]. We evaluate the quality of our synthetic RIRs on the far-field LibriSpeech test set created using real-world RIRs from the BUT ReverbDB [2] and AIR [3] datasets. Our IR-GAN reports up to an 8.95% lower error rate than Geometric Acoustic Simulator (GAS) in far-field speech recognition benchmarks. We further improve the performance when we combine our synthetic RIRs with synthetic impulse responses generated using GAS. This combination can reduce the word error rate by up to 14.3% in far-field speech recognition benchmarks",
    "checked": true,
    "id": "23e208eb3251fd3521c14bcbd3ee2d97a369ed89",
    "semantic_title": "ir-gan: room impulse response generator for far-field speech recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21b_interspeech.html": {
    "title": "Scaling Sparsemax Based Channel Selection for Speech Recognition with ad-hoc Microphone Arrays",
    "volume": "main",
    "abstract": "Recently, speech recognition with ad-hoc microphone arrays has received much attention. It is known that channel selection is an important problem of ad-hoc microphone arrays, however, this topic seems far from explored in speech recognition yet, particularly with a large-scale ad-hoc microphone array. To address this problem, we propose a algorithm for the channel selection problem of the speech recognition with large-scale ad-hoc microphone arrays. Specifically, we first replace the conventional Softmax operator in the stream attention mechanism of a multichannel end-to-end speech recognition system with Sparsemax, which conducts channel selection by forcing the channel weights of noisy channels to zero. Because Sparsemax punishes the weights of many channels to zero harshly, we propose Scaling Sparsemax which punishes the channels mildly by setting the weights of very noisy channels to zero only. Experimental results with ad-hoc microphone arrays of over 30 channels under the conformer speech recognition architecture show that the proposed Scaling Sparsemax yields a word error rate of over 30% lower than Softmax on simulation data sets, and over 20% lower on semi-real data sets, in test scenarios with both matched and mismatched channel numbers",
    "checked": true,
    "id": "d359eee45dc476c98324aff01240ad1163818a97",
    "semantic_title": "scaling sparsemax based channel selection for speech recognition with ad-hoc microphone arrays",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chang21_interspeech.html": {
    "title": "Multi-Channel Transformer Transducer for Speech Recognition",
    "volume": "main",
    "abstract": "Multi-channel inputs offer several advantages over single-channel, to improve the robustness of on-device speech recognition systems. Recent work on multi-channel transformer, has proposed a way to incorporate such inputs into end-to-end ASR for improved accuracy. However, this approach is characterized by a high computational complexity, which prevents it from being deployed in on-device systems. In this paper, we present a novel speech recognition model, , which features end-to-end multi-channel training, low computation cost, and low latency so that it is suitable for streaming decoding in on-device speech recognition. In a far-field in-house dataset, our MCTT outperforms stagewise multi-channel models with transformer-transducer up to 6.01% relative WER improvement (WERR). In addition, MCTT outperforms the multi-channel transformer up to 11.62% WERR, and is 15.8 times faster in terms of inference speed. We further show that we can improve the computational cost of MCTT by constraining the future and previous context in attention computations",
    "checked": true,
    "id": "f35e8a7a2bafa8ccb12ae165294e165dfdb986d8",
    "semantic_title": "multi-channel transformer transducer for speech recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tsunoo21_interspeech.html": {
    "title": "Data Augmentation Methods for End-to-End Speech Recognition on Distant-Talk Scenarios",
    "volume": "main",
    "abstract": "Although end-to-end automatic speech recognition (E2E ASR) has achieved great performance in tasks that have numerous paired data, it is still challenging to make E2E ASR robust against noisy and low-resource conditions. In this study, we investigated data augmentation methods for E2E ASR in distant-talk scenarios. E2E ASR models are trained on the series of CHiME challenge datasets, which are suitable tasks for studying robustness against noisy and spontaneous speech. We propose to use three augmentation methods and their combinations: 1) data augmentation using text-to-speech (TTS) data, 2) cycle-consistent generative adversarial network (Cycle-GAN) augmentation trained to map two different audio characteristics, the one of clean speech and of noisy recordings, to match the testing condition, and 3) pseudo-label augmentation provided by the pretrained ASR module for smoothing label distributions. Experimental results using the CHiME-6/CHiME-4 datasets show that each augmentation method individually improves the accuracy on top of the conventional SpecAugment; further improvements are obtained by combining these approaches. We achieved 4.3% word error rate (WER) reduction, which was more significant than that of the SpecAugment, when we combine all three augmentations for the CHiME-6 task",
    "checked": true,
    "id": "77cd3ae8a0b9ef6865d5324a4d62280e6f7a1053",
    "semantic_title": "data augmentation methods for end-to-end speech recognition on distant-talk scenarios",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21_interspeech.html": {
    "title": "Leveraging Phone Mask Training for Phonetic-Reduction-Robust E2E Uyghur Speech Recognition",
    "volume": "main",
    "abstract": "In Uyghur speech, consonant and vowel reduction are often encountered, especially in spontaneous speech with high speech rate, which will cause a degradation of speech recognition performance. To solve this problem, we propose an effective phone mask training method for Conformer-based Uyghur end-to-end (E2E) speech recognition. The idea is to randomly mask off a certain percentage features of phones during model training, which simulates the above verbal phenomena and facilitates E2E model to learn more contextual information. According to experiments, the above issues can be greatly alleviated. In addition, deep investigations are carried out into different units in masking, which shows the effectiveness of our proposed masking unit. We also further study the masking method and optimize filling strategy of phone mask. Finally, compared with Conformer-based E2E baseline without mask training, our model demonstrates about 5.51% relative Word Error Rate (WER) reduction on reading speech and 12.92% on spontaneous speech, respectively. The above approach has also been verified on test-set of open-source data THUYG-20, which shows 20% relative improvements",
    "checked": true,
    "id": "a090394ade7c1d094b268282e0872d5547e16c91",
    "semantic_title": "leveraging phone mask training for phonetic-reduction-robust e2e uyghur speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/likhomanenko21_interspeech.html": {
    "title": "Rethinking Evaluation in ASR: Are Our Models Robust Enough?",
    "volume": "main",
    "abstract": "Is pushing numbers on a single benchmark valuable in automatic speech recognition? Research results in acoustic modeling are typically evaluated based on performance on a single dataset. While the research community has coalesced around various benchmarks, we set out to understand generalization performance in acoustic modeling across datasets â€” in particular, if models trained on a single dataset transfer to other (possibly out-of-domain) datasets. Further, we demonstrate that when a large enough set of benchmarks is used, average word error rate (WER) performance over them provides a good proxy for performance on real-world data. Finally, we show that training a single acoustic model on the most widely-used datasets â€” combined â€” reaches competitive performance on both research and real-world benchmarks",
    "checked": true,
    "id": "62ce4d65335c32844247e0ecf3be6de1ccb924b2",
    "semantic_title": "rethinking evaluation in asr: are our models robust enough?",
    "citation_count": 65
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lam21_interspeech.html": {
    "title": "Raw Waveform Encoder with Multi-Scale Globally Attentive Locally Recurrent Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end speech recognition generally uses hand-engineered acoustic features as input and excludes the feature extraction module from its joint optimization. To extract learnable and adaptive features and mitigate information loss, we propose a new encoder that adopts globally attentive locally recurrent (GALR) networks and directly takes raw waveform as input. We observe improved ASR performance and robustness by applying GALR on different window lengths to aggregate fine-grain temporal information into multi-scale acoustic features. Experiments are conducted on a benchmark dataset and two large-scale Mandarin speech corpus of 5,000 hours and 21,000 hours. With faster speed and comparable model size, our proposed multi-scale GALR waveform encoder achieved consistent character error rate reductions (CERRs) from 7.9% to 28.1% relative over strong baselines, including Conformer and TDNN-Conformer. In particular, our approach demonstrated notable robustness than the traditional handcrafted features and outperformed the baseline MFCC-based TDNN-Conformer model by a 15.2% CERR on a music-mixed real-world speech test set",
    "checked": true,
    "id": "0610eab8e6f454486006ff16b27aed975af9dccf",
    "semantic_title": "raw waveform encoder with multi-scale globally attentive locally recurrent networks for end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hou21_interspeech.html": {
    "title": "Attention-Based Cross-Modal Fusion for Audio-Visual Voice Activity Detection in Musical Video Streams",
    "volume": "main",
    "abstract": "Many previous audio-visual voice-related works focus on speech, ignoring the singing voice in the growing number of musical video streams on the Internet. For processing diverse musical video data, voice activity detection is a necessary step. This paper attempts to detect the speech and singing voices of target performers in musical video streams using audio-visual information. To integrate information of audio and visual modalities, a multi-branch network is proposed to learn audio and image representations, and the representations are fused by attention based on semantic similarity to shape the acoustic representations through the probability of anchor vocalization. Experiments show the proposed audio-visual multi-branch network far outperforms the audio-only model in challenging acoustic environments, indicating the cross-modal information fusion based on semantic correlation is sensible and successful",
    "checked": true,
    "id": "508d61701998e2d22cf2ed53c1ca8b28f2cf39a0",
    "semantic_title": "attention-based cross-modal fusion for audio-visual voice activity detection in musical video streams",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21b_interspeech.html": {
    "title": "Noise-Tolerant Self-Supervised Learning for Audio-Visual Voice Activity Detection",
    "volume": "main",
    "abstract": "Recent audio-visual voice activity detectors based on supervised learning require large amounts of labeled training data with manual mouth-region cropping in videos, and the performance is sensitive to a mismatch between the training and testing noise conditions. This paper introduces contrastive self-supervised learning for audio-visual voice activity detection as a possible solution to such problems. In addition, a novel self-supervised learning framework is proposed to improve overall training efficiency and testing performance on noise-corrupted datasets, as in real-world scenarios. This framework includes a branched audio encoder and a noise-tolerant loss function to cope with the uncertainty of speech and noise feature separation in a self-supervised manner. Experimental results, particularly under mismatched noise conditions, demonstrate the improved performance compared with a self-supervised learning baseline and a supervised learning framework",
    "checked": true,
    "id": "bdfeba36c6cac3b3033e72ec3345d619e8b1d1fd",
    "semantic_title": "noise-tolerant self-supervised learning for audio-visual voice activity detection",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/park21_interspeech.html": {
    "title": "Noisy Student-Teacher Training for Robust Keyword Spotting",
    "volume": "main",
    "abstract": "We propose self-training with noisy student-teacher approach for streaming keyword spotting, that can utilize large-scale unlabeled data and aggressive data augmentation. The proposed method applies aggressive data augmentation (spectral augmentation) on the input of both student and teacher and utilize unlabeled data at scale, which significantly boosts the accuracy of student against challenging conditions. Such aggressive augmentation usually degrades model performance when used with supervised training with hard-labeled data. Experiments show that aggressive spec augmentation on baseline supervised training method degrades accuracy, while the proposed self-training with noisy student-teacher training improves accuracy of some difficult-conditioned test sets by as much as 60%",
    "checked": true,
    "id": "d81bfd0b57ab77a51aa2108e20861f1394629e96",
    "semantic_title": "noisy student-teacher training for robust keyword spotting",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ichikawa21_interspeech.html": {
    "title": "Multi-Channel VAD for Transcription of Group Discussion",
    "volume": "main",
    "abstract": "Attempts are being made to visualize the learning process by attaching microphones to students participating in group works conducted in classrooms, and subsequently, their speech using an automatic speech recognition (ASR) system. However, the voices of nearby students frequently become mixed with the output speech data, even when using close-talk microphones with noise robustness. To resolve this challenge, in this paper, we propose using multi-channel voice activity detection (VAD) to determine the speech segments of a target speaker while also referencing the output speech from the microphones attached to the other speakers in the group. The conducted evaluation experiments using the actual speech of middle school students during group work lessons showed that our proposed method significantly improves the frame error rate (38.7%) compared to that of the conventional technology, single-channel VAD (49.5%). In our view, conventional approaches, such as distributed microphone arrays and deep learning, are somewhat dependent on the temporal stationarity of the speakers' positions. However, the proposed method is essentially a VAD process and thus works robustly. It is the practical and proven solution in a real classroom environment",
    "checked": true,
    "id": "6b1c9a7316501307e88c04af959e726af09c8ef9",
    "semantic_title": "multi-channel vad for transcription of group discussion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21_interspeech.html": {
    "title": "Audio-Visual Information Fusion Using Cross-Modal Teacher-Student Learning for Voice Activity Detection in Realistic Environments",
    "volume": "main",
    "abstract": "We propose an information fusion approach to audio-visual voice activity detection (AV-VAD) based on cross-modal teacher-student learning leveraging on factorized bilinear pooling (FBP) and Kullback-Leibler (KL) regularization. First, we design an audio-visual network by using FBP fusion to fully utilize the interaction between audio and video modalities. Next, to transfer the rich information in audio-based VAD (A-VAD) model trained with a massive audio-only dataset to AV-VAD model built with relatively limited multi-modal data, a cross-modal teacher-student learning framework is then proposed based on cross entropy with regulated KL-divergence. Finally, evaluated on an in-house dataset recorded in realistic conditions using standard VAD metrics, the proposed approach yields consistent and significant improvements over other state-of-the-art techniques. Moreover, by applying our AV-VAD technique to an audio-visual Chinese speech recognition task, the character error rate is reduced by 24.15% and 8.66% from A-VAD and the baseline AV-VAD systems, respectively",
    "checked": true,
    "id": "36878855de3551d0dce4db978e3d87fb7483296f",
    "semantic_title": "audio-visual information fusion using cross-modal teacher-student learning for voice activity detection in realistic environments",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/makishima21_interspeech.html": {
    "title": "Enrollment-Less Training for Personalized Voice Activity Detection",
    "volume": "main",
    "abstract": "We present a novel personalized voice activity detection (PVAD) learning method that does not require enrollment data during training. PVAD is a task to detect the speech segments of a specific target speaker at the frame level using enrollment speech of the target speaker. Since PVAD must learn speakers' speech variations to clarify the boundary between speakers, studies on PVAD used large-scale datasets that contain many utterances for each speaker. However, the datasets to train a PVAD model are often limited because substantial cost is needed to prepare such a dataset. In addition, we cannot utilize the datasets used to train the standard VAD because they often lack speaker labels. To solve these problems, our key idea is to use one utterance as both a kind of enrollment speech and an input to the PVAD during training, which enables PVAD training without enrollment speech. In our proposed method, called enrollment-less training, we augment one utterance so as to create variability between the input and the enrollment speech while keeping the speaker identity, which avoids the mismatch between training and inference. Our experimental results demonstrate the efficacy of the method",
    "checked": true,
    "id": "d6c44c2faa653eef801ffbd51292bcfa23135072",
    "semantic_title": "enrollment-less training for personalized voice activity detection",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nonaka21_interspeech.html": {
    "title": "Voice Activity Detection for Live Speech of Baseball Game Based on Tandem Connection with Speech/Noise Separation Model",
    "volume": "main",
    "abstract": "When applying voice activity detection (VAD) to a noisy sound, in general, noise reduction (speech separation) and VAD are performed separately. In this case, the noise reduction may suppress the speech, and the VAD may not work well for the speech after the noise reduction. This study proposes a VAD model through the tandem connection of neural network-based noise separation and a VAD model. By training the two models simultaneously, the noise separation model is expected to be trained to consider the VAD results, and thus effective noise separation can be achieved. Moreover, the improved speech/noise separation model will improve the accuracy of the VAD model. In this research, we deal with real-live speeches from baseball games, which have a very poor signal-to-noise ratio. The VAD experiments showed that the VAD performance at the frame level achieved 4.2 points improvement in F1-score by tandemly connecting the speech/noise separation model and the VAD model",
    "checked": true,
    "id": "10dbd2366917c7ef826fb8255412ee8747163560",
    "semantic_title": "voice activity detection for live speech of baseball game based on tandem connection with speech/noise separation model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kwon21_interspeech.html": {
    "title": "FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications",
    "volume": "main",
    "abstract": "Various incremental learning (IL) approaches have been proposed to help deep learning models learn new tasks/classes continuously without forgetting what was learned previously (i.e., avoid catastrophic forgetting). With the growing number of deployed audio sensing applications that need to dynamically incorporate new tasks and changing input distribution from users, the ability of IL on-device becomes essential for both efficiency and user privacy However, prior works suffer from high computational costs and storage demands which hinders the deployment of IL on-device. In this work, to overcome these limitations, we develop an end-to-end and on-device IL framework, FastICARL, that incorporates an exemplar-based IL and quantization in the context of audio-based applications. We first employ k-nearest-neighbor to reduce the latency of IL. Then, we jointly utilize a quantization technique to decrease the storage requirements of IL. We implement FastICARL on two types of mobile devices and demonstrate that FastICARL remarkably decreases the IL time up to 78â€“92% and the storage requirements by 2â€“4 times without sacrificing its performance. FastICARL enables complete on-device IL, ensuring user privacy as the user data does not need to leave the device",
    "checked": true,
    "id": "92faea491cfb376477073aa4a070fa0973bba032",
    "semantic_title": "fasticarl: fast incremental classifier and representation learning with efficient budget allocation in audio sensing applications",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wei21_interspeech.html": {
    "title": "End-to-End Transformer-Based Open-Vocabulary Keyword Spotting with Location-Guided Local Attention",
    "volume": "main",
    "abstract": "Open-vocabulary keyword spotting (KWS) aims to detect arbitrary keywords from continuous speech, which allows users to define their personal keywords. In this paper, we propose a novel location guided end-to-end (E2E) keyword spotting system. Firstly, we predict endpoints of keyword in the entire speech based on attention mechanism. Secondly, we calculate the existence probability of keyword by fusing the located keyword speech segment and text with local attention. The results on Librispeech dataset and Google speech commands dataset show our proposed method significantly outperforms the baseline method and the latest small-footprint E2E KWS method",
    "checked": true,
    "id": "ce14a649fcf10aeaf9eca0ed7d2dcfff1383e979",
    "semantic_title": "end-to-end transformer-based open-vocabulary keyword spotting with location-guided local attention",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bhati21_interspeech.html": {
    "title": "Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation",
    "volume": "main",
    "abstract": "Automatic detection of phoneme or word-like units is one of the core objectives in zero-resource speech processing. Recent attempts employ self-supervised training methods, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework that can model the signal structure at a higher level e.g. at the phoneme level. In this framework, a convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Typically, phoneme and word segmentation are treated as separate tasks. We unify them and experimentally show that our single model outperforms existing phoneme and word segmentation methods on TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and when is the right time to include the segmental loss in the learning process",
    "checked": true,
    "id": "642dab29e680f516eb25949d616a24e0ad147a19",
    "semantic_title": "segmental contrastive predictive coding for unsupervised word segmentation",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21b_interspeech.html": {
    "title": "A Lightweight Framework for Online Voice Activity Detection in the Wild",
    "volume": "main",
    "abstract": "Voice activity detection (VAD) is an essential pre-processing component for speech-related tasks such as automatic speech recognition (ASR). Traditional VAD systems require strong frame-level supervision for training, inhibiting their performance in real-world test scenarios. Previously, the general-purpose VAD (GPVAD) framework has been proposed to enhance noise robustness significantly. However, GPVAD models are comparatively large and only work for offline evaluation. This work proposes the use of a knowledge distillation framework, where a (large, offline) teacher model provides frame-level supervision to a (light, online) student model. Our experiments verify that our proposed lightweight student models outperform GPVAD on all test sets, including clean, synthetic and real-world scenarios. Our smallest student model only uses 2.2% of the parameters and 15.9% duration cost of our teacher model for inference when evaluated on a Raspberry Pi",
    "checked": true,
    "id": "a4557f8c519bdc765c00124cc86736e9c359eb77",
    "semantic_title": "a lightweight framework for online voice activity detection in the wild",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chlebowski21_interspeech.html": {
    "title": "See what I mean, huh?\" Evaluating Visual Inspection of F",
    "volume": "main",
    "abstract": "This paper proposes to evaluate the method used in ChlÃ©bowski and Ballier [1] for the annotation of F variations in nasal grunts. We discuss and test issues raised by this kind of approach exclusively based on visual inspection of the F tracking in [2]. Results tend to show that consistency in the annotation depends on acoustic features intrinsic to the grunts such as F slope and duration that are sensitive to display settings. We nonetheless acknowledge the potential benefits of such a method for automation and implementation in IA and in this respect, we introduce [3] as an alternative material-maker",
    "checked": false,
    "id": "50b077c4c2e04dd22acd939a16266c0dc10b348f",
    "semantic_title": "see what i mean, huh?\" evaluating visual inspection of f0 tracking in nasal grunts",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21c_interspeech.html": {
    "title": "System Performance as a Function of Calibration Methods, Sample Size and Sampling Variability in Likelihood Ratio-Based Forensic Voice Comparison",
    "volume": "main",
    "abstract": "In data-driven forensic voice comparison, sample size is an issue which can have substantial effects on system output. Numerous calibration methods have been developed and some have been proposed as solutions to sample size issues. In this paper, we test four calibration methods (i.e. logistic regression, regularised logistic regression, Bayesian model, ELUB) under different conditions of sampling variability and sample size. Training and test scores were simulated from skewed distributions derived from real experiments, increasing sample sizes from 20 to 100 speakers for both the training and test sets. For each sample size, the experiments were replicated 100 times to test the susceptibility of different calibration methods to sampling variability. The C mean and range across replications were used for evaluation. The Bayesian model and regularized logistic regression produced the most stable C values when the sample size is small (i.e. 20 speakers), although mean C is consistently lowest using logistic regression. The ELUB calibration method generally is the least preferred as it is the most sensitive to sample size and sampling variability (mean = 0.66, range = 0.21â€“0.59)",
    "checked": true,
    "id": "51bee70b4a53baa635f8b4fdd523e62410ce924c",
    "semantic_title": "system performance as a function of calibration methods, sample size and sampling variability in likelihood ratio-based forensic voice comparison",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bonneau21_interspeech.html": {
    "title": "Voicing Assimilations by French Speakers of German in Stop-Fricative Sequences",
    "volume": "main",
    "abstract": "Voicing assimilations inside groups of obstruents occur in opposite directions in French and German, where they are respectively regressive and progressive. The aim of the study is to investigate (1) whether non native speakers (here French learners of German) are apt to acquire subtle L2 specificities like assimilation direction, although they are not aware of their very existence, or (2) whether their productions depend essentially upon other factors, in particular consonant place of articulation. To that purpose, a corpus made up of groups of obstruents (/t/ followed by /z/, /v/ or /f/) embedded into sentences has been recorded by 16 French learners of German (beginners and advanced speakers). The consonants are separated by a word or a syllable boundary. Results, derived from the analysis of consonant periodicity and duration, do not stand for an acquisition of progressive assimilation, even by advanced speakers, and do not show differences between the productions of advanced speakers and beginners. On the contrary the boundary type and the consonant place of articulation play an important role in the presence or absence of voicing inside obstruent groups. The role of phonetic, universal mechanisms against linguistic specific rules is discussed to interpret the data",
    "checked": true,
    "id": "6d4c67542f20ff95ce6aa41ff9c71f6bbab903d5",
    "semantic_title": "voicing assimilations by french speakers of german in stop-fricative sequences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chakraborty21_interspeech.html": {
    "title": "The Four-Way Classification of Stops with Voicing and Aspiration for Non-Native Speech Evaluation",
    "volume": "main",
    "abstract": "The four-way distinction of plosives in terms of voicing and aspiration is rare in the world's languages, but is an important characteristic of the Indo-Aryan language family. Both perception and production pose challenges to the language learner whose native tongue does not afford the specific distinctions. A study of the acoustic-phonetics of the sounds and their possible dependence on speaker characteristics, such as gender or native tongue, can inform methods for accurate feedback on the quality of the phones produced by a non-native learner. We present a system for the four-way classification of stops building on features previously proposed for aspiration detection in unvoiced and voiced plosives. Trained on an available dataset of Hindi speech by native speakers, the system works reliably on production data comprising Bangla words uttered by native Bangla and non-native (American English L1) speakers. The latter display a variety of articulation patterns for the given target contrasts, providing useful insights related to L1 influence on the voicing-aspiration production in word-initial CV contexts",
    "checked": true,
    "id": "b6f2a10ed88619aef50ae19dc486cbc3058f5b8f",
    "semantic_title": "the four-way classification of stops with voicing and aspiration for non-native speech evaluation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/urooj21_interspeech.html": {
    "title": "Acoustic and Prosodic Correlates of Emotions in Urdu Speech",
    "volume": "main",
    "abstract": "Emotional speech corpora exhibit differences in duration, intensity and fundamental frequency. We investigated acoustic as well as prosodic correlates of emotional speech in Urdu. We recorded a corpus of 23 sentences from four speakers of Urdu covering four emotional states. Main results show that: a) sadness exhibits lowest utterance rate, lowest intensity and narrow pitch range, b) anger exhibits highest utterance rate, highest intensity and wider pitch range, and c) happiness exhibits higher utterance rate and wider pitch range as compared to neutral and sadness; but no significant differences are found between the intensity and pitch range of anger and happiness. The analysis also shows differences in terms of pitch or phrase accents and boundary tones",
    "checked": true,
    "id": "634c84a5806bd95455bb11938238a07628c895aa",
    "semantic_title": "acoustic and prosodic correlates of emotions in urdu speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tamim21_interspeech.html": {
    "title": "Voicing Contrasts in the Singleton Stops of Palestinian Arabic: Production and Perception",
    "volume": "main",
    "abstract": "This study investigates the stop voicing contrast in Palestinian Arabic (PA) by examining Voice Onset Time (VOT) in both production and perception. An acoustic analysis of the recordings of 8 speakers showed that word-initial voiced stops in sentence context have an average VOT of -93 msec, and word-initial voiceless stops one of 29 msec. PA thus belongs, like most dialects of Arabic, to true voicing languages, i.e., languages with a contrast between voicing lead and short lag VOT We furthermore tested whether the phoneme /b/, without voiceless counterpart /p/ in PA, has similar VOT values to /d, d /, which have voiceless counterparts /t, t /. Similarly, we compared /k/, without counterpart /g/ in the PA dialect we investigated, to /t, t /. For /b/ we found very similar VOT values to /d, d /, while for /k/ we found a difference to /t, t /, attributable to a general tendency of velars to have longer VOT than denti-alveolars. We thus found no evidence for a less contrastive realization of unpaired plosives in PA In a categorization experiment of the denti-alveolar phoneme pairs with the same 8 speakers, VOT proved sufficient as a perceptual cue, though f0 of the following vowel also influenced the categorization",
    "checked": true,
    "id": "e1f3460fc7dc182432405a26643ecc96603cfdc4",
    "semantic_title": "voicing contrasts in the singleton stops of palestinian arabic: production and perception",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/coy21_interspeech.html": {
    "title": "A Comparison of the Accuracy of Dissen and Keshet's (2016) DeepFormants and Traditional LPC Methods for Semi-Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "There is a growing trend in the field of forensic speech science towards integrating the vanguard of speech technology with traditional linguistic methods in pursuit of both scalable (i.e. automatable) and accurate evidential methods. To this end, this paper investigates DeepFormants, a DNN formant estimator which its creators, Dissen and Keshet [1], claim constitutes an accurate tool ready for use by linguists. In the present paper, DeepFormants is integrated into semi-automatic speaker recognition systems using long-term formant distributions and compared against systems using traditional linear predictive coding. The readiness of the tool is assessed on overall speaker recognition performance, measured using equal error rates (EER) and the log LR cost functions (C ). In high-quality conditions, DeepFormants outperforms the best performing LPC systems. Much poorer overall performance is found in channel mismatch conditions for DeepFormants, suggesting it is not adaptable to conditions it was not originally trained on. However, this is also true of LPC methods, raising questions over the validity of using formant analysis at all in such cases. A major benefit of DeepFormants over LPC is that the analyst does not need to specify settings. We discuss the implications of this with regard to results for individual speakers",
    "checked": true,
    "id": "03415b1cfdb19bff136be84559da5e807187a305",
    "semantic_title": "a comparison of the accuracy of dissen and keshet's (2016) deepformants and traditional lpc methods for semi-automatic speaker recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jessen21_interspeech.html": {
    "title": "MAP Adaptation Characteristics in Forensic Long-Term Formant Analysis",
    "volume": "main",
    "abstract": "Forensic data from long-term formant analysis were used as input to the GMM-UBM approach, which is a way of deriving Likelihood Ratios. Tests were performed running 22 same-speaker comparisons and 462 different-speaker comparisons from a corpus of anonymized casework data involving telephone-intercepted speech. In a first series of tests, the number of Gaussian modules for GMM-modeling was increased from 1 to 32. In a second series of tests the duration of formant input in the compared files was reduced from 10 seconds to 5 and then to 2.5. All tests were performed both without and with the use of MAP adaptation. Results were evaluated in terms of overall performance characteristics EER and Cllr and in terms of score distributions visualized as Tippett plots. The main goal of the study was to compare the use and non-use of MAP and to look at the practical forensic implications of the difference. Results show that in terms of overall performance characteristics there is little difference between the selection and de-selection of MAP. Tippett plot patterns however reveal strong differences. Application of MAP allows for more symmetric same- and different-speaker distributions and shows more robustness against duration reductions, both of which are forensically important",
    "checked": true,
    "id": "2f2a4f02f9c5fe54ad2f99dfec14028b3f13b473",
    "semantic_title": "map adaptation characteristics in forensic long-term formant analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lo21_interspeech.html": {
    "title": "Cross-Linguistic Speaker Individuality of Long-Term Formant Distributions: Phonetic and Forensic Perspectives",
    "volume": "main",
    "abstract": "This study considers issues of language- and speaker-specificity in long-term formant distributions (LTFDs) from phonetic and forensic perspectives and examines their potential value in cases of cross-language forensic voice comparison. Acoustic analysis of 60 male Englishâ€“French bilinguals revealed systematic differences in LTFDs between the two languages, with higher LTF2â€“4 in French than in English. Cross-linguistic differences in the shapes of LTFDs were also found. These differences are argued to reflect not only vowel inventories of each language but also language-specific phonetic settings. At the same time, a high degree of within-speaker consistency was found across languages. Likelihood ratio based testing was carried out to examine the effect of language mismatch on the utility of LTFDs as speaker discriminants. Results showed that while the performance of LTFDs was worse in cross-language comparisons than in same-language comparisons, they were still capable of providing speaker-specific information. These findings demonstrate that, in spite of deteriorated performance, LTFDs are still potentially useful speaker discriminants in cases of language mismatch. These findings thus call for further empirical investigation into the use of linguistic-phonetic features in cross-language comparisons",
    "checked": true,
    "id": "7f2ddbb9a0184e81b9252d5b81538bc41c015040",
    "semantic_title": "cross-linguistic speaker individuality of long-term formant distributions: phonetic and forensic perspectives",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/soo21_interspeech.html": {
    "title": "Sound Change in Spontaneous Bilingual Speech: A Corpus Study on the Cantonese n-l Merger in Cantonese-English Bilinguals",
    "volume": "main",
    "abstract": "In Cantonese and several other Chinese languages, /n/ is merging with /l/. The Cantonese merger appears categorical, with /n/ becoming /l/ word-initially. This project aims to describe the status of /n/ and /l/ in bilingual Cantonese and English speech to better understand individual differences at the interface of crosslinguistic influence and sound change. We examine bilingual speech using the SpiCE corpus, composed of speech from 34 early Cantonese-English bilinguals. Acoustic measures were collected on pre-vocalic nasal and lateral onsets in both languages. If bilinguals maintain separate representations for corresponding segments across languages, smaller differences between /n/ and /l/ are predicted in Cantonese compared to English. Measures of mid-frequency spectral tilt suggest that the /n/ and /l/ contrast is robustly maintained in English, but not Cantonese. The spacing of F2-F1 suggests small differences between Cantonese /n/ and /l/, and robust differences in English. While cross-language categories appear independent, substantial individual differences exist in the data. These data contribute to the understanding of the /n/ and /l/ merger in Cantonese and other Chinese languages, in addition to providing empirical and theoretical insights into crosslinguistic influence in early bilinguals",
    "checked": true,
    "id": "33e65d11709fee658352706e51b3f91efde51710",
    "semantic_title": "sound change in spontaneous bilingual speech: a corpus study on the cantonese n-l merger in cantonese-english bilinguals",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lalhminghlui21_interspeech.html": {
    "title": "Characterizing Voiced and Voiceless Nasals in Mizo",
    "volume": "main",
    "abstract": "Mizo has voicing contrasts in nasals. This study investigates the acoustic properties of Mizo voiced and voiceless nasals using nasometric measurements. The dual channel data obtained for Mizo nasals is separated into oral and nasal channels and nasalance is calculated at every 10% of the duration of the nasals. Apart from that, the amount of voicing and duration of the nasals are also measured. The results show that nasalance is affected by the place of articulation of the nasals. Additionally, the voiceless nasals are found to be significantly longer than the voiced nasals",
    "checked": true,
    "id": "63040a44e70d2c4ecbd62ca15567ece42c26dbbe",
    "semantic_title": "characterizing voiced and voiceless nasals in mizo",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schuller21_interspeech.html": {
    "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation & Primates",
    "volume": "main",
    "abstract": "The INTERSPEECH 2021 Computational Paralinguistics Challenge addresses four different problems for the first time in a research competition under well-defined conditions: In the and Sub-Challenges, a binary classification on COVID-19 infection has to be made based on coughing sounds and speech; in the Sub-Challenge, a three-way assessment of the level of escalation in a dialogue is featured; and in the Sub-Challenge, four species vs background need to be classified. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the â€˜usual' ComParE and BoAW features as well as deep unsupervised representation learning using the auDeep toolkit, and deep feature extraction from pre-trained CNNs using the Deep Spectrum toolkit; in addition, we add deep end-to-end sequential modelling, and partially linguistic analysis",
    "checked": true,
    "id": "12267f4ee5fafc807b793dc09dd8a5b8cee8115e",
    "semantic_title": "the interspeech 2021 computational paralinguistics challenge: covid-19 cough, covid-19 speech, escalation & primates",
    "citation_count": 90
  },
  "https://www.isca-speech.org/archive/interspeech_2021/soleraurena21_interspeech.html": {
    "title": "Transfer Learning-Based Cough Representations for Automatic Detection of COVID-19",
    "volume": "main",
    "abstract": "In the last months, there has been an increasing interest in developing reliable, cost-effective, immediate and easy to use machine learning based tools that can help health care operators, institutions, companies, etc. to optimize their screening campaigns. In this line, several initiatives emerged aimed at the automatic detection of COVID-19 from speech, breathing and coughs, with inconclusive preliminary results. The ComParE 2021 COVID-19 Cough Sub-challenge provides researchers from all over the world a suitable test-bed for the evaluation and comparison of their work. In this paper, we present the INESC-ID contribution to the ComParE 2021 COVID-19 Cough Sub-challenge. We leverage transfer learning to develop a set of three expert classifiers based on deep cough representation extractors. A calibrated decision-level fusion system provides the final classification of coughs recordings as either COVID-19 positive or negative. Results show unweighted average recalls of 72.3% and 69.3% in the development and test sets, respectively. Overall, the experimental assessment shows the potential of this approach although much more research on extended respiratory sounds datasets is needed",
    "checked": true,
    "id": "287547a8cf4c372b975535222caabf03202bcc32",
    "semantic_title": "transfer learning-based cough representations for automatic detection of covid-19",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/klumpp21_interspeech.html": {
    "title": "The Phonetic Footprint of Covid-19?",
    "volume": "main",
    "abstract": "Against the background of the ongoing pandemic, this year's Computational Paralinguistics Challenge featured a classification problem to detect Covid-19 from speech recordings. The presented approach is based on a phonetic analysis of speech samples, thus it enabled us not only to discriminate between Covid and non-Covid samples, but also to better understand how the condition influenced an individual's speech signal Our deep acoustic model was trained with datasets collected exclusively from healthy speakers. It served as a tool for segmentation and feature extraction on the samples from the challenge dataset. Distinct patterns were found in the embeddings of phonetic classes that have their place of articulation deep inside the vocal tract. We observed profound differences in classification results for development and test splits, similar to the baseline method We concluded that, based on our phonetic findings, it was safe to assume that our classifier was able to reliably detect a pathological condition located in the respiratory tract. However, we found no evidence to claim that the system was able to discriminate between Covid-19 and other respiratory diseases",
    "checked": true,
    "id": "608d76a77413cf7e1e47a926f9838b6fa5cba32d",
    "semantic_title": "the phonetic footprint of covid-19?",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/casanova21_interspeech.html": {
    "title": "Transfer Learning and Data Augmentation Techniques to the COVID-19 Identification Tasks in ComParE 2021",
    "volume": "main",
    "abstract": "In this work, we propose several techniques to address data scarceness in ComParE 2021 COVID-19 identification tasks for the application of deep models such as Convolutional Neural Networks. Data is initially preprocessed into spectrogram or MFCC-gram formats. After preprocessing, we combine three different data augmentation techniques to be applied in model training. Then we employ transfer learning techniques from pretrained audio neural networks. Those techniques are applied to several distinct neural architectures. For COVID-19 identification in speech segments, we obtained competitive results. On the other hand, in the identification task based on cough data, we succeeded in producing a noticeable improvement on existing baselines, reaching 75.9% unweighted average recall (UAR)",
    "checked": true,
    "id": "9a4d3eaf07bd3b90a055f7ca53a07de6683d8291",
    "semantic_title": "transfer learning and data augmentation techniques to the covid-19 identification tasks in compare 2021",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/illium21_interspeech.html": {
    "title": "Visual Transformers for Primates Classification and Covid Detection",
    "volume": "main",
    "abstract": "We apply the vision transformer, a deep machine learning model build around the attention mechanism, on mel-spectrogram representations of raw audio recordings. When adding mel-based data augmentation techniques and sample-weighting, we achieve comparable performance on both (PRS and CCS challenge) tasks of ComParE21, outperforming most single model baselines. We further introduce overlapping vertical patching and evaluate the influence of parameter configurations",
    "checked": true,
    "id": "65fe63a40f54f3c4d231f10f351001a27c6151c5",
    "semantic_title": "visual transformers for primates classification and covid detection",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pellegrini21_interspeech.html": {
    "title": "Deep-Learning-Based Central African Primate Species Classification with MixUp and SpecAugment",
    "volume": "main",
    "abstract": "In this paper, we report experiments in which we aim to automatically classify primate vocalizations according to four primate species of interest, plus a background category with forest sound events. We compare several standard deep neural networks architectures: standard deep convolutional neural networks (CNNs), MobileNets and ResNets. To tackle the small size of the training dataset, less than seven thousand audio files, the data augmentation techniques SpecAugment and MixUp proved to be very useful. Against the very unbalanced classes of the dataset, we used a balanced data sampler that showed to be efficient. An exponential moving average of the model weights allowed to get slight further gains. The best model was a standard 10-layer CNN, comprised of about five million parameters. It achieved a 93.6% Unweighted Average Recall (UAR) on the development set, and generalized well on the test set with a 92.5% UAR, outperforming an official baseline of 86.6%. We quantify the performance gains brought by the augmentations and training tricks, and report fusion and classification experiments based on embeddings that did not bring better results",
    "checked": true,
    "id": "e343a2d6792e3739a9c0de9857e34b7eb59b72cc",
    "semantic_title": "deep-learning-based central african primate species classification with mixup and specaugment",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muller21_interspeech.html": {
    "title": "A Deep and Recurrent Architecture for Primate Vocalization Classification",
    "volume": "main",
    "abstract": "Wildlife monitoring is an essential part of most conservation efforts where one of the many building blocks is acoustic monitoring. Acoustic monitoring has the advantage of being non-invasive and applicable in areas of high vegetation. In this work, we present a deep and recurrent architecture for the classification of primate vocalizations that is based upon well proven modules such as bidirectional Long Short-Term Memory neural networks, pooling, normalized softmax and focal loss. Additionally, we apply Bayesian optimization to obtain a suitable set of hyperparameters. We test our approach on a recently published dataset of primate vocalizations that were recorded in an African wildlife sanctuary. Using an ensemble of the best five models found during hyperparameter optimization on the development set, we achieve a Unweighted Average Recall of 89.3% on the test set. Our approach outperforms the best baseline, an ensemble of various deep and shallow classifiers, which achieves a UAR of 87.5%",
    "checked": true,
    "id": "d4008aeffed6e875397c94433cbee47595e0ae75",
    "semantic_title": "a deep and recurrent architecture for primate vocalization classification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zwerts21_interspeech.html": {
    "title": "Introducing a Central African Primate Vocalisation Dataset for Automated Species Classification",
    "volume": "main",
    "abstract": "Automated classification of animal vocalisations is a potentially powerful wildlife monitoring tool. Training robust classifiers requires sizable annotated datasets, which are not easily recorded in the wild. To circumvent this problem, we recorded four primate species under semi-natural conditions in a wildlife sanctuary in Cameroon with the objective to train a classifier capable of detecting species in the wild. Here, we introduce the collected dataset, describe our approach and initial results of classifier development. To increase the efficiency of the annotation process, we condensed the recordings with an energy/change based automatic vocalisation detection. Segmenting the annotated chunks into training, validation and test sets, initial results reveal up to 82% unweighted average recall test set performance in four-class primate species classification",
    "checked": true,
    "id": "98a1034c5c0d30f7c83d3a637aa36de02c58e64f",
    "semantic_title": "introducing a central african primate vocalisation dataset for automated species classification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rizos21_interspeech.html": {
    "title": "Multi-Attentive Detection of the Spider Monkey Whinny in the (Actual) Wild",
    "volume": "main",
    "abstract": "We study deep bioacoustic event detection through multi-head attention based pooling, exemplified by wildlife monitoring. In the multiple instance learning framework, a core deep neural network learns a projection of the input acoustic signal into a sequence of embeddings, each representing a segment of the input. Sequence pooling is then required to aggregate the information present in the sequence such that we have a single clip-wise representation. We propose an improvement based on Squeeze-and-Excitation mechanisms upon a recently proposed audio tagging ResNet, and show that it performs significantly better than the baseline, as well as a collection of other recent audio models. We then further enhance our model, by performing an extensive comparative study of recent sequence pooling mechanisms, and achieve our best result using multi-head self-attention followed by concatenation of the head-specific pooled embeddings â€” better than prediction pooling methods, as well as compared to other recent sequence pooling tricks. We perform these experiments on a novel dataset of spider monkey whinny calls we introduce here, recorded in a rainforest in the South-Pacific coast of Costa Rica, with a promising outlook pertaining to minimally invasive wildlife monitoring",
    "checked": true,
    "id": "e472f18d522b9c49f006859ac0caf51072ad1d11",
    "semantic_title": "multi-attentive detection of the spider monkey whinny in the (actual) wild",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/egaslopez21_interspeech.html": {
    "title": "Identifying Conflict Escalation and Primates by Using Ensemble X-Vectors and Fisher Vector Features",
    "volume": "main",
    "abstract": "Computational paralinguistics is concerned with the automatic identification of non-verbal information in human speech. The Interspeech ComParE challenge features new paralinguistic tasks each year; this time, among others, a cross-corpus conflict escalation task and the identification of primates based solely on audio are the actual problems set. In our entry to ComParE 2021, we utilize x-vectors and Fisher vectors as features. To improve the robustness of the predictions, we also experiment with building an ensemble of classifiers from the x-vectors. Lastly, we exploit the fact that the Escalation Sub-Challenge is a conflict detection task, and incorporate the SSPNet Conflict Corpus in our training workflow. Using these approaches, at the time of writing, we had already surpassed the official Challenge baselines on both tasks, which demonstrates the efficiency of the employed techniques",
    "checked": true,
    "id": "f65af2bb8200779b9c09bf7be5567d713b756803",
    "semantic_title": "identifying conflict escalation and primates by using ensemble x-vectors and fisher vector features",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/verkholyak21_interspeech.html": {
    "title": "Ensemble-Within-Ensemble Classification for Escalation Prediction from Speech",
    "volume": "main",
    "abstract": "Conflict situations arise frequently in our daily life and often require timely response to resolve the issues. In order to automatically classify conflict (also referred to as escalation) speech utterances we propose ensemble learning as it improves prediction performance by combining several heterogeneous models that compensate for each other's weaknesses. However, the effectiveness of the classification ensemble greatly depends on its constituents and their fusion strategy. This paper provides experimental evidence for effectiveness of different prediction-level fusion strategies and demonstrates the performance of each proposed ensemble on the Escalation Sub-Challenge (ESS) in the framework of the Computational Paralinguistics Challenge (ComParE-2021). The ensembles comprise various machine learning approaches based on acoustic and linguistic characteristics of speech. The training strategy is specifically designed to increase the generalization performance on the unseen data, while the diverse nature of ensemble candidates ensures high prediction power and accurate classification",
    "checked": true,
    "id": "9dc936b03cae158437d844e152d01fde3641c0d5",
    "semantic_title": "ensemble-within-ensemble classification for escalation prediction from speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schiller21_interspeech.html": {
    "title": "Analysis by Synthesis: Using an Expressive TTS Model as Feature Extractor for Paralinguistic Speech Classification",
    "volume": "main",
    "abstract": "Modeling adequate features of speech prosody is one key factor to good performance in affective speech classification. However, the distinction between the prosody that is induced by â€˜how' something is said (i.e., affective prosody) and the prosody that is induced by â€˜what' is being said (i.e., linguistic prosody) is neglected in state-of-the-art feature extraction systems. This results in high variability of the calculated feature values for different sentences that are spoken with the same affective intent, which might negatively impact the performance of the classification. While this distinction between different prosody types is mostly neglected in affective speech recognition, it is explicitly modeled in expressive speech synthesis to create controlled prosodic variation. In this work, we use the expressive Text-To-Speech model Global Style Token Tacotron to extract features for a speech analysis task. We show that the learned prosodic representations outperform state-of-the-art feature extraction systems in the exemplary use case of Escalation Level Classification",
    "checked": true,
    "id": "3dd293025a3b66ae924067bb29ebee319dda55cc",
    "semantic_title": "analysis by synthesis: using an expressive tts model as feature extractor for paralinguistic speech classification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/christensen21_interspeech.html": {
    "title": "Towards Automatic Speech Recognition for People with Atypical Speech",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "34b94c4c71f0c2de77bfbaab5d9ecf2ba261f85b",
    "semantic_title": "towards automatic speech recognition for people with atypical speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luu21_interspeech.html": {
    "title": "Leveraging Speaker Attribute Information Using Multi Task Learning for Speaker Verification and Diarization",
    "volume": "main",
    "abstract": "Deep speaker embeddings have become the leading method for encoding speaker identity in speaker recognition tasks. The embedding space should ideally capture the variations between all possible speakers, encoding the multiple acoustic aspects that make up a speaker's identity, whilst being robust to non-speaker acoustic variation. Deep speaker embeddings are normally trained discriminatively, predicting speaker identity labels on the training data. We hypothesise that additionally predicting speaker-related auxiliary variables â€” such as age and nationality â€” may yield representations that are better able to generalise to unseen speakers. We propose a framework for making use of auxiliary label information, even when it is only available for speech corpora mismatched to the target application. On a test set of US Supreme Court recordings, we show that by leveraging two additional forms of speaker attribute information derived respectively from the matched training data, and VoxCeleb corpus, we improve the performance of our deep speaker embeddings for both verification and diarization tasks, achieving a relative improvement of 26.2% in DER and 6.7% in EER compared to baselines using speaker labels only. This improvement is obtained despite the auxiliary labels having been scraped from the web and being potentially noisy",
    "checked": true,
    "id": "3843ef594c733b8c8d854535c674d0fbff292eb6",
    "semantic_title": "leveraging speaker attribute information using multi task learning for speaker verification and diarization",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rybicka21_interspeech.html": {
    "title": "Spine2Net: SpineNet with Res2Net and Time-Squeeze-and-Excitation Blocks for Speaker Recognition",
    "volume": "main",
    "abstract": "Modeling speaker embeddings using deep neural networks is currently state-of-the-art in speaker recognition. Recently, ResNet-based structures have gained a broader interest, slowly becoming the baseline along with the deep-rooted Time Delay Neural Network based models. However, the scale-decreased design of the ResNet models may not preserve all of the speaker information. In this paper, we investigate the SpineNet structure with scale-permuted design to tackle this problem, in which feature size either increases or decreases depending on the processing stage in the network. Apart from the presented adjustments of the SpineNet model for the speaker recognition task, we also incorporate popular modules dedicated to the residual-like structures, namely the Res2Net and Squeeze-and-Excitation blocks, and modify them to work effectively in the presented neural network architectures. The final proposed model, i.e., the SpineNet architecture with Res2Net and Time-Squeeze-and-Excitation blocks, achieves remarkable Equal Error Rates of 0.99 and 0.92 for the Extended and Original trial lists of the well-known VoxCeleb1 dataset",
    "checked": true,
    "id": "62a007787bdf51bb58668d2a88df18850c4e9e28",
    "semantic_title": "spine2net: spinenet with res2net and time-squeeze-and-excitation blocks for speaker recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stafylakis21_interspeech.html": {
    "title": "Speaker Embeddings by Modeling Channel-Wise Correlations",
    "volume": "main",
    "abstract": "Speaker embeddings extracted with deep 2D convolutional neural networks are typically modeled as projections of first and second order statistics of channel-frequency pairs onto a linear layer, using either average or attentive pooling along the time axis. In this paper we examine an alternative pooling method, where pairwise correlations between channels for given frequencies are used as statistics. The method is inspired by style-transfer methods in computer vision, where the style of an image, modeled by the matrix of channel-wise correlations, is transferred to another image, in order to produce a new image having the style of the first and the content of the second. By drawing analogies between image style and speaker characteristics, and between image content and phonetic sequence, we explore the use of such channel-wise correlations features to train a ResNet architecture in an end-to-end fashion. Our experiments on VoxCeleb demonstrate the effectiveness of the proposed pooling method in speaker recognition",
    "checked": true,
    "id": "87382405a9f9abb7f0eabce17799b626f2e20e4b",
    "semantic_title": "speaker embeddings by modeling channel-wise correlations",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/he21_interspeech.html": {
    "title": "Multi-Task Neural Network for Robust Multiple Speaker Embedding Extraction",
    "volume": "main",
    "abstract": "This paper introduces a novel approach for extracting speaker embeddings from audio mixtures of multiple overlapping voices. This approach is based on a multi-task neural network. The network first extracts a latent feature for each direction. This feature is used for detecting sound sources as well as identifying speakers. In contrast to traditional approaches, the proposed method does not rely on explicit sound source separation. The neural network model learns from data to extract the most suitable features of the sounds at different directions. The experiments using audio recordings of overlapping sound sources show that the proposed approach outperforms a beamforming-based traditional method",
    "checked": true,
    "id": "e8a9987893824dadd9c72692a2b0448e3951a5a6",
    "semantic_title": "multi-task neural network for robust multiple speaker embedding extraction",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21_interspeech.html": {
    "title": "ICSpk: Interpretable Complex Speaker Embedding Extractor from Raw Waveform",
    "volume": "main",
    "abstract": "Recently, extracting speaker embedding directly from raw waveform has drawn increasing attention in the field of speaker verification. Parametric real-valued filters in the first convolutional layer are learned to transform the waveform into time-frequency representations. However, these methods only focus on the magnitude spectrum and the poor interpretability of the learned filters limits the performance. In this paper, we propose a complex speaker embedding extractor, named ICSpk, with higher interpretability and fewer parameters. Specifically, at first, to quantify the speaker-related frequency response of waveform, we modify the original short-term Fourier transform filters into a family of complex exponential filters, named interpretable complex (IC) filters. Each IC filter is confined by a complex exponential filter parameterized by frequency. Then, a deep complex-valued speaker embedding extractor is designed to operate on the complex-valued output of IC filters. The proposed ICSpk is evaluated on VoxCeleb and CNCeleb databases. Experimental results demonstrate the IC filters-based system exhibits a significant improvement over the complex spectrogram based systems. Furthermore, the proposed ICSpk outperforms existing raw waveform based systems by a large margin",
    "checked": true,
    "id": "4cd567a2dd9b55247179ac136cd236e4941aa4a4",
    "semantic_title": "icspk: interpretable complex speaker embedding extractor from raw waveform",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xiao21_interspeech.html": {
    "title": "Prosodic Disambiguation Using Chironomic Stylization of Intonation with Native and Non-Native Speakers",
    "volume": "main",
    "abstract": "This paper introduces an interface that enables the real-time gestural control of intonation in phrases produced by a vocal synthesizer. The melody and timing of a target phrase can be modified by tracing melodic contours on the touch-screen of a mobile tablet. Envisioning this interface as a means for non-native speakers to practice the intonation of a foreign language, we present a pilot study where native and non-native speakers imitated the pronunciation of French phrases using their voice and the interface, with a visual guide and without. Comparison of resulting F0 curves against the reference contour and a preliminary perceptual assessment of synthesized utterances suggest that for both non-native and native speakers, imitation with the help of a visual guide is comparable in accuracy to vocal imitation, and that timing control was a source of difficulty",
    "checked": true,
    "id": "f97cd9369c07403dfe622fbd74b33ae35622516b",
    "semantic_title": "prosodic disambiguation using chironomic stylization of intonation with native and non-native speakers",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/block21_interspeech.html": {
    "title": "Variation in Perceptual Sensitivity and Compensation for Coarticulation Across Adult and Child Naturally-Produced and TTS Voices",
    "volume": "main",
    "abstract": "The current study explores whether perception of coarticulatory vowel nasalization differs by speaker age (adult vs. child) and type of voice (naturally produced vs. synthetic speech). Listeners completed a 4IAX discrimination task between pairs containing acoustically identical (both nasal or oral) vowels and acoustically distinct (one oral, one nasal) vowels. Vowels occurred in either the same consonant contexts or different contexts across pairs. Listeners completed the experiment with either naturally produced speech or text-to-speech (TTS). For same-context trials, listeners were better at discriminating between oral and nasal vowels for child speech in the synthetic voices but adult speech in the natural voices. Meanwhile, in different-context trials, listeners were less able to discriminate, indicating more perceptual compensation for synthetic voices. There was no difference in different-context discrimination across talker ages, indicating that listeners did not compensate differently if the speaker was a child or adult. Findings are relevant for models of compensation, computer personification theories, and speaker-indexical perception accounts",
    "checked": true,
    "id": "9156bfc6f44686cf3391398e58989978cb87bb4e",
    "semantic_title": "variation in perceptual sensitivity and compensation for coarticulation across adult and child naturally-produced and tts voices",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/monesi21_interspeech.html": {
    "title": "Extracting Different Levels of Speech Information from EEG Using an LSTM-Based Model",
    "volume": "main",
    "abstract": "Decoding the speech signal that a person is listening to from the human brain via electroencephalography (EEG) can help us understand how our auditory system works. Linear models have been used to reconstruct the EEG from speech or vice versa. Recently, Artificial Neural Networks (ANNs) such as Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based architectures have outperformed linear models in modeling the relation between EEG and speech. Before attempting to use these models in real-world applications such as hearing tests or (second) language comprehension assessment we need to know what level of speech information is being utilized by these models. In this study, we aim to analyze the performance of an LSTM-based model using different levels of speech features. The task of the model is to determine which of two given speech segments is matched with the recorded EEG. We used low- and high-level speech features including: envelope, mel spectrogram, voice activity, phoneme identity, and word embedding. Our results suggest that the model exploits information about silences, intensity, and broad phonetic classes from the EEG. Furthermore, the mel spectrogram, which contains all this information, yields the highest accuracy (84%) among all the features",
    "checked": true,
    "id": "e7b5f3cd4ccced32cc691a47344e66fca1c52f88",
    "semantic_title": "extracting different levels of speech information from eeg using an lstm-based model",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bosch21_interspeech.html": {
    "title": "Word Competition: An Entropy-Based Approach in the DIANA Model of Human Word Comprehension",
    "volume": "main",
    "abstract": "We discuss the role of entropy of the set of unfolding word candidates in the context of DIANA, a computational model of human auditory speech comprehension. DIANA consists of three major interacting components: Activation, Decision and Execution. The Activation component computes activations of word candidates that change over time as a function of the unfolding audio input. The resulting set of word candidate activations can be associated with an entropy that is related to difficulty of the decision when one of these candidates must be selected at time T. The paper presents the close relation between entropy measures and the between-word competition during the unfolding of the auditory stimuli, and at the end of the stimuli if no decision could be made before stimulus offset. We present a way for computing the entropy that takes into account linguistic-phonetic constraints that play a role in speech comprehension and in lexical decision experiments. Using the BALDEY data set and linear mixed effects regression models for RT, we show that entropy measures explain differences between RTs of words with different morphological structure",
    "checked": true,
    "id": "912d256af6fbb78cd79b5f323aa69ceb9d5234ae",
    "semantic_title": "word competition: an entropy-based approach in the diana model of human word comprehension",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bosch21b_interspeech.html": {
    "title": "Time-to-Event Models for Analyzing Reaction Time Sequences",
    "volume": "main",
    "abstract": "We investigate reaction time (RT) sequences obtained from lexical decision experiments by applying Time-to-Event modelling (Survival Analysis). This is a branch of statistics for analyzing the expected duration until one or more events happen, associated with a set of potential â€˜causes' (in our case the decision for a â€˜word' judgment as a function of conventional predictors such as lexical frequency, stimulus duration, reduction, etc.). In this analysis, RTs are considered a by-product of an (unobservable) cumulative incidence function that results in a decision when it exceeds a certain threshold We show that Survival Analysis can be effectively used to narrow the gap between data-oriented models and process-oriented models for RT data from lexical decision experiments. Results of this analysis technique are presented for two different RT data sets. The analysis reveals time-varying patterns of predictors that reflect the differences in cognitive processes during the presentation of auditory stimuli",
    "checked": true,
    "id": "5f6773faef5ee141af529d58862c7de1efe8e574",
    "semantic_title": "time-to-event models for analyzing reaction time sequences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/brand21_interspeech.html": {
    "title": "Models of Reaction Times in Auditory Lexical Decision: RTonset versus RToffset",
    "volume": "main",
    "abstract": "We investigate how the role of predictors in models of reaction times in auditory lexical decision experiments depends on the operational definition of RT: whether the time is measured from stimulus onset or from stimulus offset. In a large body of literature, RTs are measured from the onset of the stimulus to the start of the response (often a button press or an oral response). The rationale behind this choice is that information about the stimulus becomes available to the listener starting at onset. Alternatively, the RT from offset is less dependent on stimulus duration and is assumed to focus on those cognitive processes that play a role late(r) in the word and after word offset, when all information is available The paper presents RT-onset and RT-offset-based linear mixed effects models for three different lexical decision-based data sets and explains the significant differences between these models, showing to what extent both definitions of reaction time reveal different roles for predictors and how early and later contributions to the overall RT can be differentiated",
    "checked": true,
    "id": "f5fbf5e3d6fa5e4f556c5b0853fc09b72f6716d4",
    "semantic_title": "models of reaction times in auditory lexical decision: rtonset versus rtoffset",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21c_interspeech.html": {
    "title": "SpecMix : A Mixed Sample Data Augmentation Method for Training with Time-Frequency Domain Features",
    "volume": "main",
    "abstract": "A mixed sample data augmentation strategy is proposed to enhance the performance of models on audio scene classification, sound event classification, and speech enhancement tasks. While there have been several augmentation methods shown to be effective in improving image classification performance, their efficacy toward time-frequency domain features of audio is not assured. We propose a novel audio data augmentation approach named \"Specmix\" specifically designed for dealing with time-frequency domain features. The augmentation method consists of mixing two different data samples by applying time-frequency masks effective in preserving the spectral correlation of each audio sample. Our experiments on acoustic scene classification, sound event classification, and speech enhancement tasks show that the proposed Specmix improves the performance of various neural network architectures by a maximum of 2.7%",
    "checked": false,
    "id": "c514b8b084f5e210228f44c95dd5a9ac99d914b0",
    "semantic_title": "specmix : a mixed sample data augmentation method for training withtime-frequency domain features",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21d_interspeech.html": {
    "title": "SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "In this paper, we present SpecAugment++, a novel data augmentation method for deep neural networks based acoustic scene classification (ASC). Different from other popular data augmentation methods such as SpecAugment and mixup that only work on the input space, SpecAugment++ is applied to both the input space and the hidden space of the deep neural networks to enhance the input and the intermediate feature representations. For an intermediate hidden state, the augmentation techniques consist of masking blocks of frequency channels and masking blocks of time frames, which improve generalization by enabling a model to attend not only to the most discriminative parts of the feature, but also the entire parts. Apart from using zeros for masking, we also examine two approaches for masking based on the use of other samples within the mini-batch, which helps introduce noises to the networks to make them more discriminative for classification. The experimental results on the DCASE 2018 Task1 dataset and DCASE 2019 Task1 dataset show that our proposed method can obtain 3.6% and 4.7% accuracy gains over a strong baseline without augmentation (i.e. CP-ResNet) respectively, and outperforms other previous data augmentation methods",
    "checked": true,
    "id": "254a094e5269e76cb92f7169c160fd9c67a057bd",
    "semantic_title": "specaugment++: a hidden space data augmentation method for acoustic scene classification",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zheng21_interspeech.html": {
    "title": "An Effective Mutual Mean Teaching Based Domain Adaptation Method for Sound Event Detection",
    "volume": "main",
    "abstract": "In this paper, we present a novel mutual mean teaching based domain adaptation (MMT-DA) method for sound event detection (SED) task, which can effectively exploit synthetic data to improve the SED performance. Existing methods simply treat the synthetic data as strongly-labeled data in semi-supervised learning (SSL) framework. Benefiting from the strong labels of synthetic data, superior SED performance can be achieved. However, a distribution mismatch between synthetic and real data raises an evident challenge for domain adaptation (DA). In MMT-DA, convolutional recurrent neural networks (CRNN) learned from different datasets (i.e :real+synthetic, and ) are exploited for DA. Specifically, mean teacher method using CRNN is employed for utilizing the unlabeled real data. To compensate the domain diversity, an additional domain classifier with gradient reverse layer(GRL) is used for training a mean teacher for The student CRNNs are mutually taught using the soft predictions of unlabeled data obtained from different teachers. Furthermore, a strip pooling based attention module is exploited to model the inter-dependencies between channels and time-frequency dimensions to exploit the structure information. Experimental results on Task4 of DCASE2020 demonstrate the ability of the proposed method, achieving 52.0% F1-score on the validation dataset, which outperforms the winning system's 50.6%",
    "checked": true,
    "id": "831d0bf67a37506dec36c5cfddd8bd62a2a519b5",
    "semantic_title": "an effective mutual mean teaching based domain adaptation method for sound event detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nandi21_interspeech.html": {
    "title": "Acoustic Scene Classification Using Kervolution-Based SubSpectralNet",
    "volume": "main",
    "abstract": "In this paper, a Kervolution-based SubSpectralNet model is proposed for Acoustic Scene Classification (ASC). SubSpectralNet is a competitive model which divides the mel spectrogram into horizontal slices termed as sub-spectrograms that are considered as input to the Convolutional Neural Network (CNN). In this work, the linear convolutional operation of SubSpectralNet is replaced with a non-linear operation using the kernel trick. This is also known as kervolution (kernel convolution)-based SubSpectralNet. The performance of the proposed methodology is evaluated on the DCASE (Detection and Classification of Acoustic Scenes and Events) 2018 development dataset. The proposed method achieves 73.52% and 75.76% accuracy with Polynomial and Gaussian Kernels respectively",
    "checked": true,
    "id": "7a5da2afd67169d3df47d4bff55b41878e1b7443",
    "semantic_title": "acoustic scene classification using kervolution-based subspectralnet",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sundar21_interspeech.html": {
    "title": "Event Specific Attention for Polyphonic Sound Event Detection",
    "volume": "main",
    "abstract": "The concept of multi-headed self attention (MHSA) introduced as a critical building block of a Transformer Encoder/Decoder Module has made a significant impact in the areas of natural language processing (NLP), automatic speech recognition (ASR) and recently in the area of sound event detection (SED). The current state-of-the-art approaches to SED employ a shared attention mechanism achieved through a stack of MHSA blocks to detect multiple sound events. Consequently, in a multi-label SED task, a common attention mechanism would be responsible for generating relevant feature representations for each of the events to be detected. In this paper, we show through empirical evaluation that having more MHSA blocks dedicated specifically for individual events, rather than having a stack of shared MHSA blocks, improves the overall detection performance. Interestingly, this improvement in performance comes about because the event-specific attention blocks help in resolving confusions in the case of co-occurring events. The proposed \"Event-specific Attention Network\" (ESA-Net) can be trained in an end-to-end manner. On the DCASE 2020 Task 4 data set, we show that with ESA-Net, the best single model achieves an event-based F1 score of 52.1% on the public validation data set improving over the existing state of the art result",
    "checked": true,
    "id": "6fc0c8b8beaebf9b76769f64eff495d0ef8bf88e",
    "semantic_title": "event specific attention for polyphonic sound event detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21b_interspeech.html": {
    "title": "AST: Audio Spectrogram Transformer",
    "volume": "main",
    "abstract": "In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2",
    "checked": true,
    "id": "0e2d8b8d81092037f9866c1ceddcebb87318e38b",
    "semantic_title": "ast: audio spectrogram transformer",
    "citation_count": 300
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seo21_interspeech.html": {
    "title": "Shallow Convolution-Augmented Transformer with Differentiable Neural Computer for Low-Complexity Classification of Variable-Length Acoustic Scene",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNNs) exhibit good performance in low-complexity classification with fixed-length acoustic scenes. However, previous studies have not considered variable-length acoustic scenes in which performance degradation is prevalent. In this regard, we investigate two novel architectures â€” convolution-augmented transformer (Conformer) and differentiable neural computer (DNC). Both the models show desirable performance for variable-length data but require a large amount of data. In other words, small amounts of data, such as those from acoustic scenes, lead to overfitting in these models. In this paper, we propose a shallow convolution-augmented Transformer with a differentiable neural computer (shallow Conformer-DNC) for the low-complexity classification of variable-length acoustic scenes. The shallow Conformer-DNC is enabled to converge with small amounts of data. Short-term and long-term contexts of variable-length acoustic scenes are trained by using the shallow Conformer and shallow DNC, respectively. The experiments were conducted for variable-length conditions using the TAU Urban Acoustic Scenes 2020 Mobile dataset. As a result, a peak accuracy of 61.25% was confirmed for shallow Conformer-DNC with a model parameter of 34 K. It is comparable performance to state-of-the-art CNNs",
    "checked": true,
    "id": "100bb70815cd72ab469d885d1eae43a0e0b1de29",
    "semantic_title": "shallow convolution-augmented transformer with differentiable neural computer for low-complexity classification of variable-length acoustic scene",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bear21_interspeech.html": {
    "title": "An Evaluation of Data Augmentation Methods for Sound Scene Geotagging",
    "volume": "main",
    "abstract": "Sound scene geotagging is a new topic of research which has evolved from acoustic scene classification. It is motivated by the idea of audio surveillance. Not content with only describing a scene in a recording, a machine which can locate where the recording was captured would be of use to many. In this paper we explore a series of common audio data augmentation methods to evaluate which best improves the accuracy of audio geotagging classifiers Our work improves on the state-of-the-art city geotagging method by 23% in terms of classification accuracy",
    "checked": true,
    "id": "cb767ecb7f2c1b5b33b7b09a82a3ccd9d8aa2749",
    "semantic_title": "an evaluation of data augmentation methods for sound scene geotagging",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hori21_interspeech.html": {
    "title": "Optimizing Latency for Online Video Captioning Using Audio-Visual Transformers",
    "volume": "main",
    "abstract": "Video captioning is an essential technology to understand scenes and describe events in natural language. To apply it to real-time monitoring, a system needs not only to describe events accurately but also to produce the captions as soon as possible. Low-latency captioning is needed to realize such functionality, but this research area for online video captioning has not been pursued yet. This paper proposes a novel approach to optimize each caption's output timing based on a trade-off between latency and caption quality. An audio-visual Transformer is trained to generate ground-truth captions using only a small portion of all video frames, and to mimic outputs of a pre-trained Transformer to which all the frames are given. A CNN-based timing detector is also trained to detect a proper output timing, where the captions generated by the two Transformers become sufficiently close to each other. With the jointly trained Transformer and timing detector, a caption can be generated in the early stages of an event-triggered video clip, as soon as an event happens or when it can be forecasted. Experiments with the ActivityNet Captions dataset show that our approach achieves 94% of the caption quality of the upper bound given by the pre-trained Transformer using the entire video clips, using only 28% of frames from the beginning",
    "checked": false,
    "id": "343da36b7e02087342e5e29d538fef6cc24d97c1",
    "semantic_title": "optimizing latency for online video captioningusing audio-visual transformers",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/si21_interspeech.html": {
    "title": "Variational Information Bottleneck for Effective Low-Resource Audio Classification",
    "volume": "main",
    "abstract": "Large-scale deep neural networks (DNNs) such as convolutional neural networks (CNNs) have achieved impressive performance in audio classification for their powerful capacity and strong generalization ability. However, when training a DNN model on low-resource tasks, it is usually prone to overfitting the small data and learning too much redundant information. To address this issue, we propose to use variational information bottleneck (VIB) to mitigate overfitting and suppress irrelevant information. In this work, we conduct experiments on a 4-layer CNN. However, the VIB framework is ready-to-use and could be easily utilized with many other state-of-the-art network architectures. Evaluation on a few audio datasets shows that our approach significantly outperforms baseline methods, yielding â‰¥ 5.0% improvement in terms of classification accuracy in some low-source settings",
    "checked": true,
    "id": "a384336e61895d1b05e136ab0dcef9addf8f7548",
    "semantic_title": "variational information bottleneck for effective low-resource audio classification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deshmukh21_interspeech.html": {
    "title": "Improving Weakly Supervised Sound Event Detection with Self-Supervised Auxiliary Tasks",
    "volume": "main",
    "abstract": "While multitask and transfer learning has shown to improve the performance of neural networks in limited data settings, they require pretraining of the model on large datasets beforehand. In this paper, we focus on improving the performance of weakly supervised sound event detection in low data and noisy settings simultaneously without requiring any pretraining task. To that extent, we propose a shared encoder architecture with sound event detection as a primary task and an additional secondary decoder for a self-supervised auxiliary task. We empirically evaluate the proposed framework for weakly supervised sound event detection on a remix dataset of the DCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20 dB SNR. To ensure we retain the localisation information of multiple sound events, we propose a two-step attention pooling mechanism that provides a time-frequency localisation of multiple audio events in the clip. The proposed framework with two-step attention outperforms existing benchmark models by 22.3%, 12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an ablation study to determine the contribution of the auxiliary task and two-step attention pooling to the SED performance improvement",
    "checked": true,
    "id": "ba3a4ce1e04276ddde8ba7cdb05192c24cae10ea",
    "semantic_title": "improving weakly supervised sound event detection with self-supervised auxiliary tasks",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/komatsu21_interspeech.html": {
    "title": "Acoustic Event Detection with Classifier Chains",
    "volume": "main",
    "abstract": "This paper proposes acoustic event detection (AED) with classifier chains, a new classifier based on the probabilistic chain rule. The proposed AED with classifier chains consists of a gated recurrent unit and performs iterative binary detection of each event one by one. In each iteration, the event's activity is estimated and used to condition the next output based on the probabilistic chain rule to form classifier chains. Therefore, the proposed method can handle the interdependence among events upon classification, while the conventional AED methods with multiple binary classifiers with a linear layer and sigmoid function have placed an assumption of conditional independence. In the experiments with a real-recording dataset, the proposed method demonstrates its superior AED performance to a relative 14.80% improvement compared to a convolutional recurrent neural network baseline system with the multiple binary classifiers",
    "checked": true,
    "id": "3f5b7fcb6fc50ba80318ab959f3d63253cd0ef6b",
    "semantic_title": "acoustic event detection with classifier chains",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tseng21_interspeech.html": {
    "title": "Segment and Tone Production in Continuous Speech of Hearing and Hearing-Impaired Children",
    "volume": "main",
    "abstract": "Verbal communication in daily use is conducted in the form of continuous speech that theoretically is the ideal data format for assessing oral language ability in educational and clinical domains. But as phonetic reduction and particularly lexical tones in Chinese are greatly affected by discourse context, it is a challenging task for automatic systems to evaluate continuous speech only by acoustic features. This study analyzed repetitive and storytelling speech produced by selected Chinese-speaking hearing and hearing-impaired children with distinctively high and low speech intelligibility levels. Word-based reduction types are derived by phonological properties that characterize contraction degrees of automatically generated surface forms of disyllabic words. F0-based tonal contours are visualized using the centroid-nearest data points in the major clusters computed for tonal syllables. Our results show that primary speech characteristics across different groups of children can be differentiated by means of reduction type and tone production",
    "checked": true,
    "id": "3a79c74a5f7b527b94cb93c8401fd3c37433d90d",
    "semantic_title": "segment and tone production in continuous speech of hearing and hearing-impaired children",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21e_interspeech.html": {
    "title": "Effect of Carrier Bandwidth on Understanding Mandarin Sentences in Simulated Electric-Acoustic Hearing",
    "volume": "main",
    "abstract": "For patients suffering with high-frequency hearing loss and preserving low-frequency hearing, combined electric-acoustic stimulation (EAS) may significantly improve their speech perception compared with cochlear implants (CIs). In combined EAS, a hearing aid provides low-frequency information via acoustic (A) stimulation and a CI evokes high-frequency sound sensation via electrical (E) stimulation. The present work investigated the EAS advantage when only a small number (i.e., 1 or 2) of channels were provided for electrical stimulation in a CI, and the effect of carrier bandwidth on understanding Mandarin sentences in a simulation of combined EAS experiment. The A-portion was extracted via low-pass filtering processing and the E-portion was generated with a vocoder model preserving multi-channel temporal envelope waveforms, whereas a noise-vocoder and a tone-vocoder were used to simulate the effect of carrier bandwidth. The synthesized stimuli were presented to normal-hearing listeners to recognize. Experimental results showed that while low-pass filtered Mandarin speech was not very intelligible, adding one or two E channels could significantly improve the intelligibility score to above 86.0%. Under the condition with one E channel, using a large carrier bandwidth in noise-vocoder processing provided a better intelligibility performance than using a narrow carrier bandwidth in tone-vocoder processing",
    "checked": true,
    "id": "0bd7227e80a1eb8b7b8148ba44c2447b0f83f366",
    "semantic_title": "effect of carrier bandwidth on understanding mandarin sentences in simulated electric-acoustic hearing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sharma21_interspeech.html": {
    "title": "A Comparative Study of Different EMG Features for Acoustics-to-EMG Mapping",
    "volume": "main",
    "abstract": "Electromyography (EMG) signals have been extensively used to capture facial muscle movements while speaking since they are one of the most closely related bio-signals generated during speech production. In this work, we focus on speech acoustics to EMG prediction. We present a comparative study of ten different EMG signal-based features including Time Domain (TD) features existing in the literature to examine their effectiveness in speech acoustics to EMG inverse (AEI) mapping. We propose a novel feature based on the Hilbert envelope of the filtered EMG signal. The raw EMG signal is reconstructed from these features as well. For the AEI mapping, we use a bi-directional long short-term memory (BLSTM) network in a session-dependent manner. To estimate the raw EMG signal from the EMG features, we use a CNN-BLSTM model comprising of a convolution neural network (CNN) followed by BLSTM layers. AEI mapping performance using the BLSTM network reveals that the Hilbert envelope based feature is predicted from speech with the highest accuracy, among all the features. Therefore, it could be the most representative feature of the underlying muscle activation during speech production. The proposed Hilbert envelope feature, when used together with the existing TD features, improves the raw EMG signal reconstruction performance compared to using the TD features alone",
    "checked": true,
    "id": "cd51e410d8fa732adf89f03186bada63022273a4",
    "semantic_title": "a comparative study of different emg features for acoustics-to-emg mapping",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/abraham21_interspeech.html": {
    "title": "Image-Based Assessment of Jaw Parameters and Jaw Kinematics for Articulatory Simulation: Preliminary Results",
    "volume": "main",
    "abstract": "Correcting the deficits in jaw movements have often been ignored in assessment and treatment of speech disorders. A robotic simulation is being developed to facilitate Speech Language Pathologists to demonstrate the movement of jaw, tongue and teeth during production of speech sounds, as a part of a larger study. Profiling of jaw movement is an important aspect of articulatory simulation. The present study attempts to develop a simple and efficient technique for deriving the jaw parameters and using them to simulate jaw movements through inverse kinematics Three Kannada speaking male participants in the age range of 26 to 33 years were instructed to produce selected speech sounds. The image of the final position of the jaw during production of each speech sound was recorded through CT scan and video camera. Angle of ramus and angle of body of mandible were simulated through inverse kinematics using RoboAnalyzer software. The variables for inverse kinematics were derived through kinematic analysis. The Denavit-Hartenberg (D-H) parameters required for kinematic analysis were obtained from still image. Angles simulated were compared with the angles obtained from CT scan images. No significant difference was observed",
    "checked": true,
    "id": "efd6b477a05a0a2df158a67868150486351cf4ba",
    "semantic_title": "image-based assessment of jaw parameters and jaw kinematics for articulatory simulation: preliminary results",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21f_interspeech.html": {
    "title": "An Attention Self-Supervised Contrastive Learning Based Three-Stage Model for Hand Shape Feature Representation in Cued Speech",
    "volume": "main",
    "abstract": "Cued Speech (CS) is a communication system for deaf people or hearing impaired people, in which a speaker uses it to aid a lipreader in phonetic level by clarifying potentially ambiguous mouth movements with hand shape and positions. Feature extraction of multi-modal CS is a key step in CS recognition. Recent supervised deep learning based methods suffer from noisy CS data annotations especially for hand shape modality. In this work, we first propose a self-supervised contrastive learning method to learn the feature representation of image without using labels. Secondly, a small amount of manually annotated CS data are used to fine-tune the first module. Thirdly, we present a module, which combines Bi-LSTM and self-attention networks to further learn sequential features with temporal and contextual information. Besides, to enlarge the volume and the diversity of the current limited CS datasets, we build a new British English dataset containing 5 native CS speakers. Evaluation results on both French and British English datasets show that our model achieves over 90% accuracy in hand shape recognition. Significant improvements of 8.75% (for French) and 10.09% (for British English) are achieved in CS phoneme recognition correctness compared with the state-of-the-art",
    "checked": true,
    "id": "c0aa75df231ecbe77364c0eed1a9a4a97ce01626",
    "semantic_title": "an attention self-supervised contrastive learning based three-stage model for hand shape feature representation in cued speech",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dineley21_interspeech.html": {
    "title": "Remote Smartphone-Based Speech Collection: Acceptance and Barriers in Individuals with Major Depressive Disorder",
    "volume": "main",
    "abstract": "The ease of in-the-wild speech recording using smartphones has sparked considerable interest in the combined application of speech, remote measurement technology (RMT) and advanced analytics as a research and healthcare tool. For this to be realised, the acceptability of remote speech collection to the user must be established, in addition to feasibility from an analytical perspective. To understand the acceptance, facilitators, and barriers of smartphone-based speech recording, we invited 384 individuals with major depressive disorder (MDD) from the Remote Assessment of Disease and Relapse â€” Central Nervous System (RADAR-CNS) research programme in Spain and the UK to complete a survey on their experiences recording their speech. In this analysis, we demonstrate that study participants were more comfortable completing a scripted speech task than a free speech task. For both speech tasks, we found depression severity and country to be significant predictors of comfort. Not seeing smartphone notifications of the scheduled speech tasks, low mood and forgetfulness were the most commonly reported obstacles to providing speech recordings",
    "checked": true,
    "id": "2c9cec59469b8aaefa9ab7e1e4b03d4310d71447",
    "semantic_title": "remote smartphone-based speech collection: acceptance and barriers in individuals with major depressive disorder",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21_interspeech.html": {
    "title": "An Automatic, Simple Ultrasound Biofeedback Parameter for Distinguishing Accurate and Misarticulated Rhotic Syllables",
    "volume": "main",
    "abstract": "Characterizing accurate vs. misarticulated patterns of tongue movement using ultrasound can be challenging in real time because of the fast, independent movement of tongue regions. The usefulness of ultrasound for biofeedback speech therapy is limited because speakers must mentally track and compare differences between their tongue movement and available models. It is desirable to automate this interpretive task using a single parameter representing deviation from known accurate tongue movements. In this study, displacements recorded automatically by ultrasound image tracking were transformed into a single biofeedback parameter (time-dependent difference between blade and dorsum displacements). Receiver operating characteristic (ROC) curve analysis was used to evaluate this parameter as a predictor of production accuracy over a range of different vowel contexts with initial and final /r/ in American English. Areas under ROC curves were 0.8 or above, indicating that this simple parameter may provide useful real-time biofeedback on /r/ accuracy within a range of rhotic contexts",
    "checked": true,
    "id": "35e23d8a22399dfbfaeee78e92d918d89f666812",
    "semantic_title": "an automatic, simple ultrasound biofeedback parameter for distinguishing accurate and misarticulated rhotic syllables",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ribeiro21_interspeech.html": {
    "title": "Silent versus Modal Multi-Speaker Speech Recognition from Ultrasound and Video",
    "volume": "main",
    "abstract": "We investigate multi-speaker speech recognition from ultrasound images of the tongue and video images of the lips. We train our systems on imaging data from modal speech, and evaluate on matched test sets of two speaking modes: silent and modal speech. We observe that silent speech recognition from imaging data underperforms compared to modal speech recognition, likely due to a speaking-mode mismatch between training and testing. We improve silent speech recognition performance using techniques that address the domain mismatch, such as fMLLR and unsupervised model adaptation. We also analyse the properties of silent and modal speech in terms of utterance duration and the size of the articulatory space. To estimate the articulatory space, we compute the convex hull of tongue splines, extracted from ultrasound tongue images. Overall, we observe that the duration of silent speech is longer than that of modal speech, and that silent speech covers a smaller articulatory space than modal speech. Although these two properties are statistically significant across speaking modes, they do not directly correlate with word error rates from speech recognition",
    "checked": true,
    "id": "6db077cf8ad1ebd68b5cd37b6dc533c5d89c3a78",
    "semantic_title": "silent versus modal multi-speaker speech recognition from ultrasound and video",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ferreira21_interspeech.html": {
    "title": "RaSSpeR: Radar-Based Silent Speech Recognition",
    "volume": "main",
    "abstract": "Speech is our most natural and efficient way of communication and offers a strong potential to improve how we interact with machines. However, speech communication can sometimes be limited by environmental (e.g., ambient noise), contextual (e.g., need for privacy in a public place), or health conditions (e.g., laryngectomy), hindering the consideration of audible speech. In this regard, silent speech interfaces (SSI) have been proposed (e.g., considering video, electromyography), however, many technologies still face limitations regarding their everyday use, e.g., the need to place equipment in contact with the speaker (e.g., electrodes/ultrasound probe), and raise technical (e.g., lighting conditions for video) or privacy concerns. In this context, the consideration of technologies that can help tackle these issues, e.g, by being contactless and/or placed in the environment, can foster the widespread use of SSI. In this article, continuous-wave radar is explored to assess its potential for SSI. To this end, a corpus of 13 words was acquired, for 3 speakers, and different classifiers were tested on the resulting data. The best results, obtained using Bagging classifier, trained for each speaker, with 5-fold cross-validation, yielded an average accuracy of 0.826, an encouraging result that establishes promising grounds for further exploration of this technology for silent speech recognition",
    "checked": true,
    "id": "519ba8a37856e929f87d9baf817235000dcc9ed2",
    "semantic_title": "rassper: radar-based silent speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cao21_interspeech.html": {
    "title": "Investigating Speech Reconstruction for Laryngectomees for Silent Speech Interfaces",
    "volume": "main",
    "abstract": "Silent speech interfaces (SSIs) are devices that convert non-audio bio-signals to speech, which hold the potential of recovering quality speech for laryngectomees (people who have undergone laryngectomy). Although significant progress has been made, most of the recent SSI works focused on data collected from healthy speakers. SSIs for laryngectomees have rarely been investigated. In this study, we investigated the reconstruction of speech for two laryngectomees who either use tracheoesophageal puncture (TEP) or electro-larynx (EL) speech as their post-surgery communication mode. We reconstructed their speech using two SSI designs (1) real-time recognition-and-synthesis and (2) directly articulation-to-speech synthesis (ATS). The reconstructed speech samples were measured in subjective evaluation by 20 listeners in terms of naturalness and intelligibility. The results indicated that both designs increased the naturalness of alaryngeal speech. The real-time recognition-and-synthesis design obtained higher intelligibility in electrolarynx speech as well, while the ATS did not. These preliminary results suggest the real-time recognition-and-synthesis design may have a better potential for clinical applications (for laryngectomees) than ATS",
    "checked": true,
    "id": "62825b598df8e6a6cfbace2d89e086c7d1534e78",
    "semantic_title": "investigating speech reconstruction for laryngectomees for silent speech interfaces",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schroter21_interspeech.html": {
    "title": "LACOPE: Latency-Constrained Pitch Estimation for Speech Enhancement",
    "volume": "main",
    "abstract": "Fundamental frequency (f ) estimation, also known as pitch tracking, has been a long-standing research topic in the speech and signal processing community. Many pitch estimation algorithms, however, fail in noisy conditions or introduce large delays due to their frame size or Viterbi decoding In this study, we propose a deep learning-based pitch estimation algorithm, LACOPE, which was trained in a joint pitch estimation and speech enhancement framework. In contrast to previous work, this algorithm allows for a configurable latency down to an algorithmic delay of 0. This is achieved by exploiting the smoothness properties of the pitch trajectory. That is, a recurrent neural network compensates delay introduced by the feature computation by predicting the pitch for a desired point, allowing a trade-off between pitch accuracy and latency We integrate the pitch estimation in a speech enhancement framework for hearing aids. For this application, we allow a delay on the analysis side of approx. 5ms. The pitch estimate is then used for constructing a comb filter in frequency domain as post-processing step to remove intra-harmonic noise Our pitch estimation performance is on par with SOTA algorithms like PYIN or CREPE for spoken speech in all noise conditions while introducing minimal latency",
    "checked": true,
    "id": "c72d19ac039f20dd7ce57922af4ab1d080c5b695",
    "semantic_title": "lacope: latency-constrained pitch estimation for speech enhancement",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fontaine21_interspeech.html": {
    "title": "Alpha-Stable Autoregressive Fast Multichannel Nonnegative Matrix Factorization for Joint Speech Enhancement and Dereverberation",
    "volume": "main",
    "abstract": "This paper proposes Î±-stable autoregressive fast multichannel nonnegative matrix factorization (Î±-AR-FastMNMF), a robust joint blind speech enhancement and dereverberation method for improved automatic speech recognition in a realistic adverse environment. The state-of-the-art versatile blind source separation method called FastMNMF that assumes the short-time Fourier transform (STFT) coefficients of a direct sound to follow a circular complex Gaussian distribution with jointly-diagonalizable full-rank spatial covariance matrices was extended to AR-FastMNMF with an autoregressive reverberation model. Instead of the light-tailed Gaussian distribution, we use the heavy-tailed Î±-stable distribution, which also has the reproductive property useful for the additive source modeling, to better deal with the large dynamic range of the direct sound. The experimental results demonstrate that the proposed Î±-AR-FastMNMF works well as a front-end of an automatic speech recognition system. It outperforms Î±-AR-ILRMA, which is a special case of Î±-AR-FastMNMF, and their Gaussian counterparts, i.e., AR-FastMNMF and AR-ILRMA, in terms of the speech signal quality metrics and word error rate",
    "checked": true,
    "id": "44ad09ff5a972f6e556c36d54b124805f337badd",
    "semantic_title": "alpha-stable autoregressive fast multichannel nonnegative matrix factorization for joint speech enhancement and dereverberation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21e_interspeech.html": {
    "title": "Microphone Array Generalization for Multichannel Narrowband Deep Speech Enhancement",
    "volume": "main",
    "abstract": "This paper addresses the problem of microphone array generalization for deep-learning-based end-to-end multichannel speech enhancement. We aim to train a unique deep neural network (DNN) potentially performing well on unseen microphone arrays. The microphone array geometry shapes the network's parameters when training on a fixed microphone array, and thus restricts the generalization of the trained network to another microphone array. To resolve this problem, a single network is trained using data recorded by various microphone arrays of different geometries. We design three variants of our recently proposed narrowband network to cope with the agnostic number of microphones. Overall, the goal is to make the network learn the universal information for speech enhancement that is available for any array geometry, rather than learn the one-array-dedicated characteristics. The experiments on both simulated and real room impulse responses (RIR) demonstrate the excellent across-array generalization capability of the proposed networks, in the sense that their performance measures are very close to, or even exceed the network trained with test arrays. Moreover, they notably outperform various beamforming methods and other advanced deep-learning-based methods",
    "checked": true,
    "id": "99da88791b9ee4ec1855d6017e27d9d19e63b731",
    "semantic_title": "microphone array generalization for multichannel narrowband deep speech enhancement",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2021/song21_interspeech.html": {
    "title": "Multiple Sound Source Localization Based on Interchannel Phase Differences in All Frequencies with Spectral Masks",
    "volume": "main",
    "abstract": "One of the most widely used cues for sound source localization is the interchannel phase differences (IPDs) in the frequency domain. However, the spatial aliasing makes the utilization of the IPDs in the high frequencies difficult, especially when the distance between the microphones is high. Recently, the phase replication method which considers the direction-of-arrival (DoA) candidates corresponding to all the possible unwrapped phase differences in all frequency bins was proposed. However, high frequency bins with possible spatial aliasing contribute more when constructing initial DoA histograms compared with low frequency bins, which may not be desirable for source localization. In this paper, we propose to utilize the IPDs in all the frequency bins with equal weights regardless of maximum number of phase wrapping in that frequency for dual microphone sound source localization. We applied spectral masks based on local signal-to-noise ratios and coherences between microphone signals to exclude time-frequency bins without directional audio signal from the DoA histogram construction. Experimental results show that the proposed method results in more distinct peaks in the DoA histogram and outperforms the conventional method in various noisy and reverberant environments",
    "checked": true,
    "id": "5de77bee7b188d1090495c1d3ee4c9df5321764c",
    "semantic_title": "multiple sound source localization based on interchannel phase differences in all frequencies with spectral masks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zarazaga21_interspeech.html": {
    "title": "Cancellation of Local Competing Speaker with Near-Field Localization for Distributed ad-hoc Sensor Network",
    "volume": "main",
    "abstract": "In scenarios such as remote work, open offices and call centers, multiple people may simultaneously have independent spoken interactions with their devices in the same room. The speech of competing speakers will however be picked up by all microphones, both reducing the quality of audio and exposing speakers to breaches in privacy. We propose a cooperative cross-talk cancellation solution breaking the single active speaker assumption employed by most telecommunication systems. The proposed method applies source separation on the microphone signals of independent devices, to extract the dominant speaker in each device. It is realized using a localization estimator based on a deep neural network, followed by a time-frequency mask to separate the target speech from the interfering one at each time-frequency unit referring to its orientation. By experimental evaluation, we confirm that the proposed method effectively reduces crosstalk and exceeds the baseline expectation maximization method by 10 dB in terms of interference rejection. This performance makes the proposed method a viable solution for cross-talk cancellation in near-field conditions, thus protecting the privacy of external speakers in the same acoustic space",
    "checked": true,
    "id": "9a06d45c23c7501f3e95c34c3e2365706e9c7d09",
    "semantic_title": "cancellation of local competing speaker with near-field localization for distributed ad-hoc sensor network",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21f_interspeech.html": {
    "title": "A Deep Learning Method to Multi-Channel Active Noise Control",
    "volume": "main",
    "abstract": "This paper addresses multi-channel active noise control (MCANC) on the basis of deep ANC, which performs active noise control by employing deep learning to encode the optimal control parameters corresponding to different noises and environments. The proposed method trains a convolutional recurrent network (CRN) to estimate the real and imaginary spectrograms of all the canceling signals simultaneously from the reference signals so that the corresponding anti-noises cancel or attenuate the primary noises in an MCANC system. We evaluate the proposed method under multiple MCANC setups and investigate the impact of the number of canceling loudspeakers and error microphones on the overall performance. Experimental results show that deep ANC is effective for MCANC in various scenarios. Moreover, the proposed method is robust against untrained noises and works well in the presence of loudspeaker nonlinearity",
    "checked": true,
    "id": "4f849a816bbf37faa265b2e79ce5f5a7a9039ffa",
    "semantic_title": "a deep learning method to multi-channel active noise control",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/graetzer21_interspeech.html": {
    "title": "Clarity-2021 Challenges: Machine Learning Challenges for Advancing Hearing Aid Processing",
    "volume": "main",
    "abstract": "In recent years, rapid advances in speech technology have been made possible by machine learning challenges such as CHiME, REVERB, Blizzard, and Hurricane. In the Clarity project, the machine learning approach is applied to the problem of hearing aid processing of speech-in-noise, where current technology in enhancing the speech signal for the hearing aid wearer is often ineffective. The scenario is a (simulated) cuboid-shaped living room in which there is a single listener, a single target speaker and a single interferer, which is either a competing talker or domestic noise. All sources are static, the target is always within Â±30Â° azimuth of the listener and at the same elevation, and the interferer is an omnidirectional point source at the same elevation. The target speech comes from an open source 40-speaker British English speech database collected for this purpose. This paper provides a baseline description of the round one Clarity challenges for both enhancement (CEC1) and prediction (CPC1). To the authors' knowledge, these are the first machine learning challenges to consider the problem of hearing aid speech signal processing",
    "checked": true,
    "id": "dcd58c352561634afc3573a4ddcb05e993b736fd",
    "semantic_title": "clarity-2021 challenges: machine learning challenges for advancing hearing aid processing",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tu21b_interspeech.html": {
    "title": "Optimising Hearing Aid Fittings for Speech in Noise with a Differentiable Hearing Loss Model",
    "volume": "main",
    "abstract": "Current hearing aids normally provide amplification based on a general prescriptive fitting, and the benefits provided by the hearing aids vary among different listening environments despite the inclusion of noise suppression feature. Motivated by this fact, this paper proposes a data-driven machine learning technique to develop hearing aid fittings that are customised to speech in different noisy environments. A differentiable hearing loss model is proposed and used to optimise fittings with back-propagation. The customisation is reflected on the data of speech in different noise with also the consideration of noise suppression. The objective evaluation shows the advantages of optimised custom fittings over general prescriptive fittings",
    "checked": true,
    "id": "24fdd8f9fcd5c050b9c4620c5212519538691ad2",
    "semantic_title": "optimising hearing aid fittings for speech in noise with a differentiable hearing loss model",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sivasankaran21_interspeech.html": {
    "title": "Explaining Deep Learning Models for Speech Enhancement",
    "volume": "main",
    "abstract": "We consider the problem of explaining the robustness of neural networks used to compute time-frequency masks for speech enhancement to mismatched noise conditions. We employ the Deep SHapley Additive exPlanations (DeepSHAP) feature attribution method to quantify the contribution of every time-frequency bin in the input noisy speech signal to every time-frequency bin in the output time-frequency mask. We define an objective metric â€” referred to as the speech relevance score â€” that summarizes the obtained SHAP values and show that it correlates with the enhancement performance, as measured by the word error rate on the CHiME-4 real evaluation dataset. We use the speech relevance score to explain the generalization ability of three speech enhancement models trained using synthetically generated speech-shaped noise, noise from a professional sound effects library, or real CHiME-4 noise. To the best of our knowledge, this is the first study on neural network explainability in the context of speech enhancement",
    "checked": true,
    "id": "f5c070b5ef48030379a4abcfa0de6f369db5effd",
    "semantic_title": "explaining deep learning models for speech enhancement",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21_interspeech.html": {
    "title": "Minimum-Norm Differential Beamforming for Linear Array with Directional Microphones",
    "volume": "main",
    "abstract": "Among different differential beamforming approaches, the minimum-norm one has received much attention as it maximizes the white noise gain(WNG). WNG measures the robustness of beamformer. But in practice, the conventional minimum-norm differential beamforming with omnidirectional elements still suffers in low white-noise-gain at the low frequencies. The major contributions of this paper are as follows: First, we extend the existing work by presenting a new solution with the use of the directional microphone elements, and show clearly the connection between the conventional beamforming and the proposed beamforming. Second, through the derivation as well as simulations, we show the proposed solution brings noticeable improvement in WNG at the low frequencies when the null positions of the directional elements coincide with the null-constraints of minimum norm solution",
    "checked": true,
    "id": "80eafc4a3d2d83fad181c5be66634dcee466cb32",
    "semantic_title": "minimum-norm differential beamforming for linear array with directional microphones",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cao21b_interspeech.html": {
    "title": "Improving Streaming Transformer Based ASR Under a Framework of Self-Supervised Learning",
    "volume": "main",
    "abstract": "Recently self-supervised learning has emerged as an effective approach to improve the performance of automatic speech recognition (ASR). Under such a framework, the neural network is usually pre-trained with massive unlabeled data and then fine-tuned with limited labeled data. However, the non-streaming architecture like bidirectional transformer is usually adopted by the neural network to achieve competitive results, which cannot be used in streaming scenarios. In this paper, we mainly focus on improving the performance of streaming transformer under the self-supervised learning framework. Specifically, we propose a novel two-stage training method during fine-tuning, which combines knowledge distilling and self-training. The proposed training method achieves 16.3% relative word error rate (WER) reduction on Librispeech noisy test set. Finally, by only using the 100h clean subset of Librispeech as the labeled data and the rest (860h) as the unlabeled data, our streaming transformer based model obtains competitive WERs 3.5/8.7 on Librispeech clean/noisy test sets",
    "checked": true,
    "id": "aa98a02b83ad8920a9909fae187ce0a96532a95a",
    "semantic_title": "improving streaming transformer based asr under a framework of self-supervised learning",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sadhu21_interspeech.html": {
    "title": "wav2vec-C: A Self-Supervised Model for Speech Representation Learning",
    "volume": "main",
    "abstract": "wav2vec-C introduces a novel representation learning technique combining elements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized representations from partially masked speech encoding using a contrastive loss in a way similar to wav2vec 2.0. However, the quantization process is regularized by an additional consistency network that learns to reconstruct the input features to the wav2vec 2.0 network from the quantized representations in a way similar to a VQ-VAE model. The proposed self-supervised model is trained on 10k hours of unlabeled data and subsequently used as the speech encoder in a RNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one of the very few studies of self-supervised learning on speech tasks with a large volume of real far-field labeled data. The wav2vec-C encoded representations achieve, on average, twice the error reduction over baseline and a higher codebook utilization in comparison to wav2vec 2.0",
    "checked": true,
    "id": "253c6c9e6b976d0b89052a21249ff23146a81a4b",
    "semantic_title": "wav2vec-c: a self-supervised model for speech representation learning",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wallington21_interspeech.html": {
    "title": "On the Learning Dynamics of Semi-Supervised Training for ASR",
    "volume": "main",
    "abstract": "The use of semi-supervised training (SST) has become an increasingly popular way of increasing the performance of ASR acoustic models without the need for further transcribed speech data. However, the performance of the technique can be very sensitive to the quality of the initial ASR system. This paper undertakes a comprehensive study of the improvements gained with respect to variation in the initial systems, the quantity of untranscribed data used, and the learning schedules. We postulate that the reason SST can be effective even when the initial model is poor is because it enables utterance-level information to be propagated to the frame level, and hence hypothesise that the quality of the language model plays a much larger role than the quality of the acoustic model. In experiments on Tagalog data from the IARPA MATERIAL programme, we find that indeed this is the case, and show that with an appropriately chosen recipe it is possible to achieve over 50% relative WER reductions from SST, even when the WER of the initial system is more than 80%",
    "checked": true,
    "id": "00b4536b83475504ca8e57e290235c2cc4d3fc90",
    "semantic_title": "on the learning dynamics of semi-supervised training for asr",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hsu21_interspeech.html": {
    "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
    "volume": "main",
    "abstract": "Self-supervised learning of speech representations has been a very active research area but most work is focused on a single domain such as read audio books for which there exist large quantities of labeled and unlabeled data. In this paper, we explore more general setups where the domain of the unlabeled data for pre-training data differs from the domain of the labeled data for fine-tuning, which in turn may differ from the test data domain. Our experiments show that using target domain data during pre-training leads to large performance improvements across a variety of setups. With no access to in-domain labeled data, pre-training on unlabeled in-domain data closes 66â€“73% of the performance gap between the ideal setting of in-domain labeled data and a competitive supervised out-of-domain model. This has obvious practical implications since it is much easier to obtain unlabeled target domain data than labeled data. Moreover, we find that pre-training on multiple domains improves generalization performance on domains not seen during training. We will release pre-trained models",
    "checked": true,
    "id": "5f769c5df8de29d0a2cd9c020f78047013a87b34",
    "semantic_title": "robust wav2vec 2.0: analyzing domain shift in self-supervised pre-training",
    "citation_count": 136
  },
  "https://www.isca-speech.org/archive/interspeech_2021/higuchi21_interspeech.html": {
    "title": "Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition",
    "volume": "main",
    "abstract": "Pseudo-labeling (PL) has been shown to be effective in semi-supervised automatic speech recognition (ASR), where a base model is self-trained with pseudo-labels generated from unlabeled data. While PL can be further improved by iteratively updating pseudo-labels as the model evolves, most of the previous approaches involve inefficient retraining of the model or intricate control of the label update. We present (MPL), a simple yet effective strategy for semi-supervised ASR. MPL consists of a pair of and models that interact and learn from each other, inspired by the mean teacher method. The online model is trained to predict pseudo-labels generated on the fly by the offline model. The offline model maintains a momentum-based moving average of the online model. MPL is performed in a single training process and the interaction between the two models effectively helps them reinforce each other to improve the ASR performance. We apply MPL to an end-to-end ASR model based on the connectionist temporal classification. The experimental results demonstrate that MPL effectively improves over the base model and is scalable to different semi-supervised scenarios with varying amounts of data or domain mismatch",
    "checked": true,
    "id": "60013b38457658c186197d3320209b8abd46531f",
    "semantic_title": "momentum pseudo-labeling for semi-supervised speech recognition",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2021/misra21_interspeech.html": {
    "title": "A Comparison of Supervised and Unsupervised Pre-Training of End-to-End Models",
    "volume": "main",
    "abstract": "In the absence of large-scale in-domain supervised training data, ASR models can achieve reasonable performance through pre-training on additional data that is unlabeled, mismatched or both. Given such data constraints, we compare pre-training end-to-end models on matched but unlabeled data (unsupervised) and on labeled but mismatched data (supervised), where the labeled data is mismatched in either domain or language. Across encoder architectures, pre-training methods and languages, our experiments indicate that both types of pre-training improve performance, with relative WER reductions of 15â€“30% in the domain mismatch case and up to 15% in the language mismatch condition. We further find that the advantage from unsupervised pre-training is most prominent when there is no matched and labeled fine-tuning data, provided that a sufficient amount of mismatched data is still available for supervised fine-tuning",
    "checked": true,
    "id": "9b8c447d4fcea02ca3372e3cdaa0285c6b7f0cdb",
    "semantic_title": "a comparison of supervised and unsupervised pre-training of end-to-end models",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21c_interspeech.html": {
    "title": "Semi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation",
    "volume": "main",
    "abstract": "Semi and self-supervised training techniques have the potential to improve performance of speech recognition systems without additional transcribed speech data. In this work, we demonstrate the efficacy of two approaches to semi-supervision for automated speech recognition. The two approaches leverage vast amounts of available unspoken text and untranscribed audio. First, we present to improve data augmentation on unspoken text. Next, we propose the algorithm with to learn from untranscribed speech. The algorithm is built on top of our online implementation of Noisy Student Training. We demonstrate the compatibility of these techniques yielding an overall relative reduction of word error rate of up to 14.4% on the voice search tasks on 4 Indic languages",
    "checked": true,
    "id": "a9f2e5b7933c46409456fd14b457eb3d603ffdc1",
    "semantic_title": "semi-supervision in asr: sequential mixmatch and factorized tts-based augmentation",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2021/likhomanenko21b_interspeech.html": {
    "title": "slimIPL: Language-Model-Free Iterative Pseudo-Labeling",
    "volume": "main",
    "abstract": "Recent results in end-to-end automatic speech recognition have demonstrated the efficacy of pseudo-labeling for semi-supervised models trained both with Connectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq) losses. Iterative Pseudo-Labeling (IPL), which continuously trains a single model using pseudo-labels iteratively re-generated as the model learns, has been shown to further improve performance in ASR. We improve upon the IPL algorithm: as the model learns, we propose to iteratively re-generate transcriptions with hard labels (the most probable tokens), that is, a language model. We call this approach Language-Model-Free IPL (slimIPL) and give a resultant training setup for low-resource settings with CTC-based models. slimIPL features a dynamic cache for pseudo-labels which reduces sensitivity to changes in relabeling hyperparameters and results in improved training stability. slimIPL is also highly-efficient and requires 3.5â€“4Ã— fewer computational resources to converge than other state-of-the-art semi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL is competitive with self-supervised approaches, and is state-of-the-art with 100 hours of labeled audio without the use of a language model both at test time and during pseudo-label generation",
    "checked": true,
    "id": "7f0c7c324675179f0e32c160d99c7066c7ab30ae",
    "semantic_title": "slimipl: language-model-free iterative pseudo-labeling",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yue21_interspeech.html": {
    "title": "Phonetically Motivated Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Self-supervised representation learning has seen remarkable success in encoding high-level semantic information from unlabelled speech data. The studies have been focused on exploring new pretext tasks to improve the learned speech representation and various masking schemes with reference to speech frames. We consider effective latent speech representation should be phonetically informed. In this work, we propose a novel phonetically motivated masking scheme. Specifically, we select the masked speech frames according to the phonetic segmentation in an utterance. The phonetically motivated self-supervised representation learns the speech representation that benefits downstream speech processing tasks. We evaluate the proposed learning algorithm on phoneme classification, speech recognition, and speaker recognition, and show that it consistently outperforms competitive baselines",
    "checked": true,
    "id": "076a3c66db625224f30f8cdf27fde0ea4116cc49",
    "semantic_title": "phonetically motivated self-supervised speech representation learning",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21_interspeech.html": {
    "title": "Improving RNN-T for Domain Scaling Using Semi-Supervised Training with Neural TTS",
    "volume": "main",
    "abstract": "Recurrent neural network transducer (RNN-T) has shown to be comparable with conventional hybrid model for speech recognition. However, there is still a challenge in out-of-domain scenarios with context or words different from training data. In this paper, we explore the semi-supervised training which optimizes RNN-T jointly with neural text-to-speech (TTS) to better generalize to new domains using domain-specific text data. We apply the method to two tasks: one with out-of-domain context and the other with significant out-of-vocabulary (OOV) words. The results show that the proposed method significantly improves the recognition accuracy in both tasks, resulting in 61.4% and 53.8% relative word error rate (WER) reductions respectively, from a well-trained RNN-T with 65 thousand hours of training data. We do further study on the semi-supervised training methodology: 1) which modules of RNN-T model to be updated; 2) the impact of using different neural TTS models; 3) the performance of using text with different relevancy to target domain. Finally, we compare several RNN-T customization methods, and conclude that semi-supervised training with neural TTS is comparable and complementary with Internal Language Model Estimation (ILME) or biasing",
    "checked": true,
    "id": "9e8b8d25c94aa67025a441d8fb838749d4c79dba",
    "semantic_title": "improving rnn-t for domain scaling using semi-supervised training with neural tts",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seyfarth21_interspeech.html": {
    "title": "Speaker-Conversation Factorial Designs for Diarization Error Analysis",
    "volume": "main",
    "abstract": "Speaker diarization accuracy can be affected by both acoustics and conversation characteristics. Determining the cause of diarization errors is difficult because speaker voice acoustics and conversation structure co-vary, and the interactions between acoustics, conversational structure, and diarization accuracy are complex. This paper proposes a methodology that can distinguish independent marginal effects of acoustic and conversation characteristics on diarization accuracy by remixing conversations in a factorial design. As an illustration, this approach is used to investigate gender-related and language-related accuracy differences with three diarization systems: a baseline system using subsegment x-vector clustering, a variant of it with shorter subsegments, and a third system based on a Bayesian hidden Markov model. Our analysis shows large accuracy disparities for the baseline system primarily due to conversational structure, which are partially mitigated in the other two systems. The illustration thus demonstrates how the methodology can be used to identify and guide diarization model improvements",
    "checked": true,
    "id": "19e485b3a2e95dd7445ec8daf316ddf00e5873a8",
    "semantic_title": "speaker-conversation factorial designs for diarization error analysis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mcgowan21_interspeech.html": {
    "title": "SmallER: Scaling Neural Entity Resolution for Edge Devices",
    "volume": "main",
    "abstract": "In this paper we introduce SmallER, a scalable neural entity resolution system capable of running directly on edge devices. SmallER addresses constraints imposed by the on-device setting such as bounded memory consumption for both model and catalog storage, limited compute resources, and related latency challenges introduced by those restrictions. Our model includes distinct modules to learn syntactic and semantic information and is trained to handle multiple domains within one compact architecture. We use compressed tries to reduce the space required to store catalogs and a novel implementation of spatial partitioning trees to strike a balance between reducing runtime latency and preserving recall relative to full catalog search. Our final model consumes only 3MB of memory at inference time with classification accuracy surpassing that of previously established, domain-specific baseline models on live customer utterances. For the largest catalogs we consider (300 or more entries), our proxy metric for runtime latency is reduced by more than 90%",
    "checked": true,
    "id": "03305747804b812345adce3ce2b6725d752013c7",
    "semantic_title": "smaller: scaling neural entity resolution for edge devices",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rocholl21_interspeech.html": {
    "title": "Disfluency Detection with Unlabeled Data and Small BERT Models",
    "volume": "main",
    "abstract": "Disfluency detection models now approach high accuracy on English text. However, little exploration has been done in improving the size and inference time of the model. At the same time, Automatic Speech Recognition (ASR) models are moving from server-side inference to local, on-device inference. Supporting models in the transcription pipeline (like disfluency detection) must follow suit. In this work we concentrate on the disfluency detection task, focusing on small, fast, on-device models based on the BERT architecture. We demonstrate it is possible to train disfluency detection models as small as 1.3 MiB, while retaining high performance. We build on previous work that showed the benefit of data augmentation approaches such as self-training. Then, we evaluate the effect of domain mismatch between conversational and written text on model performance. We find that domain adaptation and data augmentation strategies have a more pronounced effect on these smaller models, as compared to conventional BERT models",
    "checked": true,
    "id": "63ca82b7e846468e1aa9eacec3b1a26b7ceb94c0",
    "semantic_title": "disfluency detection with unlabeled data and small bert models",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21d_interspeech.html": {
    "title": "Discriminative Self-Training for Punctuation Prediction",
    "volume": "main",
    "abstract": "Punctuation prediction for automatic speech recognition (ASR) output transcripts plays a crucial role for improving the readability of the ASR transcripts and for improving the performance of downstream natural language processing applications. However, achieving good performance on punctuation prediction often requires large amounts of labeled speech transcripts, which is expensive and laborious. In this paper, we propose a Discriminative Self-Training approach with weighted loss and discriminative label smoothing to exploit unlabeled speech transcripts. Experimental results on the English IWSLT2011 benchmark test set and an internal Chinese spoken language dataset demonstrate that the proposed approach achieves significant improvement on punctuation prediction accuracy over strong baselines including BERT, RoBERTa, and ELECTRA models. The proposed Discriminative Self-Training approach outperforms the vanilla self-training approach. We establish a new state-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current SOTA model by 1.3% absolute gain on F ",
    "checked": true,
    "id": "178808258cb73831f5a39781842ce01419f04883",
    "semantic_title": "discriminative self-training for punctuation prediction",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ihori21_interspeech.html": {
    "title": "Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks Using Switching Tokens",
    "volume": "main",
    "abstract": "In this paper, we propose a novel spoken-text-style conversion method that can simultaneously execute multiple style conversion modules such as punctuation restoration and disfluency deletion without preparing matched datasets. In practice, transcriptions generated by automatic speech recognition systems are not highly readable because they often include many disfluencies and do not include punctuation marks. To improve their readability, multiple spoken-text-style conversion modules that individually model a single conversion task are cascaded because matched datasets that simultaneously handle multiple conversion tasks are often unavailable. However, the cascading is unstable against the order of tasks because of the chain of conversion errors. Besides, the computation cost of the cascading must be higher than the single conversion. To execute multiple conversion tasks simultaneously without preparing matched datasets, our key idea is to distinguish individual conversion tasks using the In our proposed zero-shot joint modeling, we switch the individual tasks using multiple switching tokens, enabling us to utilize a zero-shot learning approach to executing simultaneous conversions. Our experiments on joint modeling of disfluency deletion and punctuation restoration demonstrate the effectiveness of our method",
    "checked": true,
    "id": "733a84c6fae83d584c4e338d8496bc165671d6a7",
    "semantic_title": "zero-shot joint modeling of multiple spoken-text-style conversion tasks using switching tokens",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21_interspeech.html": {
    "title": "A Noise Robust Method for Word-Level Pronunciation Assessment",
    "volume": "main",
    "abstract": "The common approach for pronunciation evaluation is based on Goodness of pronunciation (GOP). It has been found that GOP may perform worse under noise conditions. Traditional methods compensate pronunciation features to improve the performance of pronunciation assessment in noise situations. This paper proposed a noise robust model for word-level pronunciation assessment based on a domain adversarial training (DAT) method. We treat the pronunciation assessment in the clean and noise situations as the source and target domains. The network is optimized by incorporating both the pronunciation assessment and noise domain discrimination. The domain labels are generated from unsupervised methods to adapt to various noise situations. We evaluate the model performance based on English words recorded by Chinese English learners and labeled by three experts. Experimental results show on average the proposed model outperforms the baseline by 3% in Pearson correlation coefficients (PCC) and 4% in accuracy under different noise conditions",
    "checked": true,
    "id": "73be6722a76b57f2855a65cbb5f796b18619c09b",
    "semantic_title": "a noise robust method for word-level pronunciation assessment",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wintrode21_interspeech.html": {
    "title": "Targeted Keyword Filtering for Accelerated Spoken Topic Identification",
    "volume": "main",
    "abstract": "We present a novel framework for spoken topic identification that simultaneously learns both topic-specific keywords and acoustic keyword filters from only document-level topic labels. At inference time, only audio segments likely to contain topic-salient keywords are fully decoded, reducing the system's overall computation cost. We show that this filtering allows for effective topic classification while decoding only 50% of ASR output word lattices, and achieves error rates within 1.2% and precision within 2.6% of an unfiltered baseline system",
    "checked": true,
    "id": "7b04775139acf18a4ad498077dddaaf49130f8cd",
    "semantic_title": "targeted keyword filtering for accelerated spoken topic identification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2021/palaskar21_interspeech.html": {
    "title": "Multimodal Speech Summarization Through Semantic Concept Learning",
    "volume": "main",
    "abstract": "We propose a cascaded multimodal abstractive speech summarization model that generates semantic concepts as an intermediate step towards summarization. We describe a method to leverage existing multimodal dataset annotations to curate groundtruth labels for such intermediate concept modeling. In addition to cascaded training, the concept labels also provide an interpretable intermediate output level that helps improve performance on the downstream summarization task. On the open-domain How2 data, we conduct utterance-level and video-level experiments for two granularities of concepts: Specific and Abstract. We compare various multimodal fusion models for concept generation based on the respective input modalities. We observe consistent improvements in concept modeling by using multimodal adaptation models over unimodal models. Using the cascaded multimodal speech summarization model, we see a significant improvement of 7.5 METEOR points and 5.1 ROUGE-L points compared to previous methods of speech summarization. Finally, we show the benefits of scalability of the proposed approaches on 2000 h of video data",
    "checked": true,
    "id": "297b515fa8e5a129d757dd48065e51656dae55ba",
    "semantic_title": "multimodal speech summarization through semantic concept learning",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21_interspeech.html": {
    "title": "Enhancing Semantic Understanding with Self-Supervised Methods for Abstractive Dialogue Summarization",
    "volume": "main",
    "abstract": "Contextualized word embeddings can lead to state-of-the-art performances in natural language understanding. Recently, a pre-trained deep contextualized text encoder such as BERT has shown its potential in improving natural language tasks including abstractive summarization. Existing approaches in dialogue summarization focus on incorporating a large language model into summarization task trained on large-scale corpora consisting of news articles rather than dialogues of multiple speakers. In this paper, we introduce self-supervised methods to compensate shortcomings to train a dialogue summarization model. Our principle is to detect incoherent information flows using pretext dialogue text to enhance BERT's ability to contextualize the dialogue text representations. We build and fine-tune an abstractive dialogue summarization model on a shared encoder-decoder architecture using the enhanced BERT. We empirically evaluate our abstractive dialogue summarizer with the SAMSum corpus, a recently introduced dataset with abstractive dialogue summaries. All of our methods have contributed improvements to abstractive summary measured in ROUGE scores. Through an extensive ablation study, we also present a sensitivity analysis to critical model hyperparameters, probabilities of switching utterances and masking interlocutors",
    "checked": true,
    "id": "e661fabf7e3408afc8bbe47b8ea096867494afd6",
    "semantic_title": "enhancing semantic understanding with self-supervised methods for abstractive dialogue summarization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wodarczak21_interspeech.html": {
    "title": "Speaker Transition Patterns in Three-Party Conversation: Evidence from English, Estonian and Swedish",
    "volume": "main",
    "abstract": "During conversation, speakers hold and relinquish the floor, resulting in turn yield and retention. We examine these phenomena in three-party conversations in English, Swedish, and Estonian. We define within- and between-speaker transitions in terms of shorter intervals of speech, silence and overlap bounded by stretches of one-party speech longer than 1 second by the same or different speakers. This method gives us insights into how turn change and retention proceed, revealing that the majority of speaker transitions are more complex and involve more intermediate activity than a single silence or overlap. We examine the composition of within and between transitions in terms of number of speakers involved, incidence and proportion of solo speech, silence and overlap. We derive the most common within- and between-speaker transitions in the three languages, finding evidence of striking commonalities in how the floor is managed. Our findings suggest that current models of turn-taking used in dialogue technology could be extended using these results to more accurately reflect the realities of human-human dialogue",
    "checked": true,
    "id": "849e1128dff8ac41315eef1962743ffc158c5bc3",
    "semantic_title": "speaker transition patterns in three-party conversation: evidence from english, estonian and swedish",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/broughton21_interspeech.html": {
    "title": "Investigating Deep Neural Structures and their Interpretability in the Domain of Voice Conversion",
    "volume": "main",
    "abstract": "Generative Adversarial Networks (GANs) are machine learning networks based around creating synthetic data. Voice Conversion (VC) is a subset of voice translation that involves translating the paralinguistic features of a source speaker to a target speaker while preserving the linguistic information. The aim of non-parallel conditional GANs for VC is to translate an acoustic speech feature sequence from one domain to another without the use of paired data. In the study reported here, we investigated the interpretability of state-of-the-art implementations of non-parallel GANs in the domain of VC. We show that the learned representations in the repeating layers of a particular GAN architecture remain close to their original random initialised parameters, demonstrating that it is the number of repeating layers that is more responsible for the quality of the output. We also analysed the learned representations of a model trained on one particular dataset when used during transfer learning on another dataset. This also showed high levels of similarity in the repeating layers. Together, these results provide new insight into how the learned representations of deep generative networks change during learning and the importance of the number of layers, which would help build better GAN-based speech conversion models",
    "checked": true,
    "id": "971818b7c6ceea6ee06ba73b7d44ee92e1afdc88",
    "semantic_title": "investigating deep neural structures and their interpretability in the domain of voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21b_interspeech.html": {
    "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training",
    "volume": "main",
    "abstract": "Emotional voice conversion (EVC) aims to change the emotional state of an utterance while preserving the linguistic content and speaker identity. In this paper, we propose a novel 2-stage training strategy for sequence-to-sequence emotional voice conversion with a limited amount of emotional speech data. We note that the proposed EVC framework leverages text-to-speech (TTS) as they share a common goal that is to generate high-quality expressive voice. In stage 1, we perform style initialization with a multi-speaker TTS corpus, to disentangle speaking style and linguistic content. In stage 2, we perform emotion training with a limited amount of emotional speech data, to learn how to disentangle emotional style and linguistic information from the speech. The proposed framework can perform both spectrum and prosody conversion and achieves significant improvement over the state-of-the-art baselines in both objective and subjective evaluation",
    "checked": true,
    "id": "21df371e20d80039a90fddd98f06eb545cf41ceb",
    "semantic_title": "limited data emotional voice conversion leveraging text-to-speech: two-stage sequence-to-sequence training",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ding21_interspeech.html": {
    "title": "Adversarial Voice Conversion Against Neural Spoofing Detectors",
    "volume": "main",
    "abstract": "The naturalness and similarity of voice conversion have been significantly improved in recent years with the development of deep-learning-based conversion models and neural vocoders. Accordingly, the task of detecting spoofing speech also attracts research attention. In the latest ASVspoof 2019 challenge, the best spoofing detection model can distinguish most artificial utterances from natural ones. Inspired by recent progress of adversarial example generation, this paper proposes an adversarial post-processing network (APN) which generates adversarial examples against a neural-network-based spoofing detector by white-box attack. The APN model post-processes the speech waveforms generated by a baseline voice conversion system. An adversarial loss derived from the spoofing detector together with two regularization losses are applied to optimize the parameters of APN. In our experiments, using the logical access (LA) dataset of ASVspoof 2019, results show that our proposed method can improve the adversarial ability of converted speech against the spoofing detectors based on light convolution neural networks (LCNNs) effectively without degrading its subjective quality",
    "checked": true,
    "id": "df1ee465fea51bec203a36f09774d558df37c00d",
    "semantic_title": "adversarial voice conversion against neural spoofing detectors",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/he21b_interspeech.html": {
    "title": "An Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation",
    "volume": "main",
    "abstract": "Emotional Voice Conversion (EVC) aims to convert the emotional style of a source speech signal to a target style while preserving its content and speaker identity information. Previous emotional conversion studies do not disentangle emotional information from emotion-independent information that should be preserved, thus transforming it all in a monolithic manner and generating audio of low quality, with linguistic distortions. To address this distortion problem, we propose a novel StarGAN framework along with a two-stage training process that separates emotional features from those independent of emotion by using an autoencoder with two encoders as the generator of the Generative Adversarial Network (GAN). The proposed model achieves favourable results in both the objective evaluation and the subjective evaluation in terms of distortion, which reveals that the proposed model can effectively reduce distortion. Furthermore, in data augmentation experiments for end-to-end speech emotion recognition, the proposed StarGAN model achieves an increase of 2% in Micro-F1 and 5% in Macro-F1 compared to the baseline StarGAN model, which indicates that the proposed model is more valuable for data augmentation",
    "checked": true,
    "id": "8afc1ea6ee94a212f5a8439a57dc6adf733f32da",
    "semantic_title": "an improved stargan for emotional voice conversion: enhancing voice quality and data augmentation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21e_interspeech.html": {
    "title": "TVQVC: Transformer Based Vector Quantized Variational Autoencoder with CTC Loss for Voice Conversion",
    "volume": "main",
    "abstract": "Techniques of voice conversion (VC) aim to modify the speaker identity and style of an utterance while preserving the linguistic content. Although there are lots of VC methods, the state of the art of VC is still cascading automatic speech recognition (ASR) and text-to-speech (TTS). This paper presents a new structure of vector-quantized autoencoder based on transformer with CTC loss for non-parallel VC, which inspired by cascading ASR and TTS VC method. Our proposed method combines CTC loss and vector quantization to get high-level linguistic information without speaker information. Objective and subjective evaluations on the mandarin datasets show that the converted speech of our proposed model is better than baselines on naturalness, rhythm and speaker similarity",
    "checked": true,
    "id": "32e4f594da0eb1d1ca72f01a52ab63dff7fd5189",
    "semantic_title": "tvqvc: transformer based vector quantized variational autoencoder with ctc loss for voice conversion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21g_interspeech.html": {
    "title": "Enriching Source Style Transfer in Recognition-Synthesis Based Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "Current voice conversion (VC) methods can successfully convert timbre of the audio. As modeling source audio's prosody effectively is a challenging task, there are still limitations of transferring source style to the converted speech. This study proposes a source style transfer method based on recognition-synthesis framework. Previously in speech generation task, prosody can be modeled explicitly with prosodic features or implicitly with a latent prosody extractor. In this paper, taking advantages of both, we model the prosody in a hybrid manner, which effectively combines explicit and implicit methods in a proposed prosody module. Specifically, prosodic features are used to explicit model prosody, while VAE and reference encoder are used to implicitly model prosody, which take Mel spectrum and bottleneck feature as input respectively. Furthermore, adversarial training is introduced to remove speaker-related information from the VAE outputs, avoiding leaking source speaker information while transferring style. Finally, we use a modified self-attention based encoder to extract sentential context from bottleneck features, which also implicitly aggregates the prosodic aspects of source speech from the layered representations. Experiments show that our approach is superior to the baseline and a competitive system in terms of style transfer; meanwhile, the speech quality and speaker similarity are well maintained",
    "checked": true,
    "id": "682482dc466a96637771f8d37da50ed3c82e487f",
    "semantic_title": "enriching source style transfer in recognition-synthesis based non-parallel voice conversion",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21b_interspeech.html": {
    "title": "S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations",
    "volume": "main",
    "abstract": "Any-to-any voice conversion (VC) aims to convert the timbre of utterances from and to any speakers seen or unseen during training. Various any-to-any VC approaches have been proposed like AutoVC, AdaINVC, and FragmentVC. AutoVC, and AdaINVC utilize source and target encoders to disentangle the content and speaker information of the features. FragmentVC utilizes two encoders to encode source and target information and adopts cross attention to align the source and target features with similar phonetic content. Moreover, pretrained features are adopted. AutoVC used d-vector to extract speaker information, and self-supervised learning (SSL) features like wav2vec 2.0 is used in FragmentVC to extract the phonetic content information. Different from previous works, we proposed S2VC that utilizes Self-Supervised features as both source and target features for the VC model. Supervised phoneme posteriorgram (PPG), which is believed to be speaker-independent and widely used in VC to extract content information, is chosen as a strong baseline for SSL features. The objective evaluation and subjective evaluation both show models taking SSL feature CPC as both source and target features outperforms that taking PPG as source feature, suggesting that SSL features have great potential in improving VC",
    "checked": true,
    "id": "6fcaafc1f8ae649899e978a7a9baf682755dc7de",
    "semantic_title": "s2vc: a framework for any-to-any voice conversion with self-supervised pretrained representations",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liberatore21_interspeech.html": {
    "title": "An Exemplar Selection Algorithm for Native-Nonnative Voice Conversion",
    "volume": "main",
    "abstract": "We present an algorithm for selecting exemplars for native-to-nonnative voice conversion (VC) using a Sparse, Anchor-Based Representation of speech (SABR). The algorithm uses phoneme labels and clustering to learn optimal exemplars when source and target speakers are affected by poor time alignment, as is common in in native-to-nonnative voice conversion. We evaluate the method on speech from the ARCTIC and L2-ARCTIC corpora and compare it to a baseline exemplar-based VC algorithm. The proposed algorithm significantly improves synthesis quality and more than doubles that of a baseline exemplar-based VC system while using two orders of magnitude fewer atoms. Additionally, the proposed algorithm significantly reduces the VC error and improves the synthesis quality as compared to unoptimized SABR models. We discuss the implications of both optimization algorithms for SABR and broader exemplar-based VC systems.Index terms should be included as shown below",
    "checked": true,
    "id": "99e695a12b396db6b75536d26304db2de52b40b4",
    "semantic_title": "an exemplar selection algorithm for native-nonnative voice conversion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21h_interspeech.html": {
    "title": "Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion",
    "volume": "main",
    "abstract": "Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC). Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors. State-of-the-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement. To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning. Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT. The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others. Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58",
    "checked": true,
    "id": "607c18c160aa66c13314b9da1e89639b79f67bca",
    "semantic_title": "adversarially learning disentangled speech representations for robust multi-factor voice conversion",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luong21_interspeech.html": {
    "title": "Many-to-Many Voice Conversion Based Feature Disentanglement Using Variational Autoencoder",
    "volume": "main",
    "abstract": "Voice conversion is a challenging task which transforms the voice characteristics of a source speaker to a target speaker without changing linguistic content. Recently, there have been many works on many-to-many Voice Conversion (VC) based on Variational Autoencoder (VAEs) achieving good results, however, these methods lack the ability to disentangle speaker identity and linguistic content to achieve good performance on unseen speaker's scenarios. In this paper, we propose a new method based on feature disentanglement to tackle many-to-many voice conversion. The method has the capability to disentangle speaker identity and linguistic content from utterances, it can convert from many source speakers to many target speakers with a single autoencoder network. Moreover, it naturally deals with the unseen target speaker's scenarios. We perform both objective and subjective evaluations to show the competitive performance of our proposed method compared with other state-of-the-art models in terms of naturalness and target speaker similarity",
    "checked": true,
    "id": "9cf89ded1cdd420f1d3acd09a843741e1367f6aa",
    "semantic_title": "many-to-many voice conversion based feature disentanglement using variational autoencoder",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chouchane21_interspeech.html": {
    "title": "Privacy-Preserving Voice Anti-Spoofing Using Secure Multi-Party Computation",
    "volume": "main",
    "abstract": "In recent years the automatic speaker verification (ASV) community has grappled with vulnerabilities to spoofing attacks whereby fraudsters masquerade as enrolled subjects to provoke illegitimate accepts. Countermeasures have hence been developed to protect ASV systems from such attacks. Given that recordings of speech contain potentially sensitive information, any system operating upon them, including spoofing countermeasures, must have provisions for privacy preservation. While privacy enhancing technologies such as Homomorphic Encryption or Secure Multi-Party Computation (MPC) are effective in preserving privacy, these tend to impact upon computational capacity and computational precision, while no available spoofing countermeasures preserve privacy. This paper reports the first solution based upon the combination of shallow neural networks with secure MPC. Experiments performed using the ASVspoof 2019 logical access database show that the proposed solution is not only computationally efficient, but that it also improves upon the performance of the ASVspoof baseline countermeasure, all while preserving privacy",
    "checked": true,
    "id": "b419209b1b34847b42ed80b52b7ce50ba3d08713",
    "semantic_title": "privacy-preserving voice anti-spoofing using secure multi-party computation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2021/aloufi21_interspeech.html": {
    "title": "Configurable Privacy-Preserving Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Voice assistive technologies have given rise to far-reaching privacy and security concerns. In this paper we investigate whether modular automatic speech recognition (ASR) can improve privacy in voice assistive systems by combining independently trained separation, recognition, and discretization modules to design configurable privacy-preserving ASR systems. We evaluate privacy concerns and the effects of applying various state-of-the-art techniques at each stage of the system, and report results using task-specific metrics (i.e., WER, ABX, and accuracy). We show that overlapping speech inputs to ASR systems present further privacy concerns, and how these may be mitigated using speech separation and optimization techniques. Our discretization module is shown to minimize paralinguistics privacy leakage from ASR acoustic models to levels commensurate with random guessing. We show that voice privacy can be , and argue this presents new opportunities for privacy-preserving applications incorporating ASR",
    "checked": true,
    "id": "6a9cc72a0b18ea375eebdc352f7244d33a8f762f",
    "semantic_title": "configurable privacy-preserving automatic speech recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/novotney21_interspeech.html": {
    "title": "Adjunct-Emeritus Distillation for Semi-Supervised Language Model Adaptation",
    "volume": "main",
    "abstract": "To improve customer privacy, commercial speech applications are reducing human transcription of customer data. This has a negative impact on language model training due to a smaller amount of in-domain transcripts. Prior work demonstrated that training on automated transcripts alone provides modest gains due to reinforcement of recognition errors. We consider a new condition, where a model trained on historical human transcripts, but not the transcripts themselves, are available to us. To overcome temporal drift in vocabulary and topics, we propose a novel extension of knowledge distillation, where two imperfect teachers jointly train a student model. We conduct experiments on an English voice assistant domain and simulate a one year gap in human transcription. Unlike fine-tuning, our approach is architecture agnostic and achieves a 14% relative reduction in perplexity over the baseline approach of freezing model development and improves over the baseline of knowledge distillation",
    "checked": true,
    "id": "aea05c410fd750d90f43aabc1fd92c3625fc20fd",
    "semantic_title": "adjunct-emeritus distillation for semi-supervised language model adaptation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ro21_interspeech.html": {
    "title": "Communication-Efficient Agnostic Federated Averaging",
    "volume": "main",
    "abstract": "In distributed learning settings such as federated learning, the training algorithm can be potentially biased towards different clients. [1] proposed a domain-agnostic learning algorithm, where the model is optimized for any target distribution formed by a mixture of the client distributions in order to overcome this bias. They further proposed an algorithm for the cross-silo federated learning setting, where the number of clients is small. We consider this problem in the cross-device setting, where the number of clients is much larger. We propose a communication-efficient distributed algorithm called Agnostic Federated Averaging (or AgnosticFedAvg) to minimize the domain-agnostic objective proposed in [1], which is amenable to other private mechanisms such as secure aggregation. We highlight two types of naturally occurring domains in federated learning and argue that AgnosticFedAvg performs well on both. To demonstrate the practical effectiveness of AgnosticFedAvg, we report positive results for large-scale language modeling tasks in both simulation and live experiments, where the latter involves training language models for Spanish virtual keyboard for millions of user devices",
    "checked": true,
    "id": "25c6e17a64548994befaca422748b6fe7478af0a",
    "semantic_title": "communication-efficient agnostic federated averaging",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2021/koppelmann21_interspeech.html": {
    "title": "Privacy-Preserving Feature Extraction for Cloud-Based Wake Word Verification",
    "volume": "main",
    "abstract": "Wake word detection and verification systems often involve a local, on-device wake word detector and a cloud-based verification node. In such systems, the audio representation sent to the cloud-based server may exhibit sensitive information that might be intercepted by an eavesdropper. To improve privacy of cloud-based wake word verification (WWV) systems, we propose to use a privacy-preserving feature representation that minimizes the automatic speech recognition (ASR) capability of a potential attacker. The proposed approach employs an adversarial training schedule that aims to minimize an attacker's word error rate (WER) while maintaining a high WWV performance. To this end, we apply an adaptive weighting factor in the combined loss function to control the balance between minimizing the WWV loss and maximizing the ASR loss. We show that the proposed training method significantly reduces possible privacy risks while maintaining a strong WWV performance",
    "checked": true,
    "id": "6b772b81e93deed6b35fbb2cffc778fb93a6b52f",
    "semantic_title": "privacy-preserving feature extraction for cloud-based wake word verification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21_interspeech.html": {
    "title": "PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification",
    "volume": "main",
    "abstract": "We propose using an adversarial autoencoder (AAE) to replace generative adversarial network (GAN) in private aggregation of teacher ensembles (PATE), a solution for ensuring differential privacy in speech applications. The AAE architecture allows us to obtain good synthetic speech leveraging upon a discriminative training of latent vectors. Such synthetic speech is used to build a privacy-preserving classifier when non-sensitive data is not sufficiently available in the public domain. This classifier follows the PATE scheme that uses an ensemble of noisy outputs to label the synthetic samples and guarantee Îµ-differential privacy (DP) on its derived classifiers. Our proposed framework thus consists of an AAE-based generator and a PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands Dataset Version II, the proposed PATE-AAE improves the average classification accuracy by +2.11% and +6.60%, respectively, when compared with alternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while maintaining a strong level of privacy target at Îµ=0.01 with a fixed Î´=10 ",
    "checked": true,
    "id": "2bbfd3671198bc23a96cb7f992e3faca62721ee6",
    "semantic_title": "pate-aae: incorporating adversarial autoencoder into private aggregation of teacher ensembles for spoken command classification",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21b_interspeech.html": {
    "title": "Continual Learning for Fake Audio Detection",
    "volume": "main",
    "abstract": "Fake audio attack becomes a major threat to the speaker verification system. Although current detection approaches have achieved promising results on dataset-specific scenarios, they encounter difficulties on unseen spoofing data. Fine-tuning and retraining from scratch have been applied to incorporate new data. However, fine-tuning leads to performance degradation on previous data. Retraining takes a lot of time and computation resources. Besides, previous data are unavailable due to privacy in some situations. To solve the above problems, this paper proposes detecting fake without forgetting, a continual-learning-based method, to make the model learn new spoofing attacks incrementally. A knowledge distillation loss is introduced to loss function to preserve the memory of original model. Supposing the distribution of genuine voice is consistent among different scenarios, an extra embedding similarity loss is used as another constraint to further do a positive sample alignment. Experiments are conducted on the ASVspoof2019 dataset. The results show that our proposed method outperforms fine-tuning by the relative reduction of average equal error rate up to 81.62%",
    "checked": true,
    "id": "8c632eb06fe6e70f20705429f612a6aeccb0a4fd",
    "semantic_title": "continual learning for fake audio detection",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shah21_interspeech.html": {
    "title": "Evaluating the Vulnerability of End-to-End Automatic Speech Recognition Models to Membership Inference Attacks",
    "volume": "main",
    "abstract": "Recent studies have shown that it may be possible to determine if a machine learning model was trained on a given data sample, using Membership Inference Attacks (MIA). In this paper we evaluate the vulnerability of state-of-the-art speech recognition models to MIA under black-box access. Using models trained with standard methods and public datasets, we demonstrate that without any knowledge of the target model's parameters or training data a MIA can successfully infer membership with precision and recall more than 60%. Furthermore, for utterances from about 39% of the speakers the precision is more than 75%, indicating that training data membership can be inferred more precisely for some speakers than others. While strong regularization reduces the overall accuracy of MIA to almost 50%, the attacker can still infer membership for utterances from 25% of the speakers with high precision. These results indicate that (1) speaker-level MIA success should be reported, along with overall accuracy, to provide a holistic view of the model's vulnerability and (2) conventional regularization is an inadequate defense against MIA.We believe that the insights gleaned from this study can direct future work towards more effective defenses",
    "checked": true,
    "id": "c91b8cb2129654ab91f9374e10c90756f64ee880",
    "semantic_title": "evaluating the vulnerability of end-to-end automatic speech recognition models to membership inference attacks",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fazel21_interspeech.html": {
    "title": "SynthASR: Unlocking Synthetic Data for Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have recently demonstrated superior performance over the traditional hybrid ASR models. Training an E2E ASR model requires a large amount of data which is not only expensive but may also raise dependency on production data. At the same time, synthetic speech generated by the state-of-the-art text-to-speech (TTS) engines has advanced to near-human naturalness. In this work, we propose to utilize synthetic speech for ASR training (SynthASR) in applications where data is sparse or hard to get for ASR model training. In addition, we apply continual learning with a novel multi-stage training strategy to address catastrophic forgetting, achieved by a mix of weighted multi-style training, data augmentation, encoder freezing, and parameter regularization. In our experiments conducted on in-house datasets for a new application of recognizing medication names, training ASR RNN-T models with synthetic audio via the proposed multi-stage training improved the recognition performance on new application by more than 65% relative, without degradation on existing general applications. Our observations show that SynthASR holds great promise in training the state-of-the-art large-scale E2E ASR models for new applications while reducing the costs and dependency on production data",
    "checked": true,
    "id": "2b7f54a8bbda05d07871f1662d5397160bde8de7",
    "semantic_title": "synthasr: unlocking synthetic data for speech recognition",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muguli21_interspeech.html": {
    "title": "DiCOVA Challenge: Dataset, Task, and Baseline System for COVID-19 Diagnosis Using Acoustics",
    "volume": "main",
    "abstract": "The DiCOVA challenge aims at accelerating research in diagnosing COVID-19 using acoustics (DiCOVA), a topic at the intersection of speech and audio processing, respiratory health diagnosis, and machine learning. This challenge is an open call for researchers to analyze a dataset of sound recordings, collected from COVID-19 infected and non-COVID-19 individuals, for a two-class classification. These recordings were collected via crowdsourcing from multiple countries, through a website application. The challenge features two tracks, one focusing on cough sounds, and the other on using a collection of breath, sustained vowel phonation, and number counting speech recordings. In this paper, we introduce the challenge and provide a detailed description of the task, and present a baseline system for the task",
    "checked": true,
    "id": "02093d2179a4eef9fa5bf0d59498f60e1a6c78b6",
    "semantic_title": "dicova challenge: dataset, task, and baseline system for covid-19 diagnosis using acoustics",
    "citation_count": 71
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kamble21_interspeech.html": {
    "title": "PANACEA Cough Sound-Based Diagnosis of COVID-19 for the DiCOVA 2021 Challenge",
    "volume": "main",
    "abstract": "The COVID-19 pandemic has led to the saturation of public health services worldwide. In this scenario, the early diagnosis of SARS-Cov-2 infections can help to stop or slow the spread of the virus and to manage the demand upon health services. This is especially important when resources are also being stretched by heightened demand linked to other seasonal diseases, such as the flu. In this context, the organisers of the DiCOVA 2021 challenge have collected a database with the aim of diagnosing COVID-19 through the use of coughing audio samples. This work presents the details of the automatic system for COVID-19 detection from cough recordings presented by team PANACEA. This team consists of researchers from two European academic institutions and one company: EURECOM (France), University of Granada (Spain), and Biometric Vox S.L. (Spain). We developed several systems based on established signal processing and machine learning methods. Our best system employs a Teager energy operator cepstral coefficients (TECCs) based front-end and Light gradient boosting machine (LightGBM) back-end. The AUC obtained by this system on the test set is 76.31% which corresponds to a 10% improvement over the official baseline",
    "checked": true,
    "id": "d095e8160ad109057369c515d5e8ffc4c607454d",
    "semantic_title": "panacea cough sound-based diagnosis of covid-19 for the dicova 2021 challenge",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karas21_interspeech.html": {
    "title": "Recognising Covid-19 from Coughing Using Ensembles of SVMs and LSTMs with Handcrafted and Deep Audio Features",
    "volume": "main",
    "abstract": "As the Covid-19 pandemic continues, digital health solutions can provide valuable insights and assist in diagnosis and prevention. Since the disease affects the respiratory system, it is hypothesised that sound formation is changed, and thus, an infection can be automatically recognised through audio analysis. We present an ensemble learning approach used in our entry to Track 1 of the DiCOVA 2021 Challenge, which aims at binary classification of Covid-19 infection on a crowd-sourced dataset of 1 040 cough sounds. Our system is based on a combination of handcrafted features for paralinguistics with deep feature extraction from spectrograms using pre-trained CNNs. We extract features both at segment level and with a sliding window approach, and process them with SVMs and LSTMs, respectively. We then perform least-squares weighted late fusion of our classifiers. Our system surpasses the challenge baseline, with a ROC-AUC on the test set of 78.18%",
    "checked": true,
    "id": "5cf0da43f1a2e92c58185235e8f3d4e319db31c0",
    "semantic_title": "recognising covid-19 from coughing using ensembles of svms and lstms with handcrafted and deep audio features",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sodergren21_interspeech.html": {
    "title": "Detecting COVID-19 from Audio Recording of Coughs Using Random Forests and Support Vector Machines",
    "volume": "main",
    "abstract": "The detection of COVID-19 is and will remain in the foreseeable future a crucial challenge, making the development of tools for the task important. One possible approach, on the confines of speech and audio processing, is detecting potential COVID-19 cases based on cough sounds. We propose a simple, yet robust method based on the well-known ComParE 2016 feature set, and two classical machine learning models, namely Random Forests, and Support Vector Machines (SVMs). Furthermore, we combine the two methods, by calculating the weighted average of their predictions. Our results in the DiCOVA challenge show that this simple approach leads to a robust solution while producing competitive results. Based on the Area Under the Receiver Operating Characteristic Curve (AUC ROC) score, both classical machine learning methods we applied markedly outperform the baseline provided by the challenge organisers. Moreover, their combination attains an AUC ROC score of 85.21, positioning us at fourth place on the leaderboard (where the second team attained a similar, 85.43 score). Here, we would describe this system in more detail, and analyse the resulting models, drawing conclusions, and determining future work directions",
    "checked": true,
    "id": "24a23e0509771e58bfcd5cb5a6d0953a1e3da5dc",
    "semantic_title": "detecting covid-19 from audio recording of coughs using random forests and support vector machines",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/das21_interspeech.html": {
    "title": "Diagnosis of COVID-19 Using Auditory Acoustic Cues",
    "volume": "main",
    "abstract": "COVID-19 can be pre-screened based on symptoms and confirmed using other laboratory tests. The cough or speech from patients are also studied in the recent time for detection of COVID-19 as they are indicators of change in anatomy and physiology of the respiratory system. Along this direction, the diagnosis of COVID-19 using acoustics (DiCOVA) challenge aims to promote such research by releasing publicly available cough/speech corpus. We participated in the Track-1 of the challenge, which deals with COVID-19 detection using cough sounds from individuals. In this challenge, we use a few novel auditory acoustic cues based on long-term transform, equivalent rectangular bandwidth spectrum and gammatone filterbank. We evaluate these representations using logistic regression, random forest and multilayer perceptron classifiers for detection of COVID-19. On the blind test set, we obtain an area under the ROC curve (AUC) of 83.49% for the best system submitted to the challenge. It is worth noting that the submitted system ranked among the top few systems on the leaderboard and outperformed the challenge baseline by a large margin",
    "checked": true,
    "id": "8140fb53adf948abdf0c00a59220a35e9c3f31db",
    "semantic_title": "diagnosis of covid-19 using auditory acoustic cues",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/harvill21_interspeech.html": {
    "title": "Classification of COVID-19 from Cough Using Autoregressive Predictive Coding Pretraining and Spectral Data Augmentation",
    "volume": "main",
    "abstract": "Serum and saliva-based testing methods have been crucial to slowing the COVID-19 pandemic, yet have been limited by slow throughput and cost. A system able to determine COVID-19 status from cough sounds alone would provide a low cost, rapid, and remote alternative to current testing methods. We explore the applicability of recent techniques such as pre-training and spectral augmentation in improving the performance of a neural cough classification system. We use Autoregressive Predictive Coding (APC) to pre-train a unidirectional LSTM on the COUGHVID dataset. We then generate our final model by fine-tuning added BLSTM layers on the DiCOVA challenge dataset. We perform various ablation studies to see how each component impacts performance and improves generalization with a small dataset. Our final system achieves an AUC of 85.35 and places third out of 29 entries in the DiCOVA challenge",
    "checked": true,
    "id": "536615589a2196967330642c8fc8ad860b069d10",
    "semantic_title": "classification of covid-19 from cough using autoregressive predictive coding pretraining and spectral data augmentation",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deshpande21_interspeech.html": {
    "title": "The DiCOVA 2021 Challenge â€” An Encoder-Decoder Approach for COVID-19 Recognition from Coughing Audio",
    "volume": "main",
    "abstract": "This paper presents the automatic recognition of COVID-19 from coughing. In particular, it describes our contribution to the DiCOVA challenge â€” Track 1, which addresses such cough sound analysis for COVID-19 detection. Pathologically, the effects of a COVID-19 infection on the respiratory system and on breathing patterns are known. We demonstrate the use of breathing patterns of the cough audio signal in identifying the COVID-19 status. Breathing patterns of the cough audio signal are derived using a model trained with the subset of the UCL Speech Breath Monitoring (UCL-SBM) database. This database provides speech recordings of the participants while their breathing values are captured by a respiratory belt. We use an encoder-decoder architecture. The encoder encodes the audio signal into breathing patterns and the decoder decodes the COVID-19 status for the corresponding breathing patterns using an attention mechanism. The encoder uses a pre-trained model which predicts breathing patterns from the speech signal, and transfers the learned patterns to cough audio signals With this architecture, we achieve an AUC of 64.42% on the evaluation set of Track 1",
    "checked": false,
    "id": "dd7de0d1fd182ec5f3bdee0705a116816f07bf9d",
    "semantic_title": "the dicova 2021 challenge - an encoder-decoder approach for covid-19 recognition from coughing audio",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ritwik21_interspeech.html": {
    "title": "COVID-19 Detection from Spectral Features on the DiCOVA Dataset",
    "volume": "main",
    "abstract": "In this paper we investigate the cues of COVID-19 on sustained phonation of Vowel-/i/, deep breathing and number counting data of the DiCOVA dataset. We use an ensemble of classifiers trained on different features, namely, super-vectors, formants, harmonics and MFCC features. We fit a two-class Weighted SVM classifier to separate the COVID-19 audio from Non-COVID-19 audio. Weighted penalties help mitigate the challenge of class imbalance in the dataset. The results are reported on the stationary (breathing, Vowel-/i/) and non-stationary (counting data) data using individual and combination of features on each type of utterance. We find that the Formant information plays a crucial role in classification. The proposed system resulted in an AUC score of 0.734 for cross validation, and 0.717 for evaluation dataset",
    "checked": true,
    "id": "021b2b919eec8add2e2a9c7a168177cc88e6ea32",
    "semantic_title": "covid-19 detection from spectral features on the dicova dataset",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mallolragolta21_interspeech.html": {
    "title": "Cough-Based COVID-19 Detection with Contextual Attention Convolutional Neural Networks and Gender Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bhosale21_interspeech.html": {
    "title": "Contrastive Learning of Cough Descriptors for Automatic COVID-19 Preliminary Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/avila21_interspeech.html": {
    "title": "Investigating Feature Selection and Explainability for COVID-19 Diagnostics from Cough Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kiss21_interspeech.html": {
    "title": "Application for Detecting Depression, Parkinson's Disease and Dysphonic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/weingartova21_interspeech.html": {
    "title": "Beey: More Than a Speech-to-Text Editor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/arai21_interspeech.html": {
    "title": "Downsizing of Vocal-Tract Models to Line up Variations and Reduce Manufacturing Costs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fabien21_interspeech.html": {
    "title": "ROXANNE Research Platform: Automate Criminal Investigations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/flucha21_interspeech.html": {
    "title": "The LIUM Human Active Correction Platform for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oh21_interspeech.html": {
    "title": "On-Device Streaming Transformer-Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cmejla21_interspeech.html": {
    "title": "Advanced Semi-Blind Speaker Extraction and Tracking Implemented in Experimental Device with Revolving Dense Microphone Array",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ney21_interspeech.html": {
    "title": "Forty Years of Speech and Language Processing: From Bayes Decision Rule to Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chorowski21_interspeech.html": {
    "title": "Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chorowski21b_interspeech.html": {
    "title": "Aligned Contrastive Predictive Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/suter21_interspeech.html": {
    "title": "Neural Text Denormalization for Speech Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/joglekar21_interspeech.html": {
    "title": "Fearless Steps Challenge Phase-3 (FSC P3): Advancing SLT for Unseen Channel and Mission Data Across NASA Apollo Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leykum21_interspeech.html": {
    "title": "Voice Quality in Verbal Irony: Electroglottographic Analyses of Ironic Utterances in Standard Austrian German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hutin21_interspeech.html": {
    "title": "Synchronic Fortition in Five Romance Languages? A Large Corpus-Based Study of Word-Initial Devoicing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kraljevski21_interspeech.html": {
    "title": "Glottal Stops in Upper Sorbian: A Data-Driven Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ludusan21_interspeech.html": {
    "title": "Cue Interaction in the Perception of Prosodic Prominence: The Role of Voice Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rodriguez21_interspeech.html": {
    "title": "Glottal Sounds in Korebaju",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chanclu21_interspeech.html": {
    "title": "Automatic Classification of Phonation Types in Spontaneous Speech: Towards a New Workflow for the Characterization of Speakers' Voice Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/son21_interspeech.html": {
    "title": "Measuring Voice Quality Parameters After Speaker Pseudonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/steinert21_interspeech.html": {
    "title": "Audio-Visual Recognition of Emotional Engagement of People with Dementia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hecker21_interspeech.html": {
    "title": "Speaking Corona? Human and Machine Recognition of COVID-19 from Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21b_interspeech.html": {
    "title": "Acoustic-Prosodic, Lexical and Demographic Cues to Persuasiveness in Competitive Debate Speeches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/borgstrom21_interspeech.html": {
    "title": "Unsupervised Bayesian Adaptation of PLDA for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21i_interspeech.html": {
    "title": "The DKU-Duke-Lenovo System Description for the Fearless Steps Challenge Phase III",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21f_interspeech.html": {
    "title": "Improved Meta-Learning Training for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21j_interspeech.html": {
    "title": "Variational Information Bottleneck Based Regularization for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/brummer21_interspeech.html": {
    "title": "Out of a Hundred Trials, How Many Errors Does Your Speaker Verifier Make?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chojnacka21_interspeech.html": {
    "title": "SpeakerStew: Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21k_interspeech.html": {
    "title": "AntVoice Neural Speaker Embedding System for FFSVC 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21b_interspeech.html": {
    "title": "Gradient Regularization for Noise-Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kataria21_interspeech.html": {
    "title": "Deep Feature CycleGANs: Speaker Identity Preserving Non-Parallel Microphone-Telephone Domain Adaptation for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pu21_interspeech.html": {
    "title": "Scaling Effect of Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21c_interspeech.html": {
    "title": "Joint Feature Enhancement and Speaker Recognition with Multi-Objective Task-Oriented Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21g_interspeech.html": {
    "title": "Multi-Level Transfer Learning from Near-Field to Far-Field Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/patino21_interspeech.html": {
    "title": "Speaker Anonymisation Using the McAdams Coefficient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21_interspeech.html": {
    "title": "Multi-Stream Gated and Pyramidal Temporal Convolutional Neural Networks for Audio-Visual Speech Separation in Multi-Talker Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21l_interspeech.html": {
    "title": "TeCANet: Temporal-Contextual Attention Network for Environment-Aware Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gu21_interspeech.html": {
    "title": "Residual Echo and Noise Cancellation with Feature Attention Module and Multi-Domain Loss Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21c_interspeech.html": {
    "title": "MIMO Self-Attentive RNN Beamformer for Multi-Speaker Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/giri21_interspeech.html": {
    "title": "Personalized PercepNet: Real-Time, Low-Complexity Target Voice Separation and Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yemini21_interspeech.html": {
    "title": "Scene-Agnostic Multi-Microphone Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html": {
    "title": "Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding Vectors Based on Regular Simplex",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21h_interspeech.html": {
    "title": "A Deep Learning Approach to Multi-Channel and Multi-Microphone Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/na21_interspeech.html": {
    "title": "Joint Online Multichannel Acoustic Echo Cancellation, Speech Dereverberation and Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sato21_interspeech.html": {
    "title": "Should We Always Separate?: Switching Between Enhanced and Observed Signals for Overlapping Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/udupa21_interspeech.html": {
    "title": "Estimating Articulatory Movements in Speech Production with Transformer Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21b_interspeech.html": {
    "title": "Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jaramillo21_interspeech.html": {
    "title": "Speech Decomposition Based on a Hybrid Speech Model and Optimal Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21b_interspeech.html": {
    "title": "Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yarra21_interspeech.html": {
    "title": "Noise Robust Pitch Stylization Using Minimum Mean Absolute Error Criterion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21b_interspeech.html": {
    "title": "An Attribute-Aligned Strategy for Learning Speech Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shahrebabaki21_interspeech.html": {
    "title": "Raw Speech-to-Articulatory Inversion by Temporal Filtering and Decimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lilley21_interspeech.html": {
    "title": "Unsupervised Training of a DNN-Based Formant Tracker",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21c_interspeech.html": {
    "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21i_interspeech.html": {
    "title": "Synchronising Speech Segments with Musical Beats in Mandarin and English Singing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peplinski21_interspeech.html": {
    "title": "FRILL: A Non-Semantic Speech Embedding for Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mori21_interspeech.html": {
    "title": "Pitch Contour Separation from Overlapping Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21_interspeech.html": {
    "title": "Do Sound Event Representations Generalize to Other Audio Tasks? A Case Study in Audio Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21b_interspeech.html": {
    "title": "Data Augmentation for Spoken Language Understanding via Pretrained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/radfar21_interspeech.html": {
    "title": "FANS: Fusing ASR and NLU for On-Device SLU",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cao21c_interspeech.html": {
    "title": "Sequential End-to-End Intent and Slot Label Classification and Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muralidharan21_interspeech.html": {
    "title": "DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21d_interspeech.html": {
    "title": "A Context-Aware Hierarchical BERT Fusion Network for Multi-Turn Dialog Act Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21g_interspeech.html": {
    "title": "Pre-Training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/do21b_interspeech.html": {
    "title": "Predicting Temporal Performance Drop of Deployed Production Spoken Language Understanding Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ganhotra21_interspeech.html": {
    "title": "Integrating Dialog History into End-to-End Spoken Language Understanding Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21_interspeech.html": {
    "title": "Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/arora21_interspeech.html": {
    "title": "Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sun21b_interspeech.html": {
    "title": "Semantic Data Augmentation for End-to-End Mandarin Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21c_interspeech.html": {
    "title": "Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21m_interspeech.html": {
    "title": "Low Resource German ASR with Untranscribed Data Spoken by Non-Native Children â€” INTERSPEECH 2021 Shared Task SPAPL System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sim21_interspeech.html": {
    "title": "Robust Continuous On-Device Personalization for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21b_interspeech.html": {
    "title": "Speaker Normalization Using Joint Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21c_interspeech.html": {
    "title": "The TAL System for the INTERSPEECH2021 Shared Task on Automatic Speech Recognition for Non-Native Childrens Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lam21b_interspeech.html": {
    "title": "On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21_interspeech.html": {
    "title": "Zero-Shot Cross-Lingual Phonetic Recognition with External Language Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21c_interspeech.html": {
    "title": "Rapid Speaker Adaptation for Conformer Transducer: Attention and Bias Are All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/das21b_interspeech.html": {
    "title": "Best of Both Worlds: Robust Accented Speech Recognition with Adversarial Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chu21_interspeech.html": {
    "title": "Extending Pronunciation Dictionary with Automatically Detected Word Mispronunciations to Improve PAII's System for Interspeech 2021 Non-Native Child English Close Track ASR Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21d_interspeech.html": {
    "title": "CVC: Contrastive Learning for Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21d_interspeech.html": {
    "title": "A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eskimez21_interspeech.html": {
    "title": "One-Shot Voice Conversion with Speaker-Agnostic StarGAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/koshizuka21_interspeech.html": {
    "title": "Fine-Tuning Pre-Trained Voice Conversion Model for Adding New Target Speakers with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21n_interspeech.html": {
    "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21e_interspeech.html": {
    "title": "StarGANv2-VC: A Diverse, Unsupervised, Non-Parallel Framework for Natural-Sounding Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21c_interspeech.html": {
    "title": "Normalization Driven Zero-Shot Multi-Speaker Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sakamoto21_interspeech.html": {
    "title": "StarGAN-VC+ASR: StarGAN-Based Non-Parallel Voice Conversion Regularized by Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21d_interspeech.html": {
    "title": "Two-Pathway Style Embedding for Arbitrary Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21c_interspeech.html": {
    "title": "Non-Parallel Any-to-Many Voice Conversion by Replacing Speaker Statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21c_interspeech.html": {
    "title": "Cross-Lingual Voice Conversion with a Cycle Consistency Loss on Linguistic Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/du21_interspeech.html": {
    "title": "Improving Robustness of One-Shot Voice Conversion with Deep Discriminative Speaker Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/white21_interspeech.html": {
    "title": "Optimizing an Automatic Creaky Voice Detection Method for Australian English Speaking Females",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/penney21_interspeech.html": {
    "title": "A Comparison of Acoustic Correlates of Voice Quality Across Different Recording Devices: A Cautionary Tale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sfakianaki21_interspeech.html": {
    "title": "Investigating Voice Function Characteristics of Greek Speakers with Hearing Loss Using Automatic Glottal Source Feature Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huckvale21_interspeech.html": {
    "title": "Automated Detection of Voice Disorder in the SaarbrÃ¼cken Voice Database: Effects of Pathology Subset and Audio Materials",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lulich21_interspeech.html": {
    "title": "Accelerometer-Based Measurements of Voice Quality in Children During Semi-Occluded Vocal Tract Exercise with a Narrow Straw in Air",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/perez21_interspeech.html": {
    "title": "Articulatory Coordination for Speech Motor Tracking in Huntington Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ferrer21_interspeech.html": {
    "title": "Modeling Dysphonia Severity as a Function of Roughness and Breathiness Ratings in the GRBAS Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karpov21_interspeech.html": {
    "title": "Golos: Russian Dataset for Speech Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sadhu21b_interspeech.html": {
    "title": "Radically Old Way of Computing Spectra: Applications in End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alghezi21_interspeech.html": {
    "title": "Self-Supervised End-to-End ASR for Low Resource L2 Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oneill21_interspeech.html": {
    "title": "SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/evain21_interspeech.html": {
    "title": "",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sturm21_interspeech.html": {
    "title": "Prosodic Accommodation in Face-to-Face and Telephone Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/riverincoutlee21_interspeech.html": {
    "title": "Dialect Features in Heterogeneous and Homogeneous Gheg Speaking Communities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zellers21_interspeech.html": {
    "title": "An Exploration of the Acoustic Space of Rhotics and Laterals in Ruruuli",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bodur21_interspeech.html": {
    "title": "Domain-Initial Strengthening in Turkish: Acoustic Cues to Prosodic Hierarchy in Stop Consonants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zmolikova21_interspeech.html": {
    "title": "Auxiliary Loss Function for Target Speech Extraction and Recognition with Weak Supervision Based on Speaker Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/borsdorf21_interspeech.html": {
    "title": "Universal Speaker Extraction in the Presence and Absence of Target Speakers for Speech of One and Two Talkers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mateju21_interspeech.html": {
    "title": "Using X-Vectors for Speech Activity Detection in Broadcast Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/salvati21_interspeech.html": {
    "title": "Time Delay Estimation for Speaker Localization Using CNN-Based Parametrized GCC-PHAT Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yousefi21_interspeech.html": {
    "title": "Real-Time Speaker Counting in a Cocktail Party Scenario Using Attention-Guided Convolutional Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21d_interspeech.html": {
    "title": "End-to-End Language Diarization for Bilingual Code-Switching Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/duroselle21_interspeech.html": {
    "title": "Modeling and Training Strategies for Language Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21o_interspeech.html": {
    "title": "A Weight Moving Average Based Alternate Decoupled Learning Algorithm for Long-Tailed Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21b_interspeech.html": {
    "title": "Improving Accent Identification and Accented Speech Recognition Under a Framework of Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fan21_interspeech.html": {
    "title": "Exploring wav2vec 2.0 on Speaker Verification and Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ramesh21_interspeech.html": {
    "title": "Self-Supervised Phonotactic Representations for Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21j_interspeech.html": {
    "title": "E2E-Based Multi-Task Learning Approach to Joint Speech and Accent Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tzudir21_interspeech.html": {
    "title": "Excitation Source Feature Based Dialect Identification in Ao â€” A Low Resource Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/khare21_interspeech.html": {
    "title": "Low Resource ASR: The Surprising Effectiveness of High Resource Transliteration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/feng21_interspeech.html": {
    "title": "Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kamper21_interspeech.html": {
    "title": "Towards Unsupervised Phone and Word Segmentation Using Self-Supervised Vector-Quantized Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jiang21_interspeech.html": {
    "title": "Speech SimCLR: Combining Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jacobs21_interspeech.html": {
    "title": "Multilingual Transfer of Acoustic Word Embeddings Improves When Training on Languages Related to the Target Zero-Resource Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/niekerk21_interspeech.html": {
    "title": "Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/takahashi21_interspeech.html": {
    "title": "Unsupervised Neural-Based Graph Clustering for Variable-Length Speech Representation Discovery of Zero-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maekaku21_interspeech.html": {
    "title": "Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cui21_interspeech.html": {
    "title": "Identifying Indicators of Vulnerability from Short Speech Segments Using Acoustic and Textual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dunbar21_interspeech.html": {
    "title": "The Zero Resource Speech Challenge 2021: Spoken Language Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gudur21_interspeech.html": {
    "title": "Zero-Shot Federated Learning with New Classes for Audio Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rouditchenko21_interspeech.html": {
    "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21b_interspeech.html": {
    "title": "N-Singer: A Non-Autoregressive Korean Singing Voice Synthesis System for Pronunciation Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maniati21_interspeech.html": {
    "title": "Cross-Lingual Low Resource Speaker Adaptation Using Phonological Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhan21_interspeech.html": {
    "title": "Improve Cross-Lingual Text-To-Speech Synthesis on Monolingual Corpora with Pitch Contour Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21d_interspeech.html": {
    "title": "Cross-Lingual Voice Conversion with Disentangled Universal Linguistic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21e_interspeech.html": {
    "title": "EfficientSing: A Chinese Singing Voice Synthesis System Using Duration-Free Acoustic Model and HiFi-GAN Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xin21_interspeech.html": {
    "title": "Cross-Lingual Speaker Adaptation Using Domain Adaptation and Speaker Consistency Loss for Text-To-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shang21_interspeech.html": {
    "title": "Incorporating Cross-Speaker Style Transfer for Multi-Language Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kesim21_interspeech.html": {
    "title": "Investigating Contributions of Speech and Facial Landmarks for Talking Head Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/si21b_interspeech.html": {
    "title": "Speech2Video: Cross-Modal Distillation for Speech to Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21c_interspeech.html": {
    "title": "NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21c_interspeech.html": {
    "title": "QISTA-Net-Audio: Audio Super-Resolution via Non-Convex â„“_q-Norm Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wen21_interspeech.html": {
    "title": "X-net: A Joint Scale Down and Scale Up Method for Voice Call",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21k_interspeech.html": {
    "title": "WSRGlow: A Glow-Based Waveform Generative Model for Audio Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yi21_interspeech.html": {
    "title": "Half-Truth: A Partially Fake Audio Detection Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chettri21_interspeech.html": {
    "title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheon21_interspeech.html": {
    "title": "Coded Speech Enhancement Using Neural Network-Based Vector-Quantized Residual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/drude21_interspeech.html": {
    "title": "Multi-Channel Opus Compression for Far-Field Automatic Speech Recognition with a Fixed Bitrate Budget",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/siegert21_interspeech.html": {
    "title": "Effects of Prosodic Variations on Accidental Triggers of a Commercial Voice Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gabrys21_interspeech.html": {
    "title": "Improving the Expressiveness of Neural Vocoding with Non-Affine Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/prajapati21_interspeech.html": {
    "title": "Voice Privacy Through x-Vector and CycleGAN-Based Anonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21d_interspeech.html": {
    "title": "A Two-Stage Approach to Speech Bandwidth Extension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/byun21_interspeech.html": {
    "title": "Development of a Psychoacoustic Loss Function for the Deep Neural Network (DNN)-Based Speech Coder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stoidis21_interspeech.html": {
    "title": "Protecting Gender and Identity with Disentangled Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/aldholmi21_interspeech.html": {
    "title": "Perception of Standard Arabic Synthetic Speech Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kishiyama21_interspeech.html": {
    "title": "The Influence of Parallel Processing on Illusory Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chingacham21_interspeech.html": {
    "title": "Exploring the Potential of Lexical Paraphrases for Mitigating Noise-Induced Comprehension Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/simantiraki21_interspeech.html": {
    "title": "SpeechAdjuster: A Tool for Investigating Listener Preferences and Speech Intelligibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saito21_interspeech.html": {
    "title": "VocalTurk: Exploring Feasibility of Crowdsourced Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21e_interspeech.html": {
    "title": "Effects of Aging and Age-Related Hearing Loss on Talker Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21l_interspeech.html": {
    "title": "Relationships Between Perceptual Distinctiveness, Articulatory Complexity and Functional Load in Speech Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/terblanche21_interspeech.html": {
    "title": "Human Spoofing Detection Performance on Degraded Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/einfeldt21_interspeech.html": {
    "title": "Reliable Estimates of Interpretable Cue Effects with Active Learning in Psycholinguistic Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21d_interspeech.html": {
    "title": "Towards the Explainability of Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zeng21_interspeech.html": {
    "title": "Primacy of Mouth over Eyes: Eye Movement Evidence from Audiovisual Mandarin Lexical Tones and Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ashihara21_interspeech.html": {
    "title": "Investigating the Impact of Spectral and Temporal Degradation on End-to-End Automatic Speech Recognition Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21c_interspeech.html": {
    "title": "Super-Human Performance in Online Low-Latency Recognition of Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/joshi21_interspeech.html": {
    "title": "Multiple Softmax Architecture for Streaming Multilingual End-to-End ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/le21_interspeech.html": {
    "title": "Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sainath21_interspeech.html": {
    "title": "An Efficient Streaming Non-Recurrent On-Device End-to-End Model with Improvements to Rare-Word Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21_interspeech.html": {
    "title": "Streaming Multi-Talker Speech Recognition with Joint Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/moriya21_interspeech.html": {
    "title": "Streaming End-to-End Speech Recognition for Hybrid RNN-T/Attention Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schwarz21_interspeech.html": {
    "title": "Improving RNN-T ASR Accuracy Using Context Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21e_interspeech.html": {
    "title": "HMM-Free Encoder Pre-Training for Streaming RNN Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cui21b_interspeech.html": {
    "title": "Reducing Exposure Bias in Training Recurrent Neural Network Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/doutre21_interspeech.html": {
    "title": "Bridging the Gap Between Streaming and Non-Streaming ASR Systems by Distilling Ensembles of CTC and RNN-T Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/audhkhasi21_interspeech.html": {
    "title": "Mixture Model Attention: Flexible Streaming and Non-Streaming Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/inaguma21_interspeech.html": {
    "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/moritz21_interspeech.html": {
    "title": "Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21d_interspeech.html": {
    "title": "Multi-Mode Transformer Transducer with Stochastic Future Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ren21_interspeech.html": {
    "title": "A Causal U-Net Based Neural Beamforming Network for Real-Time Multi-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21d_interspeech.html": {
    "title": "A Partitioned-Block Frequency-Domain Adaptive Kalman Filter for Stereophonic Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21p_interspeech.html": {
    "title": "Real-Time Independent Vector Analysis Using Semi-Supervised Nonnegative Matrix Factorization as a Source Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21b_interspeech.html": {
    "title": "Improving Channel Decorrelation for Multi-Channel Target Speech Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21f_interspeech.html": {
    "title": "Inplace Gated Convolutional Recurrent Neural Network for Dual-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/raj21_interspeech.html": {
    "title": "SRIB-LEAP Submission to Far-Field Multi-Channel Speech Enhancement Challenge for Video Conferencing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21_interspeech.html": {
    "title": "Real-Time Multi-Channel Speech Enhancement Based on Neural Network Masking with Attention Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ganapathy21_interspeech.html": {
    "title": "Uncovering the Acoustic Cues of COVID-19 Infection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fung21_interspeech.html": {
    "title": "Ethical and Technological Challenges of Conversational AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fohr21_interspeech.html": {
    "title": "BERT-Based Semantic Model for Rescoring N-Best Speech Recognition List",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/benes21_interspeech.html": {
    "title": "Text Augmentation for Language Models in High Error Recognition Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21b_interspeech.html": {
    "title": "On Sampling-Based Training Criteria for Neural Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pylkkonen21_interspeech.html": {
    "title": "Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cieri21_interspeech.html": {
    "title": "Using Games to Augment Corpora for Language Recognition and Confusability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fenu21_interspeech.html": {
    "title": "Fair Voice Biometrics: Impact of Demographic Imbalance on Group Fairness in Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21m_interspeech.html": {
    "title": "Knowledge Distillation from Multi-Modality to Single-Modality for Person Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/noe21_interspeech.html": {
    "title": "Adversarial Disentanglement of Speaker Representation for Attribute-Driven Privacy Preservation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/romana21_interspeech.html": {
    "title": "Automatically Detecting Errors and Disfluencies in Read Speech to Predict Cognitive Impairment in People with Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vaysse21_interspeech.html": {
    "title": "Automatic Extraction of Speech Rhythm Descriptors for Speech Intelligibility Assessment in the Context of Head and Neck Cancers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qi21b_interspeech.html": {
    "title": "Speech Disorder Classification Using Extended Factorized Hierarchical Variational Auto-Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mathad21_interspeech.html": {
    "title": "The Impact of Forced-Alignment Errors on Automatic Pronunciation Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/villatorotello21_interspeech.html": {
    "title": "Late Fusion of the Available Lexicon and Raw Waveform-Based Acoustic Modeling for Depression and Dementia Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shandiz21_interspeech.html": {
    "title": "Neural Speaker Embeddings for Ultrasound-Based Silent Speech Interfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lamba21_interspeech.html": {
    "title": "Cross-Modal Learning for Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cook21_interspeech.html": {
    "title": "A Psychology-Driven Computational Analysis of Political Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/santoso21_interspeech.html": {
    "title": "Speech Emotion Recognition Based on Attention Weight Correction Using Word-Level Confidence Measure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/silpachai21_interspeech.html": {
    "title": "Effects of Voice Type and Task on L2 Learners' Awareness of Pronunciation Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/menshikova21_interspeech.html": {
    "title": "Lexical Entrainment and Intra-Speaker Variability in Cooperative Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nasreen21_interspeech.html": {
    "title": "Detecting Alzheimer's Disease Using Interactional and Acoustic Features from Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kothare21_interspeech.html": {
    "title": "Investigating the Interplay Between Affective, Phonatory and Motoric Subsystems in Autism Spectrum Disorder Using a Multimodal Dialogue Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ishi21_interspeech.html": {
    "title": "Analysis of Eye Gaze Reasons and Gaze Aversions During Three-Party Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21e_interspeech.html": {
    "title": "Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21q_interspeech.html": {
    "title": "A Light-Weight Contextual Spelling Correction Model for Customizing Transducer-Based Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21_interspeech.html": {
    "title": "Incorporating External POS Tagger for Punctuation Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/papadourakis21_interspeech.html": {
    "title": "Phonetically Induced Subwords for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mansfield21_interspeech.html": {
    "title": "Revisiting Parity of Human vs. Machine Conversational Speech Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21f_interspeech.html": {
    "title": "Lookup-Table Recurrent Language Models for Long Tail Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/andresferrer21_interspeech.html": {
    "title": "Contextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21g_interspeech.html": {
    "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhao21_interspeech.html": {
    "title": "BART Based Semantic Correction for Mandarin Automatic Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dai21b_interspeech.html": {
    "title": "Class-Based Neural Network Language Model for Second-Pass Rescoring in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kurata21_interspeech.html": {
    "title": "Improving Customization of Neural Transducers by Mitigating Acoustic Mismatch of Synthesized Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saebi21_interspeech.html": {
    "title": "A Discriminative Entity-Aware Language Model for Virtual Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/namazifar21_interspeech.html": {
    "title": "Correcting Automated and Manual Speech Transcription Errors Using Warped Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21b_interspeech.html": {
    "title": "Dynamic Encoder Transducer: A Flexible Solution for Trading Off Accuracy for Latency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21n_interspeech.html": {
    "title": "Domain-Aware Self-Attention for Multi-Domain Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zeyer21_interspeech.html": {
    "title": "Librispeech Transducer Model with Internal Language Model Prior Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mavandadi21_interspeech.html": {
    "title": "A Deliberation-Based Joint Acoustic and Text Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tuske21_interspeech.html": {
    "title": "On the Limit of English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/an21_interspeech.html": {
    "title": "Deformable TDNN with Adaptive Receptive Fields for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/you21_interspeech.html": {
    "title": "SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leong21_interspeech.html": {
    "title": "Online Compressive Transformer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21e_interspeech.html": {
    "title": "End to End Transformer-Based Contextual Speech Recognition Based on Pointer Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karita21_interspeech.html": {
    "title": "A Comparative Study on Neural Architectures and Training Methods for Japanese Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hori21b_interspeech.html": {
    "title": "Advanced Long-Context End-to-End Speech Recognition Using Context-Expanded Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/haidar21_interspeech.html": {
    "title": "Transformer-Based ASR Incorporating Time-Reduction Layer and Fine-Tuning with Self-Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mahadeokar21_interspeech.html": {
    "title": "Flexi-Transducer: Optimizing Latency, Accuracy and Compute for Multi-Domain On-Device Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/falkowskigilski21_interspeech.html": {
    "title": "Difference in Perceived Speech Signal Quality Assessment Among Monolingual and Bilingual Teenage Students",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schymura21_interspeech.html": {
    "title": "PILOT: Introducing Transformers for Probabilistic Sound Event Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/togami21_interspeech.html": {
    "title": "Sound Source Localization with Majorization Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mittag21_interspeech.html": {
    "title": "NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/naderi21_interspeech.html": {
    "title": "Subjective Evaluation of Noise Suppression Algorithms in Crowdsourcing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/geng21_interspeech.html": {
    "title": "Reliable Intensity Vector Selection for Multi-Source Direction-of-Arrival Estimation Using a Single Acoustic Vector Sensor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yu21_interspeech.html": {
    "title": "MetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/toma21_interspeech.html": {
    "title": "CNN-Based Processing of Acoustic and Radio Frequency Signals for Speaker Localization from MAVs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/itoyama21_interspeech.html": {
    "title": "Assessment of von Mises-Bernoulli Deep Neural Network in Sound Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21g_interspeech.html": {
    "title": "Feature Fusion by Attention Networks for Robust DOA Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21f_interspeech.html": {
    "title": "Far-Field Speaker Localization and Adaptive GLMB Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/narayanaswamy21_interspeech.html": {
    "title": "On the Design of Deep Priors for Unsupervised Audio Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21h_interspeech.html": {
    "title": "CramÃ©r-Rao Lower Bound for DOA Estimation with an Array of Directional Microphones in Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/you21b_interspeech.html": {
    "title": "GAN Vocoder: Multi-Resolution Discriminator Is All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cong21_interspeech.html": {
    "title": "Glow-WaveGAN: Learning Speech Representations from GAN-Based Variational Auto-Encoder for High Fidelity Flow-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yoneyama21_interspeech.html": {
    "title": "Unified Source-Filter GAN: Unified Source-Filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mizuta21_interspeech.html": {
    "title": "Harmonic WaveGAN: GAN-Based Speech Waveform Generation Model with Harmonic Structure Discriminator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21f_interspeech.html": {
    "title": "Fre-GAN: Adversarial Frequency-Consistent Audio Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21e_interspeech.html": {
    "title": "GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jang21_interspeech.html": {
    "title": "UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alradhi21_interspeech.html": {
    "title": "Continuous Wavelet Vocoder-Based Decomposition of Parametric Speech Waveform Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tobing21_interspeech.html": {
    "title": "High-Fidelity and Low-Latency Universal Neural Vocoder Based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21h_interspeech.html": {
    "title": "Basis-MelGAN: Efficient Neural Vocoder Based on Audio Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hwang21_interspeech.html": {
    "title": "High-Fidelity Parallel WaveGAN with Multi-Band Harmonic-Plus-Noise Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21i_interspeech.html": {
    "title": "SpecRec: An Alternative Solution for Improving End-to-End Speech-to-Text Translation via Spectrogram Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cherry21_interspeech.html": {
    "title": "Subtitle Translation as Markup Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21r_interspeech.html": {
    "title": "Large-Scale Self- and Semi-Supervised Learning for Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21s_interspeech.html": {
    "title": "CoVoST 2 and Massively Multilingual Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheng21_interspeech.html": {
    "title": "AlloST: Low-Resource Speech Translation Without Source Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/effendi21_interspeech.html": {
    "title": "Weakly-Supervised Speech-to-Text Mapping with Visually Connected Non-Parallel Speech-Text Data Using Cyclic Partially-Aligned Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tokuyama21_interspeech.html": {
    "title": "Transcribing Paralinguistic Acoustic Cues to Target Language Text in Transformer-Based Speech-to-Text Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ye21_interspeech.html": {
    "title": "End-to-End Speech Translation via Cross-Modal Progressive Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ko21_interspeech.html": {
    "title": "ASR Posterior-Based Loss for Multi-Task End-to-End Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/perezgonzalezdemartos21_interspeech.html": {
    "title": "Towards Simultaneous Machine Interpretation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/martucci21_interspeech.html": {
    "title": "Lexical Modeling of ASR Errors for Robust Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vyas21_interspeech.html": {
    "title": "Optimally Encoding Inductive Biases into the Transformer Improves End-to-End Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ananthanarayana21_interspeech.html": {
    "title": "Effects of Feature Scaling and Fusion on Sign Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alenin21_interspeech.html": {
    "title": "The ID R&D System Description for Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/thienpondt21_interspeech.html": {
    "title": "Integrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gusev21_interspeech.html": {
    "title": "SdSVC Challenge 2021: Tips and Tricks to Boost the Short-Duration Speaker Verification System Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kang21_interspeech.html": {
    "title": "Team02 Text-Independent Speaker Verification System for SdSV Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qin21_interspeech.html": {
    "title": "Our Learned Lessons from Cross-Lingual Speaker Verification: The CRMI-DKU System Description for the Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21o_interspeech.html": {
    "title": "Investigation of IMU&Elevoc Submission for the Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21_interspeech.html": {
    "title": "The Sogou System for Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21c_interspeech.html": {
    "title": "The SJTU System for Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cho21_interspeech.html": {
    "title": "Multi-Speaker Emotional Text-to-Speech Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/prazak21_interspeech.html": {
    "title": "Live TV Subtitling Through Respeaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fragner21_interspeech.html": {
    "title": "Autonomous Robot for Measuring Room Impulse Responses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/beskow21_interspeech.html": {
    "title": "Expressive Robot Performance Based on Facial Motion Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dominguez21_interspeech.html": {
    "title": "ThemePro 2.0: Showcasing the Role of Thematic Progression in Engaging Human-Computer Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guruju21_interspeech.html": {
    "title": "Addressing Compliance in Call Centers with Entity Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gogineni21_interspeech.html": {
    "title": "Audio Segmentation Based Conversational Silence Detection for Contact Center Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/raj21b_interspeech.html": {
    "title": "Reformulating DOVER-Lap Label Mapping as a Graph Partitioning Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tak21_interspeech.html": {
    "title": "Graph Attention Networks for Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mingote21_interspeech.html": {
    "title": "Log-Likelihood-Ratio Cost Function as Objective Loss for Speaker Verification Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21c_interspeech.html": {
    "title": "Effective Phase Encoding for End-To-End Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21d_interspeech.html": {
    "title": "Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/machacek21_interspeech.html": {
    "title": "Lost in Interpreting: Speech Translation from Source or Interpreter?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pouthier21_interspeech.html": {
    "title": "Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-Based Multimodal Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wallbridge21_interspeech.html": {
    "title": "It's Not What You Said, it's How You Said it: Discriminative Perception of Speech as a Multichannel Communication System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/michael21_interspeech.html": {
    "title": "Extending the Fullband E-Model Towards Background Noise, Bursty Packet Loss, and Conversational Degradations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bergler21_interspeech.html": {
    "title": "ORCA-SLANG: An Automatic Multi-Stage Semi-Supervised Deep Learning Framework for Large-Scale Killer Whale Call Type Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/boes21_interspeech.html": {
    "title": "Audiovisual Transfer Learning for Audio Tagging and Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nessler21_interspeech.html": {
    "title": "Non-Intrusive Speech Quality Assessment with Transfer Learning and Subject-Specific Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oncescu21_interspeech.html": {
    "title": "Audio Retrieval with Natural Language Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/giollo21_interspeech.html": {
    "title": "Bootstrap an End-to-End ASR System by Multilingual Training, Transfer Learning, Text-to-Text Mapping and Synthetic Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pham21_interspeech.html": {
    "title": "Efficient Weight Factorization for Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/conneau21_interspeech.html": {
    "title": "Unsupervised Cross-Lingual Representation Learning for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hayakawa21_interspeech.html": {
    "title": "Language and Speaker-Independent Feature Transformation for End-to-End Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/n21_interspeech.html": {
    "title": "Using Large Self-Supervised Models for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21e_interspeech.html": {
    "title": "Dual Script E2E Framework for Multilingual and Code-Switching ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/diwan21_interspeech.html": {
    "title": "MUCS 2021: Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/winata21_interspeech.html": {
    "title": "Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sailor21_interspeech.html": {
    "title": "SRI-B End-to-End System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21f_interspeech.html": {
    "title": "Hierarchical Phone Recognition with Compositional Phonetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chowdhury21_interspeech.html": {
    "title": "Towards One Model to Rule All: Multilingual Strategy for Dialectal Code-Switching Arabic ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21b_interspeech.html": {
    "title": "Differentiable Allophone Graphs for Language-Universal Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/martin21_interspeech.html": {
    "title": "Automatic Speech Recognition Systems Errors for Objective Sleepiness Detection Through Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gillick21_interspeech.html": {
    "title": "Robust Laughter Detection in Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nagano21_interspeech.html": {
    "title": "Impact of Emotional State on Estimation of Willingness to Buy from Advertising Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alsofyani21_interspeech.html": {
    "title": "Stacked Recurrent Neural Networks for Speech-Based Inference of Attachment Condition in School Age Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/aloshban21_interspeech.html": {
    "title": "Language or Paralanguage, This is the Problem: Comparing Depressed and Non-Depressed Speakers Through the Analysis of Gated Multimodal Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tammewar21_interspeech.html": {
    "title": "Emotion Carrier Recognition from Personal Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/condron21_interspeech.html": {
    "title": "Non-Verbal Vocalisation and Laughter Detection Using Sequence-to-Sequence Models and Multi-Label Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cai21_interspeech.html": {
    "title": "TDCA-Net: Time-Domain Channel Attention Network for Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/botelho21_interspeech.html": {
    "title": "Visual Speech for Obstructive Sleep Apnea Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maruri21_interspeech.html": {
    "title": "Analysis of Contextual Voice Changes in Remote Meetings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seneviratne21_interspeech.html": {
    "title": "Speech Based Depression Severity Level Classification Using a Multi-Stage Dilated CNN-LSTM Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21g_interspeech.html": {
    "title": "Multi-Domain Knowledge Distillation via Uncertainty-Matching for End-to-End ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/macoskey21_interspeech.html": {
    "title": "Learning a Neural Diff for Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21p_interspeech.html": {
    "title": "Stochastic Attention Head Removal: A Simple and Effective Method for Improving Transformer Based ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21b_interspeech.html": {
    "title": "Model-Agnostic Fast Adaptive Multi-Objective Balancing Algorithm for Multilingual Automatic Speech Recognition Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chang21b_interspeech.html": {
    "title": "Towards Lifelong Learning of End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leal21_interspeech.html": {
    "title": "Self-Adaptive Distillation for Multilingual Speech Recognition: Leveraging Student Independence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21f_interspeech.html": {
    "title": "Regularizing Word Segmentation by Creating Misspellings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21t_interspeech.html": {
    "title": "Multitask Training with Text Data for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21j_interspeech.html": {
    "title": "Emitting Word Timings with HMM-Free End-to-End System in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/droppo21_interspeech.html": {
    "title": "Scaling Laws for Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/billa21_interspeech.html": {
    "title": "Leveraging Non-Target Language Resources to Improve ASR Performance in a Target Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fasoli21_interspeech.html": {
    "title": "4-Bit Quantization of LSTM-Based Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/masumura21_interspeech.html": {
    "title": "Unified Autoregressive Modeling for Joint End-to-End Multi-Talker Overlapped Speech Recognition and Speaker Attribute Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/meng21_interspeech.html": {
    "title": "Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jiang21b_interspeech.html": {
    "title": "Variable Frame Rate Acoustic Models Using Minimum Error Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kaland21_interspeech.html": {
    "title": "How f0 and Phrase Position Affect Papuan Malay Word Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jespersen21_interspeech.html": {
    "title": "On the Feasibility of the Danish Model of Intonational Transcription: Phonetic Evidence from Jutlandic Danish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/meli21_interspeech.html": {
    "title": "An Experiment in Paratone Detection in a Prosodically Annotated EAP Spoken Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gerazov21_interspeech.html": {
    "title": "ProsoBeast Prosody Annotation Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tran21_interspeech.html": {
    "title": "Assessing the Use of Prosody in Constituency Parsing of Imperfect Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21i_interspeech.html": {
    "title": "Targeted and Targetless Neutral Tones in Taiwanese Southern Min",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gosy21_interspeech.html": {
    "title": "The Interaction of Word Complexity and Word Duration in an Agglutinative Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21b_interspeech.html": {
    "title": "Taiwan Min Nan (Taiwanese) Checked Tones Sound Change",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jakob21_interspeech.html": {
    "title": "In-Group Advantage in the Perception of Emotions: Evidence from Three Varieties of German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gobl21_interspeech.html": {
    "title": "The LF Model in the Frequency Domain for Glottal Airflow Modelling Without Aliasing Distortion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wagner21_interspeech.html": {
    "title": "Parsing Speech for Grouping and Prominence, and the Typology of Rhythm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mumtaz21_interspeech.html": {
    "title": "Prosody of Case Markers in Urdu",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stefansdottir21_interspeech.html": {
    "title": "Articulatory Characteristics of Icelandic Voiced Fricative Lenition: Gradience, Categoricity, and Speaker/Gesture-Specific Effects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/johnson21_interspeech.html": {
    "title": "Leveraging the Uniformity Framework to Examine Crosslinguistic Similarity for Long-Lag Stops in Spontaneous Cantonese-English Bilingual Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sivaraman21_interspeech.html": {
    "title": "Personalized Speech Enhancement Through Self-Supervised Data Augmentation and Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saddler21_interspeech.html": {
    "title": "Speech Denoising with Auditory Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eskimez21b_interspeech.html": {
    "title": "Human Listening and Live Captioning: Multi-Task Training for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21g_interspeech.html": {
    "title": "Multi-Stage Progressive Speech Enhancement Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chang21c_interspeech.html": {
    "title": "Single-Channel Speech Enhancement Using Learnable Loss Mixup",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21q_interspeech.html": {
    "title": "A Maximum Likelihood Approach to SNR-Progressive Learning Using Generalized Gaussian Distribution for LSTM-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/agrawal21_interspeech.html": {
    "title": "Whisper Speech Enhancement Using Joint Variational Autoencoder for Improved Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21d_interspeech.html": {
    "title": "DEMUCS-Mobile : On-Device Lightweight Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kashyap21_interspeech.html": {
    "title": "Speech Denoising Without Clean Training Data: A Noise2Noise Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dang21_interspeech.html": {
    "title": "Improved Speech Enhancement Using a Complex-Domain GAN with Fused Time-Domain and Time-Frequency Domain Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21r_interspeech.html": {
    "title": "Speech Enhancement with Topology-Enhanced Generative Adversarial Networks (GANs)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bu21_interspeech.html": {
    "title": "Learning Speech Structure to Improve Time-Frequency Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21h_interspeech.html": {
    "title": "SE-Conformer: Time-Domain Speech Enhancement Using Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kongthaworn21_interspeech.html": {
    "title": "Spectral and Latent Speech Representation Distortion for TTS Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/valentinibotinhao21_interspeech.html": {
    "title": "Detection and Analysis of Attention Errors in Sequence-to-Sequence Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zandie21_interspeech.html": {
    "title": "RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21c_interspeech.html": {
    "title": "AISHELL-3: A Multi-Speaker Mandarin TTS Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eng21_interspeech.html": {
    "title": "Comparing Speech Enhancement Techniques for Voice Adaptation-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cui21c_interspeech.html": {
    "title": "EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rallabandi21_interspeech.html": {
    "title": "Perception of Social Speaker Characteristics in Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bakhturina21_interspeech.html": {
    "title": "Hi-Fi Multi-Speaker English TTS Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tseng21b_interspeech.html": {
    "title": "Utilizing Self-Supervised Representations for MOS Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mussakhojayeva21_interspeech.html": {
    "title": "KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/taylor21_interspeech.html": {
    "title": "Confidence Intervals for ASR-Based TTS Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/reddy21_interspeech.html": {
    "title": "INTERSPEECH 2021 Deep Noise Suppression Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21g_interspeech.html": {
    "title": "A Simultaneous Denoising and Dereverberation Framework with Target Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21h_interspeech.html": {
    "title": "Deep Noise Suppression with Non-Intrusive PESQNet Supervision Enabling the Use of Real Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/le21b_interspeech.html": {
    "title": "DPCRN: Dual-Path Convolution Recurrent Network for Single Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lv21_interspeech.html": {
    "title": "DCCRN+: Channel-Wise Subband DCCRN with SNR Estimation for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21s_interspeech.html": {
    "title": "DBNet: A Dual-Branch Network Architecture Processing on Spectrum and Waveform for Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21t_interspeech.html": {
    "title": "Low-Delay Speech Enhancement Using Perceptually Motivated Target and Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oostermeijer21_interspeech.html": {
    "title": "Lightweight Causal Transformer with Local Self-Attention for Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ristea21_interspeech.html": {
    "title": "Self-Paced Ensemble Learning for Speech and Audio Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kojima21_interspeech.html": {
    "title": "Knowledge Distillation for Streaming Transformerâ€“Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lohrenz21_interspeech.html": {
    "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zaiem21_interspeech.html": {
    "title": "Conditional Independence for Pretext Task Selection in Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zeineldeen21_interspeech.html": {
    "title": "Investigating Methods to Improve Language Model Integration for Attention-Based Encoder-Decoder ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vyas21b_interspeech.html": {
    "title": "Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/moine21_interspeech.html": {
    "title": "Speaker Attentive Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leem21_interspeech.html": {
    "title": "Separation of Emotional and Reconstruction Embeddings on Ladder Network to Improve Speech Emotion Recognition Robustness in Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/georgiou21_interspeech.html": {
    "title": "M",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/klejch21_interspeech.html": {
    "title": "The CSTR System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21d_interspeech.html": {
    "title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21e_interspeech.html": {
    "title": "Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/khosravani21_interspeech.html": {
    "title": "Modeling Dialectal Variation for Swiss German Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/egorova21_interspeech.html": {
    "title": "Out-of-Vocabulary Words Detection with Attention and CTC Alignments in an End-to-End ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wiesner21_interspeech.html": {
    "title": "Training Hybrid Models on Noisy Transliterated Transcripts for Code-Switched Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21c_interspeech.html": {
    "title": "Speech Intelligibility of Dysarthric Speech: Human Scores and Acoustic-Phonetic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21i_interspeech.html": {
    "title": "Analyzing Short Term Dynamic Speech Features for Understanding Behavioral Traits of Children with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jesko21_interspeech.html": {
    "title": "Vocalization Recognition of People with Profound Intellectual and Multiple Disabilities (PIMD) Using Machine Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fivela21_interspeech.html": {
    "title": "Phonetic Complexity, Speech Accuracy and Intelligibility Assessment of Italian Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ng21_interspeech.html": {
    "title": "Detection of Consonant Errors in Disordered Speech Based on Consonant-Vowel Segment Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hair21_interspeech.html": {
    "title": "Assessing Posterior-Based Mispronunciation Detection on Field-Collected Recordings from Child Speech Therapy Sessions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mirheidari21_interspeech.html": {
    "title": "Identifying Cognitive Impairment Using Sentence Representation Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yue21b_interspeech.html": {
    "title": "Parental Spoken Scaffolding and Narrative Skills in Crowd-Sourced Storytelling Samples of Young Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xia21_interspeech.html": {
    "title": "Uncertainty-Aware COVID-19 Detection from Imbalanced Sound Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21u_interspeech.html": {
    "title": "Unsupervised Domain Adaptation for Dysarthric Speech Detection via Domain Adversarial Training and Mutual Information Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bhattacharjee21_interspeech.html": {
    "title": "Source and Vocal Tract Cues for Speech-Based Classification of Patients with Parkinson's Disease and Healthy Subjects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/haulcy21_interspeech.html": {
    "title": "CLAC: A Speech Corpus of Healthy English Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nortje21_interspeech.html": {
    "title": "Direct Multimodal Few-Shot Learning of Speech and Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sanabria21_interspeech.html": {
    "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhao21b_interspeech.html": {
    "title": "A Fast Discrete Two-Step Learning Hashing for Scalable Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21v_interspeech.html": {
    "title": "Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/olaleye21_interspeech.html": {
    "title": "Attention-Based Keyword Localisation in Speech Using Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/khorrami21_interspeech.html": {
    "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21k_interspeech.html": {
    "title": "Automatic Lip-Reading with Hierarchical Pyramidal Convolution and Self-Attention for Image Sequences with No Word Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rouditchenko21b_interspeech.html": {
    "title": "Cascaded Multilingual Audio-Visual Learning from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21c_interspeech.html": {
    "title": "LiRA: Learning Visual Speech Representations from Audio Through Self-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rose21_interspeech.html": {
    "title": "End-to-End Audio-Visual Speech Recognition for Overlapping Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21e_interspeech.html": {
    "title": "Audio-Visual Multi-Talker Speech Recognition in a Cocktail Party",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21l_interspeech.html": {
    "title": "Ultra Fast Speech Separation Model with Teacher Student Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ali21_interspeech.html": {
    "title": "Group Delay Based Re-Weighted Sparse Recovery Algorithms for Robust and High-Resolution Source Separation in DOA Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21d_interspeech.html": {
    "title": "Continuous Speech Separation Using Speaker Inventory for Long Recording",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yuan21_interspeech.html": {
    "title": "Crossfire Conditional Generative Adversarial Networks for Singing Voice Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21w_interspeech.html": {
    "title": "End-to-End Speech Separation Using Orthogonal Representation in Complex and Real Time-Frequency Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nakagome21_interspeech.html": {
    "title": "Efficient and Stable Adversarial Learning Using Unpaired Data for Unsupervised Multichannel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21h_interspeech.html": {
    "title": "Stabilizing Label Assignment for Speech Separation by Self-Supervised Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21x_interspeech.html": {
    "title": "Dual-Path Filter Network: Speaker-Aware Modeling for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21f_interspeech.html": {
    "title": "Investigation of Practical Aspects of Single Channel Speech Separation for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21c_interspeech.html": {
    "title": "Implicit Filter-and-Sum Network for End-to-End Multi-Channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21i_interspeech.html": {
    "title": "Generalized Spatio-Temporal RNN Beamformer for Target Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21j_interspeech.html": {
    "title": "End-to-End Neural Diarization: From Transformer to Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jung21_interspeech.html": {
    "title": "Three-Class Overlapped Speech Detection Using a Convolutional Recurrent Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wan21_interspeech.html": {
    "title": "Online Speaker Diarization Equipped with Discriminative Modeling and Guided Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/takashima21_interspeech.html": {
    "title": "Semi-Supervised Training with Pseudo-Labeling for End-To-End Neural Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kwon21b_interspeech.html": {
    "title": "Adapting Speaker Embeddings for Speaker Diarisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21y_interspeech.html": {
    "title": "Scenario-Dependent Speaker Diarization for DIHARD-III Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bredin21_interspeech.html": {
    "title": "End-To-End Speaker Segmentation for Overlap-Aware Resegmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21d_interspeech.html": {
    "title": "Online Streaming End-to-End Neural Diarization Handling Overlapping Speech and Flexible Numbers of Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/anidjar21_interspeech.html": {
    "title": "A Thousand Words are Worth More Than One Recording:",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/futamata21_interspeech.html": {
    "title": "Phrase Break Prediction with Bidirectional Encoder Representations in Japanese Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vallesperez21_interspeech.html": {
    "title": "Improving Multi-Speaker TTS Prosody Variance with a Residual Encoder and Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/du21b_interspeech.html": {
    "title": "Rich Prosody Diversity Modelling with Phone-Level Mixture Density Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujita21_interspeech.html": {
    "title": "Phoneme Duration Modeling Using Speech Rhythm-Based Speaker Embeddings for Multi-Speaker Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zou21_interspeech.html": {
    "title": "Fine-Grained Prosody Modeling in Neural Speech Synthesis Using ToBI Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sharma21b_interspeech.html": {
    "title": "Intra-Sentential Speaking Rate Control in Neural Text-To-Speech for Automatic Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21u_interspeech.html": {
    "title": "Applying the Information Bottleneck Principle to Prosodic Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/baird21_interspeech.html": {
    "title": "A Prototypical Network Approach for Evaluating Generated Emotional Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yoshinaga21_interspeech.html": {
    "title": "A Simplified Model for the Vocal Tract of [s] with Inclined Incisors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/arai21b_interspeech.html": {
    "title": "Vocal-Tract Models to Visualize the Airstream of Human Breath and Droplets While Producing Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanji21_interspeech.html": {
    "title": "Using Transposed Convolution for Articulatory-to-Acoustic Conversion from Real-Time MRI Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/inaam21_interspeech.html": {
    "title": "Comparison Between Lumped-Mass Modeling and Flow Simulation of the Reed-Type Artificial Vocal Fold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/werner21_interspeech.html": {
    "title": "Inhalations in Speech: Acoustic and Physiological Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21j_interspeech.html": {
    "title": "Model-Based Exploration of Linking Between Vowel Articulatory Space and Acoustic Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elmers21_interspeech.html": {
    "title": "Take a Breath: Respiratory Sounds Improve Recollection in Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21m_interspeech.html": {
    "title": "Modeling Sensorimotor Adaptation in Speech Through Alterations to Forward and Inverse Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kawahara21_interspeech.html": {
    "title": "Mixture of Orthogonal Sequences Made from Extended Time-Stretched Pulses Enables Measurement of Involuntary Voice Fundamental Frequency Response to Pitch Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/you21c_interspeech.html": {
    "title": "Contextualized Attention-Based Knowledge Transfer for Spoken Conversational Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/duan21_interspeech.html": {
    "title": "Injecting Descriptive Meta-Information into Pre-Trained Language Models with Hypernetworks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rohmatillah21_interspeech.html": {
    "title": "Causal Confusion Reduction for Robust Multi-Domain Dialogue Policy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujie21_interspeech.html": {
    "title": "Timing Generating Networks: Neural Network Based Precise Turn-Taking Timing Prediction in Multiparty Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21n_interspeech.html": {
    "title": "Human-to-Human Conversation Dataset for Learning Fine-Grained Turn-Taking Action",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sundararaman21_interspeech.html": {
    "title": "PhonemeBERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21d_interspeech.html": {
    "title": "Joint Retrieval-Extraction Training for Evidence-Aware Dialog Response Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shenoy21_interspeech.html": {
    "title": "Adapting Long Context NLM for ASR Rescoring in Conversational Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21h_interspeech.html": {
    "title": "Oriental Language Recognition (OLR) 2020: Summary and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/duroselle21b_interspeech.html": {
    "title": "Language Recognition on Unknown Conditions: The LORIA-Inria-MULTISPEECH System for AP20-OLR Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kong21b_interspeech.html": {
    "title": "Dynamic Multi-Scale Convolution for Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21z_interspeech.html": {
    "title": "An End-to-End Dialect Identification System with Transfer Learning from a Multilingual Automatic Speech Recognition Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yu21b_interspeech.html": {
    "title": "Language Recognition Based on Unsupervised Pretrained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21i_interspeech.html": {
    "title": "Additive Phoneme-Aware Margin Softmax Loss for Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jahchan21_interspeech.html": {
    "title": "Towards an Accent-Robust Approach for ATC Communications Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/szoke21_interspeech.html": {
    "title": "Detecting English Speech in the Air Traffic Control Voice Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ohneiser21_interspeech.html": {
    "title": "Robust Command Recognition for Lithuanian Air Traffic Control Tower Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zuluagagomez21_interspeech.html": {
    "title": "Contextual Semi-Supervised Learning: An Approach to Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kocour21_interspeech.html": {
    "title": "Boosting of Contextual Information in ASR for Air-Traffic Call-Sign Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elie21_interspeech.html": {
    "title": "Modeling the Effect of Military Oxygen Masks on Speech Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/milde21_interspeech.html": {
    "title": "MoM: Minutes of Meeting Bot",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wilbrandt21_interspeech.html": {
    "title": "Articulatory Data Recorder: A Framework for Real-Time Articulatory Data Recording",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/codinafilba21_interspeech.html": {
    "title": "The INGENIOUS Multilingual Operations App",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rownicka21_interspeech.html": {
    "title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/geislinger21_interspeech.html": {
    "title": "Live Subtitling for BigBlueButton with Open-Source Software",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nicmanis21_interspeech.html": {
    "title": "Expressive Latvian Speech Synthesis for Dialog Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kachare21_interspeech.html": {
    "title": "ViSTAFAE: A Visual Speech-Training Aid with Feedback of Articulatory Efforts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/livescu21_interspeech.html": {
    "title": "Learning Speech Models from Multi-Modal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elhilali21_interspeech.html": {
    "title": "Adaptive Listening to Everyday Soundscapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ribeiro21b_interspeech.html": {
    "title": "Towards the Prediction of the Vocal Tract Shape from the Sequence of Phonemes to be Articulated",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/blandin21_interspeech.html": {
    "title": "Comparison of the Finite Element Method, the Multimodal Method and the Transmission-Line Model for the Computation of Vocal Tract Transfer Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wagner21b_interspeech.html": {
    "title": "Effects of Time Pressure and Spontaneity on Phonotactic Innovations in German Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/medina21_interspeech.html": {
    "title": "Importance of Parasagittal Sensor Information in Tongue Motion Capture Through a Diphonic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/georges21_interspeech.html": {
    "title": "Learning Robust Speech Representation with an Articulatory-Regularized Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/weston21_interspeech.html": {
    "title": "Changes in Glottal Source Parameter Values with Light to Moderate Physical Load",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vali21_interspeech.html": {
    "title": "End-to-End Optimized Multi-Stage Vector Quantization of Spectral Envelopes for Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nareddula21_interspeech.html": {
    "title": "Fusion-Net: Time-Frequency Information Fusion Y-Network for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/marcinek21_interspeech.html": {
    "title": "N-MTTL SI Model: Non-Intrusive Multi-Task Transfer Learning-Based Speech Intelligibility Prediction Model with Scenery Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xia21b_interspeech.html": {
    "title": "Temporal Context in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21j_interspeech.html": {
    "title": "Learning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vaaras21_interspeech.html": {
    "title": "Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qian21_interspeech.html": {
    "title": "Multimodal Sentiment Analysis with Temporal Modality Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/t21_interspeech.html": {
    "title": "Stochastic Process Regression for Cross-Cultural Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21k_interspeech.html": {
    "title": "Acted vs. Improvised: Domain Adaptation for Elicitation Approaches in Audio-Visual Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pepino21_interspeech.html": {
    "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21k_interspeech.html": {
    "title": "Graph Isomorphism Network for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumawat21_interspeech.html": {
    "title": "Applying TDNN Architectures for Analyzing Duration Dependencies on Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/keesing21_interspeech.html": {
    "title": "Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shon21_interspeech.html": {
    "title": "Leveraging Pre-Trained Language Model for Speech Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hou21b_interspeech.html": {
    "title": "Cross-Domain Speech Recognition with Unsupervised Character-Level Distribution Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kanda21_interspeech.html": {
    "title": "Large-Scale Pre-Training of End-to-End Multi-Talker ASR for Meeting Transcription with Single Distant Microphone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21b_interspeech.html": {
    "title": "On Minimum Word Error Rate Training of the Hybrid Autoregressive Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21j_interspeech.html": {
    "title": "Reducing Streaming ASR Model Delay with Self Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/diwan21b_interspeech.html": {
    "title": "Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fukuda21_interspeech.html": {
    "title": "Knowledge Distillation Based Training of Universal ASR Source Models for Cross-Lingual Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ray21_interspeech.html": {
    "title": "Listen with Intent: Improving Speech Recognition with Audio-to-Intent Front-End",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21c_interspeech.html": {
    "title": "Exploring Targeted Universal Adversarial Perturbations to End-to-End ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/delrio21_interspeech.html": {
    "title": "Earnings-21: A Practical Benchmark for ASR in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sun21c_interspeech.html": {
    "title": "Improving Multilingual Transformer Transducer Models by Reducing Language Confusions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ali21b_interspeech.html": {
    "title": "Arabic Code-Switching Speech Recognition Using Monolingual Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eisenberg21_interspeech.html": {
    "title": "Online Blind Audio Source Separation Using Recursive Expectation-Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21e_interspeech.html": {
    "title": "Empirical Analysis of Generalized Iterative Speech Separation Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/neumann21_interspeech.html": {
    "title": "Graph-PIT: Generalized Permutation Invariant Training for Continuous Separation of Arbitrary Numbers of Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21v_interspeech.html": {
    "title": "Teacher-Student MixIT for Unsupervised and Semi-Supervised Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/delcroix21_interspeech.html": {
    "title": "Few-Shot Learning of New Sound Classes for Target Sound Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21e_interspeech.html": {
    "title": "Binaural Speech Separation of Moving Speakers With Preserved Spatial Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hu21_interspeech.html": {
    "title": "AvaTr: One-Shot Speaker Extraction with Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sarkar21_interspeech.html": {
    "title": "Vocal Harmony Separation Using Time-Domain Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maciejewski21_interspeech.html": {
    "title": "Speaker Verification-Based Evaluation of Single-Channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lan21_interspeech.html": {
    "title": "Improved Speech Separation with Time-and-Frequency Cross-Domain Feature Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21c_interspeech.html": {
    "title": "Robust Speaker Extraction Network Based on Iterative Refined Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21aa_interspeech.html": {
    "title": "Neural Speaker Extraction with Speaker-Speech Cross-Attention Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rigal21_interspeech.html": {
    "title": "Deep Audio-Visual Speech Separation Based on Facial Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/singh21_interspeech.html": {
    "title": "LEAP Submission for the Third DIHARD Diarization Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21w_interspeech.html": {
    "title": "Investigation of Spatial-Acoustic Features for Overlapping Speech Detection in Multiparty Meetings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/he21c_interspeech.html": {
    "title": "Target-Speaker Voice Activity Detection with Improved i-Vector Estimation for Unknown Number of Speaker",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dawalatabad21_interspeech.html": {
    "title": "ECAPA-TDNN Embeddings for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kinoshita21_interspeech.html": {
    "title": "Advances in Integration of End-to-End Neural and Clustering-Based Diarization for Real Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ryant21_interspeech.html": {
    "title": "The Third DIHARD Diarization Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leung21_interspeech.html": {
    "title": "Robust End-to-End Speaker Diarization with Conformer and Additive Margin Penalty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/obrien21_interspeech.html": {
    "title": "Anonymous Speaker Clusters: Making Distinctions Between Anonymised Speech Recordings with Clustering Interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karra21_interspeech.html": {
    "title": "Speaker Diarization Using Two-Pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hong21_interspeech.html": {
    "title": "Federated Learning with Dynamic Transformer for Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21e_interspeech.html": {
    "title": "LiteTTS: A Lightweight Mel-Spectrogram-Free Text-to-Wave Synthesizer Based on Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tang21b_interspeech.html": {
    "title": "Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jeong21_interspeech.html": {
    "title": "Diff-TTS: A Denoising Diffusion Model for Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bae21_interspeech.html": {
    "title": "Hierarchical Context-Aware Transformers for Non-Autoregressive Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/polyak21_interspeech.html": {
    "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karanasou21_interspeech.html": {
    "title": "A Learned Conditional Prior for the VAE Acoustic Space of a TTS System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/paul21_interspeech.html": {
    "title": "A Universal Multi-Speaker Multi-Style Text-to-Speech via Disentangled Representation Learning Based on RÃ©nyi Divergence Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21g_interspeech.html": {
    "title": "Relational Data Selection for Data Augmentation of Speaker-Dependent Multi-Band MelGAN Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chung21_interspeech.html": {
    "title": "Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21g_interspeech.html": {
    "title": "Triple M: A Practical Text-to-Speech Synthesis System with Multi-Guidance Attention and Multi-Band Multi-Time LPCNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/casanova21b_interspeech.html": {
    "title": "SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker Text-To-Speech Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/palmer21_interspeech.html": {
    "title": "Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/salesky21_interspeech.html": {
    "title": "The Multilingual TEDx Corpus for Speech Recognition and Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mortensen21_interspeech.html": {
    "title": "Tusom2021: A Phonetically Transcribed Speech Dataset from an Endangered Language for Universal Phone Recognition Experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fu21b_interspeech.html": {
    "title": "AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21o_interspeech.html": {
    "title": "GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21k_interspeech.html": {
    "title": "Look Who's Talking: Active Speaker Detection in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ahmed21_interspeech.html": {
    "title": "AusKidTalk: An Auditory-Visual Corpus of 3- to 12-Year-Old Australian Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fallgren21_interspeech.html": {
    "title": "Human-in-the-Loop Efficiency Analysis for Binary Classification in Edyson",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ryumina21_interspeech.html": {
    "title": "Annotation Confidence vs. Training Sample Size: Trade-Off Solution for Partially-Continuous Categorical Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/garcesdiazmunio21_interspeech.html": {
    "title": "Europarl-ASR: A Large Corpus of Parliamentary Debates for Streaming ASR Benchmarking and Speech Data Filtering/Verbatimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kapoor21_interspeech.html": {
    "title": "Towards Automatic Speech to Sign Language Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cho21b_interspeech.html": {
    "title": "kosp2e: Korean Speech to English Translation Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21x_interspeech.html": {
    "title": "speechocean762: An Open-Source Non-Native English Speech Corpus for Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fan21b_interspeech.html": {
    "title": "An Improved Single Step Non-Autoregressive Transformer for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guo21_interspeech.html": {
    "title": "Multi-Speaker ASR Combining Non-Autoregressive Conformer CTC and Conditional Speaker Chain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ng21b_interspeech.html": {
    "title": "Pushing the Limits of Non-Autoregressive Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21l_interspeech.html": {
    "title": "Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nozaki21_interspeech.html": {
    "title": "Relaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujita21b_interspeech.html": {
    "title": "Toward Streaming ASR with Non-Autoregressive Insertion-Based Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21e_interspeech.html": {
    "title": "Layer Pruning on Demand with Intermediate CTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21l_interspeech.html": {
    "title": "Real-Time End-to-End Monaural Multi-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ba_interspeech.html": {
    "title": "Streaming End-to-End ASR Based on Blockwise Non-Autoregressive Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/beliaev21_interspeech.html": {
    "title": "TalkNet: Non-Autoregressive Depth-Wise Separable Convolutional Model for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21p_interspeech.html": {
    "title": "WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21q_interspeech.html": {
    "title": "Align-Denoise: Single-Pass Non-Autoregressive Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21d_interspeech.html": {
    "title": "VAENAR-TTS: Variational Auto-Encoder Based Non-AutoRegressive Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luz21_interspeech.html": {
    "title": "Detecting Cognitive Decline Using Speech Only: The ADReSSo Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pereztoro21_interspeech.html": {
    "title": "Influence of the Interviewer on the Automatic Assessment of Alzheimer's Disease in the Context of the ADReSSo Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21e_interspeech.html": {
    "title": "WavBERT: Exploiting Semantic and Non-Semantic Speech Using Wav2vec and BERT for Dementia Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gauder21_interspeech.html": {
    "title": "Alzheimer Disease Recognition Using Speech-Based Embeddings From Pre-Trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/balagopalan21_interspeech.html": {
    "title": "Comparing Acoustic-Based Approaches for Alzheimer's Disease Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiao21_interspeech.html": {
    "title": "Alzheimer's Disease Detection from Spontaneous Speech Through Combining Linguistic Complexity and (Dis)Fluency Features with Pretrained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21c_interspeech.html": {
    "title": "Using the Outputs of Different Automatic Speech Recognition Paradigms for Acoustic- and BERT-Based Alzheimer's Dementia Detection Through Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/syed21_interspeech.html": {
    "title": "Tackling the ADRESSO Challenge 2021: The MUET-RMIT System for Alzheimer's Dementia Recognition from Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rohanian21_interspeech.html": {
    "title": "Alzheimer's Dementia Recognition Using Acoustic, Lexical, Disfluency and Speech Pause Features Robust to Noisy Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pappagari21_interspeech.html": {
    "title": "Automatic Detection and Assessment of Alzheimer Disease Using Speech and Language Technologies in Low-Resource Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21r_interspeech.html": {
    "title": "Automatic Detection of Alzheimer's Disease Using Spontaneous Speech Only",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ca_interspeech.html": {
    "title": "Modular Multi-Modal Attention Network for Alzheimer's Disease Detection Using Patient Audio and Language Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21d_interspeech.html": {
    "title": "Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-Field Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gretter21_interspeech.html": {
    "title": "ETLT 2021: Shared Task on Automatic Speech Recognition for Non-Native Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rumberg21_interspeech.html": {
    "title": "Age-Invariant Training for End-to-End Child Speech Recognition Using Adversarial Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cornell21_interspeech.html": {
    "title": "Learning to Rank Microphones for Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gelin21_interspeech.html": {
    "title": "Simulating Reading Mistakes for Child Speech Transformer-Based Phone Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stephenson21_interspeech.html": {
    "title": "Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rijn21_interspeech.html": {
    "title": "Exploring Emotional Prototypes in a High Dimensional TTS Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mohan21_interspeech.html": {
    "title": "Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/torresquintero21_interspeech.html": {
    "title": "ADEPT: A Dataset for Evaluating Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/trang21_interspeech.html": {
    "title": "Prosodic Boundary Prediction Model for Vietnamese Text-To-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dovrat21_interspeech.html": {
    "title": "Many-Speakers Single Channel Speech Separation with Optimal Permutation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fras21_interspeech.html": {
    "title": "Combating Reverberation in NTF-Based Speech Separation Using a Sub-Source Weighted Multichannel Wiener Filter and Linear Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/strauss21_interspeech.html": {
    "title": "A Hands-On Comparison of DNNs for Dialog Separation Using Transfer Learning from Music Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/borsdorf21b_interspeech.html": {
    "title": "GlobalPhone Mix-To-Separate Out of 2: A Multilingual 2000 Speakers Mixtures Database for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tsukada21_interspeech.html": {
    "title": "Cross-Linguistic Perception of the Japanese Singleton/Geminate Contrast: Korean, Mandarin and Mongolian Compared",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/korzekwa21_interspeech.html": {
    "title": "Detection of Lexical Stress Errors in Non-Native (L2) English with Data Augmentation and Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/braun21_interspeech.html": {
    "title": "Testing Acoustic Voice Quality Classification Across Languages and Speech Styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21y_interspeech.html": {
    "title": "Acquisition of Prosodic Focus Marking by Three- to Six-Year-Old Children Learning Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mirzaei21_interspeech.html": {
    "title": "Adaptive Listening Difficulty Detection for L2 Learners Through Moderating ASR Resources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ding21b_interspeech.html": {
    "title": "F",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21h_interspeech.html": {
    "title": "A Neural Network-Based Noise Compensation Method for Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kudera21_interspeech.html": {
    "title": "Phonetic Distance and Surprisal in Multilingual Priming: Evidence from Slavic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21z_interspeech.html": {
    "title": "A Preliminary Study on Discourse Prosody Encoding in L1 and L2 English Spontaneous Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21h_interspeech.html": {
    "title": "Transformer Based End-to-End Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/graham21_interspeech.html": {
    "title": "L1 Identification from L2 Speech Using Neural Spectrogram Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oh21b_interspeech.html": {
    "title": "Leveraging Real-Time MRI for Illuminating Linguistic Velum Action",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21m_interspeech.html": {
    "title": "Segmental Alignment of English Syllables with Singleton and Cluster Onsets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hejna21_interspeech.html": {
    "title": "Exploration of Welsh English Pre-Aspiration: How Wide-Spread is it?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muhlack21_interspeech.html": {
    "title": "Revisiting Recall Effects of Filler Particles in German and English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ge21b_interspeech.html": {
    "title": "How Reliable Are Phonetic Data Collected Remotely? Comparison of Recording Devices and Environments on Acoustic Measurements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21i_interspeech.html": {
    "title": "A Cross-Dialectal Comparison of Apical Vowels in Beijing Mandarin, Northeastern Mandarin and Southwestern Mandarin: An EMA and Ultrasound Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gibson21_interspeech.html": {
    "title": "Dissecting the Aero-Acoustic Parameters of Open Articulatory Transitions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gully21_interspeech.html": {
    "title": "Quantifying Vocal Tract Shape Variation and its Acoustic Impact: A Geometric Morphometric Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guevararukoz21_interspeech.html": {
    "title": "Speech Perception and Loanword Adaptations: The Case of Copy-Vowel Epenthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guo21b_interspeech.html": {
    "title": "Speakers Coarticulate Less When Facing Real and Imagined Communicative Difficulties: An Analysis of Read and Spontaneous Speech from the LUCID Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/meister21_interspeech.html": {
    "title": "Developmental Changes of Vowel Acoustics in Adolescents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dapolito21_interspeech.html": {
    "title": "Context and Co-Text Influence on the Accuracy Production of Italian L2 Non-Native Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/heeringa21_interspeech.html": {
    "title": "A New Vowel Normalization for Sociophonetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/billington21_interspeech.html": {
    "title": "The Pacific Expansion: Optimizing Phonetic Transcription of Archival Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tian21_interspeech.html": {
    "title": "FSR: Accelerating the Inference Process of Transducer-Based Models by Applying Fast-Skip Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mitrofanov21_interspeech.html": {
    "title": "LT-LM: A Novel Non-Autoregressive Language Model for Single-Shot Lattice Rescoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/allauzen21_interspeech.html": {
    "title": "A Hybrid Seq-2-Seq ASR Design for On-Device and Server Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/inaguma21b_interspeech.html": {
    "title": "VAD-Free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yao21_interspeech.html": {
    "title": "WeNet: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanaka21b_interspeech.html": {
    "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21f_interspeech.html": {
    "title": "Deep Neural Network Calibration for E2E Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21m_interspeech.html": {
    "title": "Residual Energy-Based Models for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiu21b_interspeech.html": {
    "title": "Multi-Task Learning for End-to-End ASR Word and Utterance Confidence with Deletion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ollerenshaw21_interspeech.html": {
    "title": "Insights on Neural Representations for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/afshan21_interspeech.html": {
    "title": "Sequence-Level Confidence Classifier for ASR Utterance Accuracy and Application to Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tjandra21_interspeech.html": {
    "title": "Unsupervised Learning of Disentangled Speech Content and Style Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/choi21_interspeech.html": {
    "title": "Label Embedding for Chinese Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21aa_interspeech.html": {
    "title": "PDF: Polyphone Disambiguation in Chinese by Using FLAT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21n_interspeech.html": {
    "title": "Improving Polyphone Disambiguation for Mandarin Chinese by Combining Mix-Pooling Strategy and Window-Based Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21d_interspeech.html": {
    "title": "Polyphone Disambiguation in Mandarin Chinese with Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21s_interspeech.html": {
    "title": "A Neural-Network-Based Approach to Identifying Speakers in Novels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21f_interspeech.html": {
    "title": "UnitNet-Based Hybrid Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/novitasari21_interspeech.html": {
    "title": "Dynamically Adaptive Machine Speech Chain Inference for TTS in Noisy Environment: Listen and Speak Louder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ba_interspeech.html": {
    "title": "LinearSpeech: Parallel Text-to-Speech with Linear Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mansbach21_interspeech.html": {
    "title": "An Agent for Competing with Humans in a Deceptive Game Based on Vocal Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fakhry21_interspeech.html": {
    "title": "A Multi-Branch Deep Learning Network for Automated Detection of COVID-19",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21d_interspeech.html": {
    "title": "RW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dhamyal21_interspeech.html": {
    "title": "Fake Audio Detection in Resource-Constrained Settings Using Microfeatures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21c_interspeech.html": {
    "title": "Coughing-Based Recognition of Covid-19 with Spatial Attentive ConvLSTM Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/paul21b_interspeech.html": {
    "title": "Knowledge Distillation for Singing Voice Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/takeda21_interspeech.html": {
    "title": "Age Estimation with Speech-Age Model for Heterogeneous Speech Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/teh21_interspeech.html": {
    "title": "Open-Set Audio Classification with Limited Training Resources Based on Augmentation Enhanced Variational Auto-Encoder GAN with Detection-Classification Joint Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fukumori21_interspeech.html": {
    "title": "Deep Spectral-Cepstral Fusion for Shouted and Normal Speech Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/baghel21_interspeech.html": {
    "title": "Automatic Detection of Shouted Speech Segments in Indian News Debates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21c_interspeech.html": {
    "title": "Generalized Spoofing Detection Inspired from Audio Generation Artifacts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21t_interspeech.html": {
    "title": "Overlapped Speech Detection Based on Spectral and Spatial Feature Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/abdullah21_interspeech.html": {
    "title": "Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21d_interspeech.html": {
    "title": "Paraphrase Label Alignment for Voice Application Retrieval in Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rikhye21_interspeech.html": {
    "title": "Personalized Keyphrase Detection Using Speaker and Environment Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/garg21_interspeech.html": {
    "title": "Streaming Transformer for Hardware Efficient Voice Trigger Detection and False Trigger Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mazumder21_interspeech.html": {
    "title": "Few-Shot Keyword Spotting in Any Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21da_interspeech.html": {
    "title": "Text Anchor Based Metric Learning for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21u_interspeech.html": {
    "title": "A Meta-Learning Approach for User-Defined Spoken Term Classification with Varying Classes and Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21g_interspeech.html": {
    "title": "Auxiliary Sequence Labeling Tasks for Disfluency Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21g_interspeech.html": {
    "title": "Energy-Friendly Keyword Spotting System Using Add-Based Convolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jia21b_interspeech.html": {
    "title": "The 2020 Personalized Voice Trigger Challenge: Open Datasets, Evaluation Metrics, Baseline System and Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ea_interspeech.html": {
    "title": "Auto-KWS 2021 Challenge: Task, Datasets, and Baselines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/berg21_interspeech.html": {
    "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/awasthi21_interspeech.html": {
    "title": "Teaching Keyword Spotters to Spot New Keywords with Limited Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21fa_interspeech.html": {
    "title": "A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ca_interspeech.html": {
    "title": "An Initial Investigation for Detecting Partially Spoofed Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xie21_interspeech.html": {
    "title": "Siamese Network with wav2vec Feature for Spoofing Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheng21b_interspeech.html": {
    "title": "Cross-Database Replay Detection in Terminal-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21da_interspeech.html": {
    "title": "The Effect of Silence and Dual-Band Fusion in Anti-Spoofing System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21d_interspeech.html": {
    "title": "Pairing Weak with Strong: Twin Models for Defending Against Adversarial Attack on Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ling21_interspeech.html": {
    "title": "Attention-Based Convolutional Neural Network for ASV Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21i_interspeech.html": {
    "title": "Voting for the Right Answer: Adversarial Defense for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kinnunen21_interspeech.html": {
    "title": "Visualizing Classifier Adjacency Relations: A Case Study in Speaker Verification and Voice Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/villalba21_interspeech.html": {
    "title": "Representation Learning to Classify and Detect Adversarial Attacks Against Speaker and Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ea_interspeech.html": {
    "title": "An Empirical Study on Channel Effects for Synthetic Voice Spoofing Countermeasure Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21o_interspeech.html": {
    "title": "Channel-Wise Gated Res2Net: Towards Robust Detection of Synthetic Speech Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ge21c_interspeech.html": {
    "title": "Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peterson21_interspeech.html": {
    "title": "OpenASR20: An Open Challenge for Automatic Speech Recognition of Conversational Telephone Speech in Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/madikeri21_interspeech.html": {
    "title": "Multitask Adaptation with Lattice-Free MMI for Multi-Genre Speech Recognition of Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21f_interspeech.html": {
    "title": "An Improved Wav2Vec 2.0 Pre-Training Approach Using Enhanced Local Dependency Modeling for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21i_interspeech.html": {
    "title": "Systems for Low-Resource Speech Recognition Tasks in Open Automatic Speech Recognition and Formosa Speech Recognition Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhao21c_interspeech.html": {
    "title": "The TNT Team System Descriptions of Cantonese and Mongolian for IARPA OpenASR20",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alumae21_interspeech.html": {
    "title": "Combining Hybrid and End-to-End Approaches for the OpenASR20 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/morris21_interspeech.html": {
    "title": "One Size Does Not Fit All in Resource-Constrained ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cristia21_interspeech.html": {
    "title": "Child Language Acquisition Studied with Wearables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mikolov21_interspeech.html": {
    "title": "Language Modeling and Artificial Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gimeno21_interspeech.html": {
    "title": "Unsupervised Representation Learning for Speech Activity Detection in the Fearless Steps Challenge 2021",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vuong21_interspeech.html": {
    "title": "The Application of Learnable STRF Kernels to the 2021 Fearless Steps Phase-03 SAD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sarfjoo21_interspeech.html": {
    "title": "Speech Activity Detection Based on Multilingual Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luckenbaugh21_interspeech.html": {
    "title": "Voice Activity Detection with Teacher-Student Domain Emulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ghahabi21_interspeech.html": {
    "title": "EML Online Speech Activity Detection for the Fearless Steps Challenge Phase-III",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/opatka21_interspeech.html": {
    "title": "Device Playback Augmentation with Echo Cancellation for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yusuf21_interspeech.html": {
    "title": "End-to-End Open Vocabulary Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/merkx21_interspeech.html": {
    "title": "Semantic Sentence Similarity: Size does not Always Matter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/svec21_interspeech.html": {
    "title": "Spoken Term Detection and Relevance Score Estimation Using Dot-Product of Pronunciation Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/buet21_interspeech.html": {
    "title": "Toward Genre Adapted Closed Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/korzekwa21b_interspeech.html": {
    "title": "Weakly-Supervised Word-Level Pronunciation Error Detection in Non-Native English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kanda21b_interspeech.html": {
    "title": "End-to-End Speaker-Attributed ASR with Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/soltau21_interspeech.html": {
    "title": "Understanding Medical Conversations: Rich Transcription, Confidence Scores & Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vidal21_interspeech.html": {
    "title": "Phone-Level Pronunciation Scoring for Spanish Speakers Learning English Using a GOP-DNN System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21k_interspeech.html": {
    "title": "Explore wav2vec 2.0 for Mispronunciation Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ando21_interspeech.html": {
    "title": "Lexical Density Analysis of Word Productions in Japanese English Using Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21j_interspeech.html": {
    "title": "Deep Feature Transfer Learning for Automatic Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21fa_interspeech.html": {
    "title": "Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21e_interspeech.html": {
    "title": "A Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiao21b_interspeech.html": {
    "title": "The Impact of ASR on the Automatic Analysis of Linguistic Complexity and Sophistication in Spontaneous L2 Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanaka21c_interspeech.html": {
    "title": "End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cumbal21_interspeech.html": {
    "title": "You don't understand me!\": Comparing ASR Results for L1 and L2 Speakers of Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ga_interspeech.html": {
    "title": "NeMo Inverse Text Normalization: From Development to Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/naijo21_interspeech.html": {
    "title": "Improvement of Automatic English Pronunciation Assessment with Small Number of Utterances Using Sentence Speakability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/haider21_interspeech.html": {
    "title": "Affect Recognition Through Scalogram and Multi-Resolution Cochleagram Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21n_interspeech.html": {
    "title": "A Speech Emotion Recognition Framework for Better Discrimination of Confusions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21p_interspeech.html": {
    "title": "Speech Emotion Recognition via Multi-Level Cross-Modal Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ito21_interspeech.html": {
    "title": "Audio-Visual Speech Emotion Recognition by Disentangling Emotion and Identity Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bose21_interspeech.html": {
    "title": "Parametric Distributions to Model Numerical Emotion Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21e_interspeech.html": {
    "title": "Metric Learning Based Feature Representation with Gated Fusion Model for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cai21b_interspeech.html": {
    "title": "Speech Emotion Recognition with Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seneviratne21b_interspeech.html": {
    "title": "Generalized Dilated CNN Models for Depression Detection Using Inverted Vocal Tract Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ga_interspeech.html": {
    "title": "Learning Mutual Correlation in Multimodal Transformer for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21o_interspeech.html": {
    "title": "Time-Frequency Representation Learning with Graph Convolutional Network for Dialogue-Level Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mordido21_interspeech.html": {
    "title": "Compressing 1D Time-Channel Separable Convolutions Using Sparse Random Ternary Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheng21c_interspeech.html": {
    "title": "Weakly Supervised Construction of ASR Systems from Massive Video Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21l_interspeech.html": {
    "title": "Broadcasted Residual Learning for Efficient Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/swaminathan21_interspeech.html": {
    "title": "CoDERT: Distilling Encoder Representations with Co-Learning for Transducer-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21f_interspeech.html": {
    "title": "Extremely Low Footprint End-to-End ASR System for Smart Device",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shangguan21_interspeech.html": {
    "title": "Dissecting User-Perceived Latency of On-Device E2E Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/macoskey21b_interspeech.html": {
    "title": "Amortized Neural Networks for Low-Latency Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/botros21_interspeech.html": {
    "title": "Tied & Reduced RNN-T Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21m_interspeech.html": {
    "title": "PQK: Model Compression via Pruning, Quantization, and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nagaraja21_interspeech.html": {
    "title": "Collaborative Training of Acoustic Encoders for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ha_interspeech.html": {
    "title": "Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/parcollet21_interspeech.html": {
    "title": "The Energy and Carbon Footprint of Training End-to-End Speech Recognizers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21v_interspeech.html": {
    "title": "Graph-Based Label Propagation for Semi-Supervised Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21q_interspeech.html": {
    "title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cumani21_interspeech.html": {
    "title": "A Generative Model for Duration-Dependent Score Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pelecanos21_interspeech.html": {
    "title": "Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kataria21b_interspeech.html": {
    "title": "Multi-Channel Speaker Verification for Single and Multi-Talker Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/padfield21_interspeech.html": {
    "title": "Chronological Self-Training for Real-Time Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xiao21b_interspeech.html": {
    "title": "Adaptive Margin Circle Loss for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/obrien21b_interspeech.html": {
    "title": "Presentation Matters: Evaluating Speaker Identification Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tong21_interspeech.html": {
    "title": "Automatic Error Correction for Speaker Embedding Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liao21_interspeech.html": {
    "title": "An Integrated Framework for Two-Pass Personalized Voice Trigger",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lian21_interspeech.html": {
    "title": "Masked Proxy Loss for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21h_interspeech.html": {
    "title": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21p_interspeech.html": {
    "title": "Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sivaprasad21_interspeech.html": {
    "title": "Emotional Prosody Control for Speech Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cong21b_interspeech.html": {
    "title": "Controllable Context-Aware Conversational Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21n_interspeech.html": {
    "title": "Expressive Text-to-Speech Using Style Tag",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21d_interspeech.html": {
    "title": "Adaptive Text to Speech for Spontaneous Style",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21r_interspeech.html": {
    "title": "Towards Multi-Scale Style Control for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21d_interspeech.html": {
    "title": "Cross-Speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tan21_interspeech.html": {
    "title": "Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/an21b_interspeech.html": {
    "title": "Improving Performance of Seen and Unseen Speech Style Transfer in End-to-End Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shechtman21_interspeech.html": {
    "title": "Synthesis of Expressive Speaking Styles with Limited Training Data in a Multi-Speaker, Prosody-Controllable Sequence-to-Sequence Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dao21_interspeech.html": {
    "title": "Intent Detection and Slot Filling for Vietnamese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21k_interspeech.html": {
    "title": "Augmenting Slot Values and Contexts for Spoken Language Understanding with Pretrained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gaspers21_interspeech.html": {
    "title": "The Impact of Intent Distribution Mismatch on Semi-Supervised Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jiang21c_interspeech.html": {
    "title": "Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ia_interspeech.html": {
    "title": "Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-Trained DNN-HMM-Based Acoustic-Phonetic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cha21_interspeech.html": {
    "title": "Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ha_interspeech.html": {
    "title": "End-to-End Cross-Lingual Spoken Language Understanding Model with Multilingual Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saghir21_interspeech.html": {
    "title": "Factorization-Aware Training of Transformers for Natural Language Understanding on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saxon21_interspeech.html": {
    "title": "End-to-End Spoken Language Understanding for Generalized Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21f_interspeech.html": {
    "title": "Bi-Directional Joint Neural Networks for Intent Classification and Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cutler21_interspeech.html": {
    "title": "INTERSPEECH 2021 Acoustic Echo Cancellation Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pfeifenberger21_interspeech.html": {
    "title": "Acoustic Echo Cancellation with Cross-Domain Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ia_interspeech.html": {
    "title": "F-T-LSTM Based Complex Network for Joint Acoustic Echo Cancellation and Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seidel21_interspeech.html": {
    "title": "Y",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21f_interspeech.html": {
    "title": "Acoustic Echo Cancellation Using Deep Complex Neural Network with Nonlinear Magnitude Compression and Phase Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ivry21_interspeech.html": {
    "title": "Nonlinear Acoustic Echo Cancellation with Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/green21_interspeech.html": {
    "title": "Automatic Speech Recognition of Disordered Speech: Personalized Models Outperforming Human Listeners on Short Phrases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/neumann21b_interspeech.html": {
    "title": "Investigating the Utility of Multimodal Conversational Technology and Audiovisual Analytic Measures for the Assessment and Monitoring of Amyotrophic Lateral Sclerosis at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hermann21_interspeech.html": {
    "title": "Handling Acoustic Variation in Dysarthric Speech Recognition Systems Through Model Combination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/geng21b_interspeech.html": {
    "title": "Spectro-Temporal Deep Features for Disordered Speech Assessment and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gutz21_interspeech.html": {
    "title": "Speaking with a KN95 Face Mask: ASR Performance and Speaker Compensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jin21_interspeech.html": {
    "title": "Adversarial Data Augmentation for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xie21b_interspeech.html": {
    "title": "Variational Auto-Encoder Based Variability Encoding for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ja_interspeech.html": {
    "title": "Learning Explicit Prosody Models and Deep Speaker Embeddings for Atypical Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21d_interspeech.html": {
    "title": "Bayesian Parametric and Architectural Domain Adaptation of LF-MMI Trained TDNNs for Elderly and Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cai21c_interspeech.html": {
    "title": "A Voice-Activated Switch for Persons with Motor and Speech Impairments: Isolated-Vowel Spotting Using Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21w_interspeech.html": {
    "title": "Conformer Parrotron: A Faster and Stronger End-to-End Speech Conversion and Recognition Model for Atypical Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/macdonald21_interspeech.html": {
    "title": "Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project Euphonia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yeo21_interspeech.html": {
    "title": "Automatic Severity Classification of Korean Dysarthric Speech Using Phoneme-Level Pronunciation Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/venugopalan21_interspeech.html": {
    "title": "Comparing Supervised Models and Learned Speech Representations for Classifying Intelligibility of Disordered Speech on Selected Phrases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mitra21_interspeech.html": {
    "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kawahara21b_interspeech.html": {
    "title": "Interactive and Real-Time Acoustic Measurement Tools for Speech Data Acquisition and Presentation: Application of an Extended Member of Time Stretched Pulses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tihelka21_interspeech.html": {
    "title": "Save Your Voice: Voice Banking and TTS for Anyone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ja_interspeech.html": {
    "title": "NeMo (Inverse) Text Normalization: From Development to Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hembise21_interspeech.html": {
    "title": "Lalilo: A Reading Assistant for Children Featuring Speech Recognition-Based Reading Mistake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21f_interspeech.html": {
    "title": "Automatic Radiology Report Editing Through Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21e_interspeech.html": {
    "title": "WittyKiddy: Multilingual Spoken Language Learning for Kids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jin21b_interspeech.html": {
    "title": "Duplex Conversation in Outbound Agent System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2021/udupa21b_interspeech.html": {
    "title": "Web Interface for Estimating Articulatory Movements in Speech Production from Acoustics and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}