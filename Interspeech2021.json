{
  "https://www.isca-speech.org/archive/interspeech_2021/pucher21_interspeech.html": {
    "title": "Conversion of Airborne to Bone-Conducted Speech with Deep Neural Networks",
    "volume": "main",
    "abstract": "It is a common experience of most speakers that the playback of one's own voice sounds strange. This can be mainly attributed to the missing bone-conducted speech signal that is not present in the playback signal. It was also shown that some phonemes have a high bone-conducted relative to air-conducted sound transmission, which means that the bone-conduction filter is phone-dependent. To achieve such a phone-dependent modeling we train different speaker dependent and speaker adaptive speech conversion systems using airborne and bone-conducted speech data from 8 speakers (5 male, 3 female), which allow for the conversion of airborne speech to bone-conducted speech. The systems are based on Long Short-Term Memory (LSTM) deep neural networks, where the speaker adaptive versions with speaker embedding can be used without bone-conduction signals from the target speaker. Additionally we also used models that apply a global filtering. The different models are then evaluated by an objective error metric and a subjective listening experiment, which show that the LSTM based models outperform the global filters",
    "keywords": [],
    "checked": true,
    "id": "fa04146b59a5a17c00b28c223b415b4df45f0991",
    "semantic_title": "conversion of airborne to bone-conducted speech with deep neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rezackova21_interspeech.html": {
    "title": "T5G2P: Using Text-to-Text Transfer Transformer for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Despite the increasing popularity of end-to-end text-to-speech (TTS) systems, the correct grapheme-to-phoneme (G2P) module is still a crucial part of those relying on a phonetic input. In this paper, we, therefore, introduce a T5G2P model, a Text-to-Text Transfer Transformer (T5) neural network model which is able to convert an input text sentence into a phoneme sequence with a high accuracy. The evaluation of our trained T5 model is carried out on English and Czech, since there are different specific properties of G2P, including homograph disambiguation, cross-word assimilation and irregular pronunciation of loanwords. The paper also contains an analysis of a homographs issue in English and offers another approach to Czech phonetic transcription using the detection of pronunciation exceptions",
    "keywords": [],
    "checked": true,
    "id": "8ca9702be6bb1a9af323b778ad96894db284313e",
    "semantic_title": "t5g2p: using text-to-text transfer transformer for grapheme-to-phoneme conversion",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/perrotin21_interspeech.html": {
    "title": "Evaluating the Extrapolation Capabilities of Neural Vocoders to Extreme Pitch Values",
    "volume": "main",
    "abstract": "Neural vocoders are systematically evaluated on homogeneous train and test databases. This kind of evaluation is efficient to compare neural vocoders in their \"comfort zone\", yet it hardly reveals their limits towards unseen data during training. To compare their extrapolation capabilities, we introduce a methodology that aims at quantifying the robustness of neural vocoders in synthesising unseen data, by precisely controlling the ranges of seen/unseen data in the training database. By focusing in this study on the pitch (F ) parameter, our methodology involves a careful splitting of a dataset to control which F values are seen/unseen during training, followed by both global (utterance) and local (frame) evaluation of vocoders. Comparison of four types of vocoders (autoregressive, sourcefilter, flows, GAN) displays a wide range of behaviour towards unseen input pitch values, including excellent extrapolation (WaveGlow); widely-spread F errors (WaveRNN); and systematic generation of the training set median F (LPCNet, Parallel WaveGAN). In contrast, fewer differences between vocoders were observed when using homogeneous train and test sets, thus demonstrating the potential and need for such evaluation to better discriminate the neural vocoders abilities to generate out-of-training-range data",
    "keywords": [],
    "checked": true,
    "id": "2a1d555165bbde06837b0b8eca013f59f54c7ed7",
    "semantic_title": "evaluating the extrapolation capabilities of neural vocoders to extreme pitch values",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/do21_interspeech.html": {
    "title": "A Systematic Review and Analysis of Multilingual Data Strategies in Text-to-Speech for Low-Resource Languages",
    "volume": "main",
    "abstract": "We provide a systematic review of past studies that use multilingual data for text-to-speech (TTS) of low-resource languages (LRLs). We focus on the strategies used by these studies for incorporating multilingual data and how they affect output speech quality. To investigate the difference in output quality between corresponding monolingual and multilingual models, we propose a novel measure to compare this difference across the included studies and their various evaluation metrics. This measure, called the Multilingual Model Effect (MLME), is found to be affected by: acoustic model architecture, the difference ratio of target language data between corresponding multilingual and monolingual experiments, the balance ratio of target language data to total data, and the amount of target language data used. These findings can act as reference for data strategies in future experiments with multilingual TTS models for LRLs. Language family classification, despite being widely used, is not found to be an effective criterion for selecting source languages",
    "keywords": [],
    "checked": true,
    "id": "016064c068dfe3dd3ae8b2a8c8940c2a4e2db866",
    "semantic_title": "a systematic review and analysis of multilingual data strategies in text-to-speech for low-resource languages",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/talkar21_interspeech.html": {
    "title": "Acoustic Indicators of Speech Motor Coordination in Adults With and Without Traumatic Brain Injury",
    "volume": "main",
    "abstract": "A traumatic brain injury (TBI) can lead to various long-term effects on memory, attention, and mood, as well as the occurrence of headaches, speech, and hearing problems. There is a need to better understand the long-term effects of a TBI for objective tracking of an individual's recovery, which could be used to determine intervention trajectories. This study utilizes acoustic features derived from recordings of speech tasks completed by active-duty service members and veterans (SMVs) enrolled in the Defense and Veterans Brain Injury (DVBIC)/Traumatic Brain Injury Center of Excellence (TBICoE) 15-Year Longitudinal TBI Study. We hypothesize that the individuals diagnosed with moderate to severe TBI would demonstrate motor speech impairments through decreased coordination of the speech production subsystems as compared to individuals with no history of TBI. Speech motor coordination is measured through correlations of acoustic feature time series representing speech subsystems. Eigenspectra derived from these correlations are utilized in machine learning models to discriminate between the two groups. The fusion of correlation features derived from the recordings achieves an AUC of 0.78. This suggests that residual motor impairments from moderate to severe TBI could be detectable through objective measures of speech motor coordination",
    "keywords": [],
    "checked": true,
    "id": "29a7bc914934229c5c190d4e7e5cb2b17aa9cbc8",
    "semantic_title": "acoustic indicators of speech motor coordination in adults with and without traumatic brain injury",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vasquezcorrea21_interspeech.html": {
    "title": "On Modeling Glottal Source Information for Phonation Assessment in Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's disease produces several motor symptoms, including different speech impairments that are known as hypokinetic dysarthria. Symptoms associated to dysarthria affect different dimensions of speech such as phonation, articulation, prosody, and intelligibility. Studies in the literature have mainly focused on the analysis of articulation and prosody because they seem to be the most prominent symptoms associated to dysarthria severity. However, phonation impairments also play a significant role to evaluate the global speech severity of Parkinson's patients. This paper proposes an extensive comparison of different methods to automatically evaluate the severity of specific phonation impairments in Parkinson's patients. The considered models include the computation of perturbation and glottal-based features, in addition to features extracted from a zero frequency filtered signals. We consider as well end-to-end models based on 1D CNNs, which are trained to learn features from the raw speech waveform, reconstructed glottal signals, and zero-frequency filtered signals. The results indicate that it is possible to automatically classify between speakers with low versus high phonation severity due to the presence of dysarthria and at the same time to evaluate the severity of the phonation impairments on a continuous scale, posed as a regression problem",
    "keywords": [],
    "checked": true,
    "id": "7dfa3283092c0c780cff9228153196b2cb84d7d6",
    "semantic_title": "on modeling glottal source information for phonation assessment in parkinson's disease",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/daoudi21_interspeech.html": {
    "title": "Distortion of Voiced Obstruents for Differential Diagnosis Between Parkinson's Disease and Multiple System Atrophy",
    "volume": "main",
    "abstract": "Parkinson's disease (PD) and the parkinsonian variant of Multiple System Atrophy (MSA-P) are two neurodegenerative diseases which share similar clinical features, particularly in early disease stages. The differential diagnosis can be thus very challenging. Dysarthria is known to be a frequent and early clinical feature of PD and MSA. It can be thus used as a vehicle to provide a vocal biomarker which could help in the differential diagnosis. In particular, distortion of consonants is known to be a frequent impairment in these diseases. The aim of this study is to investigate distinctive patterns in the distortion of voiced obstruents (plosives and fricatives). It is the first study which attempts to examine such distortions in the French language for the purpose of the differential diagnosis between PD and MSA-P (and among the very few studies if we consider all languages). We carry out a perceptual and objective analysis of voiced obstruents extracted from isolated pseudo-words initials. We first show that devoicing is a significant impairment which predominates in MSA-P. We then show that voice onset time (VOT) of voiced plosives (prevoicing duration) can be a complementary feature to improve the accuracy in discrimination between PD and MSA-P",
    "keywords": [],
    "checked": true,
    "id": "07b9da3899e66c7dbd79de23a294811f8b1c34dc",
    "semantic_title": "distortion of voiced obstruents for differential diagnosis between parkinson's disease and multiple system atrophy",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21_interspeech.html": {
    "title": "A Study into Pre-Training Strategies for Spoken Language Understanding on Dysarthric Speech",
    "volume": "main",
    "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems avoid an intermediate textual representation by mapping speech directly into intents with slot values. This approach requires considerable domain-specific training data. In low-resource scenarios this is a major concern, e.g., in the present study dealing with SLU for dysarthric speech. Pretraining part of the SLU model for automatic speech recognition targets helps but no research has shown to which extent SLU on dysarthric speech benefits from knowledge transferred from other dysarthric speech tasks. This paper investigates the efficiency of pre-training strategies for SLU tasks on dysarthric speech. The designed SLU system consists of a TDNN acoustic model for feature encoding and a capsule network for intent and slot decoding. The acoustic model is pre-trained in two stages: initialization with a corpus of normal speech and finetuning on a mixture of dysarthric and normal speech. By introducing the intelligibility score as a metric of the impairment severity, this paper quantitatively analyzes the relation between generalization and pathology severity for dysarthric speech",
    "keywords": [],
    "checked": true,
    "id": "c19084b61d05c32320a6caaa739eb68c4eb9d07a",
    "semantic_title": "a study into pre-training strategies for spoken language understanding on dysarthric speech",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/turrisi21_interspeech.html": {
    "title": "EasyCall Corpus: A Dysarthric Speech Dataset",
    "volume": "main",
    "abstract": "This paper introduces a new dysarthric speech command dataset in Italian, called EasyCall corpus. The dataset consists of 21386 audio recordings from 24 healthy and 31 dysarthric speakers, whose individual degree of speech impairment was assessed by neurologists through the Therapy Outcome Measure. The corpus aims at providing a resource for the development of ASR-based assistive technologies for patients with dysarthria. In particular, it may be exploited to develop a voice-controlled contact application for commercial smartphones, aiming at improving dysarthric patients' ability to communicate with their family and caregivers. Before recording the dataset, participants were administered a survey to evaluate which commands are more likely to be employed by dysarthric individuals in a voice-controlled contact application. In addition, the dataset includes a list of non-commands (i.e., words near/inside commands or phonetically close to commands) that can be leveraged to build a more robust command recognition system. At present commercial ASR systems perform poorly on the EasyCall Corpus as we report in this paper. This result corroborates the need for dysarthric speech corpora for developing effective assistive technologies. To the best of our knowledge, this database represents the richest corpus of dysarthric speech to date",
    "keywords": [],
    "checked": true,
    "id": "6524f8f7692c43e000bea2f68677a0b3dae69015",
    "semantic_title": "easycall corpus: a dysarthric speech dataset",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bie21_interspeech.html": {
    "title": "A Benchmark of Dynamical Variational Autoencoders Applied to Speech Spectrogram Modeling",
    "volume": "main",
    "abstract": "The Variational Autoencoder (VAE) is a powerful deep generative model that is now extensively used to represent high-dimensional complex data via a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, input data vectors are processed independently. In recent years, a series of papers have presented different extensions of the VAE to process sequential data, that not only model the latent space, but also model the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks. We recently performed a comprehensive review of those models and unified them into a general class called Dynamical Variational Autoencoders (DVAEs). In the present paper, we present the results of an experimental benchmark comparing six of those DVAE models on the speech analysis-resynthesis task, as an illustration of the high potential of DVAEs for speech modeling",
    "keywords": [],
    "checked": true,
    "id": "5ace2ad34b19f668232a45806429ab0bfbd2c387",
    "semantic_title": "a benchmark of dynamical variational autoencoders applied to speech spectrogram modeling",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yurt21_interspeech.html": {
    "title": "Fricative Phoneme Detection Using Deep Neural Networks and its Comparison to Traditional Methods",
    "volume": "main",
    "abstract": "Accurate phoneme detection and processing can enhance speech intelligibility in hearing aids and audio & speech codecs. As fricative phonemes have an important part of their energy concentrated in high frequency bands, frequency lowering algorithms are used in hearing aids to improve fricative intelligibility for people with high-frequency hearing loss. In traditional audio codecs, while processing speech in blocks, spectral smearing around fricative phoneme borders results in pre and post echo artifacts. Hence, detecting the fricative borders and adapting the processing accordingly could enhance the quality of speech. Until recently, phoneme detection and analysis were mostly done by extracting features specific to the class of phonemes. In this paper, we present a deep learning based fricative phoneme detection algorithm that exceeds the state-of-the-art fricative phoneme detection accuracy on the TIMIT speech corpus. Moreover, we compare our method to other approaches that employ classical signal processing for fricative detection and also evaluate it on the TIMIT files coded with AAC codec followed by bandwidth limitation. Reported results of our deep learning approach on original TIMIT files are reproducible and come with an easy to use code that could serve as a baseline for any future research on this topic",
    "keywords": [],
    "checked": true,
    "id": "e8bd30368ce8d8103b4404bbc5bb1f619743c541",
    "semantic_title": "fricative phoneme detection using deep neural networks and its comparison to traditional methods",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/prasad21_interspeech.html": {
    "title": "Identification of F1 and F2 in Speech Using Modified Zero Frequency Filtering",
    "volume": "main",
    "abstract": "Formants are major resonances in the vocal tract system. Identification of formants is important for study of speech. In the literature, formants are typically identified by first deriving formant frequency candidates (e.g., using linear prediction) and then applying a tracking mechanism. In this paper, we propose a simple tracking-free formant identification approach based on zero frequency filtering. More precisely, formants F1-F2 are identified by modifying the trend removal operation in zero frequency filtering and picking simply the dominant peak in the short-term discrete Fourier transform spectra. We demonstrate the potential of the approach by comparing it against state-of-the-art formant identification approaches on a typical speech data set (TIMIT-VTR) and an atypical speech data set (PC-GITA)",
    "keywords": [],
    "checked": true,
    "id": "418936459718355b4c59a3b0c7698882428639c3",
    "semantic_title": "identification of f1 and f2 in speech using modified zero frequency filtering",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/teytaut21_interspeech.html": {
    "title": "Phoneme-to-Audio Alignment with Recurrent Neural Networks for Speaking and Singing Voice",
    "volume": "main",
    "abstract": "Phoneme-to-audio alignment is the task of synchronizing voice recordings and their related phonetic transcripts. In this work, we introduce a new system to forced phonetic alignment with Recurrent Neural Networks (RNN). With the Connectionist Temporal Classification (CTC) loss as training objective, and an additional reconstruction cost, we learn to infer relevant per-frame phoneme probabilities from which alignment is derived. The core of the neural architecture is a context-aware attention mechanism between mel-spectrograms and side information. We investigate two contexts given by either phoneme sequences (model PhAtt) or spectrograms themselves (model SpAtt). Evaluations show that these models produce precise alignments for both speaking and singing voice. Best results are obtained with the model PhAtt, which outperforms baseline reference with an average imprecision of 16.3ms and 29.8ms on speech and singing, respectively. The model SpAtt also appears as an interesting alternative, capable of aligning longer audio files without requiring phoneme sequences on small audio segments",
    "keywords": [],
    "checked": true,
    "id": "bfba22e6b5774780e18fb348db365df211280f2a",
    "semantic_title": "phoneme-to-audio alignment with recurrent neural networks for speaking and singing voice",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21_interspeech.html": {
    "title": "Adaptive Convolutional Neural Network for Text-Independent Speaker Recognition",
    "volume": "main",
    "abstract": "In text-independent speaker recognition, each speech is composed of different phonemes depending on spoken text. The conventional neural networks for speaker recognition are static models, so they do not reflect this phoneme-varying characteristic well. To tackle this limitation, we propose an adaptive convolutional neural network (ACNN) for text-independent speaker recognition. The utterance is divided along the time axis into short segments with small fluctuating phonemes. Frame-level features are extracted by applying input-dependent kernels adaptive to each segment. By applying time average pooling and linear layers, utterance-level embeddings extraction and speaker recognition are performed. Adaptive VGG-M using 0.356 seconds segmentation shows better speaker recognition performance than baseline models, with a Top-1 of 86.51% and an EER of 5.68%. It extracts more accurate frame-level embeddings for vowel and nasal phonemes compared to the conventional method without overfitting and large parameters. This framework for text-independent speaker recognition effectively utilizes phonemes and text-varying characteristic of speech",
    "keywords": [],
    "checked": true,
    "id": "4c653bd510d5c1bbf2e5690e7075c1b5b6ce104f",
    "semantic_title": "adaptive convolutional neural network for text-independent speaker recognition",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qi21_interspeech.html": {
    "title": "Bidirectional Multiscale Feature Aggregation for Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we propose a novel bidirectional multiscale feature aggregation (BMFA) network with attentional fusion modules for text-independent speaker verification. The feature maps from different stages of the backbone network are iteratively combined and refined in both a bottom-up and top-down manner. Furthermore, instead of simple concatenation or elementwise addition of feature maps from different stages, an attentional fusion module is designed to compute the fusion weights. Experiments are conducted on the NIST SRE16 and VoxCeleb1 datasets. The experimental results demonstrate the effectiveness of the bidirectional aggregation strategy and show that the proposed attentional fusion module can further improve the performance",
    "keywords": [],
    "checked": true,
    "id": "588856b77b4e8709746dda0db3ed5e626bf87aba",
    "semantic_title": "bidirectional multiscale feature aggregation for speaker verification",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21_interspeech.html": {
    "title": "Improving Time Delay Neural Network Based Speaker Recognition with Convolutional Block and Feature Aggregation Methods",
    "volume": "main",
    "abstract": "In this paper, we develop a system that integrates multiple ideas and techniques inspired by the convolutional block and feature aggregation methods. We begin with the state-of-the-art speaker-embedding model for speaker recognition, namely the model of Emphasized Channel Attention, Propagation, and Aggregation in Time Delay Neural Network, and then gradually experiment with the proposed network modules, including bottleneck residual blocks, attention mechanisms, and feature aggregation methods. In our final model, we replace the Res2Block with SC-Block and we use a hierarchical architecture for feature aggregation. We evaluate the performance of our model on the VoxCeleb1 test set and the 2020 VoxCeleb Speaker Recognition Challenge (VoxSRC20) validation set. The relative improvement of the proposed models over ECAPA-TDNN is 22.8% on VoxCeleb1 and 18.2% on VoxSRC20",
    "keywords": [],
    "checked": true,
    "id": "1b4de223d5393bb2abac0ecb6e46de0a020ad18e",
    "semantic_title": "improving time delay neural network based speaker recognition with convolutional block and feature aggregation methods",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21_interspeech.html": {
    "title": "Improving Deep CNN Architectures with Variable-Length Training Samples for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Deep Convolutional Neural Network (CNN) based speaker embeddings, such as r-vectors, have shown great success in text-independent speaker verification (TI-SV) task. However, previous deep CNN models usually use fixed-length samples for training and employ variable-length utterances for speaker embeddings, which generates a mismatch between training and embedding. To address this issue, we investigate the effect of employing variable-length training samples on CNN-based TI-SV systems and explore two approaches to improve the performance of deep CNN architectures on TI-SV through capturing variable-term contexts. Firstly, we present an improved selective kernel convolution which allows the networks to adaptively switch between short-term and long-term contexts based on variable-length utterances. Secondly, we propose a multi-scale statistics pooling method to aggregate multiple time-scale features from different layers of the networks. We build a novel ResNet34 based architecture with two proposed approaches. Experiments are conducted on the VoxCeleb datasets. The results demonstrate that the effect of using variable-length samples is diverse in different networks and the architecture with two proposed approaches achieves significant improvement over r-vectors baseline system",
    "keywords": [],
    "checked": true,
    "id": "899172bf6b54aeb4359fcf6c18d50b63358f34b6",
    "semantic_title": "improving deep cnn architectures with variable-length training samples for text-independent speaker verification",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21_interspeech.html": {
    "title": "Binary Neural Network for Speaker Verification",
    "volume": "main",
    "abstract": "Although deep neural networks are successful for many tasks in the speech domain, the high computational and memory costs of deep neural networks make it difficult to directly deploy high-performance Neural Network systems on low-resource embedded devices. There are several mechanisms to reduce the size of the neural networks i.e. parameter pruning, parameter quantization, etc. This paper focuses on how to apply binary neural networks to the task of speaker verification. The proposed binarization of training parameters can largely maintain the performance while significantly reducing storage space requirements and computational costs. Experiment results show that, after binarizing the Convolutional Neural Network, the ResNet34-based network achieves an EER of around 5% on the Voxceleb1 testing dataset and even outperforms the traditional real number network on the text-dependent dataset: Xiaole while having a 32× memory saving",
    "keywords": [],
    "checked": true,
    "id": "08b1a85ca48ca7f723b0b5e331c89561158d7b2e",
    "semantic_title": "binary neural network for speaker verification",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tu21_interspeech.html": {
    "title": "Mutual Information Enhanced Training for Speaker Embedding",
    "volume": "main",
    "abstract": "Mutual information (MI) is useful in unsupervised and self-supervised learning. Maximizing the MI between the low-level features and the learned embeddings can preserve meaningful information in the embeddings, which can contribute to performance gains. This strategy is called deep InfoMax (DIM) in representation learning. In this paper, we follow the DIM framework so that the speaker embeddings can capture more information from the frame-level features. However, a straightforward implementation of DIM may pose a dimensionality imbalance problem because the dimensionality of the frame-level features is much larger than that of the speaker embeddings. This problem can lead to unreliable MI estimation and can even cause detrimental effects on speaker verification. To overcome this problem, we propose to squeeze the frame-level features before MI estimation through some global pooling methods. We call the proposed method squeeze-DIM. Although the squeeze operation inevitably introduces some information loss, we empirically show that the squeeze-DIM can achieve performance gains on both Voxceleb1 and VOiCES-19 tasks. This suggests that the squeeze operation facilitates the MI estimation and maximization in a balanced dimensional space, which helps learn more informative speaker embeddings",
    "keywords": [],
    "checked": true,
    "id": "289c4486a6f54644b2c6285d6640bb30ba71fbbe",
    "semantic_title": "mutual information enhanced training for speaker embedding",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21b_interspeech.html": {
    "title": "Y-Vector: Multiscale Waveform Encoder for Speaker Embedding",
    "volume": "main",
    "abstract": "State-of-the-art text-independent speaker verification systems typically use cepstral features or filter bank energies as speech features. Recent studies attempted to extract speaker embeddings directly from raw waveforms and have shown competitive results. In this paper, we propose a novel multi-scale waveform encoder that uses three convolution branches with different time scales to compute speech features from the waveform. These features are then processed by squeeze-and-excitation blocks, a multi-level feature aggregator, and a time delayed neural network (TDNN) to compute speaker embedding. We show that the proposed embeddings outperform existing raw-waveform-based speaker embeddings on speaker verification by a large margin. A further analysis of the learned filters shows that the multi-scale encoder attends to different frequency bands at its different scales while resulting in a more flat overall frequency response than any of the single-scale counterparts",
    "keywords": [],
    "checked": true,
    "id": "165c4715fdf6a7a0229f2810f2d32a526444b216",
    "semantic_title": "y-vector: multiscale waveform encoder for speaker embedding",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21_interspeech.html": {
    "title": "Phoneme-Aware and Channel-Wise Attentive Learning for Text Dependent Speaker Verification",
    "volume": "main",
    "abstract": "This paper proposes a multi-task learning network with phoneme-aware and channel-wise attentive learning strategies for text-dependent Speaker Verification (SV). In the proposed structure, the frame-level multi-task learning along with the segment-level adversarial learning is adopted for speaker embedding extraction. The phoneme-aware attentive pooling is exploited on frame-level features in the main network for speaker classifier, with the corresponding posterior probability for the phoneme distribution in the auxiliary subnet. Further, the introduction of Squeeze and Excitation (SE-block) performs dynamic channel-wise feature recalibration, which improves the representational ability. The proposed method exploits speaker idiosyncrasies associated with pass-phrases, and is further improved by the phoneme-aware attentive pooling and SE-block from temporal and channel-wise aspects, respectively. The experiments conducted on RSR2015 Part 1 database confirm that the proposed system achieves outstanding results for text-dependent SV",
    "keywords": [],
    "checked": true,
    "id": "923a03112032a81c77ed3e8f4498e60225263442",
    "semantic_title": "phoneme-aware and channel-wise attentive learning for text dependentspeaker verification",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21c_interspeech.html": {
    "title": "Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding",
    "volume": "main",
    "abstract": "This paper proposes a serialized multi-layer multi-head attention for neural speaker embedding in text-independent speaker verification. In prior works, frame-level features from one layer are aggregated to form an utterance-level representation. Inspired by the Transformer network, our proposed method utilizes the hierarchical architecture of stacked self-attention mechanisms to derive refined features that are more correlated with speakers. Serialized attention mechanism contains a stack of self-attention modules to create fixed-dimensional representations of speakers. Instead of utilizing multi-head attention in parallel, the proposed serialized multi-layer multi-head attention is designed to aggregate and propagate attentive statistics from one layer to the next in a serialized manner. In addition, we employ an input-aware query for each utterance with the statistics pooling. With more layers stacked, the neural network can learn more discriminative speaker embeddings. Experiment results on VoxCeleb1 dataset and SITW dataset show that our proposed method outperforms other baseline methods, including x-vectors and other x-vectors + conventional attentive pooling approaches by 9.7% in EER and 8.1% in DCF10",
    "keywords": [],
    "checked": true,
    "id": "c3241cacfbb0ef38b87444634652a9202ac3da4e",
    "semantic_title": "serialized multi-layer multi-head attention for neural speaker embedding",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21_interspeech.html": {
    "title": "TacoLPCNet: Fast and Stable TTS by Conditioning LPCNet on Mel Spectrogram Predictions",
    "volume": "main",
    "abstract": "The combination of the recently proposed LPCNet vocoder and a seq-to-seq acoustic model, i.e., Tacotron, has successfully achieved lightweight speech synthesis systems. However, the quality of synthesized speech is often unstable because the precision of the pitch parameters predicted by acoustic models is insufficient, especially for some tonal languages like Chinese and Japanese. In this paper, we propose an end-to-end speech synthesis system, TacoLPCNet, by conditioning LPCNet on Mel spectrogram predictions. First, we extend LPCNet for the Mel spectrogram instead of using explicit pitch information and pitch-related network. Furthermore, we optimize the system by model pruning, multi-frame inference, and increasing frame length, to enable it to meet the conditions required for real-time applications. The objective and subjective evaluation results for various languages show that the proposed system is more stable for tonal languages within the proposed optimization strategies. The experimental results also verify that our model improves synthesis runtime by 3.12 times than that of the baseline on a standard CPU while maintaining naturalness",
    "keywords": [],
    "checked": true,
    "id": "c84bec97770c7851af0516f18bf1121d780897d1",
    "semantic_title": "tacolpcnet: fast and stable tts by conditioning lpcnet on mel spectrogram predictions",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bak21_interspeech.html": {
    "title": "FastPitchFormant: Source-Filter Based Decomposed Modeling for Speech Synthesis",
    "volume": "main",
    "abstract": "Methods for modeling and controlling prosody with acoustic features have been proposed for neural text-to-speech (TTS) models. Prosodic speech can be generated by conditioning acoustic features. However, synthesized speech with a large pitch-shift scale suffers from audio quality degradation, and speaker characteristics deformation. To address this problem, we propose a feed-forward Transformer based TTS model that is designed based on the source-filter theory. This model, called , has a unique structure that handles text and acoustic features in parallel. With modeling each feature separately, the tendency that the model learns the relationship between two features can be mitigated. Owing to its structural characteristics, FastPitchFormant is robust and accurate for pitch control and generates prosodic speech preserving speaker characteristics. The experimental results show that proposed model outperforms the baseline FastPitch",
    "keywords": [],
    "checked": true,
    "id": "034869f2f55b01f240b30923983ea197ff9fc32c",
    "semantic_title": "fastpitchformant: source-filter based decomposed modeling for speech synthesis",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nakamura21_interspeech.html": {
    "title": "Sequence-to-Sequence Learning for Deep Gaussian Process Based Speech Synthesis Using Self-Attention GP Layer",
    "volume": "main",
    "abstract": "This paper presents a speech synthesis method based on deep Gaussian process (DGP) and sequence-to-sequence (Seq2Seq) learning toward high-quality end-to-end speech synthesis. Feed-forward and recurrent models using DGP are known to produce more natural synthetic speech than deep neural networks (DNNs) because of Bayesian learning and kernel regression. However, such DGP models consist of a pipeline architecture of independent models, acoustic and duration models, and require a high level of expertise in text processing. The proposed model is based on Seq2Seq learning, which enables a unified training of acoustic and duration models. The encoder and decoder layers are represented by Gaussian process regressions (GPRs) and the parameters are trained as a Bayesian model. We also propose a self-attention mechanism with Gaussian processes to effectively model character-level input in the encoder. The subjective evaluation results show that the proposed Seq2Seq-SA-DGP can synthesize more natural speech than DNNs with self-attention and recurrent structures. Besides, Seq2Seq-SA-DGP reduces the smoothing problems of recurrent structures and is effective when a simple input for an end-to-end system is given",
    "keywords": [],
    "checked": true,
    "id": "9758aae98703469691e454ee622e5201ebfc7ae3",
    "semantic_title": "sequence-to-sequence learning for deep gaussian process based speech synthesis using self-attention gp layer",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kakegawa21_interspeech.html": {
    "title": "Phonetic and Prosodic Information Estimation from Texts for Genuine Japanese End-to-End Text-to-Speech",
    "volume": "main",
    "abstract": "The biggest obstacle to develop end-to-end Japanese text-to-speech (TTS) systems is to estimate phonetic and prosodic information (PPI) from Japanese texts. The following are the reasons: (1) the Kanji characters of the Japanese writing system have multiple corresponding pronunciations, (2) there is no separation mark between words, and (3) an accent nucleus must be assigned at appropriate positions. In this paper, we propose to solve the problems by neural machine translation (NMT) on the basis of encoder-decoder models, and compare NMT models of recurrent neural networks and the Transformer architecture. The proposed model handles texts on token (character) basis, although conventional systems handle them on word basis. To ensure the potential of the proposed approach, NMT models are trained using pairs of sentences and their PPIs that are generated by a conventional Japanese TTS system from 5 million sentences. Evaluation experiments were performed using PPIs that are manually annotated for 5,142 sentences. The experimental results showed that the Transformer architecture has the best performance, with 98.0% accuracy for phonetic information estimation and 95.0% accuracy for PPI estimation. Judging from the results, NMT models are promising toward end-to-end Japanese TTS",
    "keywords": [],
    "checked": true,
    "id": "8fb7bf026379d6e2a51ee95b6dc3aa09ca53b4bc",
    "semantic_title": "phonetic and prosodic information estimation from texts for genuine japanese end-to-end text-to-speech",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dai21_interspeech.html": {
    "title": "Information Sieve: Content Leakage Reduction in End-to-End Prosody Transfer for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "Expressive neural text-to-speech (TTS) systems incorporate a style encoder to learn a latent embedding as the style information. However, this embedding process may encode redundant textual information. This phenomenon is called content leakage. Researchers have attempted to resolve this problem by adding an ASR or other auxiliary supervision loss functions. In this study, we propose an unsupervised method called the \"information sieve\" to reduce the effect of content leakage in prosody transfer. The rationale of this approach is that the style encoder can be forced to focus on style information rather than on textual information contained in the reference speech by a well-designed downsample-upsample filter, i.e., the extracted style embeddings can be downsampled at a certain interval and then upsampled by duplication. Furthermore, we used instance normalization in convolution layers to help the system learn a better latent style space. Objective metrics such as the significantly lower word error rate (WER) demonstrate the effectiveness of this model in mitigating content leakage. Listening tests indicate that the model retains its prosody transferability compared with the baseline models such as the original GST-Tacotron and ASR-guided Tacotron",
    "keywords": [],
    "checked": true,
    "id": "155e69f30da7726a584bb293c1b8c9ff227d11fe",
    "semantic_title": "information sieve: content leakage reduction in end-to-end prosody for expressive speech synthesis",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dou21_interspeech.html": {
    "title": "Deliberation-Based Multi-Pass Speech Synthesis",
    "volume": "main",
    "abstract": "Sequence-to-sequence (seq2seq) models have achieved state-of-the-art performance in a wide range of tasks including Neural Machine Translation (NMT) and Text-To-Speech (TTS). These models are usually trained with teacher forcing, where the reference back-history is used to predict the next token. This makes training efficient, but limits performance, because during inference the free-running back-history must be used. To address this problem, deliberation-based multi-pass seq2seq has been used in NMT. Here the output sequence is generated in multiple passes, each one conditioned on the initial input and the free-running output of the previous pass. This paper investigates, and compares, deliberation-based multi-pass seq2seq for TTS and NMT. For NMT the simplest form of multi-pass approaches, where the free-running first-pass output is combined with the initial input, improves performance. However, applying this scheme to TTS is challenging: the multi-pass model tends to converge to the standard single-pass model, ignoring the previous output. To tackle this issue, a guided attention loss is added, enabling the system to make more extensive use of the free-running output. Experimental results confirm the above analysis and demonstrate that the proposed TTS model outperforms a strong baseline",
    "keywords": [],
    "checked": true,
    "id": "b8233798ff4bd3b98c884a61b84e1fa02fd1aefa",
    "semantic_title": "deliberation-based multi-pass speech synthesis",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elias21_interspeech.html": {
    "title": "Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling",
    "volume": "main",
    "abstract": "This paper introduces , a non-autoregressive neural text-to-speech model with a fully differentiable duration model which does not require supervised duration signals. The duration model is based on a novel attention mechanism and an iterative reconstruction loss based on Soft Dynamic TimeWarping, this model can learn token-frame alignments as well as token durations automatically. Experimental results show that Parallel Tacotron 2 outperforms baselines in subjective naturalness in several diverse multi speaker evaluations",
    "keywords": [],
    "checked": true,
    "id": "d500d4147509749e10d388fd4900372d01a7c5df",
    "semantic_title": "parallel tacotron 2: a non-autoregressive neural tts model with differentiable duration modeling",
    "citation_count": 53,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21b_interspeech.html": {
    "title": "Transformer-Based Acoustic Modeling for Streaming Speech Synthesis",
    "volume": "main",
    "abstract": "Transformer models have shown promising results in neural speech synthesis due to their superior ability to model long-term dependencies compared to recurrent networks. The computation complexity of transformers increases quadratically with sequence length, making it impractical for many real-time applications. To address the complexity issue in speech synthesis domain, this paper proposes an efficient transformer-based acoustic model that is constant-speed regardless of input sequence length, making it ideal for streaming speech synthesis applications. The proposed model uses a transformer network that predicts the prosody features at phone rate and then an Emformer network to predict the frame-rate spectral features in a streaming manner. Both the transformer and Emformer in the proposed architecture use a self-attention mechanism that involves explicit long-term information, thus providing improved speech naturalness for long utterances. In our experiments, we use a WaveRNN neural vocoder that takes in the predicted spectral features and generates the final audio. The overall architecture achieves human-like speech quality both on short and long utterances while maintaining a low latency and low real-time factor. Our mean opinion score (MOS) evaluation shows that for short utterances, the proposed model achieves a MOS of 4.213 compared to ground-truth with MOS of 4.307; and for long utterances, it also produces high-quality speech with a MOS of 4.201 compared to ground-truth with MOS of 4.360",
    "keywords": [],
    "checked": true,
    "id": "3b0e51ad9b89f87538f6256cda0b7a22dd71a0b8",
    "semantic_title": "transformer-based acoustic modeling for streaming speech synthesis",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jia21_interspeech.html": {
    "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS",
    "volume": "main",
    "abstract": "This paper introduces , a new encoder model for neural TTS. This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them. It can be pre-trained on a large text corpus in a self-supervised manner, and fine-tuned in a TTS task. Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training. Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers",
    "keywords": [],
    "checked": true,
    "id": "c437b1f260f294af628483d77a49239fa613aa89",
    "semantic_title": "png bert: augmented bert on phonemes and graphemes for neural tts",
    "citation_count": 59,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ge21_interspeech.html": {
    "title": "Speed up Training with Variable Length Inputs by Efficient Batching Strategies",
    "volume": "main",
    "abstract": "In the model training with neural networks, although the model performance is always the first priority to optimize, training efficiency also plays an important role in model deployment. There are many ways to speed up training with minimal performance loss, such as training with more GPUs, or with mixed precisions, optimizing training parameters, or making features more compact but more representable. Since mini-batch training is now the go-to approach for many machine learning tasks, minimizing the zero-padding to incorporate samples of different lengths into one batch, is an alternative approach to save training time. Here we propose a batching strategy based on semi-sorted samples, with dynamic batch sizes and batch randomization. By replacing the random batching with the proposed batching strategies, it saves more than 40% training time without compromising performance in training seq2seq neural text-to-speech models based on the Tacotron framework. We also compare it with two other batching strategies and show it performs similarly in terms of saving time and maintaining performance, but with a simpler concept and a smoother tuning parameter to balance between zero-padding and randomness level",
    "keywords": [],
    "checked": true,
    "id": "e8aea5f6152a76f6c552789343c248873a204246",
    "semantic_title": "speed up training with variable length inputs by efficient batching strategies",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sun21_interspeech.html": {
    "title": "Funnel Deep Complex U-Net for Phase-Aware Speech Enhancement",
    "volume": "main",
    "abstract": "The emergence of deep neural networks has made speech enhancement well developed. Most of the early models focused on estimating the magnitude of spectrum while ignoring the phase, this gives the evaluation result a certain upper limit. Some recent researches proposed deep complex network, which can handle complex inputs, and realize joint estimation of magnitude spectrum and phase spectrum by outputting real and imaginary parts respectively. The encoder-decoder structure in Deep Complex U-net (DCU) has been proven to be effective for complex-valued data. To further improve the performance, in this paper, we design a new network called Funnel Deep Complex U-net (FDCU), which could process magnitude information and phase information separately through one-encoder-two-decoders structure. Moreover, in order to achieve better training effect, we define negative stretched-SI-SNR as the loss function to avoid errors caused by the negative vector angle. Experimental results show that our FDCU model outperforms state-of-the-art approaches in all evaluation metrics",
    "keywords": [],
    "checked": true,
    "id": "c9ca35b78e5f19eb955d9173ce019adba9045854",
    "semantic_title": "funnel deep complex u-net for phase-aware speech enhancement",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21b_interspeech.html": {
    "title": "Temporal Convolutional Network with Frequency Dimension Adaptive Attention for Speech Enhancement",
    "volume": "main",
    "abstract": "Despite much progress, most temporal convolutional networks (TCN) based speech enhancement models are mainly focused on modeling the long-term temporal contextual dependencies of speech frames, without taking into account the distribution information of speech signal in frequency dimension. In this study, we propose a frequency dimension adaptive attention (FAA) mechanism to improve TCNs, which guides the model selectively emphasize the frequency-wise features with important speech information and also improves the representation capability of network. Our extensive experimental investigation demonstrates that the proposed FAA mechanism is able to consistently provide significant improvements in terms of speech quality (PESQ), intelligibility (STOI) and three other composite metrics. More promisingly, it has better generalization ability to real-world noisy environment",
    "keywords": [],
    "checked": true,
    "id": "30e79104624988e9a4da0c58044ffd59e53f4ab9",
    "semantic_title": "temporal convolutional network with frequency dimension adaptive attention for speech enhancement",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21_interspeech.html": {
    "title": "Perceptual Contributions of Vowels and Consonant-Vowel Transitions in Understanding Time-Compressed Mandarin Sentences",
    "volume": "main",
    "abstract": "Many early studies reported the importance of vowels and vowel-consonant transitions to speech intelligibility. The present work assessed their perceptual impacts to the understanding of time-compressed sentences, which could be used to measure the temporal acuity during speech understanding. Mandarin sentences were edited to selectively preserve vowel centers or vowel-consonant transitional segments, and compress the rest regions with equipment time compression rates (TCRs) up to 3, including conditions only preserving vowel centers or vowel-consonant transitions. The processed stimuli were presented to normal-hearing listeners to recognize. Results showed that, consistent with the segmental contributions in understanding uncompressed speech, the vowel-only time-compressed stimuli were highly intelligible (i.e., intelligibility score >85%) at a TCR around 3, and vowel-consonant transitions carried important intelligibility information in understanding time-compressed sentences. The time-compression conditions in the present work provided higher intelligibility scores than their counterparties in understanding the PSOLA-processed time-compressed sentences with TCRs around 3. The findings in this work suggested that the design of time compression processing could be guided towards selectively preserving perceptually important speech segments (e.g., vowels) in the future",
    "keywords": [],
    "checked": true,
    "id": "3fc7c3e0b439ff6ddf30ea0168f02311e323d1ce",
    "semantic_title": "perceptual contributions of vowels and consonant-vowel transitions in understanding time-compressed mandarin sentences",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/biswas21_interspeech.html": {
    "title": "Transfer Learning for Speech Intelligibility Improvement in Noisy Environments",
    "volume": "main",
    "abstract": "In a recent work [1], a novel Delta Function-based Formant Shifting approach was proposed for speech intelligibility improvement. The underlying principle is to dynamically relocate the formants based on their occurrence in the spectrum away from the region of noise. The manner in which the formants are shifted is decided by the parameters of the Delta Function, the optimal values of which are evaluated using Comprehensive Learning Particle Swarm Optimization (CLPSO). Although effective, CLPSO is computationally expensive to the extent that it overshadows its merits in intelligibility improvement. As a solution to this, the current work aims to improve the Short-Time Objective Intelligibility (STOI) of (target) speech using a Delta Function that has been generated using a different (source) language. This transfer learning is based upon the relative positioning of the formant frequencies and pitch values of the source & target language datasets. The proposed approach is demonstrated and validated by subjecting it to experimentation with three different languages under variable noisy conditions",
    "keywords": [],
    "checked": true,
    "id": "89a3c322ce4acf30e2d3298759fc47d2f6e5627b",
    "semantic_title": "transfer learning for speech intelligibility improvement in noisy environments",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yamamoto21_interspeech.html": {
    "title": "Comparison of Remote Experiments Using Crowdsourcing and Laboratory Experiments on Speech Intelligibility",
    "volume": "main",
    "abstract": "Many subjective experiments have been performed to develop objective speech intelligibility measures, but the novel coronavirus outbreak has made it difficult to conduct experiments in a laboratory. One solution is to perform remote testing using crowdsourcing; however, because we cannot control the listening conditions, it is unclear whether the results are entirely reliable. In this study, we compared the speech intelligibility scores obtained from remote and laboratory experiments. The results showed that the mean and standard deviation (SD) of the remote experiments' speech reception threshold (SRT) were higher than those of the laboratory experiments. However, the variance in the SRTs across the speech-enhancement conditions revealed similarities, implying that remote testing results may be as useful as laboratory experiments to develop an objective measure. We also show that practice session scores are correlated with SRT values. This is a priori information before performing the main tests and would be useful for data screening to reduce the variability of the SRT distribution",
    "keywords": [],
    "checked": true,
    "id": "6318311b8dad29622cf6fda76af9c2f249b62bfc",
    "semantic_title": "comparison of remote experiments using crowdsourcing and laboratory experiments on speech intelligibility",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21b_interspeech.html": {
    "title": "Know Your Enemy, Know Yourself: A Unified Two-Stage Framework for Speech Enhancement",
    "volume": "main",
    "abstract": "Traditional spectral subtraction-type single channel speech enhancement (SE) algorithms often need to estimate interference components including noise and/or reverberation before subtracting them while deep neural network-based SE methods often aim to realize the end-to-end target mapping. In this paper, we show that both denoising and dereverberation can be unified into a common problem by introducing a two-stage paradigm, namely for interference components estimation and speech recovery. In the first stage, we propose to explicitly extract the magnitude of interference components, which serves as the prior information. In the second stage, with the guidance of this estimated magnitude prior, we can expect to better recover the target speech. In addition, we propose a transform module to facilitate the interaction between interference components and the desired speech modalities. Meanwhile, a temporal fusion module is designed to model long-term dependencies without ignoring short-term details. We conduct the experiments on the WSJ0-SI84 corpus and the results on both denoising and dereverberation tasks show that our approach outperforms previous advanced systems and achieves state-of-the-art performance in terms of many objective metrics",
    "keywords": [],
    "checked": true,
    "id": "7caad4b855165f3f19501c105b7302a4401893a4",
    "semantic_title": "know your enemy, know yourself: a unified two-stage framework for speech enhancement",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kong21_interspeech.html": {
    "title": "Speech Enhancement with Weakly Labelled Data from AudioSet",
    "volume": "main",
    "abstract": "Speech enhancement is a task to improve the intelligibility and perceptual quality of degraded speech signals. Recently, neural network-based methods have been applied to speech enhancement. However, many neural network-based methods require users to collect clean speech and background noise for training, which can be time-consuming. In addition, speech enhancement systems trained on particular types of background noise may not generalize well to a wide range of noise. To tackle those problems, we propose a speech enhancement framework trained on weakly labelled data. We first apply a pretrained sound event detection system to detect anchor segments that contain sound events in audio clips. Then, we randomly mix two detected anchor segments as a mixture. We build a conditional source separation network using the mixture and a conditional vector as input. The conditional vector is obtained from the audio tagging predictions on the anchor segments. In inference, we input a noisy speech signal with the one-hot encoding of \"Speech\" as a condition to the trained system to predict enhanced speech. Our system achieves a PESQ of 2.28 and an SSNR of 8.75 dB on the VoiceBank-DEMAND dataset, outperforming the previous SEGAN system of 2.16 and 7.73 dB respectively",
    "keywords": [],
    "checked": true,
    "id": "89db5d6c26219fbc09ab4b1ac1853a514281f848",
    "semantic_title": "speech enhancement with weakly labelled data from audioset",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hsieh21_interspeech.html": {
    "title": "Improving Perceptual Quality by Phone-Fortified Perceptual Loss Using Wasserstein Distance for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement (SE) aims to improve speech quality and intelligibility, which are both related to a smooth transition in speech segments that may carry linguistic information, e.g. phones and syllables. In this study, we propose a novel phone-fortified perceptual loss (PFPL) that takes phonetic information into account for training SE models. To effectively incorporate the phonetic information, the PFPL is computed based on latent representations of the model, a powerful self-supervised encoder that renders rich phonetic information. To more accurately measure the distribution distances of the latent representations, the PFPL adopts the Wasserstein distance as the distance measure. Our experimental results first reveal that the PFPL is more correlated with the perceptual evaluation metrics, as compared to signal-level losses. Moreover, the results showed that the PFPL can enable a deep complex U-Net SE model to achieve highly competitive performance in terms of standardized quality and intelligibility evaluations on the Voice Bank–DEMAND dataset",
    "keywords": [],
    "checked": true,
    "id": "27fe7a76098c9344024fa5c70c8f53e9df637040",
    "semantic_title": "improving perceptual quality by phone-fortified perceptual loss using wasserstein distance for speech enhancement",
    "citation_count": 38,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fu21_interspeech.html": {
    "title": "MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement",
    "volume": "main",
    "abstract": "The discrepancy between the cost function used for training a speech enhancement model and human auditory perception usually makes the quality of enhanced speech unsatisfactory. Objective evaluation metrics which consider human perception can hence serve as a bridge to reduce the gap. Our previously proposed MetricGAN was designed to optimize objective metrics by connecting the metric with a discriminator. Because only the scores of the target evaluation functions are needed during training, the metrics can even be non-differentiable. In this study, we propose a MetricGAN+ in which three training techniques incorporating domain-knowledge of speech processing are proposed. With these techniques, experimental results on the VoiceBank-DEMAND dataset show that MetricGAN+ can increase PESQ score by 0.3 compared to the previous MetricGAN and achieve state-of-the-art results (PESQ score = 3.15)",
    "keywords": [],
    "checked": true,
    "id": "6af96d82d7ba1f4945dc208b6b0ab527bc6d8874",
    "semantic_title": "metricgan+: an improved version of metricgan for speech enhancement",
    "citation_count": 132,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/edraki21_interspeech.html": {
    "title": "A Spectro-Temporal Glimpsing Index (STGI) for Speech Intelligibility Prediction",
    "volume": "main",
    "abstract": "We propose a monaural intrusive speech intelligibility prediction (SIP) algorithm called STGI based on detecting in short-time segments in a spectro-temporal modulation decomposition of the input speech signals. Unlike existing glimpse-based SIP methods, the application of STGI is not limited to additive uncorrelated noise; STGI can be employed in a broad range of degradation conditions. Our results show that STGI performs consistently well across 15 datasets covering degradation conditions including modulated noise, noise reduction processing, reverberation, near-end listening enhancement, checkerboard noise, and gated noise",
    "keywords": [],
    "checked": true,
    "id": "88ac79710f0ff8696c8e98d7a28adcf8c183cbd9",
    "semantic_title": "a spectro-temporal glimpsing index (stgi) for speech intelligibility prediction",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiu21_interspeech.html": {
    "title": "Self-Supervised Learning Based Phone-Fortified Speech Enhancement",
    "volume": "main",
    "abstract": "For speech enhancement, deep complex network based methods have shown promising performance due to their effectiveness in dealing with complex-valued spectrums. Recent speech enhancement methods focus on further optimization of network structures and hyperparameters, however, ignore inherent speech characteristics (e.g., phonetic characteristics), which are important for networks to learn and reconstruct speech information. In this paper, we propose a novel self-supervised learning based phone-fortified (SSPF) method for speech enhancement. Our method explicitly imports phonetic characteristics into a deep complex convolutional network via a Contrastive Predictive Coding (CPC) model pre-trained with self-supervised learning. This operation can greatly improve speech representation learning and speech enhancement performance. Moreover, we also apply the self-attention mechanism to our model for learning long-range dependencies of a speech sequence, which further improves the performance of speech enhancement. The experimental results demonstrate that our SSPF method outperforms existing methods and achieves state-of-the-art performance in terms of speech quality and intelligibility",
    "keywords": [],
    "checked": true,
    "id": "b71441f85b95b0274675159c3cc1022f91bb75d3",
    "semantic_title": "self-supervised learning based phone-fortified speech enhancement",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nayem21_interspeech.html": {
    "title": "Incorporating Embedding Vectors from a Human Mean-Opinion Score Prediction Model for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Objective measures of success, such as the perceptual evaluation of speech quality (PESQ), signal-to-distortion ratio (SDR), and short-time objective intelligibility (STOI), have recently been used to optimize deep-learning based speech enhancement algorithms, in an effort to incorporate perceptual constraints into the learning process. Optimizing with these measures, however, may be sub-optimal, since the objective scores do not always strongly correlate with a listener's evaluation. This motivates the need for approaches that either are optimized with scores that are strongly correlated with human assessments or that use alternative strategies for incorporating perceptual constraints. In this work, we propose an attention-based approach that uses learned speech embedding vectors from a mean-opinion score (MOS) prediction model and a speech enhancement module to jointly enhance noisy speech. Our loss function is jointly optimized with signal approximation and MOS prediction loss terms. We train the model using real-world noisy speech data that has been captured in everyday environments. The results show that our proposed model significantly outperforms other approaches that are optimized with objective measures",
    "keywords": [],
    "checked": true,
    "id": "c1078dcb98cee7e6c5b67ff0adda5f62a4644398",
    "semantic_title": "incorporating embedding vectors from a human mean-opinion score prediction model for monaural speech enhancement",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21c_interspeech.html": {
    "title": "Restoring Degraded Speech via a Modified Diffusion Model",
    "volume": "main",
    "abstract": "There are many deterministic mathematical operations (e.g. compression, clipping, downsampling) that degrade speech quality considerably. In this paper we introduce a neural network architecture, based on a modification of the DiffWave model, that aims to restore the original speech signal. DiffWave, a recently published diffusion-based vocoder, has shown state-of-the-art synthesized speech quality and relatively shorter waveform generation times, with only a small set of parameters. We replace the mel-spectrum upsampler in DiffWave with a deep CNN upsampler, which is trained to alter the degraded speech mel-spectrum to match that of the original speech. The model is trained using the original speech waveform, but conditioned on the degraded speech mel-spectrum. Post-training, only the degraded mel-spectrum is used as input and the model generates an estimate of the original speech. Our model results in improved speech quality (original DiffWave model as baseline) on several different experiments. These include improving the quality of speech degraded by LPC-10 compression, AMR-NB compression, and signal clipping. Compared to the original DiffWave architecture, our scheme achieves better performance on several objective perceptual metrics and in subjective comparisons. Improvements over baseline are further amplified in a out-of-corpus evaluation setting",
    "keywords": [],
    "checked": true,
    "id": "28db8fd711ac13599c9921db08cd586235d303ba",
    "semantic_title": "restoring degraded speech via a modified diffusion model",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21_interspeech.html": {
    "title": "User-Initiated Repetition-Based Recovery in Multi-Utterance Dialogue Systems",
    "volume": "main",
    "abstract": "Recognition errors are common in human communication. Similar errors often lead to unwanted behaviour in dialogue systems or virtual assistants. In human communication, we can recover from them by repeating misrecognized words or phrases; however in human-machine communication this recovery mechanism is not available. In this paper, we attempt to bridge this gap and present a system that allows a user to correct speech recognition errors in a virtual assistant by repeating misunderstood words. When a user repeats part of the phrase the system rewrites the original query to incorporate the correction. This rewrite allows the virtual assistant to understand the original query successfully. We present an end-to-end 2-step attention pointer network that can generate the the rewritten query by merging together the incorrectly understood utterance with the correction follow-up. We evaluate the model on data collected for this task and compare the proposed model to a rule-based baseline and a standard pointer network. We show that rewriting the original query is an effective way to handle repetition-based recovery and that the proposed model outperforms the rule based baseline, reducing Word Error Rate by 19% relative at 2% False Alarm Rate on annotated data",
    "keywords": [],
    "checked": true,
    "id": "3722c06603b144ecdb8bb9f594751934f52b214d",
    "semantic_title": "user-initiated repetition-based recovery in multi-utterance dialogue systems",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21_interspeech.html": {
    "title": "Self-Supervised Dialogue Learning for Spoken Conversational Question Answering",
    "volume": "main",
    "abstract": "In spoken conversational question answering (SCQA), the answer to the corresponding question is generated by retrieving and then analyzing a fixed spoken document, including multi-part conversations. Most SCQA systems have considered only retrieving information from ordered utterances. However, the sequential order of dialogue is important to build a robust spoken conversational question answering system, and the changes of utterances order may severely result in low-quality and incoherent corpora. To this end, we introduce a self-supervised learning approach, including , and , to explicitly capture the coreference resolution and dialogue coherence among spoken documents. Specifically, we design a joint learning framework where the auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards more coherent and meaningful spoken dialogue learning. We also utilize the proposed self-supervised learning tasks to capture intra-sentence coherence. Experimental results demonstrate that our proposed method provides more coherent, meaningful, and appropriate responses, yielding superior performance gains compared to the original pre-trained language models. Our method achieves state-of-the-art results on the Spoken-CoQA dataset",
    "keywords": [],
    "checked": true,
    "id": "904afb6696575bb7d74f5e4e74d9cfced480a645",
    "semantic_title": "self-supervised dialogue learning for spoken conversational question answering",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/su21_interspeech.html": {
    "title": "Act-Aware Slot-Value Predicting in Multi-Domain Dialogue State Tracking",
    "volume": "main",
    "abstract": "As an essential component in task-oriented dialogue systems, dialogue state tracking (DST) aims to track human-machine interactions and generate state representations for managing the dialogue. Representations of dialogue states are dependent on the domain ontology and the user's goals. In several task-oriented dialogues with a limited scope of objectives, dialogue states can be represented as a set of slot-value pairs. As the capabilities of dialogue systems expand to support increasing naturalness in communication, incorporating dialogue act processing into dialogue model design becomes essential. The lack of such consideration limits the scalability of dialogue state tracking models for dialogues having specific objectives and ontology. To address this issue, we formulate and incorporate dialogue acts, and leverage recent advances in machine reading comprehension to predict both categorical and non-categorical types of slots for multi-domain dialogue state tracking. Experimental results show that our models can improve the overall accuracy of dialogue state tracking on the MultiWOZ 2.1 dataset, and demonstrate that incorporating dialogue acts can guide dialogue state design for future task-oriented dialogue systems",
    "keywords": [],
    "checked": true,
    "id": "8fe6ac053bc961093f46d8592e743215ad33bbad",
    "semantic_title": "act-aware slot-value predicting in multi-domain dialogue state tracking",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chiba21_interspeech.html": {
    "title": "Dialogue Situation Recognition for Everyday Conversation Using Multimodal Information",
    "volume": "main",
    "abstract": "In recent years, dialogue systems have been applied to daily living. Such systems should be able to associate conversations with dialogue situations, such as a place where a dialogue occurs and the relationship between participants. In this study, we propose a dialogue situation recognition method that understands the perspective of dialogue scenes. The target dialogue situations contain dialogue styles, places, activities, and relations between participants. We used the Corpus of Everyday Japanese Conversation (CEJC), which records natural everyday conversations in various situations for experiments. We experimentally verified the effectiveness of our proposed method using multimodal information for situation recognition",
    "keywords": [],
    "checked": true,
    "id": "c5be70f553d46a6f6413fb48c044f6b8093bbefc",
    "semantic_title": "dialogue situation recognition for everyday conversation using multimodal information",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yamazaki21_interspeech.html": {
    "title": "Neural Spoken-Response Generation Using Prosodic and Linguistic Context for Conversational Systems",
    "volume": "main",
    "abstract": "Spoken dialogue systems have become widely used in daily life. Such a system must interact with the user socially to truly operate as a partner with humans. In studies of recent dialogue systems, neural response generation led to natural response generation. However, these studies have not considered the acoustic aspects of conversational phenomena, such as the adaptation of prosody. We propose a spoken-response generation model that extends a neural conversational model to deal with pitch control signals. Our proposed model is trained using multimodal dialogue between humans. The generated pitch control signals are input to a speech synthesis system to control the pitch of synthesized speech. Our experiment shows that the proposed system can generate synthesized speech with an appropriate F0 contour as an utterance in context compared to the output of a system without pitch control, although language generation remains an issue",
    "keywords": [],
    "checked": true,
    "id": "202c9b9e772fe9a8f26ea886268420e96376fcc3",
    "semantic_title": "neural spoken-response generation using prosodic and linguistic context for conversational systems",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21_interspeech.html": {
    "title": "Semantic Transportation Prototypical Network for Few-Shot Intent Detection",
    "volume": "main",
    "abstract": "Few-shot intent detection is a problem that only a few annotated examples are available for unseen intents, and deep models could suffer from the overfitting problem because of scarce data. Existing state-of-the-art few-shot model, Prototypical Network (PN), mainly focus on computing the similarity between examples in a metric space by leveraging sentence-level instance representations. However, sentence-level representations may incorporate highly noisy signals from unrelated words which leads to performance degradation. In this paper, we propose Semantic Transportation Prototypical Network (STPN) to alleviate this issue. Different from the original PN, our approach takes word-level representation as input and uses a new distance metric to obtain better sample matching result. And we reformulate the few-shot classification task into an instance of optimal matching, in which the key word semantic information between examples are expected to be matched and the matching cost is treated as similarity. Specifically, we design Mutual-Semantic mechanism to generate word semantic information, which could reduce the unrelated word noise and enrich key word information. Then, Earth Mover's Distance (EMD) is applied to find an optimal matching solution. Comprehensive experiments on two benchmark datasets are conducted to validate the effectiveness and generalization of our proposed model",
    "keywords": [],
    "checked": true,
    "id": "8decba8a86d69c19aa87edf53e06b31298ab22c8",
    "semantic_title": "semantic transportation prototypical network for few-shot intent detection",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tang21_interspeech.html": {
    "title": "Domain-Specific Multi-Agent Dialog Policy Learning in Multi-Domain Task-Oriented Scenarios",
    "volume": "main",
    "abstract": "Traditional dialog policy learning methods train a generic dialog agent to address all situations. However, when the dialog agent encounters a complicated task that involves more than one domain, it becomes difficult to perform concordant actions due to the hybrid information in the multi-domain ontology. Inspired by a real-life scenario at a bank, there are always several specialized departments that deal with different businesses. In this paper, we propose Domain-Specific Multi-Agent Dialog Policy Learning (DSMADPL), in which the dialog system is composed of a set of agents where each agent represents a specialized skill in a particular domain. Every domain-specific agent is first pretrained with supervised learning using a dialog corpus, and then they are jointly improved with multi-agent reinforcement learning. When the dialog system interacts with the user, in each turn the system action is decided by the actions of relevant agents. Experiments conducted on the commonly used MultiWOZ dataset prove the effectiveness of the proposed method, in which dialog success rate increases from 55.0% for the traditional method to 67.2% for our method in multi-domain scenarios",
    "keywords": [],
    "checked": true,
    "id": "e0a9a3e463bbc7b65fd562c5c9a8dbc6217218c9",
    "semantic_title": "domain-specific multi-agent dialog policy learning in multi-domain task-oriented scenarios",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21b_interspeech.html": {
    "title": "Leveraging ASR N-Best in Deep Entity Retrieval",
    "volume": "main",
    "abstract": "Entity Retrieval (ER) in spoken dialog systems is a task that retrieves entities in a catalog for the entity mentions in user utterances. ER systems are susceptible to upstream errors, with Automatic Speech Recognition (ASR) errors being particularly troublesome. In this work, we propose a robust deep learning based ER system by leveraging ASR N-best hypotheses. Specifically, we evaluate different neural architectures to infuse ASR N-best through an attention mechanism. On 750 hours of audio data taken from live traffic, our best model achieves 11.07% relative error reduction while maintaining the same performance on rejecting out-of-domain ER requests",
    "keywords": [],
    "checked": true,
    "id": "e90aa6debe3bdc089f174bb8f6671e8f47798592",
    "semantic_title": "leveraging asr n-best in deep entity retrieval",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21d_interspeech.html": {
    "title": "End-to-End Spelling Correction Conditioned on Acoustic Feature for Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "In this work, we propose a new end-to-end (E2E) spelling correction method for post-processing of code-switching automatic speech recognition (ASR). Existing E2E spelling correction models take the hypotheses of ASR as inputs and annotated text as the targets. Due to the powerful modeling capabilities of the E2E model, the training of the correction system is extremely prone to over-fitting. It usually requires sufficient data diversity for reliable training. Therefore, it is difficult to apply the E2E correction models to the code-switching ASR task because of the data shortage. In this paper, we introduce the acoustic features into the spelling correction model. Our method can alleviate the problem of over-fitting and has better performance. Meanwhile, because the acoustic features are encode-free, our proposed model can be applied to the ASR model without significantly increasing the computational cost. The experimental results on ASRU 2019 Mandarin-English Code-switching Challenge data set show that the proposed method achieves 11.14% relative error rate reduction compared with baseline",
    "keywords": [],
    "checked": true,
    "id": "832344b4e8dcdebca7a78e4d1dab1b73cbc08b9b",
    "semantic_title": "end-to-end spelling correction conditioned on acoustic feature for code-switching speech recognition",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/siminyu21_interspeech.html": {
    "title": "Phoneme Recognition Through Fine Tuning of Phonetic Representations: A Case Study on Luhya Language Varieties",
    "volume": "main",
    "abstract": "Models pre-trained on multiple languages have shown significant promise for improving speech recognition, particularly for low-resource languages. In this work, we focus on phoneme recognition using Allosaurus, a method for multilingual recognition based on phonetic annotation, which incorporates phonological knowledge through a language-dependent allophone layer that associates a universal narrow phone-set with the phonemes that appear in each language. To evaluate in a challenging real-world scenario, we curate phone recognition datasets for Bukusu and Saamia, two varieties of the Luhya language cluster of western Kenya and eastern Uganda. To our knowledge, these datasets are the first of their kind. We carry out similar experiments on the dataset of an endangered Tangkhulic language, East Tusom, a Tibeto-Burman language variety spoken mostly in India. We explore both zero-shot and few-shot recognition by fine-tuning using datasets of varying sizes (10 to 1000 utterances). We find that fine-tuning of Allosaurus, even with just 100 utterances, leads to significant improvements in phone error rates",
    "keywords": [],
    "checked": true,
    "id": "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713",
    "semantic_title": "phoneme recognition through fine tuning of phonetic representations: a case study on luhya language varieties",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/loweimi21_interspeech.html": {
    "title": "Speech Acoustic Modelling Using Raw Source and Filter Components",
    "volume": "main",
    "abstract": "Source-filter modelling is among the fundamental techniques in speech processing with a wide range of applications. In acoustic modelling, features such as MFCC and PLP which parametrise the filter component are widely employed. In this paper, we investigate the efficacy of building acoustic models from the raw filter and source components. The raw magnitude spectrum, as the primary information stream, is decomposed into the excitation and vocal tract information streams via cepstral liftering. Then, acoustic models are built via multi-head CNNs which, among others, allow for processing each individual stream via a sequence of bespoke transforms and fusing them at an optimal level of abstraction. We discuss the possible advantages of such information factorisation and recombination, investigate the dynamics of these models and explore the optimal fusion level. Furthermore, we illustrate the CNN's learned filters and provide some interpretation for the captured patterns. The proposed approach with optimal fusion scheme results in up to 14% and 7% relative WER reduction in WSJ and Aurora-4 tasks",
    "keywords": [],
    "checked": true,
    "id": "956404c3e5ad68fcf7e416792889630f0f5c76f9",
    "semantic_title": "speech acoustic modelling using raw source and filter components",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujimoto21_interspeech.html": {
    "title": "Noise Robust Acoustic Modeling for Single-Channel Speech Recognition Based on a Stream-Wise Transformer Architecture",
    "volume": "main",
    "abstract": "This paper addresses a noise-robust automatic speech recognition (ASR) method under the constraints of real-time, one-pass, and single-channel processing. Under such strong constraints, single-channel speech enhancement becomes a key technology because methods with multiple-passes or batch processing, such as acoustic model adaptation, are not suitable for use. However, single-channel speech enhancement often degrades ASR performance due to speech distortion. To overcome this problem, we propose a noise robust acoustic modeling method based on the stream-wise transformer model. The proposed method accepts multi-stream features obtained by multiple single-channel speech enhancement methods as input and selectively uses an appropriate feature stream according to the noise environment by paying attention to the noteworthy stream on the basis of multi-head attention. The proposed method considers the attention for the stream direction instead of the time series direction, and it is thus capable of real-time and low-latency processing. Comparative evaluations reveal that the proposed method successfully improves the accuracy of ASR in noisy environments and reduces the number of model parameters even under strong constraints",
    "keywords": [],
    "checked": true,
    "id": "59dfc48ad807150a8db4a0b8b7e1eb8f8ac33103",
    "semantic_title": "noise robust acoustic modeling for single-channel speech recognition based on a stream-wise transformer architecture",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ratnarajah21_interspeech.html": {
    "title": "IR-GAN: Room Impulse Response Generator for Far-Field Speech Recognition",
    "volume": "main",
    "abstract": "We present a Generative Adversarial Network (GAN) based room impulse response generator (IR-GAN) for generating realistic synthetic room impulse responses (RIRs). IR-GAN extracts acoustic parameters from captured real-world RIRs and uses these parameters to generate new synthetic RIRs. We use these generated synthetic RIRs to improve far-field automatic speech recognition in new environments that are different from the ones used in training datasets. In particular, we augment the far-field speech training set by convolving our synthesized RIRs with a clean LibriSpeech dataset [1]. We evaluate the quality of our synthetic RIRs on the far-field LibriSpeech test set created using real-world RIRs from the BUT ReverbDB [2] and AIR [3] datasets. Our IR-GAN reports up to an 8.95% lower error rate than Geometric Acoustic Simulator (GAS) in far-field speech recognition benchmarks. We further improve the performance when we combine our synthetic RIRs with synthetic impulse responses generated using GAS. This combination can reduce the word error rate by up to 14.3% in far-field speech recognition benchmarks",
    "keywords": [],
    "checked": true,
    "id": "23e208eb3251fd3521c14bcbd3ee2d97a369ed89",
    "semantic_title": "ir-gan: room impulse response generator for far-field speech recognition",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21b_interspeech.html": {
    "title": "Scaling Sparsemax Based Channel Selection for Speech Recognition with ad-hoc Microphone Arrays",
    "volume": "main",
    "abstract": "Recently, speech recognition with ad-hoc microphone arrays has received much attention. It is known that channel selection is an important problem of ad-hoc microphone arrays, however, this topic seems far from explored in speech recognition yet, particularly with a large-scale ad-hoc microphone array. To address this problem, we propose a algorithm for the channel selection problem of the speech recognition with large-scale ad-hoc microphone arrays. Specifically, we first replace the conventional Softmax operator in the stream attention mechanism of a multichannel end-to-end speech recognition system with Sparsemax, which conducts channel selection by forcing the channel weights of noisy channels to zero. Because Sparsemax punishes the weights of many channels to zero harshly, we propose Scaling Sparsemax which punishes the channels mildly by setting the weights of very noisy channels to zero only. Experimental results with ad-hoc microphone arrays of over 30 channels under the conformer speech recognition architecture show that the proposed Scaling Sparsemax yields a word error rate of over 30% lower than Softmax on simulation data sets, and over 20% lower on semi-real data sets, in test scenarios with both matched and mismatched channel numbers",
    "keywords": [],
    "checked": true,
    "id": "d359eee45dc476c98324aff01240ad1163818a97",
    "semantic_title": "scaling sparsemax based channel selection for speech recognition with ad-hoc microphone arrays",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chang21_interspeech.html": {
    "title": "Multi-Channel Transformer Transducer for Speech Recognition",
    "volume": "main",
    "abstract": "Multi-channel inputs offer several advantages over single-channel, to improve the robustness of on-device speech recognition systems. Recent work on multi-channel transformer, has proposed a way to incorporate such inputs into end-to-end ASR for improved accuracy. However, this approach is characterized by a high computational complexity, which prevents it from being deployed in on-device systems. In this paper, we present a novel speech recognition model, , which features end-to-end multi-channel training, low computation cost, and low latency so that it is suitable for streaming decoding in on-device speech recognition. In a far-field in-house dataset, our MCTT outperforms stagewise multi-channel models with transformer-transducer up to 6.01% relative WER improvement (WERR). In addition, MCTT outperforms the multi-channel transformer up to 11.62% WERR, and is 15.8 times faster in terms of inference speed. We further show that we can improve the computational cost of MCTT by constraining the future and previous context in attention computations",
    "keywords": [],
    "checked": true,
    "id": "f35e8a7a2bafa8ccb12ae165294e165dfdb986d8",
    "semantic_title": "multi-channel transformer transducer for speech recognition",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tsunoo21_interspeech.html": {
    "title": "Data Augmentation Methods for End-to-End Speech Recognition on Distant-Talk Scenarios",
    "volume": "main",
    "abstract": "Although end-to-end automatic speech recognition (E2E ASR) has achieved great performance in tasks that have numerous paired data, it is still challenging to make E2E ASR robust against noisy and low-resource conditions. In this study, we investigated data augmentation methods for E2E ASR in distant-talk scenarios. E2E ASR models are trained on the series of CHiME challenge datasets, which are suitable tasks for studying robustness against noisy and spontaneous speech. We propose to use three augmentation methods and their combinations: 1) data augmentation using text-to-speech (TTS) data, 2) cycle-consistent generative adversarial network (Cycle-GAN) augmentation trained to map two different audio characteristics, the one of clean speech and of noisy recordings, to match the testing condition, and 3) pseudo-label augmentation provided by the pretrained ASR module for smoothing label distributions. Experimental results using the CHiME-6/CHiME-4 datasets show that each augmentation method individually improves the accuracy on top of the conventional SpecAugment; further improvements are obtained by combining these approaches. We achieved 4.3% word error rate (WER) reduction, which was more significant than that of the SpecAugment, when we combine all three augmentations for the CHiME-6 task",
    "keywords": [],
    "checked": true,
    "id": "77cd3ae8a0b9ef6865d5324a4d62280e6f7a1053",
    "semantic_title": "data augmentation methods for end-to-end speech recognition on distant-talk scenarios",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21_interspeech.html": {
    "title": "Leveraging Phone Mask Training for Phonetic-Reduction-Robust E2E Uyghur Speech Recognition",
    "volume": "main",
    "abstract": "In Uyghur speech, consonant and vowel reduction are often encountered, especially in spontaneous speech with high speech rate, which will cause a degradation of speech recognition performance. To solve this problem, we propose an effective phone mask training method for Conformer-based Uyghur end-to-end (E2E) speech recognition. The idea is to randomly mask off a certain percentage features of phones during model training, which simulates the above verbal phenomena and facilitates E2E model to learn more contextual information. According to experiments, the above issues can be greatly alleviated. In addition, deep investigations are carried out into different units in masking, which shows the effectiveness of our proposed masking unit. We also further study the masking method and optimize filling strategy of phone mask. Finally, compared with Conformer-based E2E baseline without mask training, our model demonstrates about 5.51% relative Word Error Rate (WER) reduction on reading speech and 12.92% on spontaneous speech, respectively. The above approach has also been verified on test-set of open-source data THUYG-20, which shows 20% relative improvements",
    "keywords": [],
    "checked": true,
    "id": "a090394ade7c1d094b268282e0872d5547e16c91",
    "semantic_title": "leveraging phone mask training for phonetic-reduction-robust e2e uyghur speech recognition",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/likhomanenko21_interspeech.html": {
    "title": "Rethinking Evaluation in ASR: Are Our Models Robust Enough?",
    "volume": "main",
    "abstract": "Is pushing numbers on a single benchmark valuable in automatic speech recognition? Research results in acoustic modeling are typically evaluated based on performance on a single dataset. While the research community has coalesced around various benchmarks, we set out to understand generalization performance in acoustic modeling across datasets — in particular, if models trained on a single dataset transfer to other (possibly out-of-domain) datasets. Further, we demonstrate that when a large enough set of benchmarks is used, average word error rate (WER) performance over them provides a good proxy for performance on real-world data. Finally, we show that training a single acoustic model on the most widely-used datasets — combined — reaches competitive performance on both research and real-world benchmarks",
    "keywords": [],
    "checked": true,
    "id": "62ce4d65335c32844247e0ecf3be6de1ccb924b2",
    "semantic_title": "rethinking evaluation in asr: are our models robust enough?",
    "citation_count": 76,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lam21_interspeech.html": {
    "title": "Raw Waveform Encoder with Multi-Scale Globally Attentive Locally Recurrent Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end speech recognition generally uses hand-engineered acoustic features as input and excludes the feature extraction module from its joint optimization. To extract learnable and adaptive features and mitigate information loss, we propose a new encoder that adopts globally attentive locally recurrent (GALR) networks and directly takes raw waveform as input. We observe improved ASR performance and robustness by applying GALR on different window lengths to aggregate fine-grain temporal information into multi-scale acoustic features. Experiments are conducted on a benchmark dataset and two large-scale Mandarin speech corpus of 5,000 hours and 21,000 hours. With faster speed and comparable model size, our proposed multi-scale GALR waveform encoder achieved consistent character error rate reductions (CERRs) from 7.9% to 28.1% relative over strong baselines, including Conformer and TDNN-Conformer. In particular, our approach demonstrated notable robustness than the traditional handcrafted features and outperformed the baseline MFCC-based TDNN-Conformer model by a 15.2% CERR on a music-mixed real-world speech test set",
    "keywords": [],
    "checked": true,
    "id": "0610eab8e6f454486006ff16b27aed975af9dccf",
    "semantic_title": "raw waveform encoder with multi-scale globally attentive locally recurrent networks for end-to-end speech recognition",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hou21_interspeech.html": {
    "title": "Attention-Based Cross-Modal Fusion for Audio-Visual Voice Activity Detection in Musical Video Streams",
    "volume": "main",
    "abstract": "Many previous audio-visual voice-related works focus on speech, ignoring the singing voice in the growing number of musical video streams on the Internet. For processing diverse musical video data, voice activity detection is a necessary step. This paper attempts to detect the speech and singing voices of target performers in musical video streams using audio-visual information. To integrate information of audio and visual modalities, a multi-branch network is proposed to learn audio and image representations, and the representations are fused by attention based on semantic similarity to shape the acoustic representations through the probability of anchor vocalization. Experiments show the proposed audio-visual multi-branch network far outperforms the audio-only model in challenging acoustic environments, indicating the cross-modal information fusion based on semantic correlation is sensible and successful",
    "keywords": [],
    "checked": true,
    "id": "508d61701998e2d22cf2ed53c1ca8b28f2cf39a0",
    "semantic_title": "attention-based cross-modal fusion for audio-visual voice activity detection in musical video streams",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21b_interspeech.html": {
    "title": "Noise-Tolerant Self-Supervised Learning for Audio-Visual Voice Activity Detection",
    "volume": "main",
    "abstract": "Recent audio-visual voice activity detectors based on supervised learning require large amounts of labeled training data with manual mouth-region cropping in videos, and the performance is sensitive to a mismatch between the training and testing noise conditions. This paper introduces contrastive self-supervised learning for audio-visual voice activity detection as a possible solution to such problems. In addition, a novel self-supervised learning framework is proposed to improve overall training efficiency and testing performance on noise-corrupted datasets, as in real-world scenarios. This framework includes a branched audio encoder and a noise-tolerant loss function to cope with the uncertainty of speech and noise feature separation in a self-supervised manner. Experimental results, particularly under mismatched noise conditions, demonstrate the improved performance compared with a self-supervised learning baseline and a supervised learning framework",
    "keywords": [],
    "checked": true,
    "id": "bdfeba36c6cac3b3033e72ec3345d619e8b1d1fd",
    "semantic_title": "noise-tolerant self-supervised learning for audio-visual voice activity detection",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/park21_interspeech.html": {
    "title": "Noisy Student-Teacher Training for Robust Keyword Spotting",
    "volume": "main",
    "abstract": "We propose self-training with noisy student-teacher approach for streaming keyword spotting, that can utilize large-scale unlabeled data and aggressive data augmentation. The proposed method applies aggressive data augmentation (spectral augmentation) on the input of both student and teacher and utilize unlabeled data at scale, which significantly boosts the accuracy of student against challenging conditions. Such aggressive augmentation usually degrades model performance when used with supervised training with hard-labeled data. Experiments show that aggressive spec augmentation on baseline supervised training method degrades accuracy, while the proposed self-training with noisy student-teacher training improves accuracy of some difficult-conditioned test sets by as much as 60%",
    "keywords": [],
    "checked": true,
    "id": "d81bfd0b57ab77a51aa2108e20861f1394629e96",
    "semantic_title": "noisy student-teacher training for robust keyword spotting",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ichikawa21_interspeech.html": {
    "title": "Multi-Channel VAD for Transcription of Group Discussion",
    "volume": "main",
    "abstract": "Attempts are being made to visualize the learning process by attaching microphones to students participating in group works conducted in classrooms, and subsequently, their speech using an automatic speech recognition (ASR) system. However, the voices of nearby students frequently become mixed with the output speech data, even when using close-talk microphones with noise robustness. To resolve this challenge, in this paper, we propose using multi-channel voice activity detection (VAD) to determine the speech segments of a target speaker while also referencing the output speech from the microphones attached to the other speakers in the group. The conducted evaluation experiments using the actual speech of middle school students during group work lessons showed that our proposed method significantly improves the frame error rate (38.7%) compared to that of the conventional technology, single-channel VAD (49.5%). In our view, conventional approaches, such as distributed microphone arrays and deep learning, are somewhat dependent on the temporal stationarity of the speakers' positions. However, the proposed method is essentially a VAD process and thus works robustly. It is the practical and proven solution in a real classroom environment",
    "keywords": [],
    "checked": true,
    "id": "6b1c9a7316501307e88c04af959e726af09c8ef9",
    "semantic_title": "multi-channel vad for transcription of group discussion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21_interspeech.html": {
    "title": "Audio-Visual Information Fusion Using Cross-Modal Teacher-Student Learning for Voice Activity Detection in Realistic Environments",
    "volume": "main",
    "abstract": "We propose an information fusion approach to audio-visual voice activity detection (AV-VAD) based on cross-modal teacher-student learning leveraging on factorized bilinear pooling (FBP) and Kullback-Leibler (KL) regularization. First, we design an audio-visual network by using FBP fusion to fully utilize the interaction between audio and video modalities. Next, to transfer the rich information in audio-based VAD (A-VAD) model trained with a massive audio-only dataset to AV-VAD model built with relatively limited multi-modal data, a cross-modal teacher-student learning framework is then proposed based on cross entropy with regulated KL-divergence. Finally, evaluated on an in-house dataset recorded in realistic conditions using standard VAD metrics, the proposed approach yields consistent and significant improvements over other state-of-the-art techniques. Moreover, by applying our AV-VAD technique to an audio-visual Chinese speech recognition task, the character error rate is reduced by 24.15% and 8.66% from A-VAD and the baseline AV-VAD systems, respectively",
    "keywords": [],
    "checked": true,
    "id": "36878855de3551d0dce4db978e3d87fb7483296f",
    "semantic_title": "audio-visual information fusion using cross-modal teacher-student learning for voice activity detection in realistic environments",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/makishima21_interspeech.html": {
    "title": "Enrollment-Less Training for Personalized Voice Activity Detection",
    "volume": "main",
    "abstract": "We present a novel personalized voice activity detection (PVAD) learning method that does not require enrollment data during training. PVAD is a task to detect the speech segments of a specific target speaker at the frame level using enrollment speech of the target speaker. Since PVAD must learn speakers' speech variations to clarify the boundary between speakers, studies on PVAD used large-scale datasets that contain many utterances for each speaker. However, the datasets to train a PVAD model are often limited because substantial cost is needed to prepare such a dataset. In addition, we cannot utilize the datasets used to train the standard VAD because they often lack speaker labels. To solve these problems, our key idea is to use one utterance as both a kind of enrollment speech and an input to the PVAD during training, which enables PVAD training without enrollment speech. In our proposed method, called enrollment-less training, we augment one utterance so as to create variability between the input and the enrollment speech while keeping the speaker identity, which avoids the mismatch between training and inference. Our experimental results demonstrate the efficacy of the method",
    "keywords": [],
    "checked": true,
    "id": "d6c44c2faa653eef801ffbd51292bcfa23135072",
    "semantic_title": "enrollment-less training for personalized voice activity detection",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nonaka21_interspeech.html": {
    "title": "Voice Activity Detection for Live Speech of Baseball Game Based on Tandem Connection with Speech/Noise Separation Model",
    "volume": "main",
    "abstract": "When applying voice activity detection (VAD) to a noisy sound, in general, noise reduction (speech separation) and VAD are performed separately. In this case, the noise reduction may suppress the speech, and the VAD may not work well for the speech after the noise reduction. This study proposes a VAD model through the tandem connection of neural network-based noise separation and a VAD model. By training the two models simultaneously, the noise separation model is expected to be trained to consider the VAD results, and thus effective noise separation can be achieved. Moreover, the improved speech/noise separation model will improve the accuracy of the VAD model. In this research, we deal with real-live speeches from baseball games, which have a very poor signal-to-noise ratio. The VAD experiments showed that the VAD performance at the frame level achieved 4.2 points improvement in F1-score by tandemly connecting the speech/noise separation model and the VAD model",
    "keywords": [],
    "checked": true,
    "id": "10dbd2366917c7ef826fb8255412ee8747163560",
    "semantic_title": "voice activity detection for live speech of baseball game based on tandem connection with speech/noise separation model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kwon21_interspeech.html": {
    "title": "FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications",
    "volume": "main",
    "abstract": "Various incremental learning (IL) approaches have been proposed to help deep learning models learn new tasks/classes continuously without forgetting what was learned previously (i.e., avoid catastrophic forgetting). With the growing number of deployed audio sensing applications that need to dynamically incorporate new tasks and changing input distribution from users, the ability of IL on-device becomes essential for both efficiency and user privacy However, prior works suffer from high computational costs and storage demands which hinders the deployment of IL on-device. In this work, to overcome these limitations, we develop an end-to-end and on-device IL framework, FastICARL, that incorporates an exemplar-based IL and quantization in the context of audio-based applications. We first employ k-nearest-neighbor to reduce the latency of IL. Then, we jointly utilize a quantization technique to decrease the storage requirements of IL. We implement FastICARL on two types of mobile devices and demonstrate that FastICARL remarkably decreases the IL time up to 78–92% and the storage requirements by 2–4 times without sacrificing its performance. FastICARL enables complete on-device IL, ensuring user privacy as the user data does not need to leave the device",
    "keywords": [],
    "checked": true,
    "id": "92faea491cfb376477073aa4a070fa0973bba032",
    "semantic_title": "fasticarl: fast incremental classifier and representation learning with efficient budget allocation in audio sensing applications",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wei21_interspeech.html": {
    "title": "End-to-End Transformer-Based Open-Vocabulary Keyword Spotting with Location-Guided Local Attention",
    "volume": "main",
    "abstract": "Open-vocabulary keyword spotting (KWS) aims to detect arbitrary keywords from continuous speech, which allows users to define their personal keywords. In this paper, we propose a novel location guided end-to-end (E2E) keyword spotting system. Firstly, we predict endpoints of keyword in the entire speech based on attention mechanism. Secondly, we calculate the existence probability of keyword by fusing the located keyword speech segment and text with local attention. The results on Librispeech dataset and Google speech commands dataset show our proposed method significantly outperforms the baseline method and the latest small-footprint E2E KWS method",
    "keywords": [],
    "checked": true,
    "id": "ce14a649fcf10aeaf9eca0ed7d2dcfff1383e979",
    "semantic_title": "end-to-end transformer-based open-vocabulary keyword spotting with location-guided local attention",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bhati21_interspeech.html": {
    "title": "Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation",
    "volume": "main",
    "abstract": "Automatic detection of phoneme or word-like units is one of the core objectives in zero-resource speech processing. Recent attempts employ self-supervised training methods, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework that can model the signal structure at a higher level e.g. at the phoneme level. In this framework, a convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Typically, phoneme and word segmentation are treated as separate tasks. We unify them and experimentally show that our single model outperforms existing phoneme and word segmentation methods on TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and when is the right time to include the segmental loss in the learning process",
    "keywords": [],
    "checked": true,
    "id": "642dab29e680f516eb25949d616a24e0ad147a19",
    "semantic_title": "segmental contrastive predictive coding for unsupervised word segmentation",
    "citation_count": 34,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21b_interspeech.html": {
    "title": "A Lightweight Framework for Online Voice Activity Detection in the Wild",
    "volume": "main",
    "abstract": "Voice activity detection (VAD) is an essential pre-processing component for speech-related tasks such as automatic speech recognition (ASR). Traditional VAD systems require strong frame-level supervision for training, inhibiting their performance in real-world test scenarios. Previously, the general-purpose VAD (GPVAD) framework has been proposed to enhance noise robustness significantly. However, GPVAD models are comparatively large and only work for offline evaluation. This work proposes the use of a knowledge distillation framework, where a (large, offline) teacher model provides frame-level supervision to a (light, online) student model. Our experiments verify that our proposed lightweight student models outperform GPVAD on all test sets, including clean, synthetic and real-world scenarios. Our smallest student model only uses 2.2% of the parameters and 15.9% duration cost of our teacher model for inference when evaluated on a Raspberry Pi",
    "keywords": [],
    "checked": true,
    "id": "a4557f8c519bdc765c00124cc86736e9c359eb77",
    "semantic_title": "a lightweight framework for online voice activity detection in the wild",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chlebowski21_interspeech.html": {
    "title": "See what I mean, huh?\" Evaluating Visual Inspection of F0 Tracking in Nasal Grunts",
    "volume": "main",
    "abstract": "This paper proposes to evaluate the method used in Chlébowski and Ballier [1] for the annotation of F variations in nasal grunts. We discuss and test issues raised by this kind of approach exclusively based on visual inspection of the F tracking in [2]. Results tend to show that consistency in the annotation depends on acoustic features intrinsic to the grunts such as F slope and duration that are sensitive to display settings. We nonetheless acknowledge the potential benefits of such a method for automation and implementation in IA and in this respect, we introduce [3] as an alternative material-maker",
    "keywords": [],
    "checked": true,
    "id": "50b077c4c2e04dd22acd939a16266c0dc10b348f",
    "semantic_title": "see what i mean, huh?\" evaluating visual inspection of f0 tracking in nasal grunts",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21c_interspeech.html": {
    "title": "System Performance as a Function of Calibration Methods, Sample Size and Sampling Variability in Likelihood Ratio-Based Forensic Voice Comparison",
    "volume": "main",
    "abstract": "In data-driven forensic voice comparison, sample size is an issue which can have substantial effects on system output. Numerous calibration methods have been developed and some have been proposed as solutions to sample size issues. In this paper, we test four calibration methods (i.e. logistic regression, regularised logistic regression, Bayesian model, ELUB) under different conditions of sampling variability and sample size. Training and test scores were simulated from skewed distributions derived from real experiments, increasing sample sizes from 20 to 100 speakers for both the training and test sets. For each sample size, the experiments were replicated 100 times to test the susceptibility of different calibration methods to sampling variability. The C mean and range across replications were used for evaluation. The Bayesian model and regularized logistic regression produced the most stable C values when the sample size is small (i.e. 20 speakers), although mean C is consistently lowest using logistic regression. The ELUB calibration method generally is the least preferred as it is the most sensitive to sample size and sampling variability (mean = 0.66, range = 0.21–0.59)",
    "keywords": [],
    "checked": true,
    "id": "51bee70b4a53baa635f8b4fdd523e62410ce924c",
    "semantic_title": "system performance as a function of calibration methods, sample size and sampling variability in likelihood ratio-based forensic voice comparison",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bonneau21_interspeech.html": {
    "title": "Voicing Assimilations by French Speakers of German in Stop-Fricative Sequences",
    "volume": "main",
    "abstract": "Voicing assimilations inside groups of obstruents occur in opposite directions in French and German, where they are respectively regressive and progressive. The aim of the study is to investigate (1) whether non native speakers (here French learners of German) are apt to acquire subtle L2 specificities like assimilation direction, although they are not aware of their very existence, or (2) whether their productions depend essentially upon other factors, in particular consonant place of articulation. To that purpose, a corpus made up of groups of obstruents (/t/ followed by /z/, /v/ or /f/) embedded into sentences has been recorded by 16 French learners of German (beginners and advanced speakers). The consonants are separated by a word or a syllable boundary. Results, derived from the analysis of consonant periodicity and duration, do not stand for an acquisition of progressive assimilation, even by advanced speakers, and do not show differences between the productions of advanced speakers and beginners. On the contrary the boundary type and the consonant place of articulation play an important role in the presence or absence of voicing inside obstruent groups. The role of phonetic, universal mechanisms against linguistic specific rules is discussed to interpret the data",
    "keywords": [],
    "checked": true,
    "id": "6d4c67542f20ff95ce6aa41ff9c71f6bbab903d5",
    "semantic_title": "voicing assimilations by french speakers of german in stop-fricative sequences",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chakraborty21_interspeech.html": {
    "title": "The Four-Way Classification of Stops with Voicing and Aspiration for Non-Native Speech Evaluation",
    "volume": "main",
    "abstract": "The four-way distinction of plosives in terms of voicing and aspiration is rare in the world's languages, but is an important characteristic of the Indo-Aryan language family. Both perception and production pose challenges to the language learner whose native tongue does not afford the specific distinctions. A study of the acoustic-phonetics of the sounds and their possible dependence on speaker characteristics, such as gender or native tongue, can inform methods for accurate feedback on the quality of the phones produced by a non-native learner. We present a system for the four-way classification of stops building on features previously proposed for aspiration detection in unvoiced and voiced plosives. Trained on an available dataset of Hindi speech by native speakers, the system works reliably on production data comprising Bangla words uttered by native Bangla and non-native (American English L1) speakers. The latter display a variety of articulation patterns for the given target contrasts, providing useful insights related to L1 influence on the voicing-aspiration production in word-initial CV contexts",
    "keywords": [],
    "checked": true,
    "id": "b6f2a10ed88619aef50ae19dc486cbc3058f5b8f",
    "semantic_title": "the four-way classification of stops with voicing and aspiration for non-native speech evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/urooj21_interspeech.html": {
    "title": "Acoustic and Prosodic Correlates of Emotions in Urdu Speech",
    "volume": "main",
    "abstract": "Emotional speech corpora exhibit differences in duration, intensity and fundamental frequency. We investigated acoustic as well as prosodic correlates of emotional speech in Urdu. We recorded a corpus of 23 sentences from four speakers of Urdu covering four emotional states. Main results show that: a) sadness exhibits lowest utterance rate, lowest intensity and narrow pitch range, b) anger exhibits highest utterance rate, highest intensity and wider pitch range, and c) happiness exhibits higher utterance rate and wider pitch range as compared to neutral and sadness; but no significant differences are found between the intensity and pitch range of anger and happiness. The analysis also shows differences in terms of pitch or phrase accents and boundary tones",
    "keywords": [],
    "checked": true,
    "id": "634c84a5806bd95455bb11938238a07628c895aa",
    "semantic_title": "acoustic and prosodic correlates of emotions in urdu speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tamim21_interspeech.html": {
    "title": "Voicing Contrasts in the Singleton Stops of Palestinian Arabic: Production and Perception",
    "volume": "main",
    "abstract": "This study investigates the stop voicing contrast in Palestinian Arabic (PA) by examining Voice Onset Time (VOT) in both production and perception. An acoustic analysis of the recordings of 8 speakers showed that word-initial voiced stops in sentence context have an average VOT of -93 msec, and word-initial voiceless stops one of 29 msec. PA thus belongs, like most dialects of Arabic, to true voicing languages, i.e., languages with a contrast between voicing lead and short lag VOT We furthermore tested whether the phoneme /b/, without voiceless counterpart /p/ in PA, has similar VOT values to /d, d /, which have voiceless counterparts /t, t /. Similarly, we compared /k/, without counterpart /g/ in the PA dialect we investigated, to /t, t /. For /b/ we found very similar VOT values to /d, d /, while for /k/ we found a difference to /t, t /, attributable to a general tendency of velars to have longer VOT than denti-alveolars. We thus found no evidence for a less contrastive realization of unpaired plosives in PA In a categorization experiment of the denti-alveolar phoneme pairs with the same 8 speakers, VOT proved sufficient as a perceptual cue, though f0 of the following vowel also influenced the categorization",
    "keywords": [],
    "checked": true,
    "id": "e1f3460fc7dc182432405a26643ecc96603cfdc4",
    "semantic_title": "voicing contrasts in the singleton stops of palestinian arabic: production and perception",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/coy21_interspeech.html": {
    "title": "A Comparison of the Accuracy of Dissen and Keshet's (2016) DeepFormants and Traditional LPC Methods for Semi-Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "There is a growing trend in the field of forensic speech science towards integrating the vanguard of speech technology with traditional linguistic methods in pursuit of both scalable (i.e. automatable) and accurate evidential methods. To this end, this paper investigates DeepFormants, a DNN formant estimator which its creators, Dissen and Keshet [1], claim constitutes an accurate tool ready for use by linguists. In the present paper, DeepFormants is integrated into semi-automatic speaker recognition systems using long-term formant distributions and compared against systems using traditional linear predictive coding. The readiness of the tool is assessed on overall speaker recognition performance, measured using equal error rates (EER) and the log LR cost functions (C ). In high-quality conditions, DeepFormants outperforms the best performing LPC systems. Much poorer overall performance is found in channel mismatch conditions for DeepFormants, suggesting it is not adaptable to conditions it was not originally trained on. However, this is also true of LPC methods, raising questions over the validity of using formant analysis at all in such cases. A major benefit of DeepFormants over LPC is that the analyst does not need to specify settings. We discuss the implications of this with regard to results for individual speakers",
    "keywords": [],
    "checked": true,
    "id": "03415b1cfdb19bff136be84559da5e807187a305",
    "semantic_title": "a comparison of the accuracy of dissen and keshet's (2016) deepformants and traditional lpc methods for semi-automatic speaker recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jessen21_interspeech.html": {
    "title": "MAP Adaptation Characteristics in Forensic Long-Term Formant Analysis",
    "volume": "main",
    "abstract": "Forensic data from long-term formant analysis were used as input to the GMM-UBM approach, which is a way of deriving Likelihood Ratios. Tests were performed running 22 same-speaker comparisons and 462 different-speaker comparisons from a corpus of anonymized casework data involving telephone-intercepted speech. In a first series of tests, the number of Gaussian modules for GMM-modeling was increased from 1 to 32. In a second series of tests the duration of formant input in the compared files was reduced from 10 seconds to 5 and then to 2.5. All tests were performed both without and with the use of MAP adaptation. Results were evaluated in terms of overall performance characteristics EER and Cllr and in terms of score distributions visualized as Tippett plots. The main goal of the study was to compare the use and non-use of MAP and to look at the practical forensic implications of the difference. Results show that in terms of overall performance characteristics there is little difference between the selection and de-selection of MAP. Tippett plot patterns however reveal strong differences. Application of MAP allows for more symmetric same- and different-speaker distributions and shows more robustness against duration reductions, both of which are forensically important",
    "keywords": [],
    "checked": true,
    "id": "2f2a4f02f9c5fe54ad2f99dfec14028b3f13b473",
    "semantic_title": "map adaptation characteristics in forensic long-term formant analysis",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lo21_interspeech.html": {
    "title": "Cross-Linguistic Speaker Individuality of Long-Term Formant Distributions: Phonetic and Forensic Perspectives",
    "volume": "main",
    "abstract": "This study considers issues of language- and speaker-specificity in long-term formant distributions (LTFDs) from phonetic and forensic perspectives and examines their potential value in cases of cross-language forensic voice comparison. Acoustic analysis of 60 male English–French bilinguals revealed systematic differences in LTFDs between the two languages, with higher LTF2–4 in French than in English. Cross-linguistic differences in the shapes of LTFDs were also found. These differences are argued to reflect not only vowel inventories of each language but also language-specific phonetic settings. At the same time, a high degree of within-speaker consistency was found across languages. Likelihood ratio based testing was carried out to examine the effect of language mismatch on the utility of LTFDs as speaker discriminants. Results showed that while the performance of LTFDs was worse in cross-language comparisons than in same-language comparisons, they were still capable of providing speaker-specific information. These findings demonstrate that, in spite of deteriorated performance, LTFDs are still potentially useful speaker discriminants in cases of language mismatch. These findings thus call for further empirical investigation into the use of linguistic-phonetic features in cross-language comparisons",
    "keywords": [],
    "checked": true,
    "id": "7f2ddbb9a0184e81b9252d5b81538bc41c015040",
    "semantic_title": "cross-linguistic speaker individuality of long-term formant distributions: phonetic and forensic perspectives",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/soo21_interspeech.html": {
    "title": "Sound Change in Spontaneous Bilingual Speech: A Corpus Study on the Cantonese n-l Merger in Cantonese-English Bilinguals",
    "volume": "main",
    "abstract": "In Cantonese and several other Chinese languages, /n/ is merging with /l/. The Cantonese merger appears categorical, with /n/ becoming /l/ word-initially. This project aims to describe the status of /n/ and /l/ in bilingual Cantonese and English speech to better understand individual differences at the interface of crosslinguistic influence and sound change. We examine bilingual speech using the SpiCE corpus, composed of speech from 34 early Cantonese-English bilinguals. Acoustic measures were collected on pre-vocalic nasal and lateral onsets in both languages. If bilinguals maintain separate representations for corresponding segments across languages, smaller differences between /n/ and /l/ are predicted in Cantonese compared to English. Measures of mid-frequency spectral tilt suggest that the /n/ and /l/ contrast is robustly maintained in English, but not Cantonese. The spacing of F2-F1 suggests small differences between Cantonese /n/ and /l/, and robust differences in English. While cross-language categories appear independent, substantial individual differences exist in the data. These data contribute to the understanding of the /n/ and /l/ merger in Cantonese and other Chinese languages, in addition to providing empirical and theoretical insights into crosslinguistic influence in early bilinguals",
    "keywords": [],
    "checked": true,
    "id": "33e65d11709fee658352706e51b3f91efde51710",
    "semantic_title": "sound change in spontaneous bilingual speech: a corpus study on the cantonese n-l merger in cantonese-english bilinguals",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lalhminghlui21_interspeech.html": {
    "title": "Characterizing Voiced and Voiceless Nasals in Mizo",
    "volume": "main",
    "abstract": "Mizo has voicing contrasts in nasals. This study investigates the acoustic properties of Mizo voiced and voiceless nasals using nasometric measurements. The dual channel data obtained for Mizo nasals is separated into oral and nasal channels and nasalance is calculated at every 10% of the duration of the nasals. Apart from that, the amount of voicing and duration of the nasals are also measured. The results show that nasalance is affected by the place of articulation of the nasals. Additionally, the voiceless nasals are found to be significantly longer than the voiced nasals",
    "keywords": [],
    "checked": true,
    "id": "63040a44e70d2c4ecbd62ca15567ece42c26dbbe",
    "semantic_title": "characterizing voiced and voiceless nasals in mizo",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schuller21_interspeech.html": {
    "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation & Primates",
    "volume": "main",
    "abstract": "The INTERSPEECH 2021 Computational Paralinguistics Challenge addresses four different problems for the first time in a research competition under well-defined conditions: In the and Sub-Challenges, a binary classification on COVID-19 infection has to be made based on coughing sounds and speech; in the Sub-Challenge, a three-way assessment of the level of escalation in a dialogue is featured; and in the Sub-Challenge, four species vs background need to be classified. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the ‘usual' ComParE and BoAW features as well as deep unsupervised representation learning using the auDeep toolkit, and deep feature extraction from pre-trained CNNs using the Deep Spectrum toolkit; in addition, we add deep end-to-end sequential modelling, and partially linguistic analysis",
    "keywords": [],
    "checked": true,
    "id": "12267f4ee5fafc807b793dc09dd8a5b8cee8115e",
    "semantic_title": "the interspeech 2021 computational paralinguistics challenge: covid-19 cough, covid-19 speech, escalation & primates",
    "citation_count": 96,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/soleraurena21_interspeech.html": {
    "title": "Transfer Learning-Based Cough Representations for Automatic Detection of COVID-19",
    "volume": "main",
    "abstract": "In the last months, there has been an increasing interest in developing reliable, cost-effective, immediate and easy to use machine learning based tools that can help health care operators, institutions, companies, etc. to optimize their screening campaigns. In this line, several initiatives emerged aimed at the automatic detection of COVID-19 from speech, breathing and coughs, with inconclusive preliminary results. The ComParE 2021 COVID-19 Cough Sub-challenge provides researchers from all over the world a suitable test-bed for the evaluation and comparison of their work. In this paper, we present the INESC-ID contribution to the ComParE 2021 COVID-19 Cough Sub-challenge. We leverage transfer learning to develop a set of three expert classifiers based on deep cough representation extractors. A calibrated decision-level fusion system provides the final classification of coughs recordings as either COVID-19 positive or negative. Results show unweighted average recalls of 72.3% and 69.3% in the development and test sets, respectively. Overall, the experimental assessment shows the potential of this approach although much more research on extended respiratory sounds datasets is needed",
    "keywords": [],
    "checked": true,
    "id": "287547a8cf4c372b975535222caabf03202bcc32",
    "semantic_title": "transfer learning-based cough representations for automatic detection of covid-19",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/klumpp21_interspeech.html": {
    "title": "The Phonetic Footprint of Covid-19?",
    "volume": "main",
    "abstract": "Against the background of the ongoing pandemic, this year's Computational Paralinguistics Challenge featured a classification problem to detect Covid-19 from speech recordings. The presented approach is based on a phonetic analysis of speech samples, thus it enabled us not only to discriminate between Covid and non-Covid samples, but also to better understand how the condition influenced an individual's speech signal Our deep acoustic model was trained with datasets collected exclusively from healthy speakers. It served as a tool for segmentation and feature extraction on the samples from the challenge dataset. Distinct patterns were found in the embeddings of phonetic classes that have their place of articulation deep inside the vocal tract. We observed profound differences in classification results for development and test splits, similar to the baseline method We concluded that, based on our phonetic findings, it was safe to assume that our classifier was able to reliably detect a pathological condition located in the respiratory tract. However, we found no evidence to claim that the system was able to discriminate between Covid-19 and other respiratory diseases",
    "keywords": [],
    "checked": true,
    "id": "608d76a77413cf7e1e47a926f9838b6fa5cba32d",
    "semantic_title": "the phonetic footprint of covid-19?",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/casanova21_interspeech.html": {
    "title": "Transfer Learning and Data Augmentation Techniques to the COVID-19 Identification Tasks in ComParE 2021",
    "volume": "main",
    "abstract": "In this work, we propose several techniques to address data scarceness in ComParE 2021 COVID-19 identification tasks for the application of deep models such as Convolutional Neural Networks. Data is initially preprocessed into spectrogram or MFCC-gram formats. After preprocessing, we combine three different data augmentation techniques to be applied in model training. Then we employ transfer learning techniques from pretrained audio neural networks. Those techniques are applied to several distinct neural architectures. For COVID-19 identification in speech segments, we obtained competitive results. On the other hand, in the identification task based on cough data, we succeeded in producing a noticeable improvement on existing baselines, reaching 75.9% unweighted average recall (UAR)",
    "keywords": [],
    "checked": true,
    "id": "9a4d3eaf07bd3b90a055f7ca53a07de6683d8291",
    "semantic_title": "transfer learning and data augmentation techniques to the covid-19 identification tasks in compare 2021",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/illium21_interspeech.html": {
    "title": "Visual Transformers for Primates Classification and Covid Detection",
    "volume": "main",
    "abstract": "We apply the vision transformer, a deep machine learning model build around the attention mechanism, on mel-spectrogram representations of raw audio recordings. When adding mel-based data augmentation techniques and sample-weighting, we achieve comparable performance on both (PRS and CCS challenge) tasks of ComParE21, outperforming most single model baselines. We further introduce overlapping vertical patching and evaluate the influence of parameter configurations",
    "keywords": [],
    "checked": true,
    "id": "65fe63a40f54f3c4d231f10f351001a27c6151c5",
    "semantic_title": "visual transformers for primates classification and covid detection",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pellegrini21_interspeech.html": {
    "title": "Deep-Learning-Based Central African Primate Species Classification with MixUp and SpecAugment",
    "volume": "main",
    "abstract": "In this paper, we report experiments in which we aim to automatically classify primate vocalizations according to four primate species of interest, plus a background category with forest sound events. We compare several standard deep neural networks architectures: standard deep convolutional neural networks (CNNs), MobileNets and ResNets. To tackle the small size of the training dataset, less than seven thousand audio files, the data augmentation techniques SpecAugment and MixUp proved to be very useful. Against the very unbalanced classes of the dataset, we used a balanced data sampler that showed to be efficient. An exponential moving average of the model weights allowed to get slight further gains. The best model was a standard 10-layer CNN, comprised of about five million parameters. It achieved a 93.6% Unweighted Average Recall (UAR) on the development set, and generalized well on the test set with a 92.5% UAR, outperforming an official baseline of 86.6%. We quantify the performance gains brought by the augmentations and training tricks, and report fusion and classification experiments based on embeddings that did not bring better results",
    "keywords": [],
    "checked": true,
    "id": "e343a2d6792e3739a9c0de9857e34b7eb59b72cc",
    "semantic_title": "deep-learning-based central african primate species classification with mixup and specaugment",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muller21_interspeech.html": {
    "title": "A Deep and Recurrent Architecture for Primate Vocalization Classification",
    "volume": "main",
    "abstract": "Wildlife monitoring is an essential part of most conservation efforts where one of the many building blocks is acoustic monitoring. Acoustic monitoring has the advantage of being non-invasive and applicable in areas of high vegetation. In this work, we present a deep and recurrent architecture for the classification of primate vocalizations that is based upon well proven modules such as bidirectional Long Short-Term Memory neural networks, pooling, normalized softmax and focal loss. Additionally, we apply Bayesian optimization to obtain a suitable set of hyperparameters. We test our approach on a recently published dataset of primate vocalizations that were recorded in an African wildlife sanctuary. Using an ensemble of the best five models found during hyperparameter optimization on the development set, we achieve a Unweighted Average Recall of 89.3% on the test set. Our approach outperforms the best baseline, an ensemble of various deep and shallow classifiers, which achieves a UAR of 87.5%",
    "keywords": [],
    "checked": true,
    "id": "d4008aeffed6e875397c94433cbee47595e0ae75",
    "semantic_title": "a deep and recurrent architecture for primate vocalization classification",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zwerts21_interspeech.html": {
    "title": "Introducing a Central African Primate Vocalisation Dataset for Automated Species Classification",
    "volume": "main",
    "abstract": "Automated classification of animal vocalisations is a potentially powerful wildlife monitoring tool. Training robust classifiers requires sizable annotated datasets, which are not easily recorded in the wild. To circumvent this problem, we recorded four primate species under semi-natural conditions in a wildlife sanctuary in Cameroon with the objective to train a classifier capable of detecting species in the wild. Here, we introduce the collected dataset, describe our approach and initial results of classifier development. To increase the efficiency of the annotation process, we condensed the recordings with an energy/change based automatic vocalisation detection. Segmenting the annotated chunks into training, validation and test sets, initial results reveal up to 82% unweighted average recall test set performance in four-class primate species classification",
    "keywords": [],
    "checked": true,
    "id": "98a1034c5c0d30f7c83d3a637aa36de02c58e64f",
    "semantic_title": "introducing a central african primate vocalisation dataset for automated species classification",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rizos21_interspeech.html": {
    "title": "Multi-Attentive Detection of the Spider Monkey Whinny in the (Actual) Wild",
    "volume": "main",
    "abstract": "We study deep bioacoustic event detection through multi-head attention based pooling, exemplified by wildlife monitoring. In the multiple instance learning framework, a core deep neural network learns a projection of the input acoustic signal into a sequence of embeddings, each representing a segment of the input. Sequence pooling is then required to aggregate the information present in the sequence such that we have a single clip-wise representation. We propose an improvement based on Squeeze-and-Excitation mechanisms upon a recently proposed audio tagging ResNet, and show that it performs significantly better than the baseline, as well as a collection of other recent audio models. We then further enhance our model, by performing an extensive comparative study of recent sequence pooling mechanisms, and achieve our best result using multi-head self-attention followed by concatenation of the head-specific pooled embeddings — better than prediction pooling methods, as well as compared to other recent sequence pooling tricks. We perform these experiments on a novel dataset of spider monkey whinny calls we introduce here, recorded in a rainforest in the South-Pacific coast of Costa Rica, with a promising outlook pertaining to minimally invasive wildlife monitoring",
    "keywords": [],
    "checked": true,
    "id": "e472f18d522b9c49f006859ac0caf51072ad1d11",
    "semantic_title": "multi-attentive detection of the spider monkey whinny in the (actual) wild",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/egaslopez21_interspeech.html": {
    "title": "Identifying Conflict Escalation and Primates by Using Ensemble X-Vectors and Fisher Vector Features",
    "volume": "main",
    "abstract": "Computational paralinguistics is concerned with the automatic identification of non-verbal information in human speech. The Interspeech ComParE challenge features new paralinguistic tasks each year; this time, among others, a cross-corpus conflict escalation task and the identification of primates based solely on audio are the actual problems set. In our entry to ComParE 2021, we utilize x-vectors and Fisher vectors as features. To improve the robustness of the predictions, we also experiment with building an ensemble of classifiers from the x-vectors. Lastly, we exploit the fact that the Escalation Sub-Challenge is a conflict detection task, and incorporate the SSPNet Conflict Corpus in our training workflow. Using these approaches, at the time of writing, we had already surpassed the official Challenge baselines on both tasks, which demonstrates the efficiency of the employed techniques",
    "keywords": [],
    "checked": true,
    "id": "f65af2bb8200779b9c09bf7be5567d713b756803",
    "semantic_title": "identifying conflict escalation and primates by using ensemble x-vectors and fisher vector features",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/verkholyak21_interspeech.html": {
    "title": "Ensemble-Within-Ensemble Classification for Escalation Prediction from Speech",
    "volume": "main",
    "abstract": "Conflict situations arise frequently in our daily life and often require timely response to resolve the issues. In order to automatically classify conflict (also referred to as escalation) speech utterances we propose ensemble learning as it improves prediction performance by combining several heterogeneous models that compensate for each other's weaknesses. However, the effectiveness of the classification ensemble greatly depends on its constituents and their fusion strategy. This paper provides experimental evidence for effectiveness of different prediction-level fusion strategies and demonstrates the performance of each proposed ensemble on the Escalation Sub-Challenge (ESS) in the framework of the Computational Paralinguistics Challenge (ComParE-2021). The ensembles comprise various machine learning approaches based on acoustic and linguistic characteristics of speech. The training strategy is specifically designed to increase the generalization performance on the unseen data, while the diverse nature of ensemble candidates ensures high prediction power and accurate classification",
    "keywords": [],
    "checked": true,
    "id": "9dc936b03cae158437d844e152d01fde3641c0d5",
    "semantic_title": "ensemble-within-ensemble classification for escalation prediction from speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schiller21_interspeech.html": {
    "title": "Analysis by Synthesis: Using an Expressive TTS Model as Feature Extractor for Paralinguistic Speech Classification",
    "volume": "main",
    "abstract": "Modeling adequate features of speech prosody is one key factor to good performance in affective speech classification. However, the distinction between the prosody that is induced by ‘how' something is said (i.e., affective prosody) and the prosody that is induced by ‘what' is being said (i.e., linguistic prosody) is neglected in state-of-the-art feature extraction systems. This results in high variability of the calculated feature values for different sentences that are spoken with the same affective intent, which might negatively impact the performance of the classification. While this distinction between different prosody types is mostly neglected in affective speech recognition, it is explicitly modeled in expressive speech synthesis to create controlled prosodic variation. In this work, we use the expressive Text-To-Speech model Global Style Token Tacotron to extract features for a speech analysis task. We show that the learned prosodic representations outperform state-of-the-art feature extraction systems in the exemplary use case of Escalation Level Classification",
    "keywords": [],
    "checked": true,
    "id": "3dd293025a3b66ae924067bb29ebee319dda55cc",
    "semantic_title": "analysis by synthesis: using an expressive tts model as feature extractor for paralinguistic speech classification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/christensen21_interspeech.html": {
    "title": "Towards Automatic Speech Recognition for People with Atypical Speech",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": true,
    "id": "34b94c4c71f0c2de77bfbaab5d9ecf2ba261f85b",
    "semantic_title": "towards automatic speech recognition for people with atypical speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luu21_interspeech.html": {
    "title": "Leveraging Speaker Attribute Information Using Multi Task Learning for Speaker Verification and Diarization",
    "volume": "main",
    "abstract": "Deep speaker embeddings have become the leading method for encoding speaker identity in speaker recognition tasks. The embedding space should ideally capture the variations between all possible speakers, encoding the multiple acoustic aspects that make up a speaker's identity, whilst being robust to non-speaker acoustic variation. Deep speaker embeddings are normally trained discriminatively, predicting speaker identity labels on the training data. We hypothesise that additionally predicting speaker-related auxiliary variables — such as age and nationality — may yield representations that are better able to generalise to unseen speakers. We propose a framework for making use of auxiliary label information, even when it is only available for speech corpora mismatched to the target application. On a test set of US Supreme Court recordings, we show that by leveraging two additional forms of speaker attribute information derived respectively from the matched training data, and VoxCeleb corpus, we improve the performance of our deep speaker embeddings for both verification and diarization tasks, achieving a relative improvement of 26.2% in DER and 6.7% in EER compared to baselines using speaker labels only. This improvement is obtained despite the auxiliary labels having been scraped from the web and being potentially noisy",
    "keywords": [],
    "checked": true,
    "id": "bfd81c5ec7dc56872f7ca20a137f660bc0a2cce7",
    "semantic_title": "leveraging speaker attribute information using multi task learning for speaker verification and diarization",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rybicka21_interspeech.html": {
    "title": "Spine2Net: SpineNet with Res2Net and Time-Squeeze-and-Excitation Blocks for Speaker Recognition",
    "volume": "main",
    "abstract": "Modeling speaker embeddings using deep neural networks is currently state-of-the-art in speaker recognition. Recently, ResNet-based structures have gained a broader interest, slowly becoming the baseline along with the deep-rooted Time Delay Neural Network based models. However, the scale-decreased design of the ResNet models may not preserve all of the speaker information. In this paper, we investigate the SpineNet structure with scale-permuted design to tackle this problem, in which feature size either increases or decreases depending on the processing stage in the network. Apart from the presented adjustments of the SpineNet model for the speaker recognition task, we also incorporate popular modules dedicated to the residual-like structures, namely the Res2Net and Squeeze-and-Excitation blocks, and modify them to work effectively in the presented neural network architectures. The final proposed model, i.e., the SpineNet architecture with Res2Net and Time-Squeeze-and-Excitation blocks, achieves remarkable Equal Error Rates of 0.99 and 0.92 for the Extended and Original trial lists of the well-known VoxCeleb1 dataset",
    "keywords": [],
    "checked": true,
    "id": "62a007787bdf51bb58668d2a88df18850c4e9e28",
    "semantic_title": "spine2net: spinenet with res2net and time-squeeze-and-excitation blocks for speaker recognition",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stafylakis21_interspeech.html": {
    "title": "Speaker Embeddings by Modeling Channel-Wise Correlations",
    "volume": "main",
    "abstract": "Speaker embeddings extracted with deep 2D convolutional neural networks are typically modeled as projections of first and second order statistics of channel-frequency pairs onto a linear layer, using either average or attentive pooling along the time axis. In this paper we examine an alternative pooling method, where pairwise correlations between channels for given frequencies are used as statistics. The method is inspired by style-transfer methods in computer vision, where the style of an image, modeled by the matrix of channel-wise correlations, is transferred to another image, in order to produce a new image having the style of the first and the content of the second. By drawing analogies between image style and speaker characteristics, and between image content and phonetic sequence, we explore the use of such channel-wise correlations features to train a ResNet architecture in an end-to-end fashion. Our experiments on VoxCeleb demonstrate the effectiveness of the proposed pooling method in speaker recognition",
    "keywords": [],
    "checked": true,
    "id": "87382405a9f9abb7f0eabce17799b626f2e20e4b",
    "semantic_title": "speaker embeddings by modeling channel-wise correlations",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/he21_interspeech.html": {
    "title": "Multi-Task Neural Network for Robust Multiple Speaker Embedding Extraction",
    "volume": "main",
    "abstract": "This paper introduces a novel approach for extracting speaker embeddings from audio mixtures of multiple overlapping voices. This approach is based on a multi-task neural network. The network first extracts a latent feature for each direction. This feature is used for detecting sound sources as well as identifying speakers. In contrast to traditional approaches, the proposed method does not rely on explicit sound source separation. The neural network model learns from data to extract the most suitable features of the sounds at different directions. The experiments using audio recordings of overlapping sound sources show that the proposed approach outperforms a beamforming-based traditional method",
    "keywords": [],
    "checked": true,
    "id": "e8a9987893824dadd9c72692a2b0448e3951a5a6",
    "semantic_title": "multi-task neural network for robust multiple speaker embedding extraction",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21_interspeech.html": {
    "title": "ICSpk: Interpretable Complex Speaker Embedding Extractor from Raw Waveform",
    "volume": "main",
    "abstract": "Recently, extracting speaker embedding directly from raw waveform has drawn increasing attention in the field of speaker verification. Parametric real-valued filters in the first convolutional layer are learned to transform the waveform into time-frequency representations. However, these methods only focus on the magnitude spectrum and the poor interpretability of the learned filters limits the performance. In this paper, we propose a complex speaker embedding extractor, named ICSpk, with higher interpretability and fewer parameters. Specifically, at first, to quantify the speaker-related frequency response of waveform, we modify the original short-term Fourier transform filters into a family of complex exponential filters, named interpretable complex (IC) filters. Each IC filter is confined by a complex exponential filter parameterized by frequency. Then, a deep complex-valued speaker embedding extractor is designed to operate on the complex-valued output of IC filters. The proposed ICSpk is evaluated on VoxCeleb and CNCeleb databases. Experimental results demonstrate the IC filters-based system exhibits a significant improvement over the complex spectrogram based systems. Furthermore, the proposed ICSpk outperforms existing raw waveform based systems by a large margin",
    "keywords": [],
    "checked": true,
    "id": "4cd567a2dd9b55247179ac136cd236e4941aa4a4",
    "semantic_title": "icspk: interpretable complex speaker embedding extractor from raw waveform",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xiao21_interspeech.html": {
    "title": "Prosodic Disambiguation Using Chironomic Stylization of Intonation with Native and Non-Native Speakers",
    "volume": "main",
    "abstract": "This paper introduces an interface that enables the real-time gestural control of intonation in phrases produced by a vocal synthesizer. The melody and timing of a target phrase can be modified by tracing melodic contours on the touch-screen of a mobile tablet. Envisioning this interface as a means for non-native speakers to practice the intonation of a foreign language, we present a pilot study where native and non-native speakers imitated the pronunciation of French phrases using their voice and the interface, with a visual guide and without. Comparison of resulting F0 curves against the reference contour and a preliminary perceptual assessment of synthesized utterances suggest that for both non-native and native speakers, imitation with the help of a visual guide is comparable in accuracy to vocal imitation, and that timing control was a source of difficulty",
    "keywords": [],
    "checked": true,
    "id": "f97cd9369c07403dfe622fbd74b33ae35622516b",
    "semantic_title": "prosodic disambiguation using chironomic stylization of intonation with native and non-native speakers",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/block21_interspeech.html": {
    "title": "Variation in Perceptual Sensitivity and Compensation for Coarticulation Across Adult and Child Naturally-Produced and TTS Voices",
    "volume": "main",
    "abstract": "The current study explores whether perception of coarticulatory vowel nasalization differs by speaker age (adult vs. child) and type of voice (naturally produced vs. synthetic speech). Listeners completed a 4IAX discrimination task between pairs containing acoustically identical (both nasal or oral) vowels and acoustically distinct (one oral, one nasal) vowels. Vowels occurred in either the same consonant contexts or different contexts across pairs. Listeners completed the experiment with either naturally produced speech or text-to-speech (TTS). For same-context trials, listeners were better at discriminating between oral and nasal vowels for child speech in the synthetic voices but adult speech in the natural voices. Meanwhile, in different-context trials, listeners were less able to discriminate, indicating more perceptual compensation for synthetic voices. There was no difference in different-context discrimination across talker ages, indicating that listeners did not compensate differently if the speaker was a child or adult. Findings are relevant for models of compensation, computer personification theories, and speaker-indexical perception accounts",
    "keywords": [],
    "checked": true,
    "id": "9156bfc6f44686cf3391398e58989978cb87bb4e",
    "semantic_title": "variation in perceptual sensitivity and compensation for coarticulation across adult and child naturally-produced and tts voices",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/monesi21_interspeech.html": {
    "title": "Extracting Different Levels of Speech Information from EEG Using an LSTM-Based Model",
    "volume": "main",
    "abstract": "Decoding the speech signal that a person is listening to from the human brain via electroencephalography (EEG) can help us understand how our auditory system works. Linear models have been used to reconstruct the EEG from speech or vice versa. Recently, Artificial Neural Networks (ANNs) such as Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based architectures have outperformed linear models in modeling the relation between EEG and speech. Before attempting to use these models in real-world applications such as hearing tests or (second) language comprehension assessment we need to know what level of speech information is being utilized by these models. In this study, we aim to analyze the performance of an LSTM-based model using different levels of speech features. The task of the model is to determine which of two given speech segments is matched with the recorded EEG. We used low- and high-level speech features including: envelope, mel spectrogram, voice activity, phoneme identity, and word embedding. Our results suggest that the model exploits information about silences, intensity, and broad phonetic classes from the EEG. Furthermore, the mel spectrogram, which contains all this information, yields the highest accuracy (84%) among all the features",
    "keywords": [],
    "checked": true,
    "id": "e7b5f3cd4ccced32cc691a47344e66fca1c52f88",
    "semantic_title": "extracting different levels of speech information from eeg using an lstm-based model",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bosch21_interspeech.html": {
    "title": "Word Competition: An Entropy-Based Approach in the DIANA Model of Human Word Comprehension",
    "volume": "main",
    "abstract": "We discuss the role of entropy of the set of unfolding word candidates in the context of DIANA, a computational model of human auditory speech comprehension. DIANA consists of three major interacting components: Activation, Decision and Execution. The Activation component computes activations of word candidates that change over time as a function of the unfolding audio input. The resulting set of word candidate activations can be associated with an entropy that is related to difficulty of the decision when one of these candidates must be selected at time T. The paper presents the close relation between entropy measures and the between-word competition during the unfolding of the auditory stimuli, and at the end of the stimuli if no decision could be made before stimulus offset. We present a way for computing the entropy that takes into account linguistic-phonetic constraints that play a role in speech comprehension and in lexical decision experiments. Using the BALDEY data set and linear mixed effects regression models for RT, we show that entropy measures explain differences between RTs of words with different morphological structure",
    "keywords": [],
    "checked": true,
    "id": "912d256af6fbb78cd79b5f323aa69ceb9d5234ae",
    "semantic_title": "word competition: an entropy-based approach in the diana model of human word comprehension",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bosch21b_interspeech.html": {
    "title": "Time-to-Event Models for Analyzing Reaction Time Sequences",
    "volume": "main",
    "abstract": "We investigate reaction time (RT) sequences obtained from lexical decision experiments by applying Time-to-Event modelling (Survival Analysis). This is a branch of statistics for analyzing the expected duration until one or more events happen, associated with a set of potential ‘causes' (in our case the decision for a ‘word' judgment as a function of conventional predictors such as lexical frequency, stimulus duration, reduction, etc.). In this analysis, RTs are considered a by-product of an (unobservable) cumulative incidence function that results in a decision when it exceeds a certain threshold We show that Survival Analysis can be effectively used to narrow the gap between data-oriented models and process-oriented models for RT data from lexical decision experiments. Results of this analysis technique are presented for two different RT data sets. The analysis reveals time-varying patterns of predictors that reflect the differences in cognitive processes during the presentation of auditory stimuli",
    "keywords": [],
    "checked": true,
    "id": "5f6773faef5ee141af529d58862c7de1efe8e574",
    "semantic_title": "time-to-event models for analyzing reaction time sequences",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/brand21_interspeech.html": {
    "title": "Models of Reaction Times in Auditory Lexical Decision: RTonset versus RToffset",
    "volume": "main",
    "abstract": "We investigate how the role of predictors in models of reaction times in auditory lexical decision experiments depends on the operational definition of RT: whether the time is measured from stimulus onset or from stimulus offset. In a large body of literature, RTs are measured from the onset of the stimulus to the start of the response (often a button press or an oral response). The rationale behind this choice is that information about the stimulus becomes available to the listener starting at onset. Alternatively, the RT from offset is less dependent on stimulus duration and is assumed to focus on those cognitive processes that play a role late(r) in the word and after word offset, when all information is available The paper presents RT-onset and RT-offset-based linear mixed effects models for three different lexical decision-based data sets and explains the significant differences between these models, showing to what extent both definitions of reaction time reveal different roles for predictors and how early and later contributions to the overall RT can be differentiated",
    "keywords": [],
    "checked": true,
    "id": "f5fbf5e3d6fa5e4f556c5b0853fc09b72f6716d4",
    "semantic_title": "models of reaction times in auditory lexical decision: rtonset versus rtoffset",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21c_interspeech.html": {
    "title": "SpecMix : A Mixed Sample Data Augmentation Method for Training with Time-Frequency Domain Features",
    "volume": "main",
    "abstract": "A mixed sample data augmentation strategy is proposed to enhance the performance of models on audio scene classification, sound event classification, and speech enhancement tasks. While there have been several augmentation methods shown to be effective in improving image classification performance, their efficacy toward time-frequency domain features of audio is not assured. We propose a novel audio data augmentation approach named \"Specmix\" specifically designed for dealing with time-frequency domain features. The augmentation method consists of mixing two different data samples by applying time-frequency masks effective in preserving the spectral correlation of each audio sample. Our experiments on acoustic scene classification, sound event classification, and speech enhancement tasks show that the proposed Specmix improves the performance of various neural network architectures by a maximum of 2.7%",
    "keywords": [],
    "checked": true,
    "id": "c514b8b084f5e210228f44c95dd5a9ac99d914b0",
    "semantic_title": "specmix : a mixed sample data augmentation method for training withtime-frequency domain features",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21d_interspeech.html": {
    "title": "SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "In this paper, we present SpecAugment++, a novel data augmentation method for deep neural networks based acoustic scene classification (ASC). Different from other popular data augmentation methods such as SpecAugment and mixup that only work on the input space, SpecAugment++ is applied to both the input space and the hidden space of the deep neural networks to enhance the input and the intermediate feature representations. For an intermediate hidden state, the augmentation techniques consist of masking blocks of frequency channels and masking blocks of time frames, which improve generalization by enabling a model to attend not only to the most discriminative parts of the feature, but also the entire parts. Apart from using zeros for masking, we also examine two approaches for masking based on the use of other samples within the mini-batch, which helps introduce noises to the networks to make them more discriminative for classification. The experimental results on the DCASE 2018 Task1 dataset and DCASE 2019 Task1 dataset show that our proposed method can obtain 3.6% and 4.7% accuracy gains over a strong baseline without augmentation (i.e. CP-ResNet) respectively, and outperforms other previous data augmentation methods",
    "keywords": [],
    "checked": true,
    "id": "254a094e5269e76cb92f7169c160fd9c67a057bd",
    "semantic_title": "specaugment++: a hidden space data augmentation method for acoustic scene classification",
    "citation_count": 36,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zheng21_interspeech.html": {
    "title": "An Effective Mutual Mean Teaching Based Domain Adaptation Method for Sound Event Detection",
    "volume": "main",
    "abstract": "In this paper, we present a novel mutual mean teaching based domain adaptation (MMT-DA) method for sound event detection (SED) task, which can effectively exploit synthetic data to improve the SED performance. Existing methods simply treat the synthetic data as strongly-labeled data in semi-supervised learning (SSL) framework. Benefiting from the strong labels of synthetic data, superior SED performance can be achieved. However, a distribution mismatch between synthetic and real data raises an evident challenge for domain adaptation (DA). In MMT-DA, convolutional recurrent neural networks (CRNN) learned from different datasets (i.e :real+synthetic, and ) are exploited for DA. Specifically, mean teacher method using CRNN is employed for utilizing the unlabeled real data. To compensate the domain diversity, an additional domain classifier with gradient reverse layer(GRL) is used for training a mean teacher for The student CRNNs are mutually taught using the soft predictions of unlabeled data obtained from different teachers. Furthermore, a strip pooling based attention module is exploited to model the inter-dependencies between channels and time-frequency dimensions to exploit the structure information. Experimental results on Task4 of DCASE2020 demonstrate the ability of the proposed method, achieving 52.0% F1-score on the validation dataset, which outperforms the winning system's 50.6%",
    "keywords": [],
    "checked": true,
    "id": "831d0bf67a37506dec36c5cfddd8bd62a2a519b5",
    "semantic_title": "an effective mutual mean teaching based domain adaptation method for sound event detection",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nandi21_interspeech.html": {
    "title": "Acoustic Scene Classification Using Kervolution-Based SubSpectralNet",
    "volume": "main",
    "abstract": "In this paper, a Kervolution-based SubSpectralNet model is proposed for Acoustic Scene Classification (ASC). SubSpectralNet is a competitive model which divides the mel spectrogram into horizontal slices termed as sub-spectrograms that are considered as input to the Convolutional Neural Network (CNN). In this work, the linear convolutional operation of SubSpectralNet is replaced with a non-linear operation using the kernel trick. This is also known as kervolution (kernel convolution)-based SubSpectralNet. The performance of the proposed methodology is evaluated on the DCASE (Detection and Classification of Acoustic Scenes and Events) 2018 development dataset. The proposed method achieves 73.52% and 75.76% accuracy with Polynomial and Gaussian Kernels respectively",
    "keywords": [],
    "checked": true,
    "id": "7a5da2afd67169d3df47d4bff55b41878e1b7443",
    "semantic_title": "acoustic scene classification using kervolution-based subspectralnet",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sundar21_interspeech.html": {
    "title": "Event Specific Attention for Polyphonic Sound Event Detection",
    "volume": "main",
    "abstract": "The concept of multi-headed self attention (MHSA) introduced as a critical building block of a Transformer Encoder/Decoder Module has made a significant impact in the areas of natural language processing (NLP), automatic speech recognition (ASR) and recently in the area of sound event detection (SED). The current state-of-the-art approaches to SED employ a shared attention mechanism achieved through a stack of MHSA blocks to detect multiple sound events. Consequently, in a multi-label SED task, a common attention mechanism would be responsible for generating relevant feature representations for each of the events to be detected. In this paper, we show through empirical evaluation that having more MHSA blocks dedicated specifically for individual events, rather than having a stack of shared MHSA blocks, improves the overall detection performance. Interestingly, this improvement in performance comes about because the event-specific attention blocks help in resolving confusions in the case of co-occurring events. The proposed \"Event-specific Attention Network\" (ESA-Net) can be trained in an end-to-end manner. On the DCASE 2020 Task 4 data set, we show that with ESA-Net, the best single model achieves an event-based F1 score of 52.1% on the public validation data set improving over the existing state of the art result",
    "keywords": [],
    "checked": true,
    "id": "6fc0c8b8beaebf9b76769f64eff495d0ef8bf88e",
    "semantic_title": "event specific attention for polyphonic sound event detection",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21b_interspeech.html": {
    "title": "AST: Audio Spectrogram Transformer",
    "volume": "main",
    "abstract": "In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2",
    "keywords": [],
    "checked": true,
    "id": "0e2d8b8d81092037f9866c1ceddcebb87318e38b",
    "semantic_title": "ast: audio spectrogram transformer",
    "citation_count": 404,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seo21_interspeech.html": {
    "title": "Shallow Convolution-Augmented Transformer with Differentiable Neural Computer for Low-Complexity Classification of Variable-Length Acoustic Scene",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNNs) exhibit good performance in low-complexity classification with fixed-length acoustic scenes. However, previous studies have not considered variable-length acoustic scenes in which performance degradation is prevalent. In this regard, we investigate two novel architectures — convolution-augmented transformer (Conformer) and differentiable neural computer (DNC). Both the models show desirable performance for variable-length data but require a large amount of data. In other words, small amounts of data, such as those from acoustic scenes, lead to overfitting in these models. In this paper, we propose a shallow convolution-augmented Transformer with a differentiable neural computer (shallow Conformer-DNC) for the low-complexity classification of variable-length acoustic scenes. The shallow Conformer-DNC is enabled to converge with small amounts of data. Short-term and long-term contexts of variable-length acoustic scenes are trained by using the shallow Conformer and shallow DNC, respectively. The experiments were conducted for variable-length conditions using the TAU Urban Acoustic Scenes 2020 Mobile dataset. As a result, a peak accuracy of 61.25% was confirmed for shallow Conformer-DNC with a model parameter of 34 K. It is comparable performance to state-of-the-art CNNs",
    "keywords": [],
    "checked": true,
    "id": "100bb70815cd72ab469d885d1eae43a0e0b1de29",
    "semantic_title": "shallow convolution-augmented transformer with differentiable neural computer for low-complexity classification of variable-length acoustic scene",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bear21_interspeech.html": {
    "title": "An Evaluation of Data Augmentation Methods for Sound Scene Geotagging",
    "volume": "main",
    "abstract": "Sound scene geotagging is a new topic of research which has evolved from acoustic scene classification. It is motivated by the idea of audio surveillance. Not content with only describing a scene in a recording, a machine which can locate where the recording was captured would be of use to many. In this paper we explore a series of common audio data augmentation methods to evaluate which best improves the accuracy of audio geotagging classifiers Our work improves on the state-of-the-art city geotagging method by 23% in terms of classification accuracy",
    "keywords": [],
    "checked": true,
    "id": "cb767ecb7f2c1b5b33b7b09a82a3ccd9d8aa2749",
    "semantic_title": "an evaluation of data augmentation methods for sound scene geotagging",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hori21_interspeech.html": {
    "title": "Optimizing Latency for Online Video Captioning Using Audio-Visual Transformers",
    "volume": "main",
    "abstract": "Video captioning is an essential technology to understand scenes and describe events in natural language. To apply it to real-time monitoring, a system needs not only to describe events accurately but also to produce the captions as soon as possible. Low-latency captioning is needed to realize such functionality, but this research area for online video captioning has not been pursued yet. This paper proposes a novel approach to optimize each caption's output timing based on a trade-off between latency and caption quality. An audio-visual Transformer is trained to generate ground-truth captions using only a small portion of all video frames, and to mimic outputs of a pre-trained Transformer to which all the frames are given. A CNN-based timing detector is also trained to detect a proper output timing, where the captions generated by the two Transformers become sufficiently close to each other. With the jointly trained Transformer and timing detector, a caption can be generated in the early stages of an event-triggered video clip, as soon as an event happens or when it can be forecasted. Experiments with the ActivityNet Captions dataset show that our approach achieves 94% of the caption quality of the upper bound given by the pre-trained Transformer using the entire video clips, using only 28% of frames from the beginning",
    "keywords": [],
    "checked": true,
    "id": "343da36b7e02087342e5e29d538fef6cc24d97c1",
    "semantic_title": "optimizing latency for online video captioningusing audio-visual transformers",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/si21_interspeech.html": {
    "title": "Variational Information Bottleneck for Effective Low-Resource Audio Classification",
    "volume": "main",
    "abstract": "Large-scale deep neural networks (DNNs) such as convolutional neural networks (CNNs) have achieved impressive performance in audio classification for their powerful capacity and strong generalization ability. However, when training a DNN model on low-resource tasks, it is usually prone to overfitting the small data and learning too much redundant information. To address this issue, we propose to use variational information bottleneck (VIB) to mitigate overfitting and suppress irrelevant information. In this work, we conduct experiments on a 4-layer CNN. However, the VIB framework is ready-to-use and could be easily utilized with many other state-of-the-art network architectures. Evaluation on a few audio datasets shows that our approach significantly outperforms baseline methods, yielding ≥ 5.0% improvement in terms of classification accuracy in some low-source settings",
    "keywords": [],
    "checked": true,
    "id": "a384336e61895d1b05e136ab0dcef9addf8f7548",
    "semantic_title": "variational information bottleneck for effective low-resource audio classification",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deshmukh21_interspeech.html": {
    "title": "Improving Weakly Supervised Sound Event Detection with Self-Supervised Auxiliary Tasks",
    "volume": "main",
    "abstract": "While multitask and transfer learning has shown to improve the performance of neural networks in limited data settings, they require pretraining of the model on large datasets beforehand. In this paper, we focus on improving the performance of weakly supervised sound event detection in low data and noisy settings simultaneously without requiring any pretraining task. To that extent, we propose a shared encoder architecture with sound event detection as a primary task and an additional secondary decoder for a self-supervised auxiliary task. We empirically evaluate the proposed framework for weakly supervised sound event detection on a remix dataset of the DCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20 dB SNR. To ensure we retain the localisation information of multiple sound events, we propose a two-step attention pooling mechanism that provides a time-frequency localisation of multiple audio events in the clip. The proposed framework with two-step attention outperforms existing benchmark models by 22.3%, 12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an ablation study to determine the contribution of the auxiliary task and two-step attention pooling to the SED performance improvement",
    "keywords": [],
    "checked": true,
    "id": "ba3a4ce1e04276ddde8ba7cdb05192c24cae10ea",
    "semantic_title": "improving weakly supervised sound event detection with self-supervised auxiliary tasks",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/komatsu21_interspeech.html": {
    "title": "Acoustic Event Detection with Classifier Chains",
    "volume": "main",
    "abstract": "This paper proposes acoustic event detection (AED) with classifier chains, a new classifier based on the probabilistic chain rule. The proposed AED with classifier chains consists of a gated recurrent unit and performs iterative binary detection of each event one by one. In each iteration, the event's activity is estimated and used to condition the next output based on the probabilistic chain rule to form classifier chains. Therefore, the proposed method can handle the interdependence among events upon classification, while the conventional AED methods with multiple binary classifiers with a linear layer and sigmoid function have placed an assumption of conditional independence. In the experiments with a real-recording dataset, the proposed method demonstrates its superior AED performance to a relative 14.80% improvement compared to a convolutional recurrent neural network baseline system with the multiple binary classifiers",
    "keywords": [],
    "checked": true,
    "id": "3f5b7fcb6fc50ba80318ab959f3d63253cd0ef6b",
    "semantic_title": "acoustic event detection with classifier chains",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tseng21_interspeech.html": {
    "title": "Segment and Tone Production in Continuous Speech of Hearing and Hearing-Impaired Children",
    "volume": "main",
    "abstract": "Verbal communication in daily use is conducted in the form of continuous speech that theoretically is the ideal data format for assessing oral language ability in educational and clinical domains. But as phonetic reduction and particularly lexical tones in Chinese are greatly affected by discourse context, it is a challenging task for automatic systems to evaluate continuous speech only by acoustic features. This study analyzed repetitive and storytelling speech produced by selected Chinese-speaking hearing and hearing-impaired children with distinctively high and low speech intelligibility levels. Word-based reduction types are derived by phonological properties that characterize contraction degrees of automatically generated surface forms of disyllabic words. F0-based tonal contours are visualized using the centroid-nearest data points in the major clusters computed for tonal syllables. Our results show that primary speech characteristics across different groups of children can be differentiated by means of reduction type and tone production",
    "keywords": [],
    "checked": true,
    "id": "3a79c74a5f7b527b94cb93c8401fd3c37433d90d",
    "semantic_title": "segment and tone production in continuous speech of hearing and hearing-impaired children",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21e_interspeech.html": {
    "title": "Effect of Carrier Bandwidth on Understanding Mandarin Sentences in Simulated Electric-Acoustic Hearing",
    "volume": "main",
    "abstract": "For patients suffering with high-frequency hearing loss and preserving low-frequency hearing, combined electric-acoustic stimulation (EAS) may significantly improve their speech perception compared with cochlear implants (CIs). In combined EAS, a hearing aid provides low-frequency information via acoustic (A) stimulation and a CI evokes high-frequency sound sensation via electrical (E) stimulation. The present work investigated the EAS advantage when only a small number (i.e., 1 or 2) of channels were provided for electrical stimulation in a CI, and the effect of carrier bandwidth on understanding Mandarin sentences in a simulation of combined EAS experiment. The A-portion was extracted via low-pass filtering processing and the E-portion was generated with a vocoder model preserving multi-channel temporal envelope waveforms, whereas a noise-vocoder and a tone-vocoder were used to simulate the effect of carrier bandwidth. The synthesized stimuli were presented to normal-hearing listeners to recognize. Experimental results showed that while low-pass filtered Mandarin speech was not very intelligible, adding one or two E channels could significantly improve the intelligibility score to above 86.0%. Under the condition with one E channel, using a large carrier bandwidth in noise-vocoder processing provided a better intelligibility performance than using a narrow carrier bandwidth in tone-vocoder processing",
    "keywords": [],
    "checked": true,
    "id": "0bd7227e80a1eb8b7b8148ba44c2447b0f83f366",
    "semantic_title": "effect of carrier bandwidth on understanding mandarin sentences in simulated electric-acoustic hearing",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sharma21_interspeech.html": {
    "title": "A Comparative Study of Different EMG Features for Acoustics-to-EMG Mapping",
    "volume": "main",
    "abstract": "Electromyography (EMG) signals have been extensively used to capture facial muscle movements while speaking since they are one of the most closely related bio-signals generated during speech production. In this work, we focus on speech acoustics to EMG prediction. We present a comparative study of ten different EMG signal-based features including Time Domain (TD) features existing in the literature to examine their effectiveness in speech acoustics to EMG inverse (AEI) mapping. We propose a novel feature based on the Hilbert envelope of the filtered EMG signal. The raw EMG signal is reconstructed from these features as well. For the AEI mapping, we use a bi-directional long short-term memory (BLSTM) network in a session-dependent manner. To estimate the raw EMG signal from the EMG features, we use a CNN-BLSTM model comprising of a convolution neural network (CNN) followed by BLSTM layers. AEI mapping performance using the BLSTM network reveals that the Hilbert envelope based feature is predicted from speech with the highest accuracy, among all the features. Therefore, it could be the most representative feature of the underlying muscle activation during speech production. The proposed Hilbert envelope feature, when used together with the existing TD features, improves the raw EMG signal reconstruction performance compared to using the TD features alone",
    "keywords": [],
    "checked": true,
    "id": "cd51e410d8fa732adf89f03186bada63022273a4",
    "semantic_title": "a comparative study of different emg features for acoustics-to-emg mapping",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/abraham21_interspeech.html": {
    "title": "Image-Based Assessment of Jaw Parameters and Jaw Kinematics for Articulatory Simulation: Preliminary Results",
    "volume": "main",
    "abstract": "Correcting the deficits in jaw movements have often been ignored in assessment and treatment of speech disorders. A robotic simulation is being developed to facilitate Speech Language Pathologists to demonstrate the movement of jaw, tongue and teeth during production of speech sounds, as a part of a larger study. Profiling of jaw movement is an important aspect of articulatory simulation. The present study attempts to develop a simple and efficient technique for deriving the jaw parameters and using them to simulate jaw movements through inverse kinematics Three Kannada speaking male participants in the age range of 26 to 33 years were instructed to produce selected speech sounds. The image of the final position of the jaw during production of each speech sound was recorded through CT scan and video camera. Angle of ramus and angle of body of mandible were simulated through inverse kinematics using RoboAnalyzer software. The variables for inverse kinematics were derived through kinematic analysis. The Denavit-Hartenberg (D-H) parameters required for kinematic analysis were obtained from still image. Angles simulated were compared with the angles obtained from CT scan images. No significant difference was observed",
    "keywords": [],
    "checked": true,
    "id": "efd6b477a05a0a2df158a67868150486351cf4ba",
    "semantic_title": "image-based assessment of jaw parameters and jaw kinematics for articulatory simulation: preliminary results",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21f_interspeech.html": {
    "title": "An Attention Self-Supervised Contrastive Learning Based Three-Stage Model for Hand Shape Feature Representation in Cued Speech",
    "volume": "main",
    "abstract": "Cued Speech (CS) is a communication system for deaf people or hearing impaired people, in which a speaker uses it to aid a lipreader in phonetic level by clarifying potentially ambiguous mouth movements with hand shape and positions. Feature extraction of multi-modal CS is a key step in CS recognition. Recent supervised deep learning based methods suffer from noisy CS data annotations especially for hand shape modality. In this work, we first propose a self-supervised contrastive learning method to learn the feature representation of image without using labels. Secondly, a small amount of manually annotated CS data are used to fine-tune the first module. Thirdly, we present a module, which combines Bi-LSTM and self-attention networks to further learn sequential features with temporal and contextual information. Besides, to enlarge the volume and the diversity of the current limited CS datasets, we build a new British English dataset containing 5 native CS speakers. Evaluation results on both French and British English datasets show that our model achieves over 90% accuracy in hand shape recognition. Significant improvements of 8.75% (for French) and 10.09% (for British English) are achieved in CS phoneme recognition correctness compared with the state-of-the-art",
    "keywords": [],
    "checked": true,
    "id": "c0aa75df231ecbe77364c0eed1a9a4a97ce01626",
    "semantic_title": "an attention self-supervised contrastive learning based three-stage model for hand shape feature representation in cued speech",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dineley21_interspeech.html": {
    "title": "Remote Smartphone-Based Speech Collection: Acceptance and Barriers in Individuals with Major Depressive Disorder",
    "volume": "main",
    "abstract": "The ease of in-the-wild speech recording using smartphones has sparked considerable interest in the combined application of speech, remote measurement technology (RMT) and advanced analytics as a research and healthcare tool. For this to be realised, the acceptability of remote speech collection to the user must be established, in addition to feasibility from an analytical perspective. To understand the acceptance, facilitators, and barriers of smartphone-based speech recording, we invited 384 individuals with major depressive disorder (MDD) from the Remote Assessment of Disease and Relapse — Central Nervous System (RADAR-CNS) research programme in Spain and the UK to complete a survey on their experiences recording their speech. In this analysis, we demonstrate that study participants were more comfortable completing a scripted speech task than a free speech task. For both speech tasks, we found depression severity and country to be significant predictors of comfort. Not seeing smartphone notifications of the scheduled speech tasks, low mood and forgetfulness were the most commonly reported obstacles to providing speech recordings",
    "keywords": [],
    "checked": true,
    "id": "2c9cec59469b8aaefa9ab7e1e4b03d4310d71447",
    "semantic_title": "remote smartphone-based speech collection: acceptance and barriers in individuals with major depressive disorder",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21_interspeech.html": {
    "title": "An Automatic, Simple Ultrasound Biofeedback Parameter for Distinguishing Accurate and Misarticulated Rhotic Syllables",
    "volume": "main",
    "abstract": "Characterizing accurate vs. misarticulated patterns of tongue movement using ultrasound can be challenging in real time because of the fast, independent movement of tongue regions. The usefulness of ultrasound for biofeedback speech therapy is limited because speakers must mentally track and compare differences between their tongue movement and available models. It is desirable to automate this interpretive task using a single parameter representing deviation from known accurate tongue movements. In this study, displacements recorded automatically by ultrasound image tracking were transformed into a single biofeedback parameter (time-dependent difference between blade and dorsum displacements). Receiver operating characteristic (ROC) curve analysis was used to evaluate this parameter as a predictor of production accuracy over a range of different vowel contexts with initial and final /r/ in American English. Areas under ROC curves were 0.8 or above, indicating that this simple parameter may provide useful real-time biofeedback on /r/ accuracy within a range of rhotic contexts",
    "keywords": [],
    "checked": true,
    "id": "35e23d8a22399dfbfaeee78e92d918d89f666812",
    "semantic_title": "an automatic, simple ultrasound biofeedback parameter for distinguishing accurate and misarticulated rhotic syllables",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ribeiro21_interspeech.html": {
    "title": "Silent versus Modal Multi-Speaker Speech Recognition from Ultrasound and Video",
    "volume": "main",
    "abstract": "We investigate multi-speaker speech recognition from ultrasound images of the tongue and video images of the lips. We train our systems on imaging data from modal speech, and evaluate on matched test sets of two speaking modes: silent and modal speech. We observe that silent speech recognition from imaging data underperforms compared to modal speech recognition, likely due to a speaking-mode mismatch between training and testing. We improve silent speech recognition performance using techniques that address the domain mismatch, such as fMLLR and unsupervised model adaptation. We also analyse the properties of silent and modal speech in terms of utterance duration and the size of the articulatory space. To estimate the articulatory space, we compute the convex hull of tongue splines, extracted from ultrasound tongue images. Overall, we observe that the duration of silent speech is longer than that of modal speech, and that silent speech covers a smaller articulatory space than modal speech. Although these two properties are statistically significant across speaking modes, they do not directly correlate with word error rates from speech recognition",
    "keywords": [],
    "checked": true,
    "id": "6db077cf8ad1ebd68b5cd37b6dc533c5d89c3a78",
    "semantic_title": "silent versus modal multi-speaker speech recognition from ultrasound and video",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ferreira21_interspeech.html": {
    "title": "RaSSpeR: Radar-Based Silent Speech Recognition",
    "volume": "main",
    "abstract": "Speech is our most natural and efficient way of communication and offers a strong potential to improve how we interact with machines. However, speech communication can sometimes be limited by environmental (e.g., ambient noise), contextual (e.g., need for privacy in a public place), or health conditions (e.g., laryngectomy), hindering the consideration of audible speech. In this regard, silent speech interfaces (SSI) have been proposed (e.g., considering video, electromyography), however, many technologies still face limitations regarding their everyday use, e.g., the need to place equipment in contact with the speaker (e.g., electrodes/ultrasound probe), and raise technical (e.g., lighting conditions for video) or privacy concerns. In this context, the consideration of technologies that can help tackle these issues, e.g, by being contactless and/or placed in the environment, can foster the widespread use of SSI. In this article, continuous-wave radar is explored to assess its potential for SSI. To this end, a corpus of 13 words was acquired, for 3 speakers, and different classifiers were tested on the resulting data. The best results, obtained using Bagging classifier, trained for each speaker, with 5-fold cross-validation, yielded an average accuracy of 0.826, an encouraging result that establishes promising grounds for further exploration of this technology for silent speech recognition",
    "keywords": [],
    "checked": true,
    "id": "519ba8a37856e929f87d9baf817235000dcc9ed2",
    "semantic_title": "rassper: radar-based silent speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cao21_interspeech.html": {
    "title": "Investigating Speech Reconstruction for Laryngectomees for Silent Speech Interfaces",
    "volume": "main",
    "abstract": "Silent speech interfaces (SSIs) are devices that convert non-audio bio-signals to speech, which hold the potential of recovering quality speech for laryngectomees (people who have undergone laryngectomy). Although significant progress has been made, most of the recent SSI works focused on data collected from healthy speakers. SSIs for laryngectomees have rarely been investigated. In this study, we investigated the reconstruction of speech for two laryngectomees who either use tracheoesophageal puncture (TEP) or electro-larynx (EL) speech as their post-surgery communication mode. We reconstructed their speech using two SSI designs (1) real-time recognition-and-synthesis and (2) directly articulation-to-speech synthesis (ATS). The reconstructed speech samples were measured in subjective evaluation by 20 listeners in terms of naturalness and intelligibility. The results indicated that both designs increased the naturalness of alaryngeal speech. The real-time recognition-and-synthesis design obtained higher intelligibility in electrolarynx speech as well, while the ATS did not. These preliminary results suggest the real-time recognition-and-synthesis design may have a better potential for clinical applications (for laryngectomees) than ATS",
    "keywords": [],
    "checked": true,
    "id": "62825b598df8e6a6cfbace2d89e086c7d1534e78",
    "semantic_title": "investigating speech reconstruction for laryngectomees for silent speech interfaces",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schroter21_interspeech.html": {
    "title": "LACOPE: Latency-Constrained Pitch Estimation for Speech Enhancement",
    "volume": "main",
    "abstract": "Fundamental frequency (f ) estimation, also known as pitch tracking, has been a long-standing research topic in the speech and signal processing community. Many pitch estimation algorithms, however, fail in noisy conditions or introduce large delays due to their frame size or Viterbi decoding In this study, we propose a deep learning-based pitch estimation algorithm, LACOPE, which was trained in a joint pitch estimation and speech enhancement framework. In contrast to previous work, this algorithm allows for a configurable latency down to an algorithmic delay of 0. This is achieved by exploiting the smoothness properties of the pitch trajectory. That is, a recurrent neural network compensates delay introduced by the feature computation by predicting the pitch for a desired point, allowing a trade-off between pitch accuracy and latency We integrate the pitch estimation in a speech enhancement framework for hearing aids. For this application, we allow a delay on the analysis side of approx. 5ms. The pitch estimate is then used for constructing a comb filter in frequency domain as post-processing step to remove intra-harmonic noise Our pitch estimation performance is on par with SOTA algorithms like PYIN or CREPE for spoken speech in all noise conditions while introducing minimal latency",
    "keywords": [],
    "checked": true,
    "id": "c72d19ac039f20dd7ce57922af4ab1d080c5b695",
    "semantic_title": "lacope: latency-constrained pitch estimation for speech enhancement",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fontaine21_interspeech.html": {
    "title": "Alpha-Stable Autoregressive Fast Multichannel Nonnegative Matrix Factorization for Joint Speech Enhancement and Dereverberation",
    "volume": "main",
    "abstract": "This paper proposes α-stable autoregressive fast multichannel nonnegative matrix factorization (α-AR-FastMNMF), a robust joint blind speech enhancement and dereverberation method for improved automatic speech recognition in a realistic adverse environment. The state-of-the-art versatile blind source separation method called FastMNMF that assumes the short-time Fourier transform (STFT) coefficients of a direct sound to follow a circular complex Gaussian distribution with jointly-diagonalizable full-rank spatial covariance matrices was extended to AR-FastMNMF with an autoregressive reverberation model. Instead of the light-tailed Gaussian distribution, we use the heavy-tailed α-stable distribution, which also has the reproductive property useful for the additive source modeling, to better deal with the large dynamic range of the direct sound. The experimental results demonstrate that the proposed α-AR-FastMNMF works well as a front-end of an automatic speech recognition system. It outperforms α-AR-ILRMA, which is a special case of α-AR-FastMNMF, and their Gaussian counterparts, i.e., AR-FastMNMF and AR-ILRMA, in terms of the speech signal quality metrics and word error rate",
    "keywords": [],
    "checked": true,
    "id": "44ad09ff5a972f6e556c36d54b124805f337badd",
    "semantic_title": "alpha-stable autoregressive fast multichannel nonnegative matrix factorization for joint speech enhancement and dereverberation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21e_interspeech.html": {
    "title": "Microphone Array Generalization for Multichannel Narrowband Deep Speech Enhancement",
    "volume": "main",
    "abstract": "This paper addresses the problem of microphone array generalization for deep-learning-based end-to-end multichannel speech enhancement. We aim to train a unique deep neural network (DNN) potentially performing well on unseen microphone arrays. The microphone array geometry shapes the network's parameters when training on a fixed microphone array, and thus restricts the generalization of the trained network to another microphone array. To resolve this problem, a single network is trained using data recorded by various microphone arrays of different geometries. We design three variants of our recently proposed narrowband network to cope with the agnostic number of microphones. Overall, the goal is to make the network learn the universal information for speech enhancement that is available for any array geometry, rather than learn the one-array-dedicated characteristics. The experiments on both simulated and real room impulse responses (RIR) demonstrate the excellent across-array generalization capability of the proposed networks, in the sense that their performance measures are very close to, or even exceed the network trained with test arrays. Moreover, they notably outperform various beamforming methods and other advanced deep-learning-based methods",
    "keywords": [],
    "checked": true,
    "id": "99da88791b9ee4ec1855d6017e27d9d19e63b731",
    "semantic_title": "microphone array generalization for multichannel narrowband deep speech enhancement",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/song21_interspeech.html": {
    "title": "Multiple Sound Source Localization Based on Interchannel Phase Differences in All Frequencies with Spectral Masks",
    "volume": "main",
    "abstract": "One of the most widely used cues for sound source localization is the interchannel phase differences (IPDs) in the frequency domain. However, the spatial aliasing makes the utilization of the IPDs in the high frequencies difficult, especially when the distance between the microphones is high. Recently, the phase replication method which considers the direction-of-arrival (DoA) candidates corresponding to all the possible unwrapped phase differences in all frequency bins was proposed. However, high frequency bins with possible spatial aliasing contribute more when constructing initial DoA histograms compared with low frequency bins, which may not be desirable for source localization. In this paper, we propose to utilize the IPDs in all the frequency bins with equal weights regardless of maximum number of phase wrapping in that frequency for dual microphone sound source localization. We applied spectral masks based on local signal-to-noise ratios and coherences between microphone signals to exclude time-frequency bins without directional audio signal from the DoA histogram construction. Experimental results show that the proposed method results in more distinct peaks in the DoA histogram and outperforms the conventional method in various noisy and reverberant environments",
    "keywords": [],
    "checked": true,
    "id": "5de77bee7b188d1090495c1d3ee4c9df5321764c",
    "semantic_title": "multiple sound source localization based on interchannel phase differences in all frequencies with spectral masks",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zarazaga21_interspeech.html": {
    "title": "Cancellation of Local Competing Speaker with Near-Field Localization for Distributed ad-hoc Sensor Network",
    "volume": "main",
    "abstract": "In scenarios such as remote work, open offices and call centers, multiple people may simultaneously have independent spoken interactions with their devices in the same room. The speech of competing speakers will however be picked up by all microphones, both reducing the quality of audio and exposing speakers to breaches in privacy. We propose a cooperative cross-talk cancellation solution breaking the single active speaker assumption employed by most telecommunication systems. The proposed method applies source separation on the microphone signals of independent devices, to extract the dominant speaker in each device. It is realized using a localization estimator based on a deep neural network, followed by a time-frequency mask to separate the target speech from the interfering one at each time-frequency unit referring to its orientation. By experimental evaluation, we confirm that the proposed method effectively reduces crosstalk and exceeds the baseline expectation maximization method by 10 dB in terms of interference rejection. This performance makes the proposed method a viable solution for cross-talk cancellation in near-field conditions, thus protecting the privacy of external speakers in the same acoustic space",
    "keywords": [],
    "checked": true,
    "id": "9a06d45c23c7501f3e95c34c3e2365706e9c7d09",
    "semantic_title": "cancellation of local competing speaker with near-field localization for distributed ad-hoc sensor network",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21f_interspeech.html": {
    "title": "A Deep Learning Method to Multi-Channel Active Noise Control",
    "volume": "main",
    "abstract": "This paper addresses multi-channel active noise control (MCANC) on the basis of deep ANC, which performs active noise control by employing deep learning to encode the optimal control parameters corresponding to different noises and environments. The proposed method trains a convolutional recurrent network (CRN) to estimate the real and imaginary spectrograms of all the canceling signals simultaneously from the reference signals so that the corresponding anti-noises cancel or attenuate the primary noises in an MCANC system. We evaluate the proposed method under multiple MCANC setups and investigate the impact of the number of canceling loudspeakers and error microphones on the overall performance. Experimental results show that deep ANC is effective for MCANC in various scenarios. Moreover, the proposed method is robust against untrained noises and works well in the presence of loudspeaker nonlinearity",
    "keywords": [],
    "checked": true,
    "id": "4f849a816bbf37faa265b2e79ce5f5a7a9039ffa",
    "semantic_title": "a deep learning method to multi-channel active noise control",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/graetzer21_interspeech.html": {
    "title": "Clarity-2021 Challenges: Machine Learning Challenges for Advancing Hearing Aid Processing",
    "volume": "main",
    "abstract": "In recent years, rapid advances in speech technology have been made possible by machine learning challenges such as CHiME, REVERB, Blizzard, and Hurricane. In the Clarity project, the machine learning approach is applied to the problem of hearing aid processing of speech-in-noise, where current technology in enhancing the speech signal for the hearing aid wearer is often ineffective. The scenario is a (simulated) cuboid-shaped living room in which there is a single listener, a single target speaker and a single interferer, which is either a competing talker or domestic noise. All sources are static, the target is always within ±30° azimuth of the listener and at the same elevation, and the interferer is an omnidirectional point source at the same elevation. The target speech comes from an open source 40-speaker British English speech database collected for this purpose. This paper provides a baseline description of the round one Clarity challenges for both enhancement (CEC1) and prediction (CPC1). To the authors' knowledge, these are the first machine learning challenges to consider the problem of hearing aid speech signal processing",
    "keywords": [],
    "checked": true,
    "id": "dcd58c352561634afc3573a4ddcb05e993b736fd",
    "semantic_title": "clarity-2021 challenges: machine learning challenges for advancing hearing aid processing",
    "citation_count": 58,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tu21b_interspeech.html": {
    "title": "Optimising Hearing Aid Fittings for Speech in Noise with a Differentiable Hearing Loss Model",
    "volume": "main",
    "abstract": "Current hearing aids normally provide amplification based on a general prescriptive fitting, and the benefits provided by the hearing aids vary among different listening environments despite the inclusion of noise suppression feature. Motivated by this fact, this paper proposes a data-driven machine learning technique to develop hearing aid fittings that are customised to speech in different noisy environments. A differentiable hearing loss model is proposed and used to optimise fittings with back-propagation. The customisation is reflected on the data of speech in different noise with also the consideration of noise suppression. The objective evaluation shows the advantages of optimised custom fittings over general prescriptive fittings",
    "keywords": [],
    "checked": true,
    "id": "24fdd8f9fcd5c050b9c4620c5212519538691ad2",
    "semantic_title": "optimising hearing aid fittings for speech in noise with a differentiable hearing loss model",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sivasankaran21_interspeech.html": {
    "title": "Explaining Deep Learning Models for Speech Enhancement",
    "volume": "main",
    "abstract": "We consider the problem of explaining the robustness of neural networks used to compute time-frequency masks for speech enhancement to mismatched noise conditions. We employ the Deep SHapley Additive exPlanations (DeepSHAP) feature attribution method to quantify the contribution of every time-frequency bin in the input noisy speech signal to every time-frequency bin in the output time-frequency mask. We define an objective metric — referred to as the speech relevance score — that summarizes the obtained SHAP values and show that it correlates with the enhancement performance, as measured by the word error rate on the CHiME-4 real evaluation dataset. We use the speech relevance score to explain the generalization ability of three speech enhancement models trained using synthetically generated speech-shaped noise, noise from a professional sound effects library, or real CHiME-4 noise. To the best of our knowledge, this is the first study on neural network explainability in the context of speech enhancement",
    "keywords": [],
    "checked": true,
    "id": "f5c070b5ef48030379a4abcfa0de6f369db5effd",
    "semantic_title": "explaining deep learning models for speech enhancement",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21_interspeech.html": {
    "title": "Minimum-Norm Differential Beamforming for Linear Array with Directional Microphones",
    "volume": "main",
    "abstract": "Among different differential beamforming approaches, the minimum-norm one has received much attention as it maximizes the white noise gain(WNG). WNG measures the robustness of beamformer. But in practice, the conventional minimum-norm differential beamforming with omnidirectional elements still suffers in low white-noise-gain at the low frequencies. The major contributions of this paper are as follows: First, we extend the existing work by presenting a new solution with the use of the directional microphone elements, and show clearly the connection between the conventional beamforming and the proposed beamforming. Second, through the derivation as well as simulations, we show the proposed solution brings noticeable improvement in WNG at the low frequencies when the null positions of the directional elements coincide with the null-constraints of minimum norm solution",
    "keywords": [],
    "checked": true,
    "id": "80eafc4a3d2d83fad181c5be66634dcee466cb32",
    "semantic_title": "minimum-norm differential beamforming for linear array with directional microphones",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cao21b_interspeech.html": {
    "title": "Improving Streaming Transformer Based ASR Under a Framework of Self-Supervised Learning",
    "volume": "main",
    "abstract": "Recently self-supervised learning has emerged as an effective approach to improve the performance of automatic speech recognition (ASR). Under such a framework, the neural network is usually pre-trained with massive unlabeled data and then fine-tuned with limited labeled data. However, the non-streaming architecture like bidirectional transformer is usually adopted by the neural network to achieve competitive results, which cannot be used in streaming scenarios. In this paper, we mainly focus on improving the performance of streaming transformer under the self-supervised learning framework. Specifically, we propose a novel two-stage training method during fine-tuning, which combines knowledge distilling and self-training. The proposed training method achieves 16.3% relative word error rate (WER) reduction on Librispeech noisy test set. Finally, by only using the 100h clean subset of Librispeech as the labeled data and the rest (860h) as the unlabeled data, our streaming transformer based model obtains competitive WERs 3.5/8.7 on Librispeech clean/noisy test sets",
    "keywords": [],
    "checked": true,
    "id": "aa98a02b83ad8920a9909fae187ce0a96532a95a",
    "semantic_title": "improving streaming transformer based asr under a framework of self-supervised learning",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sadhu21_interspeech.html": {
    "title": "wav2vec-C: A Self-Supervised Model for Speech Representation Learning",
    "volume": "main",
    "abstract": "wav2vec-C introduces a novel representation learning technique combining elements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized representations from partially masked speech encoding using a contrastive loss in a way similar to wav2vec 2.0. However, the quantization process is regularized by an additional consistency network that learns to reconstruct the input features to the wav2vec 2.0 network from the quantized representations in a way similar to a VQ-VAE model. The proposed self-supervised model is trained on 10k hours of unlabeled data and subsequently used as the speech encoder in a RNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one of the very few studies of self-supervised learning on speech tasks with a large volume of real far-field labeled data. The wav2vec-C encoded representations achieve, on average, twice the error reduction over baseline and a higher codebook utilization in comparison to wav2vec 2.0",
    "keywords": [],
    "checked": true,
    "id": "253c6c9e6b976d0b89052a21249ff23146a81a4b",
    "semantic_title": "wav2vec-c: a self-supervised model for speech representation learning",
    "citation_count": 36,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wallington21_interspeech.html": {
    "title": "On the Learning Dynamics of Semi-Supervised Training for ASR",
    "volume": "main",
    "abstract": "The use of semi-supervised training (SST) has become an increasingly popular way of increasing the performance of ASR acoustic models without the need for further transcribed speech data. However, the performance of the technique can be very sensitive to the quality of the initial ASR system. This paper undertakes a comprehensive study of the improvements gained with respect to variation in the initial systems, the quantity of untranscribed data used, and the learning schedules. We postulate that the reason SST can be effective even when the initial model is poor is because it enables utterance-level information to be propagated to the frame level, and hence hypothesise that the quality of the language model plays a much larger role than the quality of the acoustic model. In experiments on Tagalog data from the IARPA MATERIAL programme, we find that indeed this is the case, and show that with an appropriately chosen recipe it is possible to achieve over 50% relative WER reductions from SST, even when the WER of the initial system is more than 80%",
    "keywords": [],
    "checked": true,
    "id": "00b4536b83475504ca8e57e290235c2cc4d3fc90",
    "semantic_title": "on the learning dynamics of semi-supervised training for asr",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hsu21_interspeech.html": {
    "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
    "volume": "main",
    "abstract": "Self-supervised learning of speech representations has been a very active research area but most work is focused on a single domain such as read audio books for which there exist large quantities of labeled and unlabeled data. In this paper, we explore more general setups where the domain of the unlabeled data for pre-training data differs from the domain of the labeled data for fine-tuning, which in turn may differ from the test data domain. Our experiments show that using target domain data during pre-training leads to large performance improvements across a variety of setups. With no access to in-domain labeled data, pre-training on unlabeled in-domain data closes 66–73% of the performance gap between the ideal setting of in-domain labeled data and a competitive supervised out-of-domain model. This has obvious practical implications since it is much easier to obtain unlabeled target domain data than labeled data. Moreover, we find that pre-training on multiple domains improves generalization performance on domains not seen during training. We will release pre-trained models",
    "keywords": [],
    "checked": true,
    "id": "5f769c5df8de29d0a2cd9c020f78047013a87b34",
    "semantic_title": "robust wav2vec 2.0: analyzing domain shift in self-supervised pre-training",
    "citation_count": 175,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/higuchi21_interspeech.html": {
    "title": "Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition",
    "volume": "main",
    "abstract": "Pseudo-labeling (PL) has been shown to be effective in semi-supervised automatic speech recognition (ASR), where a base model is self-trained with pseudo-labels generated from unlabeled data. While PL can be further improved by iteratively updating pseudo-labels as the model evolves, most of the previous approaches involve inefficient retraining of the model or intricate control of the label update. We present (MPL), a simple yet effective strategy for semi-supervised ASR. MPL consists of a pair of and models that interact and learn from each other, inspired by the mean teacher method. The online model is trained to predict pseudo-labels generated on the fly by the offline model. The offline model maintains a momentum-based moving average of the online model. MPL is performed in a single training process and the interaction between the two models effectively helps them reinforce each other to improve the ASR performance. We apply MPL to an end-to-end ASR model based on the connectionist temporal classification. The experimental results demonstrate that MPL effectively improves over the base model and is scalable to different semi-supervised scenarios with varying amounts of data or domain mismatch",
    "keywords": [],
    "checked": true,
    "id": "60013b38457658c186197d3320209b8abd46531f",
    "semantic_title": "momentum pseudo-labeling for semi-supervised speech recognition",
    "citation_count": 34,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/misra21_interspeech.html": {
    "title": "A Comparison of Supervised and Unsupervised Pre-Training of End-to-End Models",
    "volume": "main",
    "abstract": "In the absence of large-scale in-domain supervised training data, ASR models can achieve reasonable performance through pre-training on additional data that is unlabeled, mismatched or both. Given such data constraints, we compare pre-training end-to-end models on matched but unlabeled data (unsupervised) and on labeled but mismatched data (supervised), where the labeled data is mismatched in either domain or language. Across encoder architectures, pre-training methods and languages, our experiments indicate that both types of pre-training improve performance, with relative WER reductions of 15–30% in the domain mismatch case and up to 15% in the language mismatch condition. We further find that the advantage from unsupervised pre-training is most prominent when there is no matched and labeled fine-tuning data, provided that a sufficient amount of mismatched data is still available for supervised fine-tuning",
    "keywords": [],
    "checked": true,
    "id": "9b8c447d4fcea02ca3372e3cdaa0285c6b7f0cdb",
    "semantic_title": "a comparison of supervised and unsupervised pre-training of end-to-end models",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21c_interspeech.html": {
    "title": "Semi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation",
    "volume": "main",
    "abstract": "Semi and self-supervised training techniques have the potential to improve performance of speech recognition systems without additional transcribed speech data. In this work, we demonstrate the efficacy of two approaches to semi-supervision for automated speech recognition. The two approaches leverage vast amounts of available unspoken text and untranscribed audio. First, we present to improve data augmentation on unspoken text. Next, we propose the algorithm with to learn from untranscribed speech. The algorithm is built on top of our online implementation of Noisy Student Training. We demonstrate the compatibility of these techniques yielding an overall relative reduction of word error rate of up to 14.4% on the voice search tasks on 4 Indic languages",
    "keywords": [],
    "checked": true,
    "id": "a9f2e5b7933c46409456fd14b457eb3d603ffdc1",
    "semantic_title": "semi-supervision in asr: sequential mixmatch and factorized tts-based augmentation",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/likhomanenko21b_interspeech.html": {
    "title": "slimIPL: Language-Model-Free Iterative Pseudo-Labeling",
    "volume": "main",
    "abstract": "Recent results in end-to-end automatic speech recognition have demonstrated the efficacy of pseudo-labeling for semi-supervised models trained both with Connectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq) losses. Iterative Pseudo-Labeling (IPL), which continuously trains a single model using pseudo-labels iteratively re-generated as the model learns, has been shown to further improve performance in ASR. We improve upon the IPL algorithm: as the model learns, we propose to iteratively re-generate transcriptions with hard labels (the most probable tokens), that is, a language model. We call this approach Language-Model-Free IPL (slimIPL) and give a resultant training setup for low-resource settings with CTC-based models. slimIPL features a dynamic cache for pseudo-labels which reduces sensitivity to changes in relabeling hyperparameters and results in improved training stability. slimIPL is also highly-efficient and requires 3.5–4× fewer computational resources to converge than other state-of-the-art semi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL is competitive with self-supervised approaches, and is state-of-the-art with 100 hours of labeled audio without the use of a language model both at test time and during pseudo-label generation",
    "keywords": [],
    "checked": true,
    "id": "7f0c7c324675179f0e32c160d99c7066c7ab30ae",
    "semantic_title": "slimipl: language-model-free iterative pseudo-labeling",
    "citation_count": 47,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yue21_interspeech.html": {
    "title": "Phonetically Motivated Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Self-supervised representation learning has seen remarkable success in encoding high-level semantic information from unlabelled speech data. The studies have been focused on exploring new pretext tasks to improve the learned speech representation and various masking schemes with reference to speech frames. We consider effective latent speech representation should be phonetically informed. In this work, we propose a novel phonetically motivated masking scheme. Specifically, we select the masked speech frames according to the phonetic segmentation in an utterance. The phonetically motivated self-supervised representation learns the speech representation that benefits downstream speech processing tasks. We evaluate the proposed learning algorithm on phoneme classification, speech recognition, and speaker recognition, and show that it consistently outperforms competitive baselines",
    "keywords": [],
    "checked": true,
    "id": "076a3c66db625224f30f8cdf27fde0ea4116cc49",
    "semantic_title": "phonetically motivated self-supervised speech representation learning",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21_interspeech.html": {
    "title": "Improving RNN-T for Domain Scaling Using Semi-Supervised Training with Neural TTS",
    "volume": "main",
    "abstract": "Recurrent neural network transducer (RNN-T) has shown to be comparable with conventional hybrid model for speech recognition. However, there is still a challenge in out-of-domain scenarios with context or words different from training data. In this paper, we explore the semi-supervised training which optimizes RNN-T jointly with neural text-to-speech (TTS) to better generalize to new domains using domain-specific text data. We apply the method to two tasks: one with out-of-domain context and the other with significant out-of-vocabulary (OOV) words. The results show that the proposed method significantly improves the recognition accuracy in both tasks, resulting in 61.4% and 53.8% relative word error rate (WER) reductions respectively, from a well-trained RNN-T with 65 thousand hours of training data. We do further study on the semi-supervised training methodology: 1) which modules of RNN-T model to be updated; 2) the impact of using different neural TTS models; 3) the performance of using text with different relevancy to target domain. Finally, we compare several RNN-T customization methods, and conclude that semi-supervised training with neural TTS is comparable and complementary with Internal Language Model Estimation (ILME) or biasing",
    "keywords": [],
    "checked": true,
    "id": "9e8b8d25c94aa67025a441d8fb838749d4c79dba",
    "semantic_title": "improving rnn-t for domain scaling using semi-supervised training with neural tts",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seyfarth21_interspeech.html": {
    "title": "Speaker-Conversation Factorial Designs for Diarization Error Analysis",
    "volume": "main",
    "abstract": "Speaker diarization accuracy can be affected by both acoustics and conversation characteristics. Determining the cause of diarization errors is difficult because speaker voice acoustics and conversation structure co-vary, and the interactions between acoustics, conversational structure, and diarization accuracy are complex. This paper proposes a methodology that can distinguish independent marginal effects of acoustic and conversation characteristics on diarization accuracy by remixing conversations in a factorial design. As an illustration, this approach is used to investigate gender-related and language-related accuracy differences with three diarization systems: a baseline system using subsegment x-vector clustering, a variant of it with shorter subsegments, and a third system based on a Bayesian hidden Markov model. Our analysis shows large accuracy disparities for the baseline system primarily due to conversational structure, which are partially mitigated in the other two systems. The illustration thus demonstrates how the methodology can be used to identify and guide diarization model improvements",
    "keywords": [],
    "checked": true,
    "id": "19e485b3a2e95dd7445ec8daf316ddf00e5873a8",
    "semantic_title": "speaker-conversation factorial designs for diarization error analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mcgowan21_interspeech.html": {
    "title": "SmallER: Scaling Neural Entity Resolution for Edge Devices",
    "volume": "main",
    "abstract": "In this paper we introduce SmallER, a scalable neural entity resolution system capable of running directly on edge devices. SmallER addresses constraints imposed by the on-device setting such as bounded memory consumption for both model and catalog storage, limited compute resources, and related latency challenges introduced by those restrictions. Our model includes distinct modules to learn syntactic and semantic information and is trained to handle multiple domains within one compact architecture. We use compressed tries to reduce the space required to store catalogs and a novel implementation of spatial partitioning trees to strike a balance between reducing runtime latency and preserving recall relative to full catalog search. Our final model consumes only 3MB of memory at inference time with classification accuracy surpassing that of previously established, domain-specific baseline models on live customer utterances. For the largest catalogs we consider (300 or more entries), our proxy metric for runtime latency is reduced by more than 90%",
    "keywords": [],
    "checked": true,
    "id": "03305747804b812345adce3ce2b6725d752013c7",
    "semantic_title": "smaller: scaling neural entity resolution for edge devices",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rocholl21_interspeech.html": {
    "title": "Disfluency Detection with Unlabeled Data and Small BERT Models",
    "volume": "main",
    "abstract": "Disfluency detection models now approach high accuracy on English text. However, little exploration has been done in improving the size and inference time of the model. At the same time, Automatic Speech Recognition (ASR) models are moving from server-side inference to local, on-device inference. Supporting models in the transcription pipeline (like disfluency detection) must follow suit. In this work we concentrate on the disfluency detection task, focusing on small, fast, on-device models based on the BERT architecture. We demonstrate it is possible to train disfluency detection models as small as 1.3 MiB, while retaining high performance. We build on previous work that showed the benefit of data augmentation approaches such as self-training. Then, we evaluate the effect of domain mismatch between conversational and written text on model performance. We find that domain adaptation and data augmentation strategies have a more pronounced effect on these smaller models, as compared to conventional BERT models",
    "keywords": [],
    "checked": true,
    "id": "63ca82b7e846468e1aa9eacec3b1a26b7ceb94c0",
    "semantic_title": "disfluency detection with unlabeled data and small bert models",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21d_interspeech.html": {
    "title": "Discriminative Self-Training for Punctuation Prediction",
    "volume": "main",
    "abstract": "Punctuation prediction for automatic speech recognition (ASR) output transcripts plays a crucial role for improving the readability of the ASR transcripts and for improving the performance of downstream natural language processing applications. However, achieving good performance on punctuation prediction often requires large amounts of labeled speech transcripts, which is expensive and laborious. In this paper, we propose a Discriminative Self-Training approach with weighted loss and discriminative label smoothing to exploit unlabeled speech transcripts. Experimental results on the English IWSLT2011 benchmark test set and an internal Chinese spoken language dataset demonstrate that the proposed approach achieves significant improvement on punctuation prediction accuracy over strong baselines including BERT, RoBERTa, and ELECTRA models. The proposed Discriminative Self-Training approach outperforms the vanilla self-training approach. We establish a new state-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current SOTA model by 1.3% absolute gain on F",
    "keywords": [],
    "checked": true,
    "id": "178808258cb73831f5a39781842ce01419f04883",
    "semantic_title": "discriminative self-training for punctuation prediction",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ihori21_interspeech.html": {
    "title": "Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks Using Switching Tokens",
    "volume": "main",
    "abstract": "In this paper, we propose a novel spoken-text-style conversion method that can simultaneously execute multiple style conversion modules such as punctuation restoration and disfluency deletion without preparing matched datasets. In practice, transcriptions generated by automatic speech recognition systems are not highly readable because they often include many disfluencies and do not include punctuation marks. To improve their readability, multiple spoken-text-style conversion modules that individually model a single conversion task are cascaded because matched datasets that simultaneously handle multiple conversion tasks are often unavailable. However, the cascading is unstable against the order of tasks because of the chain of conversion errors. Besides, the computation cost of the cascading must be higher than the single conversion. To execute multiple conversion tasks simultaneously without preparing matched datasets, our key idea is to distinguish individual conversion tasks using the In our proposed zero-shot joint modeling, we switch the individual tasks using multiple switching tokens, enabling us to utilize a zero-shot learning approach to executing simultaneous conversions. Our experiments on joint modeling of disfluency deletion and punctuation restoration demonstrate the effectiveness of our method",
    "keywords": [],
    "checked": true,
    "id": "733a84c6fae83d584c4e338d8496bc165671d6a7",
    "semantic_title": "zero-shot joint modeling of multiple spoken-text-style conversion tasks using switching tokens",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21_interspeech.html": {
    "title": "A Noise Robust Method for Word-Level Pronunciation Assessment",
    "volume": "main",
    "abstract": "The common approach for pronunciation evaluation is based on Goodness of pronunciation (GOP). It has been found that GOP may perform worse under noise conditions. Traditional methods compensate pronunciation features to improve the performance of pronunciation assessment in noise situations. This paper proposed a noise robust model for word-level pronunciation assessment based on a domain adversarial training (DAT) method. We treat the pronunciation assessment in the clean and noise situations as the source and target domains. The network is optimized by incorporating both the pronunciation assessment and noise domain discrimination. The domain labels are generated from unsupervised methods to adapt to various noise situations. We evaluate the model performance based on English words recorded by Chinese English learners and labeled by three experts. Experimental results show on average the proposed model outperforms the baseline by 3% in Pearson correlation coefficients (PCC) and 4% in accuracy under different noise conditions",
    "keywords": [],
    "checked": true,
    "id": "73be6722a76b57f2855a65cbb5f796b18619c09b",
    "semantic_title": "a noise robust method for word-level pronunciation assessment",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wintrode21_interspeech.html": {
    "title": "Targeted Keyword Filtering for Accelerated Spoken Topic Identification",
    "volume": "main",
    "abstract": "We present a novel framework for spoken topic identification that simultaneously learns both topic-specific keywords and acoustic keyword filters from only document-level topic labels. At inference time, only audio segments likely to contain topic-salient keywords are fully decoded, reducing the system's overall computation cost. We show that this filtering allows for effective topic classification while decoding only 50% of ASR output word lattices, and achieves error rates within 1.2% and precision within 2.6% of an unfiltered baseline system",
    "keywords": [],
    "checked": true,
    "id": "7b04775139acf18a4ad498077dddaaf49130f8cd",
    "semantic_title": "targeted keyword filtering for accelerated spoken topic identification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/palaskar21_interspeech.html": {
    "title": "Multimodal Speech Summarization Through Semantic Concept Learning",
    "volume": "main",
    "abstract": "We propose a cascaded multimodal abstractive speech summarization model that generates semantic concepts as an intermediate step towards summarization. We describe a method to leverage existing multimodal dataset annotations to curate groundtruth labels for such intermediate concept modeling. In addition to cascaded training, the concept labels also provide an interpretable intermediate output level that helps improve performance on the downstream summarization task. On the open-domain How2 data, we conduct utterance-level and video-level experiments for two granularities of concepts: Specific and Abstract. We compare various multimodal fusion models for concept generation based on the respective input modalities. We observe consistent improvements in concept modeling by using multimodal adaptation models over unimodal models. Using the cascaded multimodal speech summarization model, we see a significant improvement of 7.5 METEOR points and 5.1 ROUGE-L points compared to previous methods of speech summarization. Finally, we show the benefits of scalability of the proposed approaches on 2000 h of video data",
    "keywords": [],
    "checked": true,
    "id": "297b515fa8e5a129d757dd48065e51656dae55ba",
    "semantic_title": "multimodal speech summarization through semantic concept learning",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21_interspeech.html": {
    "title": "Enhancing Semantic Understanding with Self-Supervised Methods for Abstractive Dialogue Summarization",
    "volume": "main",
    "abstract": "Contextualized word embeddings can lead to state-of-the-art performances in natural language understanding. Recently, a pre-trained deep contextualized text encoder such as BERT has shown its potential in improving natural language tasks including abstractive summarization. Existing approaches in dialogue summarization focus on incorporating a large language model into summarization task trained on large-scale corpora consisting of news articles rather than dialogues of multiple speakers. In this paper, we introduce self-supervised methods to compensate shortcomings to train a dialogue summarization model. Our principle is to detect incoherent information flows using pretext dialogue text to enhance BERT's ability to contextualize the dialogue text representations. We build and fine-tune an abstractive dialogue summarization model on a shared encoder-decoder architecture using the enhanced BERT. We empirically evaluate our abstractive dialogue summarizer with the SAMSum corpus, a recently introduced dataset with abstractive dialogue summaries. All of our methods have contributed improvements to abstractive summary measured in ROUGE scores. Through an extensive ablation study, we also present a sensitivity analysis to critical model hyperparameters, probabilities of switching utterances and masking interlocutors",
    "keywords": [],
    "checked": true,
    "id": "e661fabf7e3408afc8bbe47b8ea096867494afd6",
    "semantic_title": "enhancing semantic understanding with self-supervised methods for abstractive dialogue summarization",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wodarczak21_interspeech.html": {
    "title": "Speaker Transition Patterns in Three-Party Conversation: Evidence from English, Estonian and Swedish",
    "volume": "main",
    "abstract": "During conversation, speakers hold and relinquish the floor, resulting in turn yield and retention. We examine these phenomena in three-party conversations in English, Swedish, and Estonian. We define within- and between-speaker transitions in terms of shorter intervals of speech, silence and overlap bounded by stretches of one-party speech longer than 1 second by the same or different speakers. This method gives us insights into how turn change and retention proceed, revealing that the majority of speaker transitions are more complex and involve more intermediate activity than a single silence or overlap. We examine the composition of within and between transitions in terms of number of speakers involved, incidence and proportion of solo speech, silence and overlap. We derive the most common within- and between-speaker transitions in the three languages, finding evidence of striking commonalities in how the floor is managed. Our findings suggest that current models of turn-taking used in dialogue technology could be extended using these results to more accurately reflect the realities of human-human dialogue",
    "keywords": [],
    "checked": true,
    "id": "849e1128dff8ac41315eef1962743ffc158c5bc3",
    "semantic_title": "speaker transition patterns in three-party conversation: evidence from english, estonian and swedish",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/broughton21_interspeech.html": {
    "title": "Investigating Deep Neural Structures and their Interpretability in the Domain of Voice Conversion",
    "volume": "main",
    "abstract": "Generative Adversarial Networks (GANs) are machine learning networks based around creating synthetic data. Voice Conversion (VC) is a subset of voice translation that involves translating the paralinguistic features of a source speaker to a target speaker while preserving the linguistic information. The aim of non-parallel conditional GANs for VC is to translate an acoustic speech feature sequence from one domain to another without the use of paired data. In the study reported here, we investigated the interpretability of state-of-the-art implementations of non-parallel GANs in the domain of VC. We show that the learned representations in the repeating layers of a particular GAN architecture remain close to their original random initialised parameters, demonstrating that it is the number of repeating layers that is more responsible for the quality of the output. We also analysed the learned representations of a model trained on one particular dataset when used during transfer learning on another dataset. This also showed high levels of similarity in the repeating layers. Together, these results provide new insight into how the learned representations of deep generative networks change during learning and the importance of the number of layers, which would help build better GAN-based speech conversion models",
    "keywords": [],
    "checked": true,
    "id": "971818b7c6ceea6ee06ba73b7d44ee92e1afdc88",
    "semantic_title": "investigating deep neural structures and their interpretability in the domain of voice conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21b_interspeech.html": {
    "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training",
    "volume": "main",
    "abstract": "Emotional voice conversion (EVC) aims to change the emotional state of an utterance while preserving the linguistic content and speaker identity. In this paper, we propose a novel 2-stage training strategy for sequence-to-sequence emotional voice conversion with a limited amount of emotional speech data. We note that the proposed EVC framework leverages text-to-speech (TTS) as they share a common goal that is to generate high-quality expressive voice. In stage 1, we perform style initialization with a multi-speaker TTS corpus, to disentangle speaking style and linguistic content. In stage 2, we perform emotion training with a limited amount of emotional speech data, to learn how to disentangle emotional style and linguistic information from the speech. The proposed framework can perform both spectrum and prosody conversion and achieves significant improvement over the state-of-the-art baselines in both objective and subjective evaluation",
    "keywords": [],
    "checked": true,
    "id": "21df371e20d80039a90fddd98f06eb545cf41ceb",
    "semantic_title": "limited data emotional voice conversion leveraging text-to-speech: two-stage sequence-to-sequence training",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ding21_interspeech.html": {
    "title": "Adversarial Voice Conversion Against Neural Spoofing Detectors",
    "volume": "main",
    "abstract": "The naturalness and similarity of voice conversion have been significantly improved in recent years with the development of deep-learning-based conversion models and neural vocoders. Accordingly, the task of detecting spoofing speech also attracts research attention. In the latest ASVspoof 2019 challenge, the best spoofing detection model can distinguish most artificial utterances from natural ones. Inspired by recent progress of adversarial example generation, this paper proposes an adversarial post-processing network (APN) which generates adversarial examples against a neural-network-based spoofing detector by white-box attack. The APN model post-processes the speech waveforms generated by a baseline voice conversion system. An adversarial loss derived from the spoofing detector together with two regularization losses are applied to optimize the parameters of APN. In our experiments, using the logical access (LA) dataset of ASVspoof 2019, results show that our proposed method can improve the adversarial ability of converted speech against the spoofing detectors based on light convolution neural networks (LCNNs) effectively without degrading its subjective quality",
    "keywords": [],
    "checked": true,
    "id": "df1ee465fea51bec203a36f09774d558df37c00d",
    "semantic_title": "adversarial voice conversion against neural spoofing detectors",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/he21b_interspeech.html": {
    "title": "An Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation",
    "volume": "main",
    "abstract": "Emotional Voice Conversion (EVC) aims to convert the emotional style of a source speech signal to a target style while preserving its content and speaker identity information. Previous emotional conversion studies do not disentangle emotional information from emotion-independent information that should be preserved, thus transforming it all in a monolithic manner and generating audio of low quality, with linguistic distortions. To address this distortion problem, we propose a novel StarGAN framework along with a two-stage training process that separates emotional features from those independent of emotion by using an autoencoder with two encoders as the generator of the Generative Adversarial Network (GAN). The proposed model achieves favourable results in both the objective evaluation and the subjective evaluation in terms of distortion, which reveals that the proposed model can effectively reduce distortion. Furthermore, in data augmentation experiments for end-to-end speech emotion recognition, the proposed StarGAN model achieves an increase of 2% in Micro-F1 and 5% in Macro-F1 compared to the baseline StarGAN model, which indicates that the proposed model is more valuable for data augmentation",
    "keywords": [],
    "checked": true,
    "id": "8afc1ea6ee94a212f5a8439a57dc6adf733f32da",
    "semantic_title": "an improved stargan for emotional voice conversion: enhancing voice quality and data augmentation",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21e_interspeech.html": {
    "title": "TVQVC: Transformer Based Vector Quantized Variational Autoencoder with CTC Loss for Voice Conversion",
    "volume": "main",
    "abstract": "Techniques of voice conversion (VC) aim to modify the speaker identity and style of an utterance while preserving the linguistic content. Although there are lots of VC methods, the state of the art of VC is still cascading automatic speech recognition (ASR) and text-to-speech (TTS). This paper presents a new structure of vector-quantized autoencoder based on transformer with CTC loss for non-parallel VC, which inspired by cascading ASR and TTS VC method. Our proposed method combines CTC loss and vector quantization to get high-level linguistic information without speaker information. Objective and subjective evaluations on the mandarin datasets show that the converted speech of our proposed model is better than baselines on naturalness, rhythm and speaker similarity",
    "keywords": [],
    "checked": true,
    "id": "32e4f594da0eb1d1ca72f01a52ab63dff7fd5189",
    "semantic_title": "tvqvc: transformer based vector quantized variational autoencoder with ctc loss for voice conversion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21g_interspeech.html": {
    "title": "Enriching Source Style Transfer in Recognition-Synthesis Based Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "Current voice conversion (VC) methods can successfully convert timbre of the audio. As modeling source audio's prosody effectively is a challenging task, there are still limitations of transferring source style to the converted speech. This study proposes a source style transfer method based on recognition-synthesis framework. Previously in speech generation task, prosody can be modeled explicitly with prosodic features or implicitly with a latent prosody extractor. In this paper, taking advantages of both, we model the prosody in a hybrid manner, which effectively combines explicit and implicit methods in a proposed prosody module. Specifically, prosodic features are used to explicit model prosody, while VAE and reference encoder are used to implicitly model prosody, which take Mel spectrum and bottleneck feature as input respectively. Furthermore, adversarial training is introduced to remove speaker-related information from the VAE outputs, avoiding leaking source speaker information while transferring style. Finally, we use a modified self-attention based encoder to extract sentential context from bottleneck features, which also implicitly aggregates the prosodic aspects of source speech from the layered representations. Experiments show that our approach is superior to the baseline and a competitive system in terms of style transfer; meanwhile, the speech quality and speaker similarity are well maintained",
    "keywords": [],
    "checked": true,
    "id": "682482dc466a96637771f8d37da50ed3c82e487f",
    "semantic_title": "enriching source style transfer in recognition-synthesis based non-parallel voice conversion",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21b_interspeech.html": {
    "title": "S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations",
    "volume": "main",
    "abstract": "Any-to-any voice conversion (VC) aims to convert the timbre of utterances from and to any speakers seen or unseen during training. Various any-to-any VC approaches have been proposed like AutoVC, AdaINVC, and FragmentVC. AutoVC, and AdaINVC utilize source and target encoders to disentangle the content and speaker information of the features. FragmentVC utilizes two encoders to encode source and target information and adopts cross attention to align the source and target features with similar phonetic content. Moreover, pretrained features are adopted. AutoVC used d-vector to extract speaker information, and self-supervised learning (SSL) features like wav2vec 2.0 is used in FragmentVC to extract the phonetic content information. Different from previous works, we proposed S2VC that utilizes Self-Supervised features as both source and target features for the VC model. Supervised phoneme posteriorgram (PPG), which is believed to be speaker-independent and widely used in VC to extract content information, is chosen as a strong baseline for SSL features. The objective evaluation and subjective evaluation both show models taking SSL feature CPC as both source and target features outperforms that taking PPG as source feature, suggesting that SSL features have great potential in improving VC",
    "keywords": [],
    "checked": true,
    "id": "6fcaafc1f8ae649899e978a7a9baf682755dc7de",
    "semantic_title": "s2vc: a framework for any-to-any voice conversion with self-supervised pretrained representations",
    "citation_count": 39,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liberatore21_interspeech.html": {
    "title": "An Exemplar Selection Algorithm for Native-Nonnative Voice Conversion",
    "volume": "main",
    "abstract": "We present an algorithm for selecting exemplars for native-to-nonnative voice conversion (VC) using a Sparse, Anchor-Based Representation of speech (SABR). The algorithm uses phoneme labels and clustering to learn optimal exemplars when source and target speakers are affected by poor time alignment, as is common in in native-to-nonnative voice conversion. We evaluate the method on speech from the ARCTIC and L2-ARCTIC corpora and compare it to a baseline exemplar-based VC algorithm. The proposed algorithm significantly improves synthesis quality and more than doubles that of a baseline exemplar-based VC system while using two orders of magnitude fewer atoms. Additionally, the proposed algorithm significantly reduces the VC error and improves the synthesis quality as compared to unoptimized SABR models. We discuss the implications of both optimization algorithms for SABR and broader exemplar-based VC systems.Index terms should be included as shown below",
    "keywords": [],
    "checked": true,
    "id": "99e695a12b396db6b75536d26304db2de52b40b4",
    "semantic_title": "an exemplar selection algorithm for native-nonnative voice conversion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21h_interspeech.html": {
    "title": "Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion",
    "volume": "main",
    "abstract": "Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC). Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors. State-of-the-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement. To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning. Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT. The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others. Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58",
    "keywords": [],
    "checked": true,
    "id": "607c18c160aa66c13314b9da1e89639b79f67bca",
    "semantic_title": "adversarially learning disentangled speech representations for robust multi-factor voice conversion",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luong21_interspeech.html": {
    "title": "Many-to-Many Voice Conversion Based Feature Disentanglement Using Variational Autoencoder",
    "volume": "main",
    "abstract": "Voice conversion is a challenging task which transforms the voice characteristics of a source speaker to a target speaker without changing linguistic content. Recently, there have been many works on many-to-many Voice Conversion (VC) based on Variational Autoencoder (VAEs) achieving good results, however, these methods lack the ability to disentangle speaker identity and linguistic content to achieve good performance on unseen speaker's scenarios. In this paper, we propose a new method based on feature disentanglement to tackle many-to-many voice conversion. The method has the capability to disentangle speaker identity and linguistic content from utterances, it can convert from many source speakers to many target speakers with a single autoencoder network. Moreover, it naturally deals with the unseen target speaker's scenarios. We perform both objective and subjective evaluations to show the competitive performance of our proposed method compared with other state-of-the-art models in terms of naturalness and target speaker similarity",
    "keywords": [],
    "checked": true,
    "id": "9cf89ded1cdd420f1d3acd09a843741e1367f6aa",
    "semantic_title": "many-to-many voice conversion based feature disentanglement using variational autoencoder",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chouchane21_interspeech.html": {
    "title": "Privacy-Preserving Voice Anti-Spoofing Using Secure Multi-Party Computation",
    "volume": "main",
    "abstract": "In recent years the automatic speaker verification (ASV) community has grappled with vulnerabilities to spoofing attacks whereby fraudsters masquerade as enrolled subjects to provoke illegitimate accepts. Countermeasures have hence been developed to protect ASV systems from such attacks. Given that recordings of speech contain potentially sensitive information, any system operating upon them, including spoofing countermeasures, must have provisions for privacy preservation. While privacy enhancing technologies such as Homomorphic Encryption or Secure Multi-Party Computation (MPC) are effective in preserving privacy, these tend to impact upon computational capacity and computational precision, while no available spoofing countermeasures preserve privacy. This paper reports the first solution based upon the combination of shallow neural networks with secure MPC. Experiments performed using the ASVspoof 2019 logical access database show that the proposed solution is not only computationally efficient, but that it also improves upon the performance of the ASVspoof baseline countermeasure, all while preserving privacy",
    "keywords": [],
    "checked": true,
    "id": "b419209b1b34847b42ed80b52b7ce50ba3d08713",
    "semantic_title": "privacy-preserving voice anti-spoofing using secure multi-party computation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/aloufi21_interspeech.html": {
    "title": "Configurable Privacy-Preserving Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Voice assistive technologies have given rise to far-reaching privacy and security concerns. In this paper we investigate whether modular automatic speech recognition (ASR) can improve privacy in voice assistive systems by combining independently trained separation, recognition, and discretization modules to design configurable privacy-preserving ASR systems. We evaluate privacy concerns and the effects of applying various state-of-the-art techniques at each stage of the system, and report results using task-specific metrics (i.e., WER, ABX, and accuracy). We show that overlapping speech inputs to ASR systems present further privacy concerns, and how these may be mitigated using speech separation and optimization techniques. Our discretization module is shown to minimize paralinguistics privacy leakage from ASR acoustic models to levels commensurate with random guessing. We show that voice privacy can be , and argue this presents new opportunities for privacy-preserving applications incorporating ASR",
    "keywords": [],
    "checked": true,
    "id": "6a9cc72a0b18ea375eebdc352f7244d33a8f762f",
    "semantic_title": "configurable privacy-preserving automatic speech recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/novotney21_interspeech.html": {
    "title": "Adjunct-Emeritus Distillation for Semi-Supervised Language Model Adaptation",
    "volume": "main",
    "abstract": "To improve customer privacy, commercial speech applications are reducing human transcription of customer data. This has a negative impact on language model training due to a smaller amount of in-domain transcripts. Prior work demonstrated that training on automated transcripts alone provides modest gains due to reinforcement of recognition errors. We consider a new condition, where a model trained on historical human transcripts, but not the transcripts themselves, are available to us. To overcome temporal drift in vocabulary and topics, we propose a novel extension of knowledge distillation, where two imperfect teachers jointly train a student model. We conduct experiments on an English voice assistant domain and simulate a one year gap in human transcription. Unlike fine-tuning, our approach is architecture agnostic and achieves a 14% relative reduction in perplexity over the baseline approach of freezing model development and improves over the baseline of knowledge distillation",
    "keywords": [],
    "checked": true,
    "id": "aea05c410fd750d90f43aabc1fd92c3625fc20fd",
    "semantic_title": "adjunct-emeritus distillation for semi-supervised language model adaptation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ro21_interspeech.html": {
    "title": "Communication-Efficient Agnostic Federated Averaging",
    "volume": "main",
    "abstract": "In distributed learning settings such as federated learning, the training algorithm can be potentially biased towards different clients. [1] proposed a domain-agnostic learning algorithm, where the model is optimized for any target distribution formed by a mixture of the client distributions in order to overcome this bias. They further proposed an algorithm for the cross-silo federated learning setting, where the number of clients is small. We consider this problem in the cross-device setting, where the number of clients is much larger. We propose a communication-efficient distributed algorithm called Agnostic Federated Averaging (or AgnosticFedAvg) to minimize the domain-agnostic objective proposed in [1], which is amenable to other private mechanisms such as secure aggregation. We highlight two types of naturally occurring domains in federated learning and argue that AgnosticFedAvg performs well on both. To demonstrate the practical effectiveness of AgnosticFedAvg, we report positive results for large-scale language modeling tasks in both simulation and live experiments, where the latter involves training language models for Spanish virtual keyboard for millions of user devices",
    "keywords": [],
    "checked": true,
    "id": "25c6e17a64548994befaca422748b6fe7478af0a",
    "semantic_title": "communication-efficient agnostic federated averaging",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/koppelmann21_interspeech.html": {
    "title": "Privacy-Preserving Feature Extraction for Cloud-Based Wake Word Verification",
    "volume": "main",
    "abstract": "Wake word detection and verification systems often involve a local, on-device wake word detector and a cloud-based verification node. In such systems, the audio representation sent to the cloud-based server may exhibit sensitive information that might be intercepted by an eavesdropper. To improve privacy of cloud-based wake word verification (WWV) systems, we propose to use a privacy-preserving feature representation that minimizes the automatic speech recognition (ASR) capability of a potential attacker. The proposed approach employs an adversarial training schedule that aims to minimize an attacker's word error rate (WER) while maintaining a high WWV performance. To this end, we apply an adaptive weighting factor in the combined loss function to control the balance between minimizing the WWV loss and maximizing the ASR loss. We show that the proposed training method significantly reduces possible privacy risks while maintaining a strong WWV performance",
    "keywords": [],
    "checked": true,
    "id": "6b772b81e93deed6b35fbb2cffc778fb93a6b52f",
    "semantic_title": "privacy-preserving feature extraction for cloud-based wake word verification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21_interspeech.html": {
    "title": "PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification",
    "volume": "main",
    "abstract": "We propose using an adversarial autoencoder (AAE) to replace generative adversarial network (GAN) in private aggregation of teacher ensembles (PATE), a solution for ensuring differential privacy in speech applications. The AAE architecture allows us to obtain good synthetic speech leveraging upon a discriminative training of latent vectors. Such synthetic speech is used to build a privacy-preserving classifier when non-sensitive data is not sufficiently available in the public domain. This classifier follows the PATE scheme that uses an ensemble of noisy outputs to label the synthetic samples and guarantee ε-differential privacy (DP) on its derived classifiers. Our proposed framework thus consists of an AAE-based generator and a PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands Dataset Version II, the proposed PATE-AAE improves the average classification accuracy by +2.11% and +6.60%, respectively, when compared with alternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while maintaining a strong level of privacy target at ε=0.01 with a fixed δ=10",
    "keywords": [],
    "checked": true,
    "id": "2bbfd3671198bc23a96cb7f992e3faca62721ee6",
    "semantic_title": "pate-aae: incorporating adversarial autoencoder into private aggregation of teacher ensembles for spoken command classification",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21b_interspeech.html": {
    "title": "Continual Learning for Fake Audio Detection",
    "volume": "main",
    "abstract": "Fake audio attack becomes a major threat to the speaker verification system. Although current detection approaches have achieved promising results on dataset-specific scenarios, they encounter difficulties on unseen spoofing data. Fine-tuning and retraining from scratch have been applied to incorporate new data. However, fine-tuning leads to performance degradation on previous data. Retraining takes a lot of time and computation resources. Besides, previous data are unavailable due to privacy in some situations. To solve the above problems, this paper proposes detecting fake without forgetting, a continual-learning-based method, to make the model learn new spoofing attacks incrementally. A knowledge distillation loss is introduced to loss function to preserve the memory of original model. Supposing the distribution of genuine voice is consistent among different scenarios, an extra embedding similarity loss is used as another constraint to further do a positive sample alignment. Experiments are conducted on the ASVspoof2019 dataset. The results show that our proposed method outperforms fine-tuning by the relative reduction of average equal error rate up to 81.62%",
    "keywords": [],
    "checked": true,
    "id": "8c632eb06fe6e70f20705429f612a6aeccb0a4fd",
    "semantic_title": "continual learning for fake audio detection",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shah21_interspeech.html": {
    "title": "Evaluating the Vulnerability of End-to-End Automatic Speech Recognition Models to Membership Inference Attacks",
    "volume": "main",
    "abstract": "Recent studies have shown that it may be possible to determine if a machine learning model was trained on a given data sample, using Membership Inference Attacks (MIA). In this paper we evaluate the vulnerability of state-of-the-art speech recognition models to MIA under black-box access. Using models trained with standard methods and public datasets, we demonstrate that without any knowledge of the target model's parameters or training data a MIA can successfully infer membership with precision and recall more than 60%. Furthermore, for utterances from about 39% of the speakers the precision is more than 75%, indicating that training data membership can be inferred more precisely for some speakers than others. While strong regularization reduces the overall accuracy of MIA to almost 50%, the attacker can still infer membership for utterances from 25% of the speakers with high precision. These results indicate that (1) speaker-level MIA success should be reported, along with overall accuracy, to provide a holistic view of the model's vulnerability and (2) conventional regularization is an inadequate defense against MIA.We believe that the insights gleaned from this study can direct future work towards more effective defenses",
    "keywords": [],
    "checked": true,
    "id": "c91b8cb2129654ab91f9374e10c90756f64ee880",
    "semantic_title": "evaluating the vulnerability of end-to-end automatic speech recognition models to membership inference attacks",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fazel21_interspeech.html": {
    "title": "SynthASR: Unlocking Synthetic Data for Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have recently demonstrated superior performance over the traditional hybrid ASR models. Training an E2E ASR model requires a large amount of data which is not only expensive but may also raise dependency on production data. At the same time, synthetic speech generated by the state-of-the-art text-to-speech (TTS) engines has advanced to near-human naturalness. In this work, we propose to utilize synthetic speech for ASR training (SynthASR) in applications where data is sparse or hard to get for ASR model training. In addition, we apply continual learning with a novel multi-stage training strategy to address catastrophic forgetting, achieved by a mix of weighted multi-style training, data augmentation, encoder freezing, and parameter regularization. In our experiments conducted on in-house datasets for a new application of recognizing medication names, training ASR RNN-T models with synthetic audio via the proposed multi-stage training improved the recognition performance on new application by more than 65% relative, without degradation on existing general applications. Our observations show that SynthASR holds great promise in training the state-of-the-art large-scale E2E ASR models for new applications while reducing the costs and dependency on production data",
    "keywords": [],
    "checked": true,
    "id": "2b7f54a8bbda05d07871f1662d5397160bde8de7",
    "semantic_title": "synthasr: unlocking synthetic data for speech recognition",
    "citation_count": 31,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muguli21_interspeech.html": {
    "title": "DiCOVA Challenge: Dataset, Task, and Baseline System for COVID-19 Diagnosis Using Acoustics",
    "volume": "main",
    "abstract": "The DiCOVA challenge aims at accelerating research in diagnosing COVID-19 using acoustics (DiCOVA), a topic at the intersection of speech and audio processing, respiratory health diagnosis, and machine learning. This challenge is an open call for researchers to analyze a dataset of sound recordings, collected from COVID-19 infected and non-COVID-19 individuals, for a two-class classification. These recordings were collected via crowdsourcing from multiple countries, through a website application. The challenge features two tracks, one focusing on cough sounds, and the other on using a collection of breath, sustained vowel phonation, and number counting speech recordings. In this paper, we introduce the challenge and provide a detailed description of the task, and present a baseline system for the task",
    "keywords": [],
    "checked": true,
    "id": "02093d2179a4eef9fa5bf0d59498f60e1a6c78b6",
    "semantic_title": "dicova challenge: dataset, task, and baseline system for covid-19 diagnosis using acoustics",
    "citation_count": 82,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kamble21_interspeech.html": {
    "title": "PANACEA Cough Sound-Based Diagnosis of COVID-19 for the DiCOVA 2021 Challenge",
    "volume": "main",
    "abstract": "The COVID-19 pandemic has led to the saturation of public health services worldwide. In this scenario, the early diagnosis of SARS-Cov-2 infections can help to stop or slow the spread of the virus and to manage the demand upon health services. This is especially important when resources are also being stretched by heightened demand linked to other seasonal diseases, such as the flu. In this context, the organisers of the DiCOVA 2021 challenge have collected a database with the aim of diagnosing COVID-19 through the use of coughing audio samples. This work presents the details of the automatic system for COVID-19 detection from cough recordings presented by team PANACEA. This team consists of researchers from two European academic institutions and one company: EURECOM (France), University of Granada (Spain), and Biometric Vox S.L. (Spain). We developed several systems based on established signal processing and machine learning methods. Our best system employs a Teager energy operator cepstral coefficients (TECCs) based front-end and Light gradient boosting machine (LightGBM) back-end. The AUC obtained by this system on the test set is 76.31% which corresponds to a 10% improvement over the official baseline",
    "keywords": [],
    "checked": true,
    "id": "d095e8160ad109057369c515d5e8ffc4c607454d",
    "semantic_title": "panacea cough sound-based diagnosis of covid-19 for the dicova 2021 challenge",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karas21_interspeech.html": {
    "title": "Recognising Covid-19 from Coughing Using Ensembles of SVMs and LSTMs with Handcrafted and Deep Audio Features",
    "volume": "main",
    "abstract": "As the Covid-19 pandemic continues, digital health solutions can provide valuable insights and assist in diagnosis and prevention. Since the disease affects the respiratory system, it is hypothesised that sound formation is changed, and thus, an infection can be automatically recognised through audio analysis. We present an ensemble learning approach used in our entry to Track 1 of the DiCOVA 2021 Challenge, which aims at binary classification of Covid-19 infection on a crowd-sourced dataset of 1 040 cough sounds. Our system is based on a combination of handcrafted features for paralinguistics with deep feature extraction from spectrograms using pre-trained CNNs. We extract features both at segment level and with a sliding window approach, and process them with SVMs and LSTMs, respectively. We then perform least-squares weighted late fusion of our classifiers. Our system surpasses the challenge baseline, with a ROC-AUC on the test set of 78.18%",
    "keywords": [],
    "checked": true,
    "id": "5cf0da43f1a2e92c58185235e8f3d4e319db31c0",
    "semantic_title": "recognising covid-19 from coughing using ensembles of svms and lstms with handcrafted and deep audio features",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sodergren21_interspeech.html": {
    "title": "Detecting COVID-19 from Audio Recording of Coughs Using Random Forests and Support Vector Machines",
    "volume": "main",
    "abstract": "The detection of COVID-19 is and will remain in the foreseeable future a crucial challenge, making the development of tools for the task important. One possible approach, on the confines of speech and audio processing, is detecting potential COVID-19 cases based on cough sounds. We propose a simple, yet robust method based on the well-known ComParE 2016 feature set, and two classical machine learning models, namely Random Forests, and Support Vector Machines (SVMs). Furthermore, we combine the two methods, by calculating the weighted average of their predictions. Our results in the DiCOVA challenge show that this simple approach leads to a robust solution while producing competitive results. Based on the Area Under the Receiver Operating Characteristic Curve (AUC ROC) score, both classical machine learning methods we applied markedly outperform the baseline provided by the challenge organisers. Moreover, their combination attains an AUC ROC score of 85.21, positioning us at fourth place on the leaderboard (where the second team attained a similar, 85.43 score). Here, we would describe this system in more detail, and analyse the resulting models, drawing conclusions, and determining future work directions",
    "keywords": [],
    "checked": true,
    "id": "24a23e0509771e58bfcd5cb5a6d0953a1e3da5dc",
    "semantic_title": "detecting covid-19 from audio recording of coughs using random forests and support vector machines",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/das21_interspeech.html": {
    "title": "Diagnosis of COVID-19 Using Auditory Acoustic Cues",
    "volume": "main",
    "abstract": "COVID-19 can be pre-screened based on symptoms and confirmed using other laboratory tests. The cough or speech from patients are also studied in the recent time for detection of COVID-19 as they are indicators of change in anatomy and physiology of the respiratory system. Along this direction, the diagnosis of COVID-19 using acoustics (DiCOVA) challenge aims to promote such research by releasing publicly available cough/speech corpus. We participated in the Track-1 of the challenge, which deals with COVID-19 detection using cough sounds from individuals. In this challenge, we use a few novel auditory acoustic cues based on long-term transform, equivalent rectangular bandwidth spectrum and gammatone filterbank. We evaluate these representations using logistic regression, random forest and multilayer perceptron classifiers for detection of COVID-19. On the blind test set, we obtain an area under the ROC curve (AUC) of 83.49% for the best system submitted to the challenge. It is worth noting that the submitted system ranked among the top few systems on the leaderboard and outperformed the challenge baseline by a large margin",
    "keywords": [],
    "checked": true,
    "id": "8140fb53adf948abdf0c00a59220a35e9c3f31db",
    "semantic_title": "diagnosis of covid-19 using auditory acoustic cues",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/harvill21_interspeech.html": {
    "title": "Classification of COVID-19 from Cough Using Autoregressive Predictive Coding Pretraining and Spectral Data Augmentation",
    "volume": "main",
    "abstract": "Serum and saliva-based testing methods have been crucial to slowing the COVID-19 pandemic, yet have been limited by slow throughput and cost. A system able to determine COVID-19 status from cough sounds alone would provide a low cost, rapid, and remote alternative to current testing methods. We explore the applicability of recent techniques such as pre-training and spectral augmentation in improving the performance of a neural cough classification system. We use Autoregressive Predictive Coding (APC) to pre-train a unidirectional LSTM on the COUGHVID dataset. We then generate our final model by fine-tuning added BLSTM layers on the DiCOVA challenge dataset. We perform various ablation studies to see how each component impacts performance and improves generalization with a small dataset. Our final system achieves an AUC of 85.35 and places third out of 29 entries in the DiCOVA challenge",
    "keywords": [],
    "checked": true,
    "id": "536615589a2196967330642c8fc8ad860b069d10",
    "semantic_title": "classification of covid-19 from cough using autoregressive predictive coding pretraining and spectral data augmentation",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deshpande21_interspeech.html": {
    "title": "The DiCOVA 2021 Challenge — An Encoder-Decoder Approach for COVID-19 Recognition from Coughing Audio",
    "volume": "main",
    "abstract": "This paper presents the automatic recognition of COVID-19 from coughing. In particular, it describes our contribution to the DiCOVA challenge — Track 1, which addresses such cough sound analysis for COVID-19 detection. Pathologically, the effects of a COVID-19 infection on the respiratory system and on breathing patterns are known. We demonstrate the use of breathing patterns of the cough audio signal in identifying the COVID-19 status. Breathing patterns of the cough audio signal are derived using a model trained with the subset of the UCL Speech Breath Monitoring (UCL-SBM) database. This database provides speech recordings of the participants while their breathing values are captured by a respiratory belt. We use an encoder-decoder architecture. The encoder encodes the audio signal into breathing patterns and the decoder decodes the COVID-19 status for the corresponding breathing patterns using an attention mechanism. The encoder uses a pre-trained model which predicts breathing patterns from the speech signal, and transfers the learned patterns to cough audio signals With this architecture, we achieve an AUC of 64.42% on the evaluation set of Track 1",
    "keywords": [],
    "checked": true,
    "id": "dd7de0d1fd182ec5f3bdee0705a116816f07bf9d",
    "semantic_title": "the dicova 2021 challenge - an encoder-decoder approach for covid-19 recognition from coughing audio",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ritwik21_interspeech.html": {
    "title": "COVID-19 Detection from Spectral Features on the DiCOVA Dataset",
    "volume": "main",
    "abstract": "In this paper we investigate the cues of COVID-19 on sustained phonation of Vowel-/i/, deep breathing and number counting data of the DiCOVA dataset. We use an ensemble of classifiers trained on different features, namely, super-vectors, formants, harmonics and MFCC features. We fit a two-class Weighted SVM classifier to separate the COVID-19 audio from Non-COVID-19 audio. Weighted penalties help mitigate the challenge of class imbalance in the dataset. The results are reported on the stationary (breathing, Vowel-/i/) and non-stationary (counting data) data using individual and combination of features on each type of utterance. We find that the Formant information plays a crucial role in classification. The proposed system resulted in an AUC score of 0.734 for cross validation, and 0.717 for evaluation dataset",
    "keywords": [],
    "checked": true,
    "id": "021b2b919eec8add2e2a9c7a168177cc88e6ea32",
    "semantic_title": "covid-19 detection from spectral features on the dicova dataset",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mallolragolta21_interspeech.html": {
    "title": "Cough-Based COVID-19 Detection with Contextual Attention Convolutional Neural Networks and Gender Information",
    "volume": "main",
    "abstract": "The aim of this contribution is to automatically detect COVID-19 patients by analysing the acoustic information embedded in coughs. COVID-19 affects the respiratory system, and, consequently, respiratory-related signals have the potential to contain salient information for the task at hand. We focus on analysing the spectrogram representations of cough samples with the aim to investigate whether COVID-19 alters the frequency content of these signals. Furthermore, this work also assesses the impact of gender in the automatic detection of COVID-19. To extract deep-learnt representations of the spectrograms, we compare the performance of a cough-specific, and a Resnet18 pre-trained Convolutional Neural Network (CNN). Additionally, our approach explores the use of contextual attention, so the model can learn to highlight the most relevant deep-learnt features extracted by the CNN. We conduct our experiments on the dataset released for the Cough Sound Track of the DICOVA 2021 Challenge. The best performance on the test set is obtained using the Resnet18 pre-trained CNN with contextual attention, which scored an Area Under the Curve (AUC) of 70.91% at 80% sensitivity",
    "keywords": [],
    "checked": true,
    "id": "8857a1de6e8a3ba32c0a321958ad21eaff1aa622",
    "semantic_title": "cough-based covid-19 detection with contextual attention convolutional neural networks and gender information",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bhosale21_interspeech.html": {
    "title": "Contrastive Learning of Cough Descriptors for Automatic COVID-19 Preliminary Diagnosis",
    "volume": "main",
    "abstract": "Cough sounds as a descriptor have been used for detecting various respiratory ailments based on its intensity, duration of intermediate phase between two cough sounds, repetitions, dryness etc. However, COVID-19 diagnosis using only cough sounds is challenging because of cough being a common symptom among many non COVID-19 health diseases and inherent data imbalance within the available datasets. As one of the approach in this direction, we explore the robustness of multi-domain representation by performing the early fusion over a wide set of temporal, spectral and tempo-spectral handcrafted features, followed by training a Support Vector Machine (SVM) classifier. In our second approach, using a contrastive loss function we learn a latent space from Mel Filter Cepstral Coefficients (MFCCs) where representations belonging to samples having similar cough characteristics are closer. This helps learn representations for the highly varied COVID-negative class (healthy and symptomatic COVID-negative), by learning multiple smaller clusters. Using only the DiCOVA data, multi-domain features yields an absolute improvement of 0.74% and 1.07%, whereas our second approach shows an improvement of 2.09% and 3.98%, over the blind test and validation set, respectively, when compared with challenge baseline",
    "keywords": [],
    "checked": true,
    "id": "c6fc8a0b000404d62961c121301bfda0fe105ad4",
    "semantic_title": "contrastive learning of cough descriptors for automatic covid-19 preliminary diagnosis",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/avila21_interspeech.html": {
    "title": "Investigating Feature Selection and Explainability for COVID-19 Diagnostics from Cough Sounds",
    "volume": "main",
    "abstract": "In this paper, we propose an approach to automatically classify COVID-19 and non-COVID-19 cough samples based on the combination of both feature engineering and deep learning models. In the feature engineering approach, we develop a support vector machine classifier over high dimensional (6373D) space of acoustic features. In the deep learning-based approach, on the other hand, we apply a convolutional neural network trained on the log-mel spectrograms. These two methodologically diverse models are then combined by fusing the probability scores of the models. The proposed system, which ranked 9 on the 2021 Diagnosing COVID-19 using Acoustics (DiCOVA) challenge leaderboard, obtained an area under the receiver operating characteristic curve (AUC) of 0.81 on the blind test data set, which is a 10.9% absolute improvement compared to the baseline. Moreover, we analyze the explainability of the deep learning-based model when detecting COVID-19 from cough signals",
    "keywords": [],
    "checked": true,
    "id": "db0e656171e98c44b24b124c67f0d7df623cae29",
    "semantic_title": "investigating feature selection and explainability for covid-19 diagnostics from cough sounds",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kiss21_interspeech.html": {
    "title": "Application for Detecting Depression, Parkinson's Disease and Dysphonic Speech",
    "volume": "main",
    "abstract": "In this Show&Tell presentation we demonstrate an application that is able to assess a voice sample according to three different voice disorders: depression, Parkinson's disease and dysphonic speech. Affection probability of each disorder is analyzed along with their severity estimation. Although the acoustic models (support vector machine and regression models) are trained on Hungarian voice samples, English samples can also be utilized for assessment. The results are displayed by as pie chart for probabilities and separate severity scores. The input of the application is a read text with a fixed linguistic content. It is possible to load a pre-recorded voice sample or create a live recording. The developed system could evaluate a speaker's voice sample, assisting medical staff",
    "keywords": [],
    "checked": true,
    "id": "a8d8176fc5f695df8103998bd533482d2458dfc2",
    "semantic_title": "application for detecting depression, parkinson's disease and dysphonic speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/weingartova21_interspeech.html": {
    "title": "Beey: More Than a Speech-to-Text Editor",
    "volume": "main",
    "abstract": "We present Beey, a newly developed web-based multimedia platform for producing Automatic Speech Recognition (ASR) and editing its output. In addition to ASR, Beey employs modules for speaker diarization and identification, text formatting, automatic punctuation insertion, subtitling, automatic translation, transcription of stream and more The platform and its development are focused on user experience and fast document creation. Our aim is to transfer research results in the field of speech recognition and signal processing into practice and enable Beey's users to make their production processes faster and cheaper by minimizing human effort and costs",
    "keywords": [],
    "checked": true,
    "id": "35592cfd5da2a2dd721302b37c3916bccf36dc52",
    "semantic_title": "beey: more than a speech-to-text editor",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/arai21_interspeech.html": {
    "title": "Downsizing of Vocal-Tract Models to Line up Variations and Reduce Manufacturing Costs",
    "volume": "main",
    "abstract": "Demonstrating vowel production with physical models of the human vocal tract is a part of intuitive education in speech science. The adult male vocal tract was most often used as a model in the past because of the limited availability of physical models, but discussions on different vocal tract sizes were ongoing. Therefore, we focused on downsizing the vocal-tract models in this study, especially the straight models. We reduced the cross-sectional area function for the sliding three-tube model (including the total length) to female adult and child sizes. Furthermore, we created fixed straight models of similar dimensions for the five Japanese vowels. We found that the intelligibility of each model was preserved as long as the ratios of the cross-sectional areas were maintained even if the cross-sections were less than the average human sizes. This indicates that we can reduce the cost of manufacturing the models, as cost is typically a barrier when the models are used for pedagogical purposes",
    "keywords": [],
    "checked": true,
    "id": "e85d8a3618832fd00d3512fea2b41f0a514eab8d",
    "semantic_title": "downsizing of vocal-tract models to line up variations and reduce manufacturing costs",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fabien21_interspeech.html": {
    "title": "ROXANNE Research Platform: Automate Criminal Investigations",
    "volume": "main",
    "abstract": "Criminal investigations require manual intervention of several investigators and translators. However, the amount and the diversity of the data collected raises many challenges, and cross-border investigations against organized crime can quickly impossible to handle. We developed ROXANNE Research platform, an all-in-one platform which processes intercepted phone calls, runs state-of-the-art components such as speaker identification, automatic speech recognition or named entity detection, and builds a knowledge graph of the extracted information. Our aim for this work is to do a first step in the direction of an open research platform combining speech, text, and video processing algorithms with criminal network analysis for combating organized crime",
    "keywords": [],
    "checked": true,
    "id": "f9220a2365e4859f306dadf73741a963765c1d3d",
    "semantic_title": "roxanne research platform: automate criminal investigations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/flucha21_interspeech.html": {
    "title": "The LIUM Human Active Correction Platform for Speaker Diarization",
    "volume": "main",
    "abstract": "We developed a human assisted speaker diarization platform that enables a human annotator to correct the output of any speaker diarization system by providing a graphical view of the diarization segmentation and clustering steps while guiding the human annotator to optimize the correction process and easily improve the resulting diarization",
    "keywords": [],
    "checked": true,
    "id": "4c1a33cd483f7608dbc9c14a000c9fa74d83a91f",
    "semantic_title": "the lium human active correction platform for speaker diarization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oh21_interspeech.html": {
    "title": "On-Device Streaming Transformer-Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "This work is the first attempt to run streaming Transformer-based end-to-end speech recognition on embedded scale IoT systems. Recently there are many researches on online Transformer-based speech recognition such as a contextual block encoder [1] and a block-wise synchronous beam search [2]. Based on them we designed a novel fully-streaming end-to-end speech recognition method using Transformer. By efficiently utilizing a connectionist temporal classification network to detect symbol and sentence boundaries, we make decoder in streaming manner. Moreover, by using the optimized model structure, the proposed method could be deployed on a low-power edge device such as Raspberry Pi 4B with the high accuracy and the small latency. With the experiments with Librispeech corpus, the methods achieved word error rates of 3.76% and 9.25% respectively. Also the recognition speed is measured in two aspects; the real-time factor and the user perceived latency. The system is evaluated to have 0.84 xRT and the average latency of 0.75±0.62 seconds on Raspberry Pi 4B",
    "keywords": [],
    "checked": true,
    "id": "7b93d0d442d3fcc0dde20208509d35cd08b606eb",
    "semantic_title": "on-device streaming transformer-based end-to-end speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cmejla21_interspeech.html": {
    "title": "Advanced Semi-Blind Speaker Extraction and Tracking Implemented in Experimental Device with Revolving Dense Microphone Array",
    "volume": "main",
    "abstract": "We present a new device for speaker extraction and physical tracking and demonstrate its use in real conditions. The device is equipped with a dense planar array consisting of 64 microphones mounted on a rotating platform. State-of-the-art blind source extraction algorithms controlled by x-vector piloting are used to extract the desired speaker, which is being tracked by the rotating microphone array. The audience will experience the functionality of the device and the potential of the blind algorithms to extract the speaker from multi-source noisy recordings in a live situation",
    "keywords": [],
    "checked": true,
    "id": "69cdda7308c708a515b9af9aa63a37f75f4325d4",
    "semantic_title": "advanced semi-blind speaker extraction and tracking implemented in experimental device with revolving dense microphone array",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ney21_interspeech.html": {
    "title": "Forty Years of Speech and Language Processing: From Bayes Decision Rule to Deep Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": true,
    "id": "ee6871b3ed3fc7bed4016f959d82b885e7eebc18",
    "semantic_title": "forty years of speech and language processing: from bayes decision rule to deep learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chorowski21_interspeech.html": {
    "title": "Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw",
    "volume": "main",
    "abstract": "We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval",
    "keywords": [],
    "checked": true,
    "id": "77ba536360ba5e46d3072424264c02ce9d9223ab",
    "semantic_title": "information retrieval for zerospeech 2021: the submission by university of wroclaw",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chorowski21b_interspeech.html": {
    "title": "Aligned Contrastive Predictive Coding",
    "volume": "main",
    "abstract": "We investigate the possibility of forcing a self-supervised model trained using a contrastive predictive loss, to extract slowly varying latent representations. Rather than producing individual predictions for each of the future representations, the model emits a sequence of predictions shorter than the sequence of upcoming representations to which they will be aligned. In this way, the prediction network solves a simpler task of predicting the next symbols, but not their exact timing, while the encoding network is trained to produce piece-wise constant latent codes. We evaluate the model on a speech coding task and demonstrate that the proposed Aligned Contrastive Predictive Coding (ACPC) leads to higher linear phone prediction accuracy and lower ABX error rates, while being slightly faster to train due to the reduced number of prediction heads",
    "keywords": [],
    "checked": true,
    "id": "1a03d08a8ce390b8f99fd919c95fb2c2f4c0d567",
    "semantic_title": "aligned contrastive predictive coding",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/suter21_interspeech.html": {
    "title": "Neural Text Denormalization for Speech Transcripts",
    "volume": "main",
    "abstract": "This paper presents a simple sequence-to-sequence approach to restore standard orthography in raw, normalized speech transcripts, including insertion of punctuation marks, prediction of capitalization, restoration of numeric forms, formatting of dates and times, and other, fully data-driven adjustments. We further describe our method to generate synthetic parallel training data, and explore suitable performance metrics, which we align with human judgment through subjective MOS-like evaluations Our models for English, Russian, and German have a word error rate of 6.36%, 4.88%, and 5.23%, respectively. We focus on simplicity and reproducibility, make our framework available under a BSD license, and share our base models for English and Russian",
    "keywords": [],
    "checked": true,
    "id": "cf532e151c34eb2bb1c668e9f6ae1d38764a9379",
    "semantic_title": "neural text denormalization for speech transcripts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/joglekar21_interspeech.html": {
    "title": "Fearless Steps Challenge Phase-3 (FSC P3): Advancing SLT for Unseen Channel and Mission Data Across NASA Apollo Audio",
    "volume": "main",
    "abstract": "The Fearless Steps Challenge (FSC) initiative was designed to host a series of progressively complex tasks to promote advanced speech research across naturalistic \"Big Data\" corpora. The Center for Robust Speech Systems at UT-Dallas in collaboration with the National Institute of Standards and Technology (NIST) and Linguistic Data Consortium (LDC) conducted Phase-3 of the FSC series (FSC P3), with a focus on motivating speech and language technology (SLT) system generalizability across channel and mission diversity under the same training conditions as in Phase-2. The FSC P3 introduced 10 hours of previously unseen channel audio from Apollo-11 and 5 hours of novel audio from Apollo-13 to be evaluated over both previously established and newly introduced SLT tasks with streamlined tracks. This paper presents an overview of the newly introduced conversational analysis tracks, Apollo-13 data, and analysis of system performance for matched and mismatched challenge conditions. We also discuss the Phase-3 challenge results, evolution of system performance across the three Phases, and next steps in the Challenge Series",
    "keywords": [],
    "checked": true,
    "id": "e896215b18529d671f88c0b25f5799109721b94d",
    "semantic_title": "fearless steps challenge phase-3 (fsc p3): advancing slt for unseen channel and mission data across nasa apollo audio",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leykum21_interspeech.html": {
    "title": "Voice Quality in Verbal Irony: Electroglottographic Analyses of Ironic Utterances in Standard Austrian German",
    "volume": "main",
    "abstract": "When using verbal irony in interpersonal communication, paraverbal cues can reduce the risk of misunderstandings. Besides fundamental frequency, intensity and duration, speakers could use voice quality parameters to disambiguate between ironic and literal utterances. How these paraverbal cues are used to mark irony appears to be language- and/or culture-specific. Since the role of voice quality in ironic utterances has not yet been investigated in Austrian German, the present study addresses this issue. In addition to the acoustic signal, the vocal fold vibration is recorded via electroglottography (EGG). The detailed analysis of the EGG data as well as the acoustic data, provides insight into voice quality characteristics of ironic and literal realisations of short utterances. The analyses reveal that, in Standard Austrian German, some differences in voice quality exist between ironic and literal realisations of utterances: When being ironic, speakers' voices tend to be breathier, creakier or rougher. Differences are more pronounced in the older age group and in male speakers",
    "keywords": [],
    "checked": true,
    "id": "3c6e426889a2a66e3198bc5b8800e302bc6a21ae",
    "semantic_title": "voice quality in verbal irony: electroglottographic analyses of ironic utterances in standard austrian german",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hutin21_interspeech.html": {
    "title": "Synchronic Fortition in Five Romance Languages? A Large Corpus-Based Study of Word-Initial Devoicing",
    "volume": "main",
    "abstract": "Devoicing is a process whereby a voiced consonant such as /bdg/ is realized as voiceless [ptk]. Some theorists [1,2] propose that this phenomenon is an instance of fortition, or consonant strengthening, especially when it occurs word-initially. This study proposes an in-depth exploration of voicing alternations in word-initial position in five Romance languages (Portuguese, Spanish, French, Italian, Romanian) using large corpora (ca. 1000h of speech) and automatic alignment. Our results show that (i) there is initial devoicing in all languages, and (ii) this devoicing is conditioned by the preceding context. This allows the languages to be divided into those displaying (a) only phrase-initial fortition (Spanish), (b) phrase-initial and post-obstruent fortition (French, Romanian and possibly Italian) and (c) generalized word-initial fortition (Portuguese)",
    "keywords": [],
    "checked": true,
    "id": "a2bc2547710fa9d52870f0f043eda73ca5f10e8a",
    "semantic_title": "synchronic fortition in five romance languages? a large corpus-based study of word-initial devoicing",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kraljevski21_interspeech.html": {
    "title": "Glottal Stops in Upper Sorbian: A Data-Driven Approach",
    "volume": "main",
    "abstract": "We present a data-driven approach for the quantitative analysis of glottal stops before word-initial vowels in Upper Sorbian, a West Slavic minority language spoken in Germany. Glottal stops are word-boundary markers and their detection can improve the performance of automatic speech recognition and speech synthesis systems We employed cross-language transfer using an acoustic model in German to develop a forced-alignment method for the phonetic segmentation of a read-speech corpus in Upper Sorbian. The missing phonemic units were created by combining the existing phoneme models. In the forced-alignment procedure, the glottal stops were considered optional in front of word-initial vowels To investigate the influence of speaker type (males, females, and children) and vowel on the occurrence of glottal stops, binomial regression analysis with a generalized linear mixed model was performed. Results show that children glottalize word-initial vowels more frequently than adults, and that glottal stop occurrences are influenced by vowel quality",
    "keywords": [],
    "checked": true,
    "id": "f39b4141333b0d6a1966c733d758c8e0cb20bf3a",
    "semantic_title": "glottal stops in upper sorbian: a data-driven approach",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ludusan21_interspeech.html": {
    "title": "Cue Interaction in the Perception of Prosodic Prominence: The Role of Voice Quality",
    "volume": "main",
    "abstract": "Voice quality is an important dimension in human communication, used to mark a variety of phenomena in speech, including prosodic prominence. Even though numerous studies have shown that speakers modify their voice quality parameters for marking prosodic prominence, the impact of these modifications on perceived prominence is less studied. Our investigation looks at the effect of a well-known measure of voice quality, cepstral peak prominence (CPP), on syllabic prominence ratings given by both naive and expert listeners. Employing read speech materials in German, we quantify the role of CPP alone and in combination with other acoustic cues marking prominence, namely intensity, duration and fundamental frequency. While CPP, by itself, had a significant effect on the perceived prominence for most of the listeners, when used in conjunction with the other cues, its impact was reduced. Moreover, when assessing the importance of each of these four cues for determining the perceived prominence score we found important individual variation, as well as differences between naive and expert listeners",
    "keywords": [],
    "checked": true,
    "id": "427170c3bb548edecc824bf622b83faf81d08ce3",
    "semantic_title": "cue interaction in the perception of prosodic prominence: the role of voice quality",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rodriguez21_interspeech.html": {
    "title": "Glottal Sounds in Korebaju",
    "volume": "main",
    "abstract": "Korebaju (ISO639-3: coe) [́kòrèβàhí̵] is a tonal language spoken in the foothills of the Colombian Amazon. Three field surveys carried out between 2017 and 2019 with six native speakers (3 females and 3 males) from the same village provide a set of glottal productions at both phonetic and phonological levels. This study focuses on the four types of glottal units we have found in this language: A set of vowels /a /, /e /, /o /, [i ] and [ɨ ] including 3 phonemes; the glottal stop [ʔ] and the consonant [*] transcribed and described as a by [1]. Both consonants occurred in intervocalic contexts and can be analyzed as a suprasegmental feature [constricted glottis] which marks the syllable onset. Finally, we have also found a clear and systematic burst which accompanies the release of the nasal consonants [m , n , ɲ ]. No change was found in the EGG signal for these consonants suggesting an abrupt release of the aeroacoustic pressure",
    "keywords": [],
    "checked": true,
    "id": "02a7fa355efd1db9970dba5803d0f04fe0af3a69",
    "semantic_title": "glottal sounds in korebaju",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chanclu21_interspeech.html": {
    "title": "Automatic Classification of Phonation Types in Spontaneous Speech: Towards a New Workflow for the Characterization of Speakers' Voice Quality",
    "volume": "main",
    "abstract": "Voice quality is known to be an important factor for the characterization of a speaker's voice, both in terms of physiological features (mainly laryngeal and supralaryngeal) and of the speaker's habits (sociolinguistic factors). This paper is devoted to one of the main components of voice quality: phonation type. It proposes neural representations of speech followed by a cascade of two binary neural network-based classifiers, one dedicated to the detection of modal and nonmodal vowels, and one for the classification of nonmodal vowels into creaky and breathy types. This approach is evaluated on the spontaneous part of the PTSVOX database, following an expert manual labelling of the data by phonation type. The results of the proposed classifiers reaches on average 85%accuracy at the frame-level and up to 95% accuracy at the segment-level. Further research is planned to generalize the classifiers on more contexts and speakers, and thus pave the way for a new workflow aimed at characterizing phonation types",
    "keywords": [],
    "checked": true,
    "id": "12b0265127906da817e4d762f6628727d5a920b6",
    "semantic_title": "automatic classification of phonation types in spontaneous speech: towards a new workflow for the characterization of speakers' voice quality",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/son21_interspeech.html": {
    "title": "Measuring Voice Quality Parameters After Speaker Pseudonymization",
    "volume": "main",
    "abstract": "Collecting and sharing speech resources is important for progress in speech science and technology. Often, speech resources cannot be shared because of concerns over the privacy of the speakers, e.g., minors or people with medical conditions. Current technologies for pseudonymizing speech have only been tested on \"standard\" speech for which pseudonymization methods are evaluated on speaker identification risk, intelligibility, and naturalness. For many applications, the important characteristics are para-linguistic aspects of the speech, e.g., voice quality, emotion, or disease progression. Little information is available about the extent to which speaker pseudonymization methods preserve such paralinguistic information. The current study investigates how well voice quality parameters are preserved by an example speech pseudonymization application. Correlations prove to be high between original and pseudonymized recordings for seven acoustic parameters and a composite measure of dysphonia, the Root mean square errors for these parameters were reasonably small. A linear mixed effect model shows a link between the difference between source and target speaker and the size of the absolute difference in the It is argued that new measures of quality are needed for pseudonymized non-standard speech before wide-spread application of pseudonymized speech can be considered in research and clinical practise",
    "keywords": [],
    "checked": true,
    "id": "8e98709e1cd7fca3a15e2775f9acbb78b379db79",
    "semantic_title": "measuring voice quality parameters after speaker pseudonymization",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/steinert21_interspeech.html": {
    "title": "Audio-Visual Recognition of Emotional Engagement of People with Dementia",
    "volume": "main",
    "abstract": "Dementia places an immeasurable burden on affected individuals and caregivers. In addition to general cognitive decline, dementia has a negative impact on communication. Technical activation systems are thus in high demand, as cognitive activation may help to moderate the decline. However, effective activation requires sustained engagement — which, in turn, first needs to be reliably recognized. In this study, we examine emotional engagement recognition for People with Dementia (PwD) using non-intrusive biosignals resulting from speech communication and facial expressions. PwD suffering from mild to severe dementia used a tablet-based activation system over multiple sessions. We demonstrate that they retained their ability to verbally express emotional engagement even at severe stages of the disease. For recognition of emotional engagement, we propose an architecture of Bidirectional Long-Short-Term-Memory Networks that combines video information with up to three speech-based feature sets (eGeMAPS, ComParE'13, DeepSpectrum). Using data of 24 PwD, we show that adding speech improves recognition performance significantly compared to a video-only model. Interestingly, disease-progression did not appear to have a substantial impact on recognition performance in this sample. We further discuss the opportunities and challenges of detecting emotional engagement from speech in PwD",
    "keywords": [],
    "checked": true,
    "id": "2cb5bd1f22bcd83e878171d635c207139e3e6711",
    "semantic_title": "audio-visual recognition of emotional engagement of people with dementia",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hecker21_interspeech.html": {
    "title": "Speaking Corona? Human and Machine Recognition of COVID-19 from Voice",
    "volume": "main",
    "abstract": "With the COVID-19 pandemic, several research teams have reported successful advances in automated recognition of COVID-19 by voice. Resulting voice-based screening tools for COVID-19 could support large-scale testing efforts. While capabilities of machines on this task are progressing, we approach the so far unexplored aspect whether human raters can distinguish COVID-19 positive and negative tested speakers from voice samples, and compare their performance to a machine learning baseline. To account for the challenging symptom similarity between COVID-19 and other respiratory diseases, we use a carefully balanced dataset of voice samples, in which COVID-19 positive and negative tested speakers are matched by their symptoms alongside COVID-19 negative speakers without symptoms. Both human raters and the machine struggle to reliably identify COVID-19 positive speakers in our dataset. These results indicate that particular attention should be paid to the distribution of symptoms across all speakers of a dataset when assessing the capabilities of existing systems. The identification of acoustic aspects of COVID-19-related symptom manifestations might be the key for a reliable voice-based COVID-19 detection in the future by both trained human raters and machine learning models",
    "keywords": [],
    "checked": true,
    "id": "983c6877511b509ed42322604c15e3492114a56c",
    "semantic_title": "speaking corona? human and machine recognition of covid-19 from voice",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21b_interspeech.html": {
    "title": "Acoustic-Prosodic, Lexical and Demographic Cues to Persuasiveness in Competitive Debate Speeches",
    "volume": "main",
    "abstract": "We analyze the acoustic-prosodic and lexical correlates of persuasiveness, taking into account speaker, judge and debate characteristics in a novel data set of 674 audio profiles, transcripts, evaluation scores and demographic data from professional debate tournament speeches. By conducting 10-fold cross validation experiments with linear, LASSO and random forest regression, we predict how different feature combinations contribute toward speech scores (i.e. persuasiveness) between men and women. Overall, lexical features, i.e. word complexity, nouns, fillers and hedges, are the most predictive features of speech evaluation scores; in addition to the gender composition of judge panels and opponents. In a combined lexical and demographic feature model, we achieve an R of 0.40. Different lexical features predict speech evaluation scores for male vs. female speakers, and further investigation is necessary to understand whether differential evaluation standards applied across genders. This work contributes a larger-scale debate data set in a democratically relevant, competitive format with high external relevance to persuasive speech education in other competitive settings",
    "keywords": [],
    "checked": true,
    "id": "da08ac507456cd1e794ac63e36ed737c75f13cbe",
    "semantic_title": "acoustic-prosodic, lexical and demographic cues to persuasiveness in competitive debate speeches",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/borgstrom21_interspeech.html": {
    "title": "Unsupervised Bayesian Adaptation of PLDA for Speaker Verification",
    "volume": "main",
    "abstract": "This paper presents a Bayesian framework for unsupervised domain adaptation of Probabilistic Linear Discriminant Analysis (PLDA). By interpreting class labels as latent random variables, Variational Bayes (VB) is used to derive a maximum (MAP) solution of the adapted PLDA model when labels are missing, referred to as VB-MAP. The VB solution iteratively infers class labels and updates PLDA hyperparameters, offering a systematic framework for dealing with unlabeled data. While presented as a general solution, this paper includes experimental results for domain adaptation in speaker verification. VB-MAP estimation is applied to the 2016 and 2018 NIST Speaker Recognition Evaluations (SREs), both of which included small and unlabeled in-domain data sets, and is shown to provide performance improvements over a variety of state-of-the-art domain adaptation methods. Additionally, VB-MAP estimation is used to train a fully unsupervised PLDA model, suffering only minor performance degradation relative to conventional supervised training, offering promise for training PLDA models when no relevant labeled data exists",
    "keywords": [],
    "checked": true,
    "id": "e70e19c203f302014d166ea0696d8e80fc96d71f",
    "semantic_title": "unsupervised bayesian adaptation of plda for speaker verification",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21i_interspeech.html": {
    "title": "The DKU-Duke-Lenovo System Description for the Fearless Steps Challenge Phase III",
    "volume": "main",
    "abstract": "This paper describes the systems developed by the DKU-Duke-Lenovo team for the Fearless Steps Challenge Phase III. For the speech activity detection (SAD) task, we employ the U-Net-based model which has not been used for SAD before, observing a DCF of 1.915% on the eval set. For the speaker identification (SID) task, we adopt the ResNet-SE and ECAPA-TDNN model, and we obtain a Top-5 accuracy of 86.21%. For the speaker diarization (SD) task, we employ several different clustering methods. Besides, domain adaptation, system fusion, and Target-Speaker Voice Activity Detection (TS-VAD) significantly improve the SD performance. We obtain a DER of 12.32% on track 2, and the major contribution is from our ResNet-based TS-VAD model. We finally achieve a first-place ranking for SD and SID and a second-place for SAD in the challenge",
    "keywords": [],
    "checked": true,
    "id": "243bc87ec4be8f1b2f8032b5d7899ae1d89d010c",
    "semantic_title": "the dku-duke-lenovo system description for the fearless steps challenge phase iii",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21f_interspeech.html": {
    "title": "Improved Meta-Learning Training for Speaker Verification",
    "volume": "main",
    "abstract": "Meta-learning (ML) has recently become a research hotspot in speaker verification (SV). We introduce two methods to improve the meta-learning training for SV in this paper. For the first method, a backbone embedding network is first jointly trained with the conventional cross entropy loss and prototypical networks (PN) loss. Then, inspired by speaker adaptive training in speech recognition, additional transformation coefficients are trained with only the PN loss. The transformation coefficients are used to modify the original backbone embedding network in the x-vector extraction process. Furthermore, the random erasing (RE) data augmentation technique is applied to all support samples in each episode to construct positive pairs, and a contrastive loss between the augmented and the original support samples is added to the objective in model training. Experiments are carried out on the Speaker in the Wild (SITW) and VOiCES databases. Both of the methods can obtain consistent improvements over existing meta-learning training frameworks. By combining these two methods, we can observe further improvements on these two databases",
    "keywords": [],
    "checked": true,
    "id": "a2c4227eb8073e4238237efc667a10576b65ce89",
    "semantic_title": "improved meta-learning training for speaker verification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21j_interspeech.html": {
    "title": "Variational Information Bottleneck Based Regularization for Speaker Recognition",
    "volume": "main",
    "abstract": "Speaker recognition (SR) is inevitably affected by noise in real-life scenarios, resulting in decreased recognition accuracy. In this paper, we introduce a novel regularization method, variable information bottleneck (VIB), in speaker recognition to extract robust speaker embeddings. VIB prompts the neural network to ignore as much speaker-identity irrelevant information as possible. We also propose a more effective network, VovNet with an ultra-lightweight subspace attention module (ULSAM), as a feature extractor. ULSAM infers different attention maps for each feature map subspace, enabling efficient learning of cross-channel information along with multi-scale and multi-frequency feature representation. The experimental results demonstrate that our proposed framework outperforms the ResNet-based baseline by 11.4% in terms of equal error rate (EER). The VIB regularization method gives a further performance boost with an 18.9% EER decrease",
    "keywords": [],
    "checked": true,
    "id": "43ea4f86b12fc01b8ae29756806131f9ccd995bd",
    "semantic_title": "variational information bottleneck based regularization for speaker recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/brummer21_interspeech.html": {
    "title": "Out of a Hundred Trials, How Many Errors Does Your Speaker Verifier Make?",
    "volume": "main",
    "abstract": "Out of a hundred trials, how many errors does your speaker verifier make? For the user this is an important, practical question, but researchers and vendors typically sidestep it and supply instead the conditional error-rates that are given by the ROC/DET curve. We posit that the user's question is answered by the Bayes error-rate. We present a tutorial to show how to compute the error-rate that results when making Bayes decisions with calibrated likelihood ratios, supplied by the verifier, and an hypothesis prior, supplied by the user. For perfect calibration, the Bayes error-rate is upper bounded by min(EER,P,1-P), where EER is the equal-error-rate and P, 1-P are the prior probabilities of the competing hypotheses. The EER represents the accuracy of the verifier, while min(P,1-P) represents the hardness of the classification problem. We further show how the Bayes error-rate can be computed also for non-perfect calibration and how to generalize from error-rate to expected cost. We offer some criticism of decisions made by direct score thresholding. Finally, we demonstrate by analyzing error-rates of the recently published DCA-PLDA speaker verifier",
    "keywords": [],
    "checked": true,
    "id": "6e0fc506853f73cf2b9e867938bd7750da9493c5",
    "semantic_title": "out of a hundred trials, how many errors does your speaker verifier make?",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chojnacka21_interspeech.html": {
    "title": "SpeakerStew: Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System",
    "volume": "main",
    "abstract": "In this paper, we describe — a hybrid system to perform speaker verification on 46 languages. Two core ideas were explored in this system: (1) Pooling training data of different languages together for multilingual generalization and reducing development cycles; (2) A novel triage mechanism between text-dependent and text-independent models to reduce runtime cost and expected latency. To the best of our knowledge, this is the first study of speaker verification systems at the scale of 46 languages. The problem is framed from the perspective of using a smart speaker device with interactions consisting of a wake-up keyword (text-dependent) followed by a speech query (text-independent). Experimental evidence suggests that training on multiple languages can generalize to unseen varieties while maintaining performance on seen varieties. We also found that it can reduce computational requirements for training models by an order of magnitude. Furthermore, during model inference on English data, we observe that leveraging a triage framework can reduce the number of calls to the more computationally expensive text-independent system by 73% (and reduce latency by 59%) while maintaining an EER no worse than the text-independent setup",
    "keywords": [],
    "checked": true,
    "id": "638dd69688b38b808b90ab7a3992b90e543c9142",
    "semantic_title": "speakerstew: scaling to many languages with a triaged multilingual text-dependent and text-independent speaker verification system",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21k_interspeech.html": {
    "title": "AntVoice Neural Speaker Embedding System for FFSVC 2020",
    "volume": "main",
    "abstract": "This paper presents a comprehensive description of the AntVoice system for the first two tracks of far-field speaker verification from single microphone array in FFSVC 2020 [1]. The system is based on neural speaker embeddings from deep neural network-based encoder networks. These encoder networks for acoustic modeling include 2D convolutional residual-like networks that are shown to be effective on the tasks. Specifically, we apply the Squeeze-and-Excitation residual network (SE-ResNet) [2] to model cross-channel inter-dependency information. On short utterances, we observe that SE-ResNet outperforms alternative methods in the text-dependent verification task. The system adopts a joint loss function that combines the additive cosine margin softmax loss [3] with the equidistant triplet-based loss[4]. This loss function results in performance gains with more discriminative speaker embeddings from enhanced intra-class similarity and increased inter-class variances. We also apply speech enhancement and data augmentation to improve data quality and diversity. Even without using model ensembles, the proposed system significantly outperforms the baselines [1] in both tracks of the speaker verification challenge. With fusion of several encoder neural networks, this system is able to achieve further performance improvements consistently. In the end, the AntVoice system achieves the third place in the text-dependent verification task",
    "keywords": [],
    "checked": true,
    "id": "6a12130f7269bd348eec6b7ee8f0ff58cd90f0ac",
    "semantic_title": "antvoice neural speaker embedding system for ffsvc 2020",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21b_interspeech.html": {
    "title": "Gradient Regularization for Noise-Robust Speaker Verification",
    "volume": "main",
    "abstract": "Noise robustness is a challenge for speaker recognition systems. To solve this problem, one of the most common approaches is to joint-train a model by using both clean and noisy utterances. However, the gradients calculated on noisy utterances generally contain speaker-irrelevant noisy components, resulting in overfitting for the seen noisy data and poor generalization for the unseen noisy environments. To alleviate this problem, we propose the gradient regularization method to reduce the speaker-irrelevant noisy components by aligning the gradients among the noisy utterances and their clean counterparts. Specifically, the gradients on noisy utterances are forced to follow the directions of the gradients calculated on their clean counterparts, and the gradients across different types of noisy utterances are also aligned to point in similar directions. Since the noise-related components of the gradients can be reduced by the above alignment, the speaker model can be prevented from encoding irrelevant noisy information. To achieve the gradient regularization goals, a novel sequential inner training strategy is also proposed. Experiments on the VoxCeleb1 dataset indicate that our method achieves the best performance in seen and unseen noisy environments",
    "keywords": [],
    "checked": true,
    "id": "37f9f829bd3383524d336125ede257bbbab4fcd8",
    "semantic_title": "gradient regularization for noise-robust speaker verification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kataria21_interspeech.html": {
    "title": "Deep Feature CycleGANs: Speaker Identity Preserving Non-Parallel Microphone-Telephone Domain Adaptation for Speaker Verification",
    "volume": "main",
    "abstract": "With the increase in the availability of speech from varied domains, it is imperative to use such out-of-domain data to improve existing speech systems. Domain adaptation is a prominent pre-processing approach for this. We investigate it to adapt microphone speech to the telephone domain. Specifically, we explore CycleGAN-based unpaired translation of microphone data to improve the x-vector/speaker embedding network for Telephony Speaker Verification. We first demonstrate the efficacy of this on real challenging data and then, to improve further, we modify the CycleGAN formulation to make the adaptation We modify CycleGAN's identity loss, cycle-consistency loss, and adversarial loss to operate in the space of a signal are extracted from an auxiliary (speaker embedding) network and, hence, preserves speaker identity. Our 3D convolution-based Deep Feature Discriminators (DFD) show relative improvements of 5–10% in terms of equal error rate. To dive deeper, we study a challenging scenario of pooling (adapted) microphone and telephone data with data augmentations and telephone codecs. Finally, we highlight the sensitivity of CycleGAN hyper-parameters and introduce a parameter called",
    "keywords": [],
    "checked": true,
    "id": "c3bb7ff3eba44535c9b704ee52041f91bde7bcd0",
    "semantic_title": "deep feature cyclegans: speaker identity preserving non-parallel microphone-telephone domain adaptation for speaker verification",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pu21_interspeech.html": {
    "title": "Scaling Effect of Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "The success of modern deep learning systems is built on two cornerstones, massive amount of annotated training data and advanced computational infrastructure to support large-scale computation. In recent years, the model size of state-of-the-art deep learning systems has rapidly increased and sometimes reached to billions of parameters. Herein we take a close look into this phenomenon and present an empirical study on the scaling effect of model size for self-supervised speech models. In particular, we investigate the quantitative relationship between the model size and the loss/accuracy performance on speech tasks. First, the power-law scaling property between the number of parameters and the L self-supervised loss is verified for speech models. Then the advantage of large speech models in learning effective speech representations is demonstrated in two downstream tasks: i) speaker recognition and ii) phoneme classification. Moreover, it has been shown that the model size of self-supervised speech networks is able to compensate the lack of annotation when there is insufficient training data",
    "keywords": [],
    "checked": true,
    "id": "752985595ce02327b76bb11e0afafb9d31522215",
    "semantic_title": "scaling effect of self-supervised speech models",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21c_interspeech.html": {
    "title": "Joint Feature Enhancement and Speaker Recognition with Multi-Objective Task-Oriented Network",
    "volume": "main",
    "abstract": "Recently, increasing attention has been paid to the joint training of upstream and downstream tasks, and to address the challenge of how to synchronize various loss functions in a multi-objective scenario. In this paper, to address the competing gradient directions between the speaker classification loss and the feature enhancement loss, we propose an asynchronous subregion optimization approach for the joint training of feature enhancement and speaker embedding neural networks. For the asynchronous subregion optimization, the squeeze and excitation (SE) method is introduced in the enhancement network to adaptively select important channels for speaker embedding. Furthermore, channel-wise feature concatenation is applied between the input feature and the enhanced feature to address the distortion of speaker information that is caused by enhancement loss. By using the proposed joint training network with asynchronous subregion optimization and channel-wise feature concatenation, we obtained relative gains of 11.95% and 6.43% in equal error rate on a noisy version of Voxceleb1 and VOiCES corpus, respectively",
    "keywords": [],
    "checked": true,
    "id": "701d9058db1539b3f0c84c04d8e4a17c801eab05",
    "semantic_title": "joint feature enhancement and speaker recognition with multi-objective task-oriented network",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21g_interspeech.html": {
    "title": "Multi-Level Transfer Learning from Near-Field to Far-Field Speaker Verification",
    "volume": "main",
    "abstract": "In far-field speaker verification, the performance of speaker embeddings is susceptible to degradation when there is a mismatch between the conditions of enrollment and test speech. To solve this problem, we propose the feature-level and instance-level transfer learning in the teacher-student framework to learn a domain-invariant embedding space. For the feature-level knowledge transfer, we develop the contrastive loss to transfer knowledge from teacher model to student model, which not only decrease the intra-class distance, but also enlarge the inter-class distance. Moreover, we propose the instance-level pairwise distance transfer method to force the student model to preserve pairwise instances distance from the well optimized embedding space of the teacher model. On FFSVC 2020 evaluation set, our EER on Full-eval trials is relatively reduced by 13.9% compared with the fusion system result on Partial-eval trials of Task2. On Task1, compared with the winner's DenseNet result on Partial-eval trials, our minDCF on Full-eval trials is relatively reduced by 6.3%. On Task3, the EER and minDCF of our proposed method on Full-eval trials are very close to the result of the fusion system on Partial-eval trials. Our results also outperform other competitive domain adaptation methods",
    "keywords": [],
    "checked": true,
    "id": "00b6cac57da8fa92c4a145b4556220c8b185de67",
    "semantic_title": "multi-level transfer learning from near-field to far-field speaker verification",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/patino21_interspeech.html": {
    "title": "Speaker Anonymisation Using the McAdams Coefficient",
    "volume": "main",
    "abstract": "Anonymisation has the goal of manipulating speech signals in order to degrade the reliability of automatic approaches to speaker recognition, while preserving other aspects of speech, such as those relating to intelligibility and naturalness. This paper reports an approach to anonymisation that, unlike other current approaches, requires no training data, is based upon well-known signal processing techniques and is both efficient and effective. The proposed solution uses the McAdams coefficient to transform the spectral envelope of speech signals. Results derived using common VoicePrivacy 2020 databases and protocols show that random, optimised transformations can outperform competing solutions in terms of anonymisation while causing only modest, additional degradations to intelligibility, even in the case of a semi-informed privacy adversary",
    "keywords": [],
    "checked": true,
    "id": "857ef52c30dddce03ecd2ffba2697b9dd2f9cb21",
    "semantic_title": "speaker anonymisation using the mcadams coefficient",
    "citation_count": 55,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21_interspeech.html": {
    "title": "Multi-Stream Gated and Pyramidal Temporal Convolutional Neural Networks for Audio-Visual Speech Separation in Multi-Talker Environments",
    "volume": "main",
    "abstract": "Speech separation is the task of extracting target speech from noisy mixture. In applications like video telephones or video conferencing, lip movements of the target speaker are accessible, which can be leveraged for speech separation. This paper proposes a time-domain audio-visual speech separation model under multi-talker environments. The model receives audio-visual inputs including noisy mixture and speaker lip embedding, and reconstructs clean speech waveform for the target speaker. Once trained, the model can be flexibly applied to unknown number of total speakers. This paper introduces and investigates the multi-stream gating mechanism and pyramidal convolution in temporal convolutional neural networks for audio-visual speech separation task. Speaker- and noise-independent multi-talker separation experiments are conducted on GRID benchmark dataset. The experimental results demonstrate the proposed method achieves 3.9 dB and 1.0 dB SI-SNRi improvement when compared with audio-only and audio-visual baselines respectively, showing effectiveness of the proposed method",
    "keywords": [],
    "checked": true,
    "id": "d11d97bc016932b35b0e20dfde21b5cb81c29e42",
    "semantic_title": "multi-stream gated and pyramidal temporal convolutional neural networks for audio-visual speech separation in multi-talker environments",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21l_interspeech.html": {
    "title": "TeCANet: Temporal-Contextual Attention Network for Environment-Aware Speech Dereverberation",
    "volume": "main",
    "abstract": "In this paper, we exploit the effective way to leverage contextual information to improve the speech dereverberation performance in real-world reverberant environments. We propose a temporal-contextual attention approach on the deep neural network (DNN) for environment-aware speech dereverberation, which can adaptively attend to the contextual information. More specifically, a FullBand based Temporal Attention approach (FTA) is proposed, which models the correlations between the fullband information of the context frames. In addition, considering the difference between the attenuation of high frequency bands and low frequency bands (high frequency bands attenuate faster than low frequency bands) in the room impulse response (RIR), we also propose a SubBand based Temporal Attention approach (STA). In order to guide the network to be more aware of the reverberant environments, we jointly optimize the dereverberation network and the reverberation time (RT60) estimator in a multi-task manner. Our experimental results indicate that the proposed method outperforms our previously proposed reverberation-time-aware DNN and the learned attention weights are fully physical consistent. We also report a preliminary yet promising dereverberation and recognition experiment on real test data",
    "keywords": [],
    "checked": true,
    "id": "c891e58c4705ff73dedc3bca703580d7b2c33adb",
    "semantic_title": "tecanet: temporal-contextual attention network for environment-aware speech dereverberation",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gu21_interspeech.html": {
    "title": "Residual Echo and Noise Cancellation with Feature Attention Module and Multi-Domain Loss Function",
    "volume": "main",
    "abstract": "For real-time acoustic echo cancellation in noisy environments, the classical linear adaptive filters (LAFs) can only remove the linear components of acoustic echo. To further attenuate the non-linear echo components and background noise, this paper proposes a deep learning-based residual echo and noise cancellation (RENC) model, where multiple inputs are utilized and weighted by a feature attention module. More specifically, input features extracted from the far-end reference and the echo estimated by the LAF are scaled with time-frequency attention weights, depending on their correlation with the residual interference in LAF's output. Moreover, a scale-independent mean square error and perceptual loss function are further suggested for training the RENC model. Experimental results validate the efficacy of the proposed feature attention module and multi-domain loss function, which achieve an 8.4%, 14.9% and 29.5% improvement in perceptual evaluation of speech quality (PESQ), scale-invariant signal-to-distortion ratio (SI-SDR) and echo return loss enhancement (ERLE), respectively",
    "keywords": [],
    "checked": true,
    "id": "4364b77c1a756bfe2a6256f417f088edc6353496",
    "semantic_title": "residual echo and noise cancellation with feature attention module and multi-domain loss function",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21c_interspeech.html": {
    "title": "MIMO Self-Attentive RNN Beamformer for Multi-Speaker Speech Separation",
    "volume": "main",
    "abstract": "Recently, our proposed recurrent neural network (RNN) based all deep learning minimum variance distortionless response (ADL-MVDR) beamformer method yielded superior performance over the conventional MVDR by replacing the matrix inversion and eigenvalue decomposition with two RNNs. In this work, we present a self-attentive RNN beamformer to further improve our previous RNN-based beamformer by leveraging on the powerful modeling capability of self-attention. Temporal-spatial self-attention module is proposed to better learn the beamforming weights from the speech and noise spatial covariance matrices. The temporal self-attention module could help RNN to learn global statistics of covariance matrices. The spatial self-attention module is designed to attend on the cross-channel correlation in the covariance matrices. Furthermore, a multi-channel input with multi-speaker directional features and multi-speaker speech separation outputs (MIMO) model is developed to improve the inference efficiency. The evaluations demonstrate that our proposed MIMO self-attentive RNN beamformer improves both the automatic speech recognition (ASR) accuracy and the perceptual estimation of speech quality (PESQ) against prior arts",
    "keywords": [],
    "checked": true,
    "id": "1abd71fb70b8c3639af1d087cd179eaef8c718b0",
    "semantic_title": "mimo self-attentive rnn beamformer for multi-speaker speech separation",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/giri21_interspeech.html": {
    "title": "Personalized PercepNet: Real-Time, Low-Complexity Target Voice Separation and Enhancement",
    "volume": "main",
    "abstract": "The presence of multiple talkers in the surrounding environment poses a difficult challenge for real-time speech communication systems considering the constraints on network size and complexity. In this paper, we present Personalized PercepNet, a real-time speech enhancement model that separates a target speaker from a noisy multi-talker mixture without compromising on complexity of the recently proposed PercepNet. To enable speaker-dependent speech enhancement, we first show how we can train a perceptually motivated speaker embedder network to produce a representative embedding vector for the given speaker. Personalized PercepNet uses the target speaker embedding as additional information to pick out and enhance only the target speaker while suppressing all other competing sounds. Our experiments show that the proposed model significantly outperforms PercepNet and other baselines, both in terms of objective speech enhancement metrics and human opinion scores",
    "keywords": [],
    "checked": true,
    "id": "9d41cfe2c1ef1691d0d5e256ac7d14a43327437e",
    "semantic_title": "personalized percepnet: real-time, low-complexity target voice separation and enhancement",
    "citation_count": 33,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yemini21_interspeech.html": {
    "title": "Scene-Agnostic Multi-Microphone Speech Dereverberation",
    "volume": "main",
    "abstract": "Neural networks (NNs) have been widely applied in speech processing tasks, and, in particular, those employing microphone arrays. Nevertheless, most existing NN architectures can only deal with fixed and position-specific microphone arrays. In this paper, we present an NN architecture that can cope with microphone arrays whose number and positions of the microphones are unknown, and demonstrate its applicability in the speech dereverberation task. To this end, our approach harnesses recent advances in deep learning on set-structured data to design an architecture that enhances the reverberant log-spectrum. We use noisy and noiseless versions of a simulated reverberant dataset to test the proposed architecture. Our experiments on the noisy data show that the proposed scene-agnostic setup outperforms a powerful scene-aware framework, sometimes even with fewer microphones. With the noiseless dataset we show that, in most cases, our method outperforms the position-aware network as well as the state-of-the-art weighted linear prediction error (WPE) algorithm",
    "keywords": [],
    "checked": true,
    "id": "4c49623df1f10a51408ebcc07558b74778f69aca",
    "semantic_title": "scene-agnostic multi-microphone speech dereverberation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html": {
    "title": "Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding Vectors Based on Regular Simplex",
    "volume": "main",
    "abstract": "This paper presents a new deep clustering (DC) method called manifold-aware DC (M-DC) that can enhance hyperspace utilization more effectively than the original DC. The original DC has a limitation in that a pair of two speakers has to be embedded having an orthogonal relationship due to its use of the one-hot vector-based loss function, while our method derives a unique loss function aimed at maximizing the target angle in the hyperspace based on the nature of a regular simplex. Our proposed loss imposes a higher penalty than the original DC when the speaker is assigned incorrectly. The change from DC to M-DC can be easily achieved by rewriting just one term in the loss function of DC, without any other modifications to the network architecture or model parameters. As such, our method has high practicability because it does not affect the original inference part. The experimental results show that the proposed method improves the performances of the original DC and its expansion method",
    "keywords": [],
    "checked": true,
    "id": "4f29e8403f388cfe8571640c95e55f4b7a820fe6",
    "semantic_title": "manifold-aware deep clustering: maximizing angles between embedding vectors based on regular simplex",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21h_interspeech.html": {
    "title": "A Deep Learning Approach to Multi-Channel and Multi-Microphone Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Building on deep learning based acoustic echo cancellation (AEC) in the single-loudspeaker (single-channel) and single-microphone setup, this paper investigates multi-channel (multi-loudspeaker) AEC (MCAEC) and multi-microphone AEC (MMAEC). A convolutional recurrent network (CRN) is trained to predict the near-end speech from microphone signals with far-end signals used as additional information. We find that the deep learning based MCAEC approach avoids the non-uniqueness problem in traditional MCAEC algorithms. For the AEC setup with multiple microphones, rather than employing AEC for each microphone, we propose to train a single network to achieve echo removal for all microphones. Combining deep learning based AEC with supervised beamforming further improves the system performance. Experimental results show the effectiveness of deep learning approach to MCAEC and MMAEC. Furthermore, deep learning based methods are capable of removing echo and noise simultaneously and work well in the presence of nonlinear distortions",
    "keywords": [],
    "checked": true,
    "id": "deb4812c80ac3e0a508e531770fd27cb3dad7ac2",
    "semantic_title": "a deep learning approach to multi-channel and multi-microphone acoustic echo cancellation",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/na21_interspeech.html": {
    "title": "Joint Online Multichannel Acoustic Echo Cancellation, Speech Dereverberation and Source Separation",
    "volume": "main",
    "abstract": "This paper presents a joint source separation algorithm that simultaneously reduces acoustic echo, reverberation and interfering sources. Target speeches are separated from the mixture by maximizing independence with respect to the other sources. It is shown that the separation process can be decomposed into cascading sub-processes that separately relate to acoustic echo cancellation, speech dereverberation and source separation, all of which are solved using the auxiliary function based independent component/vector analysis techniques, and their solving orders are exchangeable. The cascaded solution not only leads to lower computational complexity but also better separation performance than the vanilla joint algorithm",
    "keywords": [],
    "checked": true,
    "id": "0629a5e5ddf82a2bc8f9e272f7ae220e35ce6427",
    "semantic_title": "joint online multichannel acoustic echo cancellation, speech dereverberation and source separation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sato21_interspeech.html": {
    "title": "Should We Always Separate?: Switching Between Enhanced and Observed Signals for Overlapping Speech Recognition",
    "volume": "main",
    "abstract": "Although recent advances in deep learning technology improved automatic speech recognition (ASR), it remains difficult to recognize speech when it overlaps other people's voices. Speech separation or extraction is often used as a front-end to ASR to handle such overlapping speech. However, deep neural network-based speech enhancement can generate ‘processing artifacts' as a side effect of the enhancement, which degrades ASR performance. For example, it is well known that single-channel noise reduction for non-speech noise (non-overlapping speech) often does not improve ASR. Likewise, the processing artifacts may also be detrimental to ASR in some conditions when processing overlapping speech with a separation/extraction method, although it is usually believed that separation/extraction improves ASR. In order to answer the question ‘Do we always have to separate/extract speech from mixtures?', we analyze ASR performance on observed and enhanced speech at various noise and interference conditions, and show that speech enhancement degrades ASR under some conditions even for overlapping speech. Based on these findings, we propose a simple switching algorithm between observed and enhanced speech based on the estimated signal-to-interference ratio and signal-to-noise ratio. We demonstrated experimentally that such a simple switching mechanism can improve recognition performance when processing artifacts are detrimental to ASR",
    "keywords": [],
    "checked": true,
    "id": "f5cfb670174746894bad943f364063c09468ae93",
    "semantic_title": "should we always separate?: switching between enhanced and observed signals for overlapping speech recognition",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/udupa21_interspeech.html": {
    "title": "Estimating Articulatory Movements in Speech Production with Transformer Networks",
    "volume": "main",
    "abstract": "We estimate articulatory movements in speech production from different modalities - acoustics and phonemes. Acoustic-to-articulatory inversion (AAI) is a sequence-to-sequence task. On the other hand, phoneme to articulatory (PTA) motion estimation faces a key challenge in reliably aligning the text and the articulatory movements. To address this challenge, we explore the use of a transformer architecture — FastSpeech, with explicit duration modelling to learn hard alignments between the phonemes and articulatory movements. We also train a transformer model on AAI. We use correlation coefficient (CC) and root mean squared error (rMSE) to assess the estimation performance in comparison to existing methods on both tasks. We observe 154%, 11.8% & 4.8% relative improvement in CC with subject-dependent, pooled and fine-tuning strategies, respectively, for PTA estimation. Additionally, on the AAI task, we obtain 1.5%, 3% and 3.1% relative gain in CC on the same setups compared to the state-of-the-art baseline. We further present the computational benefits of having transformer architecture as representation blocks",
    "keywords": [],
    "checked": true,
    "id": "19be83b770b96905288fa47dd882665a4c64bb45",
    "semantic_title": "estimating articulatory movements in speech production with transformer networks",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21b_interspeech.html": {
    "title": "Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "It is well known that the mismatch between training (source) and test (target) data distribution will significantly decrease the performance of acoustic scene classification (ASC) systems. To address this issue, domain adaptation (DA) is one solution and many unsupervised DA methods have been proposed. These methods focus on a scenario of single source domain to single target domain. However, we will face such problem that test data comes from multiple target domains. This problem can be addressed by producing one model per target domain, but this solution is too costly. In this paper, we propose a novel unsupervised multi-target domain adaption (MTDA) method for ASC, which can adapt to multiple target domains simultaneously and make use of the underlying relation among multiple domains. Specifically, our approach combines traditional adversarial adaptation with two novel discriminator tasks that learns a common subspace shared by all domains. Furthermore, we propose to divide the target domain into the easy-to-adapt and hard-to-adapt domain, which enables the system to pay more attention to hard-to-adapt domain in training. The experimental results on the DCASE 2020 Task 1-A dataset and the DCASE 2019 Task 1-B dataset show that our proposed method significantly outperforms the previous unsupervised DA methods",
    "keywords": [],
    "checked": true,
    "id": "dfcb28fd454ade5de3773a995d15abb5c39083d5",
    "semantic_title": "unsupervised multi-target domain adaptation for acoustic scene classification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jaramillo21_interspeech.html": {
    "title": "Speech Decomposition Based on a Hybrid Speech Model and Optimal Segmentation",
    "volume": "main",
    "abstract": "In a hybrid speech model, both voiced and unvoiced components can coexist in a segment. Often, the voiced speech is regarded as the deterministic component, and the unvoiced speech and additive noise are the stochastic components. Typically, the speech signal is considered stationary within fixed segments of 20–40 ms, but the degree of stationarity varies over time. For decomposing noisy speech into its voiced and unvoiced components, a fixed segmentation may be too crude, and we here propose to adapt the segment length according to the signal local characteristics. The segmentation relies on parameter estimates of a hybrid speech model and the maximum a posteriori (MAP) and log-likelihood criteria as rules for model selection among the possible segment lengths, for voiced and unvoiced speech, respectively. Given the optimal segmentation markers and the estimated statistics, both components are estimated using linear filtering. A codebook-based approach differentiates between unvoiced speech and noise. A better extraction of the components is possible by taking into account the adaptive segmentation, compared to a fixed one. Also, a lower distortion for voiced speech and higher segSNR for both components is possible, as compared to other decomposition methods",
    "keywords": [],
    "checked": true,
    "id": "d93c6ae0fa63530cf1470c366a5a2898c9887645",
    "semantic_title": "speech decomposition based on a hybrid speech model and optimal segmentation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21b_interspeech.html": {
    "title": "Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation",
    "volume": "main",
    "abstract": "Predicting the altered acoustic frames is an effective way of self-supervised learning for speech representation. However, it is challenging to prevent the pretrained model from overfitting. In this paper, we proposed to introduce two dropout regularization methods into the pretraining of transformer encoder: (1) attention dropout, (2) layer dropout. Both of the two dropout methods encourage the model to utilize global speech information, and avoid just copying local spectrum features when reconstructing the masked frames. We evaluated the proposed methods on phoneme classification and speaker recognition tasks. The experiments demonstrate that our dropout approaches achieve competitive results, and improve the performance of classification accuracy on downstream tasks",
    "keywords": [],
    "checked": true,
    "id": "d918c11715bf8e24a81b4988916e8478c970deee",
    "semantic_title": "dropout regularization for self-supervised learning of transformer encoder speech representation",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yarra21_interspeech.html": {
    "title": "Noise Robust Pitch Stylization Using Minimum Mean Absolute Error Criterion",
    "volume": "main",
    "abstract": "We propose a pitch stylization technique in the presence of pitch halving and doubling errors. The technique uses an optimization criterion based on a minimum mean absolute error to make the stylization robust to such pitch estimation errors, particularly under noisy conditions. We obtain segments for the stylization automatically using dynamic programming. Experiments are performed at the frame level and the syllable level. At the frame level, the closeness of stylized pitch is analyzed with the ground truth pitch, which is obtained using a laryngograph signal, considering root mean square error (RMSE) measure. At the syllable level, the effectiveness of perceptual relevant embeddings in the stylized pitch is analyzed by estimating syllabic tones and comparing those with manual tone markings using the Levenshtein distance measure. The proposed approach performs better than a minimum mean squared error criterion based pitch stylization scheme at the frame level and a knowledge-based tone estimation scheme at the syllable level under clean and 20dB, 10dB and 0dB SNR conditions with five noises and four pitch estimation techniques. Among all the combinations of SNR, noise and pitch estimation techniques, the highest absolute RMSE and mean distance improvements are found to be 6.49Hz and 0.23, respectively",
    "keywords": [],
    "checked": true,
    "id": "9b5193f7fb14813d42b4572fa298325143ec4b7c",
    "semantic_title": "noise robust pitch stylization using minimum mean absolute error criterion",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21b_interspeech.html": {
    "title": "An Attribute-Aligned Strategy for Learning Speech Representation",
    "volume": "main",
    "abstract": "Advancement in speech technology has brought convenience to our life. However, the concern is on the rise as speech signal contains multiple personal attributes, which would lead to either sensitive information leakage or bias toward decision. In this work, we propose an attribute-aligned learning strategy to derive speech representation that can flexibly address these issues by attribute-selection mechanism. Specifically, we propose a layered-representation variational autoencoder (LR-VAE), which factorizes speech representation into attribute-sensitive nodes, to derive an identity-free representation for speech emotion recognition (SER), and an emotionless representation for speaker verification (SV). Our proposed method achieves competitive performances on identity-free SER and a better performance on emotionless SV, comparing to the current state-of-the-art method of using adversarial learning applied on a large emotion corpora, the MSP-Podcast. Also, our proposed learning strategy reduces the model and training process needed to achieve multiple privacy-preserving tasks",
    "keywords": [],
    "checked": true,
    "id": "54cd75e17690e051fcac3ee0503ea35be406efa5",
    "semantic_title": "an attribute-aligned strategy for learning speech representation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shahrebabaki21_interspeech.html": {
    "title": "Raw Speech-to-Articulatory Inversion by Temporal Filtering and Decimation",
    "volume": "main",
    "abstract": "We propose a novel sequence-to-sequence acoustic-to-articulatory inversion (AAI) neural architecture in the temporal waveform domain. In contrast to traditional AAI approaches that leverage hand-crafted short-time spectral features obtained from the windowed signal, such as LSFs, or MFCCs, our solution directly process the input speech signal in the time domain, avoiding any intermediate signal transformation, using a cascade of 1D convolutional filters in a deep model. The time-rate synchronization between raw speech signal and the articulatory signal is obtained through a decimation process that acts upon each convolution step. Decimation in time thus avoids degradation phenomena observed in the conventional AAI procedure, caused by the need of framing the speech signal to produce a feature sequence that perfectly matches the articulatory data rate. Experimental evidence on the \"Haskins Production Rate Comparison\" corpus demonstrates the effectiveness of the proposed solution, which outperforms a conventional state-of-the-art AAI system leveraging MFCCs with an 20% relative improvement in terms of Pearson correlation coefficient (PCC) in mismatched speaking rate conditions. Finally, the proposed approach attains the same accuracy as the conventional AAI solution in the typical matched speaking rate condition",
    "keywords": [],
    "checked": true,
    "id": "e43b0f4bce2b66f3b01b3441ff1187400ed58b80",
    "semantic_title": "raw speech-to-articulatory inversion by temporal filtering and decimation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lilley21_interspeech.html": {
    "title": "Unsupervised Training of a DNN-Based Formant Tracker",
    "volume": "main",
    "abstract": "Phonetic analysis often requires reliable estimation of formants, but estimates provided by popular programs can be unreliable. Recently, Dissen et al. [1] described DNN-based formant trackers that produced more accurate frequency estimates than several others, but require manually-corrected formant data for training. Here we describe a novel unsupervised training method for corpus-based DNN formant parameter estimation and tracking with accuracy similar to [1]. Frame-wise spectral envelopes serve as the input. The output is estimates of the frequencies and bandwidths plus amplitude adjustments for a prespecified number of poles and zeros, hereafter referred to as \"formant parameters.\" A custom loss measure based on the difference between the input envelope and one generated from the estimated formant parameters is calculated and back-propagated through the network to establish the gradients with respect to the formant parameters. The approach is similar to that of autoencoders, in that the model is trained to reproduce its input in order to discover latent features, in this case, the formant parameters. Our results demonstrate that a reliable formant tracker can be constructed for a speech corpus without the need for hand-corrected training data",
    "keywords": [],
    "checked": true,
    "id": "cd4b4e29143f293c8855ed8408bb8992811413ac",
    "semantic_title": "unsupervised training of a dnn-based formant tracker",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21c_interspeech.html": {
    "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a on large volumes of unlabeled data and achieves state-of-the-art (SOTA) However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL for its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized prediction heads on top of the model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing",
    "keywords": [],
    "checked": true,
    "id": "7e386158f474a395618c5e065ac55844b507007c",
    "semantic_title": "superb: speech processing universal performance benchmark",
    "citation_count": 539,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21i_interspeech.html": {
    "title": "Synchronising Speech Segments with Musical Beats in Mandarin and English Singing",
    "volume": "main",
    "abstract": "Generating synthesised singing voice with models trained on speech data has many advantages due to the models' flexibility and controllability. However, since the information about the temporal relationship between segments and beats are lacking in speech training data, the synthesised singing may sound off-beat at times. Therefore, the availability of the information on the temporal relationship between speech segments and music beats is crucial. The current study investigated the segment-beat synchronisation in singing data, with hypotheses formed based on the linguistics theories of P-centre and sonority hierarchy. A Mandarin corpus and an English corpus of professional singing data were manually annotated and analysed. The results showed that the presence of musical beats was more dependent on segment duration than sonority. However, the sonority hierarchy and the P-centre theory were highly related to the location of beats. Mandarin and English demonstrated cross-linguistic variations despite exhibiting common patterns",
    "keywords": [],
    "checked": true,
    "id": "ef7fae7c75735cee3cd6db7f7f19d7fb7f962d25",
    "semantic_title": "synchronising speech segments with musical beats in mandarin and english singing",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peplinski21_interspeech.html": {
    "title": "FRILL: A Non-Semantic Speech Embedding for Mobile Devices",
    "volume": "main",
    "abstract": "Learned speech representations can drastically improve performance on tasks with limited labeled data. However, due to their size and complexity, learned representations have limited utility in mobile settings where run-time performance can be a significant bottleneck. In this work, we propose a class of lightweight non-semantic speech embedding models that run efficiently on mobile devices based on the recently proposed TRILL speech embedding. We combine novel architectural modifications with existing speed-up techniques to create embedding models that are fast enough to run in real-time on a mobile device and exhibit minimal performance degradation on a benchmark of non-semantic speech tasks. One such model (FRILL) is 32× faster on a Pixel 1 smartphone and 40% the size of TRILL, with an average decrease in accuracy of only 2%. To our knowledge, FRILL is the highest-quality non-semantic embedding designed for use on mobile devices. Furthermore, we demonstrate that these representations are useful for mobile health tasks such as non-speech human sounds detection and face-masked speech detection. Our models and code are publicly available",
    "keywords": [],
    "checked": true,
    "id": "32b31ee920fb2b3487ba1a019e9035fff3b29c2f",
    "semantic_title": "frill: a non-semantic speech embedding for mobile devices",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mori21_interspeech.html": {
    "title": "Pitch Contour Separation from Overlapping Speech",
    "volume": "main",
    "abstract": "In everyday conversation, speakers' utterances often overlap. For conversation corpora that are recorded in diverse environments, results of pitch extraction in the overlapping parts may be incorrect. The goal of this study is to establish the technique of separating each speaker's pitch contour from an overlapping speech in conversation. The proposed method estimates statistically most plausible f contour from the spectrogram of overlapping speech, along with the information of the speaker to extract. Visual inspection of the separation results showed that the proposed model was able to extract accurate f contours from overlapping speeches of specified speakers. By applying this method, voicing decision errors and gross pitch errors were reduced by 63% compared to simple pitch extraction for overlapping speech",
    "keywords": [],
    "checked": true,
    "id": "354cdc8b1c315871765316d9d27d2811c7ff9324",
    "semantic_title": "pitch contour separation from overlapping speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21_interspeech.html": {
    "title": "Do Sound Event Representations Generalize to Other Audio Tasks? A Case Study in Audio Transfer Learning",
    "volume": "main",
    "abstract": "Transfer learning is critical for efficient information transfer across multiple related learning problems. A simple, yet effective transfer learning approach utilizes deep neural networks trained on a large-scale task for feature extraction. Such representations are then used to learn related downstream tasks. In this paper, we investigate transfer learning capacity of audio representations obtained from neural networks trained on a large-scale sound event detection dataset. We build and evaluate these representations across a wide range of other audio tasks, via a simple linear classifier transfer mechanism. We show that such simple linear transfer is already powerful enough to achieve high performance on the downstream tasks. We also provide insights into the attributes of sound event representations that enable such efficient information transfer",
    "keywords": [],
    "checked": true,
    "id": "0ce8c803608c06e8dc6eb065b7b3b8afa48476b6",
    "semantic_title": "do sound event representations generalize to other audio tasks? a case study in audio transfer learning",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21b_interspeech.html": {
    "title": "Data Augmentation for Spoken Language Understanding via Pretrained Language Models",
    "volume": "main",
    "abstract": "The training of spoken language understanding (SLU) models often faces the problem of data scarcity. In this paper, we put forward a data augmentation method using pretrained language models to boost the variability and accuracy of generated utterances. Furthermore, we investigate and propose solutions to two previously overlooked semi-supervised learning scenarios of data scarcity in SLU: i) : ontology information with numerous valid dialogue acts is given; ii) : a large number of unlabelled utterances are available. Empirical results show that our method can produce synthetic training data that boosts the performance of language understanding models in various scenarios",
    "keywords": [],
    "checked": true,
    "id": "a8ec7942f0a77bfb61a8c534ad951e66e2f94188",
    "semantic_title": "data augmentation for spoken language understanding via pretrained language models",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/radfar21_interspeech.html": {
    "title": "FANS: Fusing ASR and NLU for On-Device SLU",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) systems translate voice input commands to semantics which are encoded as an intent and pairs of slot tags and values. Most current SLU systems deploy a cascade of two neural models where the first one maps the input audio to a transcript (ASR) and the second predicts the intent and slots from the transcript (NLU). In this paper, we introduce FANS, a new end-to-end SLU model that fuses an ASR audio encoder to a multi-task NLU decoder to infer the intent, slot tags, and slot values directly from a given input audio, obviating the need for transcription. FANS consists of a shared audio encoder and three decoders, two of which are seq-to-seq decoders that predict non null slot tags and slot values in parallel and in an auto-regressive manner. FANS neural encoder and decoders architectures are flexible which allows us to leverage different combinations of LSTM, self-attention, and attenders. Our experiments show compared to the state-of-the-art end-to-end SLU models, FANS reduces ICER and IRER errors relatively by 30% and 7%, respectively, when tested on an in-house SLU dataset and by 0.86% and 2% absolute when tested on a public SLU dataset",
    "keywords": [],
    "checked": true,
    "id": "a2e20c8691cfa357fcfd6eb853f3a2df0350ff13",
    "semantic_title": "fans: fusing asr and nlu for on-device slu",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cao21c_interspeech.html": {
    "title": "Sequential End-to-End Intent and Slot Label Classification and Localization",
    "volume": "main",
    "abstract": "Human-computer interaction (HCI) is significantly impacted by delayed responses from a spoken dialogue system. Hence, end-to-end (e2e) spoken language understanding (SLU) solutions have recently been proposed to decrease latency. Such approaches allow for the extraction of semantic information directly from the speech signal, thus bypassing the need for a transcript from an automatic speech recognition (ASR) system. In this paper, we propose a compact e2e SLU architecture for streaming scenarios, where chunks of the speech signal are processed continuously to predict intent and slot values. Our model is based on a 3D convolutional neural network (3D-CNN) and a unidirectional long short-term memory (LSTM). We compare the performance of two alignment-free losses: the connectionist temporal classification (CTC) method and its adapted version, namely connectionist temporal localization (CTL). The latter performs not only the classification but also localization of sequential audio events. The proposed solution is evaluated on the Fluent Speech Command dataset and results show our model ability to process incoming speech signal, reaching accuracy as high as 98.97% for CTC and 98.78% for CTL on single-label classification, and as high as 95.69% for CTC and 95.28% for CTL on two-label prediction",
    "keywords": [],
    "checked": true,
    "id": "8c0f2b475fdf87257cd08e01cbee939cddf9ad46",
    "semantic_title": "sequential end-to-end intent and slot label classification and localization",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muralidharan21_interspeech.html": {
    "title": "DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants",
    "volume": "main",
    "abstract": "Named entity recognition (NER) is usually developed and tested on text from well-written sources. However, in intelligent voice assistants, where NER is an important component, input to NER may be noisy because of user or speech recognition error. In applications, entity labels may change frequently, and non-textual properties like topicality or popularity may be needed to choose among alternatives We describe a NER system intended to address these problems. We test and train this system on a proprietary user-derived dataset. We compare with a baseline text-only NER system; the baseline enhanced with external gazetteers; and the baseline enhanced with the search and indirect labelling techniques we describe below. The final configuration gives around 6% reduction in NER error rate. We also show that this technique improves related tasks, such as semantic parsing, with an improvement of up to 5% in error rate",
    "keywords": [],
    "checked": true,
    "id": "8a85f63b086beb94dc98ac88c43c84795dd98c1e",
    "semantic_title": "dexter: deep encoding of external knowledge for named entity recognition in virtual assistants",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21d_interspeech.html": {
    "title": "A Context-Aware Hierarchical BERT Fusion Network for Multi-Turn Dialog Act Detection",
    "volume": "main",
    "abstract": "The success of interactive dialog systems is usually associated with the quality of the spoken language understanding (SLU) task, which mainly identifies the corresponding dialog acts and slot values in each turn. By treating utterances in isolation, most SLU systems often overlook the semantic context in which a dialog act is expected. The act dependency between turns is nontrivial and yet critical to the identification of the correct semantic representations. Previous works with limited context awareness have exposed the inadequacy of dealing with complexity in multiproned user intents, which are subject to spontaneous change during turn transitions. In this work, we propose to enhance SLU in multi-turn dialogs, employing a context-aware hierarchical BERT fusion Network (CaBERT-SLU) to not only discern context information within a dialog but also jointly identify multiple dialog acts and slots in each utterance. Experimental results show that our approach reaches new state-of-the-art (SOTA) performances in two complicated multi-turn dialogue datasets with considerable improvements compared with previous methods, which only consider single utterances for multiple intents and slot filling",
    "keywords": [],
    "checked": true,
    "id": "077ca9d42ccaa8dd5c888fb7c1c413322cc6cb6f",
    "semantic_title": "a context-aware hierarchical bert fusion network for multi-turn dialog act detection",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21g_interspeech.html": {
    "title": "Pre-Training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning",
    "volume": "main",
    "abstract": "In the traditional cascading architecture for spoken language understanding (SLU), it has been observed that automatic speech recognition errors could be detrimental to the performance of natural language understanding. End-to-end (E2E) SLU models have been proposed to directly map speech input to desired semantic frame with a single model, hence mitigating ASR error propagation. Recently, pre-training technologies have been explored for these E2E models. In this paper, we propose a novel joint textual-phonetic pre-training approach for learning spoken language representations, aiming at exploring the full potentials of phonetic information to improve SLU robustness to ASR errors. We explore phoneme labels as high-level speech features, and design and compare pre-training tasks based on conditional masked language model objectives and inter-sentence relation objectives. We also investigate the efficacy of combining textual and phonetic information during fine-tuning. Experimental results on spoken language understanding benchmarks, Fluent Speech Commands and SNIPS, show that the proposed approach significantly outperforms strong baseline models and improves robustness of spoken language understanding to ASR errors",
    "keywords": [],
    "checked": true,
    "id": "75c4a2682efaf830729885f835f623c257085d21",
    "semantic_title": "pre-training for spoken language understanding with joint textual and phonetic representation learning",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/do21b_interspeech.html": {
    "title": "Predicting Temporal Performance Drop of Deployed Production Spoken Language Understanding Models",
    "volume": "main",
    "abstract": "In deployed real-world spoken language understanding (SLU) applications, data continuously flows into the system. This leads to distributional differences between training and application data that can deteriorate model performance. While regularly retraining the deployed model with new data helps mitigating this problem, it implies significant computational and human costs. In this paper, we develop a method, which can help guiding decisions on whether a model is safe to keep in production without notable performance loss or needs to be retrained. Towards this goal, we build a performance drop regression model for an SLU model that was trained offline to detect a potential model drift in the production phase. We present a wide range of experiments on multiple real-world datasets, indicating that our method is useful for guiding decisions in the SLU model development cycle and to reduce costs for model retraining",
    "keywords": [],
    "checked": true,
    "id": "f997aa03d561583512a02495349174febd218a86",
    "semantic_title": "predicting temporal performance drop of deployed production spoken language understanding models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ganhotra21_interspeech.html": {
    "title": "Integrating Dialog History into End-to-End Spoken Language Understanding Systems",
    "volume": "main",
    "abstract": "End-to-end spoken language understanding (SLU) systems that process human-human or human-computer interactions are often context independent and process each turn of a conversation independently. Spoken conversations on the other hand, are very much context dependent, and dialog history contains useful information that can improve the processing of each conversational turn. In this paper, we investigate the importance of dialog history and how it can be effectively integrated into end-to-end SLU systems. While processing a spoken utterance, our proposed RNN transducer (RNN-T) based SLU model has access to its dialog history in the form of decoded transcripts and SLU labels of previous turns. We encode the dialog history as BERT embeddings, and use them as an additional input to the SLU model along with the speech features for the current utterance. We evaluate our approach on a recently released spoken dialog data set, the HarperValleyBank corpus. We observe significant improvements: 8% for dialog action and 30% for caller intent recognition tasks, in comparison to a competitive context independent end-to-end baseline system",
    "keywords": [],
    "checked": true,
    "id": "c4f5d9b7ff6ac2f5b015363f30b0c8d940a6f852",
    "semantic_title": "integrating dialog history into end-to-end spoken language understanding systems",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21_interspeech.html": {
    "title": "Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State Tracking",
    "volume": "main",
    "abstract": "Dialogue State Tracking (DST), which is the process of inferring user goals by estimating belief states given the dialogue history, plays a critical role in task-oriented dialogue systems. A coreference phenomenon observed in multi-turn conversations is not addressed by existing DST models, leading to suboptimal performances. In this paper, we propose Coreference Dialogue State Tracker (CDST) that explicitly models the coreference feature. In particular, at each turn, the proposed model jointly predicts the coreferred domain-slot pair and extracts the coreference values from the dialogue context. Experimental results on MultiWOZ 2.1 dataset show that the proposed model achieves the state-of-the-art joint goal accuracy of 56.47%",
    "keywords": [],
    "checked": true,
    "id": "467717080eaf85fae09e5c6b937bc9777a4566d2",
    "semantic_title": "coreference augmentation for multi-domain task-oriented dialogue state tracking",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/arora21_interspeech.html": {
    "title": "Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding",
    "volume": "main",
    "abstract": "Decomposable tasks are complex and comprise of a hierarchy of sub-tasks. Spoken intent prediction, for example, combines automatic speech recognition and natural language understanding. Existing benchmarks, however, typically hold out examples for only the surface-level sub-task. As a result, models with similar performance on these benchmarks may have unobserved performance differences on the other sub-tasks. To allow insightful comparisons between competitive end-to-end architectures, we propose a framework to construct robust test sets using coordinate ascent over sub-task specific utility functions. Given a dataset for a decomposable task, our method optimally creates a test set for each sub-task to individually assess sub-components of the end-to-end model. Using spoken language understanding as a case study, we generate new splits for the Fluent Speech Commands and Snips SmartLights datasets. Each split has two test sets: one with held-out utterances assessing natural language understanding abilities, and one with held-out speakers to test speech processing skills. Our splits identify performance gaps up to 10% between end-to-end systems that were within 1% of each other on the original test sets. These performance gaps allow more realistic and actionable comparisons between different architectures, driving future model development. We release our splits and tools for the community",
    "keywords": [],
    "checked": true,
    "id": "4cc07c367e4a1f932e159678ef711e1802edf49f",
    "semantic_title": "rethinking end-to-end evaluation of decomposable tasks: a case study on spoken language understanding",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sun21b_interspeech.html": {
    "title": "Semantic Data Augmentation for End-to-End Mandarin Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end models have gradually become the preferred option for automatic speech recognition (ASR) applications. During the training of end-to-end ASR, data augmentation is a quite effective technique for regularizing the neural networks. This paper proposes a novel data augmentation technique based on semantic transposition of the transcriptions via syntax rules for end-to-end Mandarin ASR. Specifically, we first segment the transcriptions based on part-of-speech tags. Then transposition strategies, such as placing the object in front of the subject or swapping the subject and the object, are applied on the segmented sentences. Finally, the acoustic features corresponding to the transposed transcription are reassembled based on the audio-to-text forced-alignment produced by a pre-trained ASR system. The combination of original data and augmented one is used for training a new ASR system. The experiments are conducted on the Transformer[2] and Conformer[3] based ASR. The results show that the proposed method can give consistent performance gain to the system. Augmentation related issues, such as comparison of different strategies and ratios for data combination are also investigated",
    "keywords": [],
    "checked": true,
    "id": "e5ea412a2dcc7e2c64334f817a56b81eb93b5c74",
    "semantic_title": "semantic data augmentation for end-to-end mandarin speech recognition",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21c_interspeech.html": {
    "title": "Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition",
    "volume": "main",
    "abstract": "Accent variability has posed a huge challenge to automatic speech recognition (ASR) modeling. Although one-hot accent vector based adaptation systems are commonly used, they require prior knowledge about the target accent and cannot handle unseen accents. Furthermore, simply concatenating accent embeddings does not make good use of accent knowledge, which has limited improvements. In this work, we aim to tackle these problems with a novel layer-wise adaptation structure injected into the E2E ASR model encoder. The adapter layer encodes an arbitrary accent in the accent space and assists the ASR model in recognizing accented speech. Given an utterance, the adaptation structure extracts the corresponding accent information and transforms the input acoustic feature into an accent-related feature through the linear combination of all accent bases. We further explore the injection position of the adaptation layer, the number of accent bases, and different types of accent bases to achieve better accent adaptation. Experimental results show that the proposed adaptation structure brings 12% and 10% relative word error rate (WER) reduction on the AESRC2020 accent dataset and the Librispeech dataset, respectively, compared to the baseline",
    "keywords": [],
    "checked": true,
    "id": "5ba5b6f5dcef625e6697021465d29a82a9b7fffb",
    "semantic_title": "layer-wise fast adaptation for end-to-end multi-accent speech recognition",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21m_interspeech.html": {
    "title": "Low Resource German ASR with Untranscribed Data Spoken by Non-Native Children — INTERSPEECH 2021 Shared Task SPAPL System",
    "volume": "main",
    "abstract": "This paper describes the SPAPL system for the INTERSPEECH 2021 Challenge: Shared Task on Automatic Speech Recognition for Non-Native Children's Speech in German. ~5 hours of transcribed data and ~60 hours of untranscribed data are provided to develop a German ASR system for children. For the training of the transcribed data, we propose a non-speech state discriminative loss (NSDL) to mitigate the influence of long-duration non-speech segments within speech utterances. In order to explore the use of the untranscribed data, various approaches are implemented and combined together to incrementally improve the system performance. First, bidirectional autoregressive predictive coding (Bi-APC) is used to learn initial parameters for acoustic modelling using the provided untranscribed data. Second, incremental semi-supervised learning is further used to iteratively generate pseudo-transcribed data. Third, different data augmentation schemes are used at different training stages to increase the variability and size of the training data. Finally, a recurrent neural network language model (RNNLM) is used for rescoring. Our system achieves a word error rate (WER) of 39.68% on the evaluation data, an approximately 12% relative improvement over the official baseline (45.21%)",
    "keywords": [],
    "checked": true,
    "id": "1c01a697307fcec11bfa6e6a6e5f018394385601",
    "semantic_title": "low resource german asr with untranscribed data spoken by non-native children - interspeech 2021 shared task spapl system",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sim21_interspeech.html": {
    "title": "Robust Continuous On-Device Personalization for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "On-device personalization of an all-neural automatic speech recognition (ASR) model can be achieved efficiently by fine-tuning the last few layers of the model. This approach has been shown to be effective for adapting the model to recognize rare named entities using only a small amount of data. To reliably perform continuous on-device learning, it is important for the training process to be completely autonomous without manual intervention. Our simulation studies show that training over many rounds may eventually lead to a significant model drift if the personalized model is indiscriminately accepted at the end of each training round. It is important to have appropriate acceptance criteria in place to guard the model against drifting. Moreover, for storage efficiency, it is desirable to persist the model weights in quantized form. We found that quantizing and dequantizing the model weights in between training rounds can prevent the model from learning effectively. This issue can be circumvented by adding noise to the quantized weights at the start of each training round",
    "keywords": [],
    "checked": true,
    "id": "8b8b7b20108a650df42feddf2be181ab9d69d8b8",
    "semantic_title": "robust continuous on-device personalization for automatic speech recognition",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21b_interspeech.html": {
    "title": "Speaker Normalization Using Joint Variational Autoencoder",
    "volume": "main",
    "abstract": "Speaker adaptation is known to provide significant improvement in speech recognition accuracy. However, in practical scenario, only a few seconds of audio is available due to which it may be infeasible to apply speaker adaptation methods such as i-vector and fMLLR robustly. Also, decoding with fMLLR transformation happens in two-passes which is impractical for real-time applications. In recent past, mapping speech features from speaker independent (SI) space to fMLLR normalized space using denoising autoencoder (DA) has been explored. To the best of our knowledge, such mapping generally does not yield consistent improvement. In this paper, we show that our proposed joint VAE based mapping achieves a large improvements over ASR models trained using filterbank SI features. We also show that joint VAE outperforms DA by a large margin. We observe a relative improvement of 17% in word error rate (WER) compared to ASR model trained using filterbank features with i-vectors and 23% without i-vectors",
    "keywords": [],
    "checked": true,
    "id": "6dfcee22813c749870d20d19fc17296dabc5cd74",
    "semantic_title": "speaker normalization using joint variational autoencoder",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21c_interspeech.html": {
    "title": "The TAL System for the INTERSPEECH2021 Shared Task on Automatic Speech Recognition for Non-Native Childrens Speech",
    "volume": "main",
    "abstract": "This paper describes TAL's system for the INTERSPEECH 2021 shared task on Automatic Speech Recognition (ASR) for non-native children's speech. In this work, we attempt to apply the self-supervised approach to non-native German children's ASR. First, we conduct some baseline experiments to indicate that self-supervised learning can capture more acoustic information on non-native children's speech. Then, we apply the 11-fold data augmentation and combine it with data clean-up to supplement to the limited training data. Moreover, an in-domain semi-supervised VAD model is utilized to segment untranscribed audio. These strategies can significantly improve the system performance. Furthermore, we use two types of language models to further improve performance, i.e., a 4-gram LM with CTC beam-search and a Transformer LM for 2-pass rescoring. Our ASR system reduces the Word Error Rate (WER) by about 48% relatively in comparison with the baseline, achieving 1st in the evaluation period with the WER of 23.5%",
    "keywords": [],
    "checked": true,
    "id": "eab196bf9bdaf7b6042ad2c75cbc9c3563284b9b",
    "semantic_title": "the tal system for the interspeech2021 shared task on automatic speech recognition for non-native childrens speech",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lam21b_interspeech.html": {
    "title": "On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR",
    "volume": "main",
    "abstract": "We propose an on-the-fly data augmentation method for automatic speech recognition (ASR) that uses alignment information to generate effective training samples. Our method, called Aligned Data Augmentation (ADA) for ASR, replaces transcribed tokens and the speech representations in an aligned manner to generate previously unseen training pairs. The speech representations are sampled from an audio dictionary that has been extracted from the training corpus and inject speaker variations into the training examples. The transcribed tokens are either predicted by a language model such that the augmented data pairs are semantically close to the original data, or randomly sampled. Both strategies result in training pairs that improve robustness in ASR training. Our experiments on a Seq-to-Seq architecture show that ADA can be applied on top of SpecAugment, and achieves about 9–23% and 4–15% relative improvements in WER over SpecAugment alone on LibriSpeech 100h and LibriSpeech 960h test datasets, respectively",
    "keywords": [],
    "checked": true,
    "id": "1f814fb7aef27f74bbb164a0dbbc97a6d5837b6a",
    "semantic_title": "on-the-fly aligned data augmentation for sequence-to-sequence asr",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21_interspeech.html": {
    "title": "Zero-Shot Cross-Lingual Phonetic Recognition with External Language Embedding",
    "volume": "main",
    "abstract": "Many existing languages are too sparsely resourced for monolingual deep learning networks to achieve high accuracy. Multilingual phonetic recognition systems mitigate data sparsity issues by training models on data from multiple languages and learning a speech-to-phone or speech-to-text model universal to all languages. However, despite their good performance on the seen training languages, multilingual systems have poor performance on unseen languages. This paper argues that in the real world, even an unseen language has metadata: linguists can tell us the language name, its language family and, usually, its phoneme inventory. Even with no transcribed speech, it is possible to train a language embedding using only data from language typologies (phylogenetic node and phoneme inventory) that reduces ASR error rates. Experiments on a 20-language corpus show that our methods achieve phonetic token error rate (PTER) reduction on all the unseen test languages. An ablation study shows that using the wrong language embedding usually harms PTER if the two languages are from different language families. However, even the wrong language embedding often improves PTER if the language embedding belongs to another member of the same language family",
    "keywords": [],
    "checked": true,
    "id": "7f2f850e4f4cca313d984d8c71557b96c8bd12fc",
    "semantic_title": "zero-shot cross-lingual phonetic recognition with external language embedding",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21c_interspeech.html": {
    "title": "Rapid Speaker Adaptation for Conformer Transducer: Attention and Bias Are All You Need",
    "volume": "main",
    "abstract": "Conformer transducer achieves new state-of-the-art end-to-end (E2E) system performance and has become increasingly appealing for production. In this paper, we study how to effectively perform rapid speaker adaptation in a conformer transducer and how it compares with the RNN transducer. We hierarchically decompose the conformer transducer and compare adapting each component through fine-tuning. Among various interesting observations, there are three distinct findings: First, adapting the self-attention can achieve more than 80% gain of the full network adaptation. When the adaptation data is extremely scarce, attention is all you need to adapt. Second, within the self-attention, adapting the value projection outperforms adapting the key or the query projection. Lastly, bias adaptation, despite of its compact parameter space, is surprisingly effective. We conduct experiments on a state-of-the-art conformer transducer for an email dictation task. With 3 to 5 min source speech and 200 minute personalized TTS speech, the best performing encoder and joint network adaptation yields 38.37% and 19.90% relative word error rate (WER) reduction. Combining the attention and bias adaptation can achieve 90% of the gain with significantly smaller footprint. Further comparison with the RNN-T suggests the new state-of-the-art conformer transducer can benefit as much as if not more from personalization",
    "keywords": [],
    "checked": true,
    "id": "39108c8127b90392a5338d64c8cd5d179a3f1723",
    "semantic_title": "rapid speaker adaptation for conformer transducer: attention and bias are all you need",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/das21b_interspeech.html": {
    "title": "Best of Both Worlds: Robust Accented Speech Recognition with Adversarial Transfer Learning",
    "volume": "main",
    "abstract": "Training deep neural networks for automatic speech recognition (ASR) requires large amounts of transcribed speech. This becomes a bottleneck for training robust models for speech which typically contains high variability in pronunciation and other semantics, since obtaining large amounts of annotated accented data is both tedious and costly. Often, we only have access to large amounts of speech from different accents. In this work, we leverage this unannotated data to provide semantic regularization to an ASR model that has been trained only on one accent, to improve its performance for multiple accents. We propose Accent Pre-Training (Acc-PT), a semi-supervised training strategy that combines transfer learning and adversarial training. Our approach improves the performance of a state-of-the-art ASR model by 33% on average over the baseline across multiple accents, training only on annotated samples from one standard accent, and as little as 105 minutes of speech from a target accent",
    "keywords": [],
    "checked": true,
    "id": "5807b5abac23b658b003583f9b02eec4d4cf5daf",
    "semantic_title": "best of both worlds: robust accented speech recognition with adversarial transfer learning",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chu21_interspeech.html": {
    "title": "Extending Pronunciation Dictionary with Automatically Detected Word Mispronunciations to Improve PAII's System for Interspeech 2021 Non-Native Child English Close Track ASR Challenge",
    "volume": "main",
    "abstract": "This paper proposed to automatically detect mispronounced words over the regions that have low Goodness-of-Pronunciation scores through a constrained phone decoder, then add these word mispronunciations into the orthodox lexicon without colliding with existing pronunciations, finally use the expanded lexicon for decoding non-native speech. The constrained phone decoder is compiled by using a phone-level automatically generated one-edit-distance network to eliminate the need of extended recognition networks designed by phonologists. Results and analysis have shown that the pronunciation dictionary extension is effective in improving WER performance for non-native speech recognition. This paper also described the details of PAII's single-pass fusion-free hybrid system for this Interspeech 2021 non-native children English close track ASR challenge, especially showed the effective use of non-speech segments in the training set as noise sources to perform noise augmentation on the training data, and also conducted a comparison of acoustic models with different neural network architectures with analysis. Final WERs of 12.10%/28.25% are obtained compared to a well-optimized baseline with WERs of 13.37%/33.51% on development/evaluation set, respectively",
    "keywords": [],
    "checked": true,
    "id": "b85f49e35890ad7022524c22d844cb0a8973613c",
    "semantic_title": "extending pronunciation dictionary with automatically detected word mispronunciations to improve paii's system for interspeech 2021 non-native child english close track asr challenge",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21d_interspeech.html": {
    "title": "CVC: Contrastive Learning for Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "Cycle consistent generative adversarial network (CycleGAN) and variational autoencoder (VAE) based models have gained popularity in non-parallel voice conversion recently. However, they often suffer from difficult training process and unsatisfactory results. In this paper, we propose a contrastive learning-based adversarial approach for voice conversion, namely contrastive voice conversion (CVC). Compared to previous CycleGAN-based methods, CVC only requires an efficient one-way GAN training by taking the advantage of contrastive learning. When it comes to non-parallel one-to-one voice conversion, CVC is on par or better than CycleGAN and VAE while effectively reducing training time. CVC further demonstrates superior performance in many-to-one voice conversion, enabling the conversion from unseen speakers",
    "keywords": [],
    "checked": true,
    "id": "bda26246400f9e8a63ead1b8272051391570fc21",
    "semantic_title": "cvc: contrastive learning for non-parallel voice conversion",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21d_interspeech.html": {
    "title": "A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion",
    "volume": "main",
    "abstract": "We propose a new paradigm for maintaining speaker identity in dysarthric voice conversion (DVC). The poor quality of dysarthric speech can be greatly improved by statistical VC, but as the normal speech utterances of a dysarthria patient are nearly impossible to collect, previous work failed to recover the individuality of the patient. In light of this, we suggest a novel, two-stage approach for DVC, which is highly flexible in that no normal speech of the patient is required. First, a powerful parallel sequence-to-sequence model converts the input dysarthric speech into a normal speech of a reference speaker as an intermediate product, and a nonparallel, frame-wise VC model realized with a variational autoencoder then converts the speaker identity of the reference speech back to that of the patient while assumed to be capable of preserving the enhanced quality. We investigate several design options. Experimental evaluation results demonstrate the potential of our approach to improving the quality of the dysarthric speech while maintaining the speaker identity",
    "keywords": [],
    "checked": true,
    "id": "28140c67f5d01a825c709e2c7fe13eccbae24d6c",
    "semantic_title": "a preliminary study of a two-stage paradigm for preserving speaker identity in dysarthric voice conversion",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eskimez21_interspeech.html": {
    "title": "One-Shot Voice Conversion with Speaker-Agnostic StarGAN",
    "volume": "main",
    "abstract": "In this work, we propose a variant of STARGAN for many-to-many voice conversion (VC) conditioned on the d-vectors for short-duration (2–15 seconds) speech. We make several modifications to the STARGAN training and employ new network architectures. We employ a transformer encoder in the discriminator network, and we apply the discriminator loss to the cycle consistency and identity samples in addition to the generated (fake) samples. Instead of classifying the samples as either real or fake, our discriminator tries to predict the categorical speaker class, where a fake class is added for the generated samples. Furthermore, we employ a reverse gradient layer after the generator's encoder and use an auxiliary classifier to remove the speaker's information from the encoded representation. We show that our method yields better results than the baseline method in objective and subjective evaluations in terms of voice conversion quality. Moreover, we provide an ablation study and show each component's influence on speaker similarity",
    "keywords": [],
    "checked": true,
    "id": "6d4dae597c4bc7d546880f81c7c9d672297892b6",
    "semantic_title": "one-shot voice conversion with speaker-agnostic stargan",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/koshizuka21_interspeech.html": {
    "title": "Fine-Tuning Pre-Trained Voice Conversion Model for Adding New Target Speakers with Limited Data",
    "volume": "main",
    "abstract": "Voice conversion (VC) is a technique that converts speaker-dependent non-linguistic information into that of another speaker, while retaining the linguistic information of the input speech. A typical VC system comprises two modules: an encoder module that removes speaker individuality from the input speech and a decoder module that incorporates another speaker's individuality in synthesized speech. This paper proposes a training method for a vocoder-free any-to-many encoder-decoder VC model with limited data. Various pre-training techniques have been proposed to solve problems training to limited training data; some of these techniques employ the text-to-speech (TTS) task for pre-training. We pre-train the decoder module in the voice conversion task for growing our pre-training technique into continuously adding target speakers to the VC system. The experimental results show that good conversion performance can be achieved by conducting VC-based pre-training. We also confirmed that the rehearsal and pseudo-rehearsal methods can effectively fine-tune the model without degrading the conversion performance of the pre-trained target speakers",
    "keywords": [],
    "checked": true,
    "id": "3bb29f61118abbeed3419a01e88d1426d6f599b4",
    "semantic_title": "fine-tuning pre-trained voice conversion model for adding new target speakers with limited data",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21n_interspeech.html": {
    "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion",
    "volume": "main",
    "abstract": "One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement. Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance. To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner. Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics. In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems. Our code, pre-trained models and demo are publicly available",
    "keywords": [],
    "checked": true,
    "id": "2f01cabbee57e1083f3d4499f112bb220dda69a4",
    "semantic_title": "vqmivc: vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion",
    "citation_count": 79,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21e_interspeech.html": {
    "title": "StarGANv2-VC: A Diverse, Unsupervised, Non-Parallel Framework for Natural-Sounding Voice Conversion",
    "volume": "main",
    "abstract": "We present an unsupervised non-parallel many-to-many voice conversion (VC) method using a generative adversarial network (GAN) called StarGAN v2. Using a combination of adversarial source classifier loss and perceptual loss, our model significantly outperforms previous VC models. Although our model is trained only with 20 English speakers, it generalizes to a variety of voice conversion tasks, such as any-to-many, cross-lingual, and singing conversion. Using a style encoder, our framework can also convert plain reading speech into stylistic speech, such as emotional and falsetto speech. Subjective and objective evaluation experiments on a non-parallel many-to-many voice conversion task revealed that our model produces natural sounding voices, close to the sound quality of state-of-the-art text-to-speech (TTS) based voice conversion methods without the need for text labels. Moreover, our model is completely convolutional and with a faster-than-real-time vocoder such as Parallel WaveGAN can perform real-time voice conversion",
    "keywords": [],
    "checked": true,
    "id": "a58b237fe398f81fa3385c53bd341c0f4d05f3fe",
    "semantic_title": "starganv2-vc: a diverse, unsupervised, non-parallel framework for natural-sounding voice conversion",
    "citation_count": 51,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21c_interspeech.html": {
    "title": "Normalization Driven Zero-Shot Multi-Speaker Speech Synthesis",
    "volume": "main",
    "abstract": "In this paper, we present a novel zero-shot multi-speaker speech synthesis approach (ZSM-SS) that leverages the normalization architecture and speaker encoder with non-autoregressive multi-head attention driven encoder-decoder architecture. Given an input text and a reference speech sample of an unseen person, ZSM-SS can generate speech in that person's style in a zero-shot manner. Additionally, we demonstrate how the affine parameters of normalization help in capturing the prosodic features such as energy and fundamental frequency in a disentangled fashion and can be used to generate morphed speech output. We demonstrate the efficacy of our proposed architecture on multi-speaker VCTK[1] and LibriTTS [2] datasets, using multiple quantitative metrics that measure generated speech distortion and MOS, along with speaker embedding analysis of the proposed speaker encoder model",
    "keywords": [],
    "checked": true,
    "id": "87d7dedb6475f879b82d940652eff32eb0844bb7",
    "semantic_title": "normalization driven zero-shot multi-speaker speech synthesis",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sakamoto21_interspeech.html": {
    "title": "StarGAN-VC+ASR: StarGAN-Based Non-Parallel Voice Conversion Regularized by Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Preserving the linguistic content of input speech is essential during voice conversion (VC). The star generative adversarial network-based VC method (StarGAN-VC) is a recently developed method that allows non-parallel many-to-many VC. Although this method is powerful, it can fail to preserve the linguistic content of input speech when the number of available training samples is extremely small. To overcome this problem, we propose the use of automatic speech recognition to assist model training, to improve StarGAN-VC, especially in low-resource scenarios. Experimental results show that using our proposed method, StarGAN-VC can retain more linguistic information than vanilla StarGAN-VC",
    "keywords": [],
    "checked": true,
    "id": "fcac9eb041a58246edd7cdeb4014cd156c47fe1f",
    "semantic_title": "stargan-vc+asr: stargan-based non-parallel voice conversion regularized by automatic speech recognition",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21d_interspeech.html": {
    "title": "Two-Pathway Style Embedding for Arbitrary Voice Conversion",
    "volume": "main",
    "abstract": "Arbitrary voice conversion, also referred to as zero-shot voice conversion, has recently attracted increased attention in the literature. Although disentangling the linguistic and style representations for acoustic features is an effective way to achieve zero-shot voice conversion, the problem of how to convert to a natural speaker style is challenging because of the intrinsic variabilities of speech and the difficulties of completely decoupling them. For this reason, in this paper, we propose a Two-Pathway Style Embedding Voice Conversion framework (TPSE-VC) for realistic and natural speech conversion. The novel feature of this method is to simultaneously embed sentence-level and phoneme-level style information. A novel attention mechanism is proposed to implement the implicit alignment for timbre style and phoneme content, further embedding a phoneme-level style representation. In addition, we consider embedding the complete set of time steps of audio style into a fixed-length vector to obtain the sentence-level style representation. Moreover, TPSEVC does not require any pre-trained models, and is only trained with non-parallel speech data. Experimental results demonstrate that the proposed TPSE-VC outperforms the state-of-the-art results on zero-shot voice conversion",
    "keywords": [],
    "checked": true,
    "id": "d82ee934406903ee206a2c184587cbc124b4e68d",
    "semantic_title": "two-pathway style embedding for arbitrary voice conversion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21c_interspeech.html": {
    "title": "Non-Parallel Any-to-Many Voice Conversion by Replacing Speaker Statistics",
    "volume": "main",
    "abstract": "This paper proposes a non-parallel any-to-many voice conversion (VC) approach with a novel statistics replacement layer. Non-parallel VC is usually achieved by firstly disentangling linguistic and speaker representations, and then concatenating the linguistic content with the learned target speaker's embedding at the conversion stage. While such a concatenation-based approach could introduce speaker-specific characteristics into the network, it is not very effective as it entirely relies on the network to learn to combine the linguistic content and the speaker characteristics. Inspired by X-vectors, where the statistics of hidden representation such as means and standard deviations are used for speaker differentiation, we propose a statistics replacement layer in VC systems to directly modify the hidden states to have the target speaker's statistics. The speaker-specific statistics of hidden states are learned for each target speaker during training and are used as guidance for the statistics replacement layer during inference. Moreover, to better concentrate the speaker information into the statistics of hidden representation, a multitask training with X-vector based speaker classification is also performed. Experimental results with Librispeech and VCTK datasets show that the proposed method can effectively improve the converted speech's naturalness and similarity",
    "keywords": [],
    "checked": true,
    "id": "2cb242c2cfe1e3b18f8f25bc324d3867a0364adc",
    "semantic_title": "non-parallel any-to-many voice conversion by replacing speaker statistics",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21c_interspeech.html": {
    "title": "Cross-Lingual Voice Conversion with a Cycle Consistency Loss on Linguistic Representation",
    "volume": "main",
    "abstract": "Cross-Lingual Voice Conversion (XVC) aims to modify a source speaker identity towards a target while preserving the source linguistic content. This paper introduces a cycle consistency loss on linguistic representation to ensure the speech content unchanged after conversion. The proposed XVC model consists of two loss functions during optimization: a spectral reconstruction loss and a linguistic cycle consistency loss. The cycle consistency loss seeks to maintain the source speech's linguistic content. Specifically, we utilize Phonetic PosteriorGram (PPG) to represent the linguistic content. XVC experiments were conducted between English and Mandarin. Both objective and subjective evaluations demonstrated that with the proposed cycle consistency loss, converted speech is more intelligible",
    "keywords": [],
    "checked": true,
    "id": "e7e5e4f34731286e9bc34ad14bcfb6b82472e81f",
    "semantic_title": "cross-lingual voice conversion with a cycle consistency loss on linguistic representation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/du21_interspeech.html": {
    "title": "Improving Robustness of One-Shot Voice Conversion with Deep Discriminative Speaker Encoder",
    "volume": "main",
    "abstract": "One-shot voice conversion has received significant attention since only one utterance from source speaker and target speaker respectively is required. Moreover, source speaker and target speaker do not need to be seen during training. However, available one-shot voice conversion approaches are not stable for unseen speakers as the speaker embedding extracted from one utterance of an unseen speaker is not reliable. In this paper, we propose a deep discriminative speaker encoder to extract speaker embedding from one utterance more effectively. Specifically, the speaker encoder first integrates residual network and squeeze-and-excitation network to extract discriminative speaker information in frame level by modeling frame-wise and channel-wise interdependence in features. Then attention mechanism is introduced to further emphasize speaker related information via assigning different weights to frame level speaker information. Finally a statistic pooling layer is used to aggregate weighted frame level speaker information to form utterance level speaker embedding. The experimental results demonstrate that our proposed speaker encoder can improve the robustness of one-shot voice conversion for unseen speakers and outperforms baseline systems in terms of speech quality and speaker similarity",
    "keywords": [],
    "checked": true,
    "id": "2bdabff0c7c94d8e17324626391640c66b1c307a",
    "semantic_title": "improving robustness of one-shot voice conversion with deep discriminative speaker encoder",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/white21_interspeech.html": {
    "title": "Optimizing an Automatic Creaky Voice Detection Method for Australian English Speaking Females",
    "volume": "main",
    "abstract": "Creaky voice is a nonmodal phonation type that has various linguistic and sociolinguistic functions. Manually annotating creaky voice for phonetic analysis is time-consuming and labor-intensive. In recent years, automatic tools for detecting creaky voice have been proposed, which present the possibility for easier, faster and more consistent creak identification. One of these proposed tools is a Creak Detector algorithm that uses an automatic neural network taking its input from several acoustic cues to identify creaky voice. Previous work has suggested that the creak probability threshold at which this tool determines an instance to be creaky may vary depending on the speaker population. The present study investigates the optimal creak detection threshold for female Australian English speakers Results show further support for the practice of first finding the optimal threshold when using the Creak Detection algorithm on new data sets. Additionally, results show that accuracy of creaky voice detection using the Creak Detection algorithm can be significantly improved by excluding non-sonorant data",
    "keywords": [],
    "checked": true,
    "id": "ba1dbe846f1cd196f994f383ea4a94ec1a0b5b51",
    "semantic_title": "optimizing an automatic creaky voice detection method for australian english speaking females",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/penney21_interspeech.html": {
    "title": "A Comparison of Acoustic Correlates of Voice Quality Across Different Recording Devices: A Cautionary Tale",
    "volume": "main",
    "abstract": "There has been a recent increase in speech research utilizing data recorded with participants' personal devices, particularly in light of the COVID-19 pandemic and restrictions on face-to-face interactions. This raises important questions about whether these recordings are comparable to those made in traditional lab-based settings. Some previous studies have compared the viability of recordings made with personal devices for the clinical evaluation of voice quality. However, these studies rely on simple statistical analyses and do not examine acoustic correlates of voice quality typically examined in the (socio-) phonetic literature (e.g. H1-H2). In this study, we compare recordings from a set of smartphones/laptops and a solid-state recorder to assess the reliability of a range of acoustic correlates of voice quality. The results show significant differences for many acoustic measures of voice quality across devices. Further exploratory analyses demonstrate that these differences are not simple offsets, but rather that their magnitude depends on the value of the measurement of interest. We therefore urge researchers to exercise caution when examining voice quality based on recordings made with participants' devices, particularly when interested in small effect sizes. We also call on the speech research community to investigate these issues more thoroughly",
    "keywords": [],
    "checked": true,
    "id": "9ab8e9d363f5a978f0f3f62578f43e6196c81fc9",
    "semantic_title": "a comparison of acoustic correlates of voice quality across different recording devices: a cautionary tale",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sfakianaki21_interspeech.html": {
    "title": "Investigating Voice Function Characteristics of Greek Speakers with Hearing Loss Using Automatic Glottal Source Feature Extraction",
    "volume": "main",
    "abstract": "The current study investigates voice quality characteristics of Greek adults with normal hearing and hearing loss, automatically obtained from glottal inverse filtering analysis using the Aalto Aparat toolkit. Aalto Aparat has been employed in glottal flow analysis of disordered speech, but to the best of the authors' knowledge, not as yet in hearing impaired voice analysis and assessment. Five speakers, three women and two men, with normal hearing (NH) and five speakers with prelingual profound hearing impairment (HI), matched for age and sex, produced symmetrical /ˈpVpV/ disyllables, where V=/i, a, u/. A state-of-the-art method named quasi-closed phase analysis (QCP) is offered in Aparat and it is used to estimate the glottal source signal. Glottal source features were obtained using time- and frequency-domain parametrization methods and analysed statistically. The interpretation of the results attempts to shed light on potential differences between HI and NH phonation strategies, while advantages and limitations of inverse filtering methods in HI voice assessment are discussed",
    "keywords": [],
    "checked": true,
    "id": "7bf515d839e495fdc91c04093ec2efe8de84b30f",
    "semantic_title": "investigating voice function characteristics of greek speakers with hearing loss using automatic glottal source feature extraction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huckvale21_interspeech.html": {
    "title": "Automated Detection of Voice Disorder in the Saarbrücken Voice Database: Effects of Pathology Subset and Audio Materials",
    "volume": "main",
    "abstract": "The Saarbrücken Voice Database contains speech and simultaneous electroglottography recordings of 1002 speakers exhibiting a wide range of voice disorders, together with recordings of 851 controls. Previous studies have used this database to build systems for automated detection of voice disorders and for differential diagnosis. These studies have varied considerably in the subset of pathologies tested, the audio materials analyzed, the cross-validation method used and the performance metric reported. This variation has made it hard to determine the most promising approaches to the problem of detecting voice disorders. In this study we re-implement three recently published systems that have been trained to detect pathology using the SVD and compare their performance on the same pathologies with the same audio materials using a common cross-validation protocol and performance metric. We show that under this approach, there is much less difference in performance across systems than in their original publication. We also show that voice disorder detection on the basis of a short phrase gives similar performance to that based on a sequence of vowels of different pitch. Our evaluation protocol may be useful for future studies on voice disorder detection with the SVD",
    "keywords": [],
    "checked": true,
    "id": "b3b5b914d7914a140c60b2850fd47039cba7bfc8",
    "semantic_title": "automated detection of voice disorder in the saarbrücken voice database: effects of pathology subset and audio materials",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lulich21_interspeech.html": {
    "title": "Accelerometer-Based Measurements of Voice Quality in Children During Semi-Occluded Vocal Tract Exercise with a Narrow Straw in Air",
    "volume": "main",
    "abstract": "Non-invasive measures of voice quality, such as H1-H2, rely on oral flow signals, inverse filtered speech signals, or corrections for the effects of formants. Voice quality measures play especially important roles in the assessment of voice disorders and the evaluation of treatment efficacy. One type of treatment that is increasingly common in voice therapy, as well as in voice training for singers and actors, is semi-occluded vocal tract exercises (SOVTEs). The goal of SOVTEs is to change patterns of vocal fold vibration and thereby improve voice quality and vocal efficiency. Accelerometers applied to the skin of the neck have been used to investigate subglottal acoustics, to inverse-filter speech signals, and to obtain voice quality metrics. This paper explores the application of neck-skin accelerometers to measure voice quality without oral flow, inverse filtering, or formant correction. Accelerometer-based measures (uncorrected K1-K2 and corrected K1*-K2*, analogous to microphone-based H1-H2 and H1*-H2*) were obtained from typically developing children with healthy voice, before and during SOVTEs. Traditional microphone-based H1-H2 measures (corrected and uncorrected) were also obtained. Results showed that K1-K2 and K1*-K2* were not substantially affected by vocal tract acoustic changes in formant frequencies",
    "keywords": [],
    "checked": true,
    "id": "93a2ec748931748a5adbc56289d32acf2214a23f",
    "semantic_title": "accelerometer-based measurements of voice quality in children during semi-occluded vocal tract exercise with a narrow straw in air",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/perez21_interspeech.html": {
    "title": "Articulatory Coordination for Speech Motor Tracking in Huntington Disease",
    "volume": "main",
    "abstract": "Huntington Disease (HD) is a progressive disorder which often manifests in motor impairment. Motor severity (captured via motor score) is a key component in assessing overall HD severity. However, motor score evaluation involves in-clinic visits with a trained medical professional, which are expensive and not always accessible. Speech analysis provides an attractive avenue for tracking HD severity because speech is easy to collect remotely and provides insight into motor changes. HD speech is typically characterized as having irregular articulation. With this in mind, acoustic features that can capture vocal tract movement and articulatory coordination are particularly promising for characterizing motor symptom progression in HD. In this paper, we present an experiment that uses Vocal Tract Coordination (VTC) features extracted from read speech to estimate a motor score. When using an elastic-net regression model, we find that VTC features significantly outperform other acoustic features across varied-length audio segments, which highlights the effectiveness of these features for both short- and long-form reading tasks. Lastly, we analyze the F-value scores of VTC features to visualize which channels are most related to motor score. This work enables future research efforts to consider VTC features for acoustic analyses which target HD motor symptomatology tracking",
    "keywords": [],
    "checked": true,
    "id": "8c7156e57b36b09416590a6e1c94d1b46fa7e6e7",
    "semantic_title": "articulatory coordination for speech motor tracking in huntington disease",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ferrer21_interspeech.html": {
    "title": "Modeling Dysphonia Severity as a Function of Roughness and Breathiness Ratings in the GRBAS Scale",
    "volume": "main",
    "abstract": "Dysphonia comprises many perceptually deviating aspects of voice, and its overall severity perception is made by the listener according to methods of aggregating the single dimensions which are personally conceived and not well studied. Roughness and breathiness are constituent dimensions in most devised rating scales in clinical use. In this paper, we evaluate several ways to model the mapping of the overall severity as a function of the particular ratings of roughness and breathiness. The models include the simple linear averaging as well as several non-linear variants suggested elsewhere, and some minor adjustments. The models are evaluated on four datasets from different countries, allowing a more global evaluation of how the mapping is conceived Results show the limitations of the most widely assumed linear approach, while also hinting at a need for a more uniform coverage of the sample space in voice pathology datasets. The models explored in this paper can be expanded to higher-dimensional scales",
    "keywords": [],
    "checked": true,
    "id": "bf399a85062282b9bc9d6eb3a91d31258463fdab",
    "semantic_title": "modeling dysphonia severity as a function of roughness and breathiness ratings in the grbas scale",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karpov21_interspeech.html": {
    "title": "Golos: Russian Dataset for Speech Research",
    "volume": "main",
    "abstract": "This paper introduces a novel Russian speech dataset called Golos, a large corpus suitable for speech research. The dataset mainly consists of recorded audio files manually annotated on the crowd-sourcing platform. The total duration of the audio is about 1240 hours. We have made the corpus freely available to download, along with the acoustic model with CTC loss prepared on this corpus. Additionally, transfer learning was applied to improve the performance of the acoustic model. In order to evaluate the quality of the dataset with the beam-search algorithm, we have built a 3-gram language model on the open Common Crawl dataset. The total word error rate (WER) metrics turned out to be about 3.3% and 11.5%",
    "keywords": [],
    "checked": true,
    "id": "39dd1c778673976fdc7e9b8158068e1ac5b6edcc",
    "semantic_title": "golos: russian dataset for speech research",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sadhu21b_interspeech.html": {
    "title": "Radically Old Way of Computing Spectra: Applications in End-to-End ASR",
    "volume": "main",
    "abstract": "We propose a technique to compute spectrograms using Frequency Domain Linear Prediction (FDLP) that uses all-pole models to fit the squared Hilbert envelope of speech in different frequency sub-bands. The spectrogram of a complete speech utterance is computed by overlap-add of contiguous all-pole model responses. A long context window of 1.5 seconds allows us to capture the low frequency temporal modulations of speech in the spectrogram. For an end-to-end automatic speech recognition task, the FDLP spectrogram performs on par with the standard mel spectrogram features for clean read speech training and test data. For more realistic speech data with train-test domain mismatches or reverberations, FDLP spectrogram shows up to 25% and 22% relative WER improvements over mel spectrogram respectively",
    "keywords": [],
    "checked": true,
    "id": "22895f07cbcbde21d115ebb744edf230ee7a7e18",
    "semantic_title": "radically old way of computing spectra: applications in end-to-end asr",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alghezi21_interspeech.html": {
    "title": "Self-Supervised End-to-End ASR for Low Resource L2 Swedish",
    "volume": "main",
    "abstract": "Unlike traditional (hybrid) Automatic Speech Recognition (ASR), end-to-end ASR systems simplify the training procedure by directly mapping acoustic features to sequences of graphemes or characters, thereby eliminating the need for specialized acoustic, language, or pronunciation models. However, one drawback of end-to-end ASR systems is that they require more training data than conventional ASR systems to achieve similar word error rate (WER). This makes it difficult to develop ASR systems for tasks where transcribed target data is limited such as developing ASR for Second Language (L2) speakers of Swedish. Nonetheless, recent advancements in self-supervised acoustic learning, manifested in wav2vec models [1, 2, 3], leverage the available untranscribed speech data to provide compact acoustic representation that can achieve low WER when incorporated in end-to-end systems. To this end, we experiment with several monolingual and cross-lingual self-supervised acoustic models to develop end-to-end ASR system for L2 Swedish. Even though our test is very small, it indicates that these systems are competitive in performance with traditional ASR pipeline. Our best model seems to reduce the WER by 7% relative to our traditional ASR baseline trained on the same target data",
    "keywords": [],
    "checked": true,
    "id": "ee2bb24439ee90a58b0ff28d95b5568903d7ee96",
    "semantic_title": "self-supervised end-to-end asr for low resource l2 swedish",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oneill21_interspeech.html": {
    "title": "SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "In the English speech-to-text (STT) machine learning task, acoustic models are conventionally trained on uncased Latin characters, and any necessary orthography (such as capitalization, punctuation, and denormalization of non-standard words) is imputed by separate post-processing models. This adds complexity and limits performance, as many formatting tasks benefit from semantic information present in the acoustic signal but absent in transcription. Here we propose a new STT task: end-to-end neural transcription with fully formatted text for target labels. We present baseline Conformer-based models trained on a corpus of 5,000 hours of professionally transcribed earnings calls, achieving a CER of 1.7. As a contribution to the STT research community, we release the corpus free for non-commercial use",
    "keywords": [],
    "checked": true,
    "id": "ca201db9980e49647feedf39eb30b19f074bf68a",
    "semantic_title": "spgispeech: 5, 000 hours of transcribed financial audio for fully formatted end-to-end speech recognition",
    "citation_count": 35,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/evain21_interspeech.html": {
    "title": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) using huge unlabeled data has been successfully explored for image and natural language processing. Recent works also investigated SSL from speech. They were notably successful to improve performance on downstream tasks such as automatic speech recognition (ASR). While these works suggest it is possible to reduce dependence on labeled data for building efficient speech systems, their evaluation was mostly made on ASR and using multiple and heterogeneous experimental settings (most of them for English). This questions the objective comparison of SSL approaches and the evaluation of their impact on building speech systems. In this paper, we propose : a reproducible framework for assessing SSL from speech. It not only includes ASR (high and low resource) tasks but also spoken language understanding, speech translation and emotion recognition. We also focus on speech technologies in a language different than English: French. SSL models of different sizes are trained from carefully sourced and documented datasets. Experiments show that SSL is beneficial for most but not all tasks which confirms the need for exhaustive and reliable benchmarks to evaluate its real impact is shared with the scientific community for reproducible research in SSL from speech",
    "keywords": [],
    "checked": true,
    "id": "b257038ef379092509b1dd1d66a351f47363d6eb",
    "semantic_title": "lebenchmark: a reproducible framework for assessing self-supervised representation learning from speech",
    "citation_count": 56,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sturm21_interspeech.html": {
    "title": "Prosodic Accommodation in Face-to-Face and Telephone Dialogues",
    "volume": "main",
    "abstract": "The study of phonetic accommodation in various communicative situations is still relatively limited. This paper examines accommodation in spontaneous conversations of eight pairs of Czech young male speakers in two communicative conditions: unconstrained face-to-face conversation and goal-oriented interaction via mobile telephone. Articulation rate and measures of f0 level, range and variability were measured in 40 prosodic phrases per speaker in each condition. Analyses of LME models did not reveal a significant global effect of time throughout the interaction on the distance between speakers (convergence) in any of the examined parameters, or that of preceding phrase value on the subsequent turn-initial value (synchrony). However, more consistent patterns were observed when speaker pairs were examined separately, revealing substantial individual variation on the one hand and non-linear effects on the other. This shows that aggregate analyses can be misleading in the study of phonetic accommodation and that speakers dynamically employ different strategies throughout natural conversations",
    "keywords": [],
    "checked": true,
    "id": "e40dcddbfc5d832a6a40b3b4b94e43aeae978542",
    "semantic_title": "prosodic accommodation in face-to-face and telephone dialogues",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/riverincoutlee21_interspeech.html": {
    "title": "Dialect Features in Heterogeneous and Homogeneous Gheg Speaking Communities",
    "volume": "main",
    "abstract": "This apparent and real time study analyses how dialect features in the speech of children and adults are differently affected depending on whether they live in homogeneous or heterogeneous speech communities. The general hypotheses are that speakers in such high contact settings as heterogeneous urban centers are more prone to innovation than speakers in homogeneous tightly-knit communities, and that children accelerate leveling, especially through schooling and socialization. This study is of Gheg Albanian, a dialect spoken in and around the capital Tirana. Two features were investigated: rounding of /a/ and vowel length contrasts. Two groups of adults and children were compared: one from Tirana and one from a nearby village. Additionally, the children were recorded twice over a period of 12 months and were compared longitudinally. The results showed that length contrasts were still present in both communities and age groups. Rounding of /a/ was lost in the city, but undergoing change in the village, with differences measured in apparent time, but also in child speech within the 12-month span. Our study further raises the issue of combining both apparent and real time data within the same design",
    "keywords": [],
    "checked": true,
    "id": "4a3c29dd913bc82d23e1ccfc8ef45cfddf19bb3c",
    "semantic_title": "dialect features in heterogeneous and homogeneous gheg speaking communities",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zellers21_interspeech.html": {
    "title": "An Exploration of the Acoustic Space of Rhotics and Laterals in Ruruuli",
    "volume": "main",
    "abstract": "Liquid consonants — rhotics and laterals — have been shown to demonstrate unique distributional patterns cross-linguistically. It is also claimed that rhotics are more difficult to distinguish from one another phonetically than laterals, and that rhotics are less flexible than laterals when it comes to participation in consonant clusters and coarticulatory patterns. We investigate the phonetic realization of the rhotic and lateral phonemes in a Bantu language, Ruruuli. The acoustic space used for rhotics and laterals in this language is extremely similar, although the density peaks in terms of formant values are different. Formant values as well as formant ratios can be reliably used to distinguish between rhotics and laterals. In common with many other languages, an asymmetry between laterals and rhotics is found in Ruruuli, with laterals being more positionally constrained than rhotics. The overlap in acoustic space between rhotics and laterals may cast doubt on the status or stability of the phonological contrast between rhotics and laterals in this language",
    "keywords": [],
    "checked": true,
    "id": "11da444cbc3dbee5a8399f61d4bc8f0a130f6051",
    "semantic_title": "an exploration of the acoustic space of rhotics and laterals in ruruuli",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bodur21_interspeech.html": {
    "title": "Domain-Initial Strengthening in Turkish: Acoustic Cues to Prosodic Hierarchy in Stop Consonants",
    "volume": "main",
    "abstract": "Studies have shown that cross-linguistically, consonants at the left edge of higher-level prosodic boundaries tend to be more forcefully articulated than those at lower-level boundaries, a phenomenon known as This study tests whether similar effects occur in Turkish, using the Autosegmental-Metrical model proposed by Ipek & Jun [1, 2] as the basis for assessing boundary strength. Productions of /t/ and /d/ were elicited in four domain-initial prosodic positions corresponding to progressively higher-level boundaries: syllable, word, intermediate phrase, and Intonational Phrase. A fifth position, nuclear word, was included in order to better situate it within the prosodic hierarchy. Acoustic correlates of articulatory strength were measured, including closure duration for /d/ and /t/, as well as voice onset time and burst energy for /t/. Our results show that closure duration increases cumulatively from syllable to intermediate phrase, while voice onset time and burst energy are not influenced by boundary strength. These findings provide corroborating evidence for Ipek & Jun's model, particularly for the distinction between word and intermediate phrase boundaries. Additionally, articulatory strength at the left edge of the nuclear word patterned closely with word-initial position, supporting the view that the nuclear word is not associated with a distinct phrasing domain",
    "keywords": [],
    "checked": true,
    "id": "430cb5fe510e2597b337975f86414b013c972ad3",
    "semantic_title": "domain-initial strengthening in turkish: acoustic cues to prosodic hierarchy in stop consonants",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zmolikova21_interspeech.html": {
    "title": "Auxiliary Loss Function for Target Speech Extraction and Recognition with Weak Supervision Based on Speaker Characteristics",
    "volume": "main",
    "abstract": "Automatic speech recognition systems deteriorate in presence of overlapped speech. A popular approach to alleviate this is target speech extraction. The extraction system is usually trained with a loss function measuring the discrepancy between the estimated and the reference target speech. This often leads to distortions to the target signal which is detrimental to the recognition accuracy. Additionally, it is necessary to have the strong supervision provided by parallel data consisting of speech mixtures and single-speaker signals. We propose an auxiliary loss function for retraining the target speech extraction. It is composed of two parts: first, a speaker identity loss, forcing the estimated speech to have correct speaker characteristics, and second, a mixture consistency loss, making the extracted sources sum back to the original mixture. The only supervision required for the proposed loss is speaker characteristics obtained from several segments spoken by the target speaker. Such weak supervision makes the loss suitable for adapting the system directly on real recordings. We show that the proposed loss yields signals more suitable for speech recognition and further, we can gain additional improvements by adaptation to target data. Overall, we can reduce the word error rate on LibriCSS dataset from 27.4% to 24.0%",
    "keywords": [],
    "checked": true,
    "id": "c994372b3c33bbc1ad6b504c5efb5afd515a5009",
    "semantic_title": "auxiliary loss function for target speech extraction and recognition with weak supervision based on speaker characteristics",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/borsdorf21_interspeech.html": {
    "title": "Universal Speaker Extraction in the Presence and Absence of Target Speakers for Speech of One and Two Talkers",
    "volume": "main",
    "abstract": "Speaker extraction has been studied mostly for the scenarios where a target speaker is present in a two or more talkers mixture. Such scenarios do not adequately reflect everyday conversations. For example, a target speaker can be the only active talker, be quiet for a while, or leave the conversation, that means the target speaker is absent from the mixture. Traditional speaker extraction models fail in these scenarios. We propose a novel speaker extraction approach to handle speech mixtures with one or two talkers in which the target speaker can either be present or absent. First, we formulate four speaker extraction conditions to cover the typical scenarios of everyday conversations with one and two talkers. Second, we introduce a joint training scheme with one unified loss function that works for all four conditions. We show that only a small amount of data is required to adapt the model to work well in the four conditions",
    "keywords": [],
    "checked": true,
    "id": "a83c9b63f54b5f8b644f9cac37575a80e972ef21",
    "semantic_title": "universal speaker extraction in the presence and absence of target speakers for speech of one and two talkers",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mateju21_interspeech.html": {
    "title": "Using X-Vectors for Speech Activity Detection in Broadcast Streams",
    "volume": "main",
    "abstract": "A new approach to speech activity detection (SAD) is presented in this work. It allows us to reduce the complexity and computation demands, namely in services that process streaming speech, where a SAD module usually forms the first block of the data pipeline (e.g., in a platform for 24/7 broadcast transcription). Our approach utilizes x-vectors as input features so that, within the subsequent pipeline stages, these embedding instances can also directly be employed for speaker diarization and recognition. The x-vectors are extracted by feed-forward sequential memory network (FSMN), allowing for modeling long-time dependencies; they thus form an input into a computationally undemanding binary classifier, whose output is smoothed by a decoder. Evaluation is performed on the standardized QUT-NOISE-TIMIT dataset as well as on broadcast data with large portions of music and background noise. The former data allows for comparison with other existing approaches. The latter shows the performance in terms of word error rate (WER) and reduction in real-time factor (RTF) of the transcription process",
    "keywords": [],
    "checked": true,
    "id": "c1360ea0059b055a23a18476f99d8c43b9aaf78e",
    "semantic_title": "using x-vectors for speech activity detection in broadcast streams",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/salvati21_interspeech.html": {
    "title": "Time Delay Estimation for Speaker Localization Using CNN-Based Parametrized GCC-PHAT Features",
    "volume": "main",
    "abstract": "We propose a time delay estimation (TDE) method for speaker localization based on parametrized generalized cross-correlation phase transform (PGCC-PHAT) functions and convolutional neural networks (CNNs). The PGCC-PHAT is used to build a feature matrix, which gives TDE information of two microphone signals with different normalization levels in the cross-correlation functions. The feature matrix is processed by a CNN, composed by several convolutional layers and fully connected layers and by a regression output for the directly estimation of the time difference of arrival (TDOA). Simulations in noisy and reverberant adverse conditions show that the proposed method improves the TDOA estimation performance if compared to the GCC-PHAT",
    "keywords": [],
    "checked": true,
    "id": "b60084f373ee49e5da24e3fde7171087337f3d89",
    "semantic_title": "time delay estimation for speaker localization using cnn-based parametrized gcc-phat features",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yousefi21_interspeech.html": {
    "title": "Real-Time Speaker Counting in a Cocktail Party Scenario Using Attention-Guided Convolutional Neural Network",
    "volume": "main",
    "abstract": "Most current speech technology systems are designed to operate well even in the presence of multiple active speakers. However, most solutions assume that the number of co-current speakers is known. Unfortunately, this information might not always be available in real-world applications. In this study, we propose a real-time, single-channel attention-guided Convolutional Neural Network (CNN) to estimate the number of active speakers in overlapping speech. The proposed system extracts higher-level information from the speech spectral content using a CNN model. Next, the attention mechanism summarizes the extracted information into a compact feature vector without losing critical information. Finally, the active speakers are classified using a fully connected network. Experiments on simulated overlapping speech using WSJ corpus show that the attention solution is shown to improve the performance by almost 3% absolute over conventional temporal average pooling. The proposed Attention-guided CNN achieves 76.15% for both Weighted Accuracy and average Recall, and 75.80% Precision on speech segments as short as 20 frames (i.e., 200 ms). All the classification metrics exceed 92% for the attention-guided model in offline scenarios where the input signal is more than 100 frames long (i.e., 1s)",
    "keywords": [],
    "checked": true,
    "id": "9c71e80624ad66b9c7905301f1a2194fd4a1b2b8",
    "semantic_title": "real-time speaker counting in a cocktail party scenario using attention-guided convolutional neural network",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21d_interspeech.html": {
    "title": "End-to-End Language Diarization for Bilingual Code-Switching Speech",
    "volume": "main",
    "abstract": "We propose two end-to-end neural configurations for language diarization on bilingual code-switching speech. The first, a BLSTM-E2E architecture, includes a set of stacked bidirectional LSTMs to compute embeddings and incorporates the deep clustering loss to enforce grouping of languages belonging to the same class. The second, an XSA-E2E architecture, is based on an x-vector model followed by a self-attention encoder. The former encodes frame-level features into segment-level embeddings while the latter considers all those embeddings to generate a sequence of segment-level language labels. We evaluated the proposed methods on the dataset obtained from the shared task B in WSTCSMC 2020 and our handcrafted simulated data from the SEAME dataset. Experimental results show that our proposed XSA-E2E architecture achieved a relative improvement of 12.1% in equal error rate and a 7.4% relative improvement on accuracy compared with the baseline algorithm in the WSTCSMC 2020 dataset. Our proposed XSA-E2E architecture achieved an accuracy of 89.84% with a baseline of 85.60% on the simulated data derived from the SEAME dataset",
    "keywords": [],
    "checked": true,
    "id": "9f08e77a8c072dd3994879f450d9de730b6cfe43",
    "semantic_title": "end-to-end language diarization for bilingual code-switching speech",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/duroselle21_interspeech.html": {
    "title": "Modeling and Training Strategies for Language Recognition Systems",
    "volume": "main",
    "abstract": "Automatic speech recognition is complementary to language recognition. The language recognition systems exploit this complementarity by using frame-level bottleneck features extracted from neural networks trained with a phone recognition task. Recent methods apply frame-level bottleneck features extracted from an end-to-end sequence-to-sequence speech recognition model. In this work, we study an integrated approach of the training of the speech recognition feature extractor and language recognition modules. We show that for both classical phone recognition and end-to-end sequence-to-sequence features, sequential training of the two modules is not the optimal strategy. The feature extractor can be improved by supervision with the language identification loss, either in a fine-tuning step or in a multi-task training framework. Besides, we notice that end-to-end sequence-to-sequence bottleneck features are on par with classical phone recognition bottleneck features without requiring a forced alignment of the signal with target tokens. However, for sequence-to-sequence, the architecture of the model seems to play an important role; the Conformer architectures leads to much better results than the conventional stacked DNNs approach; and can even be trained directly with the LID module in an end-to-end approach",
    "keywords": [],
    "checked": true,
    "id": "a06ff7e5c112532b413f3065432235830fef9c08",
    "semantic_title": "modeling and training strategies for language recognition systems",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21o_interspeech.html": {
    "title": "A Weight Moving Average Based Alternate Decoupled Learning Algorithm for Long-Tailed Language Identification",
    "volume": "main",
    "abstract": "Language identification (LID) research has made tremendous progress in recent years, especially with the introduction of deep learning techniques. However, for real-world applications where the distribution of different language data is highly imbalanced, the performance of existing LID systems is still far from satisfactory. This raises the challenge of In this paper, we propose an effective weight moving average (WMA) based alternate decoupled learning algorithm, termed WADCL, for long-tailed LID. The system is divided into two components, a frontend feature extractor and a backend classifier. These are then alternately learned in an end-to-end manner using different sampling schemes to alleviate the distribution mismatch between training and test datasets. Furthermore, our WMA method aims to mitigate the side-effects of re-sampling schemes, by fusing the model parameters learned along the trajectory of stochastic gradient descent (SGD) optimization. To validate the effectiveness of the proposed WADCL algorithm, we evaluate and compare several systems over a language dataset constructed to match a long-tailed distribution based on real world application [1]. The experimental results from the long-tailed language dataset demonstrate that the proposed algorithm is able to achieve significant performance gains over existing state-of-the-art x-vector based LID methods",
    "keywords": [],
    "checked": true,
    "id": "1d3eb72c47df9e19823a74e58d65c6776ef598df",
    "semantic_title": "a weight moving average based alternate decoupled learning algorithm for long-tailed language identification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21b_interspeech.html": {
    "title": "Improving Accent Identification and Accented Speech Recognition Under a Framework of Self-Supervised Learning",
    "volume": "main",
    "abstract": "Recently, self-supervised pre-training has gained success in automatic speech recognition (ASR). However, considering the difference between speech accents in real scenarios, how to identify accents and use accent features to improve ASR is still challenging. In this paper, we employ the self-supervised pre-training method for both accent identification and accented speech recognition tasks. For the former task, a standard deviation constraint loss (SDC-loss) based end-to-end (E2E) architecture is proposed to identify accents under the same language. As for accented speech recognition task, we design an accent-dependent ASR system, which can utilize additional accent input features. Furthermore, we propose a frame-level accent feature, which is extracted based on the proposed accent identification model and can be dynamically adjusted. We pre-train our models using 960 hours unlabeled LibriSpeech dataset and fine-tune them on AESRC2020 speech dataset. The experimental results show that our proposed accent-dependent ASR system is significantly ahead of the AESRC2020 baseline and achieves 6.5% relative word error rate (WER) reduction compared with our accent-independent ASR system",
    "keywords": [],
    "checked": true,
    "id": "a5db43d09f18c8c59487dc81321ff4fd358468d4",
    "semantic_title": "improving accent identification and accented speech recognition under a framework of self-supervised learning",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fan21_interspeech.html": {
    "title": "Exploring wav2vec 2.0 on Speaker Verification and Language Identification",
    "volume": "main",
    "abstract": "wav2vec 2.0 is a recently proposed self-supervised framework for speech representation learning. It follows a two-stage training process of pre-training and fine-tuning, and performs well in speech recognition tasks especially ultra-low resource cases. In this work, we attempt to extend the self-supervised framework to speaker verification and language identification. First, we use some preliminary experiments to indicate that wav2vec 2.0 can capture the information about the speaker and language. Then we demonstrate the effectiveness of wav2vec 2.0 on the two tasks respectively. For speaker verification, we obtain a competitive result with the Equal Error Rate (EER) of 3.61% on the VoxCeleb1 dataset. For language identification, we obtain an EER of 12.02% on the 1 second condition and an EER of 3.47% on the full-length condition of the AP17-OLR dataset. Finally, we utilize one model to achieve the unified modeling by the multi-task learning for the two tasks",
    "keywords": [],
    "checked": true,
    "id": "9a9d374d1dad72a0349c3a64be93660151274f41",
    "semantic_title": "exploring wav2vec 2.0 on speaker verification and language identification",
    "citation_count": 136,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ramesh21_interspeech.html": {
    "title": "Self-Supervised Phonotactic Representations for Language Identification",
    "volume": "main",
    "abstract": "Phonotactic constraints characterize the sequence of permissible phoneme structures in a language and hence form an important cue for language identification (LID) task. As phonotactic constraints span across multiple phonemes, the short-term spectral analysis (20–30 ms) alone is not sufficient to capture them. The speech signal has to be analyzed over longer contexts (100s of milliseconds) in order to extract features representing the phonotactic constraints. The supervised senone classifiers, aimed at modeling triphone context, have been used for extracting language-specific features for the LID task. However, it is difficult to get large amounts of manually labeled data to train the supervised models. In this work, we explore a self-supervised approach to extract long-term contextual features for the LID task. We have used wav2vec architecture to extract contextualized representations from multiple frames of the speech signal. The contextualized representations extracted from the pre-trained wav2vec model are used for the LID task. The performance of the proposed features is evaluated on a dataset containing 7 Indian languages. The proposed self-supervised embeddings achieved 23% absolute improvement over the acoustic features and 3% absolute improvement over their supervised counterparts",
    "keywords": [],
    "checked": true,
    "id": "faa8fcc1b5343cc85881ff16e818c31596987f02",
    "semantic_title": "self-supervised phonotactic representations for language identification",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21j_interspeech.html": {
    "title": "E2E-Based Multi-Task Learning Approach to Joint Speech and Accent Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a single multi-task learning framework to perform End-to-End (E2E) speech recognition (ASR) and accent recognition (AR) simultaneously. The proposed framework is not only more compact but can also yield comparable or even better results than standalone systems. Specifically, we found that the overall performance is predominantly determined by the ASR task, and the E2E-based ASR pretraining is essential to achieve improved performance, particularly for the AR task. Additionally, we conduct several analyses of the proposed method. First, though the objective loss for the AR task is much smaller compared with its counterpart of ASR task, a smaller weighting factor with the AR task in the joint objective function is necessary to yield better results for each task. Second, we found that sharing only a few layers of the encoder yields better AR results than sharing the overall encoder. Experimentally, the proposed method produces WER results close to the best standalone E2E ASR ones, while it achieves 7.7% and 4.2% relative improvement over standalone and single-task-based joint recognition methods on test set for accent recognition respectively",
    "keywords": [],
    "checked": true,
    "id": "1117dd82eec9f024ff6d3a3b72a909e194217822",
    "semantic_title": "e2e-based multi-task learning approach to joint speech and accent recognition",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tzudir21_interspeech.html": {
    "title": "Excitation Source Feature Based Dialect Identification in Ao — A Low Resource Language",
    "volume": "main",
    "abstract": "Ao is an under-resourced Tibeto-Burman tonal language spoken in Nagaland, India. There are three distinct dialects of the language, namely, Chungli, Mongsen and Changki. The objective of dialect identification is to identify one dialect from the other within the same language family. The goal of this study is to ascertain the potential of excitation source features for automatic dialect identification in Ao. In this direction, Integrated Linear Prediction Residual (ILPR), an approximate representation of source signal, is explored. The log Mel spectrogram of ILPR ( ) signal is used to exploit the time-frequency characteristics of the excitation source. This work proposes attention based CNN-BiGRU architecture for automatic dialect identification tasks. Additionally, log Mel spectrogram ( ), extracted from the pre-emphasized speech signal, is used as a baseline method. The ( ) contains the vocal-tract characteristics of the speech signal. A significant performance improvement of (nearly) 6% accuracy is observed when the excitation source feature ( ) is combined with the vocal tract representation ( ). To analyse the effect of segment duration, dialect identification performance is reported for three different durations, viz., 1 sec, 3 sec and 6 sec. The effect of gender in dialect identification task for Ao is also studied in this work",
    "keywords": [],
    "checked": true,
    "id": "9266103ac27c490009e60caaf2fee1aadaadb3bc",
    "semantic_title": "excitation source feature based dialect identification in ao - a low resource language",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/khare21_interspeech.html": {
    "title": "Low Resource ASR: The Surprising Effectiveness of High Resource Transliteration",
    "volume": "main",
    "abstract": "Cross-lingual transfer of knowledge from high-resource languages to low-resource languages is an important research problem in automatic speech recognition (ASR). We propose a new strategy of transfer learning by pretraining using large amounts of speech in the high-resource language but with its text transliterated to the target low-resource language. This simple mapping of scripts explicitly encourages increased sharing between the output spaces of both languages and is surprisingly effective even when the high-resource and low-resource languages are from unrelated language families. The utility of our proposed technique is more evident in very low-resource scenarios, where better initializations are more beneficial. We evaluate our technique on a transformer ASR architecture and the state-of-the-art wav2vec2.0 ASR architecture, with English as the high-resource language and six languages as low-resource targets. With access to 1 hour of target speech, we obtain relative WER reductions of up to 8.2% compared to existing transfer-learning approaches",
    "keywords": [],
    "checked": true,
    "id": "40115c0e996d5a71f589407bd2deaee4b58e0ee2",
    "semantic_title": "low resource asr: the surprising effectiveness of high resource transliteration",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/feng21_interspeech.html": {
    "title": "Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation",
    "volume": "main",
    "abstract": "This paper tackles automatically discovering phone-like acoustic units (AUD) from unlabeled speech data. Past studies usually proposed single-step approaches. We propose a two-stage approach: the first stage learns a subword-discriminative feature representation, and the second stage applies clustering to the learned representation and obtains phone-like clusters as the discovered acoustic units. In the first stage, a recently proposed method in the task of unsupervised subword modeling is improved by replacing a monolingual out-of-domain (OOD) ASR system with a multilingual one to create a subword-discriminative representation that is more language-independent. In the second stage, segment-level k-means is adopted, and two methods to represent the variable-length speech segments as fixed-dimension feature vectors are compared. Experiments on a very low-resource Mboshi language corpus show that our approach outperforms state-of-the-art AUD in both normalized mutual information (NMI) and F-score. The multilingual ASR improved upon the monolingual ASR in providing OOD phone labels and in estimating the phone boundaries. A comparison of our systems with and without knowing the ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that the current approach can significantly benefit from improved phone boundary estimation",
    "keywords": [],
    "checked": true,
    "id": "992bfc020bb91005efdfd25455f9de4872ef6c72",
    "semantic_title": "unsupervised acoustic unit discovery by leveraging a language-independent subword discriminative feature representation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kamper21_interspeech.html": {
    "title": "Towards Unsupervised Phone and Word Segmentation Using Self-Supervised Vector-Quantized Neural Networks",
    "volume": "main",
    "abstract": "We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized (VQ) neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized dynamic programming method generally performs best. While performance on individual tasks is only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate",
    "keywords": [],
    "checked": true,
    "id": "0e8a0d7606ceff40a1111a26732d8bd97ce0b66a",
    "semantic_title": "towards unsupervised phone and word segmentation using self-supervised vector-quantized neural networks",
    "citation_count": 31,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jiang21_interspeech.html": {
    "title": "Speech SimCLR: Combining Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Self-supervised visual pretraining has shown significant progress recently. Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semi-supervised learning on ImageNet. The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning. In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning. During training, Speech SimCLR applies augmentation on raw speech and its spectrogram. Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation. The proposed method achieved competitive results on speech emotion recognition and speech recognition",
    "keywords": [],
    "checked": true,
    "id": "6f5fcdceec0f86b54ace93e30cdc497b5568b714",
    "semantic_title": "speech simclr: combining contrastive and reconstruction objective for self-supervised speech representation learning",
    "citation_count": 49,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jacobs21_interspeech.html": {
    "title": "Multilingual Transfer of Acoustic Word Embeddings Improves When Training on Languages Related to the Target Zero-Resource Language",
    "volume": "main",
    "abstract": "Acoustic word embedding models map variable duration speech segments to fixed dimensional vectors, enabling efficient speech search and discovery. Previous work explored how embeddings can be obtained in zero-resource settings where no labelled data is available in the target language. The current best approach uses transfer learning: a single supervised multilingual model is trained using labelled data from multiple well-resourced languages and then applied to a target zero-resource language (without fine-tuning). However, it is still unclear how the specific choice of training languages affect downstream performance. Concretely, here we ask whether it is beneficial to use training languages related to the target. Using data from eleven languages spoken in Southern Africa, we experiment with adding data from different language families while controlling for the amount of data per language. In word discrimination and query-by-example search evaluations, we show that training on languages from the same family gives large improvements. Through finer-grained analysis, we show that training on even just a single related language gives the largest gain. We also find that adding data from unrelated languages generally doesn't hurt performance",
    "keywords": [],
    "checked": true,
    "id": "5669e41490b0854a3dd7a1f74414bf1de382235d",
    "semantic_title": "multilingual transfer of acoustic word embeddings improves when training on languages related to the target zero-resource language",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/niekerk21_interspeech.html": {
    "title": "Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing",
    "volume": "main",
    "abstract": "Contrastive predictive coding (CPC) aims to learn representations of speech by distinguishing future observations from a set of negative examples. Previous work has shown that linear classifiers trained on CPC features can accurately predict speaker and phone labels. However, it is unclear how the features actually capture speaker and phonetic information, and whether it is possible to normalize out the irrelevant details (depending on the downstream task). In this paper, we first show that the per-utterance mean of CPC features captures speaker information to a large extent. Concretely, we find that comparing means performs well on a speaker verification task. Next, probing experiments show that standardizing the features effectively removes speaker information. Based on this observation, we propose a speaker normalization step to improve acoustic unit discovery using K-means clustering of CPC features. Finally, we show that a language model trained on the resulting units achieves some of the best results in the ZeroSpeech2021 Challenge",
    "keywords": [],
    "checked": true,
    "id": "deaac42a7aac5c1210495c7346c7d7dd77b02a50",
    "semantic_title": "analyzing speaker information in self-supervised models to improve zero-resource speech processing",
    "citation_count": 25,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/takahashi21_interspeech.html": {
    "title": "Unsupervised Neural-Based Graph Clustering for Variable-Length Speech Representation Discovery of Zero-Resource Languages",
    "volume": "main",
    "abstract": "Discovering symbolic units from unannotated speech data is fundamental in zero resource speech technology. Previous studies focused on learning fixed-length frame units based on acoustic features. Although they achieve high quality, they also suffer from a high bit-rate due to time-frame encoding. In this work, to discover variable-length, low bit-rate speech representation from a limited amount of unannotated speech data, we propose an approach based on graph neural networks (GNNs), and we study the temporal closeness of salient speech features. Our approach is built upon vector-quantized neural networks (VQNNs), which learn discrete encoding by contrastive predictive coding (CPC). We exploit the predetermined finite set of embeddings (a codebook) used by VQNNs to encode input data. We consider a codebook a set of nodes in a directed graph, where each arc represents the transition from one feature to another. Subsequently, we extract and encode the topological features of nodes in the graph to cluster them using graph convolution. By this process, we can obtain coarsened speech representation. We evaluated our model on the English data set of the ZeroSpeech 2020 challenge on Track 2019. Our model successfully drops the bit rate while achieving high unit quality",
    "keywords": [],
    "checked": true,
    "id": "ecb4226501cfdbb9c0e949c96391d649d71a62be",
    "semantic_title": "unsupervised neural-based graph clustering for variable-length speech representation discovery of zero-resource languages",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maekaku21_interspeech.html": {
    "title": "Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021",
    "volume": "main",
    "abstract": "We present a system for the Zero Resource Speech Challenge 2021, which combines a Contrastive Predictive Coding (CPC) with deep cluster. In deep cluster, we first prepare pseudo-labels obtained by clustering the outputs of a CPC network with k-means. Then, we train an additional autoregressive model to classify the previously obtained pseudo-labels in a supervised manner. Phoneme discriminative representation is achieved by executing the second-round clustering with the outputs of the final layer of the autoregressive model. We show that replacing a Transformer layer with a Conformer layer leads to a further gain in a lexical metric. Experimental results show that a relative improvement of 35% in a phonetic metric, 1.5% in the lexical metric, and 2.3% in a syntactic metric are achieved compared to a baseline method of CPC-small which is trained on LibriSpeech 460h data. We achieve top results in this challenge with the syntactic metric",
    "keywords": [],
    "checked": true,
    "id": "63d99a61e798d7cb714f336a8d581ae2b75672ee",
    "semantic_title": "speech representation learning combining conformer cpc with deep cluster for the zerospeech challenge 2021",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cui21_interspeech.html": {
    "title": "Identifying Indicators of Vulnerability from Short Speech Segments Using Acoustic and Textual Features",
    "volume": "main",
    "abstract": "In order to protect vulnerable people in telemarketing, organisations have to investigate the speech recordings to identify them first. Typically, the investigation is manually conducted. As such, the procedure is costly and time-consuming. With an automatic vulnerability detection system, more vulnerable people can be identified and protected. A standard telephone conversation lasts around 5 minutes, the detection system is expected to be able to identify such a potential vulnerable speaker from speech segments. Due to the complexity of the vulnerability definition and the unavailable annotated vulnerability examples, this paper attempts to address the detection problem as three classification tasks: age classification, accent classification and patient/non-patient classification utilising publicly available datasets. In the proposed system, we trained three sub models using acoustic and textual features for each sub task. Each trained model was evaluated on multiple datasets and achieved competitive results compared to a strong baseline (i.e. in-dataset accuracy)",
    "keywords": [],
    "checked": true,
    "id": "080d6ff279a41edd4698c523704c0c3ba38d3aee",
    "semantic_title": "identifying indicators of vulnerability from short speech segments using acoustic and textual features",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dunbar21_interspeech.html": {
    "title": "The Zero Resource Speech Challenge 2021: Spoken Language Modelling",
    "volume": "main",
    "abstract": "We present the Zero Resource Speech Challenge 2021, which asks participants to learn a language model directly from audio, without any text or labels. The challenge is based on the Libri-light dataset, which provides up to 60k hours of audio from English audio books without any associated text. We provide a pipeline baseline system consisting on an encoder based on contrastive predictive coding (CPC), a quantizer (k-means) and a standard language model (BERT or LSTM). The metrics evaluate the learned representations at the acoustic (ABX discrimination), lexical (spot-the-word), syntactic (acceptability judgment) and semantic levels (similarity judgment). We present an overview of the eight submitted systems from four groups and discuss the main results",
    "keywords": [],
    "checked": true,
    "id": "2a500dae25af18424997733df2e1600250293f40",
    "semantic_title": "the zero resource speech challenge 2021: spoken language modelling",
    "citation_count": 33,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gudur21_interspeech.html": {
    "title": "Zero-Shot Federated Learning with New Classes for Audio Classification",
    "volume": "main",
    "abstract": "Federated learning is an effective way of extracting insights from different user devices while preserving the privacy of users. However, new classes with completely unseen data distributions can stream across any device in a federated learning setting, whose data cannot be accessed by the global server or other users. To this end, we propose a unified zero-shot framework to handle these aforementioned challenges during federated learning. We simulate two scenarios here — 1) when the new class labels are not reported by the user, the traditional FL setting is used; 2) when new class labels are reported by the user, we synthesize by calculating class similarity matrices corresponding to each device's new classes followed by unsupervised clustering to distinguish between new classes across different users. Moreover, our proposed framework can also handle statistical heterogeneities in both labels and models across the participating users. We empirically evaluate our framework on-device across different communication rounds (FL iterations) with new classes in both local and global updates, along with heterogeneous labels and models, on two widely used audio classification applications — keyword spotting and urban sound classification, and observe an average deterministic accuracy increase of ~4.041% and ~4.258% respectively",
    "keywords": [],
    "checked": true,
    "id": "ff5345a11bd6f73d4daad0d6b27248e3cca7770d",
    "semantic_title": "zero-shot federated learning with new classes for audio classification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rouditchenko21_interspeech.html": {
    "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos",
    "volume": "main",
    "abstract": "Current methods for learning visually grounded language from videos often rely on text annotation, such as human generated captions or machine generated automatic speech recognition (ASR) transcripts. In this work, we introduce the Audio-Video Language Network (AVLnet), a self-supervised network that learns a shared audio-visual embedding space directly from raw video inputs. To circumvent the need for text annotation, we learn audio-visual representations from randomly segmented video clips and their raw audio waveforms. We train AVLnet on HowTo100M, a large corpus of publicly available instructional videos, and evaluate on image retrieval and video retrieval tasks, achieving state-of-the-art performance. Finally, we perform analysis of AVLnet's learned representations, showing our model utilizes speech and natural sounds to learn audio-visual concepts",
    "keywords": [],
    "checked": true,
    "id": "335d0afabfd29c4aa6bb07ad67f907037a7d96b8",
    "semantic_title": "avlnet: learning audio-visual language representations from instructional videos",
    "citation_count": 114,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21b_interspeech.html": {
    "title": "N-Singer: A Non-Autoregressive Korean Singing Voice Synthesis System for Pronunciation Enhancement",
    "volume": "main",
    "abstract": "Recently, end-to-end Korean singing voice systems have been designed to generate realistic singing voices. However, these systems still suffer from a lack of robustness in terms of pronunciation accuracy. In this paper, we propose N-Singer, a non-autoregressive Korean singing voice system, to synthesize accurate and pronounced Korean singing voices in parallel. N-Singer consists of a Transformer-based mel-generator, a convolutional network-based postnet, and voicing-aware discriminators. It can contribute in the following ways. First, for accurate pronunciation, N-Singer separately models linguistic and pitch information without other acoustic features. Second, to achieve improved mel-spectrograms, N-Singer uses a combination of Transformer-based modules and convolutional network-based modules. Third, in adversarial training, voicing-aware conditional discriminators are used to capture the harmonic features of voiced segments and noise components of unvoiced segments. The experimental results prove that N-Singer can synthesize a natural singing voice in parallel with a more accurate pronunciation than the baseline model",
    "keywords": [],
    "checked": true,
    "id": "dce90fcd35e943c2e45e82b7d9343911bfc5c6f6",
    "semantic_title": "n-singer: a non-autoregressive korean singing voice synthesis system for pronunciation enhancement",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maniati21_interspeech.html": {
    "title": "Cross-Lingual Low Resource Speaker Adaptation Using Phonological Features",
    "volume": "main",
    "abstract": "The idea of using phonological features instead of phonemes as input to sequence-to-sequence TTS has been recently proposed for zero-shot multilingual speech synthesis. This approach is useful for code-switching, as it facilitates the seamless uttering of foreign text embedded in a stream of native text. In our work, we train a language-agnostic multispeaker model conditioned on a set of phonologically derived features common across different languages, with the goal of achieving cross-lingual speaker adaptation. We first experiment with the effect of language phonological similarity on cross-lingual TTS of several source-target language combinations. Subsequently, we fine-tune the model with very limited data of a new speaker's voice in either a seen or an unseen language, and achieve synthetic speech of equal quality, while preserving the target speaker's identity. With as few as 32 and 8 utterances of target speaker data, we obtain high speaker similarity scores and naturalness comparable to the corresponding literature. In the extreme case of only 2 available adaptation utterances, we find that our model behaves as a few-shot learner, as the performance is similar in both the seen and unseen adaptation language scenarios",
    "keywords": [],
    "checked": true,
    "id": "f6914a81333e62cb81c5e6b52e5fef5c859cbdb7",
    "semantic_title": "cross-lingual low resource speaker adaptation using phonological features",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhan21_interspeech.html": {
    "title": "Improve Cross-Lingual Text-To-Speech Synthesis on Monolingual Corpora with Pitch Contour Information",
    "volume": "main",
    "abstract": "Cross-lingual text-to-speech (TTS) synthesis on monolingual corpora is still a challenging task, especially when many kinds of languages are involved. In this paper, we improve the cross-lingual TTS model on monolingual corpora with pitch contour information. We propose a method to obtain pitch contour sequences for different languages without manual annotation, and extend the Tacotron-based TTS model with the proposed Pitch Contour Extraction (PCE) module. Our experimental results show that the proposed approach can effectively improve the naturalness and consistency of synthesized mixed-lingual utterances",
    "keywords": [],
    "checked": true,
    "id": "8fc04ba2024f99b0a3bcde765cd3e4c0fbb2f074",
    "semantic_title": "improve cross-lingual text-to-speech synthesis on monolingual corpora with pitch contour information",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21d_interspeech.html": {
    "title": "Cross-Lingual Voice Conversion with Disentangled Universal Linguistic Representations",
    "volume": "main",
    "abstract": "Intra-lingual voice conversion has achieved great progress recently in terms of naturalness and similarity. However, in cross-lingual voice conversion, there is still an urgent need to improve the quality of the converted speech, especially with nonparallel training data. Previous works usually use Phonetic Posteriorgrams (PPGs) as the linguistic representations. In the case of cross-lingual voice conversion, the linguistic information is therefore represented as PPGs. It is well-known that PPGs may suffer from word dropping and mispronunciation, especially when the input speech is noisy. In addition, systems using PPGs can only convert the input into a known target language that is seen during training. This paper proposes an any-to-many voice conversion system based on disentangled universal linguistic representations (ULRs), which are extracted from a mix-lingual phoneme recognition system. Two methods are proposed to remove speaker information from ULRs. Experimental results show that the proposed method can effectively improve the converted speech objectively and subjectively. The system can also convert speech utterances naturally even if the language is not seen during training",
    "keywords": [],
    "checked": true,
    "id": "ecaa4fe7f59efd5d5a857ea1112d202a1bcb85fb",
    "semantic_title": "cross-lingual voice conversion with disentangled universal linguistic representations",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21e_interspeech.html": {
    "title": "EfficientSing: A Chinese Singing Voice Synthesis System Using Duration-Free Acoustic Model and HiFi-GAN Vocoder",
    "volume": "main",
    "abstract": "In this paper, we present EfficientSing, a Chinese singing voice synthesis (SVS) system based on a non-autoregressive duration-free acoustic model and HiFi-GAN neural vocoder. Different from many existing SVS methods, no auxiliary duration prediction module is needed in this work, since a newly proposed monotonic alignment modeling mechanism is adopted. Moreover, we follow the non-autoregressive architecture of EfficientTTS with some singing-specific adaption, making training and inference fully parallel and efficient. HiFi-GAN vocoder is adopted to improve the voice quality of synthesized songs and inference efficiency. Both objective and subjective experimental results show that the proposed system can produce quite natural and high-fidelity songs and outperform the Tacotron-based baseline in terms of pronunciation, pitch and rhythm",
    "keywords": [],
    "checked": true,
    "id": "452fdd6af091f98b7a4968ff6cb4ebcf9581c1c4",
    "semantic_title": "efficientsing: a chinese singing voice synthesis system using duration-free acoustic model and hifi-gan vocoder",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xin21_interspeech.html": {
    "title": "Cross-Lingual Speaker Adaptation Using Domain Adaptation and Speaker Consistency Loss for Text-To-Speech Synthesis",
    "volume": "main",
    "abstract": "We present a cross-lingual speaker adaptation method based on domain adaptation and a speaker consistency loss for text-to-speech (TTS) synthesis. Existing monolingual speaker adaptation methods based on direct fine-tuning are not applicable for cross-lingual data. The proposed method first trains a language-independent speaker encoder by speaker verification using domain adaption on multilingual data, including the source and the target languages. Then the proposed method trains a monolingual multi-speaker TTS model on the source language's data using the speaker embeddings generated by the speaker encoder. To adapt the TTS model of the source language to new speakers the proposed method uses a speaker consistency loss to maximize the cosine similarity between speaker embeddings generated from the natural speech and the same speaker's synthesized speech. This makes fine-tuning the TTS model of source language on speech data of target language become possible. We conduct experiments on multi-speaker English and Japanese datasets with 207 speakers in total. Results of comprehensive experiments demonstrate that the proposed method can significantly improve speech naturalness compared to the baseline method",
    "keywords": [],
    "checked": true,
    "id": "8bbb8692abd912857a7b1631ec02263b8d5bb0e2",
    "semantic_title": "cross-lingual speaker adaptation using domain adaptation and speaker consistency loss for text-to-speech synthesis",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shang21_interspeech.html": {
    "title": "Incorporating Cross-Speaker Style Transfer for Multi-Language Text-to-Speech",
    "volume": "main",
    "abstract": "Recently multilingual TTS systems using only monolingual datasets have obtained significant improvement. However, the quality of cross-language speech synthesis is not comparable to the speaker's own language and often comes with a heavy foreign accent. This paper proposed a multi-speaker multi-style multi-language speech synthesis system (M3), which improves the speech quality by introducing a fine-grained style encoder and overcomes the non-authentic accent problem through cross-speaker style transfer. To avoid leaking timbre information into style encoder, we utilized a speaker conditional variational encoder and conducted adversarial speaker training using the gradient reversal layer. Then, we built a Mixture Density Network (MDN) for mapping text to extracted style vectors for each speaker. At the inference stage, cross-language style transfer could be achieved by assigning any speaker's style type in the target language. Our system uses existing speaker style and genuinely avoids foreign accents. In the MOS-speech-naturalness, the proposed method generally achieves 4.0 and significantly outperform the baseline system",
    "keywords": [],
    "checked": true,
    "id": "cc9c79b826e5e6b5c278baa318453e6c0e8d807c",
    "semantic_title": "incorporating cross-speaker style transfer for multi-language text-to-speech",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kesim21_interspeech.html": {
    "title": "Investigating Contributions of Speech and Facial Landmarks for Talking Head Generation",
    "volume": "main",
    "abstract": "Talking head generation is an active research problem. It has been widely studied as a direct speech-to-video or two stage speech-to-landmarks-to-video mapping problem. In this study, our main motivation is to assess individual and joint contributions of the speech and facial landmarks to the talking head generation quality through a state-of-the-art generative adversarial network (GAN) architecture. Incorporating frame and sequence discriminators and a feature matching loss, we investigate performances of speech only, landmark only and joint speech and landmark driven talking head generation on the CREMA-D dataset. Objective evaluations using the peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) and landmark distance (LMD) indicate that while landmarks bring PSNR and SSIM improvements to the speech driven system, speech brings LMD improvement to the landmark driven system. Furthermore, feature matching is observed to improve the speech driven talking head generation models significantly",
    "keywords": [],
    "checked": true,
    "id": "50a1608b73882e8919c99486bbf8b5611ccbd7b9",
    "semantic_title": "investigating contributions of speech and facial landmarks for talking head generation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/si21b_interspeech.html": {
    "title": "Speech2Video: Cross-Modal Distillation for Speech to Video Generation",
    "volume": "main",
    "abstract": "This paper investigates a novel task of talking face video generation solely from speeches. The speech-to-video generation technique can spark interesting applications in entertainment, customer service, and human-computer-interaction industries. Indeed, the timbre, accent and speed in speeches could contain rich information relevant to speakers' appearance. The challenge mainly lies in disentangling the distinct visual attributes from audio signals. In this article, we propose a light-weight, cross-modal distillation method to extract disentangled emotional and identity information from unlabelled video inputs. The extracted features are then integrated by a generative adversarial network into talking face video clips. With carefully crafted discriminators, the proposed framework achieves realistic generation results. Experiments with observed individuals demonstrated that the proposed framework captures the emotional expressions solely from speeches, and produces spontaneous facial motion in the video output. Compared to the baseline method where speeches are combined with a static image of the speaker, the results of the proposed framework is almost indistinguishable. User studies also show that the proposed method outperforms the existing algorithms in terms of emotion expression in the generated videos",
    "keywords": [],
    "checked": true,
    "id": "73ede03fe57e40380a0b0814adb135fd235622b3",
    "semantic_title": "speech2video: cross-modal distillation for speech to video generation",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21c_interspeech.html": {
    "title": "NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling",
    "volume": "main",
    "abstract": "In this work, we introduce , the first neural audio upsampling model to produce waveforms of sampling rate 48kHz from coarse 16kHz or 24kHz inputs, while prior works could generate only up to 16kHz. NU-Wave is the first diffusion probabilistic model for audio super-resolution which is engineered based on neural vocoders. NU-Wave generates high-quality audio that achieves high performance in terms of signal-to-noise ratio (SNR), log-spectral distance (LSD), and accuracy of the ABX test. In all cases, NU-Wave outperforms the baseline models despite the substantially smaller model capacity (3.0M parameters) than baselines (5.4–21%). The audio samples of our model are publicly available, and the code will be made available soon",
    "keywords": [],
    "checked": true,
    "id": "89cc5d8b9c1579d5a5ba905cfc95c907428a7eb4",
    "semantic_title": "nu-wave: a diffusion probabilistic model for neural audio upsampling",
    "citation_count": 48,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21c_interspeech.html": {
    "title": "QISTA-Net-Audio: Audio Super-Resolution via Non-Convex ℓ_q-Norm Minimization",
    "volume": "main",
    "abstract": "Audio super-resolution (ASR) aims to reconstruct the high-resolution signal from its corresponding low-resolution one, which is hard while the correlation between them is low In this paper, we propose a learning model, QISTA-Net-Audio, to solve ASR in a paradigm of linear inverse problem. QISTA-Net-Audio is composed of two components. First, an audio waveform can be presented as a complex-valued spectrum, which is composed of a real and an imaginary part, in the frequency domain. We treat the real and imaginary parts as an image, and predict a high-resolution spectrum but only keep the phase information from the viewpoint of image reconstruction. Second, we predict the magnitude information by solving the sparse signal reconstruction problem. By combining the predicted magnitude and the phase together, we can recover the high-resolution waveform. Comparison with the state-of-the-art method MfNet [1], in terms of measure metrics SNR, PESQ, and STOI, demonstrates the superior performance of our method",
    "keywords": [],
    "checked": true,
    "id": "081ca9b8c3db32fe0e27fb3aa18a0d0f43a89c3f",
    "semantic_title": "qista-net-audio: audio super-resolution via non-convex ℓ_q-norm minimization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wen21_interspeech.html": {
    "title": "X-net: A Joint Scale Down and Scale Up Method for Voice Call",
    "volume": "main",
    "abstract": "This paper proposes X-net, a jointly learned scale-down and scale-up architecture for data pre- and post-processing in voice calls, as a means to bandwidth extension over band-limited channels. Scale-down and scale-up are deployed separately on transmitter and receiver to perform down- and upsampling. Separate supervisions are used on the submodules so that X-net can work properly even if one submodule is missing. A two-stage training method is used to learn X-net for improved perceptual quality. Results show that jointly learned X-net achieves promising improvement over blind audio super-resolution by both objective and subjective metrics, even in a lightweight implementation with only 1k parameters",
    "keywords": [],
    "checked": true,
    "id": "ab6caf7b92275fe0cf92a5f802cea8efdf3ef3ae",
    "semantic_title": "x-net: a joint scale down and scale up method for voice call",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21k_interspeech.html": {
    "title": "WSRGlow: A Glow-Based Waveform Generative Model for Audio Super-Resolution",
    "volume": "main",
    "abstract": "Audio super-resolution is the task of constructing a high-resolution (HR) audio from a low-resolution (LR) audio by adding the missing band. Previous methods based on convolutional neural networks and mean squared error training objective have relatively low performance, while adversarial generative models are difficult to train and tune. Recently, normalizing flow has attracted a lot of attention for its high performance, simple training and fast inference. In this paper, we propose WSRGlow, a Glow-based waveform generative model to perform audio super-resolution. Specifically, 1) we integrate WaveNet and Glow to directly maximize the exact likelihood of the target HR audio conditioned on LR information; and 2) to exploit the audio information from low-resolution audio, we propose an LR audio encoder and an STFT encoder, which encode the LR information from the time domain and frequency domain respectively. The experimental results show that the proposed model is easier to train and outperforms the previous works in terms of both objective and perceptual quality. WSRGlow is also the first model to produce 48kHz waveforms from 12kHz LR audio. Audio samples are publicly available",
    "keywords": [],
    "checked": true,
    "id": "207660ecf41fc95036f1ce20544dac405e615796",
    "semantic_title": "wsrglow: a glow-based waveform generative model for audio super-resolution",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yi21_interspeech.html": {
    "title": "Half-Truth: A Partially Fake Audio Detection Dataset",
    "volume": "main",
    "abstract": "Diverse promising datasets have been designed to further the development of fake audio detection, such as ASVspoof databases. However, previous datasets ignore an attacking situation, in which the hacker hides some small fake clips in real speech audio. This poses a serious threat since that it is difficult to distinguish the small fake clip from the whole speech utterance. Therefore, this paper develops such a dataset for half-truth audio detection (HAD). Partially fake audio in the HAD dataset involves only changing a few words in an utterance. The audio of the words is generated with the very latest state-of-the-art speech synthesis technology. We can not only detect fake utterances but also localize manipulated regions in a speech using this dataset. Some benchmark results are presented on this dataset. The results show that partially fake audio presents much more challenging than fully fake audio for fake audio detection",
    "keywords": [],
    "checked": true,
    "id": "9e70326cc45455470feb2ae5826cd34f5ebf750d",
    "semantic_title": "half-truth: a partially fake audio detection dataset",
    "citation_count": 38,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chettri21_interspeech.html": {
    "title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization",
    "volume": "main",
    "abstract": "Voice anti-spoofing aims at classifying a given utterance either as a bonafide human sample, or a spoofing attack (e.g. synthetic or replayed sample). Many anti-spoofing methods have been proposed but most of them fail to generalize across domains (corpora) — and we do not know We outline a novel interpretative framework for gauging the impact of data quality upon anti-spoofing performance. Our within- and between-domain experiments pool data from seven public corpora and three anti-spoofing methods based on Gaussian mixture and convolutive neural network models. We assess the impacts of long-term spectral information, speaker population (through x-vector speaker embeddings), signal-to-noise ratio, and selected voice quality features",
    "keywords": [],
    "checked": true,
    "id": "81de553c927a84d5b7e22e9f702d2c8585cb2b14",
    "semantic_title": "data quality as predictor of voice anti-spoofing generalization",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheon21_interspeech.html": {
    "title": "Coded Speech Enhancement Using Neural Network-Based Vector-Quantized Residual Features",
    "volume": "main",
    "abstract": "Various approaches have been proposed to improve the quality of the speech coded at low bitrates. Recently, deep neural networks have also been used for speech coding, providing a high quality of speech with low bitrates. Although designing an entire codec with neural networks may be more effective, backward compatibility with the existing codecs can be desirable so that the systems with the legacy codec can still decode the coded bitstream. In this paper, we propose to generate side information based on neural networks for an existing codec and enhance the decoded speech with another neural networks using the side information. The vector-quantization variational autoencoder (VQ-VAE) is applied to generate vector-quantized side information and reconstruct the residual features, which are the difference between the features extracted from the original and decoded signals. The post-processor in the decoder side, which is another neural network, takes the decoded signal of the main codec and the reconstructed residual features to estimate the features for the original signal. Experimental results show that the proposed method can significantly improve the quality of the enhanced signals with additional bitrate of 0.6 kbps for two of the implementations of the high-efficiency advanced audio coding (HE-AAC) v1",
    "keywords": [],
    "checked": true,
    "id": "c2cf00ccf3a41e8ab8c7d0880aec6fd19f90776a",
    "semantic_title": "coded speech enhancement using neural network-based vector-quantized residual features",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/drude21_interspeech.html": {
    "title": "Multi-Channel Opus Compression for Far-Field Automatic Speech Recognition with a Fixed Bitrate Budget",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) in the cloud allows the use of larger models and more powerful multi-channel signal processing front-ends compared to on-device processing. However, it also adds an inherent latency due to the transmission of the audio signal, especially when transmitting multiple channels of a microphone array. One way to reduce the network bandwidth requirements is client-side compression with a lossy codec such as Opus. However, this compression can have a detrimental effect especially on multi-channel ASR front-ends, due to the distortion and loss of spatial information introduced by the codec. In this publication, we propose an improved approach for the compression of microphone array signals based on Opus, using a modified joint channel coding approach and additionally introducing a multi-channel spatial decorrelating transform to reduce redundancy in the transmission. We illustrate the effect of the proposed approach on the spatial information retained in multi-channel signals after compression, and evaluate the performance on far-field ASR with a multi-channel beamforming front-end. We demonstrate that our approach can lead to a 37.5% bitrate reduction or a 5.1% relative word error rate (WER) reduction for a fixed bitrate budget in a seven channel setup",
    "keywords": [],
    "checked": true,
    "id": "cbc5de75438699dc8c83cc32c0d9d1837de7db12",
    "semantic_title": "multi-channel opus compression for far-field automatic speech recognition with a fixed bitrate budget",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/siegert21_interspeech.html": {
    "title": "Effects of Prosodic Variations on Accidental Triggers of a Commercial Voice Assistant",
    "volume": "main",
    "abstract": "The use of modern voice assistants has rapidly grown and they can be found in more and more households. By design, these systems have to scan every sound in their surroundings waiting for their respective wake-word before being able to react to the users' commands. The drawback of this method is that phonetic similar expressions can activate the voice assistant and thus speech utterances or whole private conversations will be recorded and streamed to the cloud back-end for further processing. Many news articles and scientific work reported on inaccurate wake-word detection. Resulting in at least a user's confusion or at worst security breaches. The current paper is based on a broader analysis of phonetic similar accidental triggers conducted by Schönherr et al., they presented a systematic analysis to detect accidental triggers, using a pronouncing dictionary and a weighted, phone-based Levenshtein distance. In this work, the previously identified accidental triggers are recorded by several speakers under various conditions to investigate the influence of phonetic variances (i.e. intonation and speaking/articulation rate) on the robustness of accidental triggers in a real-world environment",
    "keywords": [],
    "checked": true,
    "id": "1a2981261f844ee97ac059787efb4188f812fead",
    "semantic_title": "effects of prosodic variations on accidental triggers of a commercial voice assistant",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gabrys21_interspeech.html": {
    "title": "Improving the Expressiveness of Neural Vocoding with Non-Affine Normalizing Flows",
    "volume": "main",
    "abstract": "This paper proposes a general enhancement to the Normalizing Flows (NF) used in neural vocoding. As a case study, we improve expressive speech vocoding with a revamped Parallel Wavenet (PW). Specifically, we propose to extend the affine transformation of PW to the more expressive invertible non-affine function. The greater expressiveness of the improved PW leads to better-perceived signal quality and naturalness in the waveform reconstruction and text-to-speech (TTS) tasks. We evaluate the model across different speaking styles on a multi-speaker, multi-lingual dataset. In the waveform reconstruction task, the proposed model closes the naturalness and signal quality gap from the original PW to recordings by 10%, and from other state-of-the-art neural vocoding systems by more than 60%. We also demonstrate improvements in objective metrics on the evaluation test set with L2 Spectral Distance and Cross-Entropy reduced by 3% and 6‰ comparing to the affine PW. Furthermore, we extend the probability density distillation procedure proposed by the original PW paper, so that it works with any non-affine invertible and differentiable function",
    "keywords": [],
    "checked": true,
    "id": "54aa09e4b56e1c4c4f8370ae9649e33c2dcf5975",
    "semantic_title": "improving the expressiveness of neural vocoding with non-affine normalizing flows",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/prajapati21_interspeech.html": {
    "title": "Voice Privacy Through x-Vector and CycleGAN-Based Anonymization",
    "volume": "main",
    "abstract": "With the rise in usage of voice assistants and spoken language interfaces, important concerns regarding voice data privacy have been prompted. In an attempt to reduce the threat of attacks on voice data, in this paper, we propose a speaker anonymization system based on CycleGAN. This method modifies the speaker's gender and accent information from the original speech signal. The proposed method gives a more natural-sounding anonymized voice in addition to a de-identified speaker. We have chosen baseline-1 of The Voice Privacy Challenge-2020 as our baseline system. Training of CycleGAN, ASR, and ASV experiments are performed on the subset of Librispeech corpus. In this paper, the double anonymization technique is also explored in which the CycleGAN-based anonymization technique is adopted on top of the baseline system. Experimental results show that combining the proposed method with the x-vector and neural source-filter (NSF) model-based method (baseline system) gives up to 5.61% relative improvement in EER of original-anonymized, enroll-trial pairs. However, it gives up to 19.30% relative improvement in EER for anonymized-anonymized enroll-trial pairs. We observed that along with the good speaker de-identification, the anonymized utterances have adequate speech intelligibility and naturalness",
    "keywords": [],
    "checked": true,
    "id": "d7724d319a15a916c2402ad5ee4f62795e4fbd2f",
    "semantic_title": "voice privacy through x-vector and cyclegan-based anonymization",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21d_interspeech.html": {
    "title": "A Two-Stage Approach to Speech Bandwidth Extension",
    "volume": "main",
    "abstract": "Algorithms for speech bandwidth extension (BWE) may work in either the time domain or the frequency domain. Time-domain methods often do not sufficiently recover the high-frequency content of speech signals; frequency-domain methods are better at recovering the spectral envelope, but have difficulty reconstructing the details of the waveform. In this paper, we propose a two-stage approach for BWE, which enjoys the advantages of both time- and frequency-domain methods. The first stage is a frequency-domain neural network, which predicts the high-frequency part of the wide-band spectrogram from the narrow-band input spectrogram. The wide-band spectrogram is then converted into a time-domain waveform, and passed through the second stage to refine the temporal details. For the first stage, we compare a convolutional recurrent network (CRN) with a temporal convolutional network (TCN), and find that the latter is able to capture long-span dependencies equally well as the former while using a lot fewer parameters. For the second stage, we enhance the Wave-U-Net architecture with a multi-resolution short-time Fourier transform (MSTFT) loss function. A series of comprehensive experiments show that the proposed system achieves superior performance in speech enhancement (measured by both time- and frequency-domain metrics) as well as speech recognition",
    "keywords": [],
    "checked": true,
    "id": "4d4bb42571490d8848884e51d8cac22c7fcd09b6",
    "semantic_title": "a two-stage approach to speech bandwidth extension",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/byun21_interspeech.html": {
    "title": "Development of a Psychoacoustic Loss Function for the Deep Neural Network (DNN)-Based Speech Coder",
    "volume": "main",
    "abstract": "This paper presents a loss function to compensate for the perceptual loss of the deep neural network (DNN)-based speech coder. By utilizing the psychoacoustic model (PAM), we design a loss function to maximize the mask-to-noise ratio (MNR) in multi-resolution Mel-frequency scales. Also, a perceptual entropy (PE)-based weighting scheme is incorporated onto the MNR loss so that the DNN model focuses more on perceptually important Mel-frequency bands. The proposed loss function was tested on a CNN-based autoencoder implementing the softmax quantization and entropy-based bitrate control. Objective and subjective tests conducted with speech signals showed that the proposed loss function produced higher perceptual quality than the previous perceptual loss functions",
    "keywords": [],
    "checked": true,
    "id": "5bded37c81c0b9be864c390c74e1637f748dab33",
    "semantic_title": "development of a psychoacoustic loss function for the deep neural network (dnn)-based speech coder",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stoidis21_interspeech.html": {
    "title": "Protecting Gender and Identity with Disentangled Speech Representations",
    "volume": "main",
    "abstract": "Besides its linguistic content, our speech is rich in biometric information that can be inferred by classifiers. Learning privacy-preserving representations for speech signals enables downstream tasks without sharing unnecessary, private information about an individual. In this paper, we show that protecting gender information in speech is more effective than modelling speaker-identity information only when generating a non-sensitive representation of speech. Our method relies on reconstructing speech by decoding linguistic content along with gender information using a variational autoencoder. Specifically, we exploit disentangled representation learning to encode information about different attributes into separate subspaces that can be factorised independently. We present a novel way to encode gender information and disentangle two sensitive biometric identifiers, namely gender and identity, in a privacy-protecting setting. Experiments on the LibriSpeech dataset show that gender recognition and speaker verification can be reduced to a random guess, protecting against classification-based attacks",
    "keywords": [],
    "checked": true,
    "id": "018202ad19f53b5dafee7797b057a142df5f429c",
    "semantic_title": "protecting gender and identity with disentangled speech representations",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/aldholmi21_interspeech.html": {
    "title": "Perception of Standard Arabic Synthetic Speech Rate",
    "volume": "main",
    "abstract": "This experiment investigated how Arabic speakers perceive synthetic Standard Arabic speech rate produced by Google TTS, at normal vs. accelerated rates. Twenty syntactically identical Standard Arabic sentences with a similar length ( = 22 syllables per sentence, = 1) were auditorily presented in a female voice to thirty female participants who were instructed to rate the tempo of the normal ( ≈ 4.5 syllable per second) and accelerated (by 10%, 20%, and 30%) stimuli on a 1–7 Likert scale (1= extremely slow, 4= normal, 7= extremely fast). The results show that differences in the four-condition synthetic speech rates were reflected in the ratings provided by the participants: the more the speech was accelerated, the higher rating it received. More importantly, the findings support the observation that the current normal speech rate of Google TTS synthetic speech is not perceived as normal by Arabic speakers, but rather is perceived as slow. This may negatively affect the likelihood that users are comfortable using this technology. Hence, the outcome of this study does not only call for further investigation into Standard Arabic synthetic speech rates, but also reveals the need to define a baseline for a natural speech rate in Arabic",
    "keywords": [],
    "checked": true,
    "id": "84760c81c9a26f7e83a50f26f5ff53dbbea8d4eb",
    "semantic_title": "perception of standard arabic synthetic speech rate",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kishiyama21_interspeech.html": {
    "title": "The Influence of Parallel Processing on Illusory Vowels",
    "volume": "main",
    "abstract": "Research has shown that listeners perceive illusory vowels inside consonant clusters that are not allowed in their L1. This phenomenon has been examined using several psycholinguistic and computational models, including hidden Markov models (HMMs), applied to human phoneme perception. However, the inference algorithm of HMMs assumes that parallel processing, which has not been proven to have psychological reality, is a valid cognitive process. This study tested the psychological reality of parallel processing by attempting to duplicate two results from previous studies: First, listeners perceive an illusory vowel in consonant clusters that are not permissible in their L1. Second, the illusory vowel is based on the characteristics of the preceding consonant, indicating that listeners integrate phonotactics and acoustic information. The experiment manipulated the number of candidates that the model can refer to, and the algorithm can be considered parallel when it allows models to use more than two candidates that are stored in memory. In addition, the transition probabilities between consonants were manipulated to represent the different phonotactics. The results showed that only the parallel processing condition reproduced the two observations above, supporting the psychological reality of parallel processing",
    "keywords": [],
    "checked": true,
    "id": "5b87b109743cdb0b63678cbdcc3f5cb5a2717a69",
    "semantic_title": "the influence of parallel processing on illusory vowels",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chingacham21_interspeech.html": {
    "title": "Exploring the Potential of Lexical Paraphrases for Mitigating Noise-Induced Comprehension Errors",
    "volume": "main",
    "abstract": "Listening in noisy environments can be difficult even for individuals with a normal hearing thresholds. The speech signal can be masked by noise, which may lead to word misperceptions on the side of the listener, and overall difficulty to understand the message. To mitigate hearing difficulties on listeners, a co-operative speaker utilizes voice modulation strategies like Lombard speech to generate noise-robust utterances, and similar solutions have been developed for speech synthesis systems. In this work, we propose an alternate solution of choosing noise-robust lexical paraphrases to represent an intended meaning. Our results show that lexical paraphrases differ in their intelligibility in noise. We evaluate the intelligibility of synonyms in context and find that choosing a lexical unit that is less risky to be misheard than its synonym introduced an average gain in comprehension of 37% at SNR -5 dB and 21% at SNR 0 dB for babble noise",
    "keywords": [],
    "checked": true,
    "id": "1b25a3726a6215485be96787819ea38b958ebf7a",
    "semantic_title": "exploring the potential of lexical paraphrases for mitigating noise-induced comprehension errors",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/simantiraki21_interspeech.html": {
    "title": "SpeechAdjuster: A Tool for Investigating Listener Preferences and Speech Intelligibility",
    "volume": "main",
    "abstract": "Most of what we know about speech perception has been gleaned from tests in which listeners respond to stimuli chosen by an experimenter. This paper presents SpeechAdjuster, an open source tool that reverses the roles of listener and experimenter by allowing listeners direct control of speech characteristics in real-time. This change of paradigm enables listener preferences — reflecting factors such as cognitive effort, naturalness or distortion — to be measured directly, without recourse to rating scales. Incorporation of a test phase in which listener preferences are frozen also enables intelligibility to be estimated within the same trial. Offline computation and smooth online interpolation within the tool permits the impact of changes in practically any target speech feature (e.g. fundamental frequency or spectral slope) or background characteristic (e.g. noise spectrum), regardless of complexity, to be measured. The paper describes the tool's capabilities, presents a range of visualisations, and notes some potential applications and limitations",
    "keywords": [],
    "checked": true,
    "id": "70403365dc60964a8ecf7c6274bb586e625a9ced",
    "semantic_title": "speechadjuster: a tool for investigating listener preferences and speech intelligibility",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saito21_interspeech.html": {
    "title": "VocalTurk: Exploring Feasibility of Crowdsourced Speaker Identification",
    "volume": "main",
    "abstract": "This paper presents VocalTurk, a feasibility study of crowdsourced speaker identification based on our worker dataset collected in Amazon Mechanical Turk. Crowdsourced data labeling has already been acknowledged in speech data processing nowadays, but empirical analysis that answer to common questions such as and still remain underexplored, which would limit the quality and scale of the dataset collection. Focusing on the speaker identification task in particular, we thus conducted two studies in Amazon Mechanical Turk: i) hired 3,800+ unique workers to test their performances and confidences in giving answers to voice pair comparison tasks, and ii) additionally assigned more-difficult tasks of voice set comparisons to 350+ top-scoring workers to test their accuracy-speed performances across patterns of N = 1, 3, 5. The results revealed some positive findings that would motivate speech researchers toward crowdsourced data labeling, such as that the top-scoring workers were capable of giving labels to our voice comparison pairs with 99% accuracy after majority voting, as well as they were even capable of batch-labeling which significantly shortened up to 34% of their completion time but still with no statistically-significant degradation in accuracy",
    "keywords": [],
    "checked": true,
    "id": "9eaa86e72341fae70054f96743f1c60c7a1e2128",
    "semantic_title": "vocalturk: exploring feasibility of crowdsourced speaker identification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21e_interspeech.html": {
    "title": "Effects of Aging and Age-Related Hearing Loss on Talker Discrimination",
    "volume": "main",
    "abstract": "Paralinguistic information is as important as linguistic information. Being familiar with talker's voice may facilitate speech perception, especially in challenging conditions. Previous studies have suggested that aging and age-related hearing loss lead to the deterioration of the phonetic and phonological processing ability. The current study aims to explore whether these two factors exert effects on the talker' voice discrimination. Three groups of participants, including young adults (YA) and older adults (OA) with and without hearing loss, were tested on talker discrimination in four types of stimuli varying in language familiarity: Mandarin real words, pseudowords, Arabic words and reversed Mandarin words. The results showed that OA with and without hearing loss performed worse than YA in both nonnative and native conditions. OA with hearing loss further performed worse than OA with normal hearing in Mandarin real word condition. These findings indicated that aging and hearing loss affected both low-level phonetic and high-level phonological processing, but hearing loss had extra effect on phonological processing. Altogether, these results implied that OA could not utilize phonetic and phonological cues as effectively as YA, and OA with hearing loss encountered more difficulties in utilizing phonological cues in talker discrimination",
    "keywords": [],
    "checked": true,
    "id": "cdcf706f4f2c0415fd76c82484c6576fdd18d8df",
    "semantic_title": "effects of aging and age-related hearing loss on talker discrimination",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21l_interspeech.html": {
    "title": "Relationships Between Perceptual Distinctiveness, Articulatory Complexity and Functional Load in Speech Communication",
    "volume": "main",
    "abstract": "Work on communicative efficiency has hypothesized that phonological contrasts signaling more meaning distinctions (i.e., of high functional load (FL)) tend to have the least articulatory complexity and the highest perceptual salience. However, only a few studies have examined the preference for perceptual distinctiveness based on the traditional measures of FL (e.g., the number of minimal pairs, the change in entropy of the lexicon), which are weak in modeling contexts of individual words. And little attention has been devoted to investigating the need to minimize effort. This study explores whether and how the communicative pressures to minimize the likelihood of confusion and minimize articulatory effort influence phonemic contrasts' functional contributions to speech communication. We used a revised definition of FL capable of modeling contextual information (i.e., the change in mutual information between phoneme sequences and spoken texts after the contrast in question is neutralized) and quantified information contributions of phonemic contrasts in English. The results indicated that FL of each phoneme pair increased significantly with its perceptual distinctiveness, and decreased significantly with articulatory complexity of the phoneme requiring less articulatory effort in the contrast. Altogether, these findings suggest that communicative pressures modulate the work a phonemic contrast does in distinguishing words",
    "keywords": [],
    "checked": true,
    "id": "77f9fa6eb796b3f6ee6cfb95071b4d312d72021c",
    "semantic_title": "relationships between perceptual distinctiveness, articulatory complexity and functional load in speech communication",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/terblanche21_interspeech.html": {
    "title": "Human Spoofing Detection Performance on Degraded Speech",
    "volume": "main",
    "abstract": "Over the past few years attention has been focused on the automatic detection of spoofing in the context of automatic speaker verification (ASV) systems. However, little is known about how well humans perform at detecting spoofed speech, particularly under degraded conditions. Using the latest synthesis technologies from ASVspoof 2019, this paper explores human judgements of speech authenticity by considering three common channel degradations — a GSM network, a VoIP network, and background noise — in conjunction with varying synthesis quality. The results reveal that channel degradation reduces the size of the perceptual difference between genuine and spoofed speech, and overall participants correctly identified human and spoofed speech only 56% of the time. In background noise and GSM transmission, lower-quality synthetic speech was judged as more human, and in VoIP transmission all speech, including genuine recordings, was judged as less human. Under all conditions, state-of-the-art synthetic speech was judged as human, or more human than, genuine recorded speech. The paper also considers the listener factors which may contribute to an individual's spoofing detection performance, and finds that a listener's familiarity with the accents involved, their age, and the audio equipment used for playback, have an effect on their spoofing detection performance",
    "keywords": [],
    "checked": true,
    "id": "76c5c04f47ef2c09636b9599d6c19f4db722113b",
    "semantic_title": "human spoofing detection performance on degraded speech",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/einfeldt21_interspeech.html": {
    "title": "Reliable Estimates of Interpretable Cue Effects with Active Learning in Psycholinguistic Research",
    "volume": "main",
    "abstract": "Studying the relative weighting of different cues for the interpretation of a linguistic phenomenon is a core element in psycholinguistic research. This research needs to strike a balance between two things: generalisability to diverse lexical settings, which requires a high number of different lexicalisations and the investigation of a large number of different cues, which requires a high number of different test conditions. Optimizing both is impossible with classical psycholinguistic designs as this would leave the participants with too many experimental trials. Previously we showed that Active Learning (AL) systems allow to test numerous conditions (eight) and items (32) within the same experiment. As stimulus selection was informed by the system's learning mechanism, AL sped-up the labelling process. In the present study, we extend the use case to an experiment with 16 conditions, manipulated through four binary factors (the experimental setting and three prosodic cues; two levels each). Our findings show that the AL system correctly predicted the intended result pattern after twelve trials only. Hence, AL further confirmed previous findings and proved to be an efficient tool, which offers a promising solution to complex study designs in psycholinguistic research",
    "keywords": [],
    "checked": true,
    "id": "5e69a7964542f1e4cf72428aa6ad00ff837fa03c",
    "semantic_title": "reliable estimates of interpretable cue effects with active learning in psycholinguistic research",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21d_interspeech.html": {
    "title": "Towards the Explainability of Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this paper, a multimodal speech emotion recognition system has been developed, and a novel technique to explain its predictions has been proposed. The audio and textual features are extracted separately using attention-based Gated Recurrent Unit (GRU) and pre-trained Bidirectional Encoder Representations from Transformers (BERT), respectively. Then they are concatenated and used to predict the final emotion class. The weighted and unweighted emotion recognition accuracy of 71.7% and 75.0% has been achieved on Emotional Dyadic Motion Capture (IEMOCAP) dataset containing speech utterances and corresponding text transcripts. The training and predictions of network layers have been analyzed qualitatively through emotion embedding plots and quantitatively by analyzing the intersection matrices for various emotion classes' embeddings",
    "keywords": [],
    "checked": true,
    "id": "a48f858401e5dc8572e093a6088e537e108c4b78",
    "semantic_title": "towards the explainability of multimodal speech emotion recognition",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zeng21_interspeech.html": {
    "title": "Primacy of Mouth over Eyes: Eye Movement Evidence from Audiovisual Mandarin Lexical Tones and Vowels",
    "volume": "main",
    "abstract": "This study investigated Chinese speakers' eye movements when they were asked to identify audiovisual Mandarin lexical tones and vowels. In the lexical tone identification task, Chinese speakers were presented with an audiovisual clip of Mandarin monosyllables (/ă/, /à/, /ĭ/, /ì/) and asked to identify whether the syllables were presented in a dipping (/ă/, /ĭ/) or falling tone (/à/, /ì/). In the vowel identification task, they were asked to identify whether the vowels were /a/ or /i/ regardless of lexical tone. These audiovisual syllables were presented in clear, noisy, and silent conditions. An eye-tracker recorded the participants' eye movements Results showed participants gazed more at the mouth than the eyes in both lexical tones and vowels. Additionally, when acoustic conditions degraded from clear to noisy and eventually silent, Chinese speakers increased their gaze towards the mouth rather than the eyes. These findings suggest the mouth to be the primary area that is utilised during audiovisual speech perception. The similar patterns of eye movements between vowels and lexical tones indicate that the mouth acts as a perceptual cue that provides articulatory information",
    "keywords": [],
    "checked": true,
    "id": "7f37de92224b07a1fa5384b60afd9ccdd6b113c9",
    "semantic_title": "primacy of mouth over eyes: eye movement evidence from audiovisual mandarin lexical tones and vowels",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ashihara21_interspeech.html": {
    "title": "Investigating the Impact of Spectral and Temporal Degradation on End-to-End Automatic Speech Recognition Performance",
    "volume": "main",
    "abstract": "Humans have a sophisticated capability to robustly handle incomplete sensory input, as often happens in real environments. In earlier studies, the robustness of human speech perception was observed qualitatively by spectrally and temporally degraded stimuli. The current study investigates how machine speech recognition, especially end-to-end automatic speech recognition (E2E-ASR), can yield similar robustness against distorted acoustic cues. To evaluate the performance of E2E-ASR, we employ four types of distorted speech based on previous studies: locally time-reversed speech, noise-vocoded speech, phonemic restoration, and modulation-filtered speech. Those stimuli are synthesized by spectral and/or temporal manipulation from original speech samples whose human speech intelligibility scores have been well-reported. An experiment was conducted on the TED-LIUM2 for English and the Corpus of Spontaneous Japanese (CSJ) for Japanese. We found that while there is a tendency to exhibit similar robustness in some experiments, full recovery from the harmful effect of the severe spectral degradation is not achieved",
    "keywords": [],
    "checked": true,
    "id": "4e52517eae7e132595fe8d96be25d8fc33bf6a81",
    "semantic_title": "investigating the impact of spectral and temporal degradation on end-to-end automatic speech recognition performance",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21c_interspeech.html": {
    "title": "Super-Human Performance in Online Low-Latency Recognition of Conversational Speech",
    "volume": "main",
    "abstract": "Achieving super-human performance in recognizing human speech has been a goal for several decades as researchers have worked on increasingly challenging tasks. In the 1990's it was discovered, that conversational speech between two humans turns out to be considerably more difficult than read speech as hesitations, disfluencies, false starts and sloppy articulation complicate acoustic processing and require robust joint handling of acoustic, lexical and language context. Early attempts with statistical models could only reach word error rates (WER) of over 50% which is far from human performance with shows a WER of around 5.5%. Neural hybrid models and recent attention-based encoder-decoder models have considerably improved performance as such contexts can now be learned in an integral fashion. However, processing such contexts requires an entire utterance presentation and thus introduces unwanted delays before a recognition result can be output. In this paper, we address performance latency. We present results for a system that can achieve super-human performance, i.e. a WER of 5.0% on the Switchboard conversational benchmark, at a word based latency of only 1 second behind a speaker's speech. The system uses multiple attention-based encoder-decoder networks integrated within a novel low latency incremental inference approach",
    "keywords": [],
    "checked": true,
    "id": "1bb2f240870803d9a0fc6d26f0e8e793a609f88e",
    "semantic_title": "super-human performance in online low-latency recognition of conversational speech",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/joshi21_interspeech.html": {
    "title": "Multiple Softmax Architecture for Streaming Multilingual End-to-End ASR Systems",
    "volume": "main",
    "abstract": "Improving multilingual end-to-end (E2E) automatic speech recognition (ASR) systems have manifold advantages. They simplify the training strategy, are easier to scale and exhibit better performance over monolingual models. However, it is still challenging to use a single multilingual model to recognize multiple languages without knowing the input language, as most multilingual models assume the availability of the input language. In this paper, we introduce multi-softmax model to improve the multilingual recurrent neural network transducer (RNN-T) models, by having language specific softmax, joint and embedding layers, while sharing rest of the parameters. We extend the multi-softmax model to work without knowing the input language, by integrating a language identification (LID) model, that estimates the LID on-the-fly and also does the recognition at the same time. The multi-softmax model outperforms monolingual models with an average word error rate relative (WERR) reduction of 4.65% on Indian languages. Finetuning further improves the WERR reduction to 12.2%. The multi-softmax model with on-the-fly LID estimation, shows WERR reduction of 13.86% compared to the multilingual baseline",
    "keywords": [],
    "checked": true,
    "id": "95a2bbe105558b5f2056f862e153090fb6239a62",
    "semantic_title": "multiple softmax architecture for streaming multilingual end-to-end asr systems",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/le21_interspeech.html": {
    "title": "Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion",
    "volume": "main",
    "abstract": "How to leverage dynamic contextual information in end-to-end speech recognition has remained an active research area. Previous solutions to this problem were either designed for specialized use cases that did not generalize well to open-domain scenarios, did not scale to large biasing lists, or underperformed on rare long-tail words. We address these limitations by proposing a novel solution that combines shallow fusion, trie-based deep biasing, and neural network language model contextualization. These techniques result in significant 19.5% relative Word Error Rate improvement over existing contextual biasing approaches and 5.4%–9.3% improvement compared to a strong hybrid baseline on both open-domain and constrained contextualization tasks, where the targets consist of mostly rare long-tail words. Our final system remains lightweight and modular, allowing for quick modification without model re-training",
    "keywords": [],
    "checked": true,
    "id": "30f25b585b58c3ffbb53029ed2f546c0f201acfe",
    "semantic_title": "contextualized streaming end-to-end speech recognition with trie-based deep biasing and shallow fusion",
    "citation_count": 59,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sainath21_interspeech.html": {
    "title": "An Efficient Streaming Non-Recurrent On-Device End-to-End Model with Improvements to Rare-Word Modeling",
    "volume": "main",
    "abstract": "On-device end-to-end (E2E) models have shown improvements over a conventional model on Search test sets in both quality, as measured by Word Error Rate (WER) [1], and latency [2], measured by the time the result is finalized after the user stops speaking. However, the E2E model is trained on a small fraction of audio-text pairs compared to the 100 billion text utterances that a conventional language model (LM) is trained with. Thus E2E models perform poorly on rare words and phrases. In this paper, building upon the two-pass streaming Cascaded Encoder E2E model [3], we explore using a Hybrid Autoregressive Transducer (HAT) [4] factorization to better integrate an on-device neural LM trained on text-only data. Furthermore, to further improve decoder latency we introduce a non-recurrent embedding decoder, in place of the typical LSTM decoder, into the Cascaded Encoder model. Overall, we present a streaming on-device model that incorporates an external neural LM and outperforms the conventional model in both search and rare-word quality, as well as latency, and is 318× smaller",
    "keywords": [],
    "checked": true,
    "id": "0fc229cfb6f79a6c5778b80ba78dd6584f921ab9",
    "semantic_title": "an efficient streaming non-recurrent on-device end-to-end model with improvements to rare-word modeling",
    "citation_count": 46,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21_interspeech.html": {
    "title": "Streaming Multi-Talker Speech Recognition with Joint Speaker Identification",
    "volume": "main",
    "abstract": "In multi-talker scenarios such as meetings and conversations, speech processing systems are usually required to transcribe the audio as well as identify the speakers for downstream applications. Since overlapped speech is common in this case, conventional approaches usually address this problem in a cascaded fashion that involves speech separation, speech recognition and speaker identification that are trained independently. In this paper, we propose Streaming Unmixing, Recognition and Identification Transducer (SURIT) — a new framework that deals with this problem in an end-to-end streaming fashion. SURIT employs the recurrent neural network transducer (RNN-T) as the backbone for both speech recognition and speaker identification. We validate our idea on the LibrispeechMix dataset — a multi-talker dataset derived from Librispeech, and present encouraging results",
    "keywords": [],
    "checked": true,
    "id": "0ba116b21dd982e2d3d0f4c6f56bd5d3cdeb0072",
    "semantic_title": "streaming multi-talker speech recognition with joint speaker identification",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/moriya21_interspeech.html": {
    "title": "Streaming End-to-End Speech Recognition for Hybrid RNN-T/Attention Architecture",
    "volume": "main",
    "abstract": "We present a novel architecture with its decoding approach for improving recurrent neural network-transducer (RNN-T) performance. RNN-T is promising for building time-synchronous automatic speech recognition (ASR) systems and thus enhancing streaming ASR applications. We note that encoder-decoder-based sequence-to-sequence models (S2S) have been also used successfully by the ASR community. In this paper, we integrate these popular models in the RNN-T+S2S approach; higher recognition performance than either is achieved due to their integration. However, it is generally deemed to be complicated to use S2S in streaming systems, because the attention mechanism can use arbitrarily long past and future contexts during decoding. Our RNN-T+S2S is composed of the shared encoder, an RNN-T decoder and a triggered attention-based decoder which uses time restricted encoder outputs for attention weight computation. By using the trigger points generated from RNN-T outputs, the S2S branch of RNN-T+S2S activates only when the triggers are detected, which makes streaming ASR practical. Experiments on public and private datasets created to research various tasks demonstrate that our proposal can yield superior recognition performance",
    "keywords": [],
    "checked": true,
    "id": "4db2063a3bd0ae621a1b8d177d098e84c0a45844",
    "semantic_title": "streaming end-to-end speech recognition for hybrid rnn-t/attention architecture",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schwarz21_interspeech.html": {
    "title": "Improving RNN-T ASR Accuracy Using Context Audio",
    "volume": "main",
    "abstract": "We present a training scheme for streaming automatic speech recognition (ASR) based on recurrent neural network transducers (RNN-T) which allows the encoder network to learn to exploit context audio from a stream, using segmented or partially labeled sequences of the stream during training. We show that the use of context audio during training and inference can lead to word error rate reductions of more than 6% in a realistic production setting for a voice assistant ASR system. We investigate the effect of the proposed training approach on acoustically challenging data containing background speech and present data points which indicate that this approach helps the network learn both speaker and environment adaptation. To gain further insight into the ability of a long short-term memory (LSTM) based ASR encoder to exploit long-term context, we also visualize RNN-T loss gradients with respect to the input",
    "keywords": [],
    "checked": true,
    "id": "89af788d650810ce769cfa6a75dd4adc0f15c344",
    "semantic_title": "improving rnn-t asr accuracy using context audio",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21e_interspeech.html": {
    "title": "HMM-Free Encoder Pre-Training for Streaming RNN Transducer",
    "volume": "main",
    "abstract": "This work describes an encoder pre-training procedure using frame-wise label to improve the training of streaming recurrent neural network transducer (RNN-T) model. Streaming RNN-T trained from scratch usually performs worse than non-streaming RNN-T. Although it is common to address this issue through pre-training components of RNN-T with other criteria or frame-wise alignment guidance, the alignment is not easily available in end-to-end manner. In this work, frame-wise alignment, used to pre-train streaming RNN-T's encoder, is generated without using a HMM-based system. Therefore an all-neural framework equipping HMM-free encoder pre-training is constructed. This is achieved by expanding the spikes of CTC model to their left/right blank frames, and two expanding strategies are proposed. To our best knowledge, this is the first work to simulate HMM-based frame-wise label using CTC model for pre-training. Experiments conducted on LibriSpeech and MLS English tasks show the proposed pre-training procedure, compared with random initialization, reduces the WER by relatively 5%~11% and the emission latency by 60 ms. Besides, the method is lexicon-free, so it is friendly to new languages without manually designed lexicon",
    "keywords": [],
    "checked": true,
    "id": "57c9f31ce00bc9947da6d230ecc98dd25a6fef6c",
    "semantic_title": "hmm-free encoder pre-training for streaming rnn transducer",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cui21b_interspeech.html": {
    "title": "Reducing Exposure Bias in Training Recurrent Neural Network Transducers",
    "volume": "main",
    "abstract": "When recurrent neural network transducers (RNNTs) are trained using the typical maximum likelihood criterion, the prediction network is trained only on ground truth label sequences. This leads to a mismatch during inference, known as exposure bias, when the model must deal with label sequences containing errors. In this paper we investigate approaches to reducing exposure bias in training to improve the generalization of RNNT models for automatic speech recognition (ASR). A label-preserving input perturbation to the prediction network is introduced. The input token sequences are perturbed using SwitchOut and scheduled sampling based on an additional token language model. Experiments conducted on the 300-hour Switchboard dataset demonstrate their effectiveness. By reducing the exposure bias, we show that we can further improve the accuracy of a high-performance RNNT ASR model and obtain state-of-the-art results on the 300-hour Switchboard dataset",
    "keywords": [],
    "checked": true,
    "id": "5094860b4096b98c34bc7f32fe1b6c779c02fddb",
    "semantic_title": "reducing exposure bias in training recurrent neural network transducers",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/doutre21_interspeech.html": {
    "title": "Bridging the Gap Between Streaming and Non-Streaming ASR Systems by Distilling Ensembles of CTC and RNN-T Models",
    "volume": "main",
    "abstract": "Streaming end-to-end automatic speech recognition (ASR) systems are widely used in everyday applications that require transcribing speech to text in real-time. Their minimal latency makes them suitable for such tasks. Unlike their non-streaming counterparts, streaming models are constrained to be causal with no future context and suffer from higher word error rates (WER). To improve streaming models, a recent study [1] proposed to distill a non-streaming teacher model on unsupervised utterances, and then train a streaming student using the teachers' predictions. However, the performance gap between teacher and student WERs remains high. In this paper, we aim to close this gap by using a diversified set of non-streaming teacher models and combining them using Recognizer Output Voting Error Reduction (ROVER). In particular, we show that, despite being weaker than RNN-T models, CTC models are remarkable teachers. Further, by fusing RNN-T and CTC models together, we build the strongest teachers. The resulting student models drastically improve upon streaming models of previous work [1]: the WER decreases by 41% on Spanish, 27% on Portuguese, and 13% on French",
    "keywords": [],
    "checked": true,
    "id": "2f69d6b095c6dd51a8a94854887f136a7a68efeb",
    "semantic_title": "bridging the gap between streaming and non-streaming asr systems bydistilling ensembles of ctc and rnn-t models",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/audhkhasi21_interspeech.html": {
    "title": "Mixture Model Attention: Flexible Streaming and Non-Streaming Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Streaming automatic speech recognition (ASR) hypothesizes words as soon as the input audio arrives, whereas non-streaming ASR can potentially wait for the completion of the entire utterance to hypothesize words. Streaming and non-streaming ASR systems have typically used different acoustic encoders. Recent work has attempted to unify them by either jointly training a fixed stack of streaming and non-streaming layers or using knowledge distillation during training to ensure consistency between the streaming and non-streaming predictions. We propose mixture model (MiMo) attention as a simpler and theoretically-motivated alternative that replaces only the attention mechanism, requires no change to the training loss, and allows greater flexibility of switching between streaming and non-streaming mode during inference. Our experiments on the public Librispeech data set and a few Indic language data sets show that MiMo attention endows a single ASR model with the ability to operate in both streaming and non-streaming modes without any overhead and without significant loss in accuracy compared to separately-trained streaming and non-streaming models. We also illustrate this benefit of MiMo attention in a second-pass rescoring setting",
    "keywords": [],
    "checked": true,
    "id": "3933c9336ca4a3935fc2c74d44177239edac2f13",
    "semantic_title": "mixture model attention: flexible streaming and non-streaming automatic speech recognition",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/inaguma21_interspeech.html": {
    "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR",
    "volume": "main",
    "abstract": "While attention-based encoder-decoder (AED) models have been successfully extended to the online variants for streaming automatic speech recognition (ASR), such as monotonic chunkwise attention (MoChA), the models still have a large label emission latency because of the unconstrained end-to-end training objective. Previous works tackled this problem by leveraging alignment information to control the timing to emit tokens during training. In this work, we propose a simple regularization method, , to encourage MoChA to emit tokens earlier. StableEmit discounts the selection probabilities in hard monotonic attention for token boundary detection by a constant factor and regularizes them to recover the total attention mass during training. As a result, the scale of the selection probabilities is increased, and the values can reach a threshold for token emission earlier, leading to a reduction of emission latency and deletion errors. Moreover, StableEmit can be combined with methods that constraint alignments to further improve the accuracy and latency. Experimental evaluations with LSTM and Conformer encoders demonstrate that StableEmit significantly reduces the recognition errors and the emission latency simultaneously. We also show that the use of alignment information is complementary in both metrics",
    "keywords": [],
    "checked": true,
    "id": "a255b4674ecac416eb37d9441e1cda2fed147b0c",
    "semantic_title": "stableemit: selection probability discount for reducing emission latency of streaming monotonic attention asr",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/moritz21_interspeech.html": {
    "title": "Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Attention-based end-to-end automatic speech recognition (ASR) systems have recently demonstrated state-of-the-art results for numerous tasks. However, the application of self-attention and attention-based encoder-decoder models remains challenging for streaming ASR, where each word must be recognized shortly after it was spoken. In this work, we present the dual causal/non-causal self-attention (DCN) architecture, which in contrast to restricted self-attention prevents the overall context to grow beyond the look-ahead of a single layer when used in a deep architecture. DCN is compared to chunk-based and restricted self-attention using streaming transformer and conformer architectures, showing improved ASR performance over restricted self-attention and competitive ASR results compared to chunk-based self-attention, while providing the advantage of frame-synchronous processing. Combined with triggered attention, the proposed streaming end-to-end ASR systems obtained state-of-the-art results on the LibriSpeech, HKUST, and Switchboard ASR tasks",
    "keywords": [],
    "checked": true,
    "id": "5ba5de727130ffcc93f4c26f9d563df0bf5b727c",
    "semantic_title": "dual causal/non-causal self-attention for streaming end-to-end speech recognition",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21d_interspeech.html": {
    "title": "Multi-Mode Transformer Transducer with Stochastic Future Context",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) models make fewer errors when more surrounding speech information is presented as context. Unfortunately, acquiring a larger future context leads to higher latency. There exists an inevitable trade-off between speed and accuracy. Naïvely, to fit different latency requirements, people have to store multiple models and pick the best one under the constraints. Instead, a more desirable approach is to have a single model that can dynamically adjust its latency based on different constraints, which we refer to as A Multi-mode ASR model can fulfill various latency requirements during inference — when a larger latency becomes acceptable, the model can process longer future context to achieve higher accuracy and when a latency budget is not flexible, the model can be less dependent on future context but still achieve reliable accuracy. In pursuit of Multi-mode ASR, we propose , a simple training procedure that samples one streaming configuration in each iteration. Through extensive experiments on AISHELL-1 and LibriSpeech datasets, we show that a Multi-mode ASR model rivals, if not surpasses, a set of competitive streaming baselines trained with different latency budgets",
    "keywords": [],
    "checked": true,
    "id": "2dbe78aa516cc911a71ff333a35a5ce0b1a49640",
    "semantic_title": "multi-mode transformer transducer with stochastic future context",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ren21_interspeech.html": {
    "title": "A Causal U-Net Based Neural Beamforming Network for Real-Time Multi-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "People are meeting through video conferencing more often. While single channel speech enhancement techniques are useful for the individual participants, the speech quality will be significantly degraded in large meeting rooms where the far-field and reverberate conditions are introduced. Approaches based on microphone array signal processing are proposed to explore the inter-channel correlation among the individual microphone channels. In this work, a new causal U-net based multiple-in-multiple-out structure is proposed for real-time multi-channel speech enhancement. The proposed method incorporates the traditional beamforming structure with the multi-channel causal U-net by explicitly adding a beamforming operation at the end of the neural beamformer. The proposed method has entered the INTERSPEECH Far-field Multi-Channel Speech Enhancement Challenge for Video Conferencing. With 1.97M model parameters and 0.25 real-time factor on Intel Core i7 (2.6GHz) CPU, the proposed method has outperforms the baseline system of this challenge on PESQ, Si-SNR and STOI metrics",
    "keywords": [],
    "checked": true,
    "id": "04a18ea30d4fe1999cd8a352ed9ce636da29233c",
    "semantic_title": "a causal u-net based neural beamforming network for real-time multi-channel speech enhancement",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21d_interspeech.html": {
    "title": "A Partitioned-Block Frequency-Domain Adaptive Kalman Filter for Stereophonic Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "The rapid development of online video conferencing systems has caused renewed attention to the multi-channel recording and playback systems. Stereophonic acoustic echo cancellation (SAEC) is the key issue of this systems. This paper proposes an optimally designed partitioned-block frequency-domain Kalman filter (PBFDKF) algorithm for SAEC. We establish the frequency-domain observation equation using the overlap-and-save method and we use the first-order Markov model to describe the state equation. The exact PBFDKF algorithm is derived under the umbrella of Kalman filter theory and two fast implementations are then presented to reduce the complexity. The proposed algorithm is equivalent to the dual-channel partitioned-block frequency-domain gradient-based algorithm with optimum step-size control, and hence it exhibits very good convergence performance and is found to be robust to near-end interference without a double-talk detector. Extensive experiments in different SAEC conditions confirm the effectiveness of the proposed algorithm",
    "keywords": [],
    "checked": true,
    "id": "6adeab80186ffa3236f643050dd8db40a5bd72f1",
    "semantic_title": "a partitioned-block frequency-domain adaptive kalman filter for stereophonic acoustic echo cancellation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21p_interspeech.html": {
    "title": "Real-Time Independent Vector Analysis Using Semi-Supervised Nonnegative Matrix Factorization as a Source Model",
    "volume": "main",
    "abstract": "Online independent vector analysis (IVA) based on auxiliary technology is effective to separate audio source in real time. However, the separated signal may contain residual interference noise because the source model of IVA lacks flexibility and cannot treat the specific harmonic structures of sources. This paper presents a real-time IVA method where the amplitude spectrum of separated signal is modeled by semi-supervised nonnegative matrix factorization (SSNMF). Using the pre-trained basis matrix which contains source structures, we can extract the target source from the separated signal in real time. The advantage of the proposed method is that the extracted source can provide a more accurate variance than the separated signal and hence the proposed method can obtain a better separation performance than the oracle IVA. Experimental results in speech denoising task show the effectiveness and the robustness of the proposed method with different types of noise",
    "keywords": [],
    "checked": true,
    "id": "266b63de7312569f98bb35cf4a6d4cb94507ab8b",
    "semantic_title": "real-time independent vector analysis using semi-supervised nonnegative matrix factorization as a source model",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21b_interspeech.html": {
    "title": "Improving Channel Decorrelation for Multi-Channel Target Speech Extraction",
    "volume": "main",
    "abstract": "Target speech extraction has attracted widespread attention. When microphone arrays are available, the additional spatial information can be helpful in extracting the target speech. We have recently proposed a channel decorrelation (CD) mechanism to extract the inter-channel differential information to enhance the reference channel encoder representation. Although the proposed mechanism has shown promising results for extracting the target speech from mixtures, the extraction performance is still limited by the nature of the original decorrelation theory. In this paper, we propose two methods to broaden the horizon of the original channel decorrelation, by replacing the original softmax-based inter-channel similarity between encoder representations, using an unrolled probability and a normalized cosine-based similarity at the dimensional-level. Moreover, new combination strategies of the CD-based spatial information and target speaker adaptation of parallel encoder outputs are also investigated. Experiments on the reverberant WSJ0 2-mix show that the improved CD can result in more discriminative differential information and the new adaptation strategy is also very effective to improve the target speech extraction",
    "keywords": [],
    "checked": true,
    "id": "b4ed4c9ca182f0e134398cb20283c814c95ec5fa",
    "semantic_title": "improving channel decorrelation for multi-channel target speech extraction",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21f_interspeech.html": {
    "title": "Inplace Gated Convolutional Recurrent Neural Network for Dual-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "For dual-channel speech enhancement, it is a promising idea to design an end-to-end model based on the traditional array signal processing guideline and the manifold space of multi-channel signals. We found that the idea above can be effectively implemented by the classical convolutional recurrent neural networks (CRN) architecture. We propose a very compact inplace gated convolutional recurrent neural network (inplace GCRN) for end-to-end multi-channel speech enhancement, which utilizes inplace-convolution for frequency pattern extraction and reconstruction. The inplace characteristics efficiently preserve spatial cues in each frequency bin for channel-wise long short-term memory neural networks (LSTM) tracing the spatial source. In addition, we come up with a new spectrum recovery method by predict amplitude mask, mapping, and phase, which effectively improves the speech quality",
    "keywords": [],
    "checked": true,
    "id": "0a3a3ce908fdce43c93d70ed3b8d7a1edbbdaa88",
    "semantic_title": "inplace gated convolutional recurrent neural network for dual-channel speech enhancement",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/raj21_interspeech.html": {
    "title": "SRIB-LEAP Submission to Far-Field Multi-Channel Speech Enhancement Challenge for Video Conferencing",
    "volume": "main",
    "abstract": "This paper presents the details of the SRIB-LEAP submission to the ConferencingSpeech challenge 2021. The challenge involved the task of multi-channel speech enhancement to improve the quality of far field speech from microphone arrays in a video conferencing room. We propose a two stage method involving a beamformer followed by single channel enhancement. For the beamformer, we incorporated self-attention mechanism as inter-channel processing layer in the filter-and-sum network (FaSNet), an end-to-end time-domain beamforming system. The single channel speech enhancement is done in log spectral domain using convolution neural network (CNN)-long short term memory (LSTM) based architecture. We achieved improvements in objective quality metrics — perceptual evaluation of speech quality (PESQ) of 0.5 on the noisy data. On subjective quality evaluation, the proposed approach improved the mean opinion score (MOS) by an absolute measure of 0.9 over the noisy audio",
    "keywords": [],
    "checked": true,
    "id": "d902f4f6dbc62f1847b725c66e83c600dca7e5f9",
    "semantic_title": "srib-leap submission to far-field multi-channel speech enhancement challenge for video conferencing",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21_interspeech.html": {
    "title": "Real-Time Multi-Channel Speech Enhancement Based on Neural Network Masking with Attention Model",
    "volume": "main",
    "abstract": "In this paper, we propose a real-time multi-channel speech enhancement method for noise reduction and dereverberation in far-field environments. The proposed method consists of two components: differential beamforming and mask estimation network. The differential beamforming is employed to suppress the interference signals from non-target directions such that a relatively clean speech can be obtained. The mask estimation network with an attention model is developed to capture the signal correlation among different channels in the feature extraction stage and enhance the feature representation that needs to be reconstructed into the target speech in the estimation mask stage. In the inference phase, the spectrum after differential beamforming is filtered by the estimated mask to obtain the final output. The spectrum after differential beamforming can provide a higher signal-to-noise ratio (SNR) than the original spectrum, so the estimated mask can more easily filter out the noise. We conducted experiments on the ConferencingSpeech2021 challenge (INTERSPEECH 2021) dataset to evaluate the proposed method. With only 2.9M parameters, the proposed method achieved competitive performance",
    "keywords": [],
    "checked": true,
    "id": "70522e78ee928cefc6e787c33e05421de1542717",
    "semantic_title": "real-time multi-channel speech enhancement based on neural network masking with attention model",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ganapathy21_interspeech.html": {
    "title": "Uncovering the Acoustic Cues of COVID-19 Infection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": true,
    "id": "84369e9d4978e4cb7ce8123d0e7983de3baa14b7",
    "semantic_title": "uncovering the acoustic cues of covid-19 infection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fung21_interspeech.html": {
    "title": "Ethical and Technological Challenges of Conversational AI",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": true,
    "id": "cc73b78f1433f7522c957dc39371da1d17a36737",
    "semantic_title": "ethical and technological challenges of conversational ai",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fohr21_interspeech.html": {
    "title": "BERT-Based Semantic Model for Rescoring N-Best Speech Recognition List",
    "volume": "main",
    "abstract": "This work aims to improve automatic speech recognition (ASR) by modeling long-term semantic relations. We propose to perform this through rescoring the ASR N-best hypotheses list. To achieve this, we propose two deep neural network (DNN) models and combine semantic, acoustic, and linguistic information. Our DNN rescoring models are aimed at selecting hypotheses that have better semantic consistency and therefore lower WER. We investigate a powerful representation as part of input features to our DNN model: dynamic contextual embeddings from Transformer-based BERT. Acoustic and linguistic features are also included. We perform experiments on the publicly available dataset TED-LIUM. We evaluate in clean and in noisy conditions, with n-gram and Recurrent Neural Network Language Model (RNNLM), more precisely Long Short-Term Memory (LSTM) model. The proposed rescoring approaches give significant WER improvements over the ASR system without rescoring models. Furthermore, the combination of rescoring methods based on BERT and GPT-2 scores achieves the best results",
    "keywords": [],
    "checked": true,
    "id": "469daba80b45c6c4c8345a7ec00bf9c51f46c58e",
    "semantic_title": "dnn-based semantic model for rescoring n-best speech recognition list",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/benes21_interspeech.html": {
    "title": "Text Augmentation for Language Models in High Error Recognition Scenario",
    "volume": "main",
    "abstract": "In this paper, we explore several data augmentation strategies for training of language models for speech recognition. We compare augmentation based on global error statistics with one based on unigram statistics of ASR errors and with label-smoothing and its sampled variant. Additionally, we investigate the stability and the predictive power of perplexity estimated on augmented data. Despite being trivial, augmentation driven by global substitution, deletion and insertion rates achieves the best rescoring results. On the other hand, even though the associated perplexity measure is stable, it gives no better prediction of the final error rate than the vanilla one. Our best augmentation scheme increases the WER improvement from second-pass rescoring from 1.1% to 1.9% absolute on the CHiMe-6 challenge",
    "keywords": [],
    "checked": true,
    "id": "a959b6a45dbb55464abcb94e6fde5f4fef236b5b",
    "semantic_title": "text augmentation for language models in high error recognition scenario",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21b_interspeech.html": {
    "title": "On Sampling-Based Training Criteria for Neural Language Modeling",
    "volume": "main",
    "abstract": "As the vocabulary size of modern word-based language models becomes ever larger, many sampling-based training criteria are proposed and investigated. The essence of these sampling methods is that the softmax-related traversal over the entire vocabulary can be simplified, giving speedups compared to the baseline. A problem we notice about the current landscape of such sampling methods is the lack of a systematic comparison and some myths about preferring one over another. In this work, we consider Monte Carlo sampling, importance sampling, a novel method we call compensated partial summation, and noise contrastive estimation. Linking back to the three traditional criteria, namely mean squared error, binary cross-entropy, and cross-entropy, we derive the theoretical solutions to the training problems. Contrary to some common belief, we show that all these sampling methods can perform equally well, as long as we correct for the intended class posterior probabilities. Experimental results in language modeling and automatic speech recognition on Switchboard and LibriSpeech support our claim, with all sampling-based methods showing similar perplexities and word error rates while giving the expected speedups",
    "keywords": [],
    "checked": true,
    "id": "1672bd443cde01614d06aa700728ebab24f97961",
    "semantic_title": "on sampling-based training criteria for neural language modeling",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pylkkonen21_interspeech.html": {
    "title": "Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network",
    "volume": "main",
    "abstract": "Adaption of end-to-end speech recognition systems to new tasks is known to be challenging. A number of solutions have been proposed which apply external language models with various fusion methods, possibly with a combination of two-pass decoding. Also TTS systems have been used to generate adaptation data for the end-to-end models. In this paper we show that RNN-transducer models can be effectively adapted to new domains using only small amounts of textual data. By taking advantage of model's inherent structure, where the prediction network is interpreted as a language model, we can apply fast adaptation to the model. Adapting the model avoids the need for complicated decoding time fusions and external language models. Using appropriate regularization, the prediction network can be adapted to new domains while still retaining good generalization capabilities. We show with multiple ASR evaluation tasks how this method can provide relative gains of 10–45% in target task WER. We also share insights how RNN-transducer prediction network performs as a language model",
    "keywords": [],
    "checked": true,
    "id": "51090aacfdb4357e5c5bc12cdfef67652ed51a75",
    "semantic_title": "fast text-only domain adaptation of rnn-transducer prediction network",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cieri21_interspeech.html": {
    "title": "Using Games to Augment Corpora for Language Recognition and Confusability",
    "volume": "main",
    "abstract": "We present a Game with a Purpose to elicit judgements of the language spoken in short audio clips of broadcast and conversational telephone speech, the resulting corpus and their potential use in research on language recognition and confusability",
    "keywords": [],
    "checked": true,
    "id": "6ca5f89f87e9e78655ad54339dc1c4e78d92844e",
    "semantic_title": "using games to augment corpora for language recognition and confusability",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fenu21_interspeech.html": {
    "title": "Fair Voice Biometrics: Impact of Demographic Imbalance on Group Fairness in Speaker Recognition",
    "volume": "main",
    "abstract": "Speaker recognition systems are playing a key role in modern online applications. Though the susceptibility of these systems to discrimination according to group fairness metrics has been recently studied, their assessment has been mainly focused on the difference in equal error rate across groups, not accounting for other fairness criteria important in anti-discrimination policies, defined for demographic groups characterized by sensitive attributes. In this paper, we therefore study how existing group fairness metrics relate with the balancing settings of the training data set in speaker recognition. We conduct this analysis by operationalizing several definitions of fairness and monitoring them under varied data balancing settings. Experiments performed on three deep neural architectures, evaluated on a data set including gender/age-based groups, show that balancing group representation positively impacts on fairness and that the friction across security, usability, and fairness depends on the fairness metric and the recognition threshold",
    "keywords": [],
    "checked": true,
    "id": "2af87760df633309b220d29447e2fb08831b7683",
    "semantic_title": "fair voice biometrics: impact of demographic imbalance on group fairness in speaker recognition",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21m_interspeech.html": {
    "title": "Knowledge Distillation from Multi-Modality to Single-Modality for Person Verification",
    "volume": "main",
    "abstract": "Voice and face are two important biometric characteristics that can be used for person identity verification. Previous works have proved the strong complementarity between audio and visual modalities in person verification tasks that multi-modality system can achieve significant performance improvement compared to single-modality system. However, due to the limitations in the real world, it is hard to access both audio and visual data at the same time. In this paper, we investigate several strategies to distill the knowledge from a multi-modality system and transfer it to the single-modality system in a teacher-student mode. We applied the knowledge distillation at three different levels: label level, embedding level, and distribution level. All the experiments are based on the VoxCeleb dataset. The results show that the visual single-modality system achieves 10% EER (equal error rate) improvement on the VoxCeleb1 evaluation set using our proposed knowledge distillation method. Besides, the improvement on the audio system is only reflected on part of the evaluation trials, and we give a detailed analysis for this phenomenon",
    "keywords": [],
    "checked": true,
    "id": "262dea118ab7abb805fa4c58cac9cfb29c2a2439",
    "semantic_title": "knowledge distillation from multi-modality to single-modality for person verification",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/noe21_interspeech.html": {
    "title": "Adversarial Disentanglement of Speaker Representation for Attribute-Driven Privacy Preservation",
    "volume": "main",
    "abstract": "In speech technologies, speaker's voice representation is used in many applications such as speech recognition, voice conversion, speech synthesis and, obviously, user authentication. Modern vocal representations of the speaker are based on neural embeddings. In addition to the targeted information, these representations usually contain sensitive information about the speaker, like the age, sex, physical state, education level or ethnicity. In order to allow the user to choose which information to protect, we introduce in this paper the concept of in speaker voice representation. It allows a person to hide one or more personal aspects to a potential malicious interceptor and to the application provider. As a first solution to this concept, we propose to use an adversarial autoencoding method that disentangles in the voice representation a given speaker attribute thus allowing its concealment. We focus here on the sex attribute for an Automatic Speaker Verification (ASV) task. Experiments carried out using the VoxCeleb datasets have shown that the proposed method enables the concealment of this attribute while preserving ASV ability",
    "keywords": [],
    "checked": true,
    "id": "780bc04c6b2cb0f1b9e6c5c3ac6306eb0ec2d8b9",
    "semantic_title": "adversarial disentanglement of speaker representation for attribute-driven privacy preservation",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/romana21_interspeech.html": {
    "title": "Automatically Detecting Errors and Disfluencies in Read Speech to Predict Cognitive Impairment in People with Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's disease (PD) is a central nervous system disorder that causes motor impairment. Recent studies have found that people with PD also often suffer from cognitive impairment (CI). While a large body of work has shown that speech can be used to predict motor symptom severity in people with PD, much less has focused on cognitive symptom severity. Existing work has investigated if acoustic features, derived from speech, can be used to detect CI in people with PD. However, these acoustic features are general and are not targeted toward capturing CI. Speech errors and disfluencies provide additional insight into CI. In this study, we focus on read speech, which offers a controlled template from which we can detect errors and disfluencies, and we analyze how errors and disfluencies vary with CI. The novelty of this work is an automated pipeline, including transcription and error and disfluency detection, capable of predicting CI in people with PD. This will enable efficient analyses of how cognition modulates speech for people with PD, leading to scalable speech assessments of CI",
    "keywords": [],
    "checked": true,
    "id": "185b4739bca77fd2e22d5efcd7e14b0cbc5d0a63",
    "semantic_title": "automatically detecting errors and disfluencies in read speech to predict cognitive impairment in people with parkinson's disease",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vaysse21_interspeech.html": {
    "title": "Automatic Extraction of Speech Rhythm Descriptors for Speech Intelligibility Assessment in the Context of Head and Neck Cancers",
    "volume": "main",
    "abstract": "The temporal dimension of speech acoustics is rarely taken into account in automatic models for Speech Intelligibility evaluation, although the rhythmic recurrence of phonemes, syllables and prosodic groups are allegedly good predictors of speech intelligibility. The present study aims at unravelling those automatic parameters that best account for the different levels of the speech signal's rhythmic structure, and to evaluate their correlation with a perceptual intelligibility measure. The parameters are extracted from the Fourier Transform of the amplitude modulation of the signal (Envelope Modulation Spectrum) [1, 2]. A Lasso linear model for feature selection is first implemented to select the most relevant parameters, and a SVR regression analysis is run to reveal the best parameters' combination. Our analyses of EMS, using data from the French corpora of cancer speech C2SI [3], show strong performances of the automatic prediction, with a correlation of 0.70 between our model and an intelligibility evaluation score by speech-pathologists. In particular, the highest correlation with speech intelligibility lies in the ratio between the energy in the low frequency band (0.5–4 Hz that represents slow rhythmic modulations indicative of prosodic groups) and in the higher one (4–10 Hz that represents fast rhythmic modulations like phonemes)",
    "keywords": [],
    "checked": true,
    "id": "14e5d0dd998505adf8b997a757a9023f89346e17",
    "semantic_title": "automatic extraction of speech rhythm descriptors for speech intelligibility assessment in the context of head and neck cancers",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qi21b_interspeech.html": {
    "title": "Speech Disorder Classification Using Extended Factorized Hierarchical Variational Auto-Encoders",
    "volume": "main",
    "abstract": "Objective speech disorder classification for speakers with communication difficulty is desirable for diagnosis and administering therapy. With the current state of speech technology, it is evident to propose neural networks for this application. But neural network model training is hampered by a lack of labeled disordered speech data. In this research, we apply an extended version of Factorized Hierarchical Variational Auto-encoders (FHVAE) for representation learning on disordered speech. The FHVAE model extracts both content-related and sequence-related latent variables from speech data, and we utilize the extracted variables to explore how disorder type information is represented in the latent variables. For better classification performance, the latent variables are aggregated at the word and sentence level. We show that an extension of the FHVAE model succeeds in the better disentanglement of the content-related and sequence-related related representations, but both representations are still required for best results on disorder type classification",
    "keywords": [],
    "checked": true,
    "id": "2c3a6791877e60d556f0605a4df0545dfb1d5388",
    "semantic_title": "speech disorder classification using extended factorized hierarchical variational auto-encoders",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mathad21_interspeech.html": {
    "title": "The Impact of Forced-Alignment Errors on Automatic Pronunciation Evaluation",
    "volume": "main",
    "abstract": "Automatic evaluation of phone-level pronunciation scores typically involves two stages: (1) automatic phonetic segmentation via text-constrained phoneme alignment and (2) quantification of acoustic deviation for each phoneme-level relative to a database of correctly-pronounced speech. It's clear that the second stage depends on the first. That is, if there is misalignment, the acoustic deviation will also be impacted. In this paper, we analyzed the impact of alignment error on a measure of goodness of pronunciation. We computed (1) automatic pronunciation scores using force-aligned samples, (2) the forced-alignment error rate, and (3) acoustic deviation using manually-aligned samples. We used a bivariate linear regression model to characterize the contributions of forced alignment errors and acoustic deviation on the automatic pronunciation scores. This was done across two different children speech databases, namely children with cleft lip/palate and typically developing children between the ages of 3–6 years. The analysis shows that, for speech from typically-developing children, most of the variation in the automatic pronunciation scores is explained by acoustic deviation, with the errors in forced alignment playing a relatively minor role. The forced alignment errors have a small but significant downstream impact on pronunciation assessment for children with cleft lip/palate",
    "keywords": [],
    "checked": true,
    "id": "06ad429975f0f9cf5a1b3ef5d33e7db878999db3",
    "semantic_title": "the impact of forced-alignment errors on automatic pronunciation evaluation",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/villatorotello21_interspeech.html": {
    "title": "Late Fusion of the Available Lexicon and Raw Waveform-Based Acoustic Modeling for Depression and Dementia Recognition",
    "volume": "main",
    "abstract": "Mental disorders, e.g. depression and dementia, are categorized as priority conditions according to the World Health Organization (WHO). When diagnosing, psychologists employ structured questionnaires/interviews, and different cognitive tests. Although accurate, there is an increasing necessity of developing digital mental health support technologies to alleviate the burden faced by professionals. In this paper, we propose a multi-modal approach for modeling the communication process employed by patients being part of a clinical interview or a cognitive test. The language-based modality, inspired by the Lexical Availability (LA) theory from psycho-linguistics, identifies the most vocabulary of the interviewed subject and use it as features in a classification process. The acoustic-based modality is processed by a Convolutional Neural Network (CNN) trained on signals of speech that predominantly contained voice source characteristics. In the end, a late fusion technique, based on majority voting, assigns the final classification. Results show the complementarity of both modalities, reaching an overall Macro-F1 of 84% and 90% for Depression and Alzheimer's dementia respectively",
    "keywords": [],
    "checked": true,
    "id": "3648a00c23d3fa9835363fa33b22225813d00f18",
    "semantic_title": "late fusion of the available lexicon and raw waveform-based acoustic modeling for depression and dementia recognition",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shandiz21_interspeech.html": {
    "title": "Neural Speaker Embeddings for Ultrasound-Based Silent Speech Interfaces",
    "volume": "main",
    "abstract": "Articulatory-to-acoustic mapping seeks to reconstruct speech from a recording of the articulatory movements, for example, an ultrasound video. Just like speech signals, these recordings represent not only the linguistic content, but are also highly specific to the actual speaker. Hence, due to the lack of multi-speaker data sets, researchers have so far concentrated on speaker-dependent modeling. Here, we present multi-speaker experiments using the recently published TaL80 corpus. To model speaker characteristics, we adjusted the x-vector framework popular in speech processing to operate with ultrasound tongue videos. Next, we performed speaker recognition experiments using 50 speakers from the corpus. Then, we created speaker embedding vectors and evaluated them on the remaining speakers. Finally, we examined how the embedding vector influences the accuracy of our ultrasound-to-speech conversion network in a multi-speaker scenario. In the experiments we attained speaker recognition error rates below 3%, and we also found that the embedding vectors generalize nicely to unseen speakers. Our first attempt to apply them in a multi-speaker silent speech framework brought about a marginal reduction in the error rate of the spectral estimation step",
    "keywords": [],
    "checked": true,
    "id": "879a91d5c7aa3ca2745e704b264134e50fa0214d",
    "semantic_title": "neural speaker embeddings for ultrasound-based silent speech interfaces",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lamba21_interspeech.html": {
    "title": "Cross-Modal Learning for Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "In this paper, we present a novel approach to the audio-visual video parsing (AVVP) task that demarcates events from a video separately for audio and visual modalities. The proposed parsing approach simultaneously detects the temporal boundaries in terms of start and end times of such events. We show how AVVP can benefit from the following techniques geared towards effective cross-modal learning: (i) adversarial training and skip connections (ii) global context aware attention and, (iii) self-supervised pretraining using an audio-video grounding objective to obtain cross-modal audio-video representations. We present extensive experimental evaluations on the Look, Listen, and Parse (LLP) dataset and show that we outperform the state-of-the-art Hybrid Attention Network (HAN) on all five metrics proposed for AVVP. We also present several ablations to validate the effect of pretraining, global attention and adversarial training",
    "keywords": [],
    "checked": true,
    "id": "8a15ba1646009f2ba7c64e1bc66f6a85ef0a05f3",
    "semantic_title": "cross-modal learning for audio-visual video parsing",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cook21_interspeech.html": {
    "title": "A Psychology-Driven Computational Analysis of Political Interviews",
    "volume": "main",
    "abstract": "Can an interviewer influence the cooperativeness of an interviewee? The role of an interviewer in actualising a successful interview is an active field of social psychological research. A large-scale analysis of interviews, however, typically involves time-exorbitant manual tasks and considerable human effort. Despite recent advances in computational fields, many automated methods continue to rely on manually labelled training data to establish ground-truth. This reliance obscures explainability and hinders the mobility of analysis between applications. In this work, we introduce a cross-disciplinary approach to analysing interviewer efficacy. We suggest computational success measures as a transparent, automated, and reproducible alternative for pre-labelled data. We validate these measures with a small-scale study with human-responders. To study the interviewer's influence on the interviewee we utilise features informed by social psychological theory to predict interview quality based on the interviewer's linguistic behaviour. Our psychologically informed model significantly outperforms a bag-of-words model, demonstrating the strength of a cross-disciplinary approach toward the analysis of conversational data at scale",
    "keywords": [],
    "checked": true,
    "id": "85fecc977213b8acaa9b281b9b55c0555c88013f",
    "semantic_title": "a psychology-driven computational analysis of political interviews",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/santoso21_interspeech.html": {
    "title": "Speech Emotion Recognition Based on Attention Weight Correction Using Word-Level Confidence Measure",
    "volume": "main",
    "abstract": "Emotion recognition is essential for human behavior analysis and possible through various inputs such as speech and images. However, in practical situations, such as in call center analysis, the available information is limited to speech. This leads to the study of speech emotion recognition (SER). Considering the complexity of emotions, SER is a challenging task. Recently, automatic speech recognition (ASR) has played a role in obtaining text information from speech. The combination of speech and ASR results has improved the SER performance. However, ASR results are highly affected by speech recognition errors. Although there is a method to improve ASR performance on emotional speech, it requires the fine-tuning of ASR, which is costly. To mitigate the errors in SER using ASR systems, we propose the use of the combination of a self-attention mechanism and a word-level confidence measure (CM), which indicates the reliability of ASR results, to reduce the importance of words with a high chance of error. Experimental results confirmed that the combination of self-attention mechanism and CM reduced the effects of incorrectly recognized words in ASR results, providing a better focus on words that determine emotion recognition. Our proposed method outperformed the state-of-the-art methods on the IEMOCAP dataset",
    "keywords": [],
    "checked": true,
    "id": "bdcc37d525145b07fedbe2c21fedced26240e96d",
    "semantic_title": "speech emotion recognition based on attention weight correction using word-level confidence measure",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/silpachai21_interspeech.html": {
    "title": "Effects of Voice Type and Task on L2 Learners' Awareness of Pronunciation Errors",
    "volume": "main",
    "abstract": "Research suggests learners may improve their second language (L2) pronunciation by imitating voices with similar acoustic profiles. However, previously reported improvements have been in suprasegmentals (prosodic features such as intonation). It remains unclear if voice similarity applies to L2 segmentals (consonants and vowels). To address this issue, this study investigates how voice similarity facilitates awareness of pronunciation errors, a necessary step in pronunciation improvement. In two experiments, advanced L2 learners identified their pronunciation errors by comparing their production to the production of a resynthesized model voice using learners' voices as the base (Golden Speaker voice), or to an unfamiliar resynthesized voice with the same gender as the learner (Silver Speaker voice). In Experiment 1, L2 learners identified all syllables with vowel and consonant errors when comparing their production to the model voice. Their choices were compared to identifications by expert judges. In Experiment 2, learners were told how many errors the expert judges had identified before identifying the same number of errors. Results did not support facilitative effects of Golden Speaker voices in either experiment, but Experiment 2 resulted in higher identification percentages. Discussion of the challenges in self-identification of errors in relation to voice similarity are offered",
    "keywords": [],
    "checked": true,
    "id": "cb31c803b8bced1631e2b25e1261a9060975ec45",
    "semantic_title": "effects of voice type and task on l2 learners' awareness of pronunciation errors",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/menshikova21_interspeech.html": {
    "title": "Lexical Entrainment and Intra-Speaker Variability in Cooperative Dialogues",
    "volume": "main",
    "abstract": "In dialogues, intra-speaker variability is often explained by the relationship between interlocutors. A person may speak differently with a friend and a stranger or depending on the interlocutor's gender or age — in all these cases we expect speech entrainment, but the degree of entrainment may vary. In this research, we measured lexical entrainment in a series of dialogues, where each one of 20 \"core\" speakers talked to five different interlocutors: a sibling, a close friend, an unfamiliar person of the same gender and similar age, an unfamiliar person of the other gender and similar age, and an unfamiliar person of the same gender, greater age and higher job position. We hypothesized that the degree of speech entrainment systematically varies according to the type of interlocutor, across all the \"core\" speakers. The following measures of entrainment were used: parts of speech statistics, verb forms statistics, language style matching, and lexical density. Our data have shown that a person speaks very similarly to his/her sibling; dialogues with a friend or a same-gender stranger of similar age show fewer similarities; the least \"common language\" is observed in dialogues with a stranger of the opposite gender and with a stranger of greater age and higher job position",
    "keywords": [],
    "checked": true,
    "id": "fcbfa52637de891dbfe053731830dab792547c8e",
    "semantic_title": "lexical entrainment and intra-speaker variability in cooperative dialogues",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nasreen21_interspeech.html": {
    "title": "Detecting Alzheimer's Disease Using Interactional and Acoustic Features from Spontaneous Speech",
    "volume": "main",
    "abstract": "Alzheimer's Disease (AD) is a form of Dementia that manifests in cognitive decline including memory, language, and changes in behavior. Speech data has proven valuable for inferring cognitive status, used in many health assessment tasks, and can be easily elicited in natural settings. Much work focuses on analysis using linguistic features; here, we focus on non-linguistic features and their use in distinguishing AD patients from similar-age Non-AD patients with other health conditions in the Carolinas Conversation Collection (CCC) dataset. We used two types of features: patterns of including pausing behaviour and floor control, and features including pitch, amplitude, energy, and cepstral coefficients. Fusion of the two kinds of features, combined with feature selection, obtains very promising classification results: classification accuracy of 90% using standard models such as support vector machines and logistic regression. We also obtain promising results using interactional features alone (87% accuracy), which can be easily extracted from natural conversations in daily life and thus have the potential for future implementation as a non-invasive method for AD diagnosis and monitoring",
    "keywords": [],
    "checked": true,
    "id": "ceb4b96216d4538589fd7dcd3c043e1cd365cdee",
    "semantic_title": "detecting alzheimer's disease using interactional and acoustic features from spontaneous speech",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kothare21_interspeech.html": {
    "title": "Investigating the Interplay Between Affective, Phonatory and Motoric Subsystems in Autism Spectrum Disorder Using a Multimodal Dialogue Agent",
    "volume": "main",
    "abstract": "We explore the utility of an on-demand multimodal conversational platform in extracting speech and facial metrics in children with Autism Spectrum Disorder (ASD). We investigate the extent to which these metrics correlate with objective clinical measures, particularly as they pertain to the interplay between the affective, phonatory and motoric subsystems. 22 participants diagnosed with ASD engaged with a virtual agent in conversational affect production tasks designed to elicit facial and vocal affect. We found significant correlations between vocal pitch and loudness extracted by our platform during these tasks and accuracy in recognition of facial and vocal affect, assessed via the Diagnostic Analysis of Nonverbal Accuracy-2 (DANVA-2) neuropsychological task. We also found significant correlations between jaw kinematic metrics extracted using our platform and motor speed of the dominant hand assessed via a standardised neuropsychological finger tapping task. These findings offer preliminary evidence for the usefulness of these audiovisual analytic metrics and could help us better model the interplay between different physiological subsystems in individuals with ASD",
    "keywords": [],
    "checked": true,
    "id": "91ad145fc6fba556c87c0e7dbb1c195d96ea8709",
    "semantic_title": "investigating the interplay between affective, phonatory and motoric subsystems in autism spectrum disorder using a multimodal dialogue agent",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ishi21_interspeech.html": {
    "title": "Analysis of Eye Gaze Reasons and Gaze Aversions During Three-Party Conversations",
    "volume": "main",
    "abstract": "The background of this study is the generation of natural gaze behaviors in human-robot multimodal interaction. For that purpose, in this study we analyzed gaze behaviors of multiple speakers in a dataset containing three-party conversations, in terms of the reasons/intentions of their gaze events Analyses of the gaze reasons were conducted separately for the gaze behaviors towards a dialogue partner, and for gaze aversions (i.e., gazing away from a person's face). Analysis on the eyeball movements during gaze aversions was also conducted. Different distributions for average durations and gaze direction patterns were observed depending on the gaze reasons (e.g., in listening mode, speaking mode, towards dialogue partner's reactions, in gaze aversions during thinking and remembering, and during the speaker's own behaviors like nodding and laughing)",
    "keywords": [],
    "checked": true,
    "id": "4d174bd440fba1cb720c9edecaa0c3fcca113286",
    "semantic_title": "analysis of eye gaze reasons and gaze aversions during three-party conversations",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21e_interspeech.html": {
    "title": "Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding",
    "volume": "main",
    "abstract": "Word Error Rate (WER) has been the predominant metric used to evaluate the performance of automatic speech recognition (ASR) systems. However, WER is sometimes not a good indicator for downstream Natural Language Understanding (NLU) tasks, such as intent recognition, slot filling, and semantic parsing in task-oriented dialog systems. This is because WER takes into consideration only literal correctness instead of semantic correctness, the latter of which is typically more important for these downstream tasks. In this study, we propose a novel Semantic Distance (SemDist) measure as an alternative evaluation metric for ASR systems to address this issue. We define SemDist as the distance between a reference and hypothesis pair in a sentence-level embedding space. To represent the reference and hypothesis as a sentence embedding, we exploit RoBERTa, a state-of-the-art pre-trained deep contextualized language model based on the transformer architecture. We demonstrate the effectiveness of our proposed metric on various downstream tasks, including intent recognition, semantic parsing, and named entity recognition",
    "keywords": [],
    "checked": true,
    "id": "f56d7fba6ca1b00f852bfafd013af6a5e781ed65",
    "semantic_title": "semantic distance: a new metric for asr performance analysis towards spoken language understanding",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21q_interspeech.html": {
    "title": "A Light-Weight Contextual Spelling Correction Model for Customizing Transducer-Based Speech Recognition Systems",
    "volume": "main",
    "abstract": "It's challenging to customize transducer-based automatic speech recognition (ASR) system with context information which is dynamic and unavailable during model training. In this work, we introduce a light-weight contextual spelling correction model to correct context-related recognition errors in transducer-based ASR systems. We incorporate the context information into the spelling correction model with a shared context encoder and use a filtering algorithm to handle large-size context lists. Experiments show that the model improves baseline ASR model performance with about 50% relative word error rate reduction, which also significantly outperforms the baseline method such as contextual LM biasing. The model also shows excellent performance for out-of-vocabulary terms not seen during training",
    "keywords": [],
    "checked": true,
    "id": "6c138a8808780e5648f334a8c8d48e5ea8554068",
    "semantic_title": "a light-weight contextual spelling correction model for customizing transducer-based speech recognition systems",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21_interspeech.html": {
    "title": "Incorporating External POS Tagger for Punctuation Restoration",
    "volume": "main",
    "abstract": "Punctuation restoration is an important post-processing step in automatic speech recognition. Among other kinds of external information, part-of-speech (POS) taggers provide informative tags, suggesting each input token's syntactic role, which has been shown to be beneficial for the punctuation restoration task. In this work, we incorporate an external POS tagger and fuse its predicted labels into the existing language model to provide syntactic information. Besides, we propose sequence boundary sampling (SBS) to learn punctuation positions more efficiently as a sequence tagging task. Experimental results show that our methods can consistently obtain performance gains and achieve a new state-of-the-art on the common IWSLT benchmark. Further ablation studies illustrate that both large pre-trained language models and the external POS tagger take essential parts to improve the model's performance",
    "keywords": [],
    "checked": true,
    "id": "f611534c9e35e8245f290a54ad5be99c50b10462",
    "semantic_title": "incorporating external pos tagger for punctuation restoration",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/papadourakis21_interspeech.html": {
    "title": "Phonetically Induced Subwords for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition systems map a sequence of acoustic features to text. In modern systems, text is encoded to grapheme subwords which are generated by methods designed for text processing tasks and therefore don't model or take advantage of the statistics of the acoustic features. Here, we present a novel method for generating grapheme subwords that are derived from phoneme sequences, therefore capturing phonetical statistics. The phonetically induced subwords can be used for training and inference in any system that benefits from subwords, regardless of architecture and without the need of a pronunciation lexicon. We compare our method to other commonly used methods, which are based on text statistics or on text-phoneme correspondence and present experiments on CTC and RNN-T architectures, evaluating subword sets of different sizes. We find that our phonetically induced subwords can improve performance of RNN-T models with relative improvements of up to 15.21% compared to other subword methods",
    "keywords": [],
    "checked": true,
    "id": "a249c42042ec86c38fabc026f382e475c39e63da",
    "semantic_title": "phonetically induced subwords for end-to-end speech recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mansfield21_interspeech.html": {
    "title": "Revisiting Parity of Human vs. Machine Conversational Speech Transcription",
    "volume": "main",
    "abstract": "A number of studies have compared human and machine transcription, showing that automatic speech recognition (ASR) is approaching human performance in some contexts. Most studies look at differences as measured by the standard speech recognition scoring criterion: word error rate (WER). This study looks at more fine-grained analysis of differences for conversational speech data where systems have reached human parity in terms of average WER, specifically insertions vs. deletions, word category, and word context characterized by linguistic surprisal. In contrast to ASR systems, humans are more likely to miss words than to misrecognize them, and they are much more likely to make errors in transcribing words associated primarily with conversational contexts (fillers, backchannels and discourse cue words). The differences are more pronounced for more informal contexts, i.e. conversations between family members. Although human transcribers may miss these words, conversational partners seem to use them in turntaking and processing disfluencies. Thus, ASR systems may need superhuman transcription performance for spoken language technology to achieve human-level conversation skills",
    "keywords": [],
    "checked": true,
    "id": "98307acfb63dd1b09cd3c53d5b57db8349ce126c",
    "semantic_title": "revisiting parity of human vs. machine conversational speech transcription",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21f_interspeech.html": {
    "title": "Lookup-Table Recurrent Language Models for Long Tail Speech Recognition",
    "volume": "main",
    "abstract": "We introduce Lookup-Table Language Models (LookupLM), a method for scaling up the size of RNN language models with only a constant increase in the floating point operations, by increasing the expressivity of the embedding table. In particular, we instantiate an (additional) embedding table which embeds the previous n-gram token sequence, rather than a single token. This allows the embedding table to be scaled up arbitrarily — with a commensurate increase in performance — without changing the token vocabulary. Since embeddings are sparsely retrieved from the table via a lookup; increasing the size of the table adds neither extra operations to each forward pass nor extra parameters that need to be stored on limited GPU/TPU memory. We explore scaling n-gram embedding tables up to nearly a billion parameters. When trained on a 3-billion sentence corpus, we find that LookupLM improves long tail log perplexity by 2.44 and long tail WER by 23.4% on a downstream speech recognition task over a standard RNN language model baseline, an improvement comparable to a scaling up the baseline by 6.2× the number of floating point operations",
    "keywords": [],
    "checked": true,
    "id": "4b734d4fb14acbc5135eb382e1388840df84e9f2",
    "semantic_title": "lookup-table recurrent language models for long tail speech recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/andresferrer21_interspeech.html": {
    "title": "Contextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems",
    "volume": "main",
    "abstract": "End-2-end (E2E) models have become increasingly popular in some ASR tasks because of their performance and advantages. These E2E models directly approximate the posterior distribution of tokens given the acoustic inputs. Consequently, the E2E systems implicitly define a language model (LM) over the output tokens, which makes the exploitation of independently trained language models less straightforward than in conventional ASR systems. This makes it difficult to dynamically adapt E2E ASR system to contextual profiles for better recognizing special words such as named entities. In this work, we propose a contextual density ratio approach for both training a contextual aware E2E model and adapting the language model to named entities. We apply the aforementioned technique to an E2E ASR system, which transcribes doctor and patient conversations, for better adapting the E2E system to the names in the conversations. Our proposed technique achieves a relative improvement of up to 46.5% on the names over an E2E baseline without degrading the overall recognition accuracy of the whole test set. Moreover, it also surpasses a contextual shallow fusion baseline by 22.1% relative",
    "keywords": [],
    "checked": true,
    "id": "8462f003cddd26bbd570c0ecb62fe67a733dab7c",
    "semantic_title": "contextual density ratio for language model biasing of sequence to sequence asr systems",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21g_interspeech.html": {
    "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration",
    "volume": "main",
    "abstract": "Punctuation is critical in understanding natural language text. Currently, most automatic speech recognition (ASR) systems do not generate punctuation, which affects the performance of downstream tasks, such as intent detection and slot filling. This gives rise to the need for punctuation restoration. Recent work in punctuation restoration heavily utilizes pre-trained language models without considering data imbalance when predicting punctuation classes. In this work, we address this problem by proposing a token-level supervised contrastive learning method that aims at maximizing the distance of representation of different punctuation marks in the embedding space. The result shows that training with token-level supervised contrastive learning obtains up to 3.2% absolute F improvement on the test set",
    "keywords": [],
    "checked": true,
    "id": "51589057074154ecd3daf67ebbb988e87b8e8808",
    "semantic_title": "token-level supervised contrastive learning for punctuation restoration",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhao21_interspeech.html": {
    "title": "BART Based Semantic Correction for Mandarin Automatic Speech Recognition System",
    "volume": "main",
    "abstract": "Although automatic speech recognition (ASR) systems achieved significantly improvements in recent years, spoken language recognition error occurs which can be easily spotted by human beings. Various language modeling techniques have been developed on post recognition tasks like semantic correction. In this paper, we propose a Transformer based semantic correction method with pretrained BART initialization, Experiments on 10000 hours Mandarin speech dataset show that character error rate (CER) can be effectively reduced by 21.7% relatively compared to our baseline ASR system. Expert evaluation demonstrates that actual improvement of our model surpasses what CER indicates",
    "keywords": [],
    "checked": true,
    "id": "184e00d63aedfd96c10ad88dc98ad01fe1635a01",
    "semantic_title": "bart based semantic correction for mandarin automatic speech recognition system",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dai21b_interspeech.html": {
    "title": "Class-Based Neural Network Language Model for Second-Pass Rescoring in ASR",
    "volume": "main",
    "abstract": "Language model rescoring, especially neural network language model (NNLM) rescoring, is widely used to achieve improved performance in a second-pass automatic speech recognition (ASR) system. The rescoring NNLM is usually trained separately from the ASR system. Typically, the two's training corpora are different, leading to the vocabulary mismatch problem, consequently degrading ASR performance. Previous research focuses more on the language domain mismatch problem, while the vocabulary mismatch problem, which may also cause significant performance degradation, has not been well studied. This paper proposes a novel class-based NNLM framework to address the vocabulary mismatch problem for language model rescoring. Here, OOV words (unknown words to the rescoring NNLM are called OOV words for short) are assigned to well-trained classes of NNLM and inherit the class probability. Experiments show that class-based NNLM rescoring can significantly reduce performance degradation due to vocabulary mismatch",
    "keywords": [],
    "checked": true,
    "id": "faa3244ca65e49b78a66efe1447d3376d17ef0bd",
    "semantic_title": "class-based neural network language model for second-pass rescoring in asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kurata21_interspeech.html": {
    "title": "Improving Customization of Neural Transducers by Mitigating Acoustic Mismatch of Synthesized Audio",
    "volume": "main",
    "abstract": "Customization of automatic speech recognition (ASR) models using text data from a target domain is essential to deploying ASR in various domains. End-to-end (E2E) modeling for ASR has made remarkable progress, but the advantage of E2E modeling, where all neural network parameters are jointly optimized, is offset by the challenge of customizing such models. In conventional hybrid models, it is easy to directly modify a language model or a lexicon using text data, but this is not true for E2E models. One popular approach for customizing E2E models uses audio synthesized from the target domain text, but the acoustic mismatch between the synthesized and real audio can be problematic. We propose a method that avoids the negative effect of synthesized audio by (1) adding a mapping network before the encoder network to map the acoustic features of the synthesized audio to those of the source domain, (2) training the added mapping network using text and synthesized audio from the source domain while freezing all layers in the E2E model, (3) training the E2E model with text and synthesized audio from the target domain, and (4) removing the added mapping network when decoding real audio from the target domain. Experiments on customizing RNN Transducer and Conformer Transducer models demonstrate the advantage of the proposed method over encoder freezing, a popular customization method for E2E models",
    "keywords": [],
    "checked": true,
    "id": "481e8dc47e4068b15096383fadaaff1054a47468",
    "semantic_title": "improving customization of neural transducers by mitigating acoustic mismatch of synthesized audio",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saebi21_interspeech.html": {
    "title": "A Discriminative Entity-Aware Language Model for Virtual Assistants",
    "volume": "main",
    "abstract": "High-quality automatic speech recognition (ASR) is essential for virtual assistants (VAs) to work well. However, ASR often performs poorly on VA requests containing named entities. In this work, we start from the observation that many ASR errors on named entities are inconsistent with real-world knowledge. We extend previous discriminative n-gram language modeling approaches to incorporate real-world knowledge from a Knowledge Graph (KG), using features that capture entity type-entity and entity-entity relationships. We apply our model through an efficient lattice rescoring process, achieving relative sentence error rate reductions of more than 25% on some synthesized test sets covering less popular entities, with minimal degradation on a uniformly sampled VA test set",
    "keywords": [],
    "checked": true,
    "id": "f1c7502d624c02490c5a534a106f347f75b6b2c2",
    "semantic_title": "a discriminative entity-aware language model for virtual assistants",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/namazifar21_interspeech.html": {
    "title": "Correcting Automated and Manual Speech Transcription Errors Using Warped Language Models",
    "volume": "main",
    "abstract": "Masked language models have revolutionized natural language processing systems in the past few years. A recently introduced generalization of masked language models called warped language models are trained to be more robust to the types of errors that appear in automatic or manual transcriptions of spoken language by exposing the language model to the same types of errors during the training of language models. In this work we propose a novel approach that takes advantage of the robustness of warped language models to transcription noise for correcting transcriptions of spoken language. We show that our proposed approach is able to achieve up to 10% reduction in word error rates of both automatic and manual transcriptions of spoken language",
    "keywords": [],
    "checked": true,
    "id": "a4bc44a41b0ddfb70d3a1a189127991cd3d68b7c",
    "semantic_title": "correcting automated and manual speech transcription errors using warped language models",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21b_interspeech.html": {
    "title": "Dynamic Encoder Transducer: A Flexible Solution for Trading Off Accuracy for Latency",
    "volume": "main",
    "abstract": "We propose a dynamic encoder transducer (DET) for on-device speech recognition. One DET model scales to multiple devices with different computation capacities without retraining or finetuning. To trading off accuracy and latency, DET assigns different encoders to decode different parts of an utterance. We apply and compare the layer dropout and the collaborative learning for DET training. The layer dropout method that randomly drops out encoder layers in the training phase, can do on-demand layer dropout in decoding. Collaborative learning jointly trains multiple encoders with different depths in one single model. Experiment results on Librispeech and in-house data show that DET provides a flexible accuracy and latency trade-off. Results on Librispeech show that the full-size encoder in DET relatively reduces the word error rate of the same size baseline by over 8%. The lightweight encoder in DET trained with collaborative learning reduces the model size by 25% but still gets similar WER as the full-size baseline. DET gets similar accuracy as a baseline model with better latency on a large in-house data set by assigning a lightweight encoder for the beginning part of one utterance and a full-size encoder for the rest",
    "keywords": [],
    "checked": true,
    "id": "b97d785e12b6fbdd5d74695342b0b6491cd1373b",
    "semantic_title": "dynamic encoder transducer: a flexible solution for trading off accuracy for latency",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21n_interspeech.html": {
    "title": "Domain-Aware Self-Attention for Multi-Domain Neural Machine Translation",
    "volume": "main",
    "abstract": "In this paper, we investigate multi-domain neural machine translation (NMT) that translates sentences of different domains in a single model. To this end, we propose a domain-aware self-attention mechanism that jointly learns domain representations with the single NMT model. The learned domain representations are integrated into both the encoder and decoder. We further propose two different domain representation learning approaches: 1) word-level unsupervised learning via a domain attention network and 2) guided learning with an auxiliary loss. The two learning approaches allow our multi-domain NMT to work in different settings as to whether the domain information is available or not. Experiments on both Chinese-English and English-French demonstrate that our multi-domain model outperforms a strong baseline built on the Transformer and other previous multi-domain NMT approaches. Further analyses show that our model is able to learn domain clusters even without prior knowledge about the domain structure",
    "keywords": [],
    "checked": true,
    "id": "48c724fd5396a1ee4262d078c7d01c36071f8dad",
    "semantic_title": "domain-aware self-attention for multi-domain neural machine translation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zeyer21_interspeech.html": {
    "title": "Librispeech Transducer Model with Internal Language Model Prior Correction",
    "volume": "main",
    "abstract": "We present our transducer model on Librispeech. We study variants to include an external language model (LM) with shallow fusion and subtract an estimated internal LM. This is justified by a Bayesian interpretation where the transducer model prior is given by the estimated internal LM. The subtraction of the internal LM gives us over 14% relative improvement over normal shallow fusion. Our transducer has a separate probability distribution for the non-blank labels which allows for easier combination with the external LM, and easier estimation of the internal LM. We additionally take care of including the end-of-sentence (EOS) probability of the external LM in the last blank probability which further improves the performance. All our code and setups are published",
    "keywords": [],
    "checked": true,
    "id": "07b631a9fd4901d23ad1e7ed35a6c3d6ad1b6307",
    "semantic_title": "librispeech transducer model with internal language model prior correction",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mavandadi21_interspeech.html": {
    "title": "A Deliberation-Based Joint Acoustic and Text Decoder",
    "volume": "main",
    "abstract": "We propose a new two-pass E2E speech recognition model that improves ASR performance by training on a combination of paired data and unpaired text data. Previously, the joint acoustic and text decoder (JATD) has shown promising results through the use of text data during model training and the recently introduced deliberation architecture has reduced recognition errors by leveraging first-pass decoding results. Our method, dubbed Deliberation-JATD, combines the spelling correcting abilities of deliberation with JATD's use of unpaired text data to further improve performance. The proposed model produces substantial gains across multiple test sets, especially those focused on rare words, where it reduces word error rate (WER) by between 12% and 22.5% relative. This is done without increasing model size or requiring multi-stage training, making Deliberation-JATD an efficient candidate for on-device applications",
    "keywords": [],
    "checked": true,
    "id": "985def82fc31b6448a737909fbb1c2898a803ab3",
    "semantic_title": "a deliberation-based joint acoustic and text decoder",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tuske21_interspeech.html": {
    "title": "On the Limit of English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "In our previous work we demonstrated that a single headed attention encoder-decoder model is able to reach state-of-the-art results in conversational speech recognition. In this paper, we further improve the results for both Switchboard 300 and 2000. Through use of an improved optimizer, speaker vector embeddings, and alternative speech representations we reduce the recognition errors of our LSTM system on Switchboard-300 by 4% relative. Compensation of the decoder model with the probability ratio approach allows more efficient integration of an external language model, and we report 5.9% and 11.5% WER on the SWB and CHM parts of Hub5'00 with very simple LSTM models. Our study also considers the recently proposed conformer, and more advanced self-attention based language models. Overall, the conformer shows similar performance to the LSTM; nevertheless, their combination and decoding with an improved LM reaches a new record on Switchboard-300, 5.0% and 10.0% WER on SWB and CHM. Our findings are also confirmed on Switchboard-2000, and a new state of the art is reported, practically reaching the limit of the benchmark",
    "keywords": [],
    "checked": true,
    "id": "2fc885d669263a151e8906f124d3388029b114a1",
    "semantic_title": "on the limit of english conversational speech recognition",
    "citation_count": 44,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/an21_interspeech.html": {
    "title": "Deformable TDNN with Adaptive Receptive Fields for Speech Recognition",
    "volume": "main",
    "abstract": "Time Delay Neural Networks (TDNNs) are widely used in both DNN-HMM based hybrid speech recognition systems and recent end-to-end systems. Nevertheless, the receptive fields of TDNNs are limited and fixed, which is not desirable for tasks like speech recognition, where the temporal dynamics of speech are varied and affected by many factors. In this paper, we propose to use deformable TDNNs for adaptive temporal dynamics modeling in end-to-end speech recognition. Inspired by deformable ConvNets, deformable TDNNs augment the temporal sampling locations with additional offsets and learn the offsets automatically based on the ASR criterion, without additional supervision. Experiments show that deformable TDNNs obtain state-of-the-art results on WSJ benchmarks (1.42%/3.45% WER on WSJ eval92/dev93 respectively), outperforming standard TDNNs significantly. Furthermore, we propose the latency control mechanism for deformable TDNNs, which enables deformable TDNNs to do streaming ASR without accuracy degradation",
    "keywords": [],
    "checked": true,
    "id": "9b5ac7de7b971c3f73cdd87208f491838b85d13e",
    "semantic_title": "deformable tdnn with adaptive receptive fields for speech recognition",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/you21_interspeech.html": {
    "title": "SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture of Experts",
    "volume": "main",
    "abstract": "Recently, Mixture of Experts (MoE) based Transformer has shown promising results in many domains. This is largely due to the following advantages of this architecture: firstly, MoE based Transformer can increase model capacity without computational cost increasing both at training and inference time. Besides, MoE based Transformer is a dynamic network which can adapt to the varying complexity of input instances in real-world applications. In this work, we explore the MoE based model for speech recognition, named SpeechMoE. To further control the sparsity of router activation and improve the diversity of gate values, we propose a sparsity L1 loss and a mean importance loss respectively. In addition, a new router architecture is used in SpeechMoE which can simultaneously utilize the information from a shared embedding network and the hierarchical representation of different MoE layers. Experimental results show that SpeechMoE can achieve lower character error rate (CER) with comparable computation cost than traditional static networks, providing 7.0%~23.0% relative CER improvements on four evaluation datasets",
    "keywords": [],
    "checked": true,
    "id": "58d695a7db086c9788c22bada4877a59c4cbf92b",
    "semantic_title": "speechmoe: scaling to large acoustic models with dynamic routing mixture of experts",
    "citation_count": 32,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leong21_interspeech.html": {
    "title": "Online Compressive Transformer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Traditionally, transformer with connectionist temporal classification (CTC) was developed for offline speech recognition where the transcription was generated after the whole utterance has been spoken. However, it is crucial to carry out online transcription of speech signal for many applications including live broadcasting and meeting. This paper presents an online transformer for real-time speech recognition where online transcription is generated chunk by chuck. In particular, an online compressive transformer (OCT) is proposed for end-to-end speech recognition. This OCT aims to generate immediate transcription for each audio chunk while the comparable performance with offline speech recognition can be still achieved. In the implementation, OCT tightly combines with both CTC and recurrent neural network transducer by minimizing their losses for training. In addition, this OCT systematically merges with compressive memory to reduce potential performance degradation due to online processing. This degradation is caused by online transcription which is generated by the chunks without history information. The experiments on speech recognition show that OCT does not only obtain comparable performance with offline transformer, but also work faster than the baseline model",
    "keywords": [],
    "checked": true,
    "id": "199dbed0f3104b5d7367d62ee89561351a26d2ce",
    "semantic_title": "online compressive transformer for end-to-end speech recognition",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21e_interspeech.html": {
    "title": "End to End Transformer-Based Contextual Speech Recognition Based on Pointer Network",
    "volume": "main",
    "abstract": "Most spoken language assessment systems rely on the text features extracted from the automatic speech recognition (ASR) transcripts and thus depend heavily on the accuracy of the ASR systems. Automatic speech scoring tasks such as reading aloud and spontaneous speech are commonly provided with the prompts in advance to guide test takers' answers, which contain information that should be included in the answers (e.g., listening passage, and sample response). Utilizing these texts to improve ASR performance is of great importance for these tasks. In this paper, we develop an end-to-end (E2E) ASR system incorporating contextual information provided by prompts. Specifically, we add an extra prompt encoder to a transformer-based E2E ASR system. To fuse the probabilities of the ASR output and the prompts dynamically, we train a soft gate based on the pointer network with carefully constructed prompt training corpus. We experiment the proposed method with data collected from English speaking proficiency tests recorded by Chinese teenagers from 16 to 18 years old. The results show the improved performance of speech recognition with a nearly 50% drop in word error rate (WER) utilizing prompts. Furthermore, the proposed network performs well in rare word recognition such as locations and personal names",
    "keywords": [],
    "checked": true,
    "id": "8bd5485ff3da6afc97b7b247f47ebbbf430de781",
    "semantic_title": "end to end transformer-based contextual speech recognition based on pointer network",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karita21_interspeech.html": {
    "title": "A Comparative Study on Neural Architectures and Training Methods for Japanese Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end (E2E) modeling is advantageous for automatic speech recognition (ASR) especially for Japanese since word-based tokenization of Japanese is not trivial, and E2E modeling is able to model character sequences directly. This paper focuses on the latest E2E modeling techniques, and investigates their performances on character-based Japanese ASR by conducting comparative experiments. The results are analyzed and discussed in order to understand the relative advantages of long short-term memory (LSTM), and Conformer models in combination with connectionist temporal classification, transducer, and attention-based loss functions. Furthermore, the paper investigates on effectivity of the recent training techniques such as data augmentation (SpecAugment), variational noise injection, and exponential moving average. The best configuration found in the paper achieved the state-of-the-art character error rates of 4.1%, 3.2%, and 3.5% for Corpus of Spontaneous Japanese (CSJ) eval1, eval2, and eval3 tasks, respectively. The system is also shown to be computationally efficient thanks to the efficiency of Conformer transducers",
    "keywords": [],
    "checked": true,
    "id": "f4c40a8224156851b2858ab679315dd5853311a2",
    "semantic_title": "a comparative study on neural architectures and training methods for japanese speech recognition",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hori21b_interspeech.html": {
    "title": "Advanced Long-Context End-to-End Speech Recognition Using Context-Expanded Transformers",
    "volume": "main",
    "abstract": "This paper addresses end-to-end automatic speech recognition (ASR) for long audio recordings such as lecture and conversational speeches. Most end-to-end ASR models are designed to recognize independent utterances, but contextual information (e.g., speaker or topic) over multiple utterances is known to be useful for ASR. In our prior work, we proposed a context-expanded Transformer that accepts multiple consecutive utterances at the same time and predicts an output sequence for the last utterance, achieving 5–15% relative error reduction from utterance-based baselines in lecture and conversational ASR benchmarks. Although the results have shown remarkable performance gain, there is still potential to further improve the model architecture and the decoding process. In this paper, we extend our prior work by (1) introducing the Conformer architecture to further improve the accuracy, (2) accelerating the decoding process with a novel activation recycling technique, and (3) enabling streaming decoding with triggered attention. We demonstrate that the extended Transformer provides state-of-the-art end-to-end ASR performance, obtaining a 17.3% character error rate for the HKUST dataset and 12.0%/6.3% word error rates for the Switchboard-300 Eval2000 CallHome/Switchboard test sets. The new decoding method reduces decoding time by more than 50% and further enables streaming ASR with limited accuracy degradation",
    "keywords": [],
    "checked": true,
    "id": "d0d67f32ce0a99ac9eb86a2fa7864a364c7e19ac",
    "semantic_title": "advanced long-context end-to-end speech recognition using context-expanded transformers",
    "citation_count": 28,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/haidar21_interspeech.html": {
    "title": "Transformer-Based ASR Incorporating Time-Reduction Layer and Fine-Tuning with Self-Knowledge Distillation",
    "volume": "main",
    "abstract": "Reducing the input sequence length of speech features to alleviate the complexity of alignment between speech features and text transcript by sub-sampling approaches is an important way to get better results in end-to-end (E2E) automatic speech recognition (ASR) systems. This issue is more important in Transformer-based ASR, because the self-attention mechanism in Transformers has O(n ) order of complexity in both training and inference. In this paper, we propose a Transformer-based ASR model with the time-reduction layer, in which we incorporate time-reduction layer inside transformer encoder layers in addition to traditional sub-sampling methods to input features that further reduce the frame-rate. This can help in reducing the computational cost of the self-attention process for training and inference with performance improvement. Moreover, we introduce a fine-tuning approach for pre-trained ASR models using self-knowledge distillation (S-KD) which further improves the performance of our ASR model. Experiments on LibriSpeech datasets show that our proposed methods outperform all other Transformer-based ASR systems. Furthermore, with language model (LM) fusion, we achieve new state-of-the-art word error rate (WER) results for Transformer-based ASR models with just 30 million parameters trained without any external data",
    "keywords": [],
    "checked": true,
    "id": "be9d45202839a1381bb0c20df1f2324722add627",
    "semantic_title": "transformer-based asr incorporating time-reduction layer and fine-tuning with self-knowledge distillation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mahadeokar21_interspeech.html": {
    "title": "Flexi-Transducer: Optimizing Latency, Accuracy and Compute for Multi-Domain On-Device Scenarios",
    "volume": "main",
    "abstract": "Often, the storage and computational constraints of embedded devices demand that a single on-device ASR model serve multiple use-cases / domains. In this paper, we propose a (FlexiT) for on-device automatic speech recognition to flexibly deal with multiple use-cases / domains with different accuracy and latency requirements. Specifically, using a single compact model, FlexiT provides a fast response for , and accurate transcription but with more latency for In order to achieve flexible and better accuracy and latency trade-offs, the following techniques are used. Firstly, we propose using domain-specific altering of segment size for Emformer encoder that enables FlexiT to achieve flexible decoding. Secondly, we use Alignment Restricted RNNT loss to achieve flexible fine-grained control on token emission latency for different domains. Finally, we add a domain indicator vector as an additional input to the FlexiT model. Using the combination of techniques, we show that a single model can be used to improve WERs and real time factor for dictation scenarios while maintaining optimal latency for voice commands use-cases",
    "keywords": [],
    "checked": true,
    "id": "a9279eb2b5268fa3b15686ebe0cb856e32b41318",
    "semantic_title": "flexi-transducer: optimizing latency, accuracy and compute formulti-domain on-device scenarios",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/falkowskigilski21_interspeech.html": {
    "title": "Difference in Perceived Speech Signal Quality Assessment Among Monolingual and Bilingual Teenage Students",
    "volume": "main",
    "abstract": "The user perceived quality is a mixture of factors, including the background of an individual. The process of auditory perception is discussed in a wide variety of fields, ranging from engineering to medicine. Many studies examine the difference between musicians and non-musicians. Since musical training develops musical hearing and other various auditory capabilities, similar enhancements should be observable in case of bilingual people. This paper examines the difference in perceived speech signal quality between students from monolingual and bilingual classes. The subjective study was carried out on a group of 30 people, with 15 individuals in each class, aged 16–18 years old, considering three languages: English, German, and Polish. Results of this study may aid researchers as well as professionals active in the field of auditory perception, hearing loss related with ageing, and of course evaluation of networks and services",
    "keywords": [],
    "checked": true,
    "id": "0638e115a7c1fafee64e6f8f97186e87956a3e55",
    "semantic_title": "difference in perceived speech signal quality assessment among monolingual and bilingual teenage students",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/schymura21_interspeech.html": {
    "title": "PILOT: Introducing Transformers for Probabilistic Sound Event Localization",
    "volume": "main",
    "abstract": "Sound event localization aims at estimating the positions of sound sources in the environment with respect to an acoustic receiver (e.g. a microphone array). Recent advances in this domain most prominently focused on utilizing deep recurrent neural networks. Inspired by the success of transformer architectures as a suitable alternative to classical recurrent neural networks, this paper introduces a novel transformer-based sound event localization framework, where temporal dependencies in the received multi-channel audio signals are captured via self-attention mechanisms. Additionally, the estimated sound event positions are represented as multivariate Gaussian variables, yielding an additional notion of uncertainty, which many previously proposed deep learning-based systems designed for this application do not provide. The framework is evaluated on three publicly available multi-source sound event localization datasets and compared against state-of-the-art methods in terms of localization error and event detection accuracy. It outperforms all competing systems on all datasets with statistical significant differences in performance",
    "keywords": [],
    "checked": true,
    "id": "a59a0abdb5fe9f110e132efef2a0b6cbf0ea2131",
    "semantic_title": "pilot: introducing transformers for probabilistic sound event localization",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/togami21_interspeech.html": {
    "title": "Sound Source Localization with Majorization Minimization",
    "volume": "main",
    "abstract": "We propose a sound source localization technique that estimates a speech source location without precise grid searching. The source location is estimated in a parameter optimization manner to minimize the steered-response power (SRP) function with the near-field assumption. Because there is no closed-form solution for the SRP function, we introduce an auxiliary function of the SRP function based on the majorization-minimization (MM) algorithm. Parameters are updated iteratively to minimize the auxiliary function with alternate execution of time-difference-of-arrival (TDOA) estimation and range-difference (RD) based localization. When TDOA estimation and RD-based localization are performed in a cascade manner, the estimation accuracy of the source location is strongly affected by the estimation accuracy of the TDOA. On contrary, the proposed method corrects the estimated TDOA by referring to the estimated source location in the previous iteration. Thus, it is expected for the proposed method to be robust against TDOA estimation error which occurs under reverberant environments. Experimental results show that the proposed method outperforms conventional techniques under a reverberant environment",
    "keywords": [],
    "checked": true,
    "id": "6dae5ee2737cf92f500924b9c24d04a158693b38",
    "semantic_title": "sound source localization with majorization minimization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mittag21_interspeech.html": {
    "title": "NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets",
    "volume": "main",
    "abstract": "In this paper, we present an update to the NISQA speech quality prediction model that is focused on distortions that occur in communication networks. In contrast to the previous version, the model is trained end-to-end and the time-dependency modelling and time-pooling is achieved through a Self-Attention mechanism. Besides overall speech quality, the model also predicts the four speech quality dimensions , , , and , and in this way gives more insight into the cause of a quality degradation. Furthermore, new datasets with over 13,000 speech files were created for training and validation of the model. The model was finally tested on a new, live-talking test dataset that contains recordings of real telephone calls. Overall, NISQA was trained and evaluated on 81 datasets from different sources and showed to provide reliable predictions also for unknown speech samples. The code, model weights, and datasets are open-sourced",
    "keywords": [],
    "checked": true,
    "id": "99c2911afc6fddbdf6f0046f0198abe596931856",
    "semantic_title": "nisqa: a deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets",
    "citation_count": 103,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/naderi21_interspeech.html": {
    "title": "Subjective Evaluation of Noise Suppression Algorithms in Crowdsourcing",
    "volume": "main",
    "abstract": "The quality of the speech communication systems, which include noise suppression algorithms, are typically evaluated in laboratory experiments according to the ITU-T Rec. P.835, in which participants rate background noise, speech signal, and overall quality separately. This paper introduces an open-source toolkit for conducting subjective quality evaluation of noise suppressed speech in crowdsourcing. We followed the ITU-T Rec. P.835, and P.808 and highly automate the process to prevent moderator's error. To assess the validity of our evaluation method, we compared the Mean Opinion Scores (MOS), calculated using ratings collected with our implementation and the MOS values from a standard laboratory experiment conducted according to the ITU-T Rec P.835. Results show a high validity in all three scales, namely background noise, speech signal and overall quality (average Pearson Correlation Coefficient (PCC) = 0.961). Results of a round-robin test (N=5) showed that our implementation is also a highly reproducible evaluation method (PCC=0.99). Finally, we used our implementation in the INTERSPEECH 2021 Deep Noise Suppression Challenge [1] as the primary evaluation metric, which demonstrates it is practical to use at scale. The results are analyzed to determine why the overall performance was the best in terms of background noise and speech quality",
    "keywords": [],
    "checked": true,
    "id": "3a76433b07df7887a33f9d432cb5fe22cb9d6b24",
    "semantic_title": "subjective evaluation of noise suppression algorithms in crowdsourcing",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/geng21_interspeech.html": {
    "title": "Reliable Intensity Vector Selection for Multi-Source Direction-of-Arrival Estimation Using a Single Acoustic Vector Sensor",
    "volume": "main",
    "abstract": "In the context of multi-source direction of arrival (DOA) estimation using a single acoustic vector sensor (AVS), the received signal is usually a mixture of noise, reverberation and source signals. The identification of the time-frequency (TF) bins that are dominated by the source signals can significantly improve the robustness of the DOA estimation. In this paper, a TF bin selection based DOA estimation pipeline is proposed. The proposed pipeline mainly involves three key steps: key frame identification, TF bin selection and DOA extraction. We identify the key frames by frame-wisely examining the effective rank. Subsequently, the geometric medians of the selected key frames are extracted to alleviate the impact of extreme outliers. The simulation results show that the accuracy and the robustness of the proposed pipeline outperform the state-of-the-art (SOTA) techniques",
    "keywords": [],
    "checked": true,
    "id": "7115b67b59d884720855b7cd1bf5f5158d39aabb",
    "semantic_title": "reliable intensity vector selection for multi-source direction-of-arrival estimation using a single acoustic vector sensor",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yu21_interspeech.html": {
    "title": "MetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "The objective speech quality assessment is usually conducted by comparing received speech signal with its clean reference, while human beings are capable of evaluating the speech quality without any reference, such as in the mean opinion score (MOS) tests. Non-intrusive speech quality assessment has attracted much attention recently due to the lack of access to clean reference signals for objective evaluations in real scenarios. In this paper, we propose a novel non-intrusive speech quality measurement model, MetricNet, which leverages label distribution learning and joint speech reconstruction learning to achieve significantly improved performance compared to the existing non-intrusive speech quality measurement models. We demonstrate that the proposed approach yields promisingly high correlation to the intrusive objective evaluation of speech quality on clean, noisy and processed speech data",
    "keywords": [],
    "checked": true,
    "id": "50c839f04f1d881bc0960e6f33d3f03f1ddd1914",
    "semantic_title": "metricnet: towards improved modeling for non-intrusive speech quality assessment",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/toma21_interspeech.html": {
    "title": "CNN-Based Processing of Acoustic and Radio Frequency Signals for Speaker Localization from MAVs",
    "volume": "main",
    "abstract": "A novel speaker localization algorithm from micro aerial vehicles (MAVs) is investigated. It introduces a joint direction of arrival (DOA) and distance prediction method based on processing and fusion of the multi-channel speech data with radio frequency (RF) measurements of the received signal strength. Possible applications include unmanned aerial vehicles (UAVs)-based reconnaissance and surveillance against intrusions and search and rescue in hostile environments. A 3-stages convolutional neural network (CNN) with a fusion layer is proposed to perform this task with the objective of augmenting the source localization from multi-channel speech signals. Two parallel CNNs process the speech and RF data, and the regression network produces predictions of the angle and distance from the source after the fusion layer. To show the performance and effectiveness of this RF-assisted method, the experimental scenario and datasets are presented and experiments are then discussed along with the results that have been obtained",
    "keywords": [],
    "checked": true,
    "id": "0641b733c2a3754dfeaaafd93f7fa14793aceb76",
    "semantic_title": "cnn-based processing of acoustic and radio frequency signals for speaker localization from mavs",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/itoyama21_interspeech.html": {
    "title": "Assessment of von Mises-Bernoulli Deep Neural Network in Sound Source Localization",
    "volume": "main",
    "abstract": "This paper addresses the properties and effectiveness of the von Mises-Bernoulli deep neural network (vM-B DNN), a neural network capable of learning periodic information, in sound source localization. The phase, which is periodic information, is an important cue in sound source localization, but typical neural network cannot handle periodic input values properly. The vM-B DNN has been theoretically revealed to be able to handle periodic input values and its effectiveness has been shown in a simple case study of sound source localization using artificial sinusoids, but it was not in the case of speech signals. We conducted both numerical simulation and actual environment experiments. We compared a sound source localization method using vM-B DNN with those using ordinary neural networks, and showed that the vM-B DNN outperforms other methods under various conditions",
    "keywords": [],
    "checked": true,
    "id": "cb1c95b55176668eed07b0a0e39673b8eda13176",
    "semantic_title": "assessment of von mises-bernoulli deep neural network in sound source localization",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21g_interspeech.html": {
    "title": "Feature Fusion by Attention Networks for Robust DOA Estimation",
    "volume": "main",
    "abstract": "Direction of arrival (DOA) estimation is a key front-end technology for many speech-based intelligent systems. Deep neural networks-based DOA systems have recently demonstrated better performances than conventional ones. However, most of the existing networks use only one specific acoustical feature as input, limiting their noise-robustness. This paper proposes an attention-based feature fusion approach for DOA estimation. Two classical DOA estimation approaches, i.e., the least mean square-based adaptive filtering and the generalized cross-correlation, are adopted, and the respective features are served as input to the networks. Network with attention mechanism is built to learn the optimal weighting scheme, which can take advantage of the two features' complementary contributions in DOA estimation. Simulation and real test results show that the proposed method could use the complementary DOA information in different features and improve estimation accuracy under acoustic conditions with both noise and reverberation",
    "keywords": [],
    "checked": true,
    "id": "0b6fc45f014b6c62ab9944f8e55b69a2889ae30d",
    "semantic_title": "feature fusion by attention networks for robust doa estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21f_interspeech.html": {
    "title": "Far-Field Speaker Localization and Adaptive GLMB Tracking",
    "volume": "main",
    "abstract": "In the speech signal processing area, far-field speaker localization using only the audio modality has been a fundamental but challenging problem, especially in presence of reverberation and a varying number of moving speakers. Many existing methods use speech onsets as reliable directional cues against reverberation and interference. However, signal processing can be computationally costly especially in time domain. In this paper, we present a computationally efficient implementation of the recently proposed Onset-Multichannel Cross Correlation Coefficient (MCCC) method. Instead of scanning the entire spatial grid, reverse mapping and linear interpolation are used. The proposed algorithm with better efficiency is referred to as the Onset-MCC in this paper. Performance of the Onset-MCC is studied over various reverberant and noisy scenarios. To further suppress outliers and address miss-detections, as well as for the adaptive tracking of a varying number of moving speakers, we present an adaptive implementation of the generalized labeled multi-Bernoulli (GLMB) filter. As shown in studied cases, the proposed system demonstrates reliable and accurate location estimates in far-field (T = 1s), and is applicable to tracking an unknown and time-varying number of moving speakers",
    "keywords": [],
    "checked": true,
    "id": "029f4d9fd13a99ab3a8c550270c6d6c73c6477c6",
    "semantic_title": "far-field speaker localization and adaptive glmb tracking",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/narayanaswamy21_interspeech.html": {
    "title": "On the Design of Deep Priors for Unsupervised Audio Restoration",
    "volume": "main",
    "abstract": "Unsupervised deep learning methods for solving audio restoration problems extensively rely on carefully tailored neural architectures that carry strong inductive biases for defining priors in the time or spectral domain. In this context, lot of recent success has been achieved with sophisticated convolutional network constructions that recover audio signals in the spectral domain. However, in practice, audio priors require careful engineering of the convolutional kernels to be effective at solving ill-posed restoration tasks, while also being easy to train. To this end, in this paper, we propose a new U-Net based prior that does not impact either the network complexity or convergence behavior of existing convolutional architectures, yet leads to significantly improved restoration. In particular, we advocate the use of carefully designed dilation schedules and dense connections in the U-Net architecture to obtain powerful audio priors. Using empirical studies on standard benchmarks and a variety of ill-posed restoration tasks, such as audio denoising, in-painting and source separation, we demonstrate that our proposed approach consistently outperforms widely adopted audio prior architectures",
    "keywords": [],
    "checked": true,
    "id": "bb4daed4b1e20f838703af81392ff5be5103ce5c",
    "semantic_title": "on the design of deep priors for unsupervised audio restoration",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21h_interspeech.html": {
    "title": "Cramér-Rao Lower Bound for DOA Estimation with an Array of Directional Microphones in Reverberant Environments",
    "volume": "main",
    "abstract": "Existing direction-of-arrival (DOA) estimation methods usually assume that signals are received by an array of omnidirectional microphones. The performance can be seriously degraded due to heavy reverberation and noise. In this paper, DOA estimation using an array with directional microphones is considered. As the signal response varies over different DOAs, the magnitude information as well as the phase information can be employed to estimate the DOA. We first introduce the spherically isotropic noise field using directional microphones. The Cramér-Rao Lower Bound (CRLB) for DOA estimation is then derived and compared with that using omnidirectional microphones under different signal-to-reverberation ratio (SRR) environments. In addition, we extend existing steered response power (SRP), minimum variance distortionless response (MVDR) and multiple signal classification (MUSIC) estimators for the DOA estimation using directional microphone arrays. Both CRLB Analysis and DOA estimation show that better DOA estimation performance can be achieved by using a directional microphone array",
    "keywords": [],
    "checked": true,
    "id": "8d1dc64602da22b54a82102dd56444288c062a73",
    "semantic_title": "cramér-rao lower bound for doa estimation with an array of directional microphones in reverberant environments",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/you21b_interspeech.html": {
    "title": "GAN Vocoder: Multi-Resolution Discriminator Is All You Need",
    "volume": "main",
    "abstract": "Several of the latest GAN-based vocoders show remarkable achievements, outperforming autoregressive and flow-based competitors in both qualitative and quantitative measures while synthesizing orders of magnitude faster. In this work, we hypothesize that the common factor underlying their success is the multi-resolution discriminating framework, not the minute details in architecture, loss function, or training strategy. We experimentally test the hypothesis by evaluating six different generators paired with one shared multi-resolution discriminating framework. For all evaluative measures with respect to text-to-speech syntheses and for all perceptual metrics, their performances are not distinguishable from one another, which supports our hypothesis",
    "keywords": [],
    "checked": true,
    "id": "1ee20991b5b3441bb14badbadc4d61d7ed33752f",
    "semantic_title": "gan vocoder: multi-resolution discriminator is all you need",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cong21_interspeech.html": {
    "title": "Glow-WaveGAN: Learning Speech Representations from GAN-Based Variational Auto-Encoder for High Fidelity Flow-Based Speech Synthesis",
    "volume": "main",
    "abstract": "Current two-stage TTS framework typically integrates an acoustic model with a vocoder — the acoustic model predicts a low resolution intermediate representation such as Mel-spectrum while the vocoder generates waveform from the intermediate representation. Although the intermediate representation is served as a bridge, there still exists critical mismatch between the acoustic model and the vocoder as they are commonly separately learned and work on different distributions of representation, leading to inevitable artifacts in the synthesized speech. In this work, different from using pre-designed intermediate representation in most previous studies, we propose to use VAE combining with GAN to learn a latent representation directly from speech and then utilize a flow-based acoustic model to model the distribution of the latent representation from text. In this way, the mismatch problem is migrated as the two stages work on the same distribution. Results demonstrate that the flow-based acoustic model can exactly model the distribution of our learned speech representation and the proposed TTS framework, namely Glow-WaveGAN, can produce high fidelity speech outperforming the state-of-the-art GAN-based model",
    "keywords": [],
    "checked": true,
    "id": "52eac03a7f65224db8c357199d6ae211230813db",
    "semantic_title": "glow-wavegan: learning speech representations from gan-based variational auto-encoder for high fidelity flow-based speech synthesis",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yoneyama21_interspeech.html": {
    "title": "Unified Source-Filter GAN: Unified Source-Filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN",
    "volume": "main",
    "abstract": "We propose a unified approach to data-driven source-filter modeling using a single neural network for developing a neural vocoder capable of generating high-quality synthetic speech waveforms while retaining flexibility of the source-filter model to control their voice characteristics. Our proposed network called unified source-filter generative adversarial networks (uSFGAN) is developed by factorizing quasi-periodic parallel WaveGAN (QPPWG), one of the neural vocoders based on a single neural network, into a source excitation generation network and a vocal tract resonance filtering network by additionally implementing a regularization loss. Moreover, inspired by neural source filter (NSF), only a sinusoidal waveform is additionally used as the simplest clue to generate a periodic source excitation waveform while minimizing the effect of approximations in the source filter model. The experimental results demonstrate that uSFGAN outperforms conventional neural vocoders, such as QPPWG and NSF in both speech quality and pitch controllability",
    "keywords": [],
    "checked": true,
    "id": "64c626a5f9bed0d5d4f0d675e76de349b5f9340f",
    "semantic_title": "unified source-filter gan: unified source-filter network based on factorization of quasi-periodic parallel wavegan",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mizuta21_interspeech.html": {
    "title": "Harmonic WaveGAN: GAN-Based Speech Waveform Generation Model with Harmonic Structure Discriminator",
    "volume": "main",
    "abstract": "This paper proposes Harmonic WaveGAN, a GAN-based waveform generation model that focuses on the harmonic structure of a speech waveform. Our proposed model uses two discriminators to capture characteristics of a speech waveform in a time domain and in a frequency domain, respectively. In one of them, a harmonic structure discriminator, a 2-D convolution layer called \"harmonic convolution\" is inserted to model a harmonic structure of a speech waveform. Although harmonic convolution has been shown to perform well in audio restoration tasks, this convolution layer has not yet been fully explored in the field of speech synthesis. Therefore, we seek to improve the perceptual quality of speech samples synthesized by the waveform generation model and investigate the usefulness of harmonic convolution in the field of speech synthesis. Mean opinion score tests showed that the Harmonic WaveGAN can synthesize more natural speech than conventional Parallel WaveGAN. We also showed that a spectrogram of a speech waveform showed a clearer harmonic structure when synthesized by our model than a speech waveform synthesized by the original Parallel WaveGAN",
    "keywords": [],
    "checked": true,
    "id": "bf4fcac7709ddbb7714ef5abc365d62c40d14aa4",
    "semantic_title": "harmonic wavegan: gan-based speech waveform generation model with harmonic structure discriminator",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21f_interspeech.html": {
    "title": "Fre-GAN: Adversarial Frequency-Consistent Audio Synthesis",
    "volume": "main",
    "abstract": "Although recent works on neural vocoder have improved the quality of synthesized audio, there still exists a gap between generated and ground-truth audio in frequency space. This difference leads to spectral artifacts such as hissing noise or reverberation, and thus degrades the sample quality. In this paper, we propose Fre-GAN which achieves frequency-consistent audio synthesis with highly improved generation quality. Specifically, we first present resolution-connected generator and resolution-wise discriminators, which help learn various scales of spectral distributions over multiple frequency bands. Additionally, to reproduce high-frequency components accurately, we leverage discrete wavelet transform in the discriminators. From our experiments, Fre-GAN achieves high-fidelity waveform generation with a gap of only 0.03 MOS compared to ground-truth audio while outperforming standard models in quality",
    "keywords": [],
    "checked": true,
    "id": "84ec81808829725c3f10dd3aa6902266acad308f",
    "semantic_title": "fre-gan: adversarial frequency-consistent audio synthesis",
    "citation_count": 37,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yang21e_interspeech.html": {
    "title": "GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis",
    "volume": "main",
    "abstract": "Recent advances in neural multi-speaker text-to-speech (TTS) models have enabled the generation of reasonably good speech quality with a single model and made it possible to synthesize the speech of a speaker with limited training data. Fine-tuning to the target speaker data with the multi-speaker model can achieve better quality, however, there still exists a gap compared to the real speech sample and the model depends on the speaker. In this work, we propose GANSpeech, which is a high-fidelity multi-speaker TTS model that adopts the adversarial training method to a non-autoregressive multi-speaker TTS model. In addition, we propose simple but efficient automatic scaling methods for feature matching loss used in adversarial training. In the subjective listening tests, GANSpeech significantly outperformed the baseline multi-speaker FastSpeech and FastSpeech2 models, and showed a better MOS score than the speaker-specific fine-tuned FastSpeech2",
    "keywords": [],
    "checked": true,
    "id": "448ffadbc5d3249cdf1ad4848d73c98cac849580",
    "semantic_title": "ganspeech: adversarial training for high-fidelity multi-speaker speech synthesis",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jang21_interspeech.html": {
    "title": "UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation",
    "volume": "main",
    "abstract": "Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch",
    "keywords": [],
    "checked": true,
    "id": "fca7957c0b503f4319d6a830a24417239e0c5f09",
    "semantic_title": "univnet: a neural vocoder with multi-resolution spectrogram discriminators for high-fidelity waveform generation",
    "citation_count": 59,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alradhi21_interspeech.html": {
    "title": "Continuous Wavelet Vocoder-Based Decomposition of Parametric Speech Waveform Synthesis",
    "volume": "main",
    "abstract": "To date, various speech technology systems have adopted the vocoder approach, a method for synthesizing speech waveform that shows a major role in the performance of statistical parametric speech synthesis. However, conventional source-filter systems (i.e., STRAIGHT) and sinusoidal models (i.e., MagPhase) tend to produce over-smoothed spectra, which often result in muffled and buzzy synthesized text-to-speech (TTS). WaveNet, one of the best models that nearly resembles the human voice, has to generate a waveform in a time-consuming sequential manner with an extremely complex structure of its neural networks. WaveNet needs large quantities of voice data before accurate predictions can be obtained. In order to motivate a new, alternative approach to these issues, we present an updated synthesizer, which is a simple signal model to train and easy to generate waveforms, using Continuous Wavelet Transform (CWT) to characterize and decompose speech features. CWT provides time and frequency resolutions different from those of the short-time Fourier transform. It can also retain the fine spectral envelope and achieve high controllability of the structure closer to human auditory scales. We confirmed through experiments that our speech synthesis system was able to provide natural-sounding synthetic speech and outperformed the state-of-the-art WaveNet vocoder",
    "keywords": [],
    "checked": true,
    "id": "cfd817e29038f572c5c06321289f094af038aaf7",
    "semantic_title": "continuous wavelet vocoder-based decomposition of parametric speech waveform synthesis",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tobing21_interspeech.html": {
    "title": "High-Fidelity and Low-Latency Universal Neural Vocoder Based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling",
    "volume": "main",
    "abstract": "This paper presents a novel high-fidelity and low-latency universal neural vocoder framework based on multiband WaveRNN with data-driven linear prediction for discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN architecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit with a relatively large size of hidden units is utilized, while the multiband modeling is deployed to achieve real-time low-latency usage. A novel technique for data-driven linear prediction (LP) with discrete waveform modeling is proposed, where the LP coefficients are estimated in a data-driven manner. Moreover, a novel loss function using short-time Fourier transform (STFT) for discrete waveform modeling with Gumbel approximation is also proposed. The experimental results demonstrate that the proposed MWDLP framework generates high-fidelity synthetic speech for seen and unseen speakers and/or language on 300 speakers training data including clean and noisy/reverberant conditions, where the number of training utterances is limited to 60 per speaker, while allowing for real-time low-latency processing using a single core of ~2.1–2.7 GHz CPU with ~0.57–0.64 real-time factor including input/output and feature extraction",
    "keywords": [],
    "checked": true,
    "id": "2a4c1360e6f3845d889c5bec710b24de2c52bfd2",
    "semantic_title": "high-fidelity and low-latency universal neural vocoder based on multiband wavernn with data-driven linear prediction for discrete waveform modeling",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21h_interspeech.html": {
    "title": "Basis-MelGAN: Efficient Neural Vocoder Based on Audio Decomposition",
    "volume": "main",
    "abstract": "Recent studies have shown that neural vocoders based on generative adversarial network (GAN) can generate audios with high quality. While GAN based neural vocoders have shown to be computationally much more efficient than those based on autoregressive predictions, the real-time generation of the highest quality audio on CPU is still a very challenging task. One major computation of all GAN-based neural vocoders comes from the stacked upsampling layers, which were designed to match the length of the waveform's length of output and temporal resolution. Meanwhile, the computational complexity of upsampling networks is closely correlated with the numbers of samples generated for each window. To reduce the computation of upsampling layers, we propose a new GAN based neural vocoder called Basis-MelGAN where the raw audio samples are decomposed with a learned basis and their associated weights. As the prediction targets of Basis-MelGAN are the weight values associated with each learned basis instead of the raw audio samples, the upsampling layers in Basis-MelGAN can be designed with much simpler networks. Compared with other GAN based neural vocoders, the proposed Basis-MelGAN could produce comparable high-quality audio but significantly reduced computational complexity from HiFi-GAN V1's 17.74 GFLOPs to 7.95 GFLOPs",
    "keywords": [],
    "checked": true,
    "id": "29b22b8620772db73b9746b26849d7be803e2c63",
    "semantic_title": "basis-melgan: efficient neural vocoder based on audio decomposition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hwang21_interspeech.html": {
    "title": "High-Fidelity Parallel WaveGAN with Multi-Band Harmonic-Plus-Noise Model",
    "volume": "main",
    "abstract": "This paper proposes a multi-band harmonic-plus-noise (HN) Parallel WaveGAN (PWG) vocoder. To generate a high-fidelity speech signal, it is important to well-reflect the harmonic-noise characteristics of the speech waveform in the time-frequency domain. However, it is difficult for the conventional PWG model to accurately match this condition, as its single generator inefficiently represents the complicated nature of harmonic-noise structures. In the proposed method, the HN WaveNet models are employed to overcome this limitation, which enable the separate generation of the harmonic and noise components of speech signals from the pitch-dependent sine wave and Gaussian noise sources, respectively. Then, the energy ratios between harmonic and noise components in multiple frequency bands (i.e., subband harmonicities) are predicted by an additional harmonicity estimator. Weighted by the estimated harmonicities, the gain of harmonic and noise components in each subband is adjusted, and finally mixed together to compose the full-band speech signal. Subjective evaluation results showed that the proposed method significantly improved the perceptual quality of the synthesized speech",
    "keywords": [],
    "checked": true,
    "id": "0eff82591224a1f3914f1784f4ac114bb7d1e412",
    "semantic_title": "high-fidelity parallel wavegan with multi-band harmonic-plus-noise model",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21i_interspeech.html": {
    "title": "SpecRec: An Alternative Solution for Improving End-to-End Speech-to-Text Translation via Spectrogram Reconstruction",
    "volume": "main",
    "abstract": "End-to-end Speech-to-text Translation (E2E-ST), which directly translates source language speech to target language text, is widely useful in practice, but traditional cascaded approaches (ASR+MT) often suffer from error propagation in the pipeline. On the other hand, existing end-to-end solutions heavily depend on the source language transcriptions for pre-training or multi-task training with Automatic Speech Recognition (ASR). We instead propose a simple technique to learn a robust speech encoder in a self-supervised fashion only on the speech side, which can utilize speech data without transcription. This technique termed Spectrogram Reconstruction (SpecRec), learns better speech representation via recovering the missing speech frames and provides an alternative solution to improving E2E-ST. We conduct our experiments over 8 different translation directions. In the setting without using any transcriptions, our technique achieves an average improvement of +1.1 BLEU. SpecRec also improves the translation accuracy with +0.7 BLEU over the baseline in speech translation with ASR multitask training setting",
    "keywords": [],
    "checked": true,
    "id": "7f44267ab6e3b4139aa26fdfe065f86c44a9c290",
    "semantic_title": "specrec: an alternative solution for improving end-to-end speech-to-text translation via spectrogram reconstruction",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cherry21_interspeech.html": {
    "title": "Subtitle Translation as Markup Translation",
    "volume": "main",
    "abstract": "Automatic subtitle translation is an important technology to make video content available across language barriers. Subtitle translation complicates the normal translation problem by adding the challenge of how to format the system output into subtitles. We propose a simple technique that treats subtitle translation as standard sentence translation plus alignment driven markup transfer, which enables us to reliably maintain timing and formatting information from the source subtitles. We also introduce two metrics to measure the quality of subtitle boundaries: a Timed BLEU that penalizes mistimed tokens with respect to a reference subtitle sequence, and a measure of how much Timed BLEU is lost due to suboptimal subtitle boundary placement. In experiments on TED and YouTube subtitles, we show that we are able to achieve much better translation quality than a baseline that translates each subtitle independently, while coming very close to optimal subtitle boundary placement",
    "keywords": [],
    "checked": true,
    "id": "079bc84fee1147b19cfdf76d54d37b35205c3244",
    "semantic_title": "subtitle translation as markup translation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21r_interspeech.html": {
    "title": "Large-Scale Self- and Semi-Supervised Learning for Speech Translation",
    "volume": "main",
    "abstract": "In this paper, we improve speech translation (ST) through effectively leveraging large quantities of unlabeled speech and text data in different and complementary ways. We explore both pretraining and self-training by using the large Libri-Light speech audio corpus and language modeling with CommonCrawl. Our experiments improve over the previous state of the art by 2.8 BLEU on average on all four considered CoVoST 2 language pairs via a simple recipe of combining wav2vec 2.0 pretraining, a single iteration of self-training and decoding with a language model. Different from existing work, our approach does not leverage any other supervision than ST data. Code and models are publicly released",
    "keywords": [],
    "checked": true,
    "id": "acc2b9493bfa4fcdd3e683a2a83eb61f1a5fc2e9",
    "semantic_title": "large-scale self- and semi-supervised learning for speech translation",
    "citation_count": 34,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21s_interspeech.html": {
    "title": "CoVoST 2 and Massively Multilingual Speech Translation",
    "volume": "main",
    "abstract": "Speech translation (ST) is an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research into massive multilingual ST and ST for low resource languages, we release CoVoST 2, a large-scale multilingual ST corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date for volume and language coverage. Data checks provide evidence about the data quality. We provide extensive speech recognition (ASR), machine translation (MT) and ST baselines. We demonstrate the value of CoVoST 2 for multilingual ST research by leveraging it in 4 investigations: simplify multilingual training by removing ASR pretraining, study multilingual model scaling properties and investigate zero-shot and transfer learning capabilities of models trained on CoVoST 2",
    "keywords": [],
    "checked": true,
    "id": "a79c75e65ddbd9a652b6ee8b364f2880d35cabb7",
    "semantic_title": "covost 2 and massively multilingual speech translation",
    "citation_count": 48,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheng21_interspeech.html": {
    "title": "AlloST: Low-Resource Speech Translation Without Source Transcription",
    "volume": "main",
    "abstract": "The end-to-end architecture has made promising progress in speech translation (ST). However, the ST task is still challenging under low-resource conditions. Most ST models have shown unsatisfactory results, especially in the absence of word information from the source speech utterance. In this study, we survey methods to improve ST performance without using source transcription, and propose a learning framework that utilizes a language-independent universal phone recognizer. The framework is based on an attention-based sequence-to-sequence model, where the encoder generates the phonetic embeddings and phone-aware acoustic representations, and the decoder controls the fusion of the two embedding streams to produce the target token sequence. In addition to investigating different fusion strategies, we explore the specific usage of byte pair encoding (BPE), which compresses a phone sequence into a syllable-like segmented sequence. Due to the conversion of symbols, a segmented sequence represents not only pronunciation but also language-dependent information lacking in phones. Experiments conducted on the Fisher Spanish-English and Taigi-Mandarin drama corpora show that our method outperforms the conformer-based baseline, and the performance is close to that of the existing best method using source transcription",
    "keywords": [],
    "checked": true,
    "id": "7c54b8abe47978f418b4fc56e4a64dba69fddfed",
    "semantic_title": "allost: low-resource speech translation without source transcription",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/effendi21_interspeech.html": {
    "title": "Weakly-Supervised Speech-to-Text Mapping with Visually Connected Non-Parallel Speech-Text Data Using Cyclic Partially-Aligned Transformer",
    "volume": "main",
    "abstract": "Despite the successful development of automatic speech recognition (ASR) systems for several of the world's major languages, they require a tremendous amount of parallel speech-text data. Unfortunately, for many other languages, such resources are usually unavailable. This study addresses the speech-to-text mapping problem given only a collection of visually connected non-parallel speech-text data. We call this \"mapping\" since the system attempts to learn the semantic association between speech and text instead of recognizing the speech with the exact word-by-word transcription. Here, we propose utilizing our novel cyclic partially-aligned Transformer with two-fold mechanisms. First, we train a Transformer-based vector-quantized variational autoencoder (VQ-VAE) to produce a discrete speech representation in a self-supervised manner. Then, we use a Transformer-based sequence-to-sequence model inside a chain mechanism to map from unknown untranscribed speech utterances into a semantically equivalent text. Because this is not strictly recognizing speech, we focus on evaluating the semantic equivalence of the generated text hypothesis. Our evaluation shows that our proposed method is also effective for a multispeaker natural speech dataset and can also be applied for a cross-lingual application",
    "keywords": [],
    "checked": true,
    "id": "2239bb03688d3d1aa71280c5142693402cbe91a8",
    "semantic_title": "weakly-supervised speech-to-text mapping with visually connected non-parallel speech-text data using cyclic partially-aligned transformer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tokuyama21_interspeech.html": {
    "title": "Transcribing Paralinguistic Acoustic Cues to Target Language Text in Transformer-Based Speech-to-Text Translation",
    "volume": "main",
    "abstract": "In spoken communication, a speaker may convey their message in words (linguistic cues) with supplemental information (paralinguistic cues) such as emotion and emphasis. Transforming all spoken information into a written or verbal form is not trivial, especially if the transformation has to be done across languages. Most existing speech-to-text translation systems focus only on translating linguistic information while ignoring paralinguistic information. A few recent studies that proposed paralinguistic translation used a machine translation with hidden Markov model (HMM)-based automatic speech recognition (ASR) and text-to-speech (TTS) that were complicated and suboptimal. Furthermore, paralinguistic information was kept in the acoustic form. Here, we focused on transcribing paralinguistic acoustic cues of emphasis in the target language text. Specifically, we constructed cascade and direct neural Transformer-based speech-to-text translation, and we investigated various methods of expressing emphasis information in the written form of the target language. We performed our experiments on a Japanese-to-English linguistic and paralinguistic speech-to-text translation framework. The results revealed that our proposed method can translate both linguistic and paralinguistic information while keeping the performance as in standard linguistic translation",
    "keywords": [],
    "checked": true,
    "id": "d7bc9c2a1e466e5c226324c55ea73e38f9307581",
    "semantic_title": "transcribing paralinguistic acoustic cues to target language text in transformer-based speech-to-text translation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ye21_interspeech.html": {
    "title": "End-to-End Speech Translation via Cross-Modal Progressive Training",
    "volume": "main",
    "abstract": "End-to-end speech translation models have become a new trend in research due to their potential of reducing error propagation. However, these models still suffer from the challenge of data scarcity. How to effectively use unlabeled or other parallel corpora from machine translation is promising but still an open problem. In this paper, we propose Cross Speech-Text Network (XSTNet), an end-to-end model for speech-to-text translation. XSTNet takes both speech and text as input and outputs both transcription and translation text. The model benefits from its three key design aspects: a self-supervised pre-trained sub-network as the audio encoder, a multi-task training objective to exploit additional parallel bilingual text, and a progressive training procedure. We evaluate the performance of XSTNet and baselines on the MuST-C En-X and LibriSpeech En-Fr datasets. In particular, XSTNet achieves state-of-the-art results on all language directions with an average BLEU of 28.8, outperforming the previous best method by 3.2 BLEU. Code, models, cases, and more detailed analysis are publicly available",
    "keywords": [],
    "checked": true,
    "id": "156f761af850802ac5b67e56df136284980117c9",
    "semantic_title": "end-to-end speech translation via cross-modal progressive training",
    "citation_count": 50,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ko21_interspeech.html": {
    "title": "ASR Posterior-Based Loss for Multi-Task End-to-End Speech Translation",
    "volume": "main",
    "abstract": "End-to-end speech translation (ST) translates source language speech directly into target language without an intermediate automatic speech recognition (ASR) output, as in a cascading approach. End-to-end ST has the advantage of avoiding error propagation from the intermediate ASR results, but its performance still lags behind the cascading approach. A recent effort to increase performance is multi-task learning using an auxiliary task of ASR. However, previous multi-task learning for end-to-end ST using cross entropy (CE) loss in ASR-task targets one-hot references and does not consider ASR confusion. In this study, we propose a novel end-to-end ST training method using ASR loss against ASR posterior distributions given by a pre-trained model, which we call ASR posterior-based loss. The proposed method is expected to consider possible ASR confusion due to competing hypotheses with similar pronunciations. The proposed method demonstrated better BLEU results in our Fisher Spanish-to-English translation experiments than the baseline with standard CE loss with label smoothing",
    "keywords": [],
    "checked": true,
    "id": "73cb27cd93a3c58be2d1521be127770e016ace20",
    "semantic_title": "asr posterior-based loss for multi-task end-to-end speech translation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/perezgonzalezdemartos21_interspeech.html": {
    "title": "Towards Simultaneous Machine Interpretation",
    "volume": "main",
    "abstract": "Automatic speech-to-speech translation (S2S) is one of the most challenging speech and language processing tasks, especially when considering its application to real-time settings. Recent advances on streaming Automatic Speech Recognition (ASR), simultaneous Machine Translation (MT) and incremental neural Text-To-Speech (TTS) make it possible to develop real-time cascade S2S systems with greatly improved accuracy. On the way to simultaneous machine interpretation, a state-of-the-art cascade streaming S2S system is described and empirically assessed in the simultaneous interpretation of European Parliament debates. We pay particular attention to the TTS component, particularly in terms of speech naturalness under a variety of response-time settings, as well as in terms of speaker similarity for its cross-lingual voice cloning capabilities",
    "keywords": [],
    "checked": true,
    "id": "dfca6a5c40a4a3e88353902f20d3e3a69de8116a",
    "semantic_title": "towards simultaneous machine interpretation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/martucci21_interspeech.html": {
    "title": "Lexical Modeling of ASR Errors for Robust Speech Translation",
    "volume": "main",
    "abstract": "Error propagation from automatic speech recognition (ASR) to machine translation (MT) is a critical issue for the (still) dominant approach to speech translation. To robustify MT to ill-formed inputs, we propose a technique to artificially corrupt clean transcripts so as to emulate noisy automatic transcripts. Our model relies on estimating from ASR data: i) the probability distribution of the possible edit operations applicable to each word, and ii) the probability distribution of possible lexical substitutes for that word. Corrupted data generated from these probabilities are paired with their original clean counterpart for MT adaptation via fine-tuning. Contrastive experiments on three language pairs led to three main findings. First, on noisy transcripts, the adapted models outperform MT systems fine-tuned on synthetic data corrupted with previous noising techniques, approaching the upper bound performance obtained by fine-tuning on real ASR data. Second, the increased robustness does not come at the cost of performance drops on clean test data. Third, and crucial from the application standpoint, our approach is domain/ASR-independent: noising patterns learned from a given ASR system in a certain domain can be successfully applied to robustify MT to errors made by other ASR systems in a different domain",
    "keywords": [],
    "checked": true,
    "id": "3dc4a157ac85d75ac0fef3dada3888c18d5d932f",
    "semantic_title": "lexical modeling of asr errors for robust speech translation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vyas21_interspeech.html": {
    "title": "Optimally Encoding Inductive Biases into the Transformer Improves End-to-End Speech Translation",
    "volume": "main",
    "abstract": "Transformer-based encoder-decoder architectures have recently shown promising results in end-to-end speech translation. However, the content-based attention mechanism employed by the Transformer was designed for text sequences and can only encode global inductive bias, that alone is not sufficient for learning good representations from speech signals. In this work, we address this by putting architectural constraints on the Transformer to allow encoding of both local and global inductive biases. This is accomplished by replacing the Transformer encoder with a Conformer encoder that, in contrast to the Transformer encoder, employs convolution in addition to self-attention and feed-forward. As a result, the new model named Conformer-Transformer has an encoder that captures both local feature correlations and long-range dependencies from speech signals. Experiments on seven non-English to English language directions show that the Conformer-Transformer, compared to strong Transformer-based baselines, achieves up to 3.54 BLEU score improvements with a pre-trained encoder and up to 10.53 BLEU score improvements when trained from scratch",
    "keywords": [],
    "checked": true,
    "id": "b426ffd7741d46972dff1a41b8064aee47e6f325",
    "semantic_title": "optimally encoding inductive biases into the transformer improves end-to-end speech translation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ananthanarayana21_interspeech.html": {
    "title": "Effects of Feature Scaling and Fusion on Sign Language Translation",
    "volume": "main",
    "abstract": "Sign language translation without transcription has only recently started to gain attention. In our work, we focus on improving the state-of-the-art translation by introducing a multi-feature fusion architecture with enhanced input features. As sign language is challenging to segment, we obtain the input features by extracting overlapping scaled segments across the video and obtaining their 3D CNN representations. We exploit the attention mechanism in the fusion architecture by initially learning dependencies between different frames of the same video and later fusing them to learn the relations between different features from the same video. In addition to 3D CNN features, we also analyze pose-based features Our robust methodology outperforms the state-of-the-art sign language translation model by achieving higher BLEU 3 – BLEU 4 scores and also outperforms the state-of-the-art sequence attention models by achieving a 43.54% increase in BLEU 4 score. We conclude that the combined effects of feature scaling and feature fusion make our model more robust in predicting longer n-grams which are crucial in continuous sign language translation",
    "keywords": [],
    "checked": true,
    "id": "6d1ce07da7548a008fa293197b368f4cab5293dc",
    "semantic_title": "effects of feature scaling and fusion on sign language translation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alenin21_interspeech.html": {
    "title": "The ID R&D System Description for Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "This paper describes ID R&D team submission to the text-independent task of the Short-duration Speaker Verification (SdSV) Challenge 2021. The top performed system is a fusion of 9 Convolutional Neural Networks based on the ResNet architecture. Experiments' results of optimal NN architecture search are shown. We also present and investigate the subnetwork approach to solve the auxiliary tasks such as gender or language detection. Verification scores refinement step using quality measurements of a trial pair allowed to further minimize the target metrics. A comparative analysis of all systems used in the fusion has been provided on the VoxCeleb-1 test set, SdSV-2021 development and evaluation sets. The final submission achieves 0.69% EER and 0.0319 minDCF on the challenge evaluation set",
    "keywords": [],
    "checked": true,
    "id": "14b0276af88c3906c6ca3756216c2e52fe70e344",
    "semantic_title": "the id r&d system description for short-duration speaker verification challenge 2021",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/thienpondt21_interspeech.html": {
    "title": "Integrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification",
    "volume": "main",
    "abstract": "This paper describes the IDLab submission for the text-independent task of the Short-duration Speaker Verification Challenge 2021 (SdSVC-21). This speaker verification competition focuses on short duration test recordings and cross-lingual trials, along with the constraint of limited availability of in-domain DeepMine Farsi training data. Currently, both Time Delay Neural Networks (TDNNs) and ResNets achieve state-of-the-art results in speaker verification. These architectures are structurally very different and the construction of hybrid networks looks a promising way forward. We introduce a 2D convolutional stem in a strong ECAPA-TDNN baseline to transfer some of the strong characteristics of a ResNet based model to this hybrid CNN-TDNN architecture. Similarly, we incorporate absolute frequency positional encodings in an SE-ResNet34 architecture. These learnable feature map biases along the frequency axis offer this architecture a straightforward way to exploit frequency positional information. We also propose a frequency-wise variant of Squeeze-Excitation (SE) which better preserves frequency-specific information when rescaling the feature maps. Both modified architectures significantly outperform their corresponding baseline on the SdSVC-21 evaluation data and the original VoxCeleb1 test set. A four system fusion containing the two improved architectures achieved a third place in the final SdSVC-21 Task 2 ranking",
    "keywords": [],
    "checked": true,
    "id": "9a25a9476688026a18050c7475c7a1492f7915d0",
    "semantic_title": "integrating frequency translational invariance in tdnns and frequency positional information in 2d resnets to enhance speaker verification",
    "citation_count": 50,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gusev21_interspeech.html": {
    "title": "SdSVC Challenge 2021: Tips and Tricks to Boost the Short-Duration Speaker Verification System Performance",
    "volume": "main",
    "abstract": "This paper presents speaker recognition (SR) systems for the text-independent speaker verification under the cross-lingual (English vs Persian) task (task 2) of the Short-duration Speaker Verification Challenge (SdSVC) 2021 We present the description of applied ResNet-like and ECAPA-TDNN-like topology design solutions as well as an analysis of multi-session scoring techniques benchmarked on the SdSVC challenge datasets. We overview various modifications of the basic ResNet-like architecture and training strategies, allowing us to obtain the improved quality of speaker verification. Also, we introduce the alpha query expansion-based technique (αQE) to the enrollment embeddings aggregation at test time, which results in a 0.042 minDCF improvement from 0.12 to 0.078 for the ECAPA-TDNN system compared to the embeddings mean. We also propose a trial-level distance-based non-parametric imposter/target detector (KrTC) used to filter out the worst enrollment samples at test time to further improve the performance of the system",
    "keywords": [],
    "checked": true,
    "id": "976292bcdfb7ee6fde9d248656429f582347659b",
    "semantic_title": "sdsvc challenge 2021: tips and tricks to boost the short-duration speaker verification system performance",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kang21_interspeech.html": {
    "title": "Team02 Text-Independent Speaker Verification System for SdSV Challenge 2021",
    "volume": "main",
    "abstract": "In this paper, we provide description of our submitted systems to the Short Duration Speaker Verification (SdSV) Challenge 2021 Task 2. The challenge provides a difficult set of cross-language text-independent speaker verification trials. Our submissions employ ResNet-based embedding networks which are trained using various strategies exploiting both in-domain and out-of-domain datasets. The results show that using the recently proposed joint factor embedding (JFE) scheme can enhance the performance by disentangling the language-dependent information from the speaker embedding. However, upon analyzing the speaker embeddings, it was found that there exists a clear discrepancy between the in-domain and out-of-domain datasets. Therefore, among our submitted systems, the best performance was achieved by pre-training the embedding system using out-of-domain dataset and fine-tuning it with only the in-domain data, which resulted in a MinDCF of 0.142716 on the SdSV2021 evaluation set",
    "keywords": [],
    "checked": true,
    "id": "7b42fedaa8f3d8747e3d23cb4f66db53b2955fd8",
    "semantic_title": "team02 text-independent speaker verification system for sdsv challenge 2021",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qin21_interspeech.html": {
    "title": "Our Learned Lessons from Cross-Lingual Speaker Verification: The CRMI-DKU System Description for the Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "In this paper, we present our CRMI-DKU system description for the Short-duration Speaker Verification Challenge (SdSVC) 2021. We introduce the whole pipeline of our cross-lingual speaker verification system, including data preprocessing, training strategy, utterance-level speaker embedding extractor, domain-adaptation, and score calibration. We also propose methods to learn language-invariant features and perform domain adaptation to reduce the cross-lingual mismatch. In addition, we explore a semi-supervised method to utilize the unlabeled training data. The final submitted score level fusion system achieves 0.0476 minDCF and 0.98% EER on the evaluation set",
    "keywords": [],
    "checked": true,
    "id": "e679eace726c886c85bd463d6b62a86d51f892b6",
    "semantic_title": "our learned lessons from cross-lingual speaker verification: the crmi-dku system description for the short-duration speaker verification challenge 2021",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21o_interspeech.html": {
    "title": "Investigation of IMU&Elevoc Submission for the Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "In this paper, we present the IMU&Elevoc systems submitted to the Short-duration Verification Challenge (SdSVC) 2021. Our submissions focus on both text-dependent speaker verification (Task 1) and text-independent speaker verification (Task 2). First, we investigate several frame-level feature extractor architectures based on ResNet, Res2Net and TDNN. Then, we integrate Squeeze-Excitation block and dimension cardinality to further improve the Res2Net-based backbone network. In particular, we probe an effective transfer learning strategy that overcomes the lack of Task 1 datasets and improves in-domain performance. A knowledge distillation method fusing multiple models is proposed to obtain a stronger single model. Experimental results on the SdSVC 2021 show that our primary system yields 0.0500MinDCF in Task 1 (ranked as 4th) and 0.0448 MinDCF in Task 2 (ranked as 6th)",
    "keywords": [],
    "checked": true,
    "id": "5f70d1db897e68a8b1d71504b67dea8d93508893",
    "semantic_title": "investigation of imu&elevoc submission for the short-duration speaker verification challenge 2021",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21_interspeech.html": {
    "title": "The Sogou System for Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "In this paper we present our system for the task 2 of the Short-duration Speaker Verification (SdSV) Challenge 2021. This task focuses on benchmarking and varying degrees of phonetic variability analysis of short-duration speaker recognition system. The main difficulty exists in the variance between cross-lingual trials, along with the limited in-domain Farsi training data. Based on the state-of-the-art ResNetSE speaker embedding network, we propose a novel network architecture with in-domain data finetuning and novel scoring methods, and achieve significant improvement over the ResNetSE baselines. Furthermore, score calibration on duration efficiently improve the robustness. Finally, our system with fusion of 10 subsystems achieve satisfying results in MinDCF and EER of 0.0394 and 0.84% respectively on the SdSVC evaluation set",
    "keywords": [],
    "checked": false,
    "id": "111e60bb33b56059dc421757ecae836396c0ebea",
    "semantic_title": "the sjtu system for short-duration speaker verification challenge 2021",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21c_interspeech.html": {
    "title": "The SJTU System for Short-Duration Speaker Verification Challenge 2021",
    "volume": "main",
    "abstract": "This paper presents the SJTU system for both text-dependent and text-independent tasks in short-duration speaker verification (SdSV) challenge 2021. In this challenge, we explored different strong embedding extractors to extract robust speaker embedding. For text-independent task, language-dependent adaptive snorm is explored to improve the system performance under the cross-lingual verification condition. For text-dependent task, we mainly focus on the in-domain fine-tuning strategies based on the model pre-trained on large-scale out-of-domain data. In order to improve the distinction between different speakers uttering the same phrase, we proposed several novel phrase-aware fine-tuning strategies and phrase-aware neural PLDA. With such strategies, the system performance is further improved. Finally, we fused the scores of different systems, and our fusion systems achieved 0.0473 in Task1 (rank 3) and 0.0581 in Task2 (rank 8) on the primary evaluation metric",
    "keywords": [],
    "checked": true,
    "id": "111e60bb33b56059dc421757ecae836396c0ebea",
    "semantic_title": "the sjtu system for short-duration speaker verification challenge 2021",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cho21_interspeech.html": {
    "title": "Multi-Speaker Emotional Text-to-Speech Synthesizer",
    "volume": "main",
    "abstract": "We present a methodology to train our multi-speaker emotional text-to-speech synthesizer that can express speech for 10 speakers' 7 different emotions. All silences from audio samples are removed prior to learning. This results in fast learning by our model. Curriculum learning is applied to train our model efficiently. Our model is first trained with a large single-speaker neutral dataset, and then trained with neutral speech from all speakers. Finally, our model is trained using datasets of emotional speech from all speakers. In each stage, training samples of each speaker-emotion pair have equal probability to appear in mini-batches. Through this procedure, our model can synthesize speech for all targeted speakers and emotions. Our synthesized audio sets are available on our web page",
    "keywords": [],
    "checked": true,
    "id": "94842f30187a024dadfb201c5522467406b751cc",
    "semantic_title": "multi-speaker emotional text-to-speech synthesizer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/prazak21_interspeech.html": {
    "title": "Live TV Subtitling Through Respeaking",
    "volume": "main",
    "abstract": "In this paper, we describe our solution for live TV subtitling. The subtitling system uses the respeaking concept with respeakers closely tied with the automatic speech recognition system. The ASR is specially tailored to the live subtitling task by using respeaker-specific acoustic models and TV-show-dependent language models. The output stream of ASR could be online modified by keyboard shortcuts controlled by the respeaker. The whole subtitling service is used by Czech Television to provide high-quality subtitles of live shows for people with hearing impairments",
    "keywords": [],
    "checked": true,
    "id": "5a7ffb365fd1219ea544ff0dc80cb937ed20a5fd",
    "semantic_title": "live tv subtitling through respeaking",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fragner21_interspeech.html": {
    "title": "Autonomous Robot for Measuring Room Impulse Responses",
    "volume": "main",
    "abstract": "Far-field speech recognition for e.g. home automation or smart assistants has to cope with moving speakers in reverberant environments. Simulating stationary or even moving speakers in realistic environments enables to make speech processing technology more robust. This paper introduces an autonomous robot for recording a database of Room Impulse Responses (RIRs) at a high spatial resolution. This supports the creation of realistic simulation environments. These RIRs can be exploited to generate multi-channel speech mixtures of static or moving speakers for various applications",
    "keywords": [],
    "checked": true,
    "id": "70cc73bd3ab97e3d3532eb80e2f7da2ad2772992",
    "semantic_title": "autonomous robot for measuring room impulse responses",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/beskow21_interspeech.html": {
    "title": "Expressive Robot Performance Based on Facial Motion Capture",
    "volume": "main",
    "abstract": "The Furhat robot is a social robot that uses facial projection technology to achieve a high degree of expressivity and flexibility. In this demonstration, we will present new features that takes this facial expressiveness further. A new face engine for the robot is presented which not only drastically improves the visual fidelity of the face and the eyes, it also adds increased flexibility when it comes to designing new robotic characters as well as modifying existing ones. Most importantly, we will present a new toolset and a workflow that allows users to record their own face motion and incorporate them into skills (i.e. custom robot applications) as gestures, prompts or entire canned performances",
    "keywords": [],
    "checked": true,
    "id": "5fd49150baaae91c0818352bf644cc28defa6d85",
    "semantic_title": "expressive robot performance based on facial motion capture",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dominguez21_interspeech.html": {
    "title": "ThemePro 2.0: Showcasing the Role of Thematic Progression in Engaging Human-Computer Interaction",
    "volume": "main",
    "abstract": "Structuring speech into informative units is certainly a desirable feature in efficient human-machine communication. This paper introduces ThemePro 2.0, a toolkit that pre-processes long monologues into smaller cohesive units to be consumed by the text-to-speech module within a conversational agent. The methodology used is based upon the text's discourse structure modelled as thematic progression patterns. As shown in the demonstration, thematic progression modelling captures the underlying information structure at the discourse level and is, therefore, instrumental for cohesive speech output in the TTS component",
    "keywords": [],
    "checked": true,
    "id": "b556331c5f0c39d408d91f7fa97f80206bf5b00d",
    "semantic_title": "themepro 2.0: showcasing the role of thematic progression in engaging human-computer interaction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guruju21_interspeech.html": {
    "title": "Addressing Compliance in Call Centers with Entity Extraction",
    "volume": "main",
    "abstract": "Call centers record and store customer-agent conversations for the purpose of coaching, quality assurance and to comply with Good amount of these audio recordings contain sensitive information pertaining to their customers' financial or personal details. To ensure data security, compliance and to reduce the risk of abuse/theft, it becomes important to identify such instances in audio recordings and mask these segments. To automate this process, we propose a cascaded system; first, Automatic Speech Recognition (ASR) generates transcript and text-to-audio alignment information for an audio recording. Then, Entity Extraction is performed on generated transcripts to identify and locate sensitive information, and the corresponding sensitive segments are masked in audio recordings using alignment information. We introduce a novel system for selective masking of sensitive information in both audio and transcript",
    "keywords": [],
    "checked": true,
    "id": "c5ce43dfb72f736f2b8cb552d07503a8c5698a51",
    "semantic_title": "addressing compliance in call centers with entity extraction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gogineni21_interspeech.html": {
    "title": "Audio Segmentation Based Conversational Silence Detection for Contact Center Calls",
    "volume": "main",
    "abstract": "In a typical contact-center call, more than 35% of the call has neither the contact-center agent nor the customer speaking, we usually refer to such areas in the call as comprise mostly of hold-music, automatic-recorded-messages, or just silences when the agent or customer is engaged in some off-call work. Most of these conversational silences negatively affect important KPIs for call-centers, like dead-airs affect customer satisfaction, long-holds affect average call handling time and so on. In this paper we showcase how Observe.AI helps contact-centers identify agents who are breaching accepted levels of conversational silences by using an in-house Audio Segmenter system paired with an NLP system to classify the contexts around these This solution is provided by Observe.AI to hundreds of contact centers who use it to improve their average call handling time and customer satisfaction scores",
    "keywords": [],
    "checked": true,
    "id": "9b40ea83349fb89b5bc02dfcc6366d4c234203f2",
    "semantic_title": "audio segmentation based conversational silence detection for contact center calls",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/raj21b_interspeech.html": {
    "title": "Reformulating DOVER-Lap Label Mapping as a Graph Partitioning Problem",
    "volume": "main",
    "abstract": "We recently proposed DOVER-Lap, a method for combining overlap-aware speaker diarization system outputs. DOVER-Lap improved upon its predecessor DOVER by using a label mapping method based on globally-informed greedy search. In this paper, we analyze this label mapping in the framework of a maximum orthogonal graph partitioning problem, and present three inferences. First, we show that DOVER-Lap label mapping is exponential in the input size, which poses a challenge when combining a large number of hypotheses. We then revisit the DOVER label mapping algorithm and propose a modification which performs similar to DOVER-Lap while being computationally tractable. We also derive an approximation bound for the algorithm in terms of the maximum number of hypotheses speakers. Finally, we describe a randomized local search algorithm which provides a near-optimal (1-ε)-approximate solution to the problem with high probability. We empirically demonstrate the effectiveness of our methods on the AMI meeting corpus. Our code is publicly available",
    "keywords": [],
    "checked": true,
    "id": "88dfc766aeff22a4e5fbdb81ce6161994c745039",
    "semantic_title": "reformulating dover-lap label mapping as a graph partitioning problem",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tak21_interspeech.html": {
    "title": "Graph Attention Networks for Anti-Spoofing",
    "volume": "main",
    "abstract": "The cues needed to detect spoofing attacks against automatic speaker verification are often located in specific spectral sub-bands or temporal segments. Previous works show the potential to learn these using either spectral or temporal self-attention mechanisms but not the relationships between neighbouring sub-bands or segments. This paper reports our use of graph attention networks (GATs) to model these relationships and to improve spoofing detection performance. GATs leverage a self-attention mechanism over graph structured data to model the data manifold and the relationships between nodes. Our graph is constructed from representations produced by a ResNet. Nodes in the graph represent information either in specific sub-bands or temporal segments. Experiments performed on the ASVspoof 2019 logical access database show that our GAT-based model with temporal attention outperforms all of our baseline single systems. Furthermore, GAT-based systems are complementary to a set of existing systems. The fusion of GAT-based models with more conventional countermeasures delivers a 47% relative improvement in performance compared to the best performing single GAT system",
    "keywords": [],
    "checked": true,
    "id": "100a7a618df82acc6a73642f01e1e14e07aa0ec3",
    "semantic_title": "graph attention networks for anti-spoofing",
    "citation_count": 33,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mingote21_interspeech.html": {
    "title": "Log-Likelihood-Ratio Cost Function as Objective Loss for Speaker Verification Systems",
    "volume": "main",
    "abstract": "Many recent studies in Speaker Verification (SV) have been focused on the design of the most appropriate training loss function, which plays an important role to improve the recognition ability of the systems. However, the verification loss functions created often do not take into account the performance measures which are used for the final system evaluation. For this reason, this paper presents an alternative approach to optimize the parameters of a neural network using a loss function based on the log-likelihood-ratio cost function (CLLR). This function is an application-independent metric that measures the cost of soft detection decisions over all the operating points. Thus, prior or relevance cost parameters assumptions are not employed to obtain it. Moreover, this metric has a differentiable expression, so no approximation is needed to use it as the objective loss to train a neural network. CLLR function as optimization loss was tested on the RSR2015-Part II database for text-dependent speaker verification, providing competitive results without using score normalization and outperforming other similar loss functions as Cross-Entropy combined with Ring Loss, as well as our previous loss function based on an approximation of the Detection Cost Function (DCF)",
    "keywords": [],
    "checked": true,
    "id": "44557620362be57e2ad7446f6fbdbab6ce027bfd",
    "semantic_title": "log-likelihood-ratio cost function as objective loss for speaker verification systems",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21c_interspeech.html": {
    "title": "Effective Phase Encoding for End-To-End Speaker Verification",
    "volume": "main",
    "abstract": "The widely used magnitude spectrum based features have shown their superiority in the field of speech processing. In contrast, the importance of phase spectrum is always ignored. This is because the patterns hidden in phase cannot be intuitively modelled and interpreted, due to phase wrapping phenomenon. In this paper, we explore novel phase spectrum based features, named Learnable Group Delay (LearnGD), to capture useful information in speech signals. Specifically, firstly, the negative of the spectral derivative of the phase spectrum, called group delay (GD), is used to unwrap the phase. Then, to suppress the spiky nature of GD, which is caused by its roots close to the unit circle in the Z domain, a carefully designed light convolutional smoothing layer is employed to reconstruct the GD. Finally, an exponential hyper-parameter is introduced to reconstruct GD features to restore the spectrum range and generate LearnGD features. For performance evaluation, speaker verification experiments are conducted on the VoxCeleb2 corpus. Compared to the traditional acoustic feature derived from the magnitude spectrum, the proposed phase-based features reach a 27.8% relative improvement in terms of EER. Furthermore, experimental results on TIMIT phoneme recognition task also demonstrate the effectiveness of our proposed phase-based features",
    "keywords": [],
    "checked": true,
    "id": "057795f354a2fbf7a2e39c15b0e52cdad2b64847",
    "semantic_title": "effective phase encoding for end-to-end speaker verification",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21d_interspeech.html": {
    "title": "Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "Boosted by the simultaneous translation shared task at IWSLT 2020, promising end-to-end online speech translation approaches were recently proposed. They consist in incrementally encoding a speech input (in a source language) and decoding the corresponding text (in a target language) with the best possible trade-off between latency and translation quality. This paper investigates two key aspects of end-to-end simultaneous speech translation: (a) how to encode efficiently the continuous speech flow, and (b) how to segment the speech flow in order to alternate optimally between reading (R: encoding input) and writing (W: decoding output) operations. We extend our previously proposed end-to-end online decoding strategy and show that while replacing BLSTM by ULSTM encoding degrades performance in offline mode, it actually improves both efficiency and performance in online mode. We also measure the impact of different methods to segment the speech signal (using fixed interval boundaries, oracle word boundaries or randomly set boundaries) and show that our best end-to-end online decoding strategy is surprisingly the one that alternates R/W operations on fixed size blocks on our English-German speech translation setup",
    "keywords": [],
    "checked": true,
    "id": "e7f4c3e62ed941c4d3ffd54fe5d3131b953b074a",
    "semantic_title": "impact of encoding and segmentation strategies on end-to-end simultaneous speech translation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/machacek21_interspeech.html": {
    "title": "Lost in Interpreting: Speech Translation from Source or Interpreter?",
    "volume": "main",
    "abstract": "Interpreters facilitate multi-lingual meetings but the affordable set of languages is often smaller than what is needed. Automatic simultaneous speech translation can extend the set of provided languages. We investigate if such an automatic system should rather follow the original speaker, or an interpreter to achieve better translation quality at the cost of increased delay To answer the question, we release Europarl Simultaneous Interpreting Corpus (ESIC), 10 hours of recordings and transcripts of European Parliament speeches in English, with simultaneous interpreting into Czech and German. We evaluate quality and latency of speaker-based and interpreter-based spoken translation systems from English to Czech. We study the differences in implicit simplification and summarization of the human interpreter compared to a machine translation system trained to shorten the output to some extent. Finally, we perform human evaluation to measure information loss of each of these approaches",
    "keywords": [],
    "checked": true,
    "id": "302b03cc77ed518405161a22beac71d1ac9bb232",
    "semantic_title": "lost in interpreting: speech translation from source or interpreter?",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pouthier21_interspeech.html": {
    "title": "Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-Based Multimodal Fusion",
    "volume": "main",
    "abstract": "It is now well established from a variety of studies that there is a significant benefit from combining video and audio data in detecting active speakers. However, either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. This paper outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertainty-based multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We finally show that the proposed method significantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset",
    "keywords": [],
    "checked": true,
    "id": "3d2b81bd831150943a5705cca10c74e2fec830d6",
    "semantic_title": "active speaker detection as a multi-objective optimization with uncertainty-based multimodal fusion",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wallbridge21_interspeech.html": {
    "title": "It's Not What You Said, it's How You Said it: Discriminative Perception of Speech as a Multichannel Communication System",
    "volume": "main",
    "abstract": "People convey information extremely effectively through spoken interaction using multiple channels of information transmission: the lexical channel of is said, and the non-lexical channel of it is said. We propose studying human perception of spoken communication as a means to better understand how information is encoded across these channels, focusing on the question To investigate this, we present a novel behavioural task testing whether listeners can discriminate between the true utterance in a dialogue and utterances sampled from other contexts with the same lexical content. We characterize how perception — and subsequent discriminative capability — is affected by different degrees of additional contextual information across both the lexical and non-lexical channel of speech. Results demonstrate that people can effectively discriminate between different prosodic realisations, that non-lexical context is informative, and that this channel provides more salient information than the lexical channel, highlighting the importance of the non-lexical channel in spoken interaction",
    "keywords": [],
    "checked": true,
    "id": "75c21506ee51447361de0aa712b7e8a3452a8461",
    "semantic_title": "it's not what you said, it's how you said it: discriminative perception of speech as a multichannel communication system",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/michael21_interspeech.html": {
    "title": "Extending the Fullband E-Model Towards Background Noise, Bursty Packet Loss, and Conversational Degradations",
    "volume": "main",
    "abstract": "Quality engineering of speech communication services in the full speech transmission band (0–20,000 Hz) is facilitated by the fullband E-model, a planning tool that predicts overall quality on the basis of parameters describing the setting of the service. We presented a first version of this model at Interspeech 2019, which has since then been standardized by the International Telecommunication Union in ITU-T Rec. G.107.2. Whereas that model was limited to predict the effects of speech codecs, random packet loss, and transmission delay, more realistic settings such as ambient background noise, bursty packet loss, as well as interactive conversational degradations could not be predicted. Based on the results of two new listening-only and conversational tests, we present an approach to extend the E-model to better predict these effects in the present paper. The results show that background noise effects at both sending and receiving side can be predicted well, whereas bursty packet loss predictions still have some limitations which result from the available database. Finally, approaches from conversational analysis help to better predict the effects of delay on conversational quality",
    "keywords": [],
    "checked": true,
    "id": "885f4c1d64f231c3f99265f7884c8a0fb553f016",
    "semantic_title": "extending the fullband e-model towards background noise, bursty packet loss, and conversational degradations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bergler21_interspeech.html": {
    "title": "ORCA-SLANG: An Automatic Multi-Stage Semi-Supervised Deep Learning Framework for Large-Scale Killer Whale Call Type Identification",
    "volume": "main",
    "abstract": "Identification of animal-specific vocalization patterns is an imperative requirement to decode animal communication. In bioacoustics, passive acoustic recording setups are increasingly deployed to acquire large-scale datasets. Previous knowledge about established animal-specific call types is usually present due to historically conducted research. However, time- and human-resource constraints, combined with a lack of available machine-based approaches, only allow manual analysis of comparatively small data corpora and strongly distort the actual data representation and information value. Such data limitations cause restrictions in terms of identifying existing population-, group-, and individual-specific call types, sub-categories, as well as unseen vocalization patterns. Thus, machine learning forms the basis for animal-specific call type recognition, to facilitate more profound insights into communication. The current study is the first fusing task-specific neural networks to develop a fully automated, multi-stage, deep-learning-based framework, entitled ORCA-SLANG, performing semi-supervised call type identification in one of the largest animal-specific bioacoustic archives — the Orchive. Orca/noise segmentation, denoising, and subsequent feature learning provide robust representations for semi-supervised clustering/classification. This results in a machine-annotated call type data repository containing 235,369 unique calls",
    "keywords": [],
    "checked": true,
    "id": "8c3417da30d2f1ca439de7ae247051e11f6942ca",
    "semantic_title": "orca-slang: an automatic multi-stage semi-supervised deep learning framework for large-scale killer whale call type identification",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/boes21_interspeech.html": {
    "title": "Audiovisual Transfer Learning for Audio Tagging and Sound Event Detection",
    "volume": "main",
    "abstract": "We study the merit of transfer learning for two sound recognition problems, i.e., audio tagging and sound event detection. Employing feature fusion, we adapt a baseline system utilizing only spectral acoustic inputs to also make use of pretrained auditory and visual features, extracted from networks built for different tasks and trained with external data We perform experiments with these modified models on an audiovisual multi-label data set, of which the training partition contains a large number of unlabeled samples and a smaller amount of clips with weak annotations, indicating the clip-level presence of 10 sound categories without specifying the temporal boundaries of the active auditory events For clip-based audio tagging, this transfer learning method grants marked improvements. Addition of the visual modality on top of audio also proves to be advantageous in this context When it comes to generating transcriptions of audio recordings, the benefit of pretrained features depends on the requested temporal resolution: for coarse-grained sound event detection, their utility remains notable. But when more fine-grained predictions are required, performance gains are strongly reduced due to a mismatch between the problem at hand and the goals of the models from which the pretrained vectors were obtained",
    "keywords": [],
    "checked": true,
    "id": "d90b6d1b716563063407c092c4b36fdff46e74bd",
    "semantic_title": "audiovisual transfer learning for audio tagging and sound event detection",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nessler21_interspeech.html": {
    "title": "Non-Intrusive Speech Quality Assessment with Transfer Learning and Subject-Specific Scaling",
    "volume": "main",
    "abstract": "In communication systems, it is crucial to estimate the perceived quality of audio and speech. The industrial standards for many years have been PESQ, 3QUEST, and POLQA, which are intrusive methods. This restricts the possibilities of using these metrics in real-world conditions, where we might not have access to the clean reference signal. In this work, we develop a new non-intrusive metric based on crowd-sourced data. We build a new speech dataset by combining publicly available speech, noises, and reverberations. Then we follow the ITU P.808 recommendation to label the dataset with mean opinion scores (MOS). Finally, we train a deep neural network to estimate the MOS from the speech data in a non-intrusive way. We propose two novelties in our work. First, we explore transfer learning by pre-training a model using a larger set of POLQA scores and finetuning with the smaller (and thus cheaper) human-labeled set. Secondly, we perform a subject-specific scaling in the MOS scores to adjust for their different subjective scales. Our model yields better accuracy than PESQ, POLQA, and other non-intrusive methods when evaluated on the independent VCTK test set. We also report misleading POLQA scores for reverberant speech",
    "keywords": [],
    "checked": true,
    "id": "cece12f114be6edd31422369e32316955997151d",
    "semantic_title": "non-intrusive speech quality assessment with transfer learning and subject-specific scaling",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oncescu21_interspeech.html": {
    "title": "Audio Retrieval with Natural Language Queries",
    "volume": "main",
    "abstract": "We consider the task of retrieving audio using free-form natural language queries. To study this problem, which has received limited attention in the existing literature, we introduce challenging new benchmarks for text-based audio retrieval using text annotations sourced from the AudioCaps and Clotho datasets. We then employ these benchmarks to establish baselines for cross-modal audio retrieval, where we demonstrate the benefits of pre-training on diverse audio tasks. We hope that our benchmarks will inspire further research into cross-modal text-based audio retrieval with free-form text queries",
    "keywords": [],
    "checked": true,
    "id": "439cea98c9ad49b419509181325c83e8bb9748bf",
    "semantic_title": "audio retrieval with natural language queries",
    "citation_count": 53,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/giollo21_interspeech.html": {
    "title": "Bootstrap an End-to-End ASR System by Multilingual Training, Transfer Learning, Text-to-Text Mapping and Synthetic Audio",
    "volume": "main",
    "abstract": "Bootstrapping speech recognition on limited data resources has been an area of active research for long. The recent transition to all-neural models and end-to-end (E2E) training brought along particular challenges as these models are known to be data hungry, but also came with opportunities around language-agnostic representations derived from multilingual data as well as shared word-piece output representations across languages that share script and roots. We investigate here the effectiveness of different strategies to bootstrap an RNN-Transducer (RNN-T) based automatic speech recognition (ASR) system in the low resource regime, while exploiting the abundant resources available in other languages as well as the synthetic audio from a text-to-speech (TTS) engine. Our experiments demonstrate that transfer learning from a multilingual model, using a post-ASR text-to-text mapping and synthetic audio deliver additive improvements, allowing us to bootstrap a model for a new language with a fraction of the data that would otherwise be needed. The best system achieved a 46% relative word error rate (WER) reduction compared to the monolingual baseline, among which 25% relative WER improvement is attributed to the post-ASR text-to-text mappings and the TTS synthetic data",
    "keywords": [],
    "checked": true,
    "id": "2133bd3c977a6a063d1b8639d60373cad218a8d9",
    "semantic_title": "bootstrap an end-to-end asr system by multilingual training, transfer learning, text-to-text mapping and synthetic audio",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pham21_interspeech.html": {
    "title": "Efficient Weight Factorization for Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end multilingual speech recognition involves using a single model training on a compositional speech corpus including many languages, resulting in a single neural network to handle transcribing different languages. Due to the fact that each language in the training data has different characteristics, the shared network may struggle to optimize for all various languages simultaneously. In this paper we propose a novel multilingual architecture that targets the core operation in neural networks: linear transformation functions. The key idea of the method is to assign fast weight matrices for each language by decomposing each weight matrix into a shared component and a language dependent component. The latter is then factorized into vectors using rank-1 assumptions to reduce the number of parameters per language. This efficient factorization scheme is proved to be effective in two multilingual settings with 7 and 27 languages, reducing the word error rates by 26% and 27% rel. for two popular architectures LSTM and Transformer, respectively",
    "keywords": [],
    "checked": true,
    "id": "b70ab6d6bcff6729be0eb0fb55c4bacad0581771",
    "semantic_title": "efficient weight factorization for multilingual speech recognition",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/conneau21_interspeech.html": {
    "title": "Unsupervised Cross-Lingual Representation Learning for Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages",
    "keywords": [],
    "checked": true,
    "id": "863d7fdbdcd4bdb1b6eeb9c99ae144d236f03259",
    "semantic_title": "unsupervised cross-lingual representation learning for speech recognition",
    "citation_count": 473,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hayakawa21_interspeech.html": {
    "title": "Language and Speaker-Independent Feature Transformation for End-to-End Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "This paper proposes a method to improve the performance of multilingual automatic speech recognition (ASR) systems through language- and speaker-independent feature transformation in a framework of end-to-end (E2E) ASR. Specifically, we propose a multi-task training method that combines a language recognizer and a speaker recognizer with an E2E ASR system based on connectionist temporal classification (CTC) loss functions. We introduce the language and speaker recognition sub-tasks into the E2E ASR network and introduce a gradient reversal layer (GRL) for each sub-task to achieve language and speaker-independent feature transformation. The evaluation results of the proposed method in the multilingual ASR system in six sorts of languages show that the proposed method achieves higher accuracy than the ASR models for each language by introducing multi-tasking and GRL",
    "keywords": [],
    "checked": true,
    "id": "2076f99dcbe5129881cc2e889daaba1d3b3b80d0",
    "semantic_title": "language and speaker-independent feature transformation for end-to-end multilingual speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/n21_interspeech.html": {
    "title": "Using Large Self-Supervised Models for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "Recently, self-supervised pre-training has shown significant improvements in many areas of machine learning, including speech and NLP. The self-supervised models are trained on a large amount of unlabelled data to learn higher-level representations for downstream tasks. In this work, we investigate the effectiveness of many self-supervised pre-trained models for the low-resource speech recognition task. We adopt pre-trained wav2vec2.0 [1] models for the speech recognition task for three Indian languages Telugu, Tamil, and Gujarati. We examine both English and multilingual pre-trained models. Our experiments show that fine-tuning the multilingual pre-trained model obtains an average relative reduction in WER of 2.88% compared to the previous state-of-the-art supervised method. We carefully analyze the generalization capability of multilingual pre-trained models for both seen and unseen languages. We also show that fine-tuning with only 25% of the training data gives competitive WER to the previous best methods",
    "keywords": [],
    "checked": true,
    "id": "3b775d43389b615d37cfb8fc66450a0b3ca318e4",
    "semantic_title": "using large self-supervised models for low-resource speech recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumar21e_interspeech.html": {
    "title": "Dual Script E2E Framework for Multilingual and Code-Switching ASR",
    "volume": "main",
    "abstract": "India is home to multiple languages, and training automatic speech recognition (ASR) systems is challenging. Over time, each language has adopted words from other languages, such as English, leading to code-mixing. Most Indian languages also have their own unique scripts, which poses a major limitation in training multilingual and code-switching ASR systems Inspired by results in text-to-speech synthesis, in this paper, we use an in-house rule-based phoneme-level common label set (CLS) representation to train multilingual and code-switching ASR for Indian languages. We propose two end-to-end (E2E) ASR systems. In the first system, the E2E model is trained on the CLS representation, and we use a novel data-driven backend to recover the native language script. In the second system, we propose a modification to the E2E model, wherein the CLS representation and the native language characters are used simultaneously for training. We show our results on the multilingual and code-switching (MUCS) ASR challenge 2021. Our best results achieve ≈6% and 5% improvement in word error rate over the baseline system for the multilingual and code-switching tasks, respectively, on the challenge development data",
    "keywords": [],
    "checked": true,
    "id": "06206dba5bb47cea888c5adaeefab35c5002c618",
    "semantic_title": "dual script e2e framework for multilingual and code-switching asr",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/diwan21_interspeech.html": {
    "title": "MUCS 2021: Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "Recently, there is an increasing interest in multilingual automatic speech recognition (ASR) where a speech recognition system caters to multiple low resource languages by taking advantage of low amounts of labelled corpora in multiple languages. With multilingualism becoming common in today's world, there has been increasing interest in code-switching ASR as well. In code-switching, multiple languages are freely interchanged within a single sentence or between sentences. The success of low-resource multilingual and code-switching (MUCS) ASR often depends on the variety of languages in terms of their acoustics, linguistic characteristics as well as the amount of data available and how these are carefully considered in building the ASR system. In this MUCS 2021 challenge, we would like to focus on building MUCS ASR systems through two different subtasks related to a total of seven Indian languages, namely Hindi, Marathi, Odia, Tamil, Telugu, Gujarati and Bengali. For this purpose, we provide a total of ~600 hours of transcribed speech data, comprising train and test sets, in these languages, including two code-switched language pairs, Hindi-English and Bengali-English. We also provide baseline recipes for both the subtasks with 30.73% and 32.45% word error rate on the MUCS test sets, respectively",
    "keywords": [],
    "checked": true,
    "id": "c35c455594a8cb04a7a4b31c1d77f5851e7ab691",
    "semantic_title": "multilingual and code-switching asr challenges for low resource indian languages",
    "citation_count": 57,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/winata21_interspeech.html": {
    "title": "Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "One crucial challenge of real-world multilingual speech recognition is the long-tailed distribution problem, where some resource-rich languages like English have abundant training data, but a long tail of low-resource languages have varying amounts of limited training data. To overcome the long-tail problem, in this paper, we propose Adapt-and-Adjust (A2), a transformer-based multi-task learning framework for end-to-end multilingual speech recognition. The A2 framework overcomes the long-tail problem via three techniques: (1) exploiting a pretrained multilingual language model to improve the performance of low-resource languages; (2) proposing dual adapters consisting of both language-specific and language-agnostic adaptation with minimal additional parameters; and (3) overcoming the class imbalance, either by imposing class priors in the loss during training or adjusting the logits of the softmax output during inference. Extensive experiments on the CommonVoice corpus show that A2 significantly outperforms conventional approaches",
    "keywords": [],
    "checked": true,
    "id": "0b2cd96f933722725c4b5d42cb4b1793fa24dee4",
    "semantic_title": "adapt-and-adjust: overcoming the long-tail problem of multilingual speech recognition",
    "citation_count": 38,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sailor21_interspeech.html": {
    "title": "SRI-B End-to-End System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "This paper describes SRI-B's end-to-end Automated Speech Recognition (ASR) system proposed for the subtask-1 on multilingual ASR challenges for Indian languages. Our end-to-end (E2E) ASR model is based on the transformer architecture trained by jointly minimizing Connectionist Temporal Classification (CTC) & Cross-Entropy (CE) losses. A conventional multilingual model which is trained by pooling data from multiple languages helps in terms of generalization, but it comes at the expense of performance degradation compared to their monolingual counterparts. In our experiments, a multilingual model is trained by conditioning the input features using a language-specific embedding vector. These language-specific embedding vectors are obtained by training a language classifier using an attention-based transformer architecture, and then considering its bottleneck features as language identification (LID) embeddings. We further adapt the multilingual system with language specific data to reduce the degradation on specific languages. We propose a novel hypothesis elimination strategy based on LID scores and length-normalized probabilities that optimally select the model from the pool of available models. The experimental results show that the proposed multilingual training and hypothesis elimination strategy gives an average 3.02% of relative word error recognition (WER) improvement for the blind set over the challenge hybrid ASR baseline system",
    "keywords": [],
    "checked": true,
    "id": "8a95e6e7bf6347ba8d3b614d7bbd6f307aea570c",
    "semantic_title": "sri-b end-to-end system for multilingual and code-switching asr challenges for low resource indian languages",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21f_interspeech.html": {
    "title": "Hierarchical Phone Recognition with Compositional Phonetics",
    "volume": "main",
    "abstract": "There is growing interest in building phone recognition systems for low-resource languages as the majority of languages do not have any writing systems. Phone recognition systems proposed so far typically derive their phone inventory from the training languages, therefore the derived inventory could only cover a limited number of phones existing in the world. It fails to recognize unseen phones in low-resource or zero-resource languages. In this work, we tackle this problem with a hierarchical model, in which we explicitly model three different entities in a hierarchical manner: phoneme, phone, and phonological articulatory attributes. In particular, we decompose phones into articulatory attributes and compute the phone embedding from the attribute embedding. The model would first predict the distribution over the phones using their embeddings, next, the language-independent phones are aggregated to the language-dependent phonemes and then optimized by the CTC loss. This compositional approach enables us to recognize phones even they do not appear in the training set. We evaluate our model on 47 unseen languages and find the proposed model outperforms baselines by 13.1% PER",
    "keywords": [],
    "checked": true,
    "id": "a06e34e45379a3255b1c1df6b6de56ef414f5a83",
    "semantic_title": "hierarchical phone recognition with compositional phonetics",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chowdhury21_interspeech.html": {
    "title": "Towards One Model to Rule All: Multilingual Strategy for Dialectal Code-Switching Arabic ASR",
    "volume": "main",
    "abstract": "With the advent of globalization, there is an increasing demand for multilingual automatic speech recognition (ASR), handling language and dialectal variation of spoken content. Recent studies show its efficacy over monolingual systems. In this study, we design a large multilingual end-to-end ASR using self-attention based conformer architecture. We trained the system using Arabic (Ar), English (En) and French (Fr) languages. We evaluate the system performance handling: (i) monolingual (Ar, En and Fr); (ii) multi-dialectal (Modern Standard Arabic, along with dialectal variation such as Egyptian and Moroccan); (iii) code-switching — cross-lingual (Ar-En/Fr) and dialectal (MSA-Egyptian dialect) test cases, and compare with current state-of-the-art systems. Furthermore, we investigate the influence of different embedding/character representations including character vs word-piece; shared vs distinct input symbol per language. Our findings demonstrate the strength of such a model by outperforming state-of-the-art monolingual dialectal Arabic and code-switching Arabic ASR",
    "keywords": [],
    "checked": true,
    "id": "81dc39d9b2952c17110a665347bfac4803b69889",
    "semantic_title": "towards one model to rule all: multilingual strategy for dialectal code-switching arabic asr",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21b_interspeech.html": {
    "title": "Differentiable Allophone Graphs for Language-Universal Speech Recognition",
    "volume": "main",
    "abstract": "Building language-universal speech recognition systems entails producing phonological units of spoken sound that can be shared across languages. While speech annotations at the language-specific phoneme or surface levels are readily available, annotations at a universal phone level are relatively rare and difficult to produce. In this work, we present a general framework to derive phone-level supervision from only phonemic transcriptions and phone-to-phoneme mappings with weights represented using weighted finite-state transducers, which we call By training multilingually, we build a universal phone-based speech recognition model with interpretable probabilistic phone-to-phoneme mappings for each language. These phone-based systems with learned allophone graphs can be used by linguists to document new languages, build phone-based lexicons that capture rich pronunciation variations, and re-evaluate the allophone mappings of seen language. We demonstrate the aforementioned benefits of our proposed framework with a system trained on 7 diverse languages",
    "keywords": [],
    "checked": true,
    "id": "987658ba918710bbce5de8d92eb44bd127cf72c5",
    "semantic_title": "differentiable allophone graphs for language-universal speech recognition",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/martin21_interspeech.html": {
    "title": "Automatic Speech Recognition Systems Errors for Objective Sleepiness Detection Through Voice",
    "volume": "main",
    "abstract": "Chronic sleepiness, and specifically Excessive Daytime Sleepiness (EDS), impacts everyday life and increases the risks of accidents. Compared with traditional measures (EEG), the detection of objective EDS through voice benefits from its ease to be implemented in ecological conditions and to be sober in terms of data processing and costs. Contrary to previous works focusing on short-term sleepiness estimation, this study focuses on long-term sleepiness detection through voice. Using the Multiple Sleep Latency Test corpus, this study introduces new features based on Automatic Speech Recognition systems errors, in an attempt to replace hand-labeled reading mistakes features. We also introduce a selection feature pipeline inspired by clinical validation practices allowing ASR features to perform on par with the state-of-the-art systems on short-term sleepiness detection through voice (73.2% of UAR). Moreover, we give insights on the decision process during classification and the specificity of the system regarding the threshold delimiting the two sleepiness classes, Sleepy and Non-Sleepy",
    "keywords": [],
    "checked": true,
    "id": "904ce09d4f4370440e394dc9a29120eff3a011e6",
    "semantic_title": "automatic speech recognition systems errors for objective sleepiness detection through voice",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gillick21_interspeech.html": {
    "title": "Robust Laughter Detection in Noisy Environments",
    "volume": "main",
    "abstract": "We investigate the problem of automatically identifying and extracting laughter from audio files in noisy environments. We conduct an empirical evaluation of several machine learning models using audio data of varying sound quality, finding that while previously published methods work relatively well in controlled environments, performance drops precipitously in real-world settings with background noise. In the process, we contribute a new dataset of laughter annotations on top of the existing AudioSet corpus, with precise segmentations for the start and end points of each laugh, and we present a new approach to laughter detection that performs comparatively well in uncontrolled environments. We discuss the utility of our approach as well as the importance of understanding the variability of model performance in a range of real-world testing environments",
    "keywords": [],
    "checked": true,
    "id": "6c5916e8c2b1677b5d979f7e33385a2a48700748",
    "semantic_title": "robust laughter detection in noisy environments",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nagano21_interspeech.html": {
    "title": "Impact of Emotional State on Estimation of Willingness to Buy from Advertising Speech",
    "volume": "main",
    "abstract": "The characteristics of a speaker's voice can affect the perceived impression or behavior of the listener. Previous studies of consumer behavior have shown that this can be well explained by the emotion-mediated behavior model. However, few studies of the emotion-mediated behavior model have used advertising speech. In this paper, we examine whether the stimulus-organism-response theory using emotional state can explain willingness to buy from advertising speech stimulus. The subjects listened to speech with modified speech features (mean F0, speech rate, spectral tilt, or standard deviation of F0) and rated their willingness to buy the products advertised in the speech and their own perceived emotions (pleasure, arousal, dominance). We found that the emotions partially mediate the influence of speech features on the willingness to buy. These results will be useful for developing a method of speech synthesis to increase people's willingness to buy",
    "keywords": [],
    "checked": true,
    "id": "a55fa70c4fcc02d14dc11d2a03675367b767bbb3",
    "semantic_title": "impact of emotional state on estimation of willingness to buy from advertising speech",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alsofyani21_interspeech.html": {
    "title": "Stacked Recurrent Neural Networks for Speech-Based Inference of Attachment Condition in School Age Children",
    "volume": "main",
    "abstract": "In Attachment Theory, children that have a positive perception of their parents are said to be secure, while the others are said to be insecure. Once adult, unless identified and supported early enough, insecure children have higher chances to experience major issues (e.g., suicidal tendencies and antisocial behavior). For this reason, this article proposes a speech-based automatic approach for the recognition of attachment in school-age children. The experiments are based on stacked RNNs and have involved 104 children of age between 5 and 9. The accuracy is up to 68.9% (F1 59.6%), meaning that the approach makes the right decision two times out of three, on average. To the best of our knowledge, this is the first work aimed at inferring attachment from speech in school-age children",
    "keywords": [],
    "checked": true,
    "id": "cf142aa43111cbc59e2ecad582b713548e651249",
    "semantic_title": "stacked recurrent neural networks for speech-based inference of attachment condition in school age children",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/aloshban21_interspeech.html": {
    "title": "Language or Paralanguage, This is the Problem: Comparing Depressed and Non-Depressed Speakers Through the Analysis of Gated Multimodal Units",
    "volume": "main",
    "abstract": "Speech-based depression detection has attracted significant attention over the last years. A debated problem is whether it is better to use language (what people say), paralanguage (how they say it) or a combination of the two. This article addresses the question through the analysis of a Gated Multimodal Unit trained to weight modalities according to how effectively they account for the condition of a speaker (depressed or non-depressed). The experiments involved 29 individuals diagnosed with depression and 30 non-depressed participants. Besides an accuracy of 83.0% (F1 score 80.0%), the results show that the Gated Multimodal Unit tends to give more weight to paralanguage. However, the relative contribution of language tends to be higher, to a statistically significant extent, in the case of non-depressed speakers",
    "keywords": [],
    "checked": true,
    "id": "f1f016ff6a58d979ad9a00a690a8f8eaa599d993",
    "semantic_title": "language or paralanguage, this is the problem: comparing depressed and non-depressed speakers through the analysis of gated multimodal units",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tammewar21_interspeech.html": {
    "title": "Emotion Carrier Recognition from Personal Narratives",
    "volume": "main",
    "abstract": "Personal Narratives (PN) — recollections of facts, events, and thoughts from one's own experience — are often used in everyday conversations. So far, PNs have mainly been explored for tasks such as valence prediction or emotion classification (e.g ). However, these tasks might overlook more fine-grained information that could prove to be relevant for understanding PNs. In this work, we propose a novel task for Narrative Understanding: Emotion Carrier Recognition (ECR). Emotion carriers, the text fragments that carry the emotions of the narrator (e.g ), provide a fine-grained description of the emotion state. We explore the task of ECR in a corpus of PNs manually annotated with emotion carriers and investigate different machine learning models for the task. We propose evaluation strategies for ECR including metrics that can be appropriate for different tasks",
    "keywords": [],
    "checked": true,
    "id": "9527eb2418580a3fc3cd2372dbdce57a851acaac",
    "semantic_title": "emotion carrier recognition from personal narratives",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/condron21_interspeech.html": {
    "title": "Non-Verbal Vocalisation and Laughter Detection Using Sequence-to-Sequence Models and Multi-Label Training",
    "volume": "main",
    "abstract": "Non-verbal vocalisations (NVVs) such as laughter are an important part of communication in social interactions and carry important information about a speaker's state or intention. There remains no clear definition of NVVs and there is no clearly defined protocol for transcribing or detecting NVVs. As such, the standard approach has been to focus on detecting a single NVV such as laughter and map all other NVVs to an \"other\" class. In this paper we hypothesise that for this task such an approach hurts performance, and that giving more information by using more classes is beneficial. To address this, we present studies using sequence-to-sequence deep neural networks where we include multiple NVV classes rather than mapping them to \"other\" and allow more than one label per sample. We show that this approach yields better performance than the standard approach on NVV detection. We also evaluate the same model on laughter detection using frame-based and utterance-based metrics and show that the proposed approach yields state-of-the-art performance on the ICSI corpus",
    "keywords": [],
    "checked": true,
    "id": "5ffa2d8b3ee363c7fda06b06c2e8ad3cede67d69",
    "semantic_title": "non-verbal vocalisation and laughter detection using sequence-to-sequence models and multi-label training",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cai21_interspeech.html": {
    "title": "TDCA-Net: Time-Domain Channel Attention Network for Depression Detection",
    "volume": "main",
    "abstract": "Depression is a psychiatric disorder and has many adverse effects on our society. Some studies have shown that speech signals are closely related to emotion and stress, and many speech-based automatic depression detection methods have been proposed. However, previous work is based on spectrogram or hand-crafted features, which may lose some useful information related to depression patterns. And there is no evidence that the filter bank designed from perceptual evidence is optimal for depression detection. In order to learn the more discriminative feature representation related to depression, we propose an end-to-end time-domain channel attention network (TDCA-Net) for depression detection. The TDCA-Net directly models time-domain speech signals based on dilated convolution block, which can increase the receptive field exponentially and aggregate multiscale contextual information associated with depression. Besides, we employ the efficient channel attention (ECA) module to model dependencies of channels and improve the sensitivity of the model to information related to depression. Experimental results on the AVEC2013 and the AVEC2014 datasets illustrate the effectiveness of our method",
    "keywords": [],
    "checked": true,
    "id": "46b1756c70569a5aaebc3a763da0746939de09a5",
    "semantic_title": "tdca-net: time-domain channel attention network for depression detection",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/botelho21_interspeech.html": {
    "title": "Visual Speech for Obstructive Sleep Apnea Detection",
    "volume": "main",
    "abstract": "Obstructive sleep apnea (OSA) affects almost one billion people worldwide and limits peoples' quality of life substantially. Furthermore, it is responsible for significant morbidity and mortality associated with hypertension, cardiovascular diseases, work and traffic accidents. Thus, the early detection of OSA can save lives. In our previous work we used speech as biomarker for automatic OSA detection. More recently, we leveraged the fact that OSA patients have anatomical and functional abnormalities of the upper airway and an altered craniofacial morphology, and therefore explore information from facial images for OSA detection. In this work, we propose to combine speech and facial image information to detect OSA from YouTube vlogs. This in-the-wild data poses an inexpensive alternative to standard data collected for medical applications, which is often scarce, imbalanced and costly to acquire. Besides speech and facial images, we propose to include as a third modality, inspired by the emerging field of silent computational paralinguistics. We hypothesize that embeddings trained from lip reading integrate information on the craniofacial structure, on speech articulation and breathing patterns, thus containing relevant cues for OSA detection. Fusion of the three modalities achieves an accuracy of 82.5% at the speaker level",
    "keywords": [],
    "checked": true,
    "id": "2fafe88bbc1b42cb12d3bf68b4152a4e7b40d77b",
    "semantic_title": "visual speech for obstructive sleep apnea detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maruri21_interspeech.html": {
    "title": "Analysis of Contextual Voice Changes in Remote Meetings",
    "volume": "main",
    "abstract": "People participating in remote meetings in open spaces might choose to speak with a restrained voice due to concerns around privacy or disturbing others. These contextual voice changes might impact the quality of communications. To investigate how people adjust their voices in certain situations, we performed an exploratory data collection study with 41 participants in 18 simulated remote meetings. A scenario was provided to the participants to naturally trigger contextual voice changes. We collected multi-modal data from the participants including in-situ labels for the voice quality. We implemented content analysis, t-test, and linear regression to analyze the multi-modal data. Results showed that the participants primarily preferred to use soft voice over whispered voice to avoid being overheard during the meetings. Speaking softly was often sufficient to successfully conceal private conversations, while using whispered voice had only a negative impact on the intelligibility. Overall, we found that participants perceived soft voice as less pleasant to listen to than normal voice during meetings and discovered factors related to speaker demographics and meeting context that impacted the concealing behavior (soft or whispered). For our future research, we will expand to different scenarios and consider the impact of audio feedback on voice concealing",
    "keywords": [],
    "checked": true,
    "id": "867c15065436e50ba2cf213b875909445e1f310a",
    "semantic_title": "analysis of contextual voice changes in remote meetings",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seneviratne21_interspeech.html": {
    "title": "Speech Based Depression Severity Level Classification Using a Multi-Stage Dilated CNN-LSTM Model",
    "volume": "main",
    "abstract": "Speech based depression classification has gained immense popularity over the recent years. However, most of the classification studies have focused on binary classification to distinguish depressed subjects from non-depressed subjects. In this paper, we formulate the depression classification task as a severity level classification problem to provide more granularity to the classification outcomes. We use articulatory coordination features (ACFs) developed to capture the changes of neuromotor coordination that happens as a result of psychomotor slowing, a necessary feature of Major Depressive Disorder. The ACFs derived from the vocal tract variables (TVs) are used to train a dilated Convolutional Neural Network based depression classification model to obtain segment-level predictions. Then, we propose a Recurrent Neural Network based approach to obtain session-level predictions from segment-level predictions. We show that strengths of the segment-wise classifier are amplified when a session-wise classifier is trained on embeddings obtained from it. The model trained on ACFs derived from TVs show relative improvement of 27.47% in Unweighted Average Recall (UAR) at the session-level classification task, compared to the ACFs derived from Mel Frequency Cepstral Coefficients (MFCCs)",
    "keywords": [],
    "checked": true,
    "id": "98b5eba74cc547be9acf87ac7b66a52297e9f5c1",
    "semantic_title": "speech based depression severity level classification using a multi-stage dilated cnn-lstm model",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21g_interspeech.html": {
    "title": "Multi-Domain Knowledge Distillation via Uncertainty-Matching for End-to-End ASR Models",
    "volume": "main",
    "abstract": "Knowledge Distillation basically matches predictive distributions of student and teacher networks to improve performance in an environment with model capacity and/or data constraints. However, it is well known that predictive distribution of neural networks not only tends to be overly confident, but also cannot directly model various factors properly that contribute to uncertainty. Recently, deep learning studies based on uncertainty have been successful in various fields, especially in several computer vision tasks. The prediction probability can implicitly show the information about how confident the network is, however, we can explicitly utilize confidence of the output by modeling the uncertainty of the network. In this paper, we propose a novel knowledge distillation method for automatic speech recognition that directly models and transfers the uncertainty inherent in data observation such as speaker variations or confusing pronunciations. Moreover, we investigate an effect of transferring knowledge more effectively using multiple teachers learned from various domains. Evaluated on WSJ which is the standard benchmark dataset with limited instances, the proposed knowledge distillation method achieves significant improvements over student baseline models",
    "keywords": [],
    "checked": true,
    "id": "908f760b280bd2608520e87345928ce6d884b7bc",
    "semantic_title": "multi-domain knowledge distillation via uncertainty-matching for end-to-end asr models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/macoskey21_interspeech.html": {
    "title": "Learning a Neural Diff for Speech Models",
    "volume": "main",
    "abstract": "As more speech processing applications execute locally on edge devices, a set of resource constraints must be considered. In this work we address one of these constraints, namely over-the-network data budgets for transferring models from server to device. We present neural update approaches for release of subsequent speech model generations abiding by a data budget. We detail two architecture-agnostic methods which learn compact representations for transmission to devices. We experimentally validate our techniques with results on two tasks (automatic speech recognition and spoken language understanding) on open source data sets by demonstrating when applied in succession, our budgeted updates outperform comparable model compression baselines by significant margins",
    "keywords": [],
    "checked": true,
    "id": "0c5533fecd7b069fd8e231ef7c47c52a53fc01a7",
    "semantic_title": "learning a neural diff for speech models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21p_interspeech.html": {
    "title": "Stochastic Attention Head Removal: A Simple and Effective Method for Improving Transformer Based ASR Models",
    "volume": "main",
    "abstract": "Recently, Transformer based models have shown competitive automatic speech recognition (ASR) performance. One key factor in the success of these models is the multi-head attention mechanism. However, for trained models, we have previously observed that many attention matrices are close to diagonal, indicating the redundancy of the corresponding attention heads. We have also found that some architectures with reduced numbers of attention heads have better performance. Since the search for the best structure is time prohibitive, we propose to randomly remove attention heads during training and keep all attention heads at test time, thus the final model is an ensemble of models with different architectures. The proposed method also forces each head independently learn the most useful patterns. We apply the proposed method to train Transformer based and Convolution-augmented Transformer (Conformer) based ASR models. Our method gives consistent performance gains over strong baselines on the Wall Street Journal, AISHELL, Switchboard and AMI datasets. To the best of our knowledge, we have achieved state-of-the-art end-to-end Transformer based model performance on Switchboard and AMI",
    "keywords": [],
    "checked": true,
    "id": "0521186488331aa28d21332546712113d11a3819",
    "semantic_title": "stochastic attention head removal: a simple and effective method for improving transformer based asr models",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21b_interspeech.html": {
    "title": "Model-Agnostic Fast Adaptive Multi-Objective Balancing Algorithm for Multilingual Automatic Speech Recognition Model Training",
    "volume": "main",
    "abstract": "This paper regards multilingual automatic speech recognition model training as a multi-objective problem because learning different languages may conflict, necessitating a trade-off. Most previous works on multilingual ASR model training mainly used data sampling to balance the performance of multiple languages but ignore the conflicts between different languages, resulting in an imbalance in multiple languages. The language-specific parameters of the multilingual ASR model are updated by the single language gradients while the update of the shared parameter is jointly determined by the gradient of every language on its shared parameter, namely shared gradient. Therefore, we propose a model-agnostic fast adaptive (MAFA) multi-objective balancing algorithm to balance multiple languages by avoiding the mutual interferences between their shared gradients. In the algorithm, based on the decrease in the training loss, we dynamically normalize the shared gradient magnitudes representing the speed of learning to balance the learning speed. To evenly learn multiple languages, the language with the worst performance is selected, and a balancing gradient nearest to the normalized gradient of the selected language and positively correlated with other normalized ones is obtained to eliminate the mutual interferences. The model trained by MAFA outperforms the baseline model on the Common Voice corpus",
    "keywords": [],
    "checked": true,
    "id": "a95e891ed67c4fe975493ca3d1fdb3c1eb8ffda0",
    "semantic_title": "model-agnostic fast adaptive multi-objective balancing algorithm for multilingual automatic speech recognition model training",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chang21b_interspeech.html": {
    "title": "Towards Lifelong Learning of End-to-End ASR",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) technologies today are primarily optimized for given datasets; thus, any changes in the application environment (e.g., acoustic conditions or topic domains) may inevitably degrade the performance. We can collect new data describing the new environment and fine-tune the system, but this naturally leads to higher error rates for the earlier datasets, referred to as catastrophic forgetting. The concept of lifelong learning (LLL) aiming to enable a machine to sequentially learn new tasks from new datasets describing the changing real world without forgetting the previously learned knowledge is thus brought to attention. This paper reports, to our knowledge, the first effort to extensively consider and analyze the use of various approaches of LLL in end-to-end (E2E) ASR, including proposing novel methods in saving data for past domains to mitigate the catastrophic forgetting problem. An overall relative reduction of 28.7% in WER was achieved compared to the fine-tuning baseline when sequentially learning on three very different benchmark corpora. This can be the first step toward the highly desired ASR technologies capable of synchronizing with the continuously changing real world",
    "keywords": [],
    "checked": true,
    "id": "6ae82a08383d92109c285db98bdca84587976752",
    "semantic_title": "towards lifelong learning of end-to-end asr",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leal21_interspeech.html": {
    "title": "Self-Adaptive Distillation for Multilingual Speech Recognition: Leveraging Student Independence",
    "volume": "main",
    "abstract": "With a large population of the world speaking more than one language, multilingual automatic speech recognition (ASR) has gained popularity in the recent years. While lower resource languages can benefit from quality improvements in a multilingual ASR system, including unrelated or higher resource languages in the mix often results in performance degradation. In this paper, we propose distilling from multiple teachers, with each language using its best teacher during training, to tackle this problem. We introduce distillation, a novel technique for automatic weighting of the distillation loss that uses the student/ teachers confidences. We analyze the effectiveness of the proposed techniques on two real world use-cases and show that the performance of the multilingual ASR models can be improved by up to 11.5% without any increase in model capacity. Furthermore, we show that when our methods are combined with increase in model capacity, we can achieve quality gains of up to 20.7%",
    "keywords": [],
    "checked": true,
    "id": "cc3c321c3a2e7854ba14d067bdb140663e95ee30",
    "semantic_title": "self-adaptive distillation for multilingual speech recognition: leveraging student independence",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21f_interspeech.html": {
    "title": "Regularizing Word Segmentation by Creating Misspellings",
    "volume": "main",
    "abstract": "This work focuses on improving subword segmentation algorithms for end-to-end speech recognition models, and makes two major contributions. Firstly, we propose a novel word segmentation algorithm. The algorithm uses the same vocabulary generated by a regular wordpiece model, is easily extensible and supports a variety of regularization techniques in the segmentation space, and outperforms the regular wordpiece model. Secondly, we propose a number of novel regularization methods that introduce randomness into the tokenization algorithm, which bring further improvements in speech recognition accuracy, with relative gains up to 8.4% compared to the original wordpiece model. We analyze the methods and show that our proposed methods are equivalent to a sophisticated form of label smoothing, which performs smoothing based on the prefix structures of subword units. A noteworthy discovery from this work is that creating artificial misspellings in words results in the best performance among all the methods, which could inspire future research for strategies in this area",
    "keywords": [],
    "checked": true,
    "id": "971af636c7d80410d9ec2854dd854b3469df0323",
    "semantic_title": "regularizing word segmentation by creating misspellings",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21t_interspeech.html": {
    "title": "Multitask Training with Text Data for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We propose a multitask training method for attention-based end-to-end speech recognition models. We regularize the decoder in a listen, attend, and spell model by multitask training it on both audio-text and text-only data. Trained on the 100-hour subset of LibriSpeech, the proposed method, without requiring an additional language model, leads to an 11% relative performance improvement over the baseline and approaches the performance of language model shallow fusion on the test-clean evaluation set. We observe a similar trend on the whole 960-hour LibriSpeech training set. Analyses of different types of errors and sample output sentences demonstrate that the proposed method can incorporate language level information, suggesting its effectiveness in real-world applications",
    "keywords": [],
    "checked": true,
    "id": "a7fa3497481c400bf2e7856cfe284717e07ef66a",
    "semantic_title": "multitask training with text data for end-to-end speech recognition",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21j_interspeech.html": {
    "title": "Emitting Word Timings with HMM-Free End-to-End System in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Word timings, which mark the start and end times of each word in ASR results, play an important part in many applications, such as computer assisted language learning. To date, end-to-end (E2E) systems outperform conventional DNN-HMM hybrid systems in ASR accuracy but have challenges to obtain accurate word timings. In this paper, we propose a two-pass method to estimate word timings under an E2E-based LAS modeling framework, which is completely free of using the DNN-HMM ASR system. Specifically, we first employ the LAS system to obtain word-piece transcripts of the input audio, we then compute forced-alignments with a frame-level-based word-piece classifier. In order to make the classifier yield accurate word-piece timing results, we propose a novel objective function to learn the classifier, utilizing the spike timings of the connectionist temporal classification (CTC) model. On Librispeech data, our E2E-based LAS system achieves 2.8%/7.0% WERs, while its word timing (start/end) accuracy are 99.0%/95.3% and 98.6%/93.7% on test-clean and test-other two test sets respectively. Compared with a DNN-HMM hybrid ASR system (here, TDNN), the LAS system is better in ASR performance, and the generated word timings are close to what the TDNN ASR system presents",
    "keywords": [],
    "checked": true,
    "id": "95774ce61e75e12fafcbcd723383d31efc58d7dc",
    "semantic_title": "emitting word timings with hmm-free end-to-end system in automatic speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/droppo21_interspeech.html": {
    "title": "Scaling Laws for Acoustic Models",
    "volume": "main",
    "abstract": "There is a recent trend in machine learning to increase model quality by growing models to sizes previously thought to be unreasonable. Recent work has shown that autoregressive generative models with cross-entropy objective functions exhibit smooth power-law relationships, or scaling laws, that predict model quality from model size, training set size, and the available compute budget. These scaling laws allow one to choose nearly optimal hyper-parameters given constraints on available training data, model parameter count, or training computation budget. In this paper, we demonstrate that acoustic models trained with an auto-predictive coding loss behave as if they are subject to similar scaling laws. We extend previous work to jointly predict loss due to model size, to training set size, and to the inherent \"irreducible loss\" of the task. We find that the scaling laws accurately match model performance over two orders of magnitude in both model size and training set size, and make predictions about the limits of model performance",
    "keywords": [],
    "checked": true,
    "id": "de8f92c8a7ebde8bffb968a536f79e5fb7cd225e",
    "semantic_title": "scaling laws for acoustic models",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/billa21_interspeech.html": {
    "title": "Leveraging Non-Target Language Resources to Improve ASR Performance in a Target Language",
    "volume": "main",
    "abstract": "This paper investigates approaches to improving automatic speech recognition (ASR) performance in a target language using resources in other languages. In particular, we assume that we have untranscribed speech in a different language and a well trained ASR system in yet another language. Concretely, we structure this as a multi-task problem, where the primary task is acoustic model training in the target language, and the secondary task is also acoustic model training but using a synthetic data set. The synthetic data set consists of pseudo transcripts generated by decoding the untranscribed speech using a well trained ASR model. We compare and contrast this with using labeled data sets, i.e. matched audio and human-generated transcripts, and show that our approach compares favorably. In most cases, we see performance improvements, and in some cases, depending on the selection of languages and nature of speech data, performance exceeds that of systems using labeled data sets as the secondary task. When extended to larger sets of data, we show that the mismatched data approach performs similarly to in-language semi-supervised training (SST) when the secondary task pseudo transcripts are generated by ASR models trained on large diverse data sets",
    "keywords": [],
    "checked": true,
    "id": "460f5882680fa3a66e37a193ddf3811d82755eb6",
    "semantic_title": "leveraging non-target language resources to improve asr performance in a target language",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fasoli21_interspeech.html": {
    "title": "4-Bit Quantization of LSTM-Based Speech Recognition Models",
    "volume": "main",
    "abstract": "We investigate the impact of aggressive low-precision representations of weights and activations in two families of large LSTM-based architectures for Automatic Speech Recognition (ASR): hybrid Deep Bidirectional LSTM - Hidden Markov Models (DBLSTM-HMMs) and Recurrent Neural Network - Transducers (RNN-Ts). Using a 4-bit integer representation, a naïve quantization approach applied to the LSTM portion of these models results in significant Word Error Rate (WER) degradation. On the other hand, we show that minimal accuracy loss is achievable with an appropriate choice of quantizers and initializations. In particular, we customize quantization schemes depending on the local properties of the network, improving recognition performance while limiting computational time. We demonstrate our solution on the Switchboard (SWB) and CallHome (CH) test sets of the NIST Hub5-2000 evaluation. DBLSTM-HMMs trained with 300 or 2000 hours of SWB data achieves <0.5% and <1% average WER degradation, respectively. On the more challenging RNN-T models, our quantization strategy limits degradation in 4-bit inference to 1.3%",
    "keywords": [],
    "checked": true,
    "id": "126a76e32c5ccc98f258f5246749339bc97f94b4",
    "semantic_title": "4-bit quantization of lstm-based speech recognition models",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/masumura21_interspeech.html": {
    "title": "Unified Autoregressive Modeling for Joint End-to-End Multi-Talker Overlapped Speech Recognition and Speaker Attribute Estimation",
    "volume": "main",
    "abstract": "In this paper, we present a novel modeling method for single-channel multi-talker overlapped automatic speech recognition (ASR) systems. Fully neural network based end-to-end models have dramatically improved the performance of multi-taker overlapped ASR tasks. One promising approach for end-to-end modeling is autoregressive modeling with serialized output training in which transcriptions of multiple speakers are recursively generated one after another. This enables us to naturally capture relationships between speakers. However, the conventional modeling method cannot explicitly take into account the speaker attributes of individual utterances such as gender and age information. In fact, the performance deteriorates when each speaker is the same gender or is close in age. To address this problem, we propose unified autoregressive modeling for joint end-to-end multi-talker overlapped ASR and speaker attribute estimation. Our key idea is to handle gender and age estimation tasks within the unified autoregressive modeling. In the proposed method, transformer-based autoregressive model recursively generates not only textual tokens but also attribute tokens of each speaker. This enables us to effectively utilize speaker attributes for improving multi-talker overlapped ASR. Experiments on Japanese multi-talker overlapped ASR tasks demonstrate the effectiveness of the proposed method",
    "keywords": [],
    "checked": true,
    "id": "37393016b39ae759cd2996c1152d54b3c419f939",
    "semantic_title": "unified autoregressive modeling for joint end-to-end multi-talker overlapped speech recognition and speaker attribute estimation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/meng21_interspeech.html": {
    "title": "Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Integrating external language models (LMs) into end-to-end (E2E) models remains a challenging task for domain-adaptive speech recognition. Recently, internal language model estimation (ILME)-based LM fusion has shown significant word error rate (WER) reduction from Shallow Fusion by subtracting a weighted internal LM score from an interpolation of E2E model and external LM scores during beam search. However, on different test sets, the optimal LM interpolation weights vary over a wide range and have to be tuned extensively on well-matched validation sets. In this work, we perform LM fusion in the minimum WER (MWER) training of an E2E model to obviate the need for LM weights tuning during inference. Besides MWER training with Shallow Fusion (MWER-SF), we propose a novel MWER training with ILME (MWER-ILME) where the ILME-based fusion is conducted to generate N-best hypotheses and their posteriors. Additional gradient is induced when internal LM is engaged in MWER-ILME loss computation. During inference, LM weights pre-determined in MWER training enable robust LM integrations on test sets from different domains. Experimented with 30K-hour trained transformer transducers, MWER-ILME achieves on average 8.8% and 5.8% relative WER reductions from MWER and MWER-SF training, respectively, on 6 different test sets",
    "keywords": [],
    "checked": true,
    "id": "9e00614a38709cb424ebd5f6a336916cb002810c",
    "semantic_title": "minimum word error rate training with language model fusion for end-to-end speech recognition",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jiang21b_interspeech.html": {
    "title": "Variable Frame Rate Acoustic Models Using Minimum Error Reinforcement Learning",
    "volume": "main",
    "abstract": "Frame selection in automatic speech recognition (ASR) systems can potentially improve the trade-off between speed and accuracy relative to fixed low frame rate methods. In this paper, a sequence training approach based on minimum error and reinforcement learning is proposed for a hybrid ASR system to operate at a variable frame rate, and uses a frame selection controller to predict the number of frames to skip before taking the next inference action. The controller is integrated into the acoustic model in a multi-task training framework as an additional regression task and the controller output can be used for distribution characterisation during reinforcement learning exploration. The reinforcement learning objective minimises a combined measure of the phone error and average frame rate. ASR experiments using British English multi-genre broadcast (MGB3) data show that the proposed approach achieved a smaller frame rate than using a fixed 1/3 low frame rate method and was able to reduce the word error rate relative to both fixed low frame rate and full frame rate systems",
    "keywords": [],
    "checked": true,
    "id": "e5ea169427ade236fa3c409a9aea9b9b767e62aa",
    "semantic_title": "variable frame rate acoustic models using minimum error reinforcement learning",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kaland21_interspeech.html": {
    "title": "How f0 and Phrase Position Affect Papuan Malay Word Identification",
    "volume": "main",
    "abstract": "This paper reports a perception experiment on Papuan Malay, an Eastern Indonesian language for which phrase prosody is largely underresearched. While phrase-final f0 movements are the most prominent ones in this language, it remains to be seen to what extent they signal phrase boundaries (demarcating) or whether they contribute to the prosodic prominence of words in that position (highlighting). Crucially, it is unclear whether these functions can actually be teased apart. In an attempt to investigate this issue, a word identification experiment was carried out using manipulated and original f0 word contours in phrase-medial and phrase-final positions. Results indicate that Papuan Malay listeners recognize words faster in phrase-final position, although the shape of the f0 movement did not significantly affect response latencies. The outcomes are discussed in a typological perspective, with particular attention to Trade Malay languages",
    "keywords": [],
    "checked": true,
    "id": "05dc2dfea93acdcbb13e63b5dd22e33edfb8f9eb",
    "semantic_title": "how f0 and phrase position affect papuan malay word identification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jespersen21_interspeech.html": {
    "title": "On the Feasibility of the Danish Model of Intonational Transcription: Phonetic Evidence from Jutlandic Danish",
    "volume": "main",
    "abstract": "Most of our knowledge of Danish f0 variation and intonation is based on the work of Grønnum and colleagues, who developed an a-phonological model in which a series of repeated \"default\" contours are superpositioned onto an overarching f0 slope. The current paper tests a range of predictions stemming from this model, most importantly the adequacy of analysing f0 modulations as a string of repeated contours differing in range but not in shape. To facilitate comparison with earlier work in the area, our material is based on read speech, 45 speakers of Jutland Danish participated in the experiment. Analyses of f0 in sentences of differing complexity supplied little evidence in favour of the existence of default contours. Instead, our acoustic data revealed an array of f0 shapes associated with various prosodic anchor points, which are influenced in both range and shape by positional context and the presence or absence of focus",
    "keywords": [],
    "checked": true,
    "id": "0b9e83521bcca3a1ab1dafc1926269e59aba77bf",
    "semantic_title": "on the feasibility of the danish model of intonational transcription: phonetic evidence from jutlandic danish",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/meli21_interspeech.html": {
    "title": "An Experiment in Paratone Detection in a Prosodically Annotated EAP Spoken Corpus",
    "volume": "main",
    "abstract": "This article describes an experiment in paratone detection based on a spoken corpus of English for Academic Purposes (EAP) recently automatically re-annotated with prosodic information. The Momel and INTSINT annotations were carried out using SPPAS. The EIIDA corpus was chosen as it offered long uninterrupted stretches of speech of academic presentations. We describe the clustering method adopted for automatic detection, contrasting a supervised and an unsupervised method of paratone boundary detection. We showcase the relevance of the annotation scheme followed for this corpus and contribute to the investigation of the phonostyle of lecture delivery. We discuss the relevance of clustering methods applied to the labels of the pitch targets for the analysis of paratones",
    "keywords": [],
    "checked": true,
    "id": "3c6e5e98698c33545f8e12e605605e55d1b0a80b",
    "semantic_title": "an experiment in paratone detection in a prosodically annotated eap spoken corpus",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gerazov21_interspeech.html": {
    "title": "ProsoBeast Prosody Annotation Tool",
    "volume": "main",
    "abstract": "The labelling of speech corpora is a laborious and time-consuming process. The ProsoBeast Annotation Tool seeks to ease and accelerate this process by providing an interactive 2D representation of the prosodic landscape of the data, in which contours are distributed based on their similarity. This interactive map allows the user to inspect and label the utterances. The tool integrates several state-of-the-art methods for dimensionality reduction and feature embedding, including variational autoencoders. The user can use these to find a good representation for their data. In addition, as most of these methods are stochastic, each can be used to generate an unlimited number of different prosodic maps. The web app then allows the user to seamlessly switch between these alternative representations in the annotation process. Experiments with a sample prosodically rich dataset have shown that the tool manages to find good representations of varied data and is helpful both for annotation and label correction. The tool is released as free software for use by the community",
    "keywords": [],
    "checked": true,
    "id": "bfcf4012d0e08671dad420a970677bd1c624dc8c",
    "semantic_title": "prosobeast prosody annotation tool",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tran21_interspeech.html": {
    "title": "Assessing the Use of Prosody in Constituency Parsing of Imperfect Transcripts",
    "volume": "main",
    "abstract": "This work explores constituency parsing on automatically recognized transcripts of conversational speech. The neural parser is based on a sentence encoder that leverages word vectors contextualized with prosodic features, jointly learning prosodic feature extraction with parsing. We assess the utility of the prosody in parsing on imperfect transcripts, i.e. transcripts with automatic speech recognition (ASR) errors, by applying the parser in an N-best reranking framework. In experiments on Switchboard, we obtain 13–15% of the oracle N-best gain relative to parsing the 1-best ASR output, with insignificant impact on word recognition error rate. Prosody provides a significant part of the gain, and analyses suggest that it leads to more grammatical utterances via recovering function words",
    "keywords": [],
    "checked": true,
    "id": "a4a88f4bace2ee17934aef27df26c30be9ab555f",
    "semantic_title": "assessing the use of prosody in constituency parsing of imperfect transcripts",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21i_interspeech.html": {
    "title": "Targeted and Targetless Neutral Tones in Taiwanese Southern Min",
    "volume": "main",
    "abstract": "This article is an acoustic study on the two types of neutral tone in Taiwanese Southern Min (TSM). Recording materials included a set of verb-clitic constructions with different preceding tones and clitics. Pitch contours in different conditions were compared using Smoothing Spline ANOVA. Our results confirmed that Type 1 neutral tone (NT1) has a low pitch target and that Type 2 neutral tone (NT2) is contextually dependent. Whether NT1 or NT2 is chosen has been treated as the lexical idiosyncrasy of the clitics in question, with idiolectal and dialectal variations. However, we found in this study that the onsets have a bearing on determining the type of neutral tone: the more sonorous the onset, the more possible it is for the clitic to be in NT2. In sum, the two distinct types of neutral tones in TSM not only are unusual among the neutral tones in Sinitic languages, but they also offer novel data for the consonant-tone interaction",
    "keywords": [],
    "checked": true,
    "id": "b1bd84dfa8a04f7f81da6c6fb9ddfbcc11d29c8b",
    "semantic_title": "targeted and targetless neutral tones in taiwanese southern min",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gosy21_interspeech.html": {
    "title": "The Interaction of Word Complexity and Word Duration in an Agglutinative Language",
    "volume": "main",
    "abstract": "The mental lexicon comprises the representations of various words either in a morphologically decomposed form, or in a conceptually non-decomposed form. The durations of mono-morphemic and multimorphemic words are assumed to contain information on the routes of their lexical access The durations of Hungarian nouns with various lengths produced spontaneously by 10 young and 10 elderly speakers (with 55 years of difference between them) were measured. Findings showed significant differences depending on the words' complexity and on age. The nouns both with and without suffixes were significantly longer in old than in young speakers. The durational differences depending on age were more pronounced in monomorphemic nouns as opposed to multimorphemic nouns. Along with the increasing number of syllables of the nouns, old speakers produced increasingly longer simple nouns (stems) than young ones did We suggest that multimorphemic nouns are accessed decompositionally in spontaneous utterances when the stem activation is followed by the activation of the suffixes. The specific storage and the corresponding lexical access of the morphemes explain the longer durations of the inflected nouns",
    "keywords": [],
    "checked": true,
    "id": "fc63d5c579304c9d1535bfb27b1543a4d42d9d1e",
    "semantic_title": "the interaction of word complexity and word duration in an agglutinative language",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21b_interspeech.html": {
    "title": "Taiwan Min Nan (Taiwanese) Checked Tones Sound Change",
    "volume": "main",
    "abstract": "The multifaced changes of Taiwan Min Nan (TMN) checked sandhi tones, S3 and S5 were investigated as well as the checked base tones, B3 and B5. Simultaneous EGG data, CQ_H and acoustic data, including duration, f0 offset at 80% vowel interval, and spectral tilt H1 -A3 from forty male and female speakers above 40 and under 30 years of age were analyzed. Though different measures progress at different paces, in general, as the coda stops [p, t, k, ʔ] from full stop closure, to energy damping and finally to complete deletion, vowel duration lengthening, f0 offset lowering, and more modal phonation were observed. Gender effects were found on f0 offset and CQ_H offset. The pace of progress is more advanced for base tone B5 with glottal coda stops. After coda deletion, the contexts conditioning the anticipatory co-articulation were removed and vowel and tone characteristics were modified to be similar to those found in open syllables",
    "keywords": [],
    "checked": true,
    "id": "43e1e9149c67b87585ba30335da172237d4f3a75",
    "semantic_title": "taiwan min nan (taiwanese) checked tones sound change",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jakob21_interspeech.html": {
    "title": "In-Group Advantage in the Perception of Emotions: Evidence from Three Varieties of German",
    "volume": "main",
    "abstract": "Various studies on the perception of vocally expressed emotions have shown that recognition rates are higher if speaker and listener belong to the same cultural or linguistic group. This so-called is commonly attributed to prosodic differences in the expression of emotion across groups. Evidence comes mostly from using cross-linguistic and/or cross-cultural study designs. Previous research suggests that varieties of German differ in their use of prosody and can be discriminated based on prosodic features alone. In this paper, we tested whether emotion recognition rates differ across varieties of German: Listeners from three dialectal areas (Hamburg, Vienna, Zurich) identified emotions on semantically neutral sentences (choosing between anger, happiness, relief, surprise or \"other\"), spoken by actors from the three regions. Correctness rates show that emotions are recognized better if speakers and listeners are native speakers of the same variety. However, further analyses suggest that the in-group advantage does not surface consistently across individual emotions. To explain these results, the prosodic realization of the sentences was tested for interactions between emotion and variety. Here, intensity seemed to differ most across varieties and emotions. Importantly, we show that the in-group advantage extends from cultural groups to dialectal groups of a language",
    "keywords": [],
    "checked": true,
    "id": "2a7c6822b6a2661789ed0c95ce360b2e1057d2db",
    "semantic_title": "in-group advantage in the perception of emotions: evidence from three varieties of german",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gobl21_interspeech.html": {
    "title": "The LF Model in the Frequency Domain for Glottal Airflow Modelling Without Aliasing Distortion",
    "volume": "main",
    "abstract": "Many of the commonly used voice source models are based on piecewise elementary functions defined in the time domain. The discrete-time implementation of such models generally causes aliasing distortion, which make them less useful for certain applications. This paper presents a method which eliminates this distortion. The key component of the proposed method is the frequency domain description of the source model. By deploying the Laplace transform and phasor arithmetic, closed-form expressions of the source model spectrum can be derived. This facilitates the calculation of the spectrum directly from the model parameters, which in turn makes it possible to obtain the ideal discrete spectrum of the model given the sampling frequency used. This discrete spectrum is entirely free of aliasing distortion, and the inverse discrete Fourier transform is used to compute the sampled glottal flow pulse. The proposed method was applied to the widely used LF model, and the complete Laplace transform of the model is presented. Also included are closed-form expressions of the amplitude spectrum and the phase spectrum for the calculation of the LF model spectrum",
    "keywords": [],
    "checked": true,
    "id": "123196474679ca6e9ca6a924c0559a7628a5c78d",
    "semantic_title": "the lf model in the frequency domain for glottal airflow modelling without aliasing distortion",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wagner21_interspeech.html": {
    "title": "Parsing Speech for Grouping and Prominence, and the Typology of Rhythm",
    "volume": "main",
    "abstract": "Humans appear to be wired to perceive acoustic events rhythmically. English speakers, for example, tend to perceive alternating short and long sounds as a series of binary groups with a final beat (iambs), and alternating soft and loud sounds as a series of trochees. This generalization, often called the ‘Iambic-trochaic Law' (ITL), although viewed as an auditory universal by some, has been argued to be shaped by language experience. Earlier work on the ITL had a crucial limitation, in that it did not tease apart the percepts of grouping and prominence, which the notions of iamb and trochee inherently confound. We explore how intensity and duration relate to percepts of prominence and grouping in six languages (English, French, German, Japanese, Mandarin, and Spanish). The results show that the ITL is not universal, and that cue interpretation is shaped by language experience. However, there are also invariances: Duration appears relatively robust across languages as a cue to prominence (longer syllables are perceived as stressed), and intensity for grouping (louder syllables are perceived as initial). The results show the beginnings of a rhythmic typology based on how the dimensions of grouping and prominence are cued",
    "keywords": [],
    "checked": true,
    "id": "ac15e32b24c52d9960276206b157c91a038d551f",
    "semantic_title": "parsing speech for grouping and prominence, and the typology of rhythm",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mumtaz21_interspeech.html": {
    "title": "Prosody of Case Markers in Urdu",
    "volume": "main",
    "abstract": "This paper studies the prosody of case clitics in Urdu, for which various different claims exist in the literature. We conducted a production experiment and controlled for effects potentially arising from the phonetics of the case clitics, the syntactic function they express and clausal position. We find that case clitics are incorporated into the prosodic phrase of the noun and that they become part of the overall LH contour found on accentual phrases in Urdu/Hindi. We also find some differences across case type and position which we tie to information structural effects",
    "keywords": [],
    "checked": true,
    "id": "dd6af80070f2393c96e90d7d848c93b342380843",
    "semantic_title": "prosody of case markers in urdu",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stefansdottir21_interspeech.html": {
    "title": "Articulatory Characteristics of Icelandic Voiced Fricative Lenition: Gradience, Categoricity, and Speaker/Gesture-Specific Effects",
    "volume": "main",
    "abstract": "Icelandic voiced fricatives frequently reduce in connected speech. However, systematic investigations of the phenomenon from acoustic and articulatory perspectives are lacking. To further the understanding of this lenition process, we present electromagnetic articulography and acoustic data from four speakers concerning the intervocalic realization of the dental and velar fricatives. The results show that lenition is mostly gradient, but some speakers and places of articulation exhibit two distinct modes suggesting a categorical distinction. Moreover, in some tokens, the fricative constriction is absent from the articulatory trajectories. Finally, the relation between lenition and speech rate, style, and stress is also subject to speaker- and gesture-specific effects. We conclude by evaluating how our findings challenge the common assumptions, made in the literature, that lenition is a change in gestural target or a perceptually driven phenomenon",
    "keywords": [],
    "checked": true,
    "id": "edea50f850967489934f1a03bc196109299b6510",
    "semantic_title": "articulatory characteristics of icelandic voiced fricative lenition: gradience, categoricity, and speaker/gesture-specific effects",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/johnson21_interspeech.html": {
    "title": "Leveraging the Uniformity Framework to Examine Crosslinguistic Similarity for Long-Lag Stops in Spontaneous Cantonese-English Bilingual Speech",
    "volume": "main",
    "abstract": "While crosslinguistic influence is widespread in bilingual speech production, it is less clear which aspects of representation are shared across languages, if any. Most prior work examines phonetically distinct yet phonologically similar sounds, for which phonetic convergence suggests a cross-language link within individuals [1]. Convergence is harder to assess when sounds are already similar, as with English and Cantonese initial long-lag stops. Here, the articulatory uniformity framework [2, 3, 4] is leveraged to assess whether bilinguals share an underlying laryngeal feature across languages, and describe the nature of cross-language links. Using the SpiCE corpus of spontaneous Cantonese-English bilingual speech [5], this paper asks whether Cantonese-English bilinguals exhibit uniform voice-onset time for long-lag stops within and across languages. Results indicate moderate patterns of uniformity within-language — replicating prior work [2, 6] — and weaker patterns across languages. The analysis, however, raises many questions, as correlations were generally lower compared to prior work, and talkers did not adhere to expected ordinal relationships by place of articulation. Talkers also retained clear differences for /t/ and /k/, despite expectations of similarity. Yet at the same time, more of the overall variation seems to derive from individual-specific differences. While many questions remain, the uniformity framework shows promise",
    "keywords": [],
    "checked": true,
    "id": "02eb1b9c76c43ddfa0ecb806bb6fd7735f15fb67",
    "semantic_title": "leveraging the uniformity framework to examine crosslinguistic similarity for long-lag stops in spontaneous cantonese-english bilingual speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sivaraman21_interspeech.html": {
    "title": "Personalized Speech Enhancement Through Self-Supervised Data Augmentation and Purification",
    "volume": "main",
    "abstract": "Training personalized speech enhancement models is innately a no-shot learning problem due to privacy constraints and limited access to noise-free speech from the target user. If there is an abundance of unlabeled noisy speech from the test-time user, one may train a personalized speech enhancement model using self-supervised learning. One straightforward approach to model personalization is to use the target speaker's noisy recordings as pseudo-sources. Then, a pseudo denoising model learns to remove injected training noises and recover the pseudo-sources. However, this approach is volatile as it depends on the quality of the pseudo-sources, which may be too noisy. To remedy this, we propose a data purification step that refines the self-supervised approach. We first train an SNR predictor model to estimate the frame-by-frame SNR of the pseudo-sources. Then, we convert the predictor's estimates into weights that adjust the pseudo-sources' frame-by-frame contribution towards training the personalized model. We empirically show that the proposed data purification step improves the usability of the speaker-specific noisy data in the context of personalized speech enhancement. Our approach may be seen as privacy-preserving as it does not rely on any clean speech recordings or speaker embeddings",
    "keywords": [],
    "checked": true,
    "id": "77e924b0103cc5f3a25071ff80b943a883b9eecd",
    "semantic_title": "personalized speech enhancement through self-supervised data augmentation and purification",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saddler21_interspeech.html": {
    "title": "Speech Denoising with Auditory Models",
    "volume": "main",
    "abstract": "Contemporary speech enhancement predominantly relies on audio transforms that are trained to reconstruct a clean speech waveform. The development of high-performing neural network sound recognition systems has raised the possibility of using deep feature representations as ‘perceptual' losses with which to train denoising systems. We explored their utility by first training deep neural networks to classify either spoken words or environmental sounds from audio. We then trained an audio transform to map noisy speech to an audio waveform that minimized the difference in the deep feature representations between the output audio and the corresponding clean audio. The resulting transforms removed noise substantially better than baseline methods trained to reconstruct clean waveforms, and also outperformed previous methods using deep feature losses. However, a similar benefit was obtained simply by using losses derived from the filter bank inputs to the deep networks. The results show that deep features can guide speech enhancement, but suggest that they do not yet outperform simple alternatives that do not involve learned features",
    "keywords": [],
    "checked": true,
    "id": "477060a1424ef7885825f14fe3fe623078da451b",
    "semantic_title": "speech denoising with auditory models",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eskimez21b_interspeech.html": {
    "title": "Human Listening and Live Captioning: Multi-Task Training for Speech Enhancement",
    "volume": "main",
    "abstract": "With the surge of online meetings, it has become more critical than ever to provide high-quality speech audio and live captioning under various noise conditions. However, most monaural speech enhancement (SE) models introduce processing artifacts and thus degrade the performance of downstream tasks, including automatic speech recognition (ASR). This paper proposes a multi-task training framework to make the SE models unharmful to ASR. Because most ASR training samples do not have corresponding clean signal references, we alternately perform two model update steps called SE-step and ASR-step. The SE-step uses clean and noisy signal pairs and a signal-based loss function. The ASR-step applies a pre-trained ASR model to training signals enhanced with the SE model. A cross-entropy loss between the ASR output and reference transcriptions is calculated to update the SE model parameters. Experimental results with realistic large-scale settings using ASR models trained on 75,000-hour data show that the proposed framework improves the word error rate for the SE output by 11.82% with little compromise in the SE quality. Performance analysis is also carried out by changing the ASR model, the data used for the ASR-step, and the schedule of the two update steps",
    "keywords": [],
    "checked": true,
    "id": "dd4c34df951a68f751ae0e8c0d2195d692c74945",
    "semantic_title": "human listening and live captioning: multi-task training for speech enhancement",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21g_interspeech.html": {
    "title": "Multi-Stage Progressive Speech Enhancement Network",
    "volume": "main",
    "abstract": "Speech enhancement is a fundamental way to separate and generate clean speech from adverse environment where the received speech is seriously corrupted by noise. This paper applies a novel progressive network for speech enhancement by using multi-stage structure, where each stage contains a channel attention block followed by dilated encoder-decoder convolutional network with gated linear units. In addition, each stage generates a prediction that is refined by a supervised attention block. What is more, a fusion block is inserted between original inputs and outputs of previous stage. Multi-stage architecture is introduced to sequentially invoke multiple deep-learning networks, and its key ingredient is the information exchange between different stages. Thus, a more flexible and robust outputs can be generated. Experimental results show that the proposed architecture obtains consistently better performance than recent state-of-the-art models in terms of both PESQ and STOI scores",
    "keywords": [],
    "checked": true,
    "id": "9a09df361d780d404581d58198086b3366aa3bb8",
    "semantic_title": "multi-stage progressive speech enhancement network",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chang21c_interspeech.html": {
    "title": "Single-Channel Speech Enhancement Using Learnable Loss Mixup",
    "volume": "main",
    "abstract": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose , a simple and effortless training diagram, to improve the generalization of deep learning-based speech enhancement models , of which is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In , by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that achieves 3.26 PESQ, outperforming the state-of-the-art",
    "keywords": [],
    "checked": true,
    "id": "58b0962b0e632b8e74089018de7585565aab9cd4",
    "semantic_title": "single-channel speech enhancement using learnable loss mixup",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21q_interspeech.html": {
    "title": "A Maximum Likelihood Approach to SNR-Progressive Learning Using Generalized Gaussian Distribution for LSTM-Based Speech Enhancement",
    "volume": "main",
    "abstract": "A maximum likelihood (ML) approach to characterizing regression errors in each target layer of SNR progressive learning (PL) using long short-term memory (LSTM) networks is proposed to improve performances of speech enhancement at low SNR levels. Each LSTM layer is guided to learn an intermediate target with a specific SNR gain. In contrast to using previously proposed minimum squared error criterion (MMSE-PL-LSTM) which leads to an un-even distribution and a broad dynamic range of the prediction errors, we model the errors with a generalized Gaussian distribution (GGD) at all intermediate layers in the newly proposed ML-PL-LSTM framework. The shape factors in GGD can be automatically updated when training the LSTM networks in a layer-wise manner to estimate the network parameters progressively. Tested on the CHiME-4 simulation set for speech enhancement in unseen noise conditions, the proposed ML-PL-LSTM approach outperforms MMSE-PL-LSTM in terms of both PESQ and STOI measures. Furthermore, when evaluated on the CHiME-4 real test set for speech recognition, using ML-enhanced speech also results in less word error rates than those obtained with MMSE-enhanced speech",
    "keywords": [],
    "checked": true,
    "id": "3c716dfdd011dc5382cd85dbe1601c93a6f5ecab",
    "semantic_title": "a maximum likelihood approach to snr-progressive learning using generalized gaussian distribution for lstm-based speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/agrawal21_interspeech.html": {
    "title": "Whisper Speech Enhancement Using Joint Variational Autoencoder for Improved Speech Recognition",
    "volume": "main",
    "abstract": "Whispering is the natural choice of communication when one wants to interact quietly and privately. Due to vast differences in acoustic characteristics of whisper and natural speech, there is drastic degradation in the performance of whisper speech when decoded by the Automatic Speech Recognition (ASR) system trained on neutral speech. Recently, to handle this mismatched train and test scenario Denoising Autoencoders (DA) are used which gives some improvement. To improve over DA performance we propose another method to map speech from whisper domain to neutral speech domain via Joint Variational Auto-Encoder (JVAE). The proposed method requires time-aligned parallel data which is not available, so we developed an algorithm to convert parallel data to time-aligned parallel data. JVAE jointly learns the characteristics of whisper and neutral speech in a common latent space which significantly improves whisper recognition accuracy and outperforms traditional autoencoder based techniques. We benchmarked our method against two baselines, first being ASR trained on neutral speech and tested on whisper dataset and second being whisper test set mapped using DA and tested on same neutral ASR. We achieved an absolute improvement of 22.31% in Word Error Rate (WER) over the first baseline and an absolute 5.52% improvement over DA",
    "keywords": [],
    "checked": true,
    "id": "abbccac2854fef7313dca60e7971b73ddaa364cd",
    "semantic_title": "whisper speech enhancement using joint variational autoencoder for improved speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21d_interspeech.html": {
    "title": "DEMUCS-Mobile : On-Device Lightweight Speech Enhancement",
    "volume": "main",
    "abstract": "As the importance of speech enhancement for real-world application increases, the compactness of the model is also becoming a crucial study. In this paper, we present compression techniques to reduce the model size and applied them to the state-of-the-art real-time speech enhancement system. We successfully reduce the model size by actively applying channel pruning while maintaining performance. In particular, we propose a method to prune more channels of convolutional neural networks (CNN) by utilizing gated linear unit (GLU) activation. In addition, lower-bit-quantization is applied to reduce model size, while minimizing performance degradation caused by quantization. We show the performance of our proposed model on a mobile device where computing resources are limited. In particular, it is implemented to enable streaming, and speech enhancement works in real-time",
    "keywords": [],
    "checked": true,
    "id": "4f77d1c735c433c34e442b7497c64e62c5e16054",
    "semantic_title": "demucs-mobile : on-device lightweight speech enhancement",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kashyap21_interspeech.html": {
    "title": "Speech Denoising Without Clean Training Data: A Noise2Noise Approach",
    "volume": "main",
    "abstract": "This paper tackles the problem of the heavy dependence of clean speech data required by deep learning based audio-denoising methods by showing that it is possible to train deep speech denoising networks using only noisy speech samples. Conventional wisdom dictates that in order to achieve good speech denoising performance, there is a requirement for a large quantity of both noisy speech samples and perfectly clean speech samples, resulting in a need for expensive audio recording equipment and extremely controlled soundproof recording studios. These requirements pose significant challenges in data collection, especially in economically disadvantaged regions and for low resource languages. This work shows that speech denoising deep neural networks can be successfully trained utilizing only noisy training audio. Furthermore it is revealed that such training regimes achieve superior denoising performance over conventional training regimes utilizing clean training audio targets, in cases involving complex noise distributions and low Signal-to-Noise ratios (high noise environments). This is demonstrated through experiments studying the efficacy of our proposed approach over both real-world noises and synthetic noises using the 20 layered Deep Complex U-Net architecture",
    "keywords": [],
    "checked": true,
    "id": "52a71c59e6161d67a645c5aad3cd07f5b039eee2",
    "semantic_title": "speech denoising without clean training data: a noise2noise approach",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dang21_interspeech.html": {
    "title": "Improved Speech Enhancement Using a Complex-Domain GAN with Fused Time-Domain and Time-Frequency Domain Constraints",
    "volume": "main",
    "abstract": "Complex-domain models have achieved promising results for speech enhancement (SE) tasks. Some complex-domain models consider only time-frequency (T-F) domain constraints and do not take advantage of the information at the time-domain waveform level. Some complex-domain models consider only time-domain constraints and do not take into account T-F domain constraints that have rich harmonic structure information. Indeed some complex-domain models consider both time-domain and T-F domain constraints but only use the simple mean square loss as time-frequency-domain constraints. This paper proposes a complex-domain-based speech enhancement method that integrates time-domain constraints and T-F domain constraints into a unified framework using a Generative Adversarial Network (GAN). The proposed framework captures information at the time-domain waveform level features while paying attention to the harmonic structure by time-domain and T-F domain constraints. We conducted experiments on the Voice Bank + DEMAND dataset to evaluate the proposed method. Experimental results show that the proposed method improves the PESQ score by 0.09 and the STOI score by 1% over the strong baseline deep complex convolution recurrent network (DCCRN) and outperforms the state-of-the-art GAN-based SE systems",
    "keywords": [],
    "checked": true,
    "id": "e083c37e9e6ec5770f90b2a784a789d25f53d5dd",
    "semantic_title": "improved speech enhancement using a complex-domain gan with fused time-domain and time-frequency domain constraints",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21r_interspeech.html": {
    "title": "Speech Enhancement with Topology-Enhanced Generative Adversarial Networks (GANs)",
    "volume": "main",
    "abstract": "Speech enhancement is one of the effective approaches in improving speech quality. Neural network models have been widely used in speech enhancement, such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and generative adversarial networks (GANs). However, some of them either handle the speech noise removal tasks in the spectral domain or lack the waveform recovery capability. As a result, the enhanced speeches still include noisy signals. In this study, we propose a topology-enhanced GAN model to tackle noisy speeches in an end-to-end structure. We use the topology features of speech waves as additional constraints and modify the objective function of the GAN by adding a penalty term. The penalty term is a Wasserstein distance of topology features measuring the difference between the generated speech and the corresponding clean speech. We evaluate the proposed speech-enhanced model on the public speech data set with 56 speakers and 20 different types of noisy conditions. The experimental results indicate that the topology features improve the performance of GANs on speech enhancement in metrics of PESQ, CBAK, COVL, and SSNR",
    "keywords": [],
    "checked": true,
    "id": "baca0529cc076700621139504f02bd76463198d9",
    "semantic_title": "speech enhancement with topology-enhanced generative adversarial networks (gans)",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bu21_interspeech.html": {
    "title": "Learning Speech Structure to Improve Time-Frequency Masks",
    "volume": "main",
    "abstract": "Time-frequency (TF) masks are widely used in speech enhancement (SE). However, accurately estimating TF masks from noisy speech remains a challenge to both statistical or neural network approaches. Statistical model-based mask estimation usually depends on a good parameter initialization, while NN-based mask estimation relies on setting proper and stable learning targets. To address these issues, we propose a novel approach to extracting TF speech structures from clean speech data, and partition a noisy speech spectrogram into mutually exclusive regions of core speech, core noise, and transition. Using such region targets derived from clean speech, we train bidirectional LSTM to learn region prediction from noisy speech, which is easier to do than mask prediction. The predicted regions can further be used in place of masks in beamforming, or integrated with statistical and NN based mask estimation to constrain mask values and model parameter updates. Our experimental results on ASR (CHiME-3) and SE (CHiME-3 and LibriSpeech) have demonstrated the effectiveness of our approach of learning speech region structure to improve TF masks",
    "keywords": [],
    "checked": true,
    "id": "d9d3216a1ec640be3ee5468123e0f2fdeedc333f",
    "semantic_title": "learning speech structure to improve time-frequency masks",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21h_interspeech.html": {
    "title": "SE-Conformer: Time-Domain Speech Enhancement Using Conformer",
    "volume": "main",
    "abstract": "Convolution-augmented transformer (conformer) has recently shown competitive results in speech-domain applications, such as automatic speech recognition, continuous speech separation, and sound event detection. Conformer can capture both the short and long-term temporal sequence information by attending to the whole sequence at once with multi-head self-attention and convolutional neural network. However, the effectiveness of conformer in speech enhancement has not been demonstrated. In this paper, we propose an end-to-end speech enhancement architecture (SE-Conformer), incorporating a convolutional encoder–decoder and conformer, designed to be directly applied to the time-domain signal. We performed evaluations on both the VoiceBank-DEMAND Corpus (VCTK) and Librispeech datasets in terms of objective speech quality metrics. The experimental results show that the proposed model outperforms other competitive baselines in speech enhancement performance",
    "keywords": [],
    "checked": true,
    "id": "786e9c16e4f200029c021e26ddcbacb00e9983fe",
    "semantic_title": "se-conformer: time-domain speech enhancement using conformer",
    "citation_count": 55,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kongthaworn21_interspeech.html": {
    "title": "Spectral and Latent Speech Representation Distortion for TTS Evaluation",
    "volume": "main",
    "abstract": "One of the main problems in the development of text-to-speech (TTS) systems is its reliance on subjective measures, typically the Mean Opinion Score (MOS). MOS requires a large number of people to reliably rate each utterance, making the development process slow and expensive. Recent research on speech quality assessment tends to focus on training models to estimate MOS, which requires a large number of training data, something that might not be available in low-resource languages. We propose an objective assessment metric based on the DTW distance using the spectrogram and the high-level features from an Automatic Speech Recognition (ASR) model to cover both acoustic and linguistic information. Experiments on Thai TTS and the Blizzard Challenge datasets show that our method outperformed other baselines in both utterance- and system-level by a large margin in terms of correlation coefficients. Our metric also outperformed the best baseline by 9.58% when used in head-to-head utterance-level comparisons. Ablation studies suggest that the middle layers of the ASR model are most suitable for TTS evaluation when used in conjunction with spectral features",
    "keywords": [],
    "checked": true,
    "id": "50d2eb6ec7b10e4b36bbb3c5787e241a402e1ff9",
    "semantic_title": "spectral and latent speech representation distortion for tts evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/valentinibotinhao21_interspeech.html": {
    "title": "Detection and Analysis of Attention Errors in Sequence-to-Sequence Text-to-Speech",
    "volume": "main",
    "abstract": "Sequence-to-sequence speech synthesis models are notorious for gross errors such as skipping and repetition, commonly associated with failures in the attention mechanism. While a lot has been done to improve attention and decrease errors, this paper focuses instead on automatic error detection and analysis. We evaluated three objective metrics against error detection scores collected by human listening. All metrics were derived from the synthesised attention matrix alone and do not require a reference signal, relying on the expectation that errors occur when attention is dispersed or insufficient. Using one of this metrics as an analysis tool, we observed that gross errors are more likely to occur in longer sentences and in sentences with punctuation marks that indicate pause or break. We also found that mechanisms such as forcibly incremented attention have the potential for decreasing gross errors but to the detriment of naturalness. The results of the error detection evaluation revealed that two of the evaluated metrics were able to detect errors with a relatively high success rate, obtaining F-scores of up to 0.89 and 0.96",
    "keywords": [],
    "checked": true,
    "id": "c336fe5d5c91772ab72c4a4866daf6f7ff60d196",
    "semantic_title": "detection and analysis of attention errors in sequence-to-sequence text-to-speech",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zandie21_interspeech.html": {
    "title": "RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "This paper introduces , a new speech corpus for research on automated text-to-speech (TTS) systems. Publicly available TTS corpora are often noisy, recorded with multiple speakers, or lack quality male speech data. In order to meet the need for a high quality, publicly available male speech corpus within the field of speech recognition, we have designed and created which contains textual materials from real-world conversational settings. These materials contain over 10 hours of a professional male voice actor's speech recorded at 44.1 kHz. This corpus's design and pipeline make ideal for developing TTS systems in real-world applications. To provide a baseline for future research, protocols, and benchmarks, we trained 4 state-of-the-art speech models and a vocoder on The results show 3.36 in mean opinion scores (MOS) in our best model. We have made both the corpus and trained models for public use",
    "keywords": [],
    "checked": true,
    "id": "6ae1d3b44794a5c2002d77e0701c52965dd19f2b",
    "semantic_title": "ryanspeech: a corpus for conversational text-to-speech synthesis",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21c_interspeech.html": {
    "title": "AISHELL-3: A Multi-Speaker Mandarin TTS Corpus",
    "volume": "main",
    "abstract": "In this paper, we present AISHELL-3, a large-scale multi-speaker Mandarin speech corpus which could be used to train multi-speaker Text-To-Speech (TTS) systems. The corpus contains roughly 85 hours of emotion-neutral recordings spanning across 218 native Chinese mandarin speakers. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Moreover, transcripts in Chinese character-level and pinyin-level are provided along with the recordings. We also present some data processing strategies and techniques which match with the characteristics of the presented corpus and conduct experiments on multiple speech-synthesis systems to assess the quality of the generated speech samples, showing promising results. The corpus is available online under Apache v2.0 license",
    "keywords": [],
    "checked": true,
    "id": "32fa1612b63bfc554e57a914c87ee46f4f6428cb",
    "semantic_title": "aishell-3: a multi-speaker mandarin tts corpus and the baselines",
    "citation_count": 145,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eng21_interspeech.html": {
    "title": "Comparing Speech Enhancement Techniques for Voice Adaptation-Based Speech Synthesis",
    "volume": "main",
    "abstract": "This study investigates the use of speech enhancement techniques in creating text-to-speech voices with degraded or noisy speech. A number of synthetic voices were created using speech that was first degraded by different noise types at various signal-to-noise ratios (SNRs), then enhanced through four speech enhancement algorithms: Subspace, Wiener filter, SEGAN and a DNN-based method. Subjective listening tests show that the quality of the synthetic voices produced by subspace and the DNN-based method enhanced speech outperforms the quality of the voices created using Wiener filter or SEGAN enhanced speech at low SNRs, and speech enhanced by the subspace method results in higher quality synthetic speech at higher SNRs",
    "keywords": [],
    "checked": true,
    "id": "93f23b3aeb2c73296c5c35a7a7924555da58a2ab",
    "semantic_title": "comparing speech enhancement techniques for voice adaptation-based speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cui21c_interspeech.html": {
    "title": "EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model",
    "volume": "main",
    "abstract": "Recently, there has been an increasing interest in neural speech synthesis. While the deep neural network achieves the state-of-the-art result in text-to-speech (TTS) tasks, how to generate a more emotional and more expressive speech is becoming a new challenge to researchers due to the scarcity of high-quality emotion speech dataset and the lack of advanced emotional TTS model. In this paper, we first briefly introduce and publicly release a Mandarin emotion speech dataset including 9,724 samples with audio files and its emotion human-labeled annotation. After that, we propose a simple but efficient architecture for emotional speech synthesis called EMSpeech. Unlike those models which need additional reference audio as input, our model could predict emotion labels just from the input text and generate more expressive speech conditioned on the emotion embedding. In the experiment phase, we first validate the effectiveness of our dataset by an emotion classification task. Then we train our model on the proposed dataset and conduct a series of subjective evaluations. Finally, by showing a comparable performance in the emotional speech synthesis task, we successfully demonstrate the ability of the proposed model",
    "keywords": [],
    "checked": true,
    "id": "c2cb6f50e97c4bf66be9b4edccd9c35101a89afe",
    "semantic_title": "emovie: a mandarin emotion speech dataset with a simple emotional text-to-speech model",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rallabandi21_interspeech.html": {
    "title": "Perception of Social Speaker Characteristics in Synthetic Speech",
    "volume": "main",
    "abstract": "With the improved computational abilities, the usage of chatbots and conversational agents has become more prevalent. Therefore, it is essential that these agents exhibit certain social speaker characteristics in the generated speech. In this paper, we study the perception of such speaker characteristics in two commercial Text-to-Speech (TTS) systems, Amazon Polly and Google TTS. We carried out a 15-item semantic differential scaling test. The factor analysis provided us with three underlying dimensions that can be perceived from synthetic speech, warmth, competence, and extraversion. Our results show that we can perceive both interpersonal relationships and also personality traits from synthetic voices. Additionally, we observed that the female participants perceived male voices to be more responsible, energetic, relaxed, and enthusiastic. In comparison, male participants found female voices to be more reliable, accessible, and confident. A discussion on the comparison of our results with that of the studies on natural speech is also provided",
    "keywords": [],
    "checked": true,
    "id": "22db1e07fad7e3b59a5841077bb6f7bd3216618f",
    "semantic_title": "perception of social speaker characteristics in synthetic speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bakhturina21_interspeech.html": {
    "title": "Hi-Fi Multi-Speaker English TTS Dataset",
    "volume": "main",
    "abstract": "This paper introduces a new multi-speaker English dataset for training text-to-speech models. The dataset is based on LibriVox audiobooks and Project Gutenberg texts, both in the public domain. The new dataset contains about 292 hours of speech from 10 speakers with at least 17 hours per speaker sampled at 44.1 kHz. To select speech samples with high quality, we considered audio recordings with a signal bandwidth of at least 13 kHz and a signal-to-noise ratio (SNR) of at least 32 dB. The dataset is publicly released",
    "keywords": [],
    "checked": true,
    "id": "784bce5095598c143131db754e4189aeaa3b9828",
    "semantic_title": "hi-fi multi-speaker english tts dataset",
    "citation_count": 53,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tseng21b_interspeech.html": {
    "title": "Utilizing Self-Supervised Representations for MOS Prediction",
    "volume": "main",
    "abstract": "Speech quality assessment has been a critical issue in speech processing for decades. Existing automatic evaluations usually require clean references or parallel ground truth data, which is infeasible when the amount of data soars. Subjective tests, on the other hand, do not need any additional clean or parallel data and correlates better to human perception. However, such a test is expensive and time-consuming because crowd work is necessary. It thus becomes highly desired to develop an automatic evaluation approach that correlates well with human perception while not requiring ground truth data. In this paper, we use self-supervised pre-trained models for MOS prediction. We show their representations can distinguish between clean and noisy audios. Then, we fine-tune these pre-trained models followed by simple linear layers in an end-to-end manner. The experiment results showed that our framework outperforms the two previous state-of-the-art models by a significant improvement on Voice Conversion Challenge 2018 and achieves comparable or superior performance on Voice Conversion Challenge 2016. We also conducted an ablation study to further investigate how each module benefits the task. The experiment results are implemented and reproducible with publicly available toolkits",
    "keywords": [],
    "checked": true,
    "id": "4481e1272a9b94ef87e8ee401cff267e6676c09e",
    "semantic_title": "utilizing self-supervised representations for mos prediction",
    "citation_count": 44,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mussakhojayeva21_interspeech.html": {
    "title": "KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset",
    "volume": "main",
    "abstract": "This paper introduces a high-quality open-source speech synthesis dataset for Kazakh, a low-resource language spoken by over 13 million people worldwide. The dataset consists of about 93 hours of transcribed audio recordings spoken by two professional speakers (female and male). It is the first publicly available large-scale dataset developed to promote Kazakh text-to-speech (TTS) applications in both academia and industry. In this paper, we share our experience by describing the dataset development procedures and faced challenges, and discuss important future directions. To demonstrate the reliability of our dataset, we built baseline end-to-end TTS models and evaluated them using the subjective mean opinion score (MOS) measure. Evaluation results show that the best TTS models trained on our dataset achieve MOS above 4 for both speakers, which makes them applicable for practical use. The dataset, training recipe, and pretrained TTS models are freely available",
    "keywords": [],
    "checked": true,
    "id": "3a4ac071f77050e73938c6beae20a47c6ee84f87",
    "semantic_title": "kazakhtts: an open-source kazakh text-to-speech synthesis dataset",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/taylor21_interspeech.html": {
    "title": "Confidence Intervals for ASR-Based TTS Evaluation",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) is increasingly used to evaluate the intelligibility of text-to-speech synthesis (TTS). ASR is less costly than traditional listening tests, but questions remain about its reliability. We re-evaluate the Blizzard Challenge's intelligibility tasks in English since 2011 using ASR. Re-analysing transcriptions collected by paid in-lab participants, online volunteers and Amazon Mechanical Turkers (the latter used only in 2011), we compare their word error rates (WERs) and statistically-significant system-groupings with those generated by an open-source, Transformer-based ASR model. This ASR model consistently decodes test stimuli with more reliable WERs than the Blizzard Challenge's (mostly non-native) speech experts and online volunteers. The model also groups systems according to statistical significance similarly to the paid in-lab participants. Using surplus semantically unpredictable sentences (SUS) submitted every year to the challenge, we investigate how confidence intervals in ASR WERs change as the number of transcribed stimuli increases. We plot the Frobenius norm of pairwise significance matrices with increasing stimuli. We find that finer groupings of systems are detected as confidence intervals narrow. The number of stimuli where p-values start to converge ranges from 400–800 stimuli. We conclude that, with enough stimuli, ASR can be more reliable than humans",
    "keywords": [],
    "checked": false,
    "id": "1ad9f30bacbf59309bf81ff1c575fc43ec479baa",
    "semantic_title": "edinburgh research explorer confidence intervals for asr-based tts evaluation",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/reddy21_interspeech.html": {
    "title": "INTERSPEECH 2021 Deep Noise Suppression Challenge",
    "volume": "main",
    "abstract": "The Deep Noise Suppression (DNS) challenge was designed to unify the research efforts in the area of noise suppression targeted for human perception. We recently organized a DNS challenge special session at INTERSPEECH 2020 and ICASSP 2021. We open-sourced training and test datasets for the wideband scenario along with a subjective evaluation framework based on ITU-T standard P.808, which was used to evaluate participants of the challenge. Many researchers from academia and industry made significant contributions to push the field forward, yet even the best noise suppressor was far from achieving superior speech quality in challenging scenarios. In this version of the challenge organized at INTERSPEECH 2021, we expanded our training and test datasets to accommodate fullband scenarios and challenging test conditions. We used ITU-T P.835 to evaluate the challenge winners as it gives additional information about the quality of processed speech and residual noise. The two tracks in this challenge focused on real-time denoising for (i) wideband, and (ii) fullband scenarios. We also made available a reliable non-intrusive objective speech quality metric for wideband called DNSMOS for the participants to use during their development phase",
    "keywords": [],
    "checked": true,
    "id": "042aa769e2779cffb7890abfeb72a6b7a4f0b434",
    "semantic_title": "interspeech 2021 deep noise suppression challenge",
    "citation_count": 122,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21g_interspeech.html": {
    "title": "A Simultaneous Denoising and Dereverberation Framework with Target Decoupling",
    "volume": "main",
    "abstract": "Background noise and room reverberation are regarded as two major factors to degrade the subjective speech quality. In this paper, we propose an integrated framework to address simultaneous denoising and dereverberation under complicated scenario environments. It adopts a chain optimization strategy and designs four sub-stages accordingly. In the first two stages, we decouple the multi-task learning w.r.t. complex spectrum into magnitude and phase, and only implement noise and reverberation removal in the magnitude domain. Based on the estimated priors above, we further polish the spectrum in the third stage, where both magnitude and phase information are explicitly repaired with the residual learning. Due to the data mismatch and nonlinear effect of DNNs, the residual noise often exists in the DNN-processed spectrum. To resolve the problem, we adopt a light-weight algorithm as the post-processing module to capture and suppress the residual noise in the non-active regions. In the Interspeech 2021 Deep Noise Suppression (DNS) Challenge, our submitted system ranked top-1 for the real-time track in terms of Mean Opinion Score (MOS) with ITU-T P.835 framework",
    "keywords": [],
    "checked": true,
    "id": "4e64cf80648a274863d577ebb68296ba4c31d3c6",
    "semantic_title": "a simultaneous denoising and dereverberation framework with target decoupling",
    "citation_count": 51,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21h_interspeech.html": {
    "title": "Deep Noise Suppression with Non-Intrusive PESQNet Supervision Enabling the Use of Real Training Data",
    "volume": "main",
    "abstract": "Data-driven speech enhancement employing deep neural networks (DNNs) can provide state-of-the-art performance even in the presence of non-stationary noise. During the training process, most of the speech enhancement neural networks are trained in a fully supervised way with losses requiring noisy speech to be synthesized by clean speech and additive noise. However, in a real implementation, only the noisy speech mixture is available, which leads to the question, how such data could be advantageously employed in training. In this work, we propose an end-to-end non-intrusive PESQNet DNN which estimates perceptual evaluation of speech quality (PESQ) scores, allowing a reference-free loss for real data. As a further novelty, we combine the PESQNet loss with denoising and dereverberation loss terms, and train a complex mask-based fully convolutional recurrent neural network (FCRN) in a \"weakly\" supervised way, each training cycle employing some synthetic data, some real data, and again synthetic data to keep the PESQNet up-to-date. In a subjective listening test, our proposed framework outperforms the Interspeech 2021 Deep Noise Suppression (DNS) Challenge baseline overall by 0.09 MOS points and in particular by 0.45 background noise MOS points",
    "keywords": [],
    "checked": true,
    "id": "b0bab1214d990fcd221a1c8734db90b1272a1e3e",
    "semantic_title": "deep noise suppression with non-intrusive pesqnet supervision enabling the use of real training data",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/le21b_interspeech.html": {
    "title": "DPCRN: Dual-Path Convolution Recurrent Network for Single Channel Speech Enhancement",
    "volume": "main",
    "abstract": "The dual-path RNN (DPRNN) was proposed to more effectively model extremely long sequences for speech separation in the time domain. By splitting long sequences to smaller chunks and applying intra-chunk and inter-chunk RNNs, the DPRNN reached promising performance in speech separation with a limited model size. In this paper, we combine the DPRNN module with Convolution Recurrent Network (CRN) and design a model called Dual-Path Convolution Recurrent Network (DPCRN) for speech enhancement in the time-frequency domain. We replace the RNNs in the CRN with DPRNN modules, where the intra-chunk RNNs are used to model the spectrum pattern in a single frame and the inter-chunk RNNs are used to model the dependence between consecutive frames. With only 0.8M parameters, the submitted DPCRN model achieves an overall mean opinion score (MOS) of 3.57 in the wide band scenario track of the Interspeech 2021 Deep Noise Suppression (DNS) challenge. Evaluations on some other test sets also show the efficacy of our model",
    "keywords": [],
    "checked": true,
    "id": "26196f075cd4381cf5e02c3dae9b02699d72371a",
    "semantic_title": "dpcrn: dual-path convolution recurrent network for single channel speech enhancement",
    "citation_count": 37,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lv21_interspeech.html": {
    "title": "DCCRN+: Channel-Wise Subband DCCRN with SNR Estimation for Speech Enhancement",
    "volume": "main",
    "abstract": "Deep complex convolution recurrent network (DCCRN), which extends CRN with complex structure, has achieved superior performance in MOS evaluation in Interspeech 2020 deep noise suppression challenge (DNS2020). This paper further extends DCCRN with the following significant revisions. We first extend the model to sub-band processing where the bands are split and merged by learnable neural network filters instead of engineered FIR filters, leading to a faster noise suppressor trained in an end-to-end manner. Then the LSTM is further substituted with a complex TF-LSTM to better model temporal dependencies along both time and frequency axes. Moreover, instead of simply concatenating the output of each encoder layer to the input of the corresponding decoder layer, we use convolution blocks to first aggregate essential information from the encoder output before feeding it to the decoder layers. We specifically formulate the decoder with an extra SNR estimation module to maintain good speech quality while removing noise. Finally a post-processing module is adopted to further suppress the unnatural residual noise. The new model, named DCCRN+, has surpassed the original DCCRN as well as several competitive models in terms of PESQ and DNSMOS, and has achieved superior performance in the new Interspeech 2021 DNS challenge",
    "keywords": [],
    "checked": true,
    "id": "5dcf3bda9c02857b9039f2d14e2ae4d115fbc510",
    "semantic_title": "dccrn+: channel-wise subband dccrn with snr estimation for speech enhancement",
    "citation_count": 66,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21s_interspeech.html": {
    "title": "DBNet: A Dual-Branch Network Architecture Processing on Spectrum and Waveform for Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "In real acoustic environment, speech enhancement is an arduous task to improve the quality and intelligibility of speech interfered by background noise and reverberation. Over the past years, deep learning has shown great potential on speech enhancement. In this paper, we propose a novel real-time framework called DBNet which is a dual-branch structure with alternate interconnection. Each branch incorporates an encoder-decoder architecture with skip connections. The two branches are responsible for spectrum and waveform modeling, respectively. A bridge layer is adopted to exchange information between the two branches. Systematic evaluation and comparison show that the proposed system substantially outperforms related algorithms under very challenging environments. And in INTERSPEECH 2021 Deep Noise Suppression (DNS) challenge, the proposed system ranks the top 8 in real-time track 1 in terms of the Mean Opinion Score (MOS) of the ITU-T P.835 framework",
    "keywords": [],
    "checked": true,
    "id": "6d5f52b8a4c5174bfb2d27fb5cf3531702f09c36",
    "semantic_title": "dbnet: a dual-branch network architecture processing on spectrum and waveform for single-channel speech enhancement",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21t_interspeech.html": {
    "title": "Low-Delay Speech Enhancement Using Perceptually Motivated Target and Loss",
    "volume": "main",
    "abstract": "Speech enhancement approaches based on deep neural network have outperformed the traditional signal processing methods. This paper presents a low-delay speech enhancement method that employs a new perceptually motivated training target and loss function. The proposed approach can achieve similar speech enhancement performance compared to the state-of-the-art approaches, but with significantly less latency and computational complexities. Judged by the MOS tests conducted by the INTERSPEECH 2021 Deep Noise Suppression Challenge organizer, the proposed method is ranked the 2 place for Background Noise MOS, and the 6 place for overall MOS",
    "keywords": [],
    "checked": true,
    "id": "f88340eb42f126e38c6b07d5bbd7927eba73c2ec",
    "semantic_title": "low-delay speech enhancement using perceptually motivated target and loss",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oostermeijer21_interspeech.html": {
    "title": "Lightweight Causal Transformer with Local Self-Attention for Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "In this paper, we describe a novel speech enhancement transformer architecture. The model uses local causal self-attention, which makes it lightweight and therefore particularly well-suited for real-time speech enhancement in computation resource-limited environments. In addition, we provide several ablation studies that focus on different parts of the model and the loss function to figure out which modifications yield best improvements. Using this knowledge, we propose a final version of our architecture, that we sent in to the INTERSPEECH 2021 DNS Challenge, where it achieved competitive results, despite using only 2% of the maximally allowed computation. Furthermore, we performed experiments to compare it with with LSTM and CNN models, that had 127% and 257% more parameters, respectively. Despite this difference in model size, we achieved significant improvements on the considered speech quality and intelligibility measures",
    "keywords": [],
    "checked": true,
    "id": "35e31785d64574f6f5a89be81ae934c616d3753a",
    "semantic_title": "lightweight causal transformer with local self-attention for real-time speech enhancement",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ristea21_interspeech.html": {
    "title": "Self-Paced Ensemble Learning for Speech and Audio Classification",
    "volume": "main",
    "abstract": "Combining multiple machine learning models into an ensemble is known to provide superior performance levels compared to the individual components forming the ensemble. This is because models can complement each other in taking better decisions. Instead of just combining the models, we propose a self-paced ensemble learning scheme in which models learn from each other over several iterations. During the self-paced learning process based on pseudo-labeling, in addition to improving the individual models, our ensemble also gains knowledge about the target domain. To demonstrate the generality of our self-paced ensemble learning (SPEL) scheme, we conduct experiments on three audio tasks. Our empirical results indicate that SPEL significantly outperforms the baseline ensemble models. We also show that applying self-paced learning on individual models is less effective, illustrating the idea that models in the ensemble actually learn from each other",
    "keywords": [],
    "checked": true,
    "id": "5acdfd80ebbf335664cea3ce2d6c8729eabdf76a",
    "semantic_title": "self-paced ensemble learning for speech and audio classification",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kojima21_interspeech.html": {
    "title": "Knowledge Distillation for Streaming Transformer–Transducer",
    "volume": "main",
    "abstract": "We explore knowledge distillation methods from nonstreaming to streaming Transformer–Transducer (T–T) models. Streaming T–T truncates future context. It leads to recognition quality degradation compared with the original T–T. In this work, we explore knowledge distillation, which minimizes internal representations in all Transformer layers between nonstreaming and streaming T–T models. In the experiment, we compared two different methods: the minimization of the L2 distance of hidden vectors and the minimization of the L2 distance of heads. All experiments were conducted using the public LibriSpeech corpus. Results of the experiment showed that hidden vector similarity-based knowledge distillation is better than multi-head similarity-based knowledge distillation. We observed 3.5% and 2.1% relative reductions in word error rate compared with the original streaming T–T in test-clean set and test-other set, respectively",
    "keywords": [],
    "checked": true,
    "id": "4cee98aa39b4fbe2ae4ab09e6b7848033e9d64a4",
    "semantic_title": "knowledge distillation for streaming transformer-transducer",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lohrenz21_interspeech.html": {
    "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Stream fusion, also known as system combination, is a common technique in automatic speech recognition for traditional hybrid hidden Markov model approaches, yet mostly unexplored for modern deep neural network end-to-end model architectures. Here, we investigate various fusion techniques for the all-attention-based encoder-decoder architecture known as the transformer, striving to achieve optimal fusion by investigating different fusion levels in an example single-microphone setting with fusion of standard magnitude and phase features. We introduce a novel multi-encoder learning method that performs a weighted combination of two encoder-decoder multi-head attention outputs during training. Employing then only the magnitude feature encoder in inference, we are able to show consistent improvement on Wall Street Journal (WSJ) with language model and on Librispeech, without increase in runtime or parameters. Combining two such multi-encoder trained models by a simple late fusion in inference, we achieve state-of-the-art performance for transformer-based models on WSJ with a significant WER reduction of 19% relative compared to the current benchmark approach",
    "keywords": [],
    "checked": true,
    "id": "48826cfff563ae41b72df10c7aa3cb946c7dada6",
    "semantic_title": "multi-encoder learning and stream fusion for transformer-based end-to-end automatic speech recognition",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zaiem21_interspeech.html": {
    "title": "Conditional Independence for Pretext Task Selection in Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Through solving pretext tasks, self-supervised learning (SSL) leverages unlabeled data to extract useful latent representations replacing traditional input features in the downstream task. A common pretext task consists in pretraining a SSL model on pseudo-labels derived from the original signal. This technique is particularly relevant for speech data where various meaningful signal processing features may serve as pseudo-labels. However, the process of selecting pseudo-labels, for speech or other types of data, remains mostly unexplored and currently relies on observing the results on the final downstream task. Nevertheless, this methodology is not sustainable at scale due to substantial computational (hence carbon) costs. Thus, this paper introduces a practical and theoretical framework to select relevant pseudo-labels with respect to a given downstream task. More precisely, we propose a functional estimator of the pseudo-label utility grounded in the conditional independence theory, which does not require any training. The experiments conducted on speaker recognition and automatic speech recognition validate our estimator, showing a significant correlation between the performance observed on the downstream task and the utility estimates obtained with our approach, facilitating the prospection of relevant pseudo-labels for self-supervised speech representation learning",
    "keywords": [],
    "checked": true,
    "id": "319eaa01b4e63b484e4ad3c3ff8980967dae0a2d",
    "semantic_title": "conditional independence for pretext task selection in self-supervised speech representation learning",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zeineldeen21_interspeech.html": {
    "title": "Investigating Methods to Improve Language Model Integration for Attention-Based Encoder-Decoder ASR Models",
    "volume": "main",
    "abstract": "Attention-based encoder-decoder (AED) models learn an implicit internal language model (ILM) from the training transcriptions. The integration with an external LM trained on much more unpaired text usually leads to better performance. A Bayesian interpretation as in the hybrid autoregressive transducer (HAT) suggests dividing by the prior of the discriminative acoustic model, which corresponds to this implicit LM, similarly as in the hybrid hidden Markov model approach. The implicit LM cannot be calculated efficiently in general and it is yet unclear what are the best methods to estimate it. In this work, we compare different approaches from the literature and propose several novel methods to estimate the ILM directly from the AED model. Our proposed methods outperform all previous approaches. We also investigate other methods to suppress the ILM mainly by decreasing the capacity of the AED model, limiting the label context, and also by training the AED model together with a pre-existing LM",
    "keywords": [],
    "checked": true,
    "id": "3ba5ccc491ebdb61d138db0a85373a35853cb568",
    "semantic_title": "investigating methods to improve language model integration for attention-based encoder-decoder asr models",
    "citation_count": 33,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vyas21b_interspeech.html": {
    "title": "Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model",
    "volume": "main",
    "abstract": "In this work, we investigate if the wav2vec 2.0 self-supervised pretraining helps mitigate the overfitting issues with connectionist temporal classification (CTC) training to reduce its performance gap with flat-start lattice-free MMI (E2E-LFMMI) for automatic speech recognition with limited training data. Towards that objective, we use the pretrained wav2vec 2.0 BASE model and fine-tune it on three different datasets including out-of-domain (Switchboard) and cross-lingual (Babel) scenarios. Our results show that for supervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI and CTC achieve similar results; significantly outperforming the baselines trained only with supervised data. Fine-tuning the wav2vec 2.0 model with E2E-LFMMI and CTC we obtain the following relative WER improvements over the supervised baseline trained with E2E-LFMMI. We get relative improvements of 40% and 44% on the clean-set and 64% and 58% on the test set of Librispeech (100h) respectively. On Switchboard (300h) we obtain relative improvements of 33% and 35% respectively. Finally, for Babel languages, we obtain relative improvements of 26% and 23% on Swahili (38h) and 18% and 17% on Tagalog (84h) respectively",
    "keywords": [],
    "checked": true,
    "id": "cb6f9229c283a8c04af3c80ead448e0cba8c3df0",
    "semantic_title": "comparing ctc and lfmmi for out-of-domain adaptation of wav2vec 2.0 acoustic model",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/moine21_interspeech.html": {
    "title": "Speaker Attentive Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) task has known significant improvements over the last years with the advent of Deep Neural Networks (DNNs). However, even the most successful methods are still rather failing when adaptation to specific speakers and scenarios is needed, inevitably leading to poorer performances when compared to humans. In this paper, we present novel work based on the idea of teaching the emotion recognition network about speaker identity. Our system is a combination of two ACRNN classifiers respectively dedicated to speaker and emotion recognition. The first informs the latter through a Self Speaker Attention (SSA) mechanism that is shown to considerably help to focus on emotional information of the speech signal. Speaker-dependant experiments on social attitudes database Att-HACK and IEMOCAP corpus demonstrate the effectiveness of the proposed method and achieve the state-of-the-art performance in terms of unweighted average recall",
    "keywords": [],
    "checked": true,
    "id": "d03e0ba08b2cb2d066a23a9769fb5be11ec1c8c6",
    "semantic_title": "speaker attentive speech emotion recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leem21_interspeech.html": {
    "title": "Separation of Emotional and Reconstruction Embeddings on Ladder Network to Improve Speech Emotion Recognition Robustness in Noisy Conditions",
    "volume": "main",
    "abstract": "When (SER) is applied in an actual application, the system should be able to cope with audio acquired in a noisy, unconstrained environment. Most studies on noise-robust SER require a parallel dataset with emotion labels, which is impractical to collect, or use speech with artificially added noise, which does not resemble practical conditions. This study builds upon the ladder network formulation, which can effectively compensate the environmental differences between a clean speech corpus and real-life recordings. This study proposes a decoupled ladder network, which increases the robustness of the SER system against the influences of non-stationary background noise by decoupling the last hidden layer embedding into emotion and reconstruction embeddings. This novel implementation allows the emotion embedding to focus exclusively on building a discriminative representation, without worrying about the reconstruction task. We introduce a noisy version of the MSP-Podcast database, which contains audio segments collected with a smartphone that simultaneously records sentences from the corpus and non-stationary noise at different (SNRs). We test the effectiveness of our proposed model with this corpus, showing that the decoupled ladder network can increase the performance of the regular ladder network when dealing with noisy recordings",
    "keywords": [],
    "checked": true,
    "id": "5ee5ddeb3340ae28fafe58765a4d124ab9def119",
    "semantic_title": "separation of emotional and reconstruction embeddings on ladder network to improve speech emotion recognition robustness in noisy conditions",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/georgiou21_interspeech.html": {
    "title": "M3: MultiModal Masking Applied to Sentiment Analysis",
    "volume": "main",
    "abstract": "A common issue when training multimodal architectures is that not all modalities contribute equally to the model's prediction and the network tends to over-rely on the strongest modality. In this work, we present M , a training procedure based on modality masking for deep multimodal architectures. During network training, we randomly select one modality and mask its features, forcing the model to make its prediction in the absence of this modality. This structured regularization allows the network to better exploit complementary information in input modalities. We implement M as a generic layer that can be integrated with any multimodal architecture. Our experiments show that M outperforms other masking schemes and improves performance for our strong baseline. We evaluate M for multimodal sentiment analysis on CMU-MOSEI, achieving results comparable to the state-of-the-art",
    "keywords": [],
    "checked": true,
    "id": "96b53994efb99ebb08fb89fa51c0818af92d3c07",
    "semantic_title": "m3: multimodal masking applied to sentiment analysis",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/klejch21_interspeech.html": {
    "title": "The CSTR System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "This paper describes the CSTR submission to the Multilingual and Code-Switching ASR Challenges at Interspeech 2021. For the multilingual track of the challenge, we trained a multilingual CNN-TDNN acoustic model for Gujarati, Hindi, Marathi, Odia, Tamil and Telugu and subsequently fine-tuned the model on monolingual training data. A language model built on a mixture of training and CommonCrawl data was used for decoding. We also demonstrate that crawled data from YouTube can be successfully used to improve the performance of the acoustic model with semi-supervised training. These models together with confidence based language identification achieve the average WER of 18.1%, a 41% relative improvement compared to the provided multilingual baseline model. For the code-switching track of the challenge we again train a multilingual model on Bengali and Hindi technical lectures and we employ a language model trained on CommonCrawl Bengali and Hindi data mixed with in-domain English data, using a novel transliteration method to generate pronunciations for the English terms. The final model improves by 18% and 34% relative compared to our multilingual baseline. Both our systems were among the top-ranked entries to the challenge",
    "keywords": [],
    "checked": true,
    "id": "1a81d6466c05244ee6f2c2610702603f07caace9",
    "semantic_title": "the cstr system for multilingual and code-switching asr challenges for low resource indian languages",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21d_interspeech.html": {
    "title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Subword units are commonly used for end-to-end automatic speech recognition (ASR), while a fully acoustic-oriented subword modeling approach is somewhat missing. We propose an acoustic data-driven subword modeling (ADSM) approach that adapts the advantages of several text-based and acoustic-based subword methods into one pipeline. With a fully acoustic-oriented label design and learning process, ADSM produces acoustic-structured subword units and acoustic-matched target sequence for further ASR training. The obtained ADSM labels are evaluated with different end-to-end ASR approaches including CTC, RNN-Transducer and attention models. Experiments on the LibriSpeech corpus show that ADSM clearly outperforms both byte pair encoding (BPE) and pronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis shows that ADSM achieves acoustically more logical word segmentation and more balanced sequence length, and thus, is suitable for both time-synchronous and label-synchronous models. We also briefly describe how to apply acoustic-based subword regularization and unseen text segmentation using ADSM",
    "keywords": [],
    "checked": true,
    "id": "012e96816bc5e0a2b5eeb26aec5175b39cefbfdc",
    "semantic_title": "acoustic data-driven subword modeling for end-to-end speech recognition",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21e_interspeech.html": {
    "title": "Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept",
    "volume": "main",
    "abstract": "With the advent of direct models in automatic speech recognition (ASR), the formerly prevalent frame-wise acoustic modeling based on hidden Markov models (HMM) diversified into a number of modeling architectures like encoder-decoder attention models, transducer models and segmental models (direct HMM). While transducer models stay with a frame-level model definition, segmental models are defined on the level of label segments directly. While (soft-)attention-based models avoid explicit alignment, transducer and segmental approach internally do model alignment, either by segment hypotheses or, more implicitly, by emitting so-called blank symbols. In this work, we prove that the widely used class of RNN-Transducer models and segmental models (direct HMM) are equivalent and therefore show equal modeling power. It is shown that blank probabilities translate into segment length probabilities and vice versa. In addition, we provide initial experiments investigating decoding and beam-pruning, comparing time-synchronous and label-/segment-synchronous search strategies and their properties using the same underlying model",
    "keywords": [],
    "checked": true,
    "id": "e3ee3bee2f091a5e54061b4aa15082a7510640e1",
    "semantic_title": "equivalence of segmental and neural transducer modeling: a proof of concept",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/khosravani21_interspeech.html": {
    "title": "Modeling Dialectal Variation for Swiss German Automatic Speech Recognition",
    "volume": "main",
    "abstract": "We describe a speech recognition system for Swiss German, a dialectal spoken language in German-speaking Switzerland. Swiss German has no standard orthography, with a significant variation in its written form. To alleviate the uncertainty associated with this variability, we automatically generate a lexicon from which multiple written forms of a given word in any dialect can be generated. The lexicon is built from a small (incomplete) handcrafted lexicon designed by linguistic experts and contains forms of common words in various Swiss German dialects. We exploit the powerful speech representation of self-supervised acoustic pre-training (wav2vec) to address the low-resource nature of the spoken dialects. The proposed approach results in an overall relative improvement of 9% word error rate compared to one based on an expert-generated lexicon for our TV Box voice assistant application",
    "keywords": [],
    "checked": true,
    "id": "ecc3d732f00915b3eaf20d40607cf8284ff36419",
    "semantic_title": "modeling dialectal variation for swiss german automatic speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/egorova21_interspeech.html": {
    "title": "Out-of-Vocabulary Words Detection with Attention and CTC Alignments in an End-to-End ASR System",
    "volume": "main",
    "abstract": "This work explores the effectiveness of detecting positions of out-of-vocabulary words (OOVs) in a decoded utterance using attention weights and CTC per-frame outputs of an end-to-end system predicting word sequences. We show that the end-to-end approach can be effective for the task of OOV detection. CTC alignments are shown to provide better temporal information about the positions of OOV words than attention, and therefore are more suitable for the task. The detected positions of OOV occurrences are utilized for the recurrent OOV recovery task in which probabilistic representations of the pronunciations of the detected OOVs are clustered in order to find repeating words. Improved detection results are shown to correlate with better performance of the recovery of recurrent OOVs",
    "keywords": [],
    "checked": true,
    "id": "5ec56c96c9c9d43e47c2ecee8c00b14ca6089d0e",
    "semantic_title": "out-of-vocabulary words detection with attention and ctc alignments in an end-to-end asr system",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wiesner21_interspeech.html": {
    "title": "Training Hybrid Models on Noisy Transliterated Transcripts for Code-Switched Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we describe the JHU-GoVivace submission for subtask 2 (code-switching task) of the Multilingual and Code-switching ASR challenges for low resource Indian languages. We built a hybrid HMM-DNN system with several improvements over the provided baseline in terms of lexical, language, and acoustic modeling. For lexical modeling, we investigate using unified pronunciations and phonesets derived from the baseline lexicon and publicly available Wikipron lexicons in Bengali and Hindi to expand the pronunciation lexicons. We explore several neural network architectures, along with supervised pretraining and multilingual training for acoustic modeling. We also describe how we used large externally crawled web text for language modeling. Since the challenge data contain artefacts such as misalignments, various data cleanup methods are explored, including acoustic-driven pronunciation learning to help discover Indian-accented pronunciations for English words as well as transcribed punctuation. As a result of these efforts, our best systems achieve transliterated WERs of 19.5% and 23.2% on the non-duplicated development sets for Hindi-English and Bengali-English, respectively",
    "keywords": [],
    "checked": true,
    "id": "dc6c49acca0d3d6f3fad0971d0962f0990c45a7d",
    "semantic_title": "training hybrid models on noisy transliterated transcripts for code-switched speech recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21c_interspeech.html": {
    "title": "Speech Intelligibility of Dysarthric Speech: Human Scores and Acoustic-Phonetic Features",
    "volume": "main",
    "abstract": "We investigated speech intelligibility in dysarthric and non-dysarthric speakers as measured by two commonly used metrics, ratings through the Visual Analogue Scale (VAS) and word accuracy (AcW) through orthographic transcriptions. To gain a better understanding of how acoustic-phonetic correlates could be employed to obtain more objective measures of speech intelligibility and a better classification of dysarthric and non-dysarthric speakers, we studied the relation between these measures of intelligibility and some important acoustic-phonetic correlates. We found that the two intelligibility measures are related, but distinct, and that they might refer to different components of the intelligibility construct. The acoustic-phonetic features showed no difference in the mean values between the two speaker types at the utterance level, but more than half of them played a role in classifying the two speaker types. We computed an acoustic-phonetic probability index (API) at the speaker level. API is moderately correlated to VAS ratings but not correlated to AcW. In addition, API and VAS complement each other in classifying dysarthric and non-dysarthric speakers. This suggests that the intelligibility measures assigned by human raters and acoustic-phonetic features relate to different constructs of intelligibility",
    "keywords": [],
    "checked": true,
    "id": "15411e44c952ec98b4d7b5b4ff29b5986b83f48e",
    "semantic_title": "speech intelligibility of dysarthric speech: human scores and acoustic-phonetic features",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21i_interspeech.html": {
    "title": "Analyzing Short Term Dynamic Speech Features for Understanding Behavioral Traits of Children with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "Computational methodologies have shown promise in advancing diagnostic and intervention research in the domain of Prior works have investigated speech features to assess disorder severity and also to differentiate between children with and without an ASD diagnosis. In this work, we explore short term dynamic functionals of speech features both within and across speakers to understand if local changes in speech provide information toward phenotyping of ASD.We compare the contributions of static and dynamic functionals representing conversational speech toward the clinical diagnosis state. Our results show that predictions obtained from a combination of dynamic and static functionals have comparable or superior performance to the predictions obtained from just static speech functionals. We also analyze the relationship between speech production and ASD diagnosis through correlation analyses between speech functionals and manually-derived behavioral codes related to autism severity. The experimental results support the notion that dynamic speech functionals capture complementary information which can facilitate enriched analysis of clinically-meaningful behavioral inference tasks",
    "keywords": [],
    "checked": true,
    "id": "ebfaf58770665a6e615ca78c6469fecb0d4d2267",
    "semantic_title": "analyzing short term dynamic speech features for understanding behavioral traits of children with autism spectrum disorder",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jesko21_interspeech.html": {
    "title": "Vocalization Recognition of People with Profound Intellectual and Multiple Disabilities (PIMD) Using Machine Learning Algorithms",
    "volume": "main",
    "abstract": "We investigate vocalization recognition for people with Profound Intellectual and Multiple Disabilities using various machine learning algorithms. The amount of training data available for people with PIMD is typically significantly limited. Due to this fact, data augmentation process was used. Various types of Machine Learning algorithms were tested: k-NN, NB, DT, RDF, MLP and LSTM. During research we also tested various regularization techniques to improve recognition performance. The best results were obtained in case of MLP network with dropout and batch normalization: 90%",
    "keywords": [],
    "checked": true,
    "id": "f1a8db1452532f74353977ed71faab407e7ed677",
    "semantic_title": "vocalization recognition of people with profound intellectual and multiple disabilities (pimd) using machine learning algorithms",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fivela21_interspeech.html": {
    "title": "Phonetic Complexity, Speech Accuracy and Intelligibility Assessment of Italian Dysarthric Speech",
    "volume": "main",
    "abstract": "Intelligibility is the degree to which the speech of a person may be understood by a listener, and is related to functional limitation and disability. In protocols for the clinical assessment of dysarthria, intelligibility checks are included, as well as evaluations of speech accuracy, which is more directly related to the disease severity. However, both evaluations are usually based on subjective ratings Aim of this work is checking the correlation between intelligibility judgements, subjectively assigned as it may be the case in clinical procedures, and acoustic measures related to linguistically contrasting units. Two novelties characterize this work: a) acoustic measurements considered in the paper relate to both segments (vowel and consonants) and prosodic-intonational phonological events (e.g., pitch accents), that is linguistically relevant speech units; b) contexts of increasing phonetic-phonological complexity are considered, in order for the phonetic characteristics to challenge production accuracy, possibly affecting the realization of phonological features and intelligibility. Increasing complexity is expected to challenge intelligibility indeed and to have an impact on the correlation between intelligibility rates and acoustic measures. Results are preliminary, but confirm both 1) the correlation between acoustic measures of linguistically relevant events and speech intelligibility, as for both the segmental and the prosodic-intonational level, and 2) the role of increasing phonetic-phonological complexity in enhancing the above mentioned correlation",
    "keywords": [],
    "checked": true,
    "id": "86148dd5be5d5e81aafb080a9ade9c8963bb5f38",
    "semantic_title": "phonetic complexity, speech accuracy and intelligibility assessment of italian dysarthric speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ng21_interspeech.html": {
    "title": "Detection of Consonant Errors in Disordered Speech Based on Consonant-Vowel Segment Embedding",
    "volume": "main",
    "abstract": "Speech sound disorder (SSD) refers to a type of developmental disorder in young children who encounter persistent difficulties in producing certain speech sounds at the expected age. Consonant errors are the major indicator of SSD in clinical assessment. Previous studies on automatic assessment of SSD revealed that detection of speech errors concerning short and transitory consonants is less satisfactory. This paper investigates a neural network based approach to detecting consonant errors in disordered speech using consonant-vowel (CV) diphone segment in comparison to using consonant monophone segment. The underlying assumption is that the vowel part of a CV segment carries important information of co-articulation from the consonant. Speech embeddings are extracted from CV segments by a recurrent neural network model. The similarity scores between the embeddings of the test segment and the reference segments are computed to determine if the test segment is the expected consonant or not. Experimental results show that using CV segments achieves improved performance on detecting speech errors concerning those \"difficult\" consonants reported in the previous studies",
    "keywords": [],
    "checked": true,
    "id": "a387e286a00c576ce43346051df7171baba189ad",
    "semantic_title": "detection of consonant errors in disordered speech based on consonant-vowel segment embedding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hair21_interspeech.html": {
    "title": "Assessing Posterior-Based Mispronunciation Detection on Field-Collected Recordings from Child Speech Therapy Sessions",
    "volume": "main",
    "abstract": "A critical component of child speech therapy is home practice with a caregiver, who can provide feedback. However, caregivers oftentimes struggle with accurately rating speech and with perceiving pronunciation errors. One potential solution for this issue is to embed automatic mispronunciation-detection (MPD) algorithms within digital speech therapy applications. To address the need for MPD within child speech therapy, we investigated posterior-based mispronunciation detection using a custom corpus of disordered speech from children that had been manually annotated by an expert clinician. Namely, we trained a family of phoneme-specific logistic regression classifiers (LRC) and support vector machines (SVM) on log posterior probability and log posterior ratio features. Our results show that these classifiers outperformed baseline Goodness of Pronunciation scoring by 11% and 10%, respectively. Even more importantly, in an offline test, the LRC and SVM classifiers outperformed student clinicians at identifying mispronunciations by 18% and 16%, respectively. These results suggest that posterior-based mispronunciation detection may be suitable to provide at-home therapy feedback for children",
    "keywords": [],
    "checked": true,
    "id": "9859d5943bbfba47ed919e3e42240ce36888e3c7",
    "semantic_title": "assessing posterior-based mispronunciation detection on field-collected recordings from child speech therapy sessions",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mirheidari21_interspeech.html": {
    "title": "Identifying Cognitive Impairment Using Sentence Representation Vectors",
    "volume": "main",
    "abstract": "The widely used word vectors can be extended at the sentence level to perform a wide range of natural language processing (NLP) tasks. Recently the Bidirectional Encoder Representations from Transformers (BERT) language representation achieved state-of-the-art performance for these applications. The model is trained with punctuated and well-formed (writ-ten) text, however, the performance of the model drops significantly when the input text is the — erroneous and unpunctuated — output of automatic speech recognition (ASR). We use a sliding window and averaging approach for pre-processing text for BERT to extract features for classifying three diagnostic categories relating to cognitive impairment: neurodegenerative dis-order (ND), mild cognitive impairment (MCI), and healthy controls (HC). The in-house dataset contains the audio recordings of an intelligent virtual agent (IVA) who asks the participants several conversational questions prompts in addition to giving a picture description prompt. For the three-way classification, we achieve a 73.88% F-score (accuracy: 76.53%) using the pre-trained, uncased base BERT and for the two-way classifier (HC vs. ND) we achieve 89.80% (accuracy: 90%). We further improve these by using a prompt selection technique, reaching the F-scores of 79.98% (accuracy: 81.63%) and 93.56% (accuracy: 93.75%) respectively",
    "keywords": [],
    "checked": true,
    "id": "b38ad35ca13e7b34d0ef5e211ab3db55f61e6726",
    "semantic_title": "identifying cognitive impairment using sentence representation vectors",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yue21b_interspeech.html": {
    "title": "Parental Spoken Scaffolding and Narrative Skills in Crowd-Sourced Storytelling Samples of Young Children",
    "volume": "main",
    "abstract": "A novel crowdsourcing project to gather children's storytelling based language samples using a mobile app was undertaken across the United Kingdom. Parents' scaffolding of children's narratives was observed in many of the samples. This study was designed to examine the relationship of scaffolding and young children's narrative language ability in a story retell context which is analysed at the macro-structural (total macro-structure score), the micro-structural (mean length of utterances in morphemes) and verbal productivity (total number of utterances) levels. Young children with and without scaffolding were statistically compared. The interaction between the level of scaffolding support, the grammar complexity and the narrative structure was explored. A bidirectional relationship was observed between scaffolding and young children's narrative language ability. Young children with better performance were observed to receive less scaffolding from parents. Scaffolding was shown to support early narrative development of young children and was more able to benefit those with low-level grammatical complexity skills. It is crucial to encourage parental scaffolding to be well-attuned to the child's narrative ability",
    "keywords": [],
    "checked": true,
    "id": "12be99e588a327a024f49da8eb4877a5b4f23032",
    "semantic_title": "parental spoken scaffolding and narrative skills in crowd-sourced storytelling samples of young children",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xia21_interspeech.html": {
    "title": "Uncertainty-Aware COVID-19 Detection from Imbalanced Sound Data",
    "volume": "main",
    "abstract": "Recently, sound-based COVID-19 detection studies have shown great promise to achieve scalable and prompt digital pre-screening. However, there are still two unsolved issues hindering the practice. First, collected datasets for model training are often imbalanced, with a considerably smaller proportion of users tested positive, making it harder to learn representative and robust features. Second, deep learning models are generally overconfident in their predictions. Clinically, false predictions aggravate healthcare costs. Estimation of the uncertainty of screening would aid this. To handle these issues, we propose an ensemble framework where multiple deep learning models for sound-based COVID-19 detection are developed from different but balanced subsets from original data. As such, data are utilized more effectively compared to traditional up-sampling and down-sampling approaches: an AUC of 0.74 with a sensitivity of 0.68 and a specificity of 0.69 is achieved. Simultaneously, we estimate uncertainty from the disagreement across multiple models. It is shown that false predictions often yield higher uncertainty, enabling us to suggest the users with certainty higher than a threshold to repeat the audio test on their phones or to take clinical tests if digital diagnosis still fails. This study paves the way for a more robust sound-based COVID-19 automated screening system",
    "keywords": [],
    "checked": true,
    "id": "26aa8f6fba7f03b35ac7f8a7d6460016e9f33e69",
    "semantic_title": "uncertainty-aware covid-19 detection from imbalanced sound data",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21u_interspeech.html": {
    "title": "Unsupervised Domain Adaptation for Dysarthric Speech Detection via Domain Adversarial Training and Mutual Information Minimization",
    "volume": "main",
    "abstract": "Dysarthric speech detection (DSD) systems aim to detect characteristics of the neuromotor disorder from speech. Such systems are particularly susceptible to domain mismatch where the training and testing data come from the source and target domains respectively, but the two domains may differ in terms of speech stimuli, disease etiology, etc. It is hard to acquire labelled data in the target domain, due to high costs of annotating sizeable datasets. This paper makes a first attempt to formulate cross-domain DSD as an unsupervised domain adaptation (UDA) problem. We use labelled source-domain data and unlabelled target-domain data, and propose a multi-task learning strategy, including dysarthria presence classification (DPC), domain adversarial training (DAT) and mutual information minimization (MIM), which aim to learn dysarthria-discriminative and domain-invariant biomarker embeddings. Specifically, DPC helps biomarker embeddings capture critical indicators of dysarthria; DAT forces biomarker embeddings to be indistinguishable in source and target domains; and MIM further reduces the correlation between biomarker embeddings and domain-related cues. By treating the UASPEECH and TORGO corpora respectively as the source and target domains, experiments show that the incorporation of UDA attains absolute increases of 22.2% and 20.0% respectively in utterance-level weighted average recall and speaker-level accuracy",
    "keywords": [],
    "checked": true,
    "id": "9bea138d5a8e76177f3080d655ecd76045f6e85b",
    "semantic_title": "unsupervised domain adaptation for dysarthric speech detection via domain adversarial training and mutual information minimization",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bhattacharjee21_interspeech.html": {
    "title": "Source and Vocal Tract Cues for Speech-Based Classification of Patients with Parkinson's Disease and Healthy Subjects",
    "volume": "main",
    "abstract": "Parkinson's disease (PD) affects both source and vocal tract components of speech. Various speech cues explored in literature for automatic classification of individuals with PD and healthy controls (HC) implicitly carry information about both these components. This work explicitly analyzes the contribution of source and vocal tract attributes toward automatic PD vs. HC classification, which has not been done earlier to the best of our knowledge. Here fundamental frequency (f ) is used to capture source information. For quantifying vocal tract information, speech waveforms are converted to unvoiced forms and mel-frequency cepstral coefficients (MFCC), denoted by voicing-removed MFCC, are obtained from them. Experimental results suggest that (1) the relative merit of source and vocal tract cues in classifying PD vs. HC largely depends on the speech task being considered, (2) both cues complement each other across all tasks, (3) while MFCC encodes both source and vocal tract features, source information captured by f is different and further complements MFCC when the classifiers are trained and tested under clean or matched noise conditions, thereby enabling the feature-level fusion of f and MFCC to achieve the best classification accuracy, (4) under unseen noise conditions, f alone proves to be a highly noise-robust feature",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8a8953733fd985dd52ba95b93663396de5e30432",
    "semantic_title": "source and vocal tract cues for speech-based classification of patients with parkinson's disease and healthy subjects",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/haulcy21_interspeech.html": {
    "title": "CLAC: A Speech Corpus of Healthy English Speakers",
    "volume": "main",
    "abstract": "This paper introduces the Crowdsourced Language Assessment Corpus (CLAC), a speech corpus consisting of audio recordings and automatically-generated transcripts for several speech and language tasks, as well as metadata for each of the speakers. The CLAC was created to provide the community with a collection of audio samples from various speakers that could be used to learn a general representation for speech from healthy subjects, as well as complement other health-related speech datasets, which tend to be limited. In this paper, we describe the data collection protocol and summarize the contents of the dataset. We also extract timing metrics from the recordings of each task to explore what those metrics look like for a large, English-speaking population. Lastly, we provide an example of how the dataset can be used by comparing the metrics to those extracted from a small sample of Frontotemporal Dementia subjects. We hope that this dataset will help advance the state of the art in the health and speech domain",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "550acf49cab2d7d4cd3a24c2263c0523f49f8fdc",
    "semantic_title": "clac: a speech corpus of healthy english speakers",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nortje21_interspeech.html": {
    "title": "Direct Multimodal Few-Shot Learning of Speech and Images",
    "volume": "main",
    "abstract": "We propose direct multimodal few-shot models that learn a shared embedding space of spoken words and images from only a few paired examples. Imagine an agent is shown an image along with a spoken word describing the object in the picture, e.g and After observing a few paired examples of each class, the model is asked to identify the \"book\" in a set of unseen pictures. Previous work used a two-step indirect approach relying on speech-speech and image-image comparisons across the support set of given speech-image pairs. Instead, we propose two direct models which learn a single multimodal space where inputs from different modalities are directly comparable: a multimodal triplet network (MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these direct models, we speech-image pairs by using the support set to pair up unlabelled in-domain speech and images. In a speech-to-image digit matching task, direct models outperform indirect models, with the MTriplet achieving the best multimodal five-shot accuracy. We show that the improvements are due to the combination of unsupervised and transfer learning in the direct models, and the absence of two-step compounding errors",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5a19b3e8dcdef157f002bca2839268c4933b7b7b",
    "semantic_title": "direct multimodal few-shot learning of speech and images",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sanabria21_interspeech.html": {
    "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval",
    "volume": "main",
    "abstract": "Speech-based image retrieval has been studied as a proxy for joint representation learning, usually without emphasis on retrieval itself. As such, it is unclear how well speech-based retrieval can work in practice — both in an absolute sense and versus alternative strategies that combine automatic speech recognition (ASR) with strong text encoders. In this work, we extensively study and expand choices of encoder architectures, training methodology (including unimodal and multimodal pretraining), and other factors. Our experiments cover different types of speech in three datasets: Flickr Audio, Places Audio, and Localized Narratives. Our best model configuration achieves large gains over state of the art, e.g., pushing recall-at-one from 21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also show our best speech-based models can match or exceed cascaded ASR-to-text encoding when speech is spontaneous, accented, or otherwise hard to automatically transcribe",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7fda2c0a086541b65038c39cb3aca7d2e40c0fa5",
    "semantic_title": "talk, don't write: a study of direct speech-based image retrieval",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhao21b_interspeech.html": {
    "title": "A Fast Discrete Two-Step Learning Hashing for Scalable Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "Recently, some cross-modal hashing methods are proposed to search data for different modality effectively. Hashing has received wide attention because of its low storage and high efficiency. Hashing-based methods project the data instances from different modalities into a Hamming space to learn hash codes for retrieval between different modality. Although obtaining promising performance, hashing-based methods have still several common limitations. First, they learn the hash codes by constructing semantic similarity matrices, resulting in the loss of information. Second, most existing methods simultaneously learn the hash codes and the hash functions, which bring a high computational complexity. Third, they utilize the relaxation-based optimization strategy to generate the hash codes which leads to the large quantization error of the hash codes. To solve the above problems, we propose a novel fast supervised hashing method, termed Fast Discrete Two-Step Learning Hashing (FDTLH) for scalable cross-modal retrieval, which learns the discriminative hash codes by adopting a effective two-step learning scheme. Extensive experiments show that the FDTLH outperforms several state-of-the-art hashing methods in terms of retrieval performance and learning efficiency",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ada66d46ede5bedba6a55576127efe6641e73dde",
    "semantic_title": "a fast discrete two-step learning hashing for scalable cross-modal retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21v_interspeech.html": {
    "title": "Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition",
    "volume": "main",
    "abstract": "Cued Speech (CS) is a visual communication system for the deaf or hearing impaired people. It combines lip movements with hand cues to obtain a complete phonetic repertoire. Current deep learning based methods on automatic CS recognition suffer from a common problem, which is the data scarcity. Until now, there are only two public single speaker datasets for French (238 sentences) and British English (97 sentences). In this work, we propose a cross-modal knowledge distillation method with teacher-student structure, which transfers audio speech information to CS to overcome the limited data problem. Firstly, we pretrain a teacher model for CS recognition with a large amount of open source audio speech data, and simultaneously pretrain the feature extractors for lips and hands using CS data. Then, we distill the knowledge from teacher model to the student model with frame-level and sequence-level distillation strategies. Importantly, for frame-level, we exploit multi-task learning to weigh losses automatically, to obtain the balance coefficient. Besides, we establish a five-speaker British English CS dataset for the first time. The proposed method is evaluated on French and British English CS datasets, showing superior CS recognition performance to the state-of-the-art (SOTA) by a large margin",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3f6431db18461fc2d26496d3e9c388951c0c1168",
    "semantic_title": "cross-modal knowledge distillation method for automatic cued speech recognition",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/olaleye21_interspeech.html": {
    "title": "Attention-Based Keyword Localisation in Speech Using Visual Grounding",
    "volume": "main",
    "abstract": "Visually grounded speech models learn from images paired with spoken captions. By tagging images with soft text labels using a trained visual classifier with a fixed vocabulary, previous work has shown that it is possible to train a model that can whether a particular text keyword occurs in speech utterances or not. Here we investigate whether visually grounded speech models can also do keyword : predicting where, within an utterance, a given textual keyword occurs without any explicit text-based or alignment supervision. We specifically consider whether incorporating attention into a convolutional model is beneficial for localisation. Although absolute localisation performance with visually supervised models is still modest (compared to using unordered bag-of-word text labels for supervision), we show that attention provides a large gain in performance over previous visually grounded models. As in many other speech-image studies, we find that many of the incorrect localisations are due to semantic confusions, e.g. locating the word ‘backstroke' for the query keyword ‘swimming'",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "940f94d196bc45cdde6ca06dc20e718edb18e3b2",
    "semantic_title": "attention-based keyword localisation in speech using visual grounding",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/khorrami21_interspeech.html": {
    "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models",
    "volume": "main",
    "abstract": "Systems that can find correspondences between multiple modalities, such as between speech and images, have great potential to solve different recognition and data analysis tasks in an unsupervised manner. This work studies multimodal learning in the context of visually grounded speech (VGS) models, and focuses on their recently demonstrated capability to extract spatiotemporal alignments between spoken words and the corresponding visual objects without ever been explicitly trained for object localization or word recognition. As the main contributions, we formalize the alignment problem in terms of an audio-visual alignment tensor that is based on earlier VGS work, introduce systematic metrics for evaluating model performance in aligning visual objects and spoken words, and propose a new VGS model variant for the alignment task utilizing cross-modal attention layer. We test our model and a previously proposed model in the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We compare the alignment performance using our proposed evaluation metrics to the semantic retrieval task commonly used to evaluate VGS models. We show that cross-modal attention layer not only helps the model to achieve higher semantic cross-modal retrieval performance, but also leads to substantial improvements in the alignment performance between image object and spoken words",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b787828ec1e4ff213cf9befb05f8d4ab4723f9bf",
    "semantic_title": "evaluation of audio-visual alignments in visually grounded speech models",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21k_interspeech.html": {
    "title": "Automatic Lip-Reading with Hierarchical Pyramidal Convolution and Self-Attention for Image Sequences with No Word Boundaries",
    "volume": "main",
    "abstract": "In this paper, we propose a novel deep learning architecture for improving word-level lip-reading. We first incorporate multi-scale processing into spatial feature extraction for lip-reading using hierarchical pyramidal convolution (HPConv) and self-attention. Specifically, HPConv is proposed to replace the conventional convolution features, leading to an improvement over the model's ability to discover fine-grained lip movements. Next to deal with fixed-length image sequences representing words in a given database, a self-attention mechanism is proposed to integrate local information in all lip frames without assuming known word boundaries, so that our deep models automatically utilize key feature in relevant frames of a given word. Experiments on the Lip Reading in the Wild corpus show that our proposed architecture achieves an accuracy of 86.83%, yielding a relative error rate reduction of about 10% from that obtained with a state-of-the-art scheme of averaging frame scores for information fusion. A detailed analysis of the experimental results also confirms that weights learned from self-attention tend to be zero at both sides of an image sequence and focus non-zero weights in the middle part of a given word",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "434c457e544ada4d559ac78da89cb7b6b11796b4",
    "semantic_title": "automatic lip-reading with hierarchical pyramidal convolution and self-attention for image sequences with no word boundaries",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rouditchenko21b_interspeech.html": {
    "title": "Cascaded Multilingual Audio-Visual Learning from Videos",
    "volume": "main",
    "abstract": "In this paper, we explore self-supervised audio-visual models that learn from instructional videos. Prior work has shown that these models can relate spoken words and sounds to visual content after training on a large-scale dataset of videos, but they were only trained and evaluated on videos in English. To learn multilingual audio-visual representations, we propose a cascaded approach that leverages a model trained on English videos and applies it to audio-visual data in other languages, such as Japanese videos. With our cascaded approach, we show an improvement in retrieval performance of nearly 10× compared to training on the Japanese videos solely. We also apply the model trained on English videos to Japanese and Hindi spoken captions of images, achieving state-of-the-art performance",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7739764dbc65d13ca8c5e8825c7cf4f98f309175",
    "semantic_title": "cascaded multilingual audio-visual learning from videos",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21c_interspeech.html": {
    "title": "LiRA: Learning Visual Speech Representations from Audio Through Self-Supervision",
    "volume": "main",
    "abstract": "The large amount of audiovisual content being shared online today has drawn substantial attention to the prospect of audio-visual self-supervised learning. Recent works have focused on each of these modalities separately, while others have attempted to model both simultaneously in a cross-modal fashion. However, comparatively little attention has been given to leveraging one modality as a training objective to learn from the other. In this work, we propose Learning visual speech Representations from Audio via self-supervision (LiRA). Specifically, we train a ResNet+Conformer model to predict acoustic features from unlabelled visual speech. We find that this pre-trained model can be leveraged towards word-level and sentence-level lip-reading through feature extraction and fine-tuning experiments. We show that our approach significantly outperforms other self-supervised methods on the Lip Reading in the Wild (LRW) dataset and achieves state-of-the-art performance on Lip Reading Sentences 2 (LRS2) using only a fraction of the total labelled data",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "dcdea4685097d288de424c33065fabf1f67eacde",
    "semantic_title": "lira: learning visual speech representations from audio through self-supervision",
    "citation_count": 27,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rose21_interspeech.html": {
    "title": "End-to-End Audio-Visual Speech Recognition for Overlapping Speech",
    "volume": "main",
    "abstract": "This paper investigates an end-to-end audio-visual (A/V) modeling approach for transcribing utterances in scenarios where there are overlapping speech utterances from multiple talkers. It assumes that overlapping audio signals and video signals in the form of mouth-tracks aligned with speech are available for overlapping talkers. The approach builds on previous work in audio-only multi-talker ASR. In that work, a conventional recurrent neural network transducer (RNN-T) architecture was extended to include a masking model for separation of encoded audio features and multiple label encoders to encode transcripts from overlapping speakers. It is shown here that incorporating an attention weighted combination of visual features in A/V multi-talker RNN-T models significantly improves speaker disambiguation in ASR on overlapping speech relative to audio-only performance. The A/V multi-talker ASR systems described here are trained and evaluated on a two speaker A/V overlapping speech dataset created from YouTube videos. A 17% reduction in WER was observed for A/V multi-talker models relative to audio-only multi-talker models",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "1116d176945af311439fd263ff3e7903bed3baf3",
    "semantic_title": "end-to-end audio-visual speech recognition for overlapping speech",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21e_interspeech.html": {
    "title": "Audio-Visual Multi-Talker Speech Recognition in a Cocktail Party",
    "volume": "main",
    "abstract": "Speech from microphones is vulnerable in a complex acoustic environment due to noise and reverberation, while the cameras are not. Thus, utilizing the visual modality in the \"cocktail party\" scenario with multi-talkers has become a promising and popular approach. In this paper, we have explored the incorporating of visual modality into the end-to-end multi-talker speech recognition task. We propose two methods based on the modality fusion position, which are encoder-based fusion and decoder-based fusion. And for each method, advanced audio-visual fusion techniques including attention mechanism and dual decoder have been explored to find the best usage of the visual modality. With the proposed methods, our best audio-visual multi-talker automatic speech recognition (ASR) model gets almost ~50.0% word error rate (WER) reduction compared to the audio-only multi-talker ASR system",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "1eeaeb261ea6b65e8500033feacb660e5da98f6d",
    "semantic_title": "audio-visual multi-talker speech recognition in a cocktail party",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21l_interspeech.html": {
    "title": "Ultra Fast Speech Separation Model with Teacher Student Learning",
    "volume": "main",
    "abstract": "Transformer has been successfully applied to speech separation recently with its strong long-dependency modeling capacity using a self-attention mechanism. However, Transformer tends to have heavy run-time costs due to the deep encoder layers, which hinders its deployment on edge devices. A small Transformer model with fewer encoder layers is preferred for computational efficiency, but it is prone to performance degradation. In this paper, an ultra fast speech separation Transformer model is proposed to achieve both better performance and efficiency with teacher student learning (T-S learning). We introduce layer-wise T-S learning and objective shifting mechanisms to guide the small student model to learn intermediate representations from the large teacher model. Compared with the small Transformer model trained from scratch, the proposed T-S learning method reduces the word error rate (WER) by more than 5% for both multi-channel and single-channel speech separation on LibriCSS dataset. Utilizing more unlabeled speech data, our ultra fast speech separation models achieve more than 10% relative WER reduction",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "053ac76f7b6e12f406eaf6e953b3bd5313fd69c2",
    "semantic_title": "ultra fast speech separation model with teacher student learning",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ali21_interspeech.html": {
    "title": "Group Delay Based Re-Weighted Sparse Recovery Algorithms for Robust and High-Resolution Source Separation in DOA Framework",
    "volume": "main",
    "abstract": "Sparse Recovery (SR) algorithms have been used widely for direction-of-arrival (DOA) estimation in spatially contiguous plane wave for their robust performance. But these algorithms have proven to be computationally costly. With a few sensors and at low SNRs, the noise dominates the data singular vectors and the sparse estimation of contiguous sources is incorrect. The magnitude spectrum-based re-weighted sparse recovery (RWSR) algorithms improve the robustness by re-weighting the sparse estimates. However, their efficiency degrades with decreasing the number of sensors at low SNRs. Therefore, this paper exhibits the significance of the phase spectrum, in the form of group-delay, for sparse and robust source estimation using RWSR algorithms for spatially contiguous sources. Further, an optimal re-weighted methodology based on simultaneously minimizing average-root-mean-square-error and maximizing the probability of separation is also proposed. The simulation results are carried out for Gaussian noise to demonstrate the excellent performance of the proposed algorithms",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c64d3bc62ab685f3bd006bbb4ec8f6c3beb82171",
    "semantic_title": "group delay based re-weighted sparse recovery algorithms for robust and high-resolution source separation in doa framework",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21d_interspeech.html": {
    "title": "Continuous Speech Separation Using Speaker Inventory for Long Recording",
    "volume": "main",
    "abstract": "Leveraging additional speaker information to facilitate speech separation has received increasing attention in recent years. Recent research includes extracting target speech by using the target speaker's voice snippet and jointly separating all participating speakers by using a pool of additional speaker signals, which is known as speech separation using speaker inventory (SSUSI). However, all these systems ideally assume that the pre-enrolled speaker signals are available and are only evaluated on simple data configurations. In realistic multi-talker conversations, the speech signal contains a large proportion of non-overlapped regions, where we can derive robust speaker embedding of individual talkers. In this work, we adopt the SSUSI model in long recordings and propose a self-informed, clustering-based inventory forming scheme for long recording, where the speaker inventory is fully built from the input signal without the need for external speaker signals. Experiment results on simulated noisy reverberant long recording datasets show that the proposed method can significantly improve the separation performance across various conditions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "576860f910ea8fde366deb03c910ab30cd776966",
    "semantic_title": "continuous speech separation using speaker inventory for long recording",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yuan21_interspeech.html": {
    "title": "Crossfire Conditional Generative Adversarial Networks for Singing Voice Extraction",
    "volume": "main",
    "abstract": "Generative adversarial networks (GANs) and Conditional GANs (cGANs) have recently been applied for singing voice extraction (SVE), since they can accurately model the vocal distributions and effectively utilize a large amount of unlabelled datasets. However, current GANs/cGANs based SVE frameworks have no explicit mechanism to eliminate the mutual interferences between different sources. In this work, we introduce a novel ‘crossfire' criterion into GANs to complement its standard adversarial training, which forms a dual-objective GANs, namely Crossfire GANs (Cr-GANs). In addition, we design a Generalized Projection Method (GPM) for cGANs based frameworks to extract more effective conditional information for SVE. Using the proposed GPM, we extend our Cr-GANs to conditional version, i.e., Crossfire Conditional GANs (Cr-cGANs). The proposed methods were evaluated on the DSD100 and CCMixter datasets. The numerical results have shown that the ‘crossfire' criterion and GPM are beneficial to each other and considerably improve the separation performance of existing GANs/cGANs based SVE methods",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "9710bed2be4ee356666d16b46e8a0f567c0870c6",
    "semantic_title": "crossfire conditional generative adversarial networks for singing voice extraction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21w_interspeech.html": {
    "title": "End-to-End Speech Separation Using Orthogonal Representation in Complex and Real Time-Frequency Domain",
    "volume": "main",
    "abstract": "Traditional single channel speech separation in the time-frequency (T-F) domain often faces the problem of phase reconstruction. Due to the fact that the real-valued network is not suitable for dealing with complex-valued representation, the performance of the T-F domain speech separation method is often constrained from reaching the state-of-the-art. In this paper, we propose improved speech separation methods in both complex and real T-F domain using orthogonal representation. For the complex-valued case, we combine the deep complex network (DCN) and Conv-TasNet to design an end-to-end complex-valued model. Specifically, we incorporate short-time Fourier transform (STFT) and learnable complex layers to build a hybrid encoder-decoder structure, and use a DCN based separator. Then we present the importance of weights orthogonality in the T-F domain transformation and propose a multi-segment orthogonality (MSO) architecture for further improvements. For the real-valued case, we performed separation in real T-F domain by introducing the short-time DCT (STDCT) with orthogonal representation as well. Experimental results show that the proposed complex model outperforms the baseline Conv-TasNet with a comparable parameter size by 1.8 dB, and the STDCT-based real-valued T-F model by 1.2 dB, showing the advantages of speech separation in the T-F domain",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4bac8cafbdccc10ecbffd0579b2606a93ce9b591",
    "semantic_title": "end-to-end speech separation using orthogonal representation in complex and real time-frequency domain",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nakagome21_interspeech.html": {
    "title": "Efficient and Stable Adversarial Learning Using Unpaired Data for Unsupervised Multichannel Speech Separation",
    "volume": "main",
    "abstract": "This study presents a framework to enable efficient and stable adversarial learning of unsupervised multichannel source separation models. When the paired data, i.e., the mixture and the corresponding clean speech, are not available for training, it is promising to exploit generative adversarial networks (GANs), where a source separation system is treated as a generator and trained to bring the distribution of the separated (fake) speech closer to that of the clean (real) speech. The separated speech, however, contains many errors, especially when the system is trained unsupervised and can be easily distinguished from the clean speech. A real/fake binary discriminator therefore will stop the adversarial learning process unreasonably early. This study aims to balance the convergence of the generator and discriminator to achieve efficient and stable learning. For that purpose, the autoencoder-based discriminator and more stable adversarial loss, which are designed in boundary equilibrium GAN (BEGAN), are introduced. In addition, generator-specific distortions are added to real examples so that the models can be trained to focus only on source separation. Experimental comparisons demonstrated that the present stabilizing learning techniques improved the performance of multiple unsupervised source separation systems",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d8608214e41f4ba5b10f6095eb02feb68eb99416",
    "semantic_title": "efficient and stable adversarial learning using unpaired data for unsupervised multichannel speech separation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21h_interspeech.html": {
    "title": "Stabilizing Label Assignment for Speech Separation by Self-Supervised Pre-Training",
    "volume": "main",
    "abstract": "Speech separation has been well developed, with the very successful permutation invariant training (PIT) approach, although the frequent label assignment switching happening during PIT training remains to be a problem when better convergence speed and achievable performance are desired. In this paper, we propose to perform self-supervised pre-training to stabilize the label assignment in training the speech separation model. Experiments over several types of self-supervised approaches, several typical speech separation models and two different datasets showed that very good improvements are achievable if a proper self-supervised approach is chosen",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "eb15014b076287456f858932200e83584bfd26c8",
    "semantic_title": "stabilizing label assignment for speech separation by self-supervised pre-training",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21x_interspeech.html": {
    "title": "Dual-Path Filter Network: Speaker-Aware Modeling for Speech Separation",
    "volume": "main",
    "abstract": "Speech separation has been extensively studied to deal with the cocktail party problem in recent years. All related approaches can be divided into two categories: time-frequency domain methods and time domain methods. In addition, some methods try to generate speaker vectors to support source separation. In this study, we propose a new model called dual-path filter network (DPFN). Our model focuses on the post-processing of speech separation to improve speech separation performance. DPFN is composed of two parts: the speaker module and the separation module. First, the speaker module infers the identities of the speakers. Then, the separation module uses the speakers' information to extract the voices of individual speakers from the mixture. DPFN constructed based on DPRNN-TasNet is not only superior to DPRNN-TasNet, but also avoids the problem of permutation-invariant training (PIT)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "a59f0199793e584c6931b1ea0f5cf17891679e2c",
    "semantic_title": "dual-path filter network: speaker-aware modeling for speech separation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21f_interspeech.html": {
    "title": "Investigation of Practical Aspects of Single Channel Speech Separation for ASR",
    "volume": "main",
    "abstract": "Speech separation has been successfully applied as a front-end processing module of conversation transcription systems thanks to its ability to handle overlapped speech and its flexibility to combine with downstream tasks such as automatic speech recognition (ASR). However, a speech separation model often introduces target speech distortion, resulting in a sub-optimum word error rate (WER). In this paper, we describe our efforts to improve the performance of a single channel speech separation system. Specifically, we investigate a two-stage training scheme that firstly applies a feature level optimization criterion for pre-training, followed by an ASR-oriented optimization criterion using an end-to-end (E2E) speech recognition model. Meanwhile, to keep the model light-weight, we introduce a modified teacher-student learning technique for model compression. By combining those approaches, we achieve a absolute average WER improvement of 2.70% and 0.77% using models with less than 10M parameters compared with the previous state-of-the-art results on the LibriCSS dataset for utterance-wise evaluation and continuous evaluation, respectively",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8c7aa01a5d57082474ed9188376eeea82608a3c0",
    "semantic_title": "investigation of practical aspects of single channel speech separation for asr",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21c_interspeech.html": {
    "title": "Implicit Filter-and-Sum Network for End-to-End Multi-Channel Speech Separation",
    "volume": "main",
    "abstract": "Various neural network architectures have been proposed in recent years for the task of multi-channel speech separation. Among them, the filter-and-sum network (FaSNet) performs end-to-end time-domain filter-and-sum beamforming and has shown effective in both ad-hoc and fixed microphone array geometries. However, whether such explicit beamforming operation is a necessary and valid formulation remains unclear. In this paper, we investigate the beamforming operation and show that it is not necessary. To further improve the performance, we change the explicit waveform-level filter-and-sum operation into an implicit feature-level filter-and-sum operation around a context of features. A feature-level normalized cross correlation (fNCC) feature is also proposed to better match the implicit operation for an improved performance. Experiment results on a simulated ad-hoc microphone array dataset show that the proposed modification to the FaSNet, which we refer to as the implicit filter-and-sum network (iFaSNet), achieve better performance than the explicit FaSNet with a similar model size and a faster training and inference speed",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5cb1f4f07e8205050348baee8e3c15392a1322d6",
    "semantic_title": "implicit filter-and-sum network for end-to-end multi-channel speech separation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21i_interspeech.html": {
    "title": "Generalized Spatio-Temporal RNN Beamformer for Target Speech Separation",
    "volume": "main",
    "abstract": "Although the conventional mask-based minimum variance distortionless response (MVDR) could reduce the non-linear distortion, the residual noise level of the MVDR separated speech is still high. In this paper, we propose a spatio-temporal recurrent neural network based beamformer (RNN-BF) for target speech separation. This new beamforming framework directly learns the beamforming weights from the estimated speech and noise spatial covariance matrices. Leveraging on the temporal modeling capability of RNNs, the RNN-BF could automatically accumulate the statistics of the speech and noise covariance matrices to learn the frame-level beamforming weights in a recursive way. An RNN-based generalized eigenvalue (RNN-GEV) beamformer and a more generalized RNN beamformer (GRNN-BF) are proposed. We further improve the RNN-GEV and the GRNN-BF by using layer normalization to replace the commonly used mask normalization on the covariance matrices. The proposed GRNN-BF obtains better performance against prior arts in terms of speech quality (PESQ), speech-to-noise ratio (SNR) and word error rate (WER)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "61c3af8e27273e621575ec48584e04ec15afb43e",
    "semantic_title": "generalized spatio-temporal rnn beamformer for target speech separation",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21j_interspeech.html": {
    "title": "End-to-End Neural Diarization: From Transformer to Conformer",
    "volume": "main",
    "abstract": "We propose a new end-to-end neural diarization (EEND) system that is based on Conformer, a recently proposed neural architecture that combines convolutional mappings and Transformer to model both local and global dependencies in speech. We first show that data augmentation and convolutional subsampling layers enhance the original self-attentive EEND in the Transformer-based EEND, and then Conformer gives an additional gain over the Transformer-based EEND. However, we notice that the Conformer-based EEND does not generalize as well from simulated to real conversation data as the Transformer-based model. This leads us to quantify the mismatch between simulated data and real speaker behavior in terms of temporal statistics reflecting turn-taking between speakers, and investigate its correlation with diarization error. By mixing simulated and real data in EEND training, we mitigate the mismatch further, with Conformer-based EEND achieving 24% error reduction over the baseline SA-EEND system, and 10% improvement over the best augmented Transformer-based system, on two-speaker CALLHOME data",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "41ebc5c059deaf223ffaf38fbee1dc6297e54ed9",
    "semantic_title": "end-to-end neural diarization: from transformer to conformer",
    "citation_count": 25,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jung21_interspeech.html": {
    "title": "Three-Class Overlapped Speech Detection Using a Convolutional Recurrent Neural Network",
    "volume": "main",
    "abstract": "In this work, we propose an overlapped speech detection system trained as a three-class classifier. Unlike conventional systems that perform binary classification as to whether or not a frame contains overlapped speech, the proposed approach classifies into three classes: non-speech, single speaker speech, and overlapped speech. By training a network with the more detailed label definition, the model can learn a better notion on deciding the number of speakers included in a given frame. A convolutional recurrent neural network architecture is explored to benefit from both convolutional layer's capability to model local patterns and recurrent layer's ability to model sequential information. The proposed overlapped speech detection model establishes a state-of-the-art performance with a precision of 0.6648 and a recall of 0.3222 on the DIHARD II evaluation set, showing a 20% increase in recall along with higher precision. In addition, we also introduce a simple approach to utilize the proposed overlapped speech detection model for speaker diarization which ranked third place in the Track 1 of the DIHARD III challenge",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "154b0a91f31777c6253b865088b93c1fe84c75ed",
    "semantic_title": "three-class overlapped speech detection using a convolutional recurrent neural network",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wan21_interspeech.html": {
    "title": "Online Speaker Diarization Equipped with Discriminative Modeling and Guided Inference",
    "volume": "main",
    "abstract": "Despite considerable efforts, online speaker diarization remains an ongoing challenge. In this study, we propose to tackle the challenge from two perspectives, to endow diarization model with discriminability and to rectify less-reliable online inference with guidance. Specifically, based on the current prior art, UIS-RNN, two enhancement approaches are proposed to concretize our motivations. The effectiveness of our proposals is experimentally validated by results on the AMI evaluation set. With substantial relative improvement of 48.7%, our online speaker diarization system significantly outperformed its baseline. More impressively, its performance in terms of diarization error rate is better than most state-of-the-art offline systems",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5516e7921f54a88e18cc68b742ea05a31c2c49f0",
    "semantic_title": "online speaker diarization equipped with discriminative modeling and guided inference",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/takashima21_interspeech.html": {
    "title": "Semi-Supervised Training with Pseudo-Labeling for End-To-End Neural Diarization",
    "volume": "main",
    "abstract": "In this paper, we present a semi-supervised training technique using pseudo-labeling for end-to-end neural diarization (EEND). The EEND system has shown promising performance compared with traditional clustering-based methods, especially in the case of overlapping speech. However, to get a well-tuned model, EEND requires labeled data for all the joint speech activities of every speaker at each time frame in a recording. In this paper, we explore a pseudo-labeling approach that employs unlabeled data. First, we propose an iterative pseudo-label method for EEND, which trains the model using unlabeled data of a target condition. Then, we also propose a committee-based training method to improve the performance of EEND. To evaluate our proposed method, we conduct the experiments of model adaptation using labeled and unlabeled data. Experimental results on the CALLHOME dataset show that our proposed pseudo-label achieved a 37.4% relative diarization error rate reduction compared to a seed model. Moreover, we analyzed the results of semi-supervised adaptation with pseudo-labeling. We also show the effectiveness of our approach on the third DIHARD dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d85c0032d7bb0bd220eb2df8ba6d2130bc87e79e",
    "semantic_title": "semi-supervised training with pseudo-labeling for end-to-end neural diarization",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kwon21b_interspeech.html": {
    "title": "Adapting Speaker Embeddings for Speaker Diarisation",
    "volume": "main",
    "abstract": "The goal of this paper is to adapt speaker embeddings for solving the problem of speaker diarisation. The quality of speaker embeddings is paramount to the performance of speaker diarisation systems. Despite this, prior works in the field have directly used embeddings designed only to be effective on the speaker verification task. In this paper, we propose three techniques that can be used to better adapt the speaker embeddings for diarisation: dimensionality reduction, attention-based embedding aggregation, and non-speech clustering. A wide range of experiments is performed on various challenging datasets. The results demonstrate that all three techniques contribute positively to the performance of the diarisation system achieving an average relative improvement of 25.07% in terms of diarisation error rate over the baseline",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b46be82e61a81d67ce0b1903fad65103e22dd620",
    "semantic_title": "adapting speaker embeddings for speaker diarisation",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21y_interspeech.html": {
    "title": "Scenario-Dependent Speaker Diarization for DIHARD-III Challenge",
    "volume": "main",
    "abstract": "In this study, we propose a scenario-dependent speaker diarization approach to handling the diversified scenarios of 11 domains encountered in DIHARD-III challenge with a divide-and-conquer strategy. First, using a ResNet-based audio domain classifier, all domains in DIHARD-III challenge could be divided into several scenarios by different impact factors, such as background noise level, speaker number, and speaker overlap ratio. In each scenario, different combinations of techniques are designed, aiming at achieving the best performance in terms of both diarization error rate (DER) and run-time efficiency. For low signal-to-noise-ration (SNR) scenarios, speech enhancement based on a progressive learning network with multiple intermediate SNR targets is adopted for pre-processing. Conventional clustering-based speaker diarization is utilized to mainly handle speech segments with non-overlapping speakers, while separation-based or neural speaker diarization is used to cope with the overlapping speech regions, which is combined with an iterative fine-tuning strategy to boost the generalization ability. We also explore post-processing to perform system fusion and selection. For DIHARD-III challenge, our scenario-dependent system won the first place among all submitted systems, and significantly outperforms the state-of-the-art clustering-based speaker diarization system, yielding relative DER reductions of 32.17% and 28.34% on development set and evaluation set on Track 1, respectively",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ba4b6b439cbcf81c035875a4b6b6dcf904545054",
    "semantic_title": "scenario-dependent speaker diarization for dihard-iii challenge",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bredin21_interspeech.html": {
    "title": "End-To-End Speaker Segmentation for Overlap-Aware Resegmentation",
    "volume": "main",
    "abstract": "Speaker segmentation consists in partitioning a conversation between one or more speakers into speaker turns. Usually addressed as the late combination of three sub-tasks (voice activity detection, speaker change detection, and overlapped speech detection), we propose to train an end-to-end segmentation model that does it directly. Inspired by the original end-to-end neural speaker diarization approach (EEND), the task is modeled as a multi-label classification problem using permutation-invariant training. The main difference is that our model operates on short audio chunks (5 seconds) but at a much higher temporal resolution (every 16ms). Experiments on multiple speaker diarization datasets conclude that our model can be used with great success on both voice activity detection and overlapped speech detection. Our proposed model can also be used as a post-processing step, to detect and correctly assign overlapped speech regions. Relative diarization error rate improvement over the best considered baseline (VBx) reaches 17% on AMI, 13% on DIHARD 3, and 13% on VoxConverse",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "32a927ae2739138eca406a2e978e5d08daaecac5",
    "semantic_title": "end-to-end speaker segmentation for overlap-aware resegmentation",
    "citation_count": 49,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xue21d_interspeech.html": {
    "title": "Online Streaming End-to-End Neural Diarization Handling Overlapping Speech and Flexible Numbers of Speakers",
    "volume": "main",
    "abstract": "We propose a streaming diarization method based on an end-to-end neural diarization (EEND) model, which handles flexible numbers of speakers and overlapping speech. In our previous study, the speaker-tracing buffer (STB) mechanism was proposed to achieve a chunk-wise streaming diarization using a pre-trained EEND model. STB traces the speaker information in previous chunks to map the speakers in a new chunk. However, it only worked with two-speaker recordings. In this paper, we propose an extended STB for flexible numbers of speakers, FLEX-STB. The proposed method uses a zero-padding followed by speaker-tracing, which alleviates the difference in the number of speakers between a buffer and a current chunk. We also examine buffer update strategies to select important frames for tracing multiple speakers. Experiments on CALLHOME and DIHARD II datasets show that the proposed method achieves comparable performance to the offline EEND method with 1-second latency. The results also show that our proposed method outperforms recently proposed chunk-wise diarization methods based on EEND (BW-EDA-EEND)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "cee96ee69adacfdeb648c230d2c9b01011724724",
    "semantic_title": "online streaming end-to-end neural diarization handling overlapping speech and flexible numbers of speakers",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/anidjar21_interspeech.html": {
    "title": "A Thousand Words are Worth More Than One Recording: Word-Embedding Based Speaker Change Detection",
    "volume": "main",
    "abstract": "Speaker Change Detection (SCD) is the task of segmenting an input audio-recording according to speaker interchanges. This task is essential for many applications, such as automatic voice transcription or Speaker Diarization (SD). This paper focuses on the essential task of audio segmentation and suggests a word-embedding-based solution for the SCD problem. Moreover, we show how to use our approach in order to outperform voice-based solutions for the SD problem. We empirically show that our method can accurately identify the speaker-turns in an audio-recording with 82.12% and 89.02% success in the Recall and F1-score measures",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "49909564d7c4abaf575bb16e2b657d460f7bf6a1",
    "semantic_title": "a thousand words are worth more than one recording: nlp based speaker change point detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/futamata21_interspeech.html": {
    "title": "Phrase Break Prediction with Bidirectional Encoder Representations in Japanese Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "We propose a novel phrase break prediction method that combines implicit features extracted from a pre-trained large language model, a.k.a BERT, and explicit features extracted from BiLSTM with linguistic features. In conventional BiLSTM-based methods, word representations and/or sentence representations are used as independent components. The proposed method takes account of both representations to extract the latent semantics, which cannot be captured by previous methods. The objective evaluation results show that the proposed method obtains an absolute improvement of 3.2 points for the F1 score compared with BiLSTM-based conventional methods using linguistic features. Moreover, the perceptual listening test results verify that a TTS system that applied our proposed method achieved a mean opinion score of 4.39 in prosody naturalness, which is highly competitive with the score of 4.37 for synthesized speech with ground-truth phrase breaks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "cb608e823089311eab56e6f0c23e44e8282f932e",
    "semantic_title": "phrase break prediction with bidirectional encoder representations in japanese text-to-speech synthesis",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vallesperez21_interspeech.html": {
    "title": "Improving Multi-Speaker TTS Prosody Variance with a Residual Encoder and Normalizing Flows",
    "volume": "main",
    "abstract": "Text-to-speech systems recently achieved almost indistinguishable quality from human speech. However, the prosody of those systems is generally flatter than natural speech, producing samples with low expressiveness. Disentanglement of speaker id and prosody is crucial in text-to-speech systems to improve on naturalness and produce more variable syntheses. This paper proposes a new neural text-to-speech model that approaches the disentanglement problem by conditioning a -like architecture on flow-normalized speaker embeddings, and by substituting the reference encoder with a new learned latent distribution responsible for modeling the intra-sentence variability due to the prosody. By removing the reference encoder dependency, the speaker-leakage problem typically happening in this kind of systems disappears, producing more distinctive syntheses at inference time. The new model achieves significantly higher prosody variance than the baseline in a set of quantitative prosody features, as well as higher speaker distinctiveness, without decreasing the speaker intelligibility. Finally, we observe that the normalized speaker embeddings enable much richer speaker interpolations, substantially improving the distinctiveness of the new interpolated speakers",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5ef9c4350ee4f001812c4bc7859f013bee899e15",
    "semantic_title": "improving multi-speaker tts prosody variance with a residual encoder and normalizing flows",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/du21b_interspeech.html": {
    "title": "Rich Prosody Diversity Modelling with Phone-Level Mixture Density Network",
    "volume": "main",
    "abstract": "Generating natural speech with a diverse and smooth prosody pattern is a challenging task. Although random sampling with phone-level prosody distribution has been investigated to generate different prosody patterns, the diversity of the generated speech is still very limited and far from what can be achieved by humans. This is largely due to the use of uni-modal distribution, such as single Gaussian, in the prior works of phone-level prosody modelling. In this work, we propose a novel approach that models phone-level prosodies with GMM based mixture density network (GMM-MDN). Experiments on the LJSpeech dataset demonstrate that phone-level prosodies can precisely control the synthetic speech and GMM-MDN can generate a more natural and smooth prosody pattern than a single Gaussian. Subjective evaluations further show that the proposed approach not only achieves better naturalness, but also significantly improves the prosody diversity in synthetic speech without the need of manual control",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4b39572dd3982adeb462ef7328f7a4c3627ea2e4",
    "semantic_title": "rich prosody diversity modelling with phone-level mixture density network",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujita21_interspeech.html": {
    "title": "Phoneme Duration Modeling Using Speech Rhythm-Based Speaker Embeddings for Multi-Speaker Speech Synthesis",
    "volume": "main",
    "abstract": "This paper proposes a novel speech-rhythm-based method for speaker embeddings. Conventionally spectral feature-based speaker embedding vectors such as the x-vector are used as auxiliary information for multi-speaker speech synthesis. However, speech synthesis with conventional embeddings has difficulty reproducing the target speaker's speech rhythm, one of the important factors among speaker characteristics, because spectral features do not explicitly include speech rhythm. In this paper, speaker embeddings that take speech rhythm information into account are introduced to achieve phoneme duration modeling using a few utterances by the target speaker. A novel point of the proposed method is that rhythm-based embeddings are extracted with phonemes and their durations. They are extracted with a speaker identification model similar to the conventional spectral feature-based one. We conducted two experiments: speaker embeddings generation and speech synthesis with generated embeddings. We show that the proposed model has an EER of 10.3% in speaker identification even with only speech rhythm. Visualizing the embeddings shows that utterances with similar rhythms are also similar in their speaker embeddings. The results of an objective and subjective evaluation on speech synthesis demonstrate that the proposed method can synthesize speech with speech rhythm closer to the target speaker",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "48858516416db1738cf08706a2639ff7108e0d51",
    "semantic_title": "phoneme duration modeling using speech rhythm-based speaker embeddings for multi-speaker speech synthesis",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zou21_interspeech.html": {
    "title": "Fine-Grained Prosody Modeling in Neural Speech Synthesis Using ToBI Representation",
    "volume": "main",
    "abstract": "Benefiting from the great development of deep learning, modern neural text-to-speech (TTS) models can generate speech indistinguishable from natural speech. However, The generated utterances often keep an average prosodic style of the database instead of having rich prosodic variation. For pitch-stressed languages, such as English, accurate intonation and stress are important for conveying semantic information. In this work, we propose a fine-grained prosody modeling method in neural speech synthesis with ToBI (Tones and Break Indices) representation. The proposed system consists of a text frontend for ToBI prediction and a Tacotron-based TTS module for prosody modeling. By introducing the ToBI representation, we can control the system to synthesize speech with accurate intonation and stress at syllable level. Compared with the two baselines (Tacotron and unsupervised method), experiments show that our model can generate more natural speech with more accurate prosody, as well as effectively control the stress, intonation, and pause of the speech",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "89a74c51e757aa5c905d264224b10183fd6e7d07",
    "semantic_title": "fine-grained prosody modeling in neural speech synthesis using tobi representation",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sharma21b_interspeech.html": {
    "title": "Intra-Sentential Speaking Rate Control in Neural Text-To-Speech for Automatic Dubbing",
    "volume": "main",
    "abstract": "Automatically dubbed speech of a video involves: (i) segmenting the target sentences into phrases to reflect the speech-pause arrangement used by the original speaker, and (ii) adjusting the speaking rate of the synthetic voice at the phrase-level to match the exact timing of each corresponding source phrase. In this work, we investigate a post-segmentation approach to control the speaking rate of neural Text-to-Speech (TTS) at the phrase-level after generating the entire sentence. Our post-segmentation method relies on the attention matrix generated by the context generation step to perform a force-alignment over pause markers inserted in the input text. We show that: (i) our approach can be more accurate than applying an off-the-shelf forced aligner, and (ii) post-segmentation method permits generation more fluent speech than pre-segmentation approach described in [1]",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "05636b11064ccafeeae18a0ba304c164886de689",
    "semantic_title": "intra-sentential speaking rate control in neural text-to-speech for automatic dubbing",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21u_interspeech.html": {
    "title": "Applying the Information Bottleneck Principle to Prosodic Representation Learning",
    "volume": "main",
    "abstract": "This paper describes a novel design of a neural network-based speech generation model for learning prosodic representation. The problem of representation learning is formulated according to the information bottleneck (IB) principle. A modified VQ-VAE quantized layer is incorporated in the speech generation model to control the IB capacity and adjust the balance between reconstruction power and disentangle capability of the learned representation. The proposed model is able to learn word-level prosodic representations from speech data. With an optimized IB capacity, the learned representations not only are adequate to reconstruct the original speech but also can be used to transfer the prosody onto different textual content. Extensive results of the objective and subjective evaluation are presented to demonstrate the effect of IB capacity control, the effectiveness, and potential usage of the learned prosodic representation in controllable neural speech generation",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8411dccd15ca8e27d08978e6f03adf365762d94a",
    "semantic_title": "applying the information bottleneck principle to prosodic representation learning",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/baird21_interspeech.html": {
    "title": "A Prototypical Network Approach for Evaluating Generated Emotional Speech",
    "volume": "main",
    "abstract": "The collection of emotional speech data is a time-consuming and costly endeavour. Generative networks can be applied to augment the limited audio data artificially. However, it is challenging to evaluate generated audio for its similarity to source data, as current quantitative metrics are not necessarily suited to the audio domain. We explore the use of a prototypical network to evaluate four classes of generated emotional audio with this in mind. We first extract spectrogram images from WaveGan generated audio and other audio augmentation approaches, comparing similarity to the class prototype and diversity within the embedding space. Furthermore, we augment the source training set with each augmentation type and perform a classification to explore the generated audio plausibility. Results suggest that quality and diversity can be quantitatively observed with this approach. In the chosen context, we see that WaveGan generated data is recognisable as a source data class (F -score 43.6%), and the samples add similar diversity as unseen source data. This result leads to more plausible data for augmentation of the source training set — achieving up to 63.9% F which is a 3.5% improvement over the source data baseline",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "a6eed884bed68e18ff66defbd5f81ec01be08db1",
    "semantic_title": "a prototypical network approach for evaluating generated emotional speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yoshinaga21_interspeech.html": {
    "title": "A Simplified Model for the Vocal Tract of [s] with Inclined Incisors",
    "volume": "main",
    "abstract": "To examine the effects of inclined incisors on the phonation of [s], a simplified vocal tract model is proposed, and the acoustic characteristics with different maxillary incisor angles are predicted by the model. As a control model, a realistic vocal tract replica of [s] was constructed from medical images, and the angle of the maxillary incisor was changed from the original position up to 30°. The simplified model was constructed with a rectangular flow channel using the average dimensions of the vocal tracts for five Japanese subjects. Both geometries were set in an anechoic chamber, and sounds generated from the geometries were recorded with a microphone. The results showed that amplitudes of the sound generated by the realistic geometry were decreased by increasing the incisor angle, and this tendency agreed well with the simplified model. Moreover, the slope value of the decrease in overall pressure levels estimated by the model was consistent with that of the realistic geometry, indicating the capability of estimating the effects of inclined incisors with dental prostheses on the production of [s] by using the simplified model",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b1165ec30f143e2e54bb5064c6d8d9607350c9ff",
    "semantic_title": "a simplified model for the vocal tract of [s] with inclined incisors",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/arai21b_interspeech.html": {
    "title": "Vocal-Tract Models to Visualize the Airstream of Human Breath and Droplets While Producing Speech",
    "volume": "main",
    "abstract": "Due to the COVID-19 pandemic, visualizing the airstream of human breath during speech production has become extremely important from the viewpoint of preventing infection. In addition, visualizing droplets and the larger drops expelled when we speak consonantal sounds may help for the same reason. One visualization technique is to pass a laser sheet through the droplet cloud produced by a human speaker. However, the laser poses certain health risks for human beings. Therefore, we developed an alternative method to passing a laser against a human body in which we utilize physical models of the human vocal tract. First, we tested a head-shaped model with a lung model from our previous study to visualize the exhaled breath during vowel production (with and without a mask). Then, we implemented an extended version of the anatomical-type vocal-tract model introduced in our previous study. With this newly developed model, lips are made of the same flexible material that was used to form the tongue part in the previous model. We also attached these lips to another previous model for producing sounds including /b/. Finally, the lip models were tested to visualize the droplet cloud including expelled drops present while producing a bilabial plosive sound",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "57593514279e91ea2d62eda6a6d410f7ec876191",
    "semantic_title": "vocal-tract models to visualize the airstream of human breath and droplets while producing speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanji21_interspeech.html": {
    "title": "Using Transposed Convolution for Articulatory-to-Acoustic Conversion from Real-Time MRI Data",
    "volume": "main",
    "abstract": "We herein propose a deep neural network-based model for articulatory-to-acoustic conversion from real-time MRI data. Although rtMRI, which can record entire articulatory organs with a high resolution, has an advantage in articulatory-to-acoustic conversion, it has a relatively low sampling rate. To address this, we incorporated the super-resolution technique in the temporal dimension with a transposed convolution. With the use of transposed convolution, the resolution can be increased by applying the inversion process of resolution reduction of a standard CNN. To evaluate the performance on the datasets with different temporal resolutions, we conducted experiments using two datasets: USC-TIMIT and Japanese rtMRI dataset. Results of the experiments performed using mel-cepstrum distortion and PESQ showed that transposed convolution is effective for generating accurate acoustic features. We also confirmed that increasing the magnification of the super-resolution leads to an improvement in the PESQ score",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "a0e26a34b43e3357e77f4689927b338a5d5787f6",
    "semantic_title": "using transposed convolution for articulatory-to-acoustic conversion from real-time mri data",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/inaam21_interspeech.html": {
    "title": "Comparison Between Lumped-Mass Modeling and Flow Simulation of the Reed-Type Artificial Vocal Fold",
    "volume": "main",
    "abstract": "The sound generated by a reed-type artificial vocal fold was predicted by a one-mass modeling and numerical flow simulation to examine the sound generation mechanisms of the artificial vocal fold. For the one-mass modeling, the reed oscillation was modeled with an equivalent spring constant, and the flow rate was estimated by Bernoulli's equation. For the flow simulation, the flow and acoustic fields were predicted with compressible Navier-Stokes Equations, while the reed oscillation was calculated by a one-dimensional beam equation. The experimentation was conducted by measuring the sound of an artificial vocal fold in an anechoic chamber. The results of the acoustic measurement showed that the sound amplitudes in the flow simulation agreed well with the experiment, while the one-mass model underestimated the amplitudes in a higher frequency range. Reed displacement and flow rate comparisons indicated that the flow retention in the reed retainer caused the asymmetry in the flow rate waveform, hence producing larger amplitudes for the flow simulation in the higher frequency range. The flow simulation enabled to predict this flow retention which cannot be modeled in the one-dimensional one-mass model, and it is anticipated to apply the flow simulation to develop a better artificial vocal fold",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fa13d274575a0a66e4daa5c83ac2d34d7568df84",
    "semantic_title": "comparison between lumped-mass modeling and flow simulation of the reed-type artificial vocal fold",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/werner21_interspeech.html": {
    "title": "Inhalations in Speech: Acoustic and Physiological Characteristics",
    "volume": "main",
    "abstract": "This paper examines the acoustic properties of breath noises in speech pauses in relation to similar speech segments and with regard to their inhalation speed. We measured intensity, center of gravity, and formants, as well as kinematic data (via Respiratory Inductance Plethysmography) for inhalations, aspirations of stops, glottal fricatives, and schwa vowels. We find that inhalations within speech are louder than those initiating speech, share spectral properties (center of gravity) with the aspiration phase of /k/-realizations, and generally involve a more open vocal tract (higher F1) than schwa-realizations. Intensity, center of gravity, and F1 are found to be positively correlated to inhalation speed. Overall, we conclude that jaw openness and inhalation speed are major contributors to inhalation noises in speech pauses",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f18fc7eda4e6ef6cd90100cb4d06e8f23b0925d1",
    "semantic_title": "inhalations in speech: acoustic and physiological characteristics",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21j_interspeech.html": {
    "title": "Model-Based Exploration of Linking Between Vowel Articulatory Space and Acoustic Space",
    "volume": "main",
    "abstract": "While the acoustic vowel space has been extensively studied in previous research, little is known about the high-dimensional articulatory space of vowels. The articulatory imaging techniques are limited to tracking only a few key articulators, leaving the rest of the articulators unmonitored. In the present study, we attempted to develop a detailed articulatory space obtained by training a 3D articulatory synthesizer to learn eleven British English vowels. An analysis-by-synthesis strategy was used to acoustically optimize vocal tract parameters that represent twenty articulatory dimensions. The results show that tongue height and retraction, larynx location and lip roundness are the most perceptually distinctive articulatory dimensions. Yet, even for these dimensions, there is a fair amount of articulatory overlap between vowels, unlike the fine-grained acoustic space. This method opens up the possibility of using modelling to investigate the link between speech production and perception",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "60099f7e6b42423bfb342e4c19293fb5ccdeaf09",
    "semantic_title": "model-based exploration of linking between vowel articulatory space and acoustic space",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elmers21_interspeech.html": {
    "title": "Take a Breath: Respiratory Sounds Improve Recollection in Synthetic Speech",
    "volume": "main",
    "abstract": "This study revisits Whalen et al. (1995, JASA) by evaluating English speaking participants in a perception experiment to determine if their recollection is affected by including breath noises in sentences generated by a speech synthesis system. Whalen found an improvement in recollection for sentences that were preceded by a breath noise compared to sentences without one. While Whalen and colleagues used formant synthesis to render the English sentences, we use a modern concatenative synthesis system. The present study uses inhalations of three different lengths: 0 ms (no breath noise), 300 ms (short breath noise), and 600 ms (long breath noise). Our results are consistent with Whalen and colleagues for the 600 ms condition, but not for the 300 ms condition, indicating that not all inhalations improved recollection. The present study also found a significant effect for sentence length, illustrating that shorter sentences have higher accuracy for recollection than longer sentences. Overall, the present study indicates that respiratory sounds are important to the recollection of synthesized speech and that researchers should focus on longer and more complex types of speech, such as paragraphs or dialogues, for future studies",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3b734f61fb7fce25e7b9050cbec1374b51ba5d49",
    "semantic_title": "take a breath: respiratory sounds improve recollection in synthetic speech",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21m_interspeech.html": {
    "title": "Modeling Sensorimotor Adaptation in Speech Through Alterations to Forward and Inverse Models",
    "volume": "main",
    "abstract": "When speakers are exposed to auditory feedback perturbations of a particular vowel, they not only adapt their productions of that vowel but also transfer this change to other, untrained, vowels. However, current models of speech sensorimotor adaptation, which rely on changes in the feedforward control of specific speech units, are unable to account for this type of generalization. Here, we developed a neural-network based model to simulate speech sensorimotor adaptation, and assess whether updates to internal control models can account for observed patterns of generalization. Based on a dataset generated from the Maeda plant, we trained two independent neural networks: 1) an inverse model, which generates motor commands for desired acoustic outcomes and 2) a forward model, which maps motor commands to acoustic outcomes (prediction). When vowel formant perturbations were given, both forward and inverse models were updated when there was a mismatch between predicted and perceived output. Our results replicate behavioral experiments: the model altered its production to counteract the perturbation, and showed gradient transfer of this learning dependent on acoustic distance between training and test vowels. These results suggest that updating paired forward and inverse models provides a plausible account for sensorimotor adaptation in speech",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d4987b12cc52ad1fdda832ce9e8d6cfa2499c17a",
    "semantic_title": "modeling sensorimotor adaptation in speech through alterations to forward and inverse models",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kawahara21_interspeech.html": {
    "title": "Mixture of Orthogonal Sequences Made from Extended Time-Stretched Pulses Enables Measurement of Involuntary Voice Fundamental Frequency Response to Pitch Perturbation",
    "volume": "main",
    "abstract": "Auditory feedback plays an essential role in the regulation of the fundamental frequency of voiced sounds. The fundamental frequency also responds to auditory stimulation other than the speaker's voice. We propose to use this response of the fundamental frequency of sustained vowels to frequency-modulated test signals for investigating involuntary control of voice pitch. This involuntary response is difficult to identify and isolate by the conventional paradigm, which uses step-shaped pitch perturbation. We recently developed a versatile measurement method using a mixture of orthogonal sequences made from a set of extended time-stretched pulses (TSP). In this article, we extended our approach and designed a set of test signals using the mixture to modulate the fundamental frequency of artificial signals. For testing the response, the experimenter presents the modulated signal aurally while the subject is voicing sustained vowels. We developed a tool for conducting this test quickly and interactively. We make the tool available as an open-source and also provide executable GUI-based applications. Preliminary tests revealed that the proposed method consistently provides compensatory responses with about 100 ms latency, representing involuntary control. Finally, we discuss future applications of the proposed method for objective and non-invasive auditory response measurements",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "72679d24401dab4e54f153241f8d17d6eb1033b1",
    "semantic_title": "mixture of orthogonal sequences made from extended time-stretched pulses enables measurement of involuntary voice fundamental frequency response to pitch perturbation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/you21c_interspeech.html": {
    "title": "Contextualized Attention-Based Knowledge Transfer for Spoken Conversational Question Answering",
    "volume": "main",
    "abstract": "Spoken conversational question answering (SCQA) requires machines to model the flow of multi-turn conversation given the speech utterances and text corpora. Different from traditional text question answering (QA) tasks, SCQA involves audio signal processing, passage comprehension, and contextual understanding. However, ASR systems introduce unexpected noisy signals to the transcriptions, which result in performance degradation on SCQA. To overcome the problem, we propose CADNet, a novel contextualized attention-based distillation approach, which applies both cross-attention and self-attention to obtain ASR-robust contextualized embedding representations of the passage and dialogue history for performance improvements. We also introduce the spoken conventional knowledge distillation framework to distill the ASR-robust knowledge from the estimated probabilities of the model to the We conduct extensive experiments on the Spoken-CoQA dataset and demonstrate that our approach achieves remarkable performance in this task",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b5c1c096373f8ca1dafd6bedd2e2c496aef9df35",
    "semantic_title": "contextualized attention-based knowledge transfer for spoken conversational question answering",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/duan21_interspeech.html": {
    "title": "Injecting Descriptive Meta-Information into Pre-Trained Language Models with Hypernetworks",
    "volume": "main",
    "abstract": "Pre-trained language models have been widely adopted as backbones in various natural language processing tasks. However, existing pre-trained language models ignore the descriptive meta-information in the text such as the distinction between the title and the mainbody, leading to over-weighted attention to insignificant text. In this paper, we propose a hypernetwork-based architecture to model the descriptive meta-information and integrate it into pre-trained language models. Evaluations on three natural language processing tasks show that our method notably improves the performance of pre-trained language models and achieves the state-of-the-art results on keyphrase extraction",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "9dc109baa16795d0248c9401e5bd95417b8b18c0",
    "semantic_title": "injecting descriptive meta-information into pre-trained language models with hypernetworks",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rohmatillah21_interspeech.html": {
    "title": "Causal Confusion Reduction for Robust Multi-Domain Dialogue Policy",
    "volume": "main",
    "abstract": "In the multi-domain dialogue system, dialog policy plays an important role since it determines the suitable actions based on the user's goals. However, in many recent works, most of the dialogue optimizations, especially that use reinforcement learning (RL) methods, do not perform well. The main problem is that the initial step of optimization that involves the behavior cloning (BC) methods suffer from the causal confusion problem, which means that the agent misidentifies true cause of an expert action in current state. This paper proposes a novel method to improve the performance of BC method in dialogue system. Instead of only predicting correct action given a state from dataset, we introduce the auxiliary tasks to predict both of current belief state and recent user utterance in order to reduce causal confusion of the expert action in the dataset since those features are important in every dialog turn. Experiments on ConvLab-2 shows that, by using this method, all of RL based optimizations are improved. Furthermore, the agent based on the proximal policy optimization shows very significant improvement with the help of the proposed BC agent weights both in policy evaluation as well as in end-to-end system evaluation",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ff18115e4ab69c3060d72086cfe1f61f2bf0746f",
    "semantic_title": "causal confusion reduction for robust multi-domain dialogue policy",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujie21_interspeech.html": {
    "title": "Timing Generating Networks: Neural Network Based Precise Turn-Taking Timing Prediction in Multiparty Conversation",
    "volume": "main",
    "abstract": "A brand new neural network based precise timing generation framework, named the Timing Generating Network (TGN), is proposed and applied to turn-taking timing decision problems. Although turn-taking problems have conventionally been formalized as users' end-of-turn detection, this approach cannot estimate the precise timing at which a spoken dialogue system should take a turn to start its utterance. Since several conventional approaches estimate precise timings but the estimation executed only at/after the end of preceding user's utterance, they highly depend on the accuracy of intermediate decision modules, such as voice activity detection, etc. The advantages of the TGN are that its parameters are tunable via error backpropagation as it is described in a differentiable form as a whole, and it is free from inter-module error propagation as it has no deterministic intermediate modules. The experimental results show that the proposed system is superior to a conventional turn-taking system that adopts the hard decisions on user's voice activity detection and response time estimation",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "944c00e20ed34f0537471017cee3b8244727b4f1",
    "semantic_title": "timing generating networks: neural network based precise turn-taking timing prediction in multiparty conversation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21n_interspeech.html": {
    "title": "Human-to-Human Conversation Dataset for Learning Fine-Grained Turn-Taking Action",
    "volume": "main",
    "abstract": "Conducting natural turn-taking behavior takes a crucial part in the user experience of modern spoken dialogue systems. One way to build such system is to learn those behaviors from real-world human-to-human dialogues, which have the most diverse and fine-grained turn-taking actions than any manual constructed sessions In this paper, we propose a Dataset — FTAD which could be used to learn turn-taking policies directly from human. First, we design an annotation mechanism to transform existing human-to-human dialogue session into structural data with most fine-grained turn-taking actions reserved. Then we explored a set of supervised learning tasks on it, showing the challenge and potential of learning complete fine-grained turn-taking policies based on such data",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "24350eed68fa7069f905c55029669e2583f8cf2b",
    "semantic_title": "human-to-human conversation dataset for learning fine-grained turn-taking action",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sundararaman21_interspeech.html": {
    "title": "PhonemeBERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript",
    "volume": "main",
    "abstract": "Recent years have witnessed significant improvement in ASR systems to recognize spoken utterances. However, it is still a challenging task for noisy and out-of-domain data, where ASR errors are prevalent in the transcribed text. These errors significantly degrade the performance of downstream tasks such as intent and sentiment detection. In this work, we propose a BERT-style language model, referred to as PhonemeBERT that learns a joint language model with phoneme sequence and ASR transcript to learn phonetic-aware representations that are robust to ASR errors. We show that PhonemeBERT leverages phoneme sequences as additional features that outperform word-only models on downstream tasks. We evaluate our approach extensively by generating noisy data for three benchmark datasets — Stanford Sentiment Treebank, TREC and ATIS for sentiment, question and intent classification tasks respectively in addition to a real-life sentiment dataset. The results of the proposed approach beats the state-of-the-art baselines comprehensively on each dataset. Additionally, we show that PhonemeBERT can also be utilized as a pre-trained encoder in a low-resource setup where we only have ASR-transcripts for the downstream tasks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "cb0bfddfa606157f1709b2793d3da36577ac946d",
    "semantic_title": "phoneme-bert: joint language modelling of phoneme sequence and asr transcript",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21d_interspeech.html": {
    "title": "Joint Retrieval-Extraction Training for Evidence-Aware Dialog Response Selection",
    "volume": "main",
    "abstract": "Neural dialog response selection models infer by scoring each candidate response given the dialog context, and the cross-encoder method yields state-of-the-art (SOTA) results for the task. In the method, the candidate scores are computed by feeding the output embedding of the first token in the input sequence, which is a concatenation of response and context, to a linear layer for making prediction. However, the embeddings of the other tokens in the sequence are not modeled explicitly, and inferring the candidate scores only with the first token makes the result not interpretable. To address the challenge, we propose a Retrieval-EXtraction encoder (REX) for dialog response selection. We augment the existing first-token- or sequence- based retrieval approach with an extraction loss. The loss provides gradient signal from each token during training and allows the model to learn token-level evidence and to select response based on important keywords. We show that REX achieves the new SOTA in the dialog response selection task. Also, our qualitative analysis suggests that REX highlights evidence it infers selections from and makes the inference result interpretable",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4c1c11977e17ac1a08e4b466bb97aa7f73d8fcce",
    "semantic_title": "joint retrieval-extraction training for evidence-aware dialog response selection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shenoy21_interspeech.html": {
    "title": "Adapting Long Context NLM for ASR Rescoring in Conversational Agents",
    "volume": "main",
    "abstract": "Neural Language Models (NLM), when trained and evaluated with context spanning multiple utterances, have been shown to consistently outperform both conventional n-gram language models and NLMs that use limited context. In this paper, we investigate various techniques to incorporate turn based context history into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent based NLMs, we explore context carry over mechanism and feature based augmentation, where we incorporate other forms of contextual information such as bot response and system dialogue acts as classified by a Natural Language Understanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem with contextual NLM, we propose the use of attention layer over lexical metadata to improve feature based augmentation. Additionally, we adapt our contextual NLM towards user provided on-the-fly speech patterns by leveraging encodings from a large pre-trained masked language model and performing fusion with a Transformer-XL based NLM. We test our proposed models using N-best rescoring of ASR hypotheses of task-oriented dialogues and also evaluate on downstream NLU tasks such as intent classification and slot labeling. The best performing model shows a relative WER between 1.6% and 9.1% and a slot labeling F1 score improvement of 4% over non-contextual baselines",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3d0718f438be9715a3f8f3250b955c84d6e0b13e",
    "semantic_title": "adapting long context nlm for asr rescoring in conversational agents",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21h_interspeech.html": {
    "title": "Oriental Language Recognition (OLR) 2020: Summary and Analysis",
    "volume": "main",
    "abstract": "The fifth Oriental Language Recognition (OLR) Challenge focuses on language recognition in a variety of complex environments to promote its development. The OLR 2020 Challenge includes three tasks: (1) cross-channel language identification, (2) dialect identification, and (3) noisy language identification. We choose as the principle evaluation metric, and the Equal Error Rate (EER) as the secondary metric. There were 58 teams participating in this challenge and one third of the teams submitted valid results. Compared with the best baseline, the values of Top 1 system for the three tasks were relatively reduced by 82%, 62% and 48%, respectively. This paper describes the three tasks, the database profile, and the final results. We also outline the novel approaches that improve the performance of language recognition systems most significantly, such as the utilization of auxiliary information",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d2ad78f75461a538db012f5d8db470c6ce904bbe",
    "semantic_title": "oriental language recognition (olr) 2020: summary and analysis",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/duroselle21b_interspeech.html": {
    "title": "Language Recognition on Unknown Conditions: The LORIA-Inria-MULTISPEECH System for AP20-OLR Challenge",
    "volume": "main",
    "abstract": "We describe the LORIA-Inria-MULTISPEECH system submitted to the Oriental Language Recognition AP20-OLR Challenge. This system has been specifically designed to be robust to unknown conditions: channel mismatch (task 1) and noisy conditions (task 3). Three sets of studies have been carried out for elaborating the system: design of multilingual bottleneck features, selection of robust features by evaluating language recognition performance on an unobserved channel, and design of the final models with different loss functions which exploit channel diversity within the training set. Key factors for robustness to unknown conditions are data augmentation techniques, stochastic weight averaging, and regularization of TDNNs with domain robustness loss functions. The final system is the combination of four TDNNs using bottleneck features and one GMM using SDC-MFCC features. Within the AP20-OLR Challenge, it achieves the top performance for tasks 1 and 3 with a of respectively 0.0239 and 0.0374. This validates the approach for generalization to unknown conditions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ace004da1eb34be4b4fdad40f1548521382faafa",
    "semantic_title": "language recognition on unknown conditions: the loria-inria-multispeech system for ap20-olr challenge",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kong21b_interspeech.html": {
    "title": "Dynamic Multi-Scale Convolution for Dialect Identification",
    "volume": "main",
    "abstract": "Time Delay Neural Networks (TDNN)-based methods are widely used in dialect identification. However, in previous work with TDNN application, subtle variant is being neglected in different feature scales. To address this issue, we propose a new architecture, named dynamic multi-scale convolution, which consists of dynamic kernel convolution, local multi-scale learning, and global multi-scale pooling. Dynamic kernel convolution captures features between short-term and long-term context adaptively. Local multi-scale learning, which represents multi-scale features at a granular level, is able to increase the range of receptive fields for convolution operation. Besides, global multi-scale pooling is applied to aggregate features from different bottleneck layers in order to collect information from multiple aspects. The proposed architecture significantly outperforms state-of-the-art system on the AP20-OLR-dialect-task of oriental language recognition (OLR) challenge 2020, with the best average cost performance ( ) of 0.067 and the best equal error rate (EER) of 6.52%. Compared with the known best results, our method achieves 9% of and 45% of EER relative improvement, respectively. Furthermore, the parameters of proposed model are 91% fewer than the best known model",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3e9e44d843240b7affe14a31db3ffac17c0d74bc",
    "semantic_title": "dynamic multi-scale convolution for dialect identification",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21z_interspeech.html": {
    "title": "An End-to-End Dialect Identification System with Transfer Learning from a Multilingual Automatic Speech Recognition Model",
    "volume": "main",
    "abstract": "In this paper, we propose an end-to-end (E2E) dialect identification system trained using transfer learning from a multilingual automatic speech recognition (ASR) model. This is also an extension of our submitted system to the Oriental Language Recognition Challenge 2020 (AP20-OLR). We verified its applicability using the dialect identification (DID) task of the AP20-OLR. First, we trained a robust conformer-based joint connectionist temporal classification (CTC) /attention multilingual E2E ASR model using the training corpora of eight languages, independent of the target dialects. Second, we initialized the E2E-based classifier with the ASR model's shared encoder using a transfer learning approach. Finally, we trained the classifier on the target dialect corpus. We obtained the final classifier by selecting the best model from the following: (1) the averaged model in term of the loss values; and (2) the averaged model in term of classification accuracy Our experiments on the DID test-set of the AP20-OLR demonstrated that significant identification improvements were achieved for three Chinese dialects. The performances of our system outperforms the winning team of the AP20-OLR, with the largest relative reductions of 19.5% in and 25.2% in EER",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "cc5e6505684f341ede9ed3ab0bb2b317f436ab01",
    "semantic_title": "an end-to-end dialect identification system with transfer learning from a multilingual automatic speech recognition model",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yu21b_interspeech.html": {
    "title": "Language Recognition Based on Unsupervised Pretrained Models",
    "volume": "main",
    "abstract": "Unsupervised pretrained models have been proven to rival or even outperform supervised systems in various speech recognition tasks. However, their performance for language recognition is still left to be explored. In this paper, we construct several language recognition systems based on existing unsupervised pretraining approaches, and explore their credibility and performance to learn high-level generalization of language. We discover that unsupervised pretrained models capture expressive and highly linear-separable features. With these representations, language recognition can perform well even when the classifiers are relatively simple or only a small amount of labeled data is available. Although linear classifiers are usable, neural nets with RNN structures improve the results. Meanwhile, unsupervised pretrained models are able to gain refined representations on audio frame level that are strongly coupled with the acoustic features of the input sequence. Therefore these features contain redundant information of speakers and channels with few relations to the identity of the language. This nature of unsupervised pretrained models causes a performance degradation in language recognition tasks on cross-channel tests",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fe3a861c719ba4f95a564e5ca716507bb9c7ec6a",
    "semantic_title": "language recognition based on unsupervised pretrained models",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21i_interspeech.html": {
    "title": "Additive Phoneme-Aware Margin Softmax Loss for Language Recognition",
    "volume": "main",
    "abstract": "This paper proposes an additive phoneme-aware margin softmax (APM-Softmax) loss to train the multi-task learning network with phonetic information for language recognition. In additive margin softmax (AM-Softmax) loss, the margin is set as a constant during the entire training for all training samples, and that is a suboptimal method since the recognition difficulty varies in training samples. In additive angular margin softmax (AAM-Softmax) loss, the additional angular margin is set as a constant as well. In this paper, we propose an APM-Softmax loss for language recognition with phoneitc multi-task learning, in which the additive phoneme-aware margin is automatically tuned for different training samples. More specifically, the margin of language recognition is adjusted according to the results of phoneme recognition. Experiments are reported on Oriental Language Recognition (OLR) datasets, and the proposed method improves AM-Softmax loss and AAM-Softmax loss in different language recognition testing conditions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "02772c8b9b0657b97233800ca744aae4524acac6",
    "semantic_title": "additive phoneme-aware margin softmax loss for language recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jahchan21_interspeech.html": {
    "title": "Towards an Accent-Robust Approach for ATC Communications Transcription",
    "volume": "main",
    "abstract": "Air Traffic Control (ATC) communications are a typical example where Automatic Speech Recognition could face various challenges: audio data are quite noisy due to the characteristics of capturing mechanisms. All speakers involved use a specific English-based phraseology and a significant number of pilots and controllers are non-native English speakers. The aim of this work is to enhance pilot-ATC communications by adding a Speech to Text (STT) capability that will transcribe ATC speech into text on the cockpit interfaces to help the pilot understand ATC speech in a more optimal manner (be able to verify what he/she heard on the radio by looking at the text transcription, be able to decipher non-native English accents from controllers, not lose time asking the ATC to repeat the message several times). In this paper, we first describe an accent analysis study which was carried out both on a theoretical level but also with the help of feedback from several hundred airline pilots. Then, we present the dataset that was set up for this work. Finally, we describe the experiments we have implemented and the impact of the speaker accent on the performance of a speech to text engine",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "cd0e4c01d97cc6fcd34be9a978a366830b7cbd1a",
    "semantic_title": "towards an accent-robust approach for atc communications transcription",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/szoke21_interspeech.html": {
    "title": "Detecting English Speech in the Air Traffic Control Voice Communication",
    "volume": "main",
    "abstract": "Developing in-cockpit voice enabled applications require a real-world dataset with labels and annotations. We launched a community platform for collecting the Air-Traffic Control (ATC) speech, world-wide in the ATCO project. Filtering out non-English speech is one of the main components in the data processing pipeline. The proposed English Language Detection (ELD) system is based on the embeddings from Bayesian subspace multinomial model. It is trained on the word confusion network from an ASR system. It is robust, easy to train, and light weighted. We achieved 0.0439 equal-error-rate (EER), a 50% relative reduction as compared to the state-of-the-art acoustic ELD system based on x-vectors, in the in-domain scenario. Further, we achieved an EER of 0.1352, a 33% relative reduction as compared to the acoustic ELD, in the unseen language (out-of-domain) condition. We plan to publish the evaluation dataset from the ATCO project",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "21696351d14a9554bac187eddc2e01d502bf27e5",
    "semantic_title": "detecting english speech in the air traffic control voice communication",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ohneiser21_interspeech.html": {
    "title": "Robust Command Recognition for Lithuanian Air Traffic Control Tower Utterances",
    "volume": "main",
    "abstract": "The maturity of automatic speech recognition (ASR) systems at controller working positions is currently a highly relevant technological topic in air traffic control (ATC). However, ATC service providers are less interested in pure word error rate (WER). They want to see benefits of ASR applications for ATC. Such applications transform recognized word sequences into semantic meanings, i.e., a number of related concepts such as callsign, type, value, unit, etc., which are combined to form commands. Digitized concepts or recognized commands can enter ATC systems based on an ontology for utterance annotation agreed between European ATC stakeholders. Command recognition (CR) has already been performed in approach control. However, spoken utterances of tower controllers are longer, include more free speech, and contain other command types than in approach. An automatic CR rate of 95.8% is achievable on perfect word recognition, i.e., manually transcribed audio recordings (gold transcriptions), taken from Lithuanian controllers in a multiple remote tower environment. This paper presents CR results for various speech-to-text models with different WERs on tower utterances. Although WERs were around 9%, we achieve CR rates of 85%. CR rates only slightly decrease with higher WERs, which enables to bring ASR applications closer to operational ATC environment",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "948b6e890616a55207e5c18a477d97d39206050a",
    "semantic_title": "robust command recognition for lithuanian air traffic control tower utterances",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zuluagagomez21_interspeech.html": {
    "title": "Contextual Semi-Supervised Learning: An Approach to Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems",
    "volume": "main",
    "abstract": "Air traffic management and specifically air-traffic control (ATC) rely mostly on voice communications between Air Traffic Controllers (ATCos) and pilots. In most cases, these voice communications follow a well-defined grammar that could be leveraged in Automatic Speech Recognition (ASR) technologies. The callsign used to address an airplane is an essential part of all ATCo-pilot communications. We propose a two-step approach to add contextual knowledge during semi-supervised training to reduce the ASR system error rates at recognizing the part of the utterance that contains the callsign. Initially, we represent in a WFST the contextual knowledge (i.e. air-surveillance data) of an ATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the contextual knowledge is added by second-pass decoding (i.e. lattice re-scoring). Results show that ‘unseen domains' (e.g. data from airports not present in the supervised training data) are further aided by contextual SSL when compared to standalone SSL. For this task, we introduce the Callsign Word Error Rate (CA-WER) as an evaluation metric, which only assesses ASR performance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER relative improvement applying SSL with an additional 17.5% CA-WER improvement by adding contextual knowledge during SSL on a challenging ATC-based test set gathered from LiveATC",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "560926f059acd6e0f87a2edae3ca9b5c9990214c",
    "semantic_title": "contextual semi-supervised learning: an approach to leverage air-surveillance and untranscribed atc data in asr systems",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kocour21_interspeech.html": {
    "title": "Boosting of Contextual Information in ASR for Air-Traffic Call-Sign Recognition",
    "volume": "main",
    "abstract": "Contextual adaptation of ASR can be very beneficial for multi-accent and often noisy Air-Traffic Control (ATC) speech. Our focus is call-sign recognition, which can be used to track conversations of ATC operators with individual airplanes. We developed a two-stage boosting strategy, consisting of HCLG boosting and Lattice boosting. Both are implemented as WFST compositions and the contextual information is specific to each utterance. In HCLG boosting we give score discounts to individual words, while in Lattice boosting the score discounts are given to word sequences. The context data have origin in surveillance database of OpenSky Network. From this, we obtain lists of call-signs that are made more likely to appear in the best hypothesis of ASR. This also improves the accuracy of the NLU module that recognizes the call-signs from the best hypothesis of ASR As part of ATCO project, we collected liveatc test set2. The boosting of call-signs leads to 4.7% absolute WER improvement and 27.1% absolute increase of Call-Sign recognition Accuracy (CSA). Our best result of 82.9% CSA is quite good, given that the data is noisy, and WER 28.4% is relatively high. We believe there is still room for improvement",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8e9368cea36e62a650c336eabeb92da9482bffe8",
    "semantic_title": "boosting of contextual information in asr for air-traffic call-sign recognition",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elie21_interspeech.html": {
    "title": "Modeling the Effect of Military Oxygen Masks on Speech Characteristics",
    "volume": "main",
    "abstract": "Wearing an oxygen mask changes the speech production of speakers. It indeed modifies the vocal apparatus and perturbs the articulatory movements of the speaker. This paper studies the impact of the oxygen mask of military aircraft pilots on formant trajectories, both dynamically (variations of the formants at a utterance level) and globally (mean value at the utterance level) for 12 speakers A comparative analysis of speech collected with and without an oxygen mask shows that the mask has a significant impact on the formant trajectories, both on the mean values and on the formant variations at the utterance level. This impact is strongly dependent on the speaker and also on the mask model. These observations suggest that the articulatory movements of the speaker are modified by the presence of the mask These observations are validated via a preliminary ASR experiment that uses a data augmentation technique based on articulatory perturbations that are driven by our experimental observations",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3c5d45fc145c07d984541733df42530637be3005",
    "semantic_title": "modeling the effect of military oxygen masks on speech characteristics",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/milde21_interspeech.html": {
    "title": "MoM: Minutes of Meeting Bot",
    "volume": "main",
    "abstract": "We present MoM (Minutes of Meeting) bot, an automatic meeting transcription system with real-time recognition, summarization and visualization capabilities. MoM works without any cloud processing and does not require a network connection. Every processing step is local, even its speech recognition component, to address privacy concerns of meetings. MoM can be used to assisted writing a (summarized) protocol of a meeting, but may also help the hearing-impaired to follow a discussion. We address meeting-related issues, e.g. local vocabulary of an organization or company with active learning of G2P models and custom vocabulary extensions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d5b8514c03c470d7756bd51d06e3cfe3a7592a87",
    "semantic_title": "mom: minutes of meeting bot",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wilbrandt21_interspeech.html": {
    "title": "Articulatory Data Recorder: A Framework for Real-Time Articulatory Data Recording",
    "volume": "main",
    "abstract": "Articulatory data can be collected using numerous modalities, such as video, ultrasound, electromagnetic articulography, or palatographic techniques. Every measurement technique requires software to visualize the incoming data and export the data for further analysis. This has led to an increase of available recording software over the past decades, including properly maintained software in regular use but also many abandoned and dead projects. In this paper, we present a new framework for real-time, simultaneous recording of acoustic and articulatory data. With the release of the Articulatory Data Recorder, our aim is to provide the experimental phonetics and articulatory research community with a common framework that is simple to use and easy to extend. It is specifically designed to cover the most common use cases in experimental phonetics: Elicit speech utterances using text prompts and record simultaneous audio and articulatory data. By following the FURPS+-system, we offer a combination of high performance and a low barrier of entrance for enrollment of any new articulatory measurement technique. The current version already supports various palatographic measurement techniques in use at our institute and future work will incorporate feedback and feature requests from the community",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "70b4e9130a4bac1a41fe14292b12d2a775dec31e",
    "semantic_title": "articulatory data recorder: a framework for real-time articulatory data recording",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/codinafilba21_interspeech.html": {
    "title": "The INGENIOUS Multilingual Operations App",
    "volume": "main",
    "abstract": "This paper presents the integration of a speech-to-speech translation service into a Telegram bot as a part of the EU funded INGENIOUS project. The bot is thought as a multilingual communication channel where First Responders talk in their own language and receive other's messages in English. The Speech-to-Speech translation system is currently being adapted to the emergency domains, so it will correctly deal with emergency codes and geographical data",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6c59e78e54de5b940b21f899ae75c885ee9df8d1",
    "semantic_title": "the ingenious multilingual operations app",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rownicka21_interspeech.html": {
    "title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI",
    "volume": "main",
    "abstract": "We describe our approach to create and deliver a custom voice for a conversational AI use-case. More specifically, we provide a voice for a Digital Einstein character, to enable human-computer interaction within the digital conversation experience. To create the voice which fits the context well, we first design a voice character and we produce the recordings which correspond to the desired speech attributes. We then model the voice. Our solution utilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes and Parallel WaveGAN to generate the waveforms. The system supports a character input and gives a speech waveform at the output. We use a custom dictionary for selected words to ensure their proper pronunciation. Our proposed cloud architecture enables for fast voice delivery, making it possible to talk to the digital version of Albert Einstein in real-time",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7903d8e93dfc0b65d0d301bbe9aba421ddffe356",
    "semantic_title": "digital einstein experience: fast text-to-speech for conversational ai",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/geislinger21_interspeech.html": {
    "title": "Live Subtitling for BigBlueButton with Open-Source Software",
    "volume": "main",
    "abstract": "We present an open source plugin for live subtitling in the popular open source video conferencing software BigBlueButton. Our plugin decodes each speaker's audio stream separately and in parallel, thereby obliviating the need for speaker diarization and seamlessly handling overlapped talk. Any Kaldi-compatible nnet3 model can be used with our plugin and we demonstrate it using freely available TDNN-HMM-based ASR models for English and German. Our subtitles can be used as they are (e.g., in loud environments) or can form the basis for further NLP processes. Our tool can also simplify the collection of remotely recorded multi-party dialogue corpora",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "0c47b0f8e1a8e6274750b25b23fe702c9b893d62",
    "semantic_title": "live subtitling for bigbluebutton with open-source software",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nicmanis21_interspeech.html": {
    "title": "Expressive Latvian Speech Synthesis for Dialog Systems",
    "volume": "main",
    "abstract": "To fully enable spoken human-computer interaction, the text-to-speech (TTS) component of such a system must produce natural human-like speech and adjust the prosody according to the dialog context While the current publicly available TTS services can produce natural-sounding speech, they usually lack emotional expressiveness In this paper, we present an expressive speech synthesis prototype for the Latvian language. The prototype is integrated into our chatbot management system and enables bot designers to specify the stylistic information for each bot response, thus making the interaction with the chatbot more natural",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6b7980a8f22fe40f6dbc9c54c84ff9026dacaf39",
    "semantic_title": "expressive latvian speech synthesis for dialog systems",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kachare21_interspeech.html": {
    "title": "ViSTAFAE: A Visual Speech-Training Aid with Feedback of Articulatory Efforts",
    "volume": "main",
    "abstract": "An app is presented as a speech-training aid for providing visual feedback of articulatory efforts using information obtained from the utterances' audiovisual recording. It has two panels to enable comparison between the articulatory efforts of the learner and the teacher or a pre-recorded reference speaker. The visual feedback consists of a slow-motion animation of lateral vocal tract shape, level, and pitch, and time-aligned display of the frontal view of the speaker's face along with playback of the time-scaled speech signal. The app comprises a graphical user interface and modules for signal acquisition, analysis, and animation. It is developed using Python as a Windows-based app and may be accessed remotely through a web browser",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b7309f0d5444ea66ecb24bb9687ed1542079e74d",
    "semantic_title": "vistafae: a visual speech-training aid with feedback of articulatory efforts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/livescu21_interspeech.html": {
    "title": "Learning Speech Models from Multi-Modal Data",
    "volume": "main",
    "abstract": "",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "edc084769ce15733c033948cc050828ad12237e4",
    "semantic_title": "learning speech models from multi-modal data",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/elhilali21_interspeech.html": {
    "title": "Adaptive Listening to Everyday Soundscapes",
    "volume": "main",
    "abstract": "",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fe2ecaf07328112ffbfd40c932b6356c41262198",
    "semantic_title": "adaptive listening to everyday soundscapes",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ribeiro21b_interspeech.html": {
    "title": "Towards the Prediction of the Vocal Tract Shape from the Sequence of Phonemes to be Articulated",
    "volume": "main",
    "abstract": "In this work, we address the prediction of speech articulators' temporal geometric position from the sequence of phonemes to be articulated. We start from a set of real-time MRI sequences uttered by a female French speaker. The contours of five articulators were tracked automatically in each of the frames in the MRI video. Then, we explore the capacity of a bidirectional GRU to correctly predict each articulator's shape and position given the sequence of phonemes and their duration. We propose a 5-fold cross-validation experiment to evaluate the generalization capacity of the model. In a second experiment, we evaluate our model's data efficiency by reducing training data. We evaluate the point-to-point Euclidean distance and the Pearson's correlations along time between the predicted and the target shapes. We also evaluate produced shapes of the critical articulators of specific phonemes. We show that our model can achieve good results with minimal data, producing very realistic vocal tract shapes",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "a1e967c0208ddae56cfe81d14d74fbbf51c95bf7",
    "semantic_title": "towards the prediction of the vocal tract shape from the sequence of phonemes to be articulated",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/blandin21_interspeech.html": {
    "title": "Comparison of the Finite Element Method, the Multimodal Method and the Transmission-Line Model for the Computation of Vocal Tract Transfer Functions",
    "volume": "main",
    "abstract": "The acoustic properties of vocal tract are usually characterized by its transfer function from the input acoustic volume flow at the glottis to the radiated acoustic pressure. These transfer functions can be computed with acoustic models. Three-dimensional acoustic simulation are used to take into account accurately the three-dimensional vocal tract shape and to generate valid results even at high frequency. Finite element models, finite difference methods, three-dimensional waveguide meshes, or the multimodal method have been used for this purpose. However, these methods require much more computation time than simple one-dimensional models. Among these methods, the multimodal method can achieve the shortest computation times. However, all the previous implementations had limitations regarding the geometrical shapes and the losses. In this work, we evaluate a new implementation that intends to overcome these limitations. Vowel transfer functions obtained with this new implementation are compared with a transmission-line model and a proven, robust and highly accurate method: the finite element method. While the finite element method remains the most reliable, the multimodal method generates similar transfer functions in much less time. The transmission line model gives valid results for the four first resonances",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "e3ed21e44e371ad182a39e05ec001ccd21f6bbcd",
    "semantic_title": "comparison of the finite element method, the multimodal method and the transmission-line model for the computation of vocal tract transfer functions",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wagner21b_interspeech.html": {
    "title": "Effects of Time Pressure and Spontaneity on Phonotactic Innovations in German Dialogues",
    "volume": "main",
    "abstract": "Speech variation is often explained by speakers' balancing of production constraints (favoring phonetic reduction of high frequency, expected items) and listener orientation (favoring more canonical productions for low frequency, unexpected items). Less well understood are processes involving a structural reorganization of articulatory plans due to re-syllabification, e.g., resulting from processes involving massive reduction, epenthesis or metathesis. In this paper, we want to focus on two kinds of re-syllabifications: (1) within-system innovations, in which non-canonical forms occur, and (2) beyond-system inventions, which do not follow the phonotactic constraints of the language under consideration. We examine these processes in a corpus of spontaneous and read dyadic interactions of German, in which time pressure was controlled as an additional factor. Results show that spontaneity and time pressure will mostly lead to within-system innovations, favoring highly trained, unmarked articulatory routines, while minimizing information loss. However, occasionally speakers leave the beaten paths of highly trained articulatory routines, and invent novel phonotactic sequences which are at odds with the phonotactic grammar of German. Our results are discussed in the light of their implications for contemporary models of speech production",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c78dda2882f51949a0888b2e38011a17b5feb769",
    "semantic_title": "effects of time pressure and spontaneity on phonotactic innovations in german dialogues",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/medina21_interspeech.html": {
    "title": "Importance of Parasagittal Sensor Information in Tongue Motion Capture Through a Diphonic Analysis",
    "volume": "main",
    "abstract": "Our study examines the information obtained by adding two parasagittal sensors to the standard midsagittal configuration of an Electromagnetic Articulography (EMA) observation of lingual articulation. In this work, we present a large and phonetically balanced corpus obtained from an EMA recording session of a single English native speaker reading 1899 sentences from the Harvard and TIMIT corpora. According to a statistical analysis of the diphones produced during the recording session, the motion captured by the parasagittal sensors has a low correlation to the midsagittal sensors in the mediolateral direction. We perform a geometric analysis of the lateral tongue by the measure of its width and using a proxy of the tongue's curvature that is computed using the Menger curvature. To provide a better understanding of the tongue sensor motion we present dynamic visualizations of all diphones. Finally, we present a summary of the velocity information computed from the tongue sensor information",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6e0a4138a39ce8259c153b917c2c41dbd62f1c69",
    "semantic_title": "importance of parasagittal sensor information in tongue motion capture through a diphonic analysis",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/georges21_interspeech.html": {
    "title": "Learning Robust Speech Representation with an Articulatory-Regularized Variational Autoencoder",
    "volume": "main",
    "abstract": "It is increasingly considered that human speech perception and production both rely on articulatory representations. In this paper, we investigate whether this type of representation could improve the performances of a deep generative model (here a variational autoencoder) trained to encode and decode acoustic speech features. First we develop an articulatory model able to associate articulatory parameters describing the jaw, tongue, lips and velum configurations with vocal tract shapes and spectral features. Then we incorporate these articulatory parameters into a variational autoencoder applied on spectral features by using a regularization technique that constrains part of the latent space to represent articulatory trajectories. We show that this articulatory constraint improves model training by decreasing time to convergence and reconstruction loss at convergence, and yields better performance in a speech denoising task",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b1f5a9a4956ff03c47426f06de31173656a79ed1",
    "semantic_title": "learning robust speech representation with an articulatory-regularized variational autoencoder",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/weston21_interspeech.html": {
    "title": "Changes in Glottal Source Parameter Values with Light to Moderate Physical Load",
    "volume": "main",
    "abstract": "Engaging in everyday physical activities, like walking, initiates physiological processes that also affect parts of the body used for speech. However, it is currently unclear to what extent such activities affect phonatory processes, and in turn, the voice. The present exploratory study investigates how selected glottal source parameters are affected by light and moderate physical activity. Recordings of sustained vowel /a/ were obtained from 39 female speakers of German at rest, and during low-intensity and moderate-intensity cycling. Ten glottal source parameters thought to reflect different physiological states were investigated using VoiceSauce. Even during light activity, significant increases were found in f0, strength of excitation and H1, and a decrease in harmonics-to-noise ratio at higher frequencies. During moderate-intensity activity, significant effects were stronger and found for most parameters. However, considerable intra- and interspeaker variability was observed. These findings may be relevant for applications in automatic speaker-state recognition. They also underscore the importance of investigating individual-level responses to better understand stress–voice interactions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5e4556666388bff5743b8d2f9af6d6d353d18d08",
    "semantic_title": "changes in glottal source parameter values with light to moderate physical load",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vali21_interspeech.html": {
    "title": "End-to-End Optimized Multi-Stage Vector Quantization of Spectral Envelopes for Speech and Audio Coding",
    "volume": "main",
    "abstract": "Spectral envelope modeling is an instrumental part of speech and audio codecs, which can be used to enable efficient entropy coding of spectral components. Overall optimization of codecs, including envelope models, has however been difficult due to the complicated interactions between different modules of the codec. In this paper, we study an end-to-end optimization methodology to optimize all modules in a codec integrally with respect to each other while capturing all these complex interactions with a global loss function. For the quantization of the spectral envelope parameters with a fixed bitrate, we use multi-stage vector quantization which gives high quality, but yet has a computational complexity which can be realistically applied in embedded devices. The obtained results demonstrate benefits in terms of PESQ and PSNR in comparison to the 3GPP EVS, as well as our recently proposed PyAWNeS codecs",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "e1c5c15149a5355a3d1fd0c0b50fae63fb056fd6",
    "semantic_title": "end-to-end optimized multi-stage vector quantization of spectral envelopes for speech and audio coding",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nareddula21_interspeech.html": {
    "title": "Fusion-Net: Time-Frequency Information Fusion Y-Network for Speech Enhancement",
    "volume": "main",
    "abstract": "This paper proposes a deep learning-based densely connected Y-Net as an effective network architecture for the fusion of time and frequency domain loss functions for speech enhancement. The proposed architecture performs speech enhancement in the time domain while fusing information from the frequency domain. Y-network consists of an encoder branch followed by two decoder branches, where the first and second decoder loss functions enforce speech enhancement in time and frequency domains respectively. Each layer of the proposed network is formed with densely connected blocks comprising dilated and causal convolutions for significant feature collection and error backpropagation. The proposed model is trained on a publicly available data set of 28 speakers with 40 different noise conditions. The evaluations are performed on an independent, unseen test set of 2 speakers and 20 different noise conditions. The results from the proposed method are compared with five state-of-the-art methods using various metrics. The proposed method has resulted in an overall perceptual evaluation of speech quality of 3.4. It has outperformed the existing methods by a significant margin in terms of all the evaluation metrics",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "0b4f6feb0c435634f371ac2db1d8de373097798b",
    "semantic_title": "fusion-net: time-frequency information fusion y-network for speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/marcinek21_interspeech.html": {
    "title": "N-MTTL SI Model: Non-Intrusive Multi-Task Transfer Learning-Based Speech Intelligibility Prediction Model with Scenery Classification",
    "volume": "main",
    "abstract": "The application of speech enhancement algorithms for hearing aids may not always be beneficial to increasing speech intelligibility. Therefore, a prior environment classification could be important. However, previous speech intelligibility models do not provide any additional information regarding the reason for a decrease in speech intelligibility. We propose a unique non-intrusive multi-task transfer learning-based speech intelligibility prediction model with scenery classification (N-MTTL SI model). The solution combines a Mel-spectrogram analysis of the degraded speech signal with transfer learning and multi-task learning to provide simultaneous speech intelligibility prediction (task 1) and scenery classification of ten real-world noise conditions (task 2). The model utilises a pre-trained ResNet architecture as an encoder for feature extraction. The prediction accuracy of the N-MTTL SI model for both tasks is high. Specifically, RMSE of speech intelligibility predictions for seen and unseen conditions is 3.76% and 4.06%. The classification accuracy is 98%. In addition, the proposed solution demonstrates the potential of using pre-trained deep learning models in the domain of speech intelligibility prediction",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d06628320a377e4520678dfeee980fecddecedf0",
    "semantic_title": "n-mttl si model: non-intrusive multi-task transfer learning-based speech intelligibility prediction model with scenery classification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xia21b_interspeech.html": {
    "title": "Temporal Context in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "We investigate the importance of temporal context for speech emotion recognition (SER). Two SER systems trained on traditional and learned features, respectively, are developed to predict categorical labels of emotion. For traditional acoustical features, we study the combination of filterbank features and prosodic features and the impact on SER when the temporal context of these features is expanded by learnable spectro-temporal receptive fields (STRFs). Experiments show that the system trained on learnable STRFs outperforms other reported systems evaluated with a similar setup. We also demonstrate that the wav2vec features, pretrained with long temporal context, are superior to traditional features. We then introduce a novel segment-based learning objective to constrain our classifier to extract local emotion features from the large temporal context. Combined with the learning objective and fine-tuning strategy, our top-line system using wav2vec features reaches state-of-the-art performance on the IEMOCAP dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "efb44818fb6ded05d7787da4f5f66b2c9672beb7",
    "semantic_title": "temporal context in speech emotion recognition",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21j_interspeech.html": {
    "title": "Learning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition is a challenging task because the emotion expression is complex, multimodal and fine-grained. In this paper, we propose a novel multimodal deep learning approach to perform fine-grained emotion recognition from real-life speeches. We design a temporal alignment mean-max pooling mechanism to capture the subtle and fine-grained emotions implied in every utterance. In addition, we propose a cross modality excitement module to conduct sample-specific adjustment on cross modality embeddings and adaptively recalibrate the corresponding values by its aligned latent features from the other modality. Our proposed model is evaluated on two well-known real-world speech emotion recognition datasets. The results demonstrate that our approach is superior on the prediction tasks for multimodal speech utterances, and it outperforms a wide range of baselines in terms of prediction accuracy. Furthermore, we conduct detailed ablation studies to show that our temporal alignment mean-max pooling mechanism and cross modality excitement significantly contribute to the promising results. In order to encourage the research reproducibility, we make the code publicly available",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "63ebb3cb79fe9d91f52b4ab24c4280ee567173a6",
    "semantic_title": "learning fine-grained cross modality excitement for speech emotion recognition",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vaaras21_interspeech.html": {
    "title": "Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit",
    "volume": "main",
    "abstract": "Researchers have recently started to study how the emotional speech heard by young infants can affect their developmental outcomes. As a part of this research, hundreds of hours of daylong recordings from preterm infants' audio environments were collected from two hospitals in Finland and Estonia in the context of so-called APPLE study. In order to analyze the emotional content of speech in such a massive dataset, an automatic speech emotion recognition (SER) system is required. However, there are no emotion labels or existing in-domain SER systems to be used for this purpose. In this paper, we introduce this initially unannotated large-scale real-world audio dataset and describe the development of a functional SER system for the Finnish subset of the data. We explore the effectiveness of alternative state-of-the-art techniques to deploy a SER system to a new domain, comparing cross-corpus generalization, WGAN-based domain adaptation, and active learning in the task. As a result, we show that the best-performing models are able to achieve a classification performance of 73.4% unweighted average recall (UAR) and 73.2% UAR for a binary classification for valence and arousal, respectively. The results also show that active learning achieves the most consistent performance compared to the two alternatives",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c6d1851c20831469cd1f6aa830ad39f14175adf0",
    "semantic_title": "automatic analysis of the emotional content of speech in daylong child-centered recordings from a neonatal intensive care unit",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qian21_interspeech.html": {
    "title": "Multimodal Sentiment Analysis with Temporal Modality Attention",
    "volume": "main",
    "abstract": "Multimodal sentiment analysis is an important research that involves integrating information from multiple modalities to identify a speaker underlying attitude. The core challenge is to model cross-modal interactions which span across both the different modalities and time. Although great progress has been made, the existing methods are still not sufficient for modeling cross-modal interactions. Inspired by previous research in cognitive neuroscience that humans perceive intentions through focusing on different modalities over time, in this paper we propose a novel attention mechanism called Temporal Modality Attention (TMA) to simulate this process. Cross-modal interactions are modeled using this human-like TMA mechanism which focuses on specific modalities dynamically as recurrent modeling proceed. To verify the effectiveness of TMA, we conduct comprehensive experiments on multiple benchmark datasets for multimodal sentiment analysis. The results show a consistently significant improvement compared to the baseline models",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "dc117daa6d995cf769c51c13602c6a2d61e0515e",
    "semantic_title": "multimodal sentiment analysis with temporal modality attention",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/t21_interspeech.html": {
    "title": "Stochastic Process Regression for Cross-Cultural Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this work, we pose continuous apparent emotion recognition from speech as a problem of learning distributions of functions, and do so using Stochastic Processes Regression. We presume that the relation between speech signals and their corresponding emotion labels is governed by some underlying stochastic process, in contrast to existing speech emotion recognition methods that are mostly based on deterministic regression models (static or recurrent). We treat each training sequence as an instance of the underlying stochastic process which we aim to discover using a neural latent variable model, which approximates the distribution of functions with a stochastic latent variable using an encoder-decoder composition: the encoder infers the distribution over the latent variable, which the decoder uses to predict the distribution of output emotion labels. To this end, we build on the previously proposed Neural Processes theory by using (a). noisy label predictions of a backbone instead of ground truth labels for latent variable inference and (b). recurrent encoder-decoder models to alleviate the effect of commonly encountered temporal misalignment between audio features and emotion labels due to annotator reaction lag. We validated our method on AVEC'19 cross-cultural emotion recognition dataset, achieving state-of-the-art results",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "62d686521936efcb89b43d1ee479fc8d1a3cfffd",
    "semantic_title": "stochastic process regression for cross-cultural speech emotion recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21k_interspeech.html": {
    "title": "Acted vs. Improvised: Domain Adaptation for Elicitation Approaches in Audio-Visual Emotion Recognition",
    "volume": "main",
    "abstract": "Key challenges in developing generalized automatic emotion recognition systems include scarcity of labeled data and lack of gold-standard references. Even for the cues that are labeled as the same emotion category, the variability of associated expressions can be high depending on the elicitation context e.g., emotion elicited during improvised conversations vs. acted sessions with predefined scripts. In this work, we regard the emotion elicitation approach as domain knowledge, and explore domain transfer learning techniques on emotional utterances collected under different emotion elicitation approaches, particularly with limited labeled target samples. Our emotion recognition model combines the gradient reversal technique with an entropy loss function as well as the softlabel loss, and the experiment results show that domain transfer learning methods can be employed to alleviate the domain mismatch between different elicitation approaches. Our work provides new insights into emotion data collection, particularly the impact of its elicitation strategies, and the importance of domain adaptation in emotion recognition aiming for generalized systems",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c23acd41de6d933ea28ca7d5048ef0e8f67eeb11",
    "semantic_title": "acted vs. improvised: domain adaptation for elicitation approaches in audio-visual emotion recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pepino21_interspeech.html": {
    "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
    "volume": "main",
    "abstract": "Emotion recognition datasets are relatively small, making the use of deep learning techniques challenging. In this work, we propose a transfer learning method for speech emotion recognition (SER) where features extracted from pre-trained wav2vec 2.0 models are used as input to shallow neural networks to recognize emotions from speech. We propose a way to combine the output of several layers from the pre-trained model, producing richer speech representations than the model's output alone. We evaluate the proposed approaches on two standard emotion databases, IEMOCAP and RAVDESS, and compare different feature extraction techniques using two wav2vec 2.0 models: a generic one, and one finetuned for speech recognition. We also experiment with different shallow architectures for our speech emotion recognition model, and report baseline results using traditional features. Finally, we show that our best performing models have better average recall than previous approaches that use deep neural networks trained on spectrograms and waveforms or shallow neural networks trained on features extracted from wav2vec 1.0",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3cef12c413453f31ffa1582896bbcad82aa5021f",
    "semantic_title": "emotion recognition from speech using wav2vec 2.0 embeddings",
    "citation_count": 170,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21k_interspeech.html": {
    "title": "Graph Isomorphism Network for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Previous deep learning approaches such as Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) have been broadly used in speech emotion recognition (SER). In these approaches, speech signals are generally modeled in the Euclidean space. In this paper, a novel SER model (LSTM-GIN) is proposed, which applies Graph Isomorphism Network (GIN) on LSTM outputs for global emotion modeling in the non-Euclidean space. In our LSTM-GIN model, speech signals are represented as graph-structured data so that we can better extract global feature representation. The deep frame-level features generated from the bidirectional LSTM are converted into an undirected graph with nodes represented by frame-level features and connections defined according to temporal relations between speech frames. GIN is adopted to classify the graph representations of utterances, as it is proved of excellent discriminative power in comparative experiments. We conduct experiments on the IEMOCAP dataset, and the results show that our proposed LSTM-GIN model surpasses other recent graph-based models and deep learning models by achieving 64.65% of weighted accuracy (WA) and 65.53% of unweighted accuracy (UA)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c2a02701af77318db0e4d372c49129c34f8ddae7",
    "semantic_title": "graph isomorphism network for speech emotion recognition",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kumawat21_interspeech.html": {
    "title": "Applying TDNN Architectures for Analyzing Duration Dependencies on Speech Emotion Recognition",
    "volume": "main",
    "abstract": "We have analyzed the Time Delay Neural Network (TDNN) based architectures for speech emotion classification. TDNN models efficiently capture the temporal information and provide an utterance level prediction. Emotions are dynamic in nature and require temporal context for reliable prediction. In our work, we have applied the TDNN based x-vector and emphasized channel attention, propagation & aggregation based TDNN (ECAPA-TDNN) architectures for speech emotion identification with RAVDESS, Emo-DB, and IEMOCAP databases. The results show that the TDNN architectures are very efficient for predicting emotion classes and ECAPA-TDNN outperforms the TDNN based x-vector architecture. Next, we investigated the performance of ECAPA-TDNN with various training chunk durations and test utterance durations. We have identified that in spite of very promising emotion recognition performance the TDNN models have a strong training chunk duration-based bias. Earlier research work revealed that individual emotion class accuracy depends largely on the test utterance duration. Most of these studies were based on frame level emotions predictions. However, utterance level based emotion recognition is relatively less explored. The results show that even with the TDNN models, the accuracy of the different emotion classes is dependent on the utterance duration",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d9a8a3c7467da2122aa982ee19c5e528369c7da1",
    "semantic_title": "applying tdnn architectures for analyzing duration dependencies on speech emotion recognition",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/keesing21_interspeech.html": {
    "title": "Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech",
    "volume": "main",
    "abstract": "Many features have been proposed for use in speech emotion recognition, from signal processing features to bag-of-audio-words (BoAW) models to abstract neural representations. Some of these feature types have not been directly compared across a large number of speech corpora to determine performance differences. We propose a full factorial design and to compare speech processing features, BoAW and neural representations on 17 emotional speech datasets. We measure the performance of features in a categorical emotion classification problem for each dataset, using speaker-independent cross-validation with diverse classifiers. Results show statistically significant differences between features and between classifiers, with large effect sizes between features. In particular, standard acoustic feature sets still perform competitively to neural representations, while neural representations have a larger range of performance, and BoAW features lie in the middle. The best and worst neural representations were wav2veq and VGGish, respectively, with wav2vec performing best out of all tested features. These results indicate that standard acoustic feature sets are still very useful baselines for emotional classification, but high quality neural speech representations can be better",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7c4662697f348b919871daa9c1f0f084bc01524e",
    "semantic_title": "acoustic features and neural representations for categorical emotion recognition from speech",
    "citation_count": 25,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shon21_interspeech.html": {
    "title": "Leveraging Pre-Trained Language Model for Speech Sentiment Analysis",
    "volume": "main",
    "abstract": "In this paper, we explore the use of pre-trained language models to learn sentiment information of written texts for speech sentiment analysis. First, we investigate how useful a pre-trained language model would be in a 2-step pipeline approach employing Automatic Speech Recognition (ASR) and transcripts-based sentiment analysis separately. Second, we propose a pseudo label-based semi-supervised training strategy using a language model on an end-to-end speech sentiment approach to take advantage of a large, but unlabeled speech dataset for training. Although spoken and written texts have different linguistic characteristics, they can complement each other in understanding sentiment. Therefore, the proposed system can not only model acoustic characteristics to bear sentiment-specific information in speech signals, but learn latent information to carry sentiments in the text representation. In these experiments, we demonstrate the proposed approaches improve F1 scores consistently compared to systems without a language model. Moreover, we also show that the proposed framework can reduce 65% of human supervision by leveraging a large amount of data without human sentiment annotation and boost performance in a low-resource condition where the human sentiment annotation is not available enough",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "42c3c50b8e368ee2e1b52d010b6c53b3d732770c",
    "semantic_title": "leveraging pre-trained language model for speech sentiment analysis",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hou21b_interspeech.html": {
    "title": "Cross-Domain Speech Recognition with Unsupervised Character-Level Distribution Matching",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (ASR) can achieve promising performance with large-scale training data. However, it is known that domain mismatch between training and testing data often leads to a degradation of recognition accuracy. In this work, we focus on the unsupervised domain adaptation for ASR and propose CMatch, a Character-level distribution matching method to perform fine-grained adaptation between each character in two domains. First, to obtain labels for the features belonging to each character, we achieve frame-level label assignment using the Connectionist Temporal Classification (CTC) pseudo labels. Then, we match the character-level distributions using Maximum Mean Discrepancy. We train our algorithm using the self-training technique. Experiments on the Libri-Adapt dataset show that our proposed approach achieves 14.39% and 16.50% relative Word Error Rate (WER) reduction on both cross-device and cross-environment ASR. We also comprehensively analyze the different strategies for frame-level label assignment and Transformer adaptations",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "259dbaea3d46478e949bcbb288a86542180273af",
    "semantic_title": "cross-domain speech recognition with unsupervised character-level distribution matching",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kanda21_interspeech.html": {
    "title": "Large-Scale Pre-Training of End-to-End Multi-Talker ASR for Meeting Transcription with Single Distant Microphone",
    "volume": "main",
    "abstract": "Transcribing meetings containing overlapped speech with only a single distant microphone (SDM) has been one of the most challenging problems for automatic speech recognition (ASR). While various approaches have been proposed, all previous studies on the monaural overlapped speech recognition problem were based on either simulation data or small-scale real data. In this paper, we extensively investigate a two-step approach where we first pre-train a serialized output training (SOT)-based multi-talker ASR by using large-scale simulation data and then fine-tune the model with a small amount of real meeting data. Experiments are conducted by utilizing 75 thousand (K) hours of our internal single-talker recording to simulate a total of 900K hours of multi-talker audio segments for supervised pre-training. With fine-tuning on the 70 hours of the AMI-SDM training data, our SOT ASR model achieves a word error rate (WER) of 21.2% for the AMI-SDM evaluation set while automatically counting speakers in each test segment. This result is not only significantly better than the previous state-of-the-art WER of 36.4% with oracle utterance boundary information but also better than a result by a similarly fine-tuned single-talker ASR model applied to beamformed audio",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d3c572545a006dfdc3c518e7dc04e2589597a718",
    "semantic_title": "large-scale pre-training of end-to-end multi-talker asr for meeting transcription with single distant microphone",
    "citation_count": 27,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21b_interspeech.html": {
    "title": "On Minimum Word Error Rate Training of the Hybrid Autoregressive Transducer",
    "volume": "main",
    "abstract": "Hybrid Autoregressive Transducer (HAT) is a recently proposed end-to-end acoustic model that extends the standard Recurrent Neural Network Transducer (RNN-T) for the purpose of the external language model (LM) fusion. In HAT, the blank probability and the label probability are estimated using two separate probability distributions, which provides a more accurate solution for internal LM score estimation, and thus works better when combining with an external LM. Previous work mainly focuses on HAT model training with the negative log-likelihood loss, while in this paper, we study the minimum word error rate (MWER) training of HAT — a criterion that is closer to the evaluation metric for speech recognition, and has been successfully applied to other types of end-to-end models such as sequence-to-sequence (S2S) and RNN-T models. From experiments with around 30,000 hours of training data, we show that MWER training can improve the accuracy of HAT models, while at the same time, improving the robustness of the model against the decoding hyper-parameters such as length normalization and decoding beam during inference",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "579587a3ca8280f75a512981c8f17db9503ab626",
    "semantic_title": "on minimum word error rate training of the hybrid autoregressive transducer",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21j_interspeech.html": {
    "title": "Reducing Streaming ASR Model Delay with Self Alignment",
    "volume": "main",
    "abstract": "Reducing prediction delay for streaming end-to-end ASR models with minimal performance regression is a challenging problem. Constrained alignment is a well-known existing approach that penalizes predicted word boundaries using external low-latency acoustic models. On the contrary, recently proposed FastEmit is a sequence-level delay regularization scheme encouraging vocabulary tokens over blanks without any reference alignments. Although all these schemes are successful in reducing delay, ASR word error rate (WER) often severely degrades after applying these delay constraining schemes. In this paper, we propose a novel delay constraining method, named self alignment. Self alignment does not require external alignment models. Instead, it utilizes Viterbi forced-alignments from the trained model to find the lower latency alignment direction. From LibriSpeech evaluation, self alignment outperformed existing schemes: 25% and 56% less delay compared to FastEmit and constrained alignment at the similar word error rate. For Voice Search evaluation, 12% and 25% delay reductions were achieved compared to FastEmit and constrained alignment with more than 2% WER improvements",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d30aa298b523d4f4eadfb53ff0a9e29b88d27e23",
    "semantic_title": "reducing streaming asr model delay with self alignment",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/diwan21b_interspeech.html": {
    "title": "Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages",
    "volume": "main",
    "abstract": "This work presents a seemingly simple but effective technique to improve low-resource ASR systems for phonetic languages. By identifying sets of acoustically similar graphemes in these languages, we first reduce the output alphabet of the ASR system using linguistically meaningful reductions and then reconstruct the original alphabet using a standalone module. We demonstrate that this lessens the burden and improves the performance of low-resource end-to-end ASR systems (because only reduced-alphabet predictions are needed) and that it is possible to design a very simple but effective reconstruction module that recovers sequences in the original alphabet from sequences in the reduced alphabet. We present a finite state transducer-based reconstruction module that operates on the 1-best ASR hypothesis in the reduced alphabet. We demonstrate the efficacy of our proposed technique using ASR systems for two Indian languages, Gujarati and Telugu. With access to only 10 hrs of speech data, we obtain relative WER reductions of up to 7% compared to systems that do not use any reduction",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3afb6565db119326f0d29f8cf4d3c8146baf1e30",
    "semantic_title": "reduce and reconstruct: asr for low-resource phonetic languages",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fukuda21_interspeech.html": {
    "title": "Knowledge Distillation Based Training of Universal ASR Source Models for Cross-Lingual Transfer",
    "volume": "main",
    "abstract": "In this paper we introduce a novel knowledge distillation based framework for training universal source models. In our proposed approach for automatic speech recognition (ASR), multilingual source models are first trained using multiple language-dependent resources before being used to initialize language specific target models in low resource settings. For the proposed source models to be effective in cross-lingual transfer to novel target languages, the training framework encourages the models to perform accurate universal phone classification while ignoring any language-dependent characteristics present in the training data set. These two goals are achieved by applying knowledge distillation to improve the models' universal phone classification performance along with a shuffling mechanism that alleviates any language specific dependencies that might be learned. The benefits of this proposed technique are demonstrated in several practical settings, where either large amounts or only limited quantities of unbalanced multilingual data resources are available for source model creation. Compared to a conventional knowledge transfer learning method, the proposed approaches achieve a relative WER reduction of 8–10% in streaming ASR settings for various low resource target languages",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "1074e038162f5382b8776387526e2e28d5b2fe13",
    "semantic_title": "knowledge distillation based training of universal asr source models for cross-lingual transfer",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ray21_interspeech.html": {
    "title": "Listen with Intent: Improving Speech Recognition with Audio-to-Intent Front-End",
    "volume": "main",
    "abstract": "Comprehending the overall intent of an utterance helps a listener recognize the individual words spoken. Inspired by this fact, we perform a novel study of the impact of explicitly incorporating intent representations as additional information to improve a recurrent neural network-transducer (RNN-T) based automatic speech recognition (ASR) system. An audio-to-intent (A2I) model encodes the intent of the utterance in the form of embeddings or posteriors, and these are used as auxiliary inputs for RNN-T training and inference. Experimenting with a 50k-hour far-field English speech corpus, this study shows that when running the system in mode, where intent representation is extracted from the entire utterance and then used to bias streaming RNN-T search from the start, it provides a 5.56% relative word error rate reduction (WERR). On the other hand, a system using per-frame intent posteriors as extra inputs for the RNN-T ASR system yields a 3.33% relative WERR. A further detailed analysis of the streaming system indicates that our proposed method brings especially good gain on media-playing related intents (e.g. 9.12% relative WERR on PlayMusicIntent)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2bd2bda8800c598adfd57d2a2aaf41bc5d829473",
    "semantic_title": "listen with intent: improving speech recognition with audio-to-intent front-end",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21c_interspeech.html": {
    "title": "Exploring Targeted Universal Adversarial Perturbations to End-to-End ASR Models",
    "volume": "main",
    "abstract": "Although end-to-end automatic speech recognition (e2e ASR) models are widely deployed in many applications, there have been very few studies to understand models' robustness against adversarial perturbations. In this paper, we explore whether a targeted universal perturbation vector exists for e2e ASR models. Our goal is to find perturbations that can mislead the models to predict the given targeted transcript such as \"thank you\" or empty string on any input utterance. We study two different attacks, namely additive and prepending perturbations, and their performances on the state-of-the-art LAS, CTC and RNN-T models. We find that LAS is the most vulnerable to perturbations among the three models. RNN-T is more robust against additive perturbations, especially on long utterances. And CTC is robust against both additive and prepending perturbations. To attack RNN-T, we find prepending perturbation is more effective than the additive perturbation, and can mislead the models to predict the same short target on utterances of arbitrary length",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2223e236e23d01bb88c519a89c07fee13f50a6be",
    "semantic_title": "exploring targeted universal adversarial perturbations to end-to-end asr models",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/delrio21_interspeech.html": {
    "title": "Earnings-21: A Practical Benchmark for ASR in the Wild",
    "volume": "main",
    "abstract": "Commonly used speech corpora inadequately challenge academic and commercial ASR systems. In particular, speech corpora lack metadata needed for detailed analysis and WER measurement. In response, we present , a 39-hour corpus of earnings calls containing entity-dense speech from nine different financial sectors. This corpus is intended to benchmark ASR systems in the wild with special attention towards named entity recognition. We benchmark four commercial ASR models, two internal models built with open-source tools, and an open-source LibriSpeech model and discuss their differences in performance on Using our recently released tool, we provide a candid analysis of each model's recognition capabilities under different partitions. Our analysis finds that ASR accuracy for certain NER categories is poor, presenting a significant impediment to transcript comprehension and usage bridges academic and commercial ASR system evaluation and enables further research on entity modeling and WER on real world audio",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5f4d69a483afe38a44ce53909a24a5535dcbf638",
    "semantic_title": "earnings-21: a practical benchmark for asr in the wild",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sun21c_interspeech.html": {
    "title": "Improving Multilingual Transformer Transducer Models by Reducing Language Confusions",
    "volume": "main",
    "abstract": "In end-to-end multilingual speech recognition, the hypotheses in one language could include word tokens from other languages. Language confusions happen even more frequently when language identifier (LID) is not present during inference. In this paper, we explore to reduce language confusions without using LID in model inference by creating models with multiple output heads and use sequence probability to select the correct head for output hypotheses. We propose head grouping to merge several language outputs into one head to save runtime cost. Head groups are decided by the distances among language clusters learned through language embedding vectors to separate confusable languages apart. We further propose prediction network sharing for languages from the same family. By jointly applying head grouping and prediction network sharing, training data from the same family languages is better shared while confusable languages are divided into different heads to reduce language confusions. Our experiments demonstrate that our multilingual transformer transducer models based on multi-head outputs achieve on average 7.8% and 10.9% relative word error rate reductions without LID being used in inference from one-head baseline model with affordably increased runtime cost on 10 European languages",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8e22e5bbe8803315e30236a7f5036161737e6c0b",
    "semantic_title": "improving multilingual transformer transducer models by reducing language confusions",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ali21b_interspeech.html": {
    "title": "Arabic Code-Switching Speech Recognition Using Monolingual Data",
    "volume": "main",
    "abstract": "Code-switching in automatic speech recognition (ASR) is an important challenge due to globalization. Recent research in multilingual ASR shows potential improvement over monolingual systems. We study key issues related to multilingual modeling for ASR through a series of large-scale ASR experiments. Our innovative framework deploys a multi-graph approach in the weighted finite state transducers (WFST) framework. We compare our WFST decoding strategies with a transformer sequence to sequence system trained on the same data. Given a code-switching scenario between Arabic and English languages, our results show that the WFST decoding approaches were more suitable for the intersentential code-switching datasets. In addition, the transformer system performed better for intrasentential code-switching task. With this study, we release an artificially generated development and test sets, along with ecological code-switching test set, to benchmark the ASR performance",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "a13fc0e6b5a3bbc04b870dd9412d372759823c27",
    "semantic_title": "arabic code-switching speech recognition using monolingual data",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/eisenberg21_interspeech.html": {
    "title": "Online Blind Audio Source Separation Using Recursive Expectation-Maximization",
    "volume": "main",
    "abstract": "The challenging problem of online multi-microphone blind audio source separation (BASS) in noisy environment is addressed in this paper. We present a sequential, non-iterative, algorithm based on the recursive EM (REM) framework. In the proposed algorithm, the compete-data, which constitutes the separated sources and residual noise, is estimated in the E-step by applying a multichannel Wiener filter (MCWF); and the corresponding parameters, comprised of acoustic transfer functions (ATFs) relating the sources and the microphones and power spectral densities (PSDs) of the desired sources, are sequentially estimated in the M-step. The separated speech signals are further enhanced using matched-filter beamformers. The performance of the algorithm is demonstrated in terms of the separation capabilities, the resulting speech intelligibility and the ability to track the direction of arrival (DOA) of the moving sources",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f1a773b83cebce46b41793890c321ca91dd7faaa",
    "semantic_title": "online blind audio source separation using recursive expectation-maximization",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luo21e_interspeech.html": {
    "title": "Empirical Analysis of Generalized Iterative Speech Separation Networks",
    "volume": "main",
    "abstract": "Although most existing speech separation networks are designed as a one-pass pipeline where the sources are directly estimated from the mixture, multi-pass or iterative pipelines have been shown to be effective by designing multiple rounds of separation and utilizing separation outputs from a previous iteration as additional inputs for the next iteration. Moreover, such iterative separation pipeline can also be extended to a more general framework where a training objective designed to minimize the discrepancy between the estimated and target sources is applied to different parts of the network. In this paper, we empirically investigate the effect of such generalized iterative separation pipeline by adjusting its configuration in multiple aspects in both training and inference phases. For the training phase, we compare the separation performance of both time-domain and frequency-domain networks with different numbers of iterations following the recent discussions on the model architecture organizations. We also evaluate the effect of parameter sharing across iterations and the necessity of additional training objectives. For the inference phase, we measure the separation performance of various numbers of iterations. Our results show that iterative speech separation is a promising direction and deserves more in-depth analysis and exploration",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "df279e5381e8417fbbb23c1a1b8eaa4198ea9dae",
    "semantic_title": "empirical analysis of generalized iterative speech separation networks",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/neumann21_interspeech.html": {
    "title": "Graph-PIT: Generalized Permutation Invariant Training for Continuous Separation of Arbitrary Numbers of Speakers",
    "volume": "main",
    "abstract": "Automatic transcription of meetings requires handling of overlapped speech, which calls for continuous speech separation (CSS) systems. The uPIT criterion was proposed for utterance-level separation with neural networks and introduces the constraint that the total number of speakers must not exceed the number of output channels. When processing meeting-like data in a segment-wise manner, i.e., by separating overlapping segments independently and stitching adjacent segments to continuous output streams, this constraint has to be fulfilled for any segment. In this contribution, we show that this constraint can be significantly relaxed. We propose a novel graph-based PIT criterion, which casts the assignment of utterances to output channels in a graph coloring problem. It only requires that the number of concurrently active speakers must not exceed the number of output channels. As a consequence, the system can process an arbitrary number of speakers and arbitrarily long segments and thus can handle more diverse scenarios. Further, the stitching algorithm for obtaining a consistent output order in neighboring segments is of less importance and can even be eliminated completely, not the least reducing the computational effort. Experiments on meeting-style WSJ data show improvements in recognition performance over using the uPIT criterion",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "07a2a9c44cc90d75cc7e21bdf91f0b48db3d93df",
    "semantic_title": "graph-pit: generalized permutation invariant training for continuous separation of arbitrary numbers of speakers",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21v_interspeech.html": {
    "title": "Teacher-Student MixIT for Unsupervised and Semi-Supervised Speech Separation",
    "volume": "main",
    "abstract": "In this paper, we introduce a novel semi-supervised learning framework for end-to-end speech separation. The proposed method first uses mixtures of unseparated sources and the mixture invariant training (MixIT) criterion to train a teacher model. The teacher model then estimates separated sources that are used to train a student model with standard permutation invariant training (PIT). The student model can be fine-tuned with supervised data, i.e., paired artificial mixtures and clean speech sources, and further improved via model distillation. Experiments with single and multi channel mixtures show that the teacher-student training resolves the over-separation problem observed in the original MixIT method. Further, the semi-supervised performance is comparable to a fully-supervised separation system trained using ten times the amount of supervised data",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "278d90715cc60c4bbf04413830a5c8236aef54b2",
    "semantic_title": "teacher-student mixit for unsupervised and semi-supervised speech separation",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/delcroix21_interspeech.html": {
    "title": "Few-Shot Learning of New Sound Classes for Target Sound Extraction",
    "volume": "main",
    "abstract": "Target sound extraction consists of extracting the sound of a target acoustic event (AE) class from a mixture of AE sounds. It can be realized using a neural network that extracts the target sound conditioned on a 1-hot vector that represents the desired AE class. With this approach, embedding vectors associated with the AE classes are directly optimized for the extraction of sound classes seen during training. However, it is not easy to extend this framework to new AE classes, i.e. unseen during training. Recently, speech, music, or AE sound extraction based on enrollment audio of the desired sound offers the potential of extracting any target sound in a mixture given only a short audio signal of a similar sound. In this work, we propose combining 1-hot- and enrollment-based target sound extraction, allowing optimal performance for seen AE classes and simple extension to new classes. In experiments with synthesized sound mixtures generated with the Freesound Dataset (FSD) datasets, we demonstrate the benefit of the combined framework for both seen and new AE classes. Besides, we also propose adapting the embedding vectors obtained from a few enrollment audio samples (few-shot) to further improve performance on new classes",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2e6b5f1fd3bd2b6624b28960dba9aa15c45f8b1c",
    "semantic_title": "few-shot learning of new sound classes for target sound extraction",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21e_interspeech.html": {
    "title": "Binaural Speech Separation of Moving Speakers With Preserved Spatial Cues",
    "volume": "main",
    "abstract": "Binaural speech separation algorithms designed for augmented hearing technologies need to both improve the signal-to-noise ratio of individual speakers and preserve their perceived location in space. The majority of binaural speech separation methods assume nonmoving speakers. As a result, their application to real-world scenarios with freely moving speakers requires block-wise adaptation which relies on short-term contextual information and limits their performance. In this study, we propose an alternative approach for utterance-level source separation with moving speakers and in reverberant conditions. Our model makes use of spectral and spatial features of speakers in a larger context compared to the block-wise adaption methods. The model can implicitly track speakers within the utterance without the need for explicit tracking modules. Experimental results on simulated moving multitalker speech show that the proposed method can significantly outperform block-wise adaptation methods in both separation performance and preserving the interaural cues across multiple conditions, which makes it suitable for real-world augmented hearing applications",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2dec1eef0c20b696f873615e9b46e5a76a713136",
    "semantic_title": "binaural speech separation of moving speakers with preserved spatial cues",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hu21_interspeech.html": {
    "title": "AvaTr: One-Shot Speaker Extraction with Transformers",
    "volume": "main",
    "abstract": "To extract the voice of a target speaker when mixed with a variety of other sounds, such as white and ambient noises or the voices of interfering speakers, we extend the Transformer network [1] to attend the most relevant information with respect to the target speaker given the characteristics of his or her voices as a form of contextual information. The idea has a natural interpretation in terms of the [2]. Specifically, we propose two models to incorporate the voice characteristics in Transformer based on different insights of where the feature selection should take place. Both models yield excellent performance, on par or better than published state-of-the-art models on the task, including separating speech of novel speakers not seen during training",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "04852ca0310453311592c28ce6ed7e7e90ae2fe8",
    "semantic_title": "avatr: one-shot speaker extraction with transformers",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sarkar21_interspeech.html": {
    "title": "Vocal Harmony Separation Using Time-Domain Neural Networks",
    "volume": "main",
    "abstract": "Polyphonic vocal recordings are an inherently challenging source separation task due to the melodic structure of the vocal parts and unique timbre of its constituents. In this work we utilise a time-domain neural network architecture re-purposed from speech separation research and modify it to separate mixtures at a high sampling rate. We use four-part (soprano, alto, tenor and bass) recordings of Bach Chorales and Barbershop Quartets for our experiments. Unlike current deep learning based choral separation models where the training objective is to separate constituent sources based on their class, we train our model using a permutation invariant objective. Using this we achieve state-of-the-art results for choral music separation. We introduce a novel method to estimate harmonic overlap between sung musical notes as a measure of task complexity. We also present an analysis of the impact of randomised mixing, input lengths and filterbank lengths for our task. Our results show a moderate negative correlation between the harmonic overlap of the target sources and source separation performance. We report that training our models with randomly mixed musically-incoherent mixtures drastically reduces the performance of vocal harmony separation as it decreases the average harmonic overlap presented during training",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "271b921ddf84c89ad0b4452b13d20c636fef5f43",
    "semantic_title": "vocal harmony separation using time-domain neural networks",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/maciejewski21_interspeech.html": {
    "title": "Speaker Verification-Based Evaluation of Single-Channel Speech Separation",
    "volume": "main",
    "abstract": "Speech enhancement techniques typically focus on intrinsic metrics of signal quality. The overwhelming majority of deep learning-based single-channel speech separation studies, for instance, have relied on a single class of metrics to evaluate the systems by. These metrics, usually variants of Signal-to-Distortion Ratio (SDR), measure fidelity to the \"ground truth\" waveform. This can be problematic, not only for lack of diversity in evaluation metrics, but also in cases where a perfect ground truth waveform may be unavailable. In this work, we explore the value of speaker verification as an extrinsic metric of separation quality, with additional utility as evidence of the benefits of separation as pre-processing for downstream tasks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "39c5740304b5f4072f92e4e012a4b57e7bc2e817",
    "semantic_title": "speaker verification-based evaluation of single-channel speech separation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lan21_interspeech.html": {
    "title": "Improved Speech Separation with Time-and-Frequency Cross-Domain Feature Selection",
    "volume": "main",
    "abstract": "Most deep learning-based monaural speech separation models only use either spectrograms or time domain speech signal as the input feature. The recently proposed cross-domain network (CDNet) demonstrates that concatenated frequency domain and time domain features helps to reach better performance. Although concatenation is a widely used feature fusion method, it has been proved that using frequency domain and time domain features to reconstruct signal makes minor difference compared with only using time domain feature in CDNet. To make better use of frequency domain feature in decoder, we propose using selection weights to select and fuse features from different domains and unify the features used in separator and decoder. In this paper, we propose using trainable weights or the global information calculated from the different domain features to generate selection weights. Given that our proposed models use element-wise fusing in the encoder, only one deconvolution layer in the decoder is needed to reconstruct signals. Experiments show that proposed methods achieve encouraging results on the large and challenging Libri2Mix dataset with a small increasing in parameters, which proves the frequency domain information is beneficial for signal reconstruction. Furthermore, proposed method has shown good generalizability on the unmatched VCTK2Mix dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3b197c8f29b09437068170ccf620f471dab7a2db",
    "semantic_title": "improved speech separation with time-and-frequency cross-domain feature selection",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21c_interspeech.html": {
    "title": "Robust Speaker Extraction Network Based on Iterative Refined Adaptation",
    "volume": "main",
    "abstract": "Speaker extraction aims to extract target speech signal from a multi-talker environment with interference speakers and surrounding noise, given a reference speech from target speaker. Most speaker extraction systems achieve satisfactory performance in the closed condition. Such systems suffer from performance degradation given unseen target speakers and/or mismatched reference speech. In this paper we propose a novel strategy named Iterative Refined Adaptation (IRA) to improve the robustness and generalization capability of speaker extraction systems in the aforementioned scenarios. Given an initial speaker embedding encoded by an auxiliary network, the extraction network can obtain a latent representation of the target speaker as the feedback of the auxiliary network to refine the speaker embedding, which provides more accurate guidance for the extraction network. Experiments show that the network with IRA confirm the superior performance over comparison approaches in terms of SI-SDRi and PESQ on WSJ0-2mix-extr and WHAM! dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4f68f498fe4131c9d157b6575bbccfd47024fc8e",
    "semantic_title": "robust speaker extraction network based on iterative refined adaptation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21aa_interspeech.html": {
    "title": "Neural Speaker Extraction with Speaker-Speech Cross-Attention Network",
    "volume": "main",
    "abstract": "In this paper, we propose a novel time-domain speaker-speech cross-attention network as a variant of SpEx [1] architecture, that features speaker-speech cross-attention. The speaker-speech cross-attention network consists of speech semantic layers that capture the high-level dependency of audio feature, and cross-attention layers that fuse speaker embedding and speech features to estimate the speaker mask. We implement cross-attention layers with both parallel and sequential concatenation techniques. Experiments show that the proposed models consistently outperform the state-of-the-art time-domain speaker extraction baseline on WSJ0-2mix dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "59b22e956f2a2ef4e2e5d6069c262fbf2a569ed5",
    "semantic_title": "neural speaker extraction with speaker-speech cross-attention network",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rigal21_interspeech.html": {
    "title": "Deep Audio-Visual Speech Separation Based on Facial Motion",
    "volume": "main",
    "abstract": "We present a deep neural network that relies on facial motion and time-domain audio for isolating speech signals from a mixture of speeches and background noises. Recent studies in deep learning-based audio-visual speech separation and speech enhancement have proven that leveraging visual information in addition to audio can yield substantial improvement to the prediction quality and robustness. We propose to use facial motion, inferred from optical flow techniques, as a visual feature input for our model. Combined with state-of-the-art audio-only speech separation approaches, we demonstrate that facial motion significantly improves the speech quality as well as the versatility of the model. Our proposed method offers a signal-to-distortion improvement of up to 4.2 dB on two-speaker mixtures when compared to other audio-visual approaches",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4c222fdd4101826f7d36ee04ba95d0fa264c2fad",
    "semantic_title": "deep audio-visual speech separation based on facial motion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/singh21_interspeech.html": {
    "title": "LEAP Submission for the Third DIHARD Diarization Challenge",
    "volume": "main",
    "abstract": "The LEAP submission for DIHARD-III challenge is described in this paper. The proposed system is composed of a speech bandwidth classifier, and diarization systems fine-tuned for narrowband and wideband speech separately. We use an end-to-end speaker diarization system for the narrowband conversational telephone speech recordings. For the wideband multi-speaker recordings, we use a neural embedding based clustering approach, similar to the baseline system. The embeddings are extracted from a time-delay neural network (called x-vectors) followed by the graph based path integral clustering (PIC) approach. The LEAP system showed 24% and 18% relative improvements for Track-1 and Track-2 respectively over the baseline system provided by the organizers. This paper describes the challenge submission, the post-evaluation analysis and improvements observed on the DIHARD-III dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "04148442ddf1d222bbe487c8a28a2194ace7c0fd",
    "semantic_title": "leap submission for the third dihard diarization challenge",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21w_interspeech.html": {
    "title": "Investigation of Spatial-Acoustic Features for Overlapping Speech Detection in Multiparty Meetings",
    "volume": "main",
    "abstract": "In this paper, we propose an overlapping speech detection (OSD) system for real multiparty meetings. Different from previous works on single-channel recordings or simulated data, we conduct research on real multi-channel data recorded by an 8-microphone array. We investigate how spatial information provided by multi-channel beamforming can benefit OSD. Specifically, we propose a two-stream DFSMN to jointly model acoustic and spatial features. Instead of performing frame-level OSD, we try to perform segment-level OSD. We come up with an attention pooling layer to model speech segments with variable length. Experimental results show that two-stream DFSMN with attention pooling can effectively model acoustic-spatial feature and significantly boost the performance of OSD, result in 3.5% (from 85.57% to 89.12%) absolute detection accuracy improvement compared to the baseline system",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "a984832a8f317ffff4b309cf55b28d2436fb3e45",
    "semantic_title": "investigation of spatial-acoustic features for overlapping speech detection in multiparty meetings",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/he21c_interspeech.html": {
    "title": "Target-Speaker Voice Activity Detection with Improved i-Vector Estimation for Unknown Number of Speaker",
    "volume": "main",
    "abstract": "Target-speaker voice activity detection (TS-VAD) has recently shown promising results for speaker diarization on highly overlapped speech. However, the original model requires a fixed (and known) number of speakers, which limits its application to real conversations. In this paper, we extend TS-VAD to speaker diarization with unknown numbers of speakers. This is achieved by two steps: first, an initial diarization system is applied for speaker number estimation, followed by TS-VAD network output masking according to this estimate. We further investigate different diarization methods, including clustering-based and region proposal networks, for estimating the initial i-vectors. Since these systems have complementary strengths, we propose a fusion-based method to combine frame-level decisions from the systems for an improved initialization. We demonstrate through experiments on variants of the LibriCSS meeting corpus that our proposed approach can improve the DER by up to 50% relative across varying numbers of speakers. This improvement also results in better downstream ASR performance approaching that using oracle segments",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "181e1d4b08dc62237277a6a743576facd8c5e572",
    "semantic_title": "target-speaker voice activity detection with improved i-vector estimation for unknown number of speaker",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dawalatabad21_interspeech.html": {
    "title": "ECAPA-TDNN Embeddings for Speaker Diarization",
    "volume": "main",
    "abstract": "Learning robust speaker embeddings is a crucial step in speaker diarization. Deep neural networks can accurately capture speaker discriminative characteristics and popular deep embeddings such as x-vectors are nowadays a fundamental component of modern diarization systems. Recently, some improvements over the standard TDNN architecture used for x-vectors have been proposed. The ECAPA-TDNN model, for instance, has shown impressive performance in the speaker verification domain, thanks to a carefully designed neural model In this work, we extend, for the first time, the use of the ECAPA-TDNN model to speaker diarization. Moreover, we improved its robustness with a powerful augmentation scheme that concatenates several contaminated versions of the same signal within the same training batch. The ECAPA-TDNN model turned out to provide robust speaker embeddings under both close-talking and distant-talking conditions. Our results on the popular AMI meeting corpus show that our system significantly outperforms recently proposed approaches",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3c1533d3710d34680690f99498719a0bddb6d88d",
    "semantic_title": "ecapa-tdnn embeddings for speaker diarization",
    "citation_count": 39,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kinoshita21_interspeech.html": {
    "title": "Advances in Integration of End-to-End Neural and Clustering-Based Diarization for Real Conversational Speech",
    "volume": "main",
    "abstract": "Recently, we proposed a novel speaker diarization method called End-to-End-Neural-Diarization-vector clustering (EEND-vector clustering) that integrates clustering-based and end-to-end neural network-based diarization approaches into one framework. The proposed method combines advantages of both frameworks, i.e. high diarization performance and handling of overlapped speech based on EEND, and robust handling of long recordings with an arbitrary number of speakers based on clustering-based approaches. However, the method was only evaluated so far on simulated 2-speaker meeting-like data. This paper is to (1) report recent advances we made to this framework, including newly introduced robust constrained clustering algorithms, and (2) experimentally show that the method can now outperform competitive diarization methods such as Encoder-Decoder Attractor (EDA)-EEND, on CALLHOME data which comprises real conversational speech data including overlapped speech and an arbitrary number of speakers. By further analyzing the experimental results, this paper also discusses pros and cons of the proposed method and reveals potential for further improvement",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "89404c4fe0663aa424ee2f5eeafc5e22c0ac5ce0",
    "semantic_title": "advances in integration of end-to-end neural and clustering-based diarization for real conversational speech",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ryant21_interspeech.html": {
    "title": "The Third DIHARD Diarization Challenge",
    "volume": "main",
    "abstract": "DIHARD III was the third in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variability in recording equipment, noise conditions, and conversational domain. Speaker diarization was evaluated under two speech activity conditions (diarization from a reference speech activity vs. diarization from scratch) and 11 diverse domains. The domains span a range of recording conditions and interaction types, including read audio-books, meeting speech, clinical interviews, web videos, and, for the first time, conversational telephone speech. A total of 30 organizations (forming 21 teams) from industry and academia submitted 499 valid system outputs. The evaluation results indicate that speaker diarization has improved markedly since DIHARD I, particularly for two-party interactions, but that for many domains (e.g., web video) the problem remains far from solved",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "0ae4e3325e9d18f933c6399fff0dce975de5aebd",
    "semantic_title": "the third dihard diarization challenge",
    "citation_count": 76,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/leung21_interspeech.html": {
    "title": "Robust End-to-End Speaker Diarization with Conformer and Additive Margin Penalty",
    "volume": "main",
    "abstract": "Traditionally, a speaker diarization system has multiple components to extract and cluster speaker embeddings. However, end-to-end diarization is more desirable as it facilitates optimizing one model in contrast to multiple components in a traditional set up. Moreover, end-to-end diarization systems are capable of handling overlapped speech. Recently proposed self-attentive end-to-end diarization model with encoder-decoder based attractors (EEND-EDA) is capable of processing speech from an unknown number of speakers, and has reported comparable performances to traditional systems. In this work, we aim to improve the EEND-EDA model. First, we increase the robustness of the model by incorporating an additive margin penalty for minimizing the intra-class variance. Second, we propose to replace the Transformer encoders with Conformer encoders to capture local information. Third, we propose to use convolutional subsampling and upsampling instead of manual subsampling only. Our proposed improvements report 21.6% relative reduction in DER on the evaluation full set of the track 2 of the DIHARD III challenge",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "943cd8b778c1ac932825adb8e21b3fc4c38191e3",
    "semantic_title": "robust end-to-end speaker diarization with conformer and additive margin penalty",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/obrien21_interspeech.html": {
    "title": "Anonymous Speaker Clusters: Making Distinctions Between Anonymised Speech Recordings with Clustering Interface",
    "volume": "main",
    "abstract": "Our study examined the performance of evaluators tasked to group natural and anonymised speech recordings into clusters based on their perceived similarities. Speech stimuli were selected from the VCTK corpus; two systems developed for the VoicePrivacy 2020 Challenge were used for anonymisation. The Baseline-1 (B1) system was developed by using x-vectors and neural waveform models, while the Baseline-2 (B2) system relied on digital-signal-processing techniques. 74 evaluators completed three trials composed of 16 recordings with either natural or anonymised speech generated from a single system. F-measure and cluster purity metrics were used to assess evaluator accuracy. Probabilistic linear discriminant analysis (PLDA) scores from an automatic speaker verification system were generated to quantify similarity between recordings and used to correlate subjective results. Our findings showed that non-native English speaking evaluators significantly lowered their F-measure means when presented anonymised recordings. We observed no significance for cluster purity. Pearson correlation procedures revealed that PLDA scores generated from natural and B2-anonymised speech recordings correlated positively to F-measure and cluster purity metrics. These findings show evaluators were able to use the interface to cluster natural and anonymised speech recordings and suggest anonymisation systems modelled like B1 are more effective at suppressing identifiable speech characteristics",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f8de5ef82d110bb1c9b2d6d6696bdf00b2d449bf",
    "semantic_title": "anonymous speaker clusters: making distinctions between anonymised speech recordings with clustering interface",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karra21_interspeech.html": {
    "title": "Speaker Diarization Using Two-Pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings",
    "volume": "main",
    "abstract": "Many modern systems for speaker diarization, such as the recently-developed VBx approach, rely on clustering of DNN speaker embeddings followed by resegmentation. Two problems with this approach are that the DNN is not directly optimized for this task, and the parameters need significant retuning for different applications. We have recently presented progress in this direction with a Leave-One-Out Gaussian PLDA (LGP) clustering algorithm and an approach to training the DNN such that embeddings directly optimize performance of this scoring method. This paper presents a new two-pass version of this system, where the second pass uses finer time resolution to significantly improve overall performance. For the Callhome corpus, we achieve the first published error rate below 4% without any task-dependent parameter tuning. We also show significant progress towards a robust single solution for multiple diarization tasks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "e6702556ae09605e81944912b95edd0baff8f736",
    "semantic_title": "speaker diarization using two-pass leave-one-out gaussian plda clustering of dnn embeddings",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hong21_interspeech.html": {
    "title": "Federated Learning with Dynamic Transformer for Text to Speech",
    "volume": "main",
    "abstract": "Text to speech (TTS) is a crucial task for user interaction, but TTS model training relies on a sizable set of high-quality original datasets. Due to privacy and security issues, the original datasets are usually unavailable directly. Recently, federated learning proposes a popular distributed machine learning paradigm with an enhanced privacy protection mechanism. It offers a practical and secure framework for data owners to collaborate with others, thus obtaining a better global model trained on the larger dataset. However, due to the high complexity of transformer models, the convergence process becomes slow and unstable in the federated learning setting. Besides, the transformer model trained in federated learning is costly communication and limited computational speed on clients, impeding its popularity. To deal with these challenges, we propose the federated dynamic transformer. On the one hand, the performance is greatly improved comparing with the federated transformer, approaching centralize-trained Transformer-TTS when increasing clients number. On the other hand, it achieves faster and more stable convergence in the training phase and significantly reduces communication time. Experiments on the LJSpeech dataset also strongly prove our method's advantage",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fbac2c0e078de78937b26cf1fa79205309d6f997",
    "semantic_title": "federated learning with dynamic transformer for text to speech",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21e_interspeech.html": {
    "title": "LiteTTS: A Lightweight Mel-Spectrogram-Free Text-to-Wave Synthesizer Based on Generative Adversarial Networks",
    "volume": "main",
    "abstract": "In this paper, we propose a lightweight end-to-end text-to-speech model that can generate high-quality speech at breakneck speed. In our proposed model, a feature prediction module and a waveform generation module are combined within a single framework. The feature prediction module, which consists of two independent sub-modules, estimates latent space embeddings for input text and prosodic information, and the waveform generation module generates speech waveforms by conditioning on the estimated latent space embeddings. Unlike conventional approaches that estimate prosodic information using a pre-trained model, our model jointly trains the prosodic embedding network with the speech waveform generation task using an effective domain transfer technique. Experimental results show that our proposed model can generate samples 7 times faster than real-time, and about 1.6 times faster than FastSpeech 2, as we use only 13.4 million parameters. We confirm that the generated speech quality is still of a high standard as evaluated by mean opinion scores",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "9cf49b8836d98510694b1a787d07738139ffe71d",
    "semantic_title": "litetts: a lightweight mel-spectrogram-free text-to-wave synthesizer based on generative adversarial networks",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tang21b_interspeech.html": {
    "title": "Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration",
    "volume": "main",
    "abstract": "Given a piece of speech and its transcript text, text-based speech editing aims to generate speech that can be seamlessly inserted into the given speech by editing the transcript. Existing methods adopt a two-stage approach: synthesize the input text using a generic text-to-speech (TTS) engine and then transform the voice to the desired voice using voice conversion (VC). A major problem of this framework is that VC is a challenging problem which usually needs a moderate amount of parallel training data to work satisfactorily. In this paper, we propose a one-stage context-aware framework to generate natural and coherent target speech without any training data of the target speaker. In particular, we manage to perform accurate zero-shot duration prediction for the inserted text. The predicted duration is used to regulate both text embedding and speech embedding. Then, based on the aligned cross-modality input, we directly generate the mel-spectrogram of the edited speech with a transformer-based decoder. Subjective listening tests show that despite the lack of training data for the speaker, our method has achieved satisfactory results. It outperforms a recent zero-shot TTS engine by a large margin",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c90d1152a288dc053afc1a1f7071cc4b72271cbc",
    "semantic_title": "zero-shot text-to-speech for text-based insertion in audio narration",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jeong21_interspeech.html": {
    "title": "Diff-TTS: A Denoising Diffusion Model for Text-to-Speech",
    "volume": "main",
    "abstract": "Although neural text-to-speech (TTS) models have attracted a lot of attention and succeeded in generating human-like speech, there is still room for improvements to its naturalness and architectural efficiency. In this work, we propose a novel non-autoregressive TTS model, namely Diff-TTS, which achieves highly natural and efficient speech synthesis. Given the text, Diff-TTS exploits a denoising diffusion framework to transform the noise signal into a mel-spectrogram via diffusion time steps. In order to learn the mel-spectrogram distribution conditioned on the text, we present a likelihood-based optimization method for TTS. Furthermore, to boost up the inference speed, we leverage the accelerated sampling method that allows Diff-TTS to generate raw waveforms much faster without significantly degrading perceptual quality. Through experiments, we verified that Diff-TTS generates 28 times faster than the real-time with a single NVIDIA 2080Ti GPU",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "844cd260a3ca9de92fa1217c146d8cda2e0c10c0",
    "semantic_title": "diff-tts: a denoising diffusion model for text-to-speech",
    "citation_count": 102,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bae21_interspeech.html": {
    "title": "Hierarchical Context-Aware Transformers for Non-Autoregressive Text to Speech",
    "volume": "main",
    "abstract": "In this paper, we propose methods for improving the modeling performance of a Transformer-based non-autoregressive text-to-speech (TNA-TTS) model. Although the text encoder and audio decoder handle different types and lengths of data (i.e., text and audio), the TNA-TTS models are not designed considering these variations. Therefore, to improve the modeling performance of the TNA-TTS model we propose a hierarchical Transformer structure-based text encoder and audio decoder that are designed to accommodate the characteristics of each module. For the text encoder, we constrain each self-attention layer so the encoder focuses on a text sequence from the local to the global scope. Conversely, the audio decoder constrains its self-attention layers to focus in the reverse direction, i.e., from global to local scope. Additionally, we further improve the pitch modeling accuracy of the audio decoder by providing sentence and word-level pitch as conditions. Various objective and subjective evaluations verified that the proposed method outperformed the baseline TNA-TTS",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "499533050ea59697e5be630eb0b954717a3bff40",
    "semantic_title": "hierarchical context-aware transformers for non-autoregressive text to speech",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/polyak21_interspeech.html": {
    "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
    "volume": "main",
    "abstract": "We propose using self-supervised discrete representations for the task of speech resynthesis. To generate disentangled representation, we separately extract low-bitrate representations for speech content, prosodic information, and speaker identity. This allows to synthesize speech in a controllable manner. We analyze various state-of-the-art, self-supervised representation learning methods and shed light on the advantages of each method while considering reconstruction quality and disentanglement properties. Specifically, we evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings' intelligibility, and overall quality using subjective human evaluation. Lastly, we demonstrate how these representations can be used for an ultra-lightweight speech codec. Using the obtained representations, we can get to a rate of 365 bits per second while providing better speech quality than the baseline methods. Audio samples are publicly available",
    "keywords": [
      []
    ],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 148,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/karanasou21_interspeech.html": {
    "title": "A Learned Conditional Prior for the VAE Acoustic Space of a TTS System",
    "volume": "main",
    "abstract": "Many factors influence speech yielding different renditions of a given sentence. Generative models, such as variational autoencoders (VAEs), capture this variability and allow multiple renditions of the same sentence via sampling. The degree of prosodic variability depends heavily on the prior that is used when sampling. In this paper, we propose a novel method to compute an informative prior for the VAE latent space of a neural text-to-speech (TTS) system. By doing so, we aim to sample with more prosodic variability, while gaining controllability over the latent space's structure By using as prior the posterior distribution of a secondary VAE, which we condition on a speaker vector, we can sample from the primary VAE taking explicitly the conditioning into account and resulting in samples from a specific region of the latent space for each condition (i.e. speaker). A formal preference test demonstrates significant preference of the proposed approach over standard Conditional VAE. We also provide visualisations of the latent space where well-separated condition-specific clusters appear, as well as ablation studies to better understand the behaviour of the system",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "a3e7162ee53660dd6137de9f089d9d7ea487717d",
    "semantic_title": "a learned conditional prior for the vae acoustic space of a tts system",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/paul21_interspeech.html": {
    "title": "A Universal Multi-Speaker Multi-Style Text-to-Speech via Disentangled Representation Learning Based on Rényi Divergence Minimization",
    "volume": "main",
    "abstract": "In this paper, we present a universal multi-speaker, multi-style Text-to-Speech (TTS) synthesis system which is able to generate speech from text with speaker characteristics and speaking style similar to a given reference signal. Training is conducted on non-parallel data and generates voices in an unsupervised manner, i.e., neither style annotation nor speaker label are required. To avoid leaking content information into the style embeddings (referred to as \"content leakage\") and leaking speaker information into style embeddings (referred to as \"style leakage\") we suggest a novel Rényi Divergence based Disentangled Representation framework through adversarial learning. Similar to mutual information minimization, the proposed approach explicitly estimates via a variational formula and then minimizes the Rényi divergence between the joint distribution and the product of marginals for the content-style and style-speaker pairs. By doing so, content, style and speaker spaces become representative and (ideally) independent of each other. Our proposed system greatly reduces content leakage by improving the word error rate by approximately 17–19% relative to the baseline system. In MOS-speech-quality, the proposed algorithm achieves an improvement of about 16–20% whereas MOS-style-similarly boost up 15% relative performance",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "39117906f5c19c43fd3112fbc3ffadba46bfe937",
    "semantic_title": "a universal multi-speaker multi-style text-to-speech via disentangled representation learning based on rényi divergence minimization",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21g_interspeech.html": {
    "title": "Relational Data Selection for Data Augmentation of Speaker-Dependent Multi-Band MelGAN Vocoder",
    "volume": "main",
    "abstract": "Nowadays, neural vocoders can generate very high-fidelity speech when a bunch of training data is available. Although a speaker-dependent (SD) vocoder usually outperforms a speaker-independent (SI) vocoder, it is impractical to collect a large amount of data of a specific target speaker for most real-world applications. To tackle the problem of limited target data, a data augmentation method based on speaker representation and similarity measurement of speaker verification is proposed in this paper. The proposed method selects utterances that have similar speaker identity to the target speaker from an external corpus, and then combines the selected utterances with the limited target data for SD vocoder adaptation. The evaluation results show that, compared with the vocoder adapted using only limited target data, the vocoder adapted using augmented data improves both the quality and similarity of synthesized speech",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2070e833a4317994c7c193ba919a8e959b43fb59",
    "semantic_title": "relational data selection for data augmentation of speaker-dependent multi-band melgan vocoder",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chung21_interspeech.html": {
    "title": "Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech",
    "volume": "main",
    "abstract": "Text-to-speech (TTS) synthesis is the process of producing synthesized speech from text or phoneme input. Traditional TTS models contain multiple processing steps and require external aligners, which provide attention alignments of phoneme-to-frame sequences. As the complexity increases and efficiency decreases with every additional step, there is expanding demand in modern synthesis pipelines for end-to-end TTS with efficient internal aligners. In this work, we propose an end-to-end text-to-waveform network with a novel reinforcement learning based duration search method. Our proposed generator is feed-forward and the aligner trains the agent to make optimal duration predictions by receiving active feedback from actions taken to maximize cumulative reward. We demonstrate accurate alignments of phoneme-to-frame sequence generated from trained agents enhance fidelity and naturalness of synthesized audio. Experimental results also show the superiority of our proposed model compared to other state-of-the-art TTS models with internal and external aligners",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8a90661297e46ed23da3461c57e9706443ea1dbb",
    "semantic_title": "reinforce-aligner: reinforcement alignment search for robust end-to-end text-to-speech",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21g_interspeech.html": {
    "title": "Triple M: A Practical Text-to-Speech Synthesis System with Multi-Guidance Attention and Multi-Band Multi-Time LPCNet",
    "volume": "main",
    "abstract": "In this work, a robust and efficient text-to-speech (TTS) synthesis system named Triple M is proposed for large-scale online application. The key components of Triple M are: 1) A sequence-to-sequence model adopts a novel multi-guidance attention to transfer complementary advantages from guiding attention mechanisms to the basic attention mechanism without in-domain performance loss and online service modification. Compared with single attention mechanism, multi-guidance attention not only brings better naturalness to long sentence synthesis, but also reduces the word error rate by 26.8%. 2) A new efficient multi-band multi-time vocoder framework, which reduces the computational complexity from 2.8 to 1.0 GFLOP and speeds up LPCNet by 2.75× on a single CPU",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "9f6320227f13293b60c5f0f51dceb95b8003a7a6",
    "semantic_title": "triple m: a practical text-to-speech synthesis system with multi-guidance attention and multi-band multi-time lpcnet",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/casanova21b_interspeech.html": {
    "title": "SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker Text-To-Speech Model",
    "volume": "main",
    "abstract": "In this paper, we propose SC-GlowTTS: an efficient zero-shot multi-speaker text-to-speech model that improves similarity for speakers unseen during training. We propose a speaker-conditional architecture that explores a flow-based decoder that works in a zero-shot scenario. As text encoders, we explore a dilated residual convolutional-based encoder, gated convolutional-based encoder, and transformer-based encoder. Additionally, we have shown that adjusting a GAN-based vocoder for the spectrograms predicted by the TTS model on the training dataset can significantly improve the similarity and speech quality for new speakers. Our model converges using only 11 speakers, reaching state-of-the-art results for similarity with new speakers, as well as high speech quality",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5d4cd1b6d3e0a469256da4ad259833878601087c",
    "semantic_title": "sc-glowtts: an efficient zero-shot multi-speaker text-to-speech model",
    "citation_count": 42,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/palmer21_interspeech.html": {
    "title": "Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset",
    "volume": "main",
    "abstract": "Visually-grounded spoken language datasets can enable models to learn cross-modal correspondences with very weak supervision. However, modern audio-visual datasets contain biases that undermine the real-world performance of models trained on that data. We introduce Spoken ObjectNet, which is designed to remove some of these biases and provide a way to better evaluate how effectively models will perform in real-world scenarios. This dataset expands upon ObjectNet, which is a bias-controlled image dataset that features similar image classes to those present in ImageNet We detail our data collection pipeline, which features several methods to improve caption quality, including automated language model checks. Lastly, we show baseline results on image retrieval and audio retrieval tasks. These results show that models trained on other datasets and then evaluated on Spoken ObjectNet tend to perform poorly due to biases in other datasets that the models have learned. We also show evidence that the performance decrease is due to the dataset controls, and not the transfer setting",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d0f0b69a14370dd5e78f76b0a82657548af67cd7",
    "semantic_title": "spoken objectnet: a bias-controlled spoken caption dataset",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/salesky21_interspeech.html": {
    "title": "The Multilingual TEDx Corpus for Speech Recognition and Translation",
    "volume": "main",
    "abstract": "We present the Multilingual TEDx corpus, built to support speech recognition (ASR) and speech translation (ST) research across many non-English source languages. The corpus is a collection of audio recordings from TEDx talks in 8 source languages. We segment transcripts into sentences and align them to the source-language audio and target-language translations. The corpus is released along with open-sourced code enabling extension to new talks and languages as they become available. Our corpus creation methodology can be applied to more languages than previous work, and creates multi-way parallel evaluation sets. We provide baselines in multiple ASR and ST settings, including multilingual models to improve translation performance for low-resource language pairs",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8e1e7741b56455056ff369fff9889b4c5f998b58",
    "semantic_title": "the multilingual tedx corpus for speech recognition and translation",
    "citation_count": 72,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mortensen21_interspeech.html": {
    "title": "Tusom2021: A Phonetically Transcribed Speech Dataset from an Endangered Language for Universal Phone Recognition Experiments",
    "volume": "main",
    "abstract": "There is growing interest in ASR systems that can recognize phones in a language-independent fashion [1, 2, 3]. There is additionally interest in building language technologies for low-resource and endangered languages. However, there is a paucity of realistic data that can be used to test such systems and technologies. This paper presents a publicly available, phonetically transcribed corpus of 2255 utterances (words and short phrases) in the endangered Tangkhulic language East Tusom (no ISO 639-3 code), a Tibeto-Burman language variety spoken mostly in India. Because the dataset is transcribed in terms of phones, rather than phonemes, it is a better match for universal phone recognition systems than many larger (phonemically transcribed) datasets. This paper describes the dataset and the methodology used to produce it. It further presents basic benchmarks of state-of-the-art universal phone recognition systems on the dataset as baselines for future experiments",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5afea71a0dc3a3cd77ae085533b37d2ad137779f",
    "semantic_title": "tusom2021: a phonetically transcribed speech dataset from an endangered language for universal phone recognition experiments",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fu21b_interspeech.html": {
    "title": "AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario",
    "volume": "main",
    "abstract": "In this paper, we present AISHELL-4, a sizable real-recorded Mandarin speech dataset collected by 8-channel circular microphone array for speech processing in conference scenario. The dataset consists of 211 recorded meeting sessions, each containing 4 to 8 speakers, with a total length of 120 hours. This dataset aims to bridge the advanced research on multi-speaker processing and the practical application scenario in three aspects. With real recorded meetings, AISHELL-4 provides realistic acoustics and rich natural speech characteristics in conversation such as short pause, speech overlap, quick speaker turn, noise, etc. Meanwhile, accurate transcription and speaker voice activity are provided for each meeting in AISHELL-4. This allows the researchers to explore different aspects in meeting processing, ranging from individual tasks such as speech front-end processing, speech recognition and speaker diarization, to multi-modality modeling and joint optimization of relevant tasks. Given most open source dataset for multi-speaker tasks are in English, AISHELL-4 is the only Mandarin dataset for conversation speech, providing additional value for data diversity in speech community. We also release a PyTorch-based training and evaluation framework as baseline system to promote reproducible research in this field",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "00a894ed3682d90dd03a82dcaae6e9a4533435e1",
    "semantic_title": "aishell-4: an open source dataset for speech enhancement, separation, recognition and speaker diarization in conference scenario",
    "citation_count": 35,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21o_interspeech.html": {
    "title": "GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio",
    "volume": "main",
    "abstract": "This paper introduces GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 33,000 hours of total audio suitable for semi-supervised and unsupervised training. Around 33,000 hours of transcribed audio is first collected from audiobooks, podcasts and YouTube, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc. A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to filter out segments with low-quality transcription. For system training, GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h. For our 10,000-hour training subset, we cap the word error rate at 4% during the filtering/ validation stage, and for all our other smaller training subsets, we cap it at 0%. The and evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality. Baseline systems are provided for popular speech recognition toolkits, namely Athena, ESPnet, Kaldi and Pika",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6f1ca0249eafa36a5762ac53f6ba2a4ee2133456",
    "semantic_title": "gigaspeech: an evolving, multi-domain asr corpus with 10, 000 hours of transcribed audio",
    "citation_count": 114,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21k_interspeech.html": {
    "title": "Look Who's Talking: Active Speaker Detection in the Wild",
    "volume": "main",
    "abstract": "In this work, we present a novel audio-visual dataset for active speaker detection in the wild. A speaker is considered active when his or her face is visible and the voice is audible simultaneously. Although active speaker detection is a crucial pre-processing step for many audio-visual tasks, there is no existing active speaker detection dataset to evaluate the performance using natural human speech. We therefore curate the (ASW) dataset which contains videos and co-occurring speech segments with dense speech activity labels. Videos and timestamps of audible segments are parsed and adopted from VoxConverse, an existing speaker diarisation dataset that consists of videos in the wild. Face tracks are extracted from the videos and active segments are annotated based on the timestamps of VoxConverse in a semi-automatic way. Two reference systems, one is self-supervised and the other is supervised system, are evaluated on the dataset to provide the baseline performances of ASW. Cross-domain evaluation and case study are conducted, in order to show the negative effect of the dubbed videos that are excluded in ASW",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fc9b665dbca8f09b5e065a645a7007a3ab073c74",
    "semantic_title": "look who's talking: active speaker detection in the wild",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ahmed21_interspeech.html": {
    "title": "AusKidTalk: An Auditory-Visual Corpus of 3- to 12-Year-Old Australian Children's Speech",
    "volume": "main",
    "abstract": "Here we present AusKidTalk [1], an audio-visual (AV) corpus of Australian children's speech collected to facilitate the development of speech based technological solutions for children. It builds upon the technology and expertise developed through the collection of an earlier corpus of Australian adult speech, AusTalk [2,3]. This multi-site initiative was established to remedy the dire shortage of children's speech corpora in Australia and around the world that are sufficiently sized to train accurate automated speech processing tools for children. We are collecting ~600 hours of speech from children aged 3–12 years that includes single word and sentence productions as well as narrative and emotional speech. In this paper, we discuss the key requirements for AusKidTalk and how we designed the recording setup and protocol to meet them. We also discuss key findings from our feasibility study of the recording protocol, recording tools, and user interface",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8aebcfa1de94fcb269630d6605ced7fb82811c43",
    "semantic_title": "auskidtalk: an auditory-visual corpus of 3- to 12-year-old australian children's speech",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fallgren21_interspeech.html": {
    "title": "Human-in-the-Loop Efficiency Analysis for Binary Classification in Edyson",
    "volume": "main",
    "abstract": "Edyson is a human-in-the-loop (HITL) tool for browsing and annotating large amounts of audio data quickly. It builds on temporally disassembled audio and massively multi-component audio environments to overcome the cumbersome time constraints that come with linear exploration of large audio data. This study adds the following contributions to Edyson: 1) We add the new use case of HITL binary classification by sample; 2) We explore the new domain oceanic hydrophone recordings with whale song, along with speech activity detection in noisy audio; 3) We propose a repeatable method of analysing the efficiency of HITL in Edyson for binary classification, specifically designed to measure the return on human time spent in a given domain. We exemplify this method on two domains, and show that for a manageable initial cost in terms of HITL, it does differentiate between suitable and unsuitable domains for our new use case — a valuable insight when working with large collections of audio",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7160a089249cd16223984955bbca714a5821b6fb",
    "semantic_title": "human-in-the-loop efficiency analysis for binary classification in edyson",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ryumina21_interspeech.html": {
    "title": "Annotation Confidence vs. Training Sample Size: Trade-Off Solution for Partially-Continuous Categorical Emotion Recognition",
    "volume": "main",
    "abstract": "Commonly adapted design of emotional corpora includes multiple annotations for the same instance from several annotators. Most of the previous studies assume the ground truth to be an average between all labels or the most frequently used label. Current study shows that this approach may not be optimal for training. By filtering training data according to the level of annotation agreement, it is possible to increase the performance of the system even on unreliable test samples. However, increasing the annotation confidence inevitably leads to a loss of data. Therefore, balancing the trade-off between annotation quality and sample size requires careful investigation. This study presents experimental findings of audio-visual emotion classification on a recently introduced RAMAS dataset, which contains rich categorical partially-continuous annotation for 6 basic emotions, and reveals important conclusions about optimal formulation of ground truth. By applying the proposed approach, it is possible to achieve classification accuracy of UAR=70.51% on the speech utterances with more than 60% agreement, which surpasses previously reported values on this corpus in the literature",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6c4594714709fd73d76766caa4d3b704d72d11a8",
    "semantic_title": "annotation confidence vs. training sample size: trade-off solution for partially-continuous categorical emotion recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/garcesdiazmunio21_interspeech.html": {
    "title": "Europarl-ASR: A Large Corpus of Parliamentary Debates for Streaming ASR Benchmarking and Speech Data Filtering/Verbatimization",
    "volume": "main",
    "abstract": "",
    "keywords": [
      []
    ],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kapoor21_interspeech.html": {
    "title": "Towards Automatic Speech to Sign Language Generation",
    "volume": "main",
    "abstract": "We aim to solve the highly challenging task of generating continuous sign language videos solely from speech segments for the first time. Recent efforts in this space have focused on generating such videos from human-annotated text transcripts without considering other modalities. However, replacing speech with sign language proves to be a practical solution while communicating with people suffering from hearing loss. Therefore, we eliminate the need of using text as input and design techniques that work for more natural, continuous, freely uttered speech covering an extensive vocabulary. Since the current datasets are inadequate for generating sign language directly from speech, we collect and release the first Indian sign language dataset comprising speech-level annotations, text transcripts, and the corresponding sign-language videos. Next, we propose a multi-tasking transformer network trained to generate signer's poses from speech segments. With speech-to-text as an auxiliary task and an additional cross-modal discriminator, our model learns to generate continuous sign pose sequences in an end-to-end manner. Extensive experiments and comparisons with other baselines demonstrate the effectiveness of our approach. We also conduct additional ablation studies to analyze the effect of different modules of our network. A demo video containing several results is attached to the supplementary material",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c31934f1e4f1efa110afc94878e00315bdcb2780",
    "semantic_title": "towards automatic speech to sign language generation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cho21b_interspeech.html": {
    "title": "kosp2e: Korean Speech to English Translation Corpus",
    "volume": "main",
    "abstract": "Most speech-to-text (S2T) translation studies use English speech as a source, which makes it difficult for non-English speakers to take advantage of the S2T technologies. For some languages, this problem was tackled through corpus construction, but the farther linguistically from English or the more under-resourced, this deficiency and underrepresentedness becomes more significant. In this paper, we introduce (read as ‘kospi'), a corpus that allows Korean speech to be translated into English text in an end-to-end manner. We adopt open license speech recognition corpus, translation corpus, and spoken language corpora to make our dataset freely available to the public, and check the performance through the pipeline and training-based approaches. Using pipeline and various end-to-end schemes, we obtain the highest BLEU of 21.3 and 18.0 for each based on the English hypothesis, validating the feasibility of our data. We plan to supplement annotations for other target languages through community contributions in the future",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5b73bfdbebfd0c1c465c1dd5b94cc8a2a86a0cd4",
    "semantic_title": "kosp2e: korean speech to english translation corpus",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21x_interspeech.html": {
    "title": "speechocean762: An Open-Source Non-Native English Speech Corpus for Pronunciation Assessment",
    "volume": "main",
    "abstract": "This paper introduces a new open-source speech corpus named \"speechocean762\" designed for pronunciation assessment use, consisting of 5000 English utterances from 250 non-native speakers, where half of the speakers are children. Five experts annotated each of the utterances at sentence-level, word-level and phoneme-level. A baseline system is released in open source to illustrate the phoneme-level pronunciation assessment workflow on this corpus. This corpus is allowed to be used freely for commercial and non-commercial purposes. It is available for free download from OpenSLR, and the corresponding baseline system is published in the Kaldi speech recognition toolkit",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3149e24ac8a27654e5b90eb4544ac662ef132578",
    "semantic_title": "speechocean762: an open-source non-native english speech corpus for pronunciation assessment",
    "citation_count": 35,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fan21b_interspeech.html": {
    "title": "An Improved Single Step Non-Autoregressive Transformer for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Non-autoregressive mechanisms can significantly decrease inference time for speech transformers, especially when the single step variant is applied. Previous work on CTC alignment-based single step non-autoregressive transformer (CASS-NAT) has shown a large real time factor (RTF) improvement over autoregressive transformers (AT). In this work, we propose several methods to improve the accuracy of the end-to-end CASS-NAT, followed by performance analyses. First, convolution augmented self-attention blocks are applied to both the encoder and decoder modules. Second, we propose to expand the trigger mask (acoustic boundary) for each token to increase the robustness of CTC alignments. In addition, iterated loss functions are used to enhance the gradient update of low-layer parameters. Without using an external language model, the WERs of the improved CASS-NAT, when using the three methods, are 3.1%/7.2% on Librispeech test clean/other sets and the CER is 5.4% on the Aishell1 test set, achieving a 7%~21% relative WER/CER improvement. For the analyses, we plot attention weight distributions in the decoders to visualize the relationships between token-level acoustic embeddings. When the acoustic embeddings are visualized, we find that they have a similar behavior to word embeddings, which explains why the improved CASS-NAT performs similarly to AT",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "24986724855f6274456bc7018d937ce9a96dbbea",
    "semantic_title": "an improved single step non-autoregressive transformer for automatic speech recognition",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guo21_interspeech.html": {
    "title": "Multi-Speaker ASR Combining Non-Autoregressive Conformer CTC and Conditional Speaker Chain",
    "volume": "main",
    "abstract": "Non-autoregressive (NAR) models have achieved a large inference computation reduction and comparable results with autoregressive (AR) models on various sequence to sequence tasks. However, there has been limited research aiming to explore the NAR approaches on sequence to multi-sequence problems, like multi-speaker automatic speech recognition (ASR). In this study, we extend our proposed conditional chain model to NAR multi-speaker ASR. Specifically, the output of each speaker is inferred one-by-one using both the input mixture speech and previously-estimated conditional speaker features. In each step, a NAR connectionist temporal classification (CTC) encoder is used to perform parallel computation. With this design, the total inference steps will be restricted to the number of mixed speakers. Besides, we also adopt the Conformer and incorporate an intermediate CTC loss to improve the performance. Experiments on WSJ0-Mix and LibriMix corpora show that our model outperforms other NAR models with only a slight increase of latency, achieving WERs of 22.3% and 24.9%, respectively. Moreover, by including the data of variable numbers of speakers, our model can even better than the PIT-Conformer AR model with only 1/7 latency, obtaining WERs of 19.9% and 34.3% on WSJ0-2mix and WSJ0-3mix sets. All of our codes are publicly available",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2467b2daea0398709d7ea57d084cc1f00f9d168f",
    "semantic_title": "multi-speaker asr combining non-autoregressive conformer ctc and conditional speaker chain",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ng21b_interspeech.html": {
    "title": "Pushing the Limits of Non-Autoregressive Speech Recognition",
    "volume": "main",
    "abstract": "We combine recent advancements in end-to-end speech recognition to non-autoregressive automatic speech recognition. We push the limits of non-autoregressive state-of-the-art results for multiple datasets: LibriSpeech, Fisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC on giant Conformer neural network architectures with SpecAugment and wav2vec2 pre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets, 5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without a language model",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "95652905de959dfb8cfb592767c7d331be0af009",
    "semantic_title": "pushing the limits of non-autoregressive speech recognition",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21l_interspeech.html": {
    "title": "Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies",
    "volume": "main",
    "abstract": "Self-supervised speech representations have been shown to be effective in a variety of speech applications. However, existing representation learning methods generally rely on the autoregressive model and/or observed global dependencies while generating the representation. In this work, we propose Non-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn a speech representation in a non-autoregressive manner by relying only on local dependencies of speech. NPC has a conceptually simple objective and can be implemented easily with the introduced Masked Convolution Blocks. NPC offers a significant speedup for inference since it is parallelizable in time and has a fixed inference time for each time step regardless of the input sequence length. We discuss and verify the effectiveness of NPC by theoretically and empirically comparing it with other methods. We show that the NPC representation is comparable to other methods in our experiments while being more efficient",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "214f944642f9331c5f314a882ca9c89ab7c26f29",
    "semantic_title": "non-autoregressive predictive coding for learning speech representations from local dependencies",
    "citation_count": 56,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nozaki21_interspeech.html": {
    "title": "Relaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions",
    "volume": "main",
    "abstract": "This paper proposes a method to relax the conditional independence assumption of connectionist temporal classification (CTC)-based automatic speech recognition (ASR) models. We train a CTC-based ASR model with auxiliary CTC losses in intermediate layers in addition to the original CTC loss in the last layer. During both training and inference, each generated prediction in the intermediate layers is summed to the input of the next layer to condition the prediction of the last layer on those intermediate predictions. Our method is easy to implement and retains the merits of CTC-based ASR: a simple model architecture and fast decoding speed. We conduct experiments on three different ASR corpora. Our proposed method improves a standard CTC model significantly (e.g., more than 20% relative word error rate reduction on the WSJ corpus) with a little computational overhead. Moreover, for the TEDLIUM2 corpus and the AISHELL-1 corpus, it achieves a comparable performance to a strong autoregressive model with beam search, but the decoding speed is at least 30 times faster",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ccdafb4eaa0ce58438ea316b56f2a21b6602d0cd",
    "semantic_title": "relaxing the conditional independence assumption of ctc-based asr by conditioning on intermediate predictions",
    "citation_count": 36,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fujita21b_interspeech.html": {
    "title": "Toward Streaming ASR with Non-Autoregressive Insertion-Based Model",
    "volume": "main",
    "abstract": "Neural end-to-end (E2E) models have become a promising technique to realize practical automatic speech recognition (ASR) systems. When realizing such a system, one important issue is the segmentation of audio to deal with streaming input or long recording. After audio segmentation, the ASR model with a small real-time factor (RTF) is preferable because the latency of the system can be faster. Recently, E2E ASR based on non-autoregressive models becomes a promising approach since it can decode an N-length token sequence with less than N iterations. We propose a system to concatenate audio segmentation and non-autoregressive ASR to realize high accuracy and low RTF ASR. As a non-autoregressive ASR, the insertion-based model is used. In addition, instead of concatenating separated models for segmentation and ASR, we introduce a new architecture that realizes audio segmentation and non-autoregressive ASR by a single neural network. Experimental results on Japanese and English dataset show that the method achieved a reasonable trade-off between accuracy and RTF compared with baseline autoregressive Transformer and connectionist temporal classification",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "92acaf505a9c738e56ed70759e8d0062f3c520d6",
    "semantic_title": "toward streaming asr with non-autoregressive insertion-based model",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21e_interspeech.html": {
    "title": "Layer Pruning on Demand with Intermediate CTC",
    "volume": "main",
    "abstract": "Deploying an end-to-end automatic speech recognition (ASR) model on mobile/embedded devices is a challenging task, since the device computational power and energy consumption requirements are dynamically changed in practice. To overcome the issue, we present a training and pruning method for ASR based on the connectionist temporal classification (CTC) which allows reduction of model depth at run-time without any extra fine-tuning. To achieve the goal, we adopt two regularization methods, intermediate CTC and stochastic depth, to train a model whose performance does not degrade much after pruning. We present an in-depth analysis of layer behaviors using singular vector canonical correlation analysis (SVCCA), and efficient strategies for finding layers which are safe to prune. Using the proposed method, we show that a Transformer-CTC model can be pruned in various depth on demand, improving real-time factor from 0.005 to 0.002 on GPU, while each pruned sub-model maintains the accuracy of individually trained model of the same depth",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "1cfd9b1db68fc320698da05fc6876dd0ea96fc9b",
    "semantic_title": "layer pruning on demand with intermediate ctc",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21l_interspeech.html": {
    "title": "Real-Time End-to-End Monaural Multi-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "The rising interest in single-channel multi-speaker speech separation has triggered the development of end-to-end multi-speaker automatic speech recognition (ASR). However, until now, most systems have adopted autoregressive mechanisms for decoding, resulting in slow decoding speed, which is not conducive to the application of multi-speaker speech recognition in real-world environments. In this paper, we first comprehensively investigate and compare the mainstream end-to-end multi-speaker speech recognition systems. Secondly, we improve the recently proposed non-autoregressive end-to-end speech recognition model Mask-CTC, and introduce it to multi-speaker speech recognition to achieve real-time decoding. Our experiments on the LibriMix data set show that under the premise of the same amount of parameters, the non-autoregressive model achieves performance close to that of the autoregressive model while having a faster decoding speed",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "aabef818ddd1b32b73fddd848c3fcc1a5215d37f",
    "semantic_title": "real-time end-to-end monaural multi-speaker speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ba_interspeech.html": {
    "title": "Streaming End-to-End ASR Based on Blockwise Non-Autoregressive Models",
    "volume": "main",
    "abstract": "Non-autoregressive (NAR) modeling has gained more and more attention in speech processing. With recent state-of-the-art attention-based automatic speech recognition (ASR) structure, NAR can realize promising real-time factor (RTF) improvement with only small degradation of accuracy compared to the autoregressive (AR) models. However, the recognition inference needs to wait for the completion of a full speech utterance, which limits their applications on low latency scenarios. To address this issue, we propose a novel end-to-end streaming NAR speech recognition system by combining blockwise-attention and connectionist temporal classification with mask-predict (Mask-CTC) NAR. During inference, the input audio is separated into small blocks and then processed in a blockwise streaming way. To address the insertion and deletion error at the edge of the output of each block, we apply an overlapping decoding strategy with a dynamic mapping trick that can produce more coherent sentences. Experimental results show that the proposed method improves online ASR recognition in low latency conditions compared to vanilla Mask-CTC. Moreover, it can achieve a much faster inference speed compared to the AR attention-based models. All of our codes will be publicly available",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "37074b2b9cebd89e4a92d20f41eec7360e11fe5a",
    "semantic_title": "streaming end-to-end asr based on blockwise non-autoregressive models",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/beliaev21_interspeech.html": {
    "title": "TalkNet: Non-Autoregressive Depth-Wise Separable Convolutional Model for Speech Synthesis",
    "volume": "main",
    "abstract": "We propose TalkNet, a non-autoregressive convolutional neural model for speech synthesis with explicit pitch and duration prediction. The model consists of three feed-forward convolutional networks. The first network predicts grapheme durations. An input text is then expanded by repeating each symbol according to the predicted duration. The second network predicts pitch value for every mel frame. The third network generates a mel-spectrogram from the expanded text conditioned on predicted pitch. All networks are based on 1D depth-wise separable convolutional architecture. The explicit duration prediction eliminates word skipping and repeating. The quality of the generated speech nearly matches the best auto-regressive models — TalkNet trained on the LJSpeech dataset got a MOS of 4.08. The model has only 13.2M parameters, almost 2× less than the present state-of-the-art text-to-speech models. The non-autoregressive architecture allows for fast training and inference. The small model size and fast inference make TalkNet an attractive candidate for embedded speech synthesis",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3793133132c9705c78939439967442c7ce4052b6",
    "semantic_title": "talknet: non-autoregressive depth-wise separable convolutional model for speech synthesis",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21p_interspeech.html": {
    "title": "WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "This paper introduces , a non-autoregressive generative model for text-to-speech synthesis. WaveGrad 2 is trained to estimate the gradient of the log conditional density of the waveform given a phoneme sequence. The model takes an input phoneme sequence, and through an iterative refinement process, generates an audio waveform. This contrasts to the original WaveGrad vocoder which conditions on mel-spectrogram features, generated by a separate model. The iterative refinement process starts from Gaussian noise, and through a series of refinement steps (e.g., 50 steps), progressively recovers the audio sequence. WaveGrad 2 offers a natural way to trade-off between inference speed and sample quality, through adjusting the number of refinement steps. Experiments show that the model can generate high fidelity audio, approaching the performance of a state-of-the-art neural TTS system. We also report various ablation studies over different model configurations. Audio samples are publicly available",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "10ae9a3d1e0874a50820766bd414f98e095cdd8a",
    "semantic_title": "wavegrad 2: iterative refinement for text-to-speech synthesis",
    "citation_count": 55,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21q_interspeech.html": {
    "title": "Align-Denoise: Single-Pass Non-Autoregressive Speech Recognition",
    "volume": "main",
    "abstract": "Deep autoregressive models start to become comparable or superior to the conventional systems for automatic speech recognition. However, for the inference computation, they still suffer from inference speed issue due to their token-by-token decoding characteristic. Non-autoregressive models greatly improve decoding speed by supporting decoding within a constant number of iterations. For example, Align-Refine was proposed to improve the performance of the non-autoregressive system by refining the alignment iteratively. In this work, we propose a new perspective to connect Align-Refine and denoising autoencoder. We introduce a novel noisy distribution to sample the alignment directly instead of obtaining it from the decoder output. The experimental results reveal that the proposed Align-Denoise speeds up both training and inference with performance improvement up to 5% relatively using single-pass decoding",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2161383af6d420450f69ada26f2e310e554750f8",
    "semantic_title": "align-denoise: single-pass non-autoregressive speech recognition",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lu21d_interspeech.html": {
    "title": "VAENAR-TTS: Variational Auto-Encoder Based Non-AutoRegressive Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "This paper describes a variational auto-encoder based non-autoregressive text-to-speech (VAENAR-TTS) model. The autoregressive TTS (AR-TTS) models based on the sequence-to-sequence architecture can generate high-quality speech, but their sequential decoding process can be time-consuming. Recently, non-autoregressive TTS (NAR-TTS) models have been shown to be more efficient with the parallel decoding process. However, these NAR-TTS models rely on phoneme-level durations to generate a hard alignment between the text and the spectrogram. Obtaining duration labels, either through forced alignment or knowledge distillation, is cumbersome. Furthermore, hard alignment based on phoneme expansion can degrade the naturalness of the synthesized speech. In contrast, the proposed model of VAENAR-TTS is an end-to-end approach that does not require phoneme-level durations. The VAENAR-TTS model does not contain recurrent structures and is completely non-autoregressive in both the training and inference phases. Based on the VAE architecture, the alignment information is encoded in the latent variable, and attention-based soft alignment between the text and the latent variable is used in the decoder to reconstruct the spectrogram. Experiments show that VAENAR-TTS achieves state-of-the-art synthesis quality, while the synthesis speed is comparable with other NAR-TTS models",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "451e0631157aaa37fa3f5b5ef4c87c929c68f255",
    "semantic_title": "vaenar-tts: variational auto-encoder based non-autoregressive text-to-speech synthesis",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luz21_interspeech.html": {
    "title": "Detecting Cognitive Decline Using Speech Only: The ADReSSo Challenge",
    "volume": "main",
    "abstract": "Building on the success of the ADReSS Challenge at Interspeech 2020, which attracted the participation of 34 teams from across the world, the ADReSSo Challenge targets three difficult automatic prediction problems of societal and medical relevance, namely: detection of Alzheimer's Dementia, inference of cognitive testing scores, and prediction of cognitive decline. This paper presents these prediction tasks in detail, describes the datasets used, and reports the results of the baseline classification and regression models we developed for each task. A combination of acoustic and linguistic features extracted directly from audio recordings, without human intervention, yielded a baseline accuracy of 78.87% for the AD classification task, a root mean squared error (RMSE) of 5.28 for prediction of cognitive scores , and 68.75% accuracy (F = 66.67) for the cognitive decline prediction task",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "360a653f4a7cf84145b9f90f42962e2abf2ddb1c",
    "semantic_title": "detecting cognitive decline using speech only: the adresso challenge",
    "citation_count": 63,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pereztoro21_interspeech.html": {
    "title": "Influence of the Interviewer on the Automatic Assessment of Alzheimer's Disease in the Context of the ADReSSo Challenge",
    "volume": "main",
    "abstract": "Alzheimer's Disease (AD) results from the progressive loss of neurons in the hippocampus, which affects the capability to produce coherent language. It affects lexical, grammatical, and semantic processes as well as speech fluency. This paper considers the analyses of speech and language for the assessment of AD in the context of the Alzheimer's Dementia Recognition through Spontaneous Speech (ADReSSo) 2021 challenge. We propose to extract acoustic features such as X-vectors, prosody, and emotional embeddings as well as linguistic features such as perplexity, and word-embeddings. The data consist of speech recordings from AD patients and healthy controls. The transcriptions are obtained using a commercial automatic speech recognition system. We outperform baseline results on the test set, both for the classification and the Mini-Mental State Examination (MMSE) prediction. We achieved a classification accuracy of 80% and an RMSE of 4.56 in the regression. Additionally, we found strong evidence for the influence of the interviewer on classification results. In cross-validation on the training set, we get classification results of 85% accuracy using the combined speech of the interviewer and the participant. Using interviewer speech only we still get an accuracy of 78%. Thus, we provide strong evidence for interviewer influence on classification results",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fbbd63e1f06d03c2d38884758c587d2f42e26935",
    "semantic_title": "influence of the interviewer on the automatic assessment of alzheimer's disease in the context of the adresso challenge",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21e_interspeech.html": {
    "title": "WavBERT: Exploiting Semantic and Non-Semantic Speech Using Wav2vec and BERT for Dementia Detection",
    "volume": "main",
    "abstract": "In this paper, we exploit semantic and non-semantic information from patient's speech data using Wav2vec and Bidirectional Encoder Representations from Transformers (BERT) for dementia detection. We first propose a basic WavBERT model by extracting semantic information from speech data using Wav2vec, and analyzing the semantic information using BERT for dementia detection. While the basic model discards the non-semantic information, we propose extended WavBERT models that convert the output of Wav2vec to the input to BERT for preserving the non-semantic information in dementia detection. Specifically, we determine the locations and lengths of inter-word pauses using the number of blank tokens from Wav2vec where the threshold for setting the pauses is automatically generated via BERT. We further design a pre-trained embedding conversion network that converts the output embedding of Wav2vec to the input embedding of BERT, enabling the fine-tuning of WavBERT with non-semantic information. Our evaluation results using the ADReSSo dataset showed that the WavBERT models achieved the highest accuracy of 83.1% in the classification task, the lowest Root-Mean-Square Error (RMSE) score of 4.44 in the regression task, and a mean F1 of 70.91% in the progression task. We confirmed the effectiveness of WavBERT models exploiting both semantic and non-semantic speech",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "274b823709ed906df09c9d68fda263467b0d33e6",
    "semantic_title": "wavbert: exploiting semantic and non-semantic speech using wav2vec and bert for dementia detection",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gauder21_interspeech.html": {
    "title": "Alzheimer Disease Recognition Using Speech-Based Embeddings From Pre-Trained Models",
    "volume": "main",
    "abstract": "This paper describes our submission to the ADreSSo Challenge, which focuses on the problem of automatic recognition of Alzheimer's Disease (AD) from speech. The audio samples contain speech from the subjects describing a picture with the guidance of an experimenter. Our approach to the problem is based on the use of embeddings extracted from different pre-trained models — trill, allosaurus, and wav2vec 2.0 — which were trained to solve different speech tasks. These features are modeled with a neural network that takes short segments of speech as input, generating an AD score per segment. The final score for an audio file is given by the average over all segments in the file. We include ablation results to show the performance of different feature types individually and in combination, a study of the effect of the segment size, and an analysis of statistical significance. Our results on the test data for the challenge reach an accuracy of 78.9%, outperforming both the acoustic and linguistic baselines provided by the organizers",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ca4082e8f644ca2837b7453934b0f7ba783e1c56",
    "semantic_title": "alzheimer disease recognition using speech-based embeddings from pre-trained models",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/balagopalan21_interspeech.html": {
    "title": "Comparing Acoustic-Based Approaches for Alzheimer's Disease Detection",
    "volume": "main",
    "abstract": "Robust strategies for Alzheimer's disease (AD) detection is important, given the high prevalence of AD. In this paper, we study the performance and generalizability of three approaches for AD detection from speech on the recent ADReSSo challenge dataset:1) using conventional acoustic features 2) using novel pre-trained acoustic embeddings 3) combining acoustic features and embeddings. We find that while feature-based approaches have a higher precision, classification approaches relying on the combination of embeddings and features prove to have a higher, and more balanced performance across multiple metrics of performance. Our best model, using such a combined approach, outperforms the acoustic baseline in the challenge by 2.8%",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6ed99d798eb838c1f8b3a3c56c4467975ac7ae60",
    "semantic_title": "comparing acoustic-based approaches for alzheimer's disease detection",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiao21_interspeech.html": {
    "title": "Alzheimer's Disease Detection from Spontaneous Speech Through Combining Linguistic Complexity and (Dis)Fluency Features with Pretrained Language Models",
    "volume": "main",
    "abstract": "In this paper, we combined linguistic complexity and (dis)fluency features with pretrained language models for the task of Alzheimer's disease detection of the 2021 ADReSSo (Alzheimer's Dementia Recognition through Spontaneous Speech) challenge. An accuracy of 83.1% was achieved on the test set, which amounts to an improvement of 4.23% over the baseline model. Our best-performing model that integrated component models using a stacking ensemble technique performed equally well on cross-validation and test data, indicating that it is robust against overfitting",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7e746ab1a6514a347ec4f155be50292e7a0d7178",
    "semantic_title": "alzheimer's disease detection from spontaneous speech through combining linguistic complexity and (dis)fluency features with pretrained language models",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21c_interspeech.html": {
    "title": "Using the Outputs of Different Automatic Speech Recognition Paradigms for Acoustic- and BERT-Based Alzheimer's Dementia Detection Through Spontaneous Speech",
    "volume": "main",
    "abstract": "Exploring acoustic and linguistic information embedded in spontaneous speech recordings has proven to be efficient for automatic Alzheimer's dementia detection. Acoustic features can be extracted directly from the audio recordings, however, linguistic features, in fully automatic systems, need to be extracted from transcripts generated by an automatic speech recognition (ASR) system. We explore two state-of-the-art ASR paradigms, Wav2vec2.0 (for transcription and feature extraction) and time delay neural networks (TDNN) on the ADReSSo dataset containing recordings of people describing the Cookie Theft (CT) picture. As no manual transcripts are provided, we train an ASR system using our in-house CT data. We further investigate the use of confidence scores and multiple ASR hypotheses to guide and augment the input for the BERT-based classification. In total, five models are proposed for exploring how to use the audio recordings only for acoustic and linguistic information extraction. The test results on best acoustic-only and best linguistic-only are 74.65% and 84.51% respectively (representing a 15% and 9% relative increase to published baseline results)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "87f32864db2e90ddde7dd7a7b4b757131f89ec9b",
    "semantic_title": "using the outputs of different automatic speech recognition paradigms for acoustic- and bert-based alzheimer's dementia detection through spontaneous speech",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/syed21_interspeech.html": {
    "title": "Tackling the ADRESSO Challenge 2021: The MUET-RMIT System for Alzheimer's Dementia Recognition from Spontaneous Speech",
    "volume": "main",
    "abstract": "This paper addresses the Interspeech Alzheimer's Dementia Recognition through Spontaneous Speech only (ADReSSo) challenge 2021. The objective of our study is to propose the approach to a three task automated screening that will aid in distinguishing between healthy individuals and subjects with dementia. The first task is to differentiate between speech recordings from individuals with dementia. The second task requires participants to estimate the Mini-Mental State Examination (MMSE) score based on an individual's speech. The third task requires participants to leverage speech recordings to identify whether individuals have suffered from cognitive decline. Here, we propose a system based on functionals of deep textual embeddings with special preprocessing steps integrating the effect of silence segments. We report that the developed system outperforms the challenge baseline for all three tasks. For Task 1, we achieve an accuracy of 84.51% compared to the baseline of 77.46%, for Task 2, we achieve a root-mean-square-error (RMSE) of 4.35 compared to the baseline of 5.28, and for Task 3, we achieve an average-f1score of 73.80% compared to the baseline of 66.67%. These results are a testament of the effectiveness of our proposed system",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "394fbb530d22b1cfb4b51e90acf0da488487b429",
    "semantic_title": "tackling the adresso challenge 2021: the muet-rmit system for alzheimer's dementia recognition from spontaneous speech",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rohanian21_interspeech.html": {
    "title": "Alzheimer's Dementia Recognition Using Acoustic, Lexical, Disfluency and Speech Pause Features Robust to Noisy Inputs",
    "volume": "main",
    "abstract": "We present two multimodal fusion-based deep learning models that consume ASR transcribed speech and acoustic data simultaneously to classify whether a speaker in a structured diagnostic task has Alzheimer's Disease and to what degree, evaluating the ADReSSo challenge 2021 data. Our best model, a BiLSTM with highway layers using words, word probabilities, disfluency features, pause information, and a variety of acoustic features, achieves an accuracy of 84% and RSME error prediction of 4.26 on MMSE cognitive scores. While predicting cognitive decline is more challenging, our models show improvement using the multimodal approach and word probabilities, disfluency, and pause information over word-only models. We show considerable gains for AD classification using multimodal fusion and gating, which can effectively deal with noisy inputs from acoustic features and ASR hypotheses",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3290c65f980ae04aef96405aa680f6bc8e85dbd7",
    "semantic_title": "alzheimer's dementia recognition using acoustic, lexical, disfluency and speech pause features robust to noisy inputs",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pappagari21_interspeech.html": {
    "title": "Automatic Detection and Assessment of Alzheimer Disease Using Speech and Language Technologies in Low-Resource Scenarios",
    "volume": "main",
    "abstract": "In this study, we analyze the use of speech and speaker recognition technologies and natural language processing to detect Alzheimer disease (AD) and estimate mini-mental status evaluation (MMSE) scores. We used speech recordings from Interspeech 2021 ADReSSo challenge dataset. Our work focuses on adapting state-of-the-art speaker recognition and language models individually and later collectively to examine their complementary behavior for the tasks. We used speech embedding techniques such as x-vectors and prosody features to characterize the speech signals. We also employed automatic speech recognition (ASR) with interpolated language models to obtain transcriptions used to fine-tune the BERT models that classify and assess the speakers. Our results indicate that the fusion of scores obtained from the multiple acoustic and linguistic models provides the best detection results, suggesting that they contain complementary information. A separate analysis of the models indicates that linguistic models outperform acoustic models in detection and prediction tasks. However, acoustic models can provide better results than linguistic models under certain circumstances due to the errors in ASR transcriptions, which indicates that the performance of linguistic models relies on the performance of ASRs. Our best models provide 84.51% accuracy in automatic detection of AD and 3.85 RMSE in MMSE prediction",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7e3deabd44eccb0fe2823d8cecf1e182efeeb0f6",
    "semantic_title": "automatic detection and assessment of alzheimer disease using speech and language technologies in low-resource scenarios",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21r_interspeech.html": {
    "title": "Automatic Detection of Alzheimer's Disease Using Spontaneous Speech Only",
    "volume": "main",
    "abstract": "Alzheimer's disease (AD) is a neurodegenerative syndrome which affects tens of millions of elders worldwide. Although there is no treatment currently available, early recognition can improve the lives of people with AD and their caretakers and families. To find a cost-effective and easy-to-use method for dementia detection and address the dementia classification task of InterSpeech 2021 ADReSSo (Alzheimer's Dementia Recognition through Spontaneous Speech only) challenge, we conduct a systematic comparison of approaches to detection of cognitive impairment based on spontaneous speech. We investigated the characteristics of acoustic modality and linguistic modality directly based on the audio recordings of narrative speech, and explored a variety of modality fusion strategies. With an ensemble over top-10 classifiers on the training set, we achieved an accuracy of 81.69% compared to the baseline of 78.87% on the test set. The results suggest that although transcription errors will be introduced through automatic speech recognition, integrating textual information generally improves classification performance. Besides, ensemble methods can boost both the accuracy and the robustness of models",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c67f9bd3209f57c2777d19b018ab4c48b206fa97",
    "semantic_title": "automatic detection of alzheimer's disease using spontaneous speech only",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ca_interspeech.html": {
    "title": "Modular Multi-Modal Attention Network for Alzheimer's Disease Detection Using Patient Audio and Language Data",
    "volume": "main",
    "abstract": "In this work, we propose a modular multi-modal architecture to automatically detect Alzheimer's disease using the dataset provided in the ADReSSo challenge. Both acoustic and text-based features are used in this architecture. Since the dataset provides only audio samples of controls and patients, we use Google cloud-based speech-to-text API to automatically transcribe the audio files to extract text-based features. Several kinds of audio features are extracted using standard packages. The proposed approach consists of 4 networks: C-attention-acoustic network (for acoustic features only), C-Attention-FT network (for linguistic features only), C-Attention-Embedding network (for language embeddings and acoustic embeddings), and a unified network (uses all of those features). The architecture combines attention networks and a convolutional neural network (C-Attention network) in order to process these features. Experimental results show that the C-Attention-Unified network with Linguistic features and X-Vector embeddings achieves the best accuracy of 80.28% and F1 score of 0.825 on the test dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "96fc2c702b66f2215e39b744fc17649456c9721b",
    "semantic_title": "modular multi-modal attention network for alzheimer's disease detection using patient audio and language data",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gong21d_interspeech.html": {
    "title": "Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-Field Speech Recognition",
    "volume": "main",
    "abstract": "When a sufficiently large far-field training data is presented, jointly optimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech Recognition (ASR) backend shows promising results. Recent literature has shown traditional beamformer designs, such as MVDR (Minimum Variance Distortionless Response) or fixed beamformers can be successfully integrated as the frontend into an E2E ASR system with learnable parameters. In this work, we propose the self-attention channel combinator (SACC) ASR frontend, which leverages the self-attention mechanism to combine multichannel audio signals in the magnitude spectral domain. Experiments conducted on a multichannel playback test data shows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed beamformer-based frontend, both jointly optimized with a ContextNet-based ASR backend. We also demonstrate the connection between the SACC and the traditional beamformers, and analyze the intermediate outputs of the SACC",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4e452d0ae1f030954b577cfa7007f894d22ae48d",
    "semantic_title": "self-attention channel combinator frontend for end-to-end multichannel far-field speech recognition",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gretter21_interspeech.html": {
    "title": "ETLT 2021: Shared Task on Automatic Speech Recognition for Non-Native Children's Speech",
    "volume": "main",
    "abstract": "The paper presents the Second ASR Challenge for Non-native Children's Speech proposed as a Special Session at Interspeech 2021, following the successful first challenge at Interspeech 2020. The goal of the challenge is to advance research on non-native children's speech recognition technology, as speech technology still struggles when applied to both children and non-native speakers. The audio data consists of spoken responses provided by L2 students in the context of both English and German speaking proficiency examinations, the latter language added for 2021. Additional training data and a new evaluation set was released for L2 English recorded by speakers of different native languages. Participants could build systems for one or both languages. Each had a closed track where a predetermined set of audio and linguistic resources were selected, and an open track where additional data was allowed. After a description of the released corpora, the paper analyzes the results achieved by the participating systems. Some issues suggested from these results are discussed",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f001beecd7cd3047902bb6e456c89eb9198e1e69",
    "semantic_title": "etlt 2021: shared task on automatic speech recognition for non-native children's speech",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rumberg21_interspeech.html": {
    "title": "Age-Invariant Training for End-to-End Child Speech Recognition Using Adversarial Multi-Task Learning",
    "volume": "main",
    "abstract": "Automatic speech recognition for children's speech is a challenging task mainly due to scarcity of publicly available child speech corpora and wide inter- and intra-speaker variability in terms of acoustic and linguistic characteristics of children's speech. We propose a framework for age-invariant training of the acoustic model of end-to-end speech recognition systems based on adversarial multi-task learning. We use age information additionally to just differentiating between the child and adult domains and thus force the acoustic model to learn age invariant features. Our results on publicly available data sets show that this leads to better leveraging of existing data during training. We further show that usage of adversarial multitask learning should not necessarily be regarded as a substitute for traditional feature space adaptation methods, but that both should be used together for best performance",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5db2799376f8a965a130d27b4e3a7d32d4b79369",
    "semantic_title": "age-invariant training for end-to-end child speech recognition using adversarial multi-task learning",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cornell21_interspeech.html": {
    "title": "Learning to Rank Microphones for Distant Speech Recognition",
    "volume": "main",
    "abstract": "Fully exploiting ad-hoc microphone networks for distant speech recognition is still an open issue. Empirical evidence shows that being able to select the best microphone leads to significant improvements in recognition without any additional effort on front-end processing. Current channel selection techniques either rely on signal, decoder or posterior-based features. Signal-based features are inexpensive to compute but do not always correlate with recognition performance. Instead decoder and posterior-based features exhibit better correlation but require substantial computational resources In this work, we tackle the channel selection problem by proposing MicRank, a learning to rank framework where a neural network is trained to rank the available channels using directly the recognition performance on the training set. The proposed approach is agnostic with respect to the array geometry and type of recognition back-end. We investigate different learning to rank strategies using a synthetic dataset developed on purpose and the CHiME-6 data. Results show that the proposed approach considerably improves over previous selection techniques, reaching comparable and in some instances better performance than oracle signal-based measures",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d0b8bedbe64bf68f4e92347e067498c2599b729d",
    "semantic_title": "learning to rank microphones for distant speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gelin21_interspeech.html": {
    "title": "Simulating Reading Mistakes for Child Speech Transformer-Based Phone Recognition",
    "volume": "main",
    "abstract": "Current performance of automatic speech recognition (ASR) for children is below that of the latest systems dedicated to adult speech. Child speech is particularly difficult to recognise, and substantial corpora are missing to train acoustic models. Furthermore, in the scope of our reading assistant for 5–8-year-old children learning to read, models need to cope with disfluencies and reading mistakes, which remain considerable challenges even for state-of-the-art ASR systems. In this paper, we adapt an end-to-end Transformer acoustic model to speech from children learning to read. Transfer learning (TL) with a small amount of child speech improves the phone error rate (PER) by 48.7% relative over an adult model and outperforms a TL-adapted DNN-HMM model by 21.0% relative PER. Multi-objective training with a Connectionist Temporal Classification (CTC) function further reduces the PER by 4.8% relative. We propose a method of reading mistakes data augmentation, where we simulate word-level repetitions and substitutions with phonetically or graphically close words. Combining these two types of reading mistakes reaches a 19.9% PER, with a 13.1% relative improvement over the baseline. A detailed analysis shows that both the CTC multi-objective training and the augmentation with synthetic repetitions help the attention mechanisms better detect children's disfluencies",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ee082aa6e947b8589a8beeadb60e30ab2087304a",
    "semantic_title": "simulating reading mistakes for child speech transformer-based phone recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/stephenson21_interspeech.html": {
    "title": "Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input",
    "volume": "main",
    "abstract": "Inferring the prosody of a word in text-to-speech synthesis requires information about its surrounding context. In incremental text-to-speech synthesis, where the synthesizer produces an output before it has access to the complete input, the full context is often unknown which can result in a loss of naturalness. In this paper, we investigate whether the use of predicted future text from a transformer language model can attenuate this loss in a neural TTS system. We compare several test conditions of next future word: (a) unknown (zero-word), (b) language model predicted, (c) randomly predicted and (d) ground-truth. We measure the prosodic features (pitch, energy and duration) and find that predicted text provides significant improvements over a zero-word lookahead, but only slight gains over random-word lookahead. We confirm these results with a perceptive test",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "1887d233ac627e6df54ca76a24a8e97b43bfbd2f",
    "semantic_title": "alternate endings: improving prosody for incremental neural tts with predicted future text input",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rijn21_interspeech.html": {
    "title": "Exploring Emotional Prototypes in a High Dimensional TTS Latent Space",
    "volume": "main",
    "abstract": "Recent TTS systems are able to generate prosodically varied and realistic speech. However, it is unclear how this prosodic variation contributes to the perception of speakers' emotional states. Here we use the recent psychological paradigm ‘Gibbs Sampling with People' to search the prosodic latent space in a trained Global Style Token Tacotron model to explore prototypes of emotional prosody. Participants are recruited online and collectively manipulate the latent space of the generative speech model in a sequentially adaptive way so that the stimulus presented to one group of participants is determined by the response of the previous groups. We demonstrate that (1) particular regions of the model's latent space are reliably associated with particular emotions, (2) the resulting emotional prototypes are well-recognized by a separate group of human raters, and (3) these emotional prototypes can be effectively transferred to new sentences. Collectively, these experiments demonstrate a novel approach to the understanding of emotional speech by providing a tool to explore the relation between the latent space of generative models and human semantics",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "abf443e48497c5b8d51f1c3a1902fdac031e590e",
    "semantic_title": "exploring emotional prototypes in a high dimensional tts latent space",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mohan21_interspeech.html": {
    "title": "Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis",
    "volume": "main",
    "abstract": "Text does not fully specify the spoken form, so text-to-speech models must be able to learn from speech data that vary in ways not explained by the corresponding text. One way to reduce the amount of unexplained variation in training data is to provide acoustic information as an additional learning signal. When generating speech, modifying this acoustic information enables multiple distinct renditions of a text to be produced Since much of the unexplained variation is in the prosody, we propose a model that generates speech explicitly conditioned on the three primary acoustic correlates of prosody: F , energy and duration. The model is flexible about how the values of these features are specified: they can be externally provided, or predicted from text, or predicted then subsequently modified Compared to a model that employs a variational auto-encoder to learn unsupervised latent features, our model provides more interpretable, temporally-precise, and disentangled control. When automatically predicting the acoustic features from text, it generates speech that is more natural than that from a Tacotron 2 model with reference encoder. Subsequent human-in-the-loop modification of the predicted acoustic features can significantly further increase naturalness",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "55e666881f2b51ba94ab504a0131707da4a21ed2",
    "semantic_title": "ctrl-p: temporal control of prosodic variation for speech synthesis",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/torresquintero21_interspeech.html": {
    "title": "ADEPT: A Dataset for Evaluating Prosody Transfer",
    "volume": "main",
    "abstract": "Text-to-speech is now able to achieve near-human naturalness and research focus has shifted to increasing expressivity. One popular method is to transfer the prosody from a reference speech sample. There have been considerable advances in using prosody transfer to generate more expressive speech, but the field lacks a clear definition of what successful prosody transfer means and a method for measuring it. We introduce a dataset of prosodically-varied reference natural speech samples for evaluating prosody transfer. The samples include global variations reflecting emotion and interpersonal attitude, and local variations reflecting topical emphasis, propositional attitude, syntactic phrasing and marked tonicity. The corpus only includes prosodic variations that listeners are able to distinguish with reasonable accuracy, and we report these figures as a benchmark against which text-to-speech prosody transfer can be compared. We conclude the paper with a demonstration of our proposed evaluation methodology, using the corpus to evaluate two text-to-speech models that perform prosody transfer",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5ca6643b4dde05a51035eb8f91d9a2dfc5964203",
    "semantic_title": "adept: a dataset for evaluating prosody transfer",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/trang21_interspeech.html": {
    "title": "Prosodic Boundary Prediction Model for Vietnamese Text-To-Speech",
    "volume": "main",
    "abstract": "This research aims to build a prosodic boundary prediction model for improving the naturalness of Vietnamese speech synthesis. This model can be used directly to predict prosodic boundaries in the synthesis phase of the statistical parametric or end-to-end speech systems. Beside conventional features related to Part-Of-Speech (POS), this paper proposes two efficient features to predict prosodic boundaries: syntactic blocks and syntactic links, based on a thorough analysis of a Vietnamese dataset. Syntactic blocks are syntactic phrases whose sizes are bounded in their constituent syntactic tree. A syntactic link of two adjacent words is calculated based on the distance between them in the syntax tree. The experimental results show that the two proposed predictors improve the quality of the boundary prediction model using a decision tree classification algorithm, about 36.4% (F1 score) higher than the model with only POS features. The final boundary prediction model with POS, syntactic block, and syntactic link features using the LightGBM algorithm gives the best F1-score results at 87.0% in test data. The proposed model helps the TTS systems, developed by either HMM-based, DNN-based, or End-to-end speech synthesis techniques, improve about 0.3 MOS points (i.e. 6 to 10%) compared to the ones without the proposed model",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "392d854d79d60d9eaeb81a58766a24d52c675ed2",
    "semantic_title": "prosodic boundary prediction model for vietnamese text-to-speech",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dovrat21_interspeech.html": {
    "title": "Many-Speakers Single Channel Speech Separation with Optimal Permutation Training",
    "volume": "main",
    "abstract": "Single channel speech separation has experienced great progress in the last few years. However, training neural speech separation for a large number of speakers (e.g., more than 10 speakers) is out of reach for the current methods, which rely on the Permutation Invariant Training (PIT). In this work, we present a permutation invariant training that employs the Hungarian algorithm in order to train with an O(C ) time complexity, where C is the number of speakers, in comparison to O(C!) of PIT based methods. Furthermore, we present a modified architecture that can handle the increased number of speakers. Our approach separates up to 20 speakers and improves the previous results for large C by a wide margin",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "179ff92e66ec2c0eae0b386d69d982a76c7334df",
    "semantic_title": "many-speakers single channel speech separation with optimal permutation training",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fras21_interspeech.html": {
    "title": "Combating Reverberation in NTF-Based Speech Separation Using a Sub-Source Weighted Multichannel Wiener Filter and Linear Prediction",
    "volume": "main",
    "abstract": "Sound source separation (SS) from the microphone signals capturing speech in reverberant conditions is a formidable task. This paper addresses the problem of joint separation and dereverberation of speech using the multichannel Wiener filter (MWF) that is tailored to the sub-source modeling of each speech source with a full-rank mixing matrix. Specifically, the parameters of the proposed sub-source-weighted (SSW) spatial filter are estimated using the sub-source based expectation maximization (EM) algorithm with multiplicative updates (MU) and the localization prior distribution (LP) on the mixing matrix (SSEM-MU-LP). In addition, we strengthen dereverberation by incorporating a Generalized Weighted Prediction Error (GWPE) algorithm. The proposed method is evaluated using a large dataset of two-channel recordings of clean speech convolved with both real and synthesized impulse responses. The results of the experiments show the superior performance of the proposed method in reverberant conditions in comparison to using the standard NTF-based separation with the vanilla MWF in terms of signal-to-distortion ratio (improvement of 3–5.6 dB) and other commonly used sound separation metrics",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2b4b616fa976ce87cde7fd5fd75a8f53db436af5",
    "semantic_title": "combating reverberation in ntf-based speech separation using a sub-source weighted multichannel wiener filter and linear prediction",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/strauss21_interspeech.html": {
    "title": "A Hands-On Comparison of DNNs for Dialog Separation Using Transfer Learning from Music Source Separation",
    "volume": "main",
    "abstract": "This paper describes a hands-on comparison on using state-of-the-art music source separation deep neural networks (DNNs) before and after task-specific fine-tuning for separating speech content from non-speech content in broadcast audio (i.e., dialog separation). The music separation models are selected as they share the number of channels (2) and sampling rate (44.1 kHz or higher) with the considered broadcast content, and vocals separation in music is considered as a parallel for dialog separation in the target application domain. These similarities are assumed to enable transfer learning between the tasks. Three models pre-trained on music (Open-Unmix, Spleeter, and Conv-TasNet) are considered in the experiments, and fine-tuned with real broadcast data. The performance of the models is evaluated before and after fine-tuning with computational evaluation metrics (SI-SIRi, SI-SDRi, 2f-model), as well as with a listening test simulating an application where the non-speech signal is partially attenuated, e.g., for better speech intelligibility. The evaluations include two reference systems specifically developed for dialog separation. The results indicate that pre-trained music source separation models can be used for dialog separation to some degree, and that they benefit from the fine-tuning, reaching a performance close to task-specific solutions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d24121a3df38b8f3e9b7177ee361610fa763ef0e",
    "semantic_title": "a hands-on comparison of dnns for dialog separation using transfer learning from music source separation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/borsdorf21b_interspeech.html": {
    "title": "GlobalPhone Mix-To-Separate Out of 2: A Multilingual 2000 Speakers Mixtures Database for Speech Separation",
    "volume": "main",
    "abstract": "Monaural speech separation has been well studied on various databases. However, these databases mostly concern English speech. Research in multi-speaker scenarios, such as speech recognition, speaker recognition, speaker diarization, and speech separation calls for speaker mixtures databases comprising multiple languages. In this paper, we propose a new extensive multilingual database for speech separation tasks derived from the GlobalPhone 2000 Speaker Package, called \"GlobalPhone Mix-to-Separate out of 2\" (GlobalPhoneMS2). We describe the construction of the database and conduct speech separation experiments in monolingual and multilingual as well as seen and unseen languages settings. When trained on a multilingual dataset, the networks improve their performances for unseen languages, and across almost all seen languages. We show that replacing a monolingual dataset with a trilingual one, while keeping the data size roughly the same, helps to improve the performance in most cases. We attribute this to a larger diversity in speech, language, speaker, and recording characteristics. Based on the GlobalPhoneMS2 database, speech separation results for two-speaker mixing scenarios are reported in 22 spoken languages for the first time",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "899d540993ef052db2ebdd618eef8e6df03a0a4b",
    "semantic_title": "globalphone mix-to-separate out of 2: a multilingual 2000 speakers mixtures database for speech separation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tsukada21_interspeech.html": {
    "title": "Cross-Linguistic Perception of the Japanese Singleton/Geminate Contrast: Korean, Mandarin and Mongolian Compared",
    "volume": "main",
    "abstract": "The perception of Japanese consonant length contrasts (i.e. short/singleton vs long/geminate) by native and non-native speakers was compared to examine the extent to which difficult foreign language (FL) sounds are processed accurately. Three groups of participants had Korean, Mandarin or Mongolian as their first language (L1) and had no experience with Japanese. Unlike Japanese, Mandarin and Mongolian do not use consonant length contrastively. The phonemic status of consonant length in Korean is debatable. Further, unlike Japanese and Mandarin which predominantly use open syllables and restrict the occurrence of consonants in coda position, Korean and Mongolian permit a wide range of consonants in that syllable position. Via the AXB task, the participants' discrimination accuracy of Japanese consonant length contrasts was assessed and compared to that of a group of 10 native Japanese speakers who served as controls. The Japanese group was at near ceiling with little individual variation. The Mongolian (but not Korean and Mandarin) group did not significantly differ from the control group when the target token (X) contained a geminate. All non-native groups were significantly less accurate than the control group when X contained a singleton. These results were interpreted as reflecting the participants' L1 quantity system",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ae9c361eb6524aab08ee0eb960fd7d7ecb474e77",
    "semantic_title": "cross-linguistic perception of the japanese singleton/geminate contrast: korean, mandarin and mongolian compared",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/korzekwa21_interspeech.html": {
    "title": "Detection of Lexical Stress Errors in Non-Native (L2) English with Data Augmentation and Attention",
    "volume": "main",
    "abstract": "This paper describes two novel complementary techniques that improve the detection of lexical stress errors in non-native (L2) English speech: attention-based feature extraction and data augmentation based on Neural Text-To-Speech (TTS). In a classical approach, audio features are usually extracted from fixed regions of speech such as the syllable nucleus. We propose an attention-based deep learning model that automatically derives optimal syllable-level representation from frame-level and phoneme-level audio features. Training this model is challenging because of the limited amount of incorrect stress patterns. To solve this problem, we propose to augment the training set with incorrectly stressed words generated with Neural TTS. Combining both techniques achieves 94.8% precision and 49.2% recall for the detection of incorrectly stressed words in L2 English speech of Slavic and Baltic speakers",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b721c93c3fb97c0cff2ce95d2558195ad02934ea",
    "semantic_title": "detection of lexical stress errors in non-native (l2) english with data augmentation and attention",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/braun21_interspeech.html": {
    "title": "Testing Acoustic Voice Quality Classification Across Languages and Speech Styles",
    "volume": "main",
    "abstract": "Many studies relate acoustic voice quality measures to perceptual classification. We extend this line of research by training a classifier on a balanced set of perceptually annotated voice quality categories with high inter-rater agreement, and test it on speech samples from a different language and on a different speech style. Annotations were done on continuous speech from different laboratory settings. In Experiment 1, we trained a random forest with Standard Chinese and German recordings labelled as modal, breathy, or glottalized. The model had an accuracy of 78.7% on unseen data from the same sample (most important variables were harmonics-to-noise ratio, cepstral-peak prominence, and H1-A2). This model was then used to classify data from a different language (Icelandic, Experiment 2) and to classify a different speech style (German infant-directed speech (IDS), Experiment 3). Cross-linguistic generalizability was high for Icelandic (78.6% accuracy), but lower for German IDS (71.7% accuracy). Accuracy of recordings of adult-directed speech from the same speakers as in Experiment 3 (77%, Experiment 4) suggests that it is the special speech style of IDS, rather than the recording setting that led to lower performance. Results are discussed in terms of efficiency of coding and generalizability across languages and speech styles",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f3dc217b9996453c0ab3e653dc4aec0585c9b002",
    "semantic_title": "testing acoustic voice quality classification across languages and speech styles",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21y_interspeech.html": {
    "title": "Acquisition of Prosodic Focus Marking by Three- to Six-Year-Old Children Learning Mandarin Chinese",
    "volume": "main",
    "abstract": "Prosodic focus plays an important role during speech communication, delivering speakers' pragmatical intention to emphasize key information, especially in contrastive scenarios. Previous studies exploring children's acquisition of prosodic focus have generally focused on Germanic and Romance languages, while it was unclear when children learning Mandarin Chinese were able to correctly interpret the pragmatic meaning of prosodic focus and integrate it into speech comprehension. The current study explored Mandarin-learning 3–6-year-olds' online interpretation of prosodic focus to identify contrastive referents. Twenty 3–4-year-olds, 23 5–6-year-olds, and 22 adult controls were tested. The visual-world paradigm was adopted, where participants were instructed to search for target pictures while listening to contrastive objects in discourse sequences, e.g., , where the second adjective was produced with or without prosodic focus. Participants' fixation patterns were recorded via eye-trackers. The results showed that while adults and 5–6 years showed faster fixation toward target pictures in the presence of prosodic focus, this was not the case for 3–4 years. These results indicated that Mandarin-learning children at 5–6 years have acquired the pragmatic meaning of prosodic focus and utilize it to guide their identification of contrastive referents",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "cdb0879301c4c223b68e64046a98cbca2951819a",
    "semantic_title": "acquisition of prosodic focus marking by three- to six-year-old children learning mandarin chinese",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mirzaei21_interspeech.html": {
    "title": "Adaptive Listening Difficulty Detection for L2 Learners Through Moderating ASR Resources",
    "volume": "main",
    "abstract": "Teaching listening skills to those learning a second language (L2) is one of the most challenging tasks mainly because predicting L2 listening difficulties is not always straightforward. Complex processes are involved in decoding connected speech, constructing meaning, and comprehending the audio material. Many studies have attempted to identify the significant factors leading to listening difficulties, yet, a comprehensive model is to be constructed. We argue that an automatic speech recognition (ASR) system with limited training can be viewed as a rough model for an L2 listener with particular language proficiency. We proposed a method to select the training samples for the ASR system to match the mistakes of L2 listeners when listening to the authentic listening materials. This model can predict the learners' listening difficulties, thus allowing for generating tailored captions to assist them with L2 listening",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "759797531c21efc71467213fc789ffa076b331e3",
    "semantic_title": "adaptive listening difficulty detection for l2 learners through moderating asr resources",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ding21b_interspeech.html": {
    "title": "F0 Patterns of L2 English Speech by Mandarin Chinese Learners",
    "volume": "main",
    "abstract": "Prosodic speech characteristics are important in the evaluation of both intelligibility and naturalness of oral proficiency for learners of English as a Second Language (ESL). Different f movement patterns between native and Mandarin Chinese learners have been an important research topic for second-language (L2) English speech learning. However, previous studies have seldom examined f movement patterns between lower-level and higher-level Mandarin ESL learners. The current study compared f change patterns extracted from the same 20 English sentences read by 20 lower- and 20 higher- level Mandarin ESL learners, and 20 native English speakers from a speech database. Appropriate procedures were applied to ensure a more accurate estimation of f values and to catch characteristic deviation in f movement patterns of ESL learners. The results showed that lower-level Mandarin speakers displayed more frequent f fluctuations and smaller standard deviation of intervals between f peaks than both native speakers and higher-level learners. The special characteristic of many smaller \"ripples\" on pitch contours of lower-level L2 English speech resembles Mandarin Chinese f movements, which suggests a negative transfer from the first language (L1) Mandarin. The findings can shed light on the assessment and learning of L2 English prosody by Mandarin ESL learners",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "661cfb2c6c74131b10e3d11a8f437b15824f983b",
    "semantic_title": "f0 patterns of l2 english speech by mandarin chinese learners",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21h_interspeech.html": {
    "title": "A Neural Network-Based Noise Compensation Method for Pronunciation Assessment",
    "volume": "main",
    "abstract": "Automatic pronunciation assessment plays an important role in computer-assisted pronunciation training (CAPT). Goodness of pronunciation (GOP) based on automatic speech recognition (ASR) has been commonly used in pronunciation assessment. It has been found that GOP normally shows deteriorating performance under noisy conditions. Traditional noise compensation methods, which compensate distorted GOP under noisy situations based on the Gaussian mixture model (GMM) or other simple mapping functions, ignore contextual influence and phonemic attributes of the utterance. This usually leads to a lack of robustness with changed conditions. In this paper, we adopt a bidirectional long short-term (BLSTM) network combining phonemic attributes to conduct the compensation for distorted GOP under noisy conditions. We evaluate the model performance based on English words recorded by Chinese learners in clean and noisy situations. Experimental results show the proposed model outperforms the traditional baselines in Pearson correlation coefficient (PCC) and accuracy for pronunciation assessment under various noisy conditions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "43af025fd77adf8cb1e63d415562d84d6760b066",
    "semantic_title": "a neural network-based noise compensation method for pronunciation assessment",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kudera21_interspeech.html": {
    "title": "Phonetic Distance and Surprisal in Multilingual Priming: Evidence from Slavic",
    "volume": "main",
    "abstract": "This study reveals the relation between surprisal, phonetic distance, and latency based on a multilingual, short-term priming framework. Four Slavic languages (Bulgarian, Czech, Polish, and Russian) are investigated across two priming conditions: associative and phonetic priming, involving true cognates and near-homophones, respectively. This research is grounded in the methodology of information theory and proposes new methods for quantifying differences between meaningful lexical primes and targets for closely related languages. It also outlines the influence of phonetic distance between cognate and noncognate pairs of primes and targets on response times in a cross-lingual lexical decision task. The experimental results show that phonetic distance moderates response times only in Polish and Czech, whereas the surprisal-based correspondence effect is an accurate predictor of latency for all tested languages. The information-theoretic approach of quantifying feature-based alternations between Slavic cognates and near-homophones appears to be a valid method for latency moderation in the auditory modality. The outcomes of this study suggest that the surprisal-based (un)expectedness of spoken stimuli is an accurate predictor of human performance in multilingual lexical decision tasks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "91c804133df7c09347ffb868cb798b5cb32076a3",
    "semantic_title": "phonetic distance and surprisal in multilingual priming: evidence from slavic",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21z_interspeech.html": {
    "title": "A Preliminary Study on Discourse Prosody Encoding in L1 and L2 English Spontaneous Narratives",
    "volume": "main",
    "abstract": "Relatively little attention has been devoted to the discourse-level prosodic encoding and speech planning in second language (L2) speech. This study reports a preliminary study on learners' discourse prosody encoding pattern and makes a comparison with that of native speakers. Using a corpus of spontaneously produced picture story narratives, we analyzed general characteristics of prosodic units (PUs) and explored relationships between pitch encoding (cross-boundary f0 heights and f0 slopes) of PUs and the semantic completeness of PUs in English spontaneous speech by native speakers, beginning learners, and advanced learners. The results indicated that beginning learners showed neither sensitivity to semantic units in discourse (DUs) in their f0 encoding nor distinct signs of pitch-related preplanning based on DUs, suggesting improper phrasing of the least proficient non-native speakers. Both native speakers and advanced learners were sensitive to the initiation and termination of DUs in their prosodic encoding; however, only native speakers showed clear signs of DU-based preplanning. We argue that the observed between-group differences in L1 and L2 speech might be attributed to differences in the scope of speech planning, i.e., compared with native speakers, who mostly produce complete semantic units, learners' speech is produced step by step with pauses between phrases",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "cefe7a434770ac379b49fbf4fa42521739f1ca9d",
    "semantic_title": "a preliminary study on discourse prosody encoding in l1 and l2 english spontaneous narratives",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21h_interspeech.html": {
    "title": "Transformer Based End-to-End Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "This paper introduces two Transformer-based architectures for Mispronunciation Detection and Diagnosis (MDD). The first Transformer architecture (T-1) is a standard setup with an encoder, a decoder, a projection part and the Cross Entropy (CE) loss. T-1 takes in Mel-Frequency Cepstral Coefficients (MFCC) as input. The second architecture (T-2) is based on wav2vec 2.0, a pretraining framework. T-2 is composed of a CNN feature encoder, several Transformer blocks capturing contextual speech representations, a projection part and the Connectionist Temporal Classification (CTC) loss. Unlike T-1, T-2 takes in raw audio data as input. Both models are trained in an end-to-end manner. Experiments are conducted on the CU-CHLOE corpus, where T-1 achieves a Phone Error Rate (PER) of 8.69% and F-measure of 77.23%; and T-2 achieves a PER of 5.97% and F-measure of 80.98%. Both models significantly outperform the previously proposed AGPM and CNN-RNN-CTC models, with PERs at 11.1% and 12.1% respectively, and F-measures at 72.61% and 74.65% respectively",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "480c3083e545c59740609e71ddcff8589d1484eb",
    "semantic_title": "transformer based end-to-end mispronunciation detection and diagnosis",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/graham21_interspeech.html": {
    "title": "L1 Identification from L2 Speech Using Neural Spectrogram Analysis",
    "volume": "main",
    "abstract": "It is well-known that the characteristics of L2 speech are highly influenced by the speakers' L1. The main objective of this study was to uncover discriminative speech features to identify the L1 background of a speaker from their L2 English speech. Traditional phonetic approaches tend to compare speakers based on a pre-selected set of acoustic features, which may not be sufficient to capture all the unique traces of the L1 in the L2 speech for forensic speaker profiling purposes. Convolutional Neural Network (CNN) has the potential to remedy this issue through the automatic processing of the visual spectrogram This paper reports a series of CNN classification experiments modelled on spectrogram images. The classification problem consisted of determining whether English speech samples are spoken by a native speaker of English, Japanese, Dutch, French, or Polish. Both phonetically transcribed and untranscribed speech data were used Overall, results showed that the CNN achieved a high level of accuracy in identifying the speakers' L1s based on spectrogram pictures without explicit phonetic segmentation. However, the results also showed that training the classifiers on certain combinations of phonetically modelled spectrogram images, which would make features more transparent, can produce results with comparable accuracy rates",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "9f045f7e0f9776caad54d6f77f03efd5412e49ca",
    "semantic_title": "l1 identification from l2 speech using neural spectrogram analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/oh21b_interspeech.html": {
    "title": "Leveraging Real-Time MRI for Illuminating Linguistic Velum Action",
    "volume": "main",
    "abstract": "Velum actions are critical to differentiating oral and nasal sounds in spoken language; specifically in the latter, the velum is lowered to open the nasal port and allow nasal airflow. However, details on how the velum is lowered for nasal production in speech are scarce. State-of-the-art real-time Magnetic Resonance Imaging (rtMRI) can directly image the entirety of the moving vocal tract, providing spatiotemporal kinematic data of articulatory actions. Most instrumental studies of speech production explore oral constriction actions such as lip or tongue movements. RtMRI makes possible a quantitative assessment of non-oral and non-constriction actions, such as velum (and larynx) dynamics. This paper illustrates articulatory aspects of consonant nasality, which have previously been inferred from acoustic or aerodynamic data. Velum actions are quantified in spatial and temporal domains: i) vertical and horizontal velum positions during nasal consonant production are quantified to measure, respectively, the degree of velum lowering and velic opening, and ii) duration intervals for velum lowering, plateau, and raising are obtained to understand which portion of the velum action is lengthened to generate phonologically long nasality. Findings demonstrate that velum action tracking using rtMRI can illuminate linguistic modulations of nasality strength and length",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "676caa5ec4c5fb5faec18cbc09d12bfa3f5e655f",
    "semantic_title": "leveraging real-time mri for illuminating linguistic velum action",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21m_interspeech.html": {
    "title": "Segmental Alignment of English Syllables with Singleton and Cluster Onsets",
    "volume": "main",
    "abstract": "Recent research has shown fresh evidence that consonant and vowel are synchronised at the syllable onset, as predicted by a number of theoretical models. The finding was made by using a minimal contrast paradigm to determine segment onset in Mandarin CV syllables, which differed from the conventional method of detecting gesture onset with a velocity threshold [1]. It has remained unclear, however, if CV co-onset also occurs between the nucleus vowel and a consonant cluster, as predicted by the articulatory syllable model [2]. This study applied the minimal contrast paradigm to British English in both CV and clusterV (CLV) syllables, and analysed the spectral patterns with signal chopping in conjunction with recurrent neural networks (RNN) with long short-term memory (LSTM) [3]. Results show that vowel onset is synchronised with the onset of the first consonant in a cluster, thus supporting the articulatory syllable model",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d1e60a666b61f707a07ed5346028156784606cbe",
    "semantic_title": "segmental alignment of english syllables with singleton and cluster onsets",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hejna21_interspeech.html": {
    "title": "Exploration of Welsh English Pre-Aspiration: How Wide-Spread is it?",
    "volume": "main",
    "abstract": "This study investigates how widespread pre-aspiration and local breathiness are in English spoken in Wales, by speakers identifying as Welsh. While the main purpose is to establish whether the phenomenon is generally present in Welsh English, the data also enables us to explore whether pre-aspiration might be conditioned by sex/gender, age, and the ability to speak Welsh. An acoustic corpus of 45 speakers producing word-final plosives and fricatives is analysed Pre-aspiration and local breathiness are produced by all speakers, representing 32 towns and 16 areas included in the analyses. Pre-aspiration and breathiness are more frequent and longer in L1 and L2 Welsh speakers than those who do not speak Welsh at all. In general, no statistically significant sex and age effects emerge In addition, a gradient allophony between pre-aspiration and glottalisation is reported for all speakers in the plosive context: the more frequently they glottalise, the less frequent the pre-aspiration. In fricatives, most speakers do not glottalise. Regarding those who do, 1. some display no relationship between pre-aspiration and glottalisation, and 2. a minority display either an indication of gradient allophony between the two, or 3. a positive correlation",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3fada251cfbb0c8e5849dcb8f9d93f53a9c55f34",
    "semantic_title": "exploration of welsh english pre-aspiration: how wide-spread is it?",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/muhlack21_interspeech.html": {
    "title": "Revisiting Recall Effects of Filler Particles in German and English",
    "volume": "main",
    "abstract": "This paper reports on two experiments that partially replicate an experiment by Fraundorf and Watson (2011, J Mem. Lang.) on the recall effect of filler particles. Their subjects listened to three passages of a story, either with or without filler particles, which they had to retell afterwards. They analysed the subjects' retelling in terms of whether important plot points were remembered or not. For their English data, they found that filler particles facilitate the recall of the plot points significantly compared to stories that did not include filler particles. As this seems to be a convincing experimental design, we aimed at evaluating this method as a web-based experiment which may, if found to be suitable, easily be applied to other languages. Furthermore, we investigated whether their results are found in German as well (Experiment 1), and evaluated whether filler duration has an effect on recall performance (Experiment 2). Our results could not replicate the findings of the original study: in fact, the opposite effect was found for German. In Experiment 1, participants performed better on recall in the fluent condition, while no significant results were found for English in Experiment 2",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3e2b49e61445a859d557f507d840975536afee2e",
    "semantic_title": "revisiting recall effects of filler particles in german and english",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ge21b_interspeech.html": {
    "title": "How Reliable Are Phonetic Data Collected Remotely? Comparison of Recording Devices and Environments on Acoustic Measurements",
    "volume": "main",
    "abstract": "The COVID-19 pandemic posed an unprecedented challenge to phonetic research. On-site collection of speech data is difficult, if not impossible. The advancement of technology in mobile devices and online conference platforms offers the opportunity to collect data remotely. This paper aims to answer the question of how reliable speech data collected remotely are based on controlled speech. Seven devices, including smartphones and laptops, were used to record speech simultaneously, locally or on the cloud using ZOOM, both in a sound-attenuated lab and a conference room. Common acoustic measurements were made on these recordings. Local recordings proved to be reliable in duration, but not for recordings made on the cloud. Different devices have comparable performances in F0 and F1. The values acquired by different devices differ a lot for F2 and higher formants, spectral moments, and voice quality measures. These differences can lead to erroneous interpretation of segmental and voice quality contrasts. The recordings made remotely by smartphones and locally using ZOOM can be useful in studying prosody, but should be used with care for segments",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "36911ff693c61a62582a745f495f42dc249f49b5",
    "semantic_title": "how reliable are phonetic data collected remotely? comparison of recording devices and environments on acoustic measurements",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/huang21i_interspeech.html": {
    "title": "A Cross-Dialectal Comparison of Apical Vowels in Beijing Mandarin, Northeastern Mandarin and Southwestern Mandarin: An EMA and Ultrasound Study",
    "volume": "main",
    "abstract": "This paper is a comparative study of the articulation of the \"apical vowels\" in three Mandarin dialects: Beijing Mandarin (BJM), Northeastern Mandarin (NEM), and Southwestern Mandarin (SWM), using co-registered EMA and ultrasound. Data from 5 BJM speakers, 5 NEM speakers and 4 SWM speakers in their twenties were analyzed and discussed. Our recording materials include the dental and retroflex apical vowels, and their -suffixed forms. Results suggest that distinct lingual configurations are found among the three dialects of Mandarin, even though these apical vowels are not perceptually distinguishable. Specifically, the dental apical vowel [ɿ] has a grooved tongue shape in BJM, a retracted tongue dorsum in NEM, and a relatively flat tongue shape in SWM. The retroflex apical vowel [ʅ] has a domed tongue shape as well as a bunched tongue body in NEM, while a slightly domed tongue posture is found in SWM. Moreover, the retroflex apical vowel [ʅ] is, articulatorily speaking, very similar to the -suffix in BJM (cf. [10]). In sum, we observed yet another instance of the articulatory-acoustic mismatch",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "14b70916970bd3ef7d28b19ce23cd35476b52df1",
    "semantic_title": "a cross-dialectal comparison of apical vowels in beijing mandarin, northeastern mandarin and southwestern mandarin: an ema and ultrasound study",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gibson21_interspeech.html": {
    "title": "Dissecting the Aero-Acoustic Parameters of Open Articulatory Transitions",
    "volume": "main",
    "abstract": "We capitalize on previously recorded kinematic and acoustic data for three languages (Georgian (GE), Spanish (SP) and Moroccan Arabic (MA)) that exhibit open articulatory transitions between the consonants in clusters in order to dissect the aero-acoustic parameters of the transitions in each language. These particular languages are of interest because they show similar patterns of interarticulatory timing in clusters, offering the unique opportunity to examine the acoustics of open transitions cross-linguistically. Our analysis centers on word initial clusters (/kl/ and /gl/), from which we extract relativized temporal values relevant to clusters and spectral parameters related to open articulatory transitions. We report baseline results using linear mixed effects models, then train a Random Forest model in a supervised learning environment on the significant variables. After training, test tokens are introduced in order to test whether the model can categorize the language based on the spectral and temporal parameters, and rank variables in terms of their feature importance. The results show that the model can categorize the data to the correct language with a 95.59% accuracy rate, where normalized zero-crossing (nzcr), modifications of the amplitude envelope (ΔE), and intensity ratio ranked highest in feature importance",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "e25bcfa3d0995b188af8f6382b480d50d4f4fe12",
    "semantic_title": "dissecting the aero-acoustic parameters of open articulatory transitions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gully21_interspeech.html": {
    "title": "Quantifying Vocal Tract Shape Variation and its Acoustic Impact: A Geometric Morphometric Approach",
    "volume": "main",
    "abstract": "The shape of the vocal tract varies considerably between individuals. The relationship between detailed variation in vocal tract shape and the acoustics of speech is not yet well understood, despite its potential for increasing understanding in the fields of voice biometrics, forensic speech science, and personalised speech synthesis. One reason that this topic has not yet been extensively explored is that 3D vocal tract shape is difficult to quantify robustly. Geometric morphometrics is a technique developed in evolutionary biology for statistically valid quantification and comparison of anatomical shapes. This study makes use of 3D magnetic resonance imaging data of the vocal tracts of eight individuals, and accompanying audio recordings, combined with geometric morphometric techniques to determine whether the method offers useful information for speech science. The results suggest a linear relationship between the shapes of the vocal tract and output spectra, and there is evidence of possible sexual dimorphism and allometry (a systematic variation of shape with size) in the vocal tract, although due to the limited sample size the results did not reach statistical significance. The results suggest that geometric morphometrics can provide useful information about the vocal tract, and justify further study using this technique",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "08d2284ee8b602442ca25c1fcbfd380ef2d78a7a",
    "semantic_title": "quantifying vocal tract shape variation and its acoustic impact: a geometric morphometric approach",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guevararukoz21_interspeech.html": {
    "title": "Speech Perception and Loanword Adaptations: The Case of Copy-Vowel Epenthesis",
    "volume": "main",
    "abstract": "Japanese allows for almost no consonants in syllable codas. In loanwords, illegal codas are transformed into onsets by means of vowel epenthesis. The default epenthetic vowel in loanwords is [ɯ], and previous work has shown that this [ɯ]-epenthesis reflects Japanese listeners' perception of illegal coda consonants. Here, we focus on one of the non-default cases: following coda [ç] and [x] the epenthetic vowel is a copy of the preceding vowel. Using an identification and a discrimination task, we provide evidence for the perceptual origin of this copy vowel phenomenon: After [ç] and [x], Japanese listeners perceive more often an epenthetic copy vowel than the default vowel [ɯ], whereas after [k] it is the reverse",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "9c31602a9c74fd1f88522652bd5d5f3b41c79a19",
    "semantic_title": "speech perception and loanword adaptations: the case of copy-vowel epenthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/guo21b_interspeech.html": {
    "title": "Speakers Coarticulate Less When Facing Real and Imagined Communicative Difficulties: An Analysis of Read and Spontaneous Speech from the LUCID Corpus",
    "volume": "main",
    "abstract": "This study investigated coarticulation of read and spontaneous speech in different communicative contexts from the LUCID corpus. Spontaneous speech samples were from Southern British English speakers who completed an interactive spot-the-differences task with no communicative barrier (NB), with their voice vocoded (VOC), and with a partner who heard their speech in babble (BABBLE) or was a non-native English speaker (L2). The same speakers also read sentences in a casual (READ-CO) and clear (READ-CL) speaking style. Tokens of a pre-defined set of keywords were extracted from the speech samples and consonant-vowel sequences in these tokens were analyzed using a whole-spectrum measure of coarticulation. Results showed that coarticulatory resistance in the six communicative contexts from highest to lowest was: BABBLE > VOC, L2, READ-CL > NB, READ-CO. Thus, in response to communicative barriers, be they real or imaginary, speakers coarticulated less, in line with the models of targeted speaker adaptations (the H&H theory [1] and Adaptive Speaker Framework [2])",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "9b7f58f1899369d71ee724d2aa93c64370a997b1",
    "semantic_title": "speakers coarticulate less when facing real and imagined communicative difficulties: an analysis of read and spontaneous speech from the lucid corpus",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/meister21_interspeech.html": {
    "title": "Developmental Changes of Vowel Acoustics in Adolescents",
    "volume": "main",
    "abstract": "The paper explores the developmental changes of vowel acoustics in Estonian adolescents as a function of age and gender. Formant frequencies F1–F4 and the duration of vowels were measured from read speech samples of 305 native Estonian subjects (173 girls and 132 boys) aged from 10 to 18 years. GAM framework was applied for the statistical analysis. The results show that both the formant frequencies and the vowel space area decrease gradually from 10 to 15 years in both gender groups and the quality of vowels stabilizes at the age of 15–18 years, whereas gender-specific differences emerge around the age of 12–13. Age-related change in the duration of vowels shows similar patterns with formants, however, with no gender difference. The findings are in line with the results reported for adolescent speech in other languages. The analysis results based on speech samples of the subjects with normal linguistic development can be considered reference data for distinguishing between normal and abnormal speech development",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "bc814be07003cb2b28fdc51164df7b7811107347",
    "semantic_title": "developmental changes of vowel acoustics in adolescents",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dapolito21_interspeech.html": {
    "title": "Context and Co-Text Influence on the Accuracy Production of Italian L2 Non-Native Sounds",
    "volume": "main",
    "abstract": "Accuracy in production of non-native sounds is analyzed by considering the influence of L1, context and co-text on Italian L2 speech. While the L1 influence is often described in the literature, careful investigations on how production accuracy may change in different contexts and co-texts are needed. This paper describes two experiments on how French learners of Italian as L2 (advanced/beginners) realize geminates depending on different contexts (the global contexts, e.g., the tasks) and co-texts (the amount of information available syntagmatically) Acoustic data acquired by recording 4 advanced and 4 beginner Italian-L2 learners (and 3 Italian natives as control) were analyzed as for the duration of the target consonant and preceding vowel, as well as speech articulation rate, taken as indexes of geminate production accuracy Results confirm the strongest influence of L1 in beginners' production, and depict a complex interplay of context and co- text. Adding information in co-text may induce different effects on speech production, depending on the local context, that is on the speakers' communication needs during speech production. Specifically, a \"rich\" co-text may favor a decrease in production accuracy or, on the contrary, an increase, depending on the need the speaker have to highlight/contrast information",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b050a93bbec7cfe476a96b715238db796969d4cb",
    "semantic_title": "context and co-text influence on the accuracy production of italian l2 non-native sounds",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/heeringa21_interspeech.html": {
    "title": "A New Vowel Normalization for Sociophonetics",
    "volume": "main",
    "abstract": "Several studies have shown that in sociophonetic research Lobanov's speaker normalization method outperforms other methods for normalizing vowel formants of speakers. An advantage of Lobanov's method compared to the method that was introduced by Watt & Fabricius in 2002 is that it is independent of the shape of the vowel space area, and also normalizes to the dispersion of the vowels. However, it does depend on the distribution of the vowels within the vowel space. When using Lobanov normalization the formant values are converted to z-scores. We present a method where the µ in the z-score formula is replaced by the center of the convex hull that encloses the vowels, and the σ is obtained on the basis of the points that constitute the convex hull. When normalizing measurements of two real data sets, and of a series of randomly generated data sets, we found that our method improved in matching vowel spaces in size and overlap",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "45311a0be2e6ae08717251a8e3274db61ce9a81d",
    "semantic_title": "a new vowel normalization for sociophonetics",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/billington21_interspeech.html": {
    "title": "The Pacific Expansion: Optimizing Phonetic Transcription of Archival Corpora",
    "volume": "main",
    "abstract": "For most of the world's languages, detailed phonetic analyses across different aspects of the sound system do not exist, due in part to limitations in available speech data and tools for efficiently processing such data for low-resource languages. Archival language documentation collections offer opportunities to extend the scope and scale of phonetic research on low-resource languages, and developments in methods for automatic recognition and alignment of speech facilitate the preparation of phonetic corpora based on these collections. We present a case study applying speech modelling and forced alignment methods to narrative data for Nafsan, an Oceanic language of central Vanuatu. We examine the accuracy of the forced-aligned phonetic labelling based on limited speech data used in the modelling process, and compare acoustic and durational measures of 17,851 vowel tokens for 11 speakers with previous experimental phonetic data for Nafsan. Results point to the suitability of archival data for large-scale studies of phonetic variation in low-resource languages, and also suggest that this approach can feasibly be used as a starting point in expanding to phonetic comparisons across closely-related Oceanic languages",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4e1b930cbd18e09c36e4c30a540ce4245eb317eb",
    "semantic_title": "the pacific expansion: optimizing phonetic transcription of archival corpora",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tian21_interspeech.html": {
    "title": "FSR: Accelerating the Inference Process of Transducer-Based Models by Applying Fast-Skip Regularization",
    "volume": "main",
    "abstract": "Transducer-based models, such as RNN-Transducer and transformer-transducer, have achieved great success in speech recognition. A typical transducer model decodes the output sequence conditioned on the current acoustic state and previously predicted tokens step by step. Statistically, The number of blank tokens in the prediction results accounts for nearly 90% of all tokens. It takes a lot of computation and time to predict the blank tokens, but only the non-blank tokens will appear in the final output sequence. Therefore, we propose a method named fast-skip regularization, which tries to align the blank position predicted by a transducer with that predicted by a connectionist temporal classification (CTC) model. During the inference, the transducer model can predict the blank tokens in advance by a simple CTC project layer without many complicated forward calculations of the transducer decoder and then skip them, which will reduce the computation and improve the inference speed greatly. All experiments are conducted on a public Chinese mandarin dataset AISHELL-1. The results show that the fast-skip regularization can indeed help the transducer model learn the blank position alignments. Besides, the inference with fast-skip can be speeded up nearly 4 times with only a little performance degradation",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ce581b268297127448efd939f3b42968b5c526cd",
    "semantic_title": "fsr: accelerating the inference process of transducer-based models by applying fast-skip regularization",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mitrofanov21_interspeech.html": {
    "title": "LT-LM: A Novel Non-Autoregressive Language Model for Single-Shot Lattice Rescoring",
    "volume": "main",
    "abstract": "Neural network-based language models are commonly used in rescoring approaches to improve the quality of modern automatic speech recognition (ASR) systems. Most of the existing methods are computationally expensive since they use autoregressive language models. We propose a novel rescoring approach, which processes the entire lattice in a single call to the model. The key feature of our rescoring policy is a novel non-autoregressive Lattice Transformer Language Model (LT-LM). This model takes the whole lattice as an input and predicts a new language score for each arc. Additionally, we propose the artificial lattices generation approach to incorporate a large amount of text data in the LT-LM training process. Our single-shot rescoring performs orders of magnitude faster than other rescoring methods in our experiments. It is more than 300 times faster than pruned RNNLM lattice rescoring and N-best rescoring while slightly inferior in terms of WER",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d23ff2d35415be2cadc4dc41c7293450efdc2188",
    "semantic_title": "lt-lm: a novel non-autoregressive language model for single-shot lattice rescoring",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/allauzen21_interspeech.html": {
    "title": "A Hybrid Seq-2-Seq ASR Design for On-Device and Server Applications",
    "volume": "main",
    "abstract": "This paper proposes and evaluates alternative speech recognition design strategies using the hybrid autoregressive transducer (HAT) model. The different strategies are designed with special attention to the choice of modeling units and to the integration of different types of external language models during first-pass beam-search or second-pass re-scoring. These approaches are compared on a large-scale voice search task and the recognition quality over the head and tail of speech data is analyzed. Our experiments show decent improvements in WER over common speech phrases and significant gains on uncommon ones compared to the state-of-the-art approaches",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "58755ae2c4ce385880f53aa3ca1efaeae139df76",
    "semantic_title": "a hybrid seq-2-seq asr design for on-device and server applications",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/inaguma21b_interspeech.html": {
    "title": "VAD-Free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording",
    "volume": "main",
    "abstract": "In this work, we propose novel decoding algorithms to enable streaming automatic speech recognition (ASR) on unsegmented long-form recordings without voice activity detection (VAD), based on monotonic chunkwise attention (MoChA) with an auxiliary connectionist temporal classification (CTC) objective. We propose a beam search decoding to take advantage of efficient batched output-synchronous and low-latency input-synchronous searches. We also propose a VAD-free inference algorithm that leverages CTC probabilities to determine a suitable timing to reset the model states to tackle the vulnerability to long-form data. Experimental evaluations demonstrate that the block-synchronous decoding achieves comparable accuracy to the label-synchronous one. Moreover, the VAD-free inference can recognize long-form speech robustly for up to a few hours",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "efd6a3a519eb06a0135e5883c378affaf300de52",
    "semantic_title": "vad-free streaming hybrid ctc/attention asr for unsegmented recording",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yao21_interspeech.html": {
    "title": "WeNet: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit",
    "volume": "main",
    "abstract": "In this paper, we propose an open source speech recognition toolkit called WeNet, in which a new two-pass approach named U2 is implemented to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. The main motivation of WeNet is to close the gap between the research and deployment of E2E speech recognition models. WeNet provides an efficient way to ship automatic speech recognition (ASR) applications in real-world scenarios, which is the main difference and advantage to other open source E2E speech recognition toolkits. We develop a hybrid connectionist temporal classification (CTC)/attention architecture with transformer or conformer as encoder and an attention decoder to rescore the CTC hypotheses. To achieve streaming and non-streaming in a unified model, we use a dynamic chunk-based attention strategy which allows the self-attention to focus on the right context with random length. Our experiments on the AISHELL-1 dataset show that our model achieves 5.03% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. After model quantification, our model achieves reasonable RTF and latency at runtime. The toolkit is publicly available",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "08bf462f138c6c4c292d882bb6b760d5aedc4eab",
    "semantic_title": "wenet: production oriented streaming and non-streaming end-to-end speech recognition toolkit",
    "citation_count": 119,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanaka21b_interspeech.html": {
    "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "We propose a cross-modal transformer-based neural correction models that refines the output of an automatic speech recognition (ASR) system so as to exclude ASR errors. Generally, neural correction models are composed of encoder-decoder networks, which can directly model sequence-to-sequence mapping problems. The most successful method is to use both input speech and its ASR output text as the input contexts for the encoder-decoder networks. However, the conventional method cannot take into account the relationships between these two different modal inputs because the input contexts are separately encoded for each modal. To effectively leverage the correlated information between the two different modal inputs, our proposed models encode two different contexts jointly on the basis of cross-modal self-attention using a transformer. We expect that cross-modal self-attention can effectively capture the relationships between two different modals for refining ASR hypotheses. We also introduce a shallow fusion technique to efficiently integrate the first-pass ASR model and our proposed neural correction model. Experiments on Japanese natural language ASR tasks demonstrated that our proposed models achieve better ASR performance than conventional neural correction models",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "0a11b53da3146f8048e6dec39bbc27f86d013762",
    "semantic_title": "cross-modal transformer-based neural correction models for automatic speech recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21f_interspeech.html": {
    "title": "Deep Neural Network Calibration for E2E Speech Recognition System",
    "volume": "main",
    "abstract": "Cross-entropy loss, which is commonly used in deep-neural-network-based (DNN) classification model training, induces models to assign a high probability value to one class. Networks trained in this fashion tend to be overconfident, which causes a problem in the decoding process of the speech recognition system, as it uses the combined probability distribution of multiple independently trained networks. Overconfidence in neural networks can be quantified as a calibration error, which is the difference between the output probability of a model and the likelihood of obtaining an actual correct answer. We show that the deep-learning-based components of an end-to-end (E2E) speech recognition system with high classification accuracy contain calibration errors and quantify them using various calibration measures. In addition, it was experimentally shown that the calibration function, which was being trained to minimize calibration errors effectively mitigates those of the speech recognition system, and as a result, can improve the performance of beam-search during decoding",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2083b71ea095ebcc21c9465a72f44ce395b4e0f7",
    "semantic_title": "deep neural network calibration for e2e speech recognition system",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21m_interspeech.html": {
    "title": "Residual Energy-Based Models for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end models with auto-regressive decoders have shown impressive results for automatic speech recognition (ASR). These models formulate the sequence-level probability as a product of the conditional probabilities of all individual tokens given their histories. However, the performance of locally normalised models can be sub-optimal because of factors such as exposure bias. Consequently, the model distribution differs from the underlying data distribution. In this paper, the residual energy-based model (R-EBM) is proposed to complement the auto-regressive ASR model to close the gap between the two distributions. Meanwhile, R-EBMs can also be regarded as utterance-level confidence estimators, which may benefit many downstream tasks. Experiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word error rates (WERs) by 8.2%/6.7% while improving areas under precision-recall curves of confidence scores by 12.6%/28.4% on test-clean/test-other sets. Furthermore, on a state-of-the-art model using self-supervised learning (wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence estimation performance",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "560a1af3be61b26c4f7bccbb7b520baa090a7b92",
    "semantic_title": "residual energy-based models for end-to-end speech recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiu21b_interspeech.html": {
    "title": "Multi-Task Learning for End-to-End ASR Word and Utterance Confidence with Deletion Prediction",
    "volume": "main",
    "abstract": "Confidence scores are very useful for downstream applications of automatic speech recognition (ASR) systems. Recent works have proposed using neural networks to learn word or utterance confidence scores for end-to-end ASR. In those studies, word confidence by itself does not model deletions, and utterance confidence does not take advantage of word-level training signals. This paper proposes to jointly learn word confidence, word deletion, and utterance confidence. Empirical results show that multi-task learning with all three objectives improves confidence metrics (NCE, AUC, RMSE) without the need for increasing the model size of the confidence estimation module. Using the utterance-level confidence for rescoring also decreases the word error rates on Google's Voice Search and Long-tail Maps datasets by 3–5% relative, without needing a dedicated neural rescorer",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f21bef4b03c5dcd4af012cb80f60fcd0377905fa",
    "semantic_title": "multi-task learning for end-to-end asr word and utterance confidence with deletion prediction",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ollerenshaw21_interspeech.html": {
    "title": "Insights on Neural Representations for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (ASR) models aim to learn a generalised speech representation. However, there are limited tools available to understand the internal functions and the effect of hierarchical dependencies within the model architecture. It is crucial to understand the correlations between the layer-wise representations, to derive insights on the relationship between neural representations and performance. Previous investigations of network similarities using correlation analysis techniques have not been explored for End-to-End ASR models. This paper analyses and explores the internal dynamics between layers during training with CNN, LSTM and Transformer based approaches using Canonical correlation analysis (CCA) and centered kernel alignment (CKA) for the experiments. It was found that neural representations within CNN layers exhibit hierarchical correlation dependencies as layer depth increases but this is mostly limited to cases where neural representation correlates more closely. This behaviour is not observed in LSTM architecture, however there is a bottom-up pattern observed across the training process, while Transformer encoder layers exhibit irregular coefficiency correlation as neural depth increases. Altogether, these results provide new insights into the role that neural architectures have upon speech recognition performance. More specifically, these techniques can be used as indicators to build better performing speech recognition models",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "bed8f6f57d4121533217823814ef1155cb3ae17d",
    "semantic_title": "insights on neural representations for end-to-end speech recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/afshan21_interspeech.html": {
    "title": "Sequence-Level Confidence Classifier for ASR Utterance Accuracy and Application to Acoustic Models",
    "volume": "main",
    "abstract": "Scores from traditional confidence classifiers (CCs) in automatic speech recognition (ASR) systems lack universal interpretation and vary with updates to the underlying confidence or acoustic models (AMs). In this work, we build interpretable confidence scores with an objective to closely align with ASR accuracy. We propose a new sequence-level CC with a richer context providing CC scores highly correlated with ASR accuracy and scores stable across CC updates. Hence, expanding CC applications. Recently, AM customization has gained traction with the widespread use of unified models. Conventional adaptation strategies that customize AM expect well-matched data for the target domain with gold-standard transcriptions. We propose a cost-effective method of using CC scores to select an optimal adaptation data set, where we maximize ASR gains from minimal data. We study data in various confidence ranges and optimally choose data for AM adaptation with KL-Divergence regularization. On the Microsoft voice search task, data selection for supervised adaptation using the sequence-level confidence scores achieves word error rate reduction (WERR) of 8.5% for row-convolution LSTM (RC-LSTM) and 5.2% for latency-controlled bidirectional LSTM (LC-BLSTM). In the semi-supervised case, with ASR hypotheses as labels, our method provides WERR of 5.9% and 2.8% for RC-LSTM and LC-BLSTM, respectively",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "93b1e45009008a7a6bec5ecdac67bb2fa2f1dba6",
    "semantic_title": "sequence-level confidence classifier for asr utterance accuracy and application to acoustic models",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tjandra21_interspeech.html": {
    "title": "Unsupervised Learning of Disentangled Speech Content and Style Representation",
    "volume": "main",
    "abstract": "Speech is influenced by a number of underlying factors, which can be broadly categorized into linguistic contents and speaking styles. However, collecting the labeled data that annotates both content and style is an expensive and time-consuming task. Here, we present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of: (1) a local encoder that captures per-frame information; (2) a global encoder that captures per-utterance information; and (3) a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that (1) the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates (WER), even with a different global encoding; (2) the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate a useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7794450325f38e14d6c0e9d1343ff961f7c1a049",
    "semantic_title": "unsupervised learning of disentangled speech content and style representation",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/choi21_interspeech.html": {
    "title": "Label Embedding for Chinese Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Chinese grapheme-to-phoneme (G2P) conversion plays a significant role in text-to-speech systems by generating pronunciations corresponding to Chinese input characters. The main challenge in Chinese G2P conversion is polyphone disambiguation, which requires selecting the appropriate pronunciation among several candidates. In polyphone disambiguation, calculating probabilities for the entire pronunciations is unnecessary since each Chinese character has only a few (mostly two or three) candidate pronunciations. In this study, we introduce a label embedding approach that matches the character embedding with the closest label embedding among the possible candidates. Specifically, negative sampling and triplet loss were applied to maximize the difference between the correct embedding and the other candidate embeddings. Experimental results show that the label embedding approach improved the polyphone disambiguation accuracy by 4.50% and 1.74% on two datasets compared to the one-hot label classification approach. Moreover, the bidirectional long short-term memory model with the label embedding approach outperformed the previous most advanced model, BERT, demonstrating outstanding performance in polyphone disambiguation. Lastly, we discuss the effect of contextual information in character embeddings on the G2P conversion task",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "358e9fd8ee370db0459d28602c12cbb1a0add253",
    "semantic_title": "label embedding for chinese grapheme-to-phoneme conversion",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21aa_interspeech.html": {
    "title": "PDF: Polyphone Disambiguation in Chinese by Using FLAT",
    "volume": "main",
    "abstract": "Polyphone disambiguation is an essential procedure in the front-end module of the Chinese text-to-speech (TTS) system. It serves to predict the pronunciation of the input polyphonic character. In the Chinese TTS system, a well-designed pronunciation dictionary plays a crucial role in supplying pinyin to words. However, the conventional system is unable to fully utilize the pronunciation dictionary while modelling because of the unavoidable Chinese segment errors and model structure. In this paper, we proposed a system named PDF: Polyphone Disambiguation by using FLAT. The proposed model encodes both the input character sequence and dictionary matched words of the sentence, enabling the model to both avoid segment errors and leverage the well-designed pronunciation dictionary in the model. Additionally, we also use the pre-trained language model (PLM) as an encoder to extract the contextual information of input sequence. The experimental results verified the effectiveness of the proposed PDF model. Our system obtains an improvement in accuracy by 0.98% compared to Bert on an open-source dataset. The experiential results demonstrate that leveraging pronunciation dictionary while modelling helps improve the performance of polyphone disambiguation system",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c3ffc6c30179eb7ef36a99aab626e6f9a78c619e",
    "semantic_title": "pdf: polyphone disambiguation in chinese by using flat",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21n_interspeech.html": {
    "title": "Improving Polyphone Disambiguation for Mandarin Chinese by Combining Mix-Pooling Strategy and Window-Based Attention",
    "volume": "main",
    "abstract": "In this paper, we propose a novel system based on word-level features and window-based attention for polyphone disambiguation, which is a fundamental task for Grapheme-to-phoneme (G2P) conversion of Mandarin Chinese. The framework aims to combine a pre-trained language model with explicit word-level information in order to get meaningful context extraction. Particularly, we employ a pre-trained bidirectional encoder from Transformers (BERT) model to extract character-level features, and an external Chinese word segmentation (CWS) tool is used to obtain the word units. We adopt a mixed pooling mechanism to convert character-level features into word-level features based on the segmentation results. A window-based attention module is utilized to incorporate contextual word-level features for the polyphonic characters. Experimental results show that our method achieves an accuracy of 99.06% on an open benchmark dataset for Mandarin Chinese polyphone disambiguation, which outperforms the baseline systems",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "394221ab30d1cce722fadb6cac86980f5d9efffa",
    "semantic_title": "improving polyphone disambiguation for mandarin chinese by combining mix-pooling strategy and window-based attention",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21d_interspeech.html": {
    "title": "Polyphone Disambiguation in Mandarin Chinese with Semi-Supervised Learning",
    "volume": "main",
    "abstract": "The majority of Chinese characters are monophonic, while a special group of characters, called polyphonic characters, have multiple pronunciations. As a prerequisite of performing speech-related generative tasks, the correct pronunciation must be identified among several candidates. This process is called Polyphone Disambiguation. Although the problem has been well explored with both knowledge-based and learning-based approaches, it remains challenging due to the lack of publicly available labeled datasets and the irregular nature of polyphone in Mandarin Chinese. In this paper, we propose a novel semi-supervised learning (SSL) framework for Mandarin Chinese polyphone disambiguation that can potentially leverage unlimited unlabeled text data. We explore the effect of various proxy labeling strategies including entropy-thresholding and lexicon-based labeling. Qualitative and quantitative experiments demonstrate that our method achieves state-of-the-art performance. In addition, we publish a novel dataset specifically for the polyphone disambiguation task to promote further researches",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4929bde24e5cfa523ebf2c904c5323649c6e7247",
    "semantic_title": "polyphone disambiguation in mandarin chinese with semi-supervised learning",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21s_interspeech.html": {
    "title": "A Neural-Network-Based Approach to Identifying Speakers in Novels",
    "volume": "main",
    "abstract": "Identifying speakers in novels aims at determining who says a quote in a given context by text analysis. This task is important for speech synthesis systems to assign appropriate voices to the quotes when producing audiobooks. However, existing approaches stick with manual features and traditional machine learning classifiers, which constrain the accuracy of speaker identification. In this paper, we propose a method to tackle this challenging problem with the help of deep learning. We formulate speaker identification as a scoring task and build a candidate scoring network (CSN) based on BERT. Candidate-specific segments are put forward to eliminate redundant context information. Moreover, a revision algorithm is designed utilizing the speaker alternation pattern in two-party dialogues. Experiments have been conducted using the dataset built on the Chinese novel The results show that our proposed method reaches a new state-of-the-art performance with an identification accuracy of 82.5%, which outperforms the baseline using manual features by 12%",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "988e8a17ec8900f1028d9f7b36759d34377cee84",
    "semantic_title": "a neural-network-based approach to identifying speakers in novels",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21f_interspeech.html": {
    "title": "UnitNet-Based Hybrid Speech Synthesis",
    "volume": "main",
    "abstract": "This paper presents a hybrid speech synthesis method based on UnitNet, a unified sequence-to-sequence (Seq2Seq) acoustic model for both statistical parametric speech synthesis (SPSS) and concatenative speech synthesis (CSS). This method combines CSS and SPSS approaches to synthesize different segments in an utterance. Comparing with the Tacotron2 model for Seq2Seq speech synthesis, UnitNet utilizes the phone boundaries of training data and its decoder contains autoregressive structures at both phone and frame levels. This hierarchical architecture can not only extract embedding vectors for representing phone-sized units in the corpus but also measure the dependency among consecutive units, which makes UnitNet capable of guiding the selection of phone-sized units for CSS. Furthermore, hybrid synthesis can be achieved by integrating the units generated by SPSS into the framework of CSS for the target phones without appropriate candidates in the corpus. Experimental results show that UnitNet can achieve comparable naturalness with Tacotron2 for SPSS and outperform our previous Tacotron-based method for CSS. Besides, the naturalness and inference efficiency of SPSS can be further improved through hybrid synthesis",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8ed0d2068fd5b383ec11277914f8a82ba378e7ba",
    "semantic_title": "unitnet-based hybrid speech synthesis",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/novitasari21_interspeech.html": {
    "title": "Dynamically Adaptive Machine Speech Chain Inference for TTS in Noisy Environment: Listen and Speak Louder",
    "volume": "main",
    "abstract": "Although machine speech chains were originally proposed to mimic a closed-loop human speech chain mechanism with auditory feedback, the existing machine speech chains are only utilized as a semi-supervised learning method that allows automatic speech recognition (ASR) and text-to-speech synthesis systems (TTS) to support each other given unpaired data. During inference, however, ASR and TTS are still performed separately. This paper focuses on machine speech chain inferences in a noisy environment. In human communication, speakers tend to talk more loudly in noisy environments, a phenomenon known as the Lombard effect. Simulating the Lombard effect, we implement a machine speech chain that enables TTS to speak louder in a noisy condition given auditory feedback. The auditory feedback includes speech-to-noise ratio prediction and ASR loss as a speech intelligibility measurement. To the best of our knowledge, this is the first deep learning framework that mimics human speech perception and production behaviors in a noisy environment",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "477f2152ee9a6dc6ad11d45e8e02727f3d91d2bc",
    "semantic_title": "dynamically adaptive machine speech chain inference for tts in noisy environment: listen and speak louder",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ba_interspeech.html": {
    "title": "LinearSpeech: Parallel Text-to-Speech with Linear Complexity",
    "volume": "main",
    "abstract": "Non-autoregressive text to speech models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. However, the memory and time complexity of self-attention hinders FastSpeech from generating long sequences, where is the length of mel-spectrograms. In this work, we propose LinearSpeech, an efficient parallel text-to-speech model with memory and computational complexity Firstly, we replace standard attention modules in decoder of the model with linear attention modules to reduce the time and memory cost. Secondly, we add a novel positional encoding to standard and linear attention modules, which enable the model to learn the order of input sequence and synthesizing long mel-spectrograms. Furthermore, we use reversible residual layers instead of the standard residuals, which reduce the memory consumption in training stage. In our experiments, LinearSpeech can be trained with doubled batch size than FastSpeech with similar number of parameters. At inference, LinearSpeech achieves more than 2.0× inference speedup on CPU when synthesizing mel-spectrograms longer than 3,500. And our model can synthesize 5.5× longer mel-spectrograms than FastSpeech when running out of 12GB GPU memory. Our subjective listening test also shows that the speech quality of LinearSpeech is comparable to FastSpeech",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "1b45714103c276ebadb12cfe0a0d08f0667d98e5",
    "semantic_title": "linearspeech: parallel text-to-speech with linear complexity",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mansbach21_interspeech.html": {
    "title": "An Agent for Competing with Humans in a Deceptive Game Based on Vocal Cues",
    "volume": "main",
    "abstract": "In this work we present the development of an autonomous agent capable of competing with humans in a deception-based game. The agent predicts whether a given statement is true or false based on vocal cues. To this end, we develop a game for collecting a large scale and high quality labeled sound data-set in a controlled environment in English and Hebrew. We develop a model that can detect deception based on vocal statements from the participants of the experiment, and show that the model is more accurate than humans We develop an agent that uses the developed deception model and interacts with humans within our deceptive environment. We show that our agent significantly outperforms a simple agent that does not use the deception model; that is, it wins significantly more games when played against human players. In addition, we use our model to detect whether a statement will be perceived as a lie or not by human subjects, based on its vocal cues",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "bf2ec001d4d8377293e61d2de3ba2014735a2734",
    "semantic_title": "an agent for competing with humans in a deceptive game based on vocal cues",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fakhry21_interspeech.html": {
    "title": "A Multi-Branch Deep Learning Network for Automated Detection of COVID-19",
    "volume": "main",
    "abstract": "Fast and affordable solutions for COVID-19 testing are necessary to contain the spread of the global pandemic and help relieve the burden on medical facilities. Currently, limited testing locations and expensive equipment pose difficulties for individuals seeking testing, especially in low-resource settings. Researchers have successfully presented models for detecting COVID-19 infection status using audio samples recorded in clinical settings, suggesting that audio-based Artificial Intelligence models can be used to identify COVID-19. Such models have the potential to be deployed on smartphones for fast, widespread, and low-resource testing. However, while previous studies have trained models on cleaned audio samples collected mainly from clinical settings, audio samples collected from average smartphones may yield suboptimal quality data that is different from the clean data that models were trained on. This discrepancy may add a bias that affects COVID-19 status predictions. To tackle this issue, we propose a multi-branch deep learning network that is trained and tested on crowdsourced data where most of the data has not been manually processed and cleaned. Furthermore, the model achieves state-of-art results for the COUGHVID dataset. After breaking down results for each category, we have shown an AUC of 0.99 for audio samples with COVID-19 positive labels",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "0bab6a4897992310b924c952fef02fc22d5ca46e",
    "semantic_title": "virufy: a multi-branch deep learning network for automated detection of covid-19",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ma21d_interspeech.html": {
    "title": "RW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform",
    "volume": "main",
    "abstract": "In recent years, synthetic speech generated by advanced text-to-speech (TTS) and voice conversion (VC) systems has caused great harms to automatic speaker verification (ASV) systems, urging us to design a synthetic speech detection system to protect ASV systems. In this paper, we propose a new speech anti-spoofing model named ResWavegram-Resnet (RW-Resnet). The model contains two parts, Conv1D Resblocks and backbone Resnet34. The Conv1D Resblock is based on the Conv1D block with a residual connection. For the first part, we use the raw waveform as input and feed it to the stacked Conv1D Resblocks to get the ResWavegram. Compared with traditional methods, ResWavegram keeps all the information from the audio signal and has a stronger ability in extracting features. For the second part, the extracted features are fed to the backbone Resnet34 for the spoofed or bonafide decision. The ASVspoof2019 logical access (LA) corpus is used to evaluate our proposed RW-Resnet. Experimental results show that the RW-Resnet achieves better performance than other state-of-the-art anti-spoofing models, which illustrates its effectiveness in detecting synthetic speech attacks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "9b0050ef1fa072d3d5b66876dc445b94facfbc4c",
    "semantic_title": "rw-resnet: a novel speech anti-spoofing model using raw waveform",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dhamyal21_interspeech.html": {
    "title": "Fake Audio Detection in Resource-Constrained Settings Using Microfeatures",
    "volume": "main",
    "abstract": "Fake audio generation has undergone remarkable improvement with the advancement in deep neural network models. This has made it increasingly important to develop lightweight yet robust mechanisms for detecting fake audios, especially for resource-constrained settings such as on edge devices and embedded controllers as well as with low-resource languages. In this paper, we analyze two : Voicing Onset Time (VOT) and coarticulation, to classify bonafide and synthesized audios. Using the ASVSpoof2019 LA dataset, we find that on average, VOT is higher in synthesized speech compared to bonafide speech and exhibits higher variance for multiple occurrences of the same stop consonants. Further, we observe that vowels in CVC form in bonafide speech have greater F1/F2 movement compared to similarly constrained vowels in synthesized speech. We also analyse the predictive power of VOT and coarticulation for detecting bonafide and synthesized speech and achieve equal error rates of 25.2% using VOT, 39.3% using coarticulation, and 23.5% using a fusion of both models. This is the first study analysing VOT and coarticulation as features for fake audio detection. We suggest these microfeatures as standalone features for speaker-dependent forensics, voice-biometrics, and for rapid pre-screening of suspicious audios, and as additional features in bigger feature sets for computationally intensive classifiers",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3aef4037c4bb4ae8a1110f7978ff0a6c2bdeb5e3",
    "semantic_title": "fake audio detection in resource-constrained settings using microfeatures",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21c_interspeech.html": {
    "title": "Coughing-Based Recognition of Covid-19 with Spatial Attentive ConvLSTM Recurrent Neural Networks",
    "volume": "main",
    "abstract": "The rapid emergence of COVID-19 has become a major public health threat around the world. Although early detection is crucial to reduce its spread, the existing diagnostic methods are still insufficient in bringing the pandemic under control. Thus, more sophisticated systems, able to easily identify the infection from a larger variety of symptoms, such as cough, are urgently needed. Deep learning models can indeed convey numerous signal features relevant to fight against the disease; yet, the performance of state-of-the-art approaches is still severely restricted by the feature information loss typically due to the high number of layers. To mitigate this phenomenon, identifying the most relevant feature areas by drawing into attention mechanisms becomes essential. In this paper, we introduce Spatial Attentive ConvLSTM-RNN (SACRNN), a novel algorithm that is using Convolutional Long-Short Term Memory Recurrent Neural Networks with embedded attention that has the ability to identify the most valuable features. The promising results achieved by the fusion between the proposed model and a conventional Attentive Convolutional Recurrent Neural Network, on the automatic recognition of COVID-19 coughing (73.2% of Unweighted Average Recall) show the great potential of the presented approach in developing efficient solutions to defeat the pandemic",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "1a4324d7396a0d1f8b1e09a9f1331c03b85984e0",
    "semantic_title": "coughing-based recognition of covid-19 with spatial attentive convlstm recurrent neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/paul21b_interspeech.html": {
    "title": "Knowledge Distillation for Singing Voice Detection",
    "volume": "main",
    "abstract": "Singing Voice Detection (SVD) has been an active area of research in music information retrieval (MIR). Currently, two deep neural network-based methods, one based on CNN and the other on RNN, exist in literature that learn optimized features for the voice detection (VD) task and achieve state-of-the-art performance on common datasets. Both these models have a huge number of parameters (1.4M for CNN and 65.7K for RNN) and hence not suitable for deployment on devices like smartphones or embedded sensors with limited capacity in terms of memory and computation power. The most popular method to address this issue is known as knowledge distillation in deep learning literature (in addition to model compression) where a large pre-trained network known as the teacher is used to train a smaller student network. Given the wide applications of SVD in music information retrieval, to the best of our knowledge, model compression for practical deployment has not yet been explored. In this paper, efforts have been made to investigate this issue using both conventional as well as ensemble knowledge distillation techniques",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "dea4269d6dbfcb3656926675d44585020139b794",
    "semantic_title": "knowledge distillation for singing voice detection",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/takeda21_interspeech.html": {
    "title": "Age Estimation with Speech-Age Model for Heterogeneous Speech Datasets",
    "volume": "main",
    "abstract": "This paper describes an age estimation method from speech signals for heterogeneous datasets. Although previous studies in the speech field evaluate age prediction models with held-out testing data within the same dataset recorded in a consistent setting, such evaluation does not measure real performance. The difficulty of heterogeneous datasets is overfitting caused by the corpus-specific properties: transfer function of the recording environment and distributions of age and speaker. We propose a speech-age model and its integration with sequence neural networks (NNs). The speech-age model represents the ambiguity of age as a probability distribution, which also virtually extends the limited range of age distribution of each corpus. A Bayesian generative model successfully integrates the speech-age model and the NNs. We also applied mean normalization technique to cope with the transfer function problem. Experiments showed that our proposed method outperformed the baseline neural classifier for completely open test sets in the age distribution and recording setting",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c9cbf5e9389b152573469866d3324910dcf2fd56",
    "semantic_title": "age estimation with speech-age model for heterogeneous speech datasets",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/teh21_interspeech.html": {
    "title": "Open-Set Audio Classification with Limited Training Resources Based on Augmentation Enhanced Variational Auto-Encoder GAN with Detection-Classification Joint Training",
    "volume": "main",
    "abstract": "In this paper, we propose a novel method to address practical problems when deploying audio classification systems in operations that are the presence of unseen sound classes (open-set) and the limitation of training resources. To solve it, a novel method which embeds variational auto-encoder (VAE), data augmentation and detection-classification joint training into conventional GAN networks is proposed. The VAE input to GAN-generator helps to generate realistic outlier samples which are not too far from in-distribution class and hence improve the open-set discrimination capabilities of classifiers. Next, the augmentation enhanced GAN scheme developed in our previous work [4] for close-set audio classification, will help to address the limited training resources by in cooperating the physical data augmentation to work together with traditional GAN produced samples to prevent overfitting and improve the optimization convergences. The detection-classification joint training further steps on advantages of VAE and Augmentation GAN to further improving the performances of detection and classification tasks. The experiments carried out on Google Speech Command database show great improvements of open-set classification accuracy from 62.41% to 88.29% when using only 10% amount of training data",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ce1dfbd9b83163763797c2c149b6bb657f75cafb",
    "semantic_title": "open-set audio classification with limited training resources based on augmentation enhanced variational auto-encoder gan with detection-classification joint training",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/fukumori21_interspeech.html": {
    "title": "Deep Spectral-Cepstral Fusion for Shouted and Normal Speech Classification",
    "volume": "main",
    "abstract": "Discrimination between shouted and normal speech is crucial in audio surveillance and monitoring. Although deep neural networks are used in recent methods, traditional low-level speech features are applied, such as mel-frequency cepstral coefficients and the mel spectrum. This paper presents a deep spectral-cepstral fusion approach that learns descriptive features for target classification from high-dimensional spectrograms and cepstrograms. We compare the following three types of architectures as base networks: convolutional neural networks (CNNs), gated recurrent unit (GRU) networks, and their combination (CNN-GRU). Using a corpus comprising real shouts and speech, we present a comprehensive comparison with conventional methods to verify the effectiveness of the proposed feature learning method. The results of experiments conducted in various noisy environments demonstrate that the CNN-GRU based on our spectral-cepstral features achieves better classification performance than single feature-based networks. This finding suggests the effectiveness of using high-dimensional sources for speech-type recognition in sound event detection",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d47c59de773faaef89e16f8100c623ffe27306e5",
    "semantic_title": "deep spectral-cepstral fusion for shouted and normal speech classification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/baghel21_interspeech.html": {
    "title": "Automatic Detection of Shouted Speech Segments in Indian News Debates",
    "volume": "main",
    "abstract": "Shouted speech detection is an essential pre-processing step in conventional speech processing systems such as speech and speaker recognition, speaker diarization, and others. Excitation source plays an important role in shouted speech production. This work explores feature computed from the Integrated Linear Prediction Residual (ILPR) signal for shouted speech detection in Indian news debates. The log spectrogram of ILPR signal provides time-frequency characteristics of excitation source signal. The proposed shouted speech detection system is deep network with CNN-based autoencoder and attention-based classifier sub-modules. The Autoencoder sub-network aids the classifier in learning discriminative deep embeddings for better classification. The proposed classifier is equipped with attention mechanism and Bidirectional Gated Recurrent Units. Classification results show that the proposed system with excitation feature performs better than baseline log spectrogram computed from the pre-emphasized speech signal. A score-level fusion of the classifiers trained on the source feature and the baseline feature provides the best performance. The performance of the proposed shouted speech detection is also evaluated at various speech segment durations",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fabc5b64deed094793786c1f18e0f00c4ea89ba9",
    "semantic_title": "automatic detection of shouted speech segments in indian news debates",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21c_interspeech.html": {
    "title": "Generalized Spoofing Detection Inspired from Audio Generation Artifacts",
    "volume": "main",
    "abstract": "State-of-the-art methods for audio generation suffer from fingerprint artifacts and repeated inconsistencies across temporal and spectral domains. Such artifacts could be well captured by the frequency domain analysis over the spectrogram. Thus, we propose a novel use of long-range spectro-temporal modulation feature — 2D DCT over log-Mel spectrogram for the audio deepfake detection. We show that this feature works better than log-Mel spectrogram, CQCC, MFCC, as a suitable candidate to capture such artifacts. We employ spectrum augmentation and feature normalization to decrease overfitting and bridge the gap between training and test dataset along with this novel feature introduction. We developed a CNN-based baseline that achieved a 0.0849 t-DCF and outperformed the previously top single systems reported in the ASVspoof 2019 challenge. Finally, by combining our baseline with our proposed 2D DCT spectro-temporal feature, we decrease the t-DCF score down by 14% to 0.0737, making it a state-of-the-art system for spoofing detection. Furthermore, we evaluate our model using two external datasets, showing the proposed feature's generalization ability. We also provide analysis and ablation studies for our proposed feature and results",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ad8974fe68fe5033265c02a8c2d8a26e055f8baa",
    "semantic_title": "generalized spoofing detection inspired from audio generation artifacts",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21t_interspeech.html": {
    "title": "Overlapped Speech Detection Based on Spectral and Spatial Feature Fusion",
    "volume": "main",
    "abstract": "Overlapped speech is widely present in conversations and can cause significant performance degradation on speech processing such as diarization, enhancement, and recognition. Detection of overlapped speech, in particular when the speakers are in the far-field, is a challenging task as the overlapped part is usually short, and heavy reverberation and noise may present in the conversation scenario. Existing solutions overwhelmingly rely on spectral features extracted from single microphone signal to perform the detection. In this paper, we propose a novel detection approach which is able to use a microphone array and fuse the spatial and spectral features extracted from multi-channel array signal. Two categories of spatial features, directional statistics which are projected to spherical location grids and generalized cross-correlation function based on phase transform (GCC-PHAT), are considered to model the speaker's spatial characteristic. Such spatial features are then fused with the spectral features to detect the overlapped speech by using a Gated Multimodal Unit (GMU). The performance of the proposed approach is studied under AMI and CHiME-6 corpora. Experimental results show that the proposed feature fusion approach achieves better performance than methods using spectral features only",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3d19d1fb0a0e0296983ca115b531627bd688e606",
    "semantic_title": "overlapped speech detection based on spectral and spatial feature fusion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/abdullah21_interspeech.html": {
    "title": "Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study",
    "volume": "main",
    "abstract": "Several variants of deep neural networks have been successfully employed for building parametric models that project variable-duration spoken word segments onto fixed-size vector representations, or acoustic word embeddings (AWEs). However, it remains unclear to what degree we can rely on the distance in the emerging AWE space as an estimate of word-form similarity. In this paper, we ask: does the distance in the acoustic embedding space correlate with phonological dissimilarity? To answer this question, we empirically investigate the performance of supervised approaches for AWEs with different neural architectures and learning objectives. We train AWE models in controlled settings for two languages (German and Czech) and evaluate the embeddings on two tasks: word discrimination and phonological similarity. Our experiments show that (1) the distance in the embedding space in the best cases only moderately correlates with phonological distance, and (2) improving the performance on the word discrimination task does not necessarily yield models that better reflect word phonological similarity. Our findings highlight the necessity to rethink the current intrinsic evaluations for AWEs",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4af326abbd9c1937e8ec1a6fcc7e76c600e2652d",
    "semantic_title": "do acoustic word embeddings capture phonological similarity? an empirical study",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21d_interspeech.html": {
    "title": "Paraphrase Label Alignment for Voice Application Retrieval in Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) smart assistants such as Amazon Alexa host hundreds of thousands of voice applications (skills) to delight end-users and fulfill their utterance requests. Sometimes utterances fail to be claimed by smart assistants due to system problems such as model incapability or routing errors. The failure may lead to customer frustration, dialog termination and eventually cause customer churn. To avoid this, we design a skill retrieval system as a downstream service to suggest fallback skills to unclaimed utterances. If the suggested skill satisfies customer intent, the conversation will be recovered with the assistant. For the sake of smooth customer experience, we only present the most relevant skill to customers, resulting in partial observation problem which constrains retrieval model training. To solve this problem, we propose a two-step approach to automatically align claimed utterance labels to unclaimed utterances. Extensive experiments on two real-world datasets demonstrate that our proposed model significantly outperforms a number of strong alternatives",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fa1da1ed8bb87591990df74962461807875fc6e7",
    "semantic_title": "paraphrase label alignment for voice application retrieval in spoken language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/rikhye21_interspeech.html": {
    "title": "Personalized Keyphrase Detection Using Speaker and Environment Information",
    "volume": "main",
    "abstract": "In this paper, we introduce a streaming keyphrase detection system that can be easily customized to accurately detect any phrase composed of words from a large vocabulary. The system is implemented with an end-to-end trained automatic speech recognition (ASR) model and a text-independent speaker verification model. To address the challenge of detecting these keyphrases under various noisy conditions, a speaker separation model is added to the feature frontend of the speaker verification model, and an adaptive noise cancellation (ANC) algorithm is included to exploit cross-microphone noise coherence. Our experiments show that the text-independent speaker verification model largely reduces the false triggering rate of the keyphrase detection, while the speaker separation model and adaptive noise cancellation largely reduce false rejections",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7edfe3d493ec317a36ed0faec0366abe21e5d340",
    "semantic_title": "personalized keyphrase detection using speaker and environment information",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/garg21_interspeech.html": {
    "title": "Streaming Transformer for Hardware Efficient Voice Trigger Detection and False Trigger Mitigation",
    "volume": "main",
    "abstract": "We present a unified and hardware efficient architecture for two stage voice trigger detection (VTD) and false trigger mitigation (FTM) tasks. Two stage VTD systems of voice assistants can get falsely activated to audio segments acoustically similar to the trigger phrase of interest. FTM systems cancel such activations by using post trigger audio context. Traditional FTM systems rely on automatic speech recognition lattices which are computationally expensive to obtain on device. We propose a streaming transformer (TF) encoder architecture, which progressively processes incoming audio chunks and maintains audio context to perform both VTD and FTM tasks using only acoustic features. The proposed joint model yields an average 18% relative reduction in false reject rate (FRR) for the VTD task at a given false alarm rate. Moreover, our model suppresses 95% of the false triggers with an additional one second of post-trigger audio. Finally, on-device measurements show 32% reduction in runtime memory and 56% reduction in inference time compared to non-streaming version of the model",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3e50dd3c1d5fd04d97e27aaa462e7a8059b88aeb",
    "semantic_title": "streaming transformer for hardware efficient voice trigger detection and false trigger mitigation",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mazumder21_interspeech.html": {
    "title": "Few-Shot Keyword Spotting in Any Language",
    "volume": "main",
    "abstract": "We introduce a few-shot transfer learning method for keyword spotting in any language. Leveraging open speech corpora in nine languages, we automate the extraction of a large multilingual keyword bank and use it to train an embedding model. With just five training examples, we fine-tune the embedding model for keyword spotting and achieve an average F score of 0.75 on keyword classification for 180 new keywords unseen by the embedding model in these nine languages. This embedding model also generalizes to new languages. We achieve an average F score of 0.65 on 5-shot models for 260 keywords sampled across 13 new languages unseen by the embedding model. We investigate streaming accuracy for our 5-shot models in two contexts: keyword spotting and keyword search. Across 440 keywords in 22 languages, we achieve an average streaming keyword spotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe promising initial results on keyword search",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2dee473b500bf3bfe843f45973c2d6f1795a082b",
    "semantic_title": "few-shot keyword spotting in any language",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21da_interspeech.html": {
    "title": "Text Anchor Based Metric Learning for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "Keyword Spotting (KWS) remains challenging to achieve the trade-off between small footprint and high accuracy. Recently proposed metric learning approaches improved the generalizability of models for the KWS task, and 1D-CNN based KWS models have achieved the state-of-the-arts (SOTA) in terms of model size. However, for metric learning, due to data limitations, the speech anchor is highly susceptible to the acoustic environment and speakers. Also, we note that the 1D-CNN models have limited capability to capture long-term temporal acoustic features. To address the above problems, we propose to utilize text anchors to improve the stability of anchors. Furthermore, a new type of model (LG-Net) is exquisitely designed to promote long-short term acoustic feature modeling based on 1D-CNN and self-attention. Experiments are conducted on Google Speech Commands Dataset version 1 (GSCDv1) and 2 (GSCDv2). The results demonstrate that the proposed text anchor based metric learning method shows consistent improvements over speech anchor on representative CNN-based models. Moreover, our LG-Net model achieves SOTA accuracy of 97.67% and 96.79% on two datasets, respectively. It is encouraged to see that our lighter LG-Net with only 74k parameters obtains 96.82% KWS accuracy on the GSCDv1 and 95.77% KWS accuracy on the GSCDv2",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2d04d6f35e56b1f6f6986f4a47ce05af4ef6156e",
    "semantic_title": "text anchor based metric learning for small-footprint keyword spotting",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21u_interspeech.html": {
    "title": "A Meta-Learning Approach for User-Defined Spoken Term Classification with Varying Classes and Examples",
    "volume": "main",
    "abstract": "Recently we formulated a user-defined spoken term classification task as a few-shot learning task and tackled the task using Model-Agnostic Meta-Learning (MAML) algorithm. Our results show that the meta-learning approach performs much better than conventional supervised learning and transfer learning in the task, especially with limited training data. In this paper, we extend our work by addressing a more practical problem in the user-defined scenario where users can define any number of spoken terms and provide any number of enrollment audio examples for each spoken term. From the perspective of few-shot learning, this is an N-way, K-shot problem with varying N and K. In our work, we relax the values of N and K of each meta-task during training instead of assigning fixed values to them, which differs from what most meta-learning algorithms do. We adopt a metric-based meta-learning algorithm named Prototypical Networks (ProtoNet) as it avoids exhaustive fine-tuning when N varies. Furthermore, we use the Max-Mahalanobis Center (MMC) loss as an effective regularizer to address the problem of ProtoNet under the condition of varying K. Experiments on the Google Speech Commands dataset demonstrate that our proposed method outperforms the conventional N-way, K-shot setting in most testing tasks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6265762e218cc45d026429d9fa572650d6de1b83",
    "semantic_title": "a meta-learning approach for user-defined spoken term classification with varying classes and examples",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21g_interspeech.html": {
    "title": "Auxiliary Sequence Labeling Tasks for Disfluency Detection",
    "volume": "main",
    "abstract": "Detecting disfluencies in spontaneous speech is an important preprocessing step in natural language processing and speech recognition applications. Existing works for disfluency detection have focused on designing a single objective only for disfluency detection, while auxiliary objectives utilizing linguistic information of a word such as named entity or part-of-speech information can be effective. In this paper, we focus on detecting disfluencies on spoken transcripts and propose a method utilizing named entity recognition (NER) and part-of-speech (POS) as auxiliary sequence labeling (SL) tasks for disfluency detection. First, we investigate cases that utilizing linguistic information of a word can prevent mispredicting important words and can be helpful for the correct detection of disfluencies. Second, we show that training a disfluency detection model with auxiliary SL tasks can improve its F-score in disfluency detection. Then, we analyze which auxiliary SL tasks are influential depending on baseline models. Experimental results on the widely used English Switchboard dataset show that our method outperforms the previous state-of-the-art in disfluency detection",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3a919e1a269f2f06d28ba22a91dfe14429d5bd87",
    "semantic_title": "auxiliary sequence labeling tasks for disfluency detection",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhou21g_interspeech.html": {
    "title": "Energy-Friendly Keyword Spotting System Using Add-Based Convolution",
    "volume": "main",
    "abstract": "Wake-up keyword of a keyword spotting (KWS) system represents brand name of a smart device. Performance of KWS is also crucial for modern speech based human-device interaction. An on-device KWS with both high accuracy and low power consumption is desired. We propose a KWS with add-based convolution layers, namely Add TC-ResNet. Add-based convolution paves a new way to reduce power consumption of KWS system, as addition is more energy efficient than multiplication at hardware level. On Google Speech Commands dataset V2, Add TC-ResNet achieves an accuracy of 97.1%, with 99% of multiplication operations are replaced by addition operations. The result is competitive to a state-of-the-art fully multiplication-based TC-ResNet KWS. We also investigate knowledge distillation and a mixed addition-multiplication design for the proposed KWS, which leads to further performance improvement",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fe2061ef8b333116507f59d7add8dbf5aabc4803",
    "semantic_title": "energy-friendly keyword spotting system using add-based convolution",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jia21b_interspeech.html": {
    "title": "The 2020 Personalized Voice Trigger Challenge: Open Datasets, Evaluation Metrics, Baseline System and Results",
    "volume": "main",
    "abstract": "The 2020 Personalized Voice Trigger Challenge (PVTC2020) addresses two different research problems in a unified setup: joint wake-up word detection with speaker verification on close-talking single microphone data and far-field multi-channel microphone array data. Specially, the second task poses an additional cross-channel matching challenge on top of the far-field condition. To simulate the real-life application scenario, the enrollment utterances are recorded from close-talking cell-phone only, while the test utterances are recorded from both the close-talking cell-phone and the far-field microphone arrays. This paper introduces our challenge setup and the released database as well as the evaluation metrics. In addition, we present a sequential two stage end-to-end neural network baseline system trained with the proposed database for speaker-dependent wake-up word detection. Results show that state-of-the-art personalized voice trigger methods are still based on the two stage design, however, this benchmark database could also be used to evaluate multi-task joint learning methods. The official website, the open-source baseline system and results of submitted systems have been released",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4f16b0d51a9c245ded3cdedf787b39bec4289615",
    "semantic_title": "the 2020 personalized voice trigger challenge: open datasets, evaluation metrics, baseline system and results",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ea_interspeech.html": {
    "title": "Auto-KWS 2021 Challenge: Task, Datasets, and Baselines",
    "volume": "main",
    "abstract": "Auto-KWS 2021 challenge calls for automated machine learning (AutoML) solutions to automate the process of applying machine learning to a customized keyword spotting task. Compared with other keyword spotting tasks, Auto-KWS challenge has the following three characteristics: 1) The challenge focuses on the problem of customized keyword spotting, where the target device can only be awakened by an enrolled speaker with his/her specified keyword. The speaker can use any language and accent to define his keyword. 2) All data of the challenge is recorded in realistic environment to simulate different user scenarios. 3) Auto-KWS is a \"code competition\", where participants need to submit AutoML solutions, then the platform automatically runs the enrollment and prediction steps with the submitted code. This challenge aims at promoting the development of a more personalized and flexible keyword spotting system. Two baseline systems are provided to all participants as references",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "212da7e9227e163ed71c50dd8deb87d6d0819044",
    "semantic_title": "auto-kws 2021 challenge: task, datasets, and baselines",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/berg21_interspeech.html": {
    "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting",
    "volume": "main",
    "abstract": "The Transformer architecture has been successful across many domains, including natural language processing, computer vision and speech recognition. In keyword spotting, self-attention has primarily been used on top of convolutional or recurrent encoders. We investigate a range of ways to adapt the Transformer architecture to keyword spotting and introduce the Keyword Transformer (KWT), a fully self-attentional architecture that exceeds state-of-the-art performance across multiple tasks without any pre-training or additional data. Surprisingly, this simple architecture outperforms more complex models that mix convolutional, recurrent and attentive layers. KWT can be used as a drop-in replacement for these models, setting two new benchmark records on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on the 12 and 35-command tasks respectively",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8e27d8069035cb22a8c1c50d3971b4a61f0143f4",
    "semantic_title": "keyword transformer: a self-attention model for keyword spotting",
    "citation_count": 73,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/awasthi21_interspeech.html": {
    "title": "Teaching Keyword Spotters to Spot New Keywords with Limited Examples",
    "volume": "main",
    "abstract": "Learning to recognize new keywords with just a few examples is essential for personalizing keyword spotting (KWS) models to a user's choice of keywords. However, modern KWS models are typically trained on large datasets and restricted to a small vocabulary of keywords, limiting their transferability to a broad range of unseen keywords. Towards easily customizable KWS models, we present KeySEM (Keyword Speech EMbedding), a speech embedding model pre-trained on the task of recognizing a large number of keywords. Speech representations offered by KeySEM are highly effective for learning new keywords from a limited number of examples. Comparisons with a diverse range of related work across several datasets show that our method achieves consistently superior performance with fewer training examples. Although KeySEM was pre-trained only on English utterances, the performance gains also extend to datasets from four other languages indicating that KeySEM learns useful representations well aligned with the task of keyword spotting. Finally, we demonstrate KeySEM's ability to learn new keywords sequentially without requiring to re-train on previously learned keywords. Our experimental observations suggest that KeySEM is well suited to on-device environments where post-deployment learning and ease of customization are often desirable",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "879576a6b83ceb1d7d69152a4fae9a9b7b4addef",
    "semantic_title": "teaching keyword spotters to spot new keywords with limited examples",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21fa_interspeech.html": {
    "title": "A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection",
    "volume": "main",
    "abstract": "A great deal of recent research effort on speech spoofing countermeasures has been invested into back-end neural networks and training criteria. We contribute to this effort with a comparative perspective in this study. Our comparison of countermeasure models on the ASVspoof 2019 logical access scenario takes into account common strategies to deal with input trials of varied length, recently proposed margin-based training criteria, and widely used front ends. We also measured intra-model differences through multiple training-evaluation rounds with random initialization. Our statistical analysis demonstrates that the performance of the same model may be statistically significantly different when just changing the random initial seed. We thus recommend similar statistical analysis or reporting results of multiple runs for further research on the database. Despite the intra-model differences, we observed a few promising techniques, including average pooling, to efficiently process varied-length inputs and a new hyper-parameter-free loss function. The two techniques led to the best single model in our experiment, which achieved an equal error rate of 1.92% and was significantly different in statistical sense from most of the other experimental models",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7d3d547871324093c76b2f994493c77bbd842285",
    "semantic_title": "a comparative study on recent neural spoofing countermeasures for synthetic speech detection",
    "citation_count": 89,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ca_interspeech.html": {
    "title": "An Initial Investigation for Detecting Partially Spoofed Audio",
    "volume": "main",
    "abstract": "All existing databases of spoofed speech contain attack data that is spoofed in its entirety. In practice, it is entirely plausible that successful attacks can be mounted with utterances that are only partially spoofed. By definition, partially-spoofed utterances contain a mix of both spoofed and bona fide segments, which will likely degrade the performance of countermeasures trained with entirely spoofed utterances. This hypothesis raises the obvious question: This paper introduces a new database of partially-spoofed data, named PartialSpoof, to help address this question. This new database enables us to investigate and compare the performance of countermeasures on both utterance- and segmental- level labels. Experimental results using the utterance-level labels reveal that the reliability of countermeasures trained to detect fully-spoofed data is found to degrade substantially when tested with partially-spoofed data, whereas training on partially-spoofed data performs reliably in the case of both fully- and partially-spoofed utterances. Additional experiments using segmental-level labels show that spotting injected spoofed segments included in an utterance is a much more challenging task even if the latest countermeasure models are used",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "19aef8a1db021580f8e322f2bbf8d0918fc08754",
    "semantic_title": "an initial investigation for detecting partially spoofed audio",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xie21_interspeech.html": {
    "title": "Siamese Network with wav2vec Feature for Spoofing Speech Detection",
    "volume": "main",
    "abstract": "Automatic speaker verification is vulnerable to spoofing attacks with synthesized or converted speech. Although high-performance anti-spoofing countermeasures can achieve high accuracy when the training and testing spoofing attack examples are similarly distributed, their performance degrades significantly when confronted with out-of-distribution spoofing speech, which is created by increasingly advanced unseen speech synthesis and voice conversion methods. Since it is unrealistic to collect enough labeled data from each new spoofing attack method, we argue that addressing the problem of out-of-distribution generalization for spoofing speech detection is essential. In this work, we propose a two-phase representation learning system based on a Siamese network for spoofing speech detection tasks. During the representation learning phase, an embedding Siamese neural network is trained with the wav2vec features to distinguish whether the speech samples in a pair belong to the same category. The proposed system decreases the equal error rate from the state-of-the-art result of 4.07% to 1.15% on the ASVspoof 2019 evaluation set",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "94c0c6b97780fc4c1da6f5ecb128f64ff175b47f",
    "semantic_title": "siamese network with wav2vec feature for spoofing speech detection",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheng21b_interspeech.html": {
    "title": "Cross-Database Replay Detection in Terminal-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "The vulnerability of automatic speaker verification (ASV) systems against replay attacks becomes a severe problem. Although various methods have been proposed for replay detection, the generalization capability is still limited. For instance, a detection model trained on one database may fully fail when tested on another database. In this paper, we adopt the one-class learning technology to address the cross-database problem. Different from conventional two-class models that discriminate genuine speeches from replay attacks, the one-class model focuses on the within-class variance of genuine speeches, which is naturally robust to unseen attacks. In this study, we choose the Gaussian mixture model (GMM) as the one-class model and design two utterance-level features which reduce the uncertainties of genuine class while still be distinguishable from non-genuine class. Experiments conducted on three public replay datasets show that, compared to the state-of-the-art methods, the proposed method demonstrates promising generalization capability under cross-database scenarios",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ba9c08e6b822bd82aaf732ccb446b93b8cdf73a2",
    "semantic_title": "cross-database replay detection in terminal-dependent speaker verification",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21da_interspeech.html": {
    "title": "The Effect of Silence and Dual-Band Fusion in Anti-Spoofing System",
    "volume": "main",
    "abstract": "The current neural network based anti-spoofing systems have poor robustness. Their performance degrades further after voice activity detection (VAD) performed, making it difficult to be applied in practice. This work investigated the effect of silence at the beginning and end of speech, finding that silent differences are part of the basis for countermeasures' judgements. The reason for the performance deterioration caused by VAD is also explored. The experimental results demonstrate that the neural network loses the information about silent segments after the VAD operation removes them. This can lead to more serious overfitting. In order to solve the overfitting problem, the work in this paper also analyzes the reasons for system overfitting from different frequency sub-bands. It is found that the high-frequency part of the feature is the main cause of system overfitting, while the low-frequency part is more robust but less accurate against known attacks. Therefore, we propose the dual-band fusion anti-spoofing algorithm, which requires only two sub-systems but outperforms all but one primary system submitted to the logical access condition of the ASVspoof 2019 challenge. Our system has an EER of 3.50% even after VAD operations performed, thus can be put into practical application",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "82acbca8735fdd7b817e965401ad7a457154b83a",
    "semantic_title": "the effect of silence and dual-band fusion in anti-spoofing system",
    "citation_count": 32,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21d_interspeech.html": {
    "title": "Pairing Weak with Strong: Twin Models for Defending Against Adversarial Attack on Speaker Verification",
    "volume": "main",
    "abstract": "Vulnerability of speaker verification (SV) systems under adversarial attack receives wide attention recently. Simple and effective countermeasures against such attack are yet to be developed. This paper formulates the task of adversarial defense as a problem of attack detection. The detection is made possible with the verification scores from a pair of purposely selected SV models. The twin-model design comprises a fragile model paired up with a relatively robust one. The two models show prominent score inconsistency under adversarial attack. To detect the score inconsistency, a simple one-class classifier is adopted. The classifier is trained with normal speech samples, which not only bypasses the need of crafting adversarial samples but also prevents itself from over-fitting to the crafted samples, and hence makes the detection robust to unseen attacks. Compared to single-model systems, the proposed system shows consistent and significant performance improvement against different attack strategies. The false acceptance rates (FARs) are reduced from over 63.54% to 2.26% under the strongest attack. Our approach has practical benefits, e.g., no need to modify a well-deployed SV model even it is well-known and can be fully accessed by the adversary. Moreover, it can be combined with existing single-model countermeasures for even stronger defenses",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6240bb751f16ed3632be7b65b064498ef1dbbf74",
    "semantic_title": "pairing weak with strong: twin models for defending against adversarial attack on speaker verification",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ling21_interspeech.html": {
    "title": "Attention-Based Convolutional Neural Network for ASV Spoofing Detection",
    "volume": "main",
    "abstract": "In recent years, automatic speaker verification (ASV) algorithms have undergone significant progress. They have been widely deployed in different applications, but the ASV systems are vulnerable to spoofing attacks, such as impersonation, replay, text-to-speech, voice conversion and the recently emerged adversarial attacks. To improve the robustness of the ASV system, researchers have designed anti-spoofing systems to resist spoofing attacks. While previously proposed systems have shown to be effective for spoof attacks detection, they are all ensemble methods based on different speech representations and architectures at the cost of increased model complexity, with similar performance not being achieved with single systems. This paper proposes an attention-based single convolutional neural network to learn discriminative feature embedding for spoof detection, achieving performance comparable to ensemble methods. The key idea is to decrease the information redundancy among channels and focus on the most informative sub-bands of speech representations. The experiments show that our proposed single system achieves an equal error rate of 1.87% on the evaluation set of ASVspoof 2019 Challenge, outperforming all single systems and comparable to the second-ranked system (EER 1.86%) among all known systems",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "578c8bc5caa4d730388bb1ea635fdf73bf297c6f",
    "semantic_title": "attention-based convolutional neural network for asv spoofing detection",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wu21i_interspeech.html": {
    "title": "Voting for the Right Answer: Adversarial Defense for Speaker Verification",
    "volume": "main",
    "abstract": "Automatic speaker verification (ASV) is a well developed technology for biometric identification, and has been ubiquitous implemented in security-critic applications, such as banking and access control. However, previous works have shown that ASV is under the radar of adversarial attacks, which are very similar to their original counterparts from human's perception, yet will manipulate the ASV render wrong prediction. Due to the very late emergence of adversarial attacks for ASV, effective countermeasures against them are limited. Given that the security of ASV is of high priority, in this work, we propose the idea of \"voting for the right answer\" to prevent risky decisions of ASV in blind spot areas, by employing random sampling and voting. Experimental results show that our proposed method improves the robustness against both the limited-knowledge attackers by pulling the adversarial samples out of the blind spots, and the sufficient-knowledge attackers by introducing randomness and increasing the attackers' budgets",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5c74a7fc56664f662fae494c835f6dc852c8c1f8",
    "semantic_title": "voting for the right answer: adversarial defense for speaker verification",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kinnunen21_interspeech.html": {
    "title": "Visualizing Classifier Adjacency Relations: A Case Study in Speaker Verification and Voice Anti-Spoofing",
    "volume": "main",
    "abstract": "Whether it be for results summarization, or the analysis of classifier fusion, some means to compare different classifiers can often provide illuminating insight into their behaviour, (dis)similarity or complementarity. We propose a simple method to derive 2D representation from detection scores produced by an arbitrary set of binary classifiers in response to a common dataset. Based upon rank correlations, our method facilitates a visual comparison of classifiers with arbitrary scores and with close relation to receiver operating characteristic (ROC) and detection error trade-off (DET) analyses. While the approach is fully versatile and can be applied to any detection task, we demonstrate the method using scores produced by automatic speaker verification and voice anti-spoofing systems. The former are produced by a Gaussian mixture model system trained with VoxCeleb data whereas the latter stem from submissions to the ASVspoof 2019 challenge",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "a5ff7330eca5f4962edf654cb614aba4f3b69b2f",
    "semantic_title": "visualizing classifier adjacency relations: a case study in speaker verification and voice anti-spoofing",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/villalba21_interspeech.html": {
    "title": "Representation Learning to Classify and Detect Adversarial Attacks Against Speaker and Speech Recognition Systems",
    "volume": "main",
    "abstract": "Adversarial attacks have become a major threat for machine learning applications. There is a growing interest in studying these attacks in the audio domain, e.g, speech and speaker recognition; and find defenses against them. In this work, we focus on using representation learning to classify/detect attacks w.r.t. the attack algorithm, threat model or signal-to-adversarial-noise ratio. We found that common attacks in the literature can be classified with accuracies as high as 90%. Also, representations trained to classify attacks against speaker identification can be used also to classify attacks against speaker verification and speech recognition. We also tested an attack verification task, where we need to decide whether two speech utterances contain the same attack. We observed that our models did not generalize well to attack algorithms not included in the attack representation model training. Motivated by this, we evaluated an unknown attack detection task. We were able to detect unknown attacks with equal error rates of about 19%, which is promising",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8abbc820db608654c4ba10203245c191566e7286",
    "semantic_title": "representation learning to classify and detect adversarial attacks against speaker and speech recognition systems",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ea_interspeech.html": {
    "title": "An Empirical Study on Channel Effects for Synthetic Voice Spoofing Countermeasure Systems",
    "volume": "main",
    "abstract": "Spoofing countermeasure (CM) systems are critical in speaker verification; they aim to discern spoofing attacks from bona fide speech trials. In practice, however, acoustic condition variability in speech utterances may significantly degrade the performance of CM systems. In this paper, we conduct a cross-dataset study on several state-of-the-art CM systems and observe significant performance degradation compared with their single-dataset performance. Observing differences of average magnitude spectra of bona fide utterances across the datasets, we hypothesize that channel mismatch among these datasets is one important reason. We then verify it by demonstrating a similar degradation of CM systems trained on original but evaluated on channel-shifted data. Finally, we propose several channel robust strategies (data augmentation, multi-task learning, adversarial learning) for CM systems, and observe a significant performance improvement on cross-dataset experiments",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4fbf48a87059316452768731f6422ed0cb57b4b3",
    "semantic_title": "an empirical study on channel effects for synthetic voice spoofing countermeasure systems",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21o_interspeech.html": {
    "title": "Channel-Wise Gated Res2Net: Towards Robust Detection of Synthetic Speech Attacks",
    "volume": "main",
    "abstract": "Existing approaches for anti-spoofing in automatic speaker verification (ASV) still lack generalizability to unseen attacks. The Res2Net approach designs a residual-like connection between feature groups within one block, which increases the possible receptive fields and improves the system's detection generalizability. However, such a residual-like connection is performed by a direct addition between feature groups without channel-wise priority. We argue that the information across channels may not contribute to spoofing cues equally, and the less relevant channels are expected to be suppressed before adding onto the next feature group, so that the system can generalize better to unseen attacks. This argument motivates the current work that presents a novel, channel-wise gated Res2Net (CG-Res2Net), which modifies Res2Net to enable a channel-wise gating mechanism in the connection between feature groups. This gating mechanism dynamically selects channel-wise features based on the input, to suppress the less relevant channels and enhance the detection generalizability. Three gating mechanisms with different structures are proposed and integrated into Res2Net. Experimental results conducted on ASVspoof 2019 logical access (LA) demonstrate that the proposed CG-Res2Net significantly outperforms Res2Net on both the overall LA evaluation set and individual difficult unseen attacks, which also outperforms other state-of-the-art single systems, depicting the effectiveness of our method",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "01d1d1f748137a6b5fe85c3421e9b95f255c33a2",
    "semantic_title": "channel-wise gated res2net: towards robust detection of synthetic speech attacks",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ge21c_interspeech.html": {
    "title": "Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection",
    "volume": "main",
    "abstract": "This paper reports the first successful application of a differentiable architecture search (DARTS) approach to the deepfake and spoofing detection problems. An example of neural architecture search, DARTS operates upon a continuous, differentiable search space which enables both the architecture and parameters to be optimised via gradient descent. Solutions based on partially-connected DARTS use random channel masking in the search space to reduce GPU time and automatically learn and optimise complex neural architectures composed of convolutional operations and residual blocks. Despite being learned quickly with little human effort, the resulting networks are competitive with the best performing systems reported in the literature. Some are also far less complex, containing 85% fewer parameters than a Res2Net competitor",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "24fae59fa1dc079a87555188f1cb78f8c73f6805",
    "semantic_title": "partially-connected differentiable architecture search for deepfake and spoofing detection",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peterson21_interspeech.html": {
    "title": "OpenASR20: An Open Challenge for Automatic Speech Recognition of Conversational Telephone Speech in Low-Resource Languages",
    "volume": "main",
    "abstract": "In 2020, the National Institute of Standards and Technology (NIST), in cooperation with the Intelligence Advanced Research Project Activity (IARPA), conducted an open challenge on automatic speech recognition (ASR) technology for low-resource languages on a challenging data type — conversational telephone speech. The OpenASR20 Challenge was offered for ten low-resource languages — Amharic, Cantonese, Guarani, Javanese, Kurmanji Kurdish, Mongolian, Pashto, Somali, Tamil, and Vietnamese. A total of nine teams from five countries fully participated, and 128 valid submissions were scored. This paper gives an overview of the challenge setup and procedures, as well as a summary of the results. The results show overall high word error rate (WER), with the best results on a severely constrained training data condition ranging from 0.4 to 0.65, depending on the language. ASR with such limited resources remains a challenging problem. Providing a computing platform may be a way to level the playing field and encourage wider participation in challenges like OpenASR",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7e148b62e3e06a63b755d06adb240bcd85b207ce",
    "semantic_title": "openasr20: an open challenge for automatic speech recognition of conversational telephone speech in low-resource languages",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/madikeri21_interspeech.html": {
    "title": "Multitask Adaptation with Lattice-Free MMI for Multi-Genre Speech Recognition of Low Resource Languages",
    "volume": "main",
    "abstract": "In this paper, we develop Automatic Speech Recognition (ASR) systems for multi-genre speech recognition of low-resource languages where training data is predominantly conversational speech but test data can be in one of the following genres: news broadcast, topical broadcast and conversational speech. ASR for low-resource languages is often developed by adapting a pre-trained model to a target language. When training data is predominantly from one genre and limited, the system's performance for other genres suffer. To handle such out-of-domain scenarios, we employ multitask adaptation by using auxiliary conversational speech data from other languages in addition to the target-language data. We aim to (1) improve adaptation through implicit data augmentation by adding other languages as auxiliary tasks, and (2) prevent the acoustic model from overfitting to the dominant genre in the training set. Pre-trained parameters are obtained from a multilingual model trained with data from 18 languages using the Lattice-Free Maximum Mutual Information (LF-MMI) criterion. The adaptation is performed with the LF-MMI criterion. We present results on MATERIAL datasets for three languages: Kazakh and Farsi and Pashto",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "0ecf686e0078651e462daf44c438c9d442ffcf4b",
    "semantic_title": "multitask adaptation with lattice-free mmi for multi-genre speech recognition of low resource languages",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhu21f_interspeech.html": {
    "title": "An Improved Wav2Vec 2.0 Pre-Training Approach Using Enhanced Local Dependency Modeling for Speech Recognition",
    "volume": "main",
    "abstract": "wav2vec 2.0 is a recently proposed self-supervised pre-training framework for learning speech representation. It utilizes a transformer to learn global contextual representation, which is effective especially in low-resource scenarios. Besides, it was shown that combining convolution neural network and transformer to model both local and global dependencies is beneficial for e.g., automatic speech recognition (ASR), natural language processing (NLP). However, how to model the local and global dependence in pre-training models is still an open question in the speech domain. In this paper, we therefore propose a new transformer encoder for enhancing the local dependency by combining convolution and self-attention modules. The transformer encoder first parallels the convolution and self-attention modules, and then serialized with another convolution module, sandwiched by a pair of feed forward modules. Experimental results show that the pre-trained model using the proposed method can reduce the word error rate (WER) compared to the reproduced wav2vec 2.0 at the cost of slightly increasing the size of training parameters",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7a24b4d87eeb205dd13f9baec573f0228acab013",
    "semantic_title": "an improved wav2vec 2.0 pre-training approach using enhanced local dependency modeling for speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21i_interspeech.html": {
    "title": "Systems for Low-Resource Speech Recognition Tasks in Open Automatic Speech Recognition and Formosa Speech Recognition Challenges",
    "volume": "main",
    "abstract": "We, in the team name of NSYSU-MITLab, have participated in low-resource speech recognition of the Open Automatic Speech Recognition Challenge 2020 (OpenASR20) and Formosa Speech Recognition Challenge 2020 (FSR-2020). For the tasks in the challenges, we build and compare end-to-end (E2E) systems and Deep Neural Network Hidden Markov Model (DNN-HMM) systems. In E2E systems, we implement an encoder with Conformer architecture and a decoder with Transformer architecture. In addition, a speaker classifier with a gradient reversal layer is included in the training phase to improve the robustness to speaker variation. In DNN-HMM systems, we implement the Time-Restricted Self-Attention and Factorized Time Delay Neural Networks for the DNN front-end acoustic representation learning. In OpenASR20, the best word error rates we achieved are 61.45% for Cantonese and 74.61% for Vietnamese. In FSR-2020, the best character error rate we achieved is 43.4% for Taiwanese Southern Min Recommended Characters and the best syllable error rate is 25.4% for Taiwan Minnanyu Luomazi Pinyin",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6d3f7f11e8d0773300ba42fd4e4090e6fe57f200",
    "semantic_title": "systems for low-resource speech recognition tasks in open automatic speech recognition and formosa speech recognition challenges",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhao21c_interspeech.html": {
    "title": "The TNT Team System Descriptions of Cantonese and Mongolian for IARPA OpenASR20",
    "volume": "main",
    "abstract": "This paper presents our work for OpenASR20 Challenge. We describe our Automatic Speech Recognition (ASR) systems for Cantonese and Mongolian under both constrained and unconstrained conditions. For constrained condition, a hybrid NN-HMM ASR system play the main role, while for unconstrained condition, an end-to-end ASR system outperforms traditional hybrid systems significantly due to adequate training data. Besides, we adapt to the challenging PSTN conditions using publicly available wideband dictated speech with similar accent, respectively for the two languages. Furthermore, data cleanup, language tailored features, multi-band training, data augmentation, pre-training and system fusions are incorporated. Our submitted systems have achieved excellent performances for the two conditions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "92f6edbe380ead7d99629ab28b85153af96b48cb",
    "semantic_title": "the tnt team system descriptions of cantonese and mongolian for iarpa openasr20",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/alumae21_interspeech.html": {
    "title": "Combining Hybrid and End-to-End Approaches for the OpenASR20 Challenge",
    "volume": "main",
    "abstract": "This paper describes the TalTech team submission to the OpenASR20 Challenge. OpenASR20 evaluated low-resource speech recognition technologies across 10 languages, using only 10 hours of training data in the constrained condition. Our ASR systems used hybrid CNN-TDNNF-based acoustic models, trained with different data augmentation strategies. We used language model adaptation, recurrent neural network language models and lattice combination for improving first pass results. The scores of our submissions were the best across all teams in six out of ten languages. The paper also describes post-evaluation experiments that focused on the unconstrained condition. We show that optimized N-best list combination of a CNN-TDNNF based system and a finetuned multilingual XLSR-53 model results in large reductions in word error rate. Using BABEL data and the combination of hybrid and end-to-end systems gives 12–22% relative improvement over the constrained condition results",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f32df61c6d230c8246273066d2fd64d7ac33eba4",
    "semantic_title": "combining hybrid and end-to-end approaches for the openasr20 challenge",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/morris21_interspeech.html": {
    "title": "One Size Does Not Fit All in Resource-Constrained ASR",
    "volume": "main",
    "abstract": "The application of deep neural networks to the task of acoustic modeling for automatic speech recognition has resulted in dramatic decreases in ASR word error rates, enabling the use of this technology for interacting with smart phones and personal home assistants in high-resource languages. Developing ASR models of this caliber, however, requires hundreds or thousands of hours of transcribed speech recordings, which presents challenges for the vast majority of the world's languages. In this paper, we investigate the utility of three distinct architectures that have previously been used for ASR in languages with limited training resources. We train and test these systems on publicly available ASR datasets for several typologically and orthographically diverse languages, which were produced under a variety of conditions using different speech collection strategies, practices, and equipment. Although these corpora are comparable in size, we find that no single ASR architecture outperforms all others. In addition, word error rates vary significantly, in some cases within the range of those typically reported for high-resource languages. Our results point to the importance of considering language-specific and corpus-specific factors and experimenting with multiple approaches when developing ASR systems for languages with limited training resources",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "bf95adeb7b15539e143a8cbe86297f5ffa9f5621",
    "semantic_title": "one size does not fit all in resource-constrained asr",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cristia21_interspeech.html": {
    "title": "Child Language Acquisition Studied with Wearables",
    "volume": "main",
    "abstract": "",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "1c6ba72caa8ce2f4f2fe9c1cb7a08dfe8f30c757",
    "semantic_title": "child language acquisition studied with wearables",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mikolov21_interspeech.html": {
    "title": "Language Modeling and Artificial Intelligence",
    "volume": "main",
    "abstract": "",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "486bcbe34931906ca16ab7f5fe9cee89f670ef1b",
    "semantic_title": "language modeling and artificial intelligence",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gimeno21_interspeech.html": {
    "title": "Unsupervised Representation Learning for Speech Activity Detection in the Fearless Steps Challenge 2021",
    "volume": "main",
    "abstract": "In this paper, we describe the ViVoLab speech activity detection (SAD) system submitted to the Fearless Steps Challenge Phase III. This series of challenges have proposed a number of speech processing task dealing with audio from Apollo space missions over the last few years. The focus in this edition is set on the generalisation capabilities of the systems, with new evaluation data from different channels. Our proposed submission is based on the use of the unsupervised representation learning paradigm, seeking to obtain a new and more discriminative audio representation than traditional perceptual features such as log Mel-filterbank energies. These new features are used to train different variations of a convolutional recurrent neural network (CRNN). Experimental results show that features learned via unsupervised learning provide a much more robust representation, significantly reducing the mismatch observed between development and evaluation partition results. Obtained results largely outperform the organisation baseline, achieving a DCF metric of 2.98% on the evaluation set and ranking third among all the participant teams",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "ca15ccf6ff94b66e0f3a39c260a9cf4ecd3faeb2",
    "semantic_title": "unsupervised representation learning for speech activity detection in the fearless steps challenge 2021",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vuong21_interspeech.html": {
    "title": "The Application of Learnable STRF Kernels to the 2021 Fearless Steps Phase-03 SAD Challenge",
    "volume": "main",
    "abstract": "We describe a deep-learning-based system developed for the Fearless Steps Phase-03 Speech Activity Detection (SAD) challenge. The system includes both learnable spectro-temporal receptive fields (STRFs) and unconstrained 2-dimensional convolutional kernels in the first layer. Experiments show that the inclusion of learnable STRFs in the first layer increases the system's robustness to additive noise. Additionally, we found that utilizing SpecAugment during training improves generalization on unseen data. By incorporating these enhancements and others our system achieved the best score in the official SAD challenge",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c150c74993c7a9740f64612733f8bf431613429f",
    "semantic_title": "the application of learnable strf kernels to the 2021 fearless steps phase-03 sad challenge",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sarfjoo21_interspeech.html": {
    "title": "Speech Activity Detection Based on Multilingual Speech Recognition System",
    "volume": "main",
    "abstract": "To better model the contextual information and increase the generalization ability of the Speech Activity Detection (SAD) system, this paper leverages a multilingual Automatic Speech Recognition (ASR) system to perform SAD. Sequence-discriminative training of Acoustic Model (AM) using Lattice-Free Maximum Mutual Information (LF-MMI) loss function, effectively extracts the contextual information of the input acoustic frame. Multilingual AM training causes the robustness to noise and language variabilities. The index of maximum output posterior is considered as a frame-level speech/non-speech decision function. Majority voting and logistic regression are applied to fuse the language-dependent decisions. The multilingual ASR is trained on 18 languages of BABEL datasets and the built SAD is evaluated on 3 different languages. On out-of-domain datasets, the proposed SAD model shows significantly better performance with respect to baseline models. On the Ester2 dataset, without using any in-domain data, this model outperforms the WebRTC, phoneme recognizer based VAD (Phn_Rec), and Pyannote baselines (respectively by 7.1, 1.7, and 2.7% absolute) in Detection Error Rate (DetER) metrics. Similarly, on the LiveATC dataset, this model outperforms the WebRTC, Phn_Rec, and Pyannote baselines (respectively by 6.4, 10.0, and 3.7% absolutely) in DetER metrics",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "14c84159a66f8c94bf2aeb78808d4fd6997be834",
    "semantic_title": "speech activity detection based on multilingual speech recognition system",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/luckenbaugh21_interspeech.html": {
    "title": "Voice Activity Detection with Teacher-Student Domain Emulation",
    "volume": "main",
    "abstract": "Transfer learning is a promising approach to increase performance for many speech-based systems, including (VAD). Domain adaptation, a subfield of transfer learning, often improves model conditioning in the presence of a mismatch between train-test conditions. This study proposes a formulation for VAD based on the teacher-student training, where the teacher model, trained with clean data, transfers knowledge to the student model trained with a noisy, paired version of the corpus resembling the test conditions. The models leverage temporal information using (RNN), implemented with either (BLSTM) or the modern, continuous-state Hopfield network. We provide evidence that in-domain noise emulation for domain adaptation is viable under unconstrained audio channel conditions for VAD \"in the wild.\" Our application domain is in healthcare, where multimodal sensors, including microphones, from portable devices are used to automatically predict social isolation in patients affected by schizophrenia. We empirically show positive results for domain emulation when the training conditions are similar to the target domain. We also show that the Hopfield network outperforms our best BLSTM for VAD on real-world benchmarks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "507923ae8027fd4c0a7d16e7d04ba4f016d164af",
    "semantic_title": "voice activity detection with teacher-student domain emulation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ghahabi21_interspeech.html": {
    "title": "EML Online Speech Activity Detection for the Fearless Steps Challenge Phase-III",
    "volume": "main",
    "abstract": "Speech Activity Detection (SAD), locating speech segments within an audio recording, is a main part of most speech technology applications. Robust SAD is usually more difficult in noisy conditions with varying signal-to-noise ratios (SNR). The Fearless Steps challenge has recently provided such data from the NASA Apollo-11 mission for different speech processing tasks including SAD. Most audio recordings are degraded by different kinds and levels of noise varying within and between channels. This paper describes the EML online algorithm for the most recent phase of this challenge. The proposed algorithm can be trained both in a supervised and unsupervised manner and assigns speech and non-speech labels at runtime approximately every 0.1 sec. The experimental results show a competitive accuracy on both development and evaluation datasets with a real-time factor of about 0.002 using a single CPU machine",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "23ebdb09cb28e7d153076c2aefb2b5f6c90ab6e1",
    "semantic_title": "eml online speech activity detection for the fearless steps challenge phase-iii",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/opatka21_interspeech.html": {
    "title": "Device Playback Augmentation with Echo Cancellation for Keyword Spotting",
    "volume": "main",
    "abstract": "Keyword spotting (KWS) is required to operate in device playback conditions in which the device itself plays interfering signals. We propose a new method to augment the training set and adapt the acoustic model to the playback environment. It is based on acoustic simulation which models the coupling between the device's loudspeakers and microphones. The employed model involves frequency response of the device, as well as room impulse response and nonlinear distortions introduced in the playback path. Finally, we pass the simulated signals through Acoustic Echo Cancellation (AEC) to model the artifacts introduced by AEC algorithm. The proposed method reduces False Rejection Rate in device playback noise by 25–60% for a Time-Delay Neural Network-based KWS engine. It is shown that the introduction of device characteristics and nonlinear filtration is necessary to achieve improvement in playback conditions. The augmentation scheme is highly independent of the architecture of the KWS system",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7ec1c69d92a3f9fd6bdeac31c6e42fed0d3290fe",
    "semantic_title": "device playback augmentation with echo cancellation for keyword spotting",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yusuf21_interspeech.html": {
    "title": "End-to-End Open Vocabulary Keyword Search",
    "volume": "main",
    "abstract": "Recently, neural approaches to spoken content retrieval have become popular. However, they tend to be restricted in their vocabulary or in their ability to deal with imbalanced test settings. These restrictions limit their applicability in keyword search, where the set of queries is not known beforehand, and where the system should return not just whether an utterance contains a query but the exact location of any such occurrences. In this work, we propose a model directly optimized for keyword search. The model takes a query and an utterance as input and returns a sequence of probabilities for each frame of the utterance of the query having occurred in that frame. Experiments show that the proposed model not only outperforms similar end-to-end models on a task where the ratio of positive and negative trials is artificially balanced, but it is also able to deal with the far more challenging task of keyword search with its inherent imbalance. Furthermore, using our system to rescore the outputs an LVCSR-based keyword search system leads to significant improvements on the latter",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8f0b1d1974c3ea1fd771a7c3dc91684d22ebf5bd",
    "semantic_title": "end-to-end open vocabulary keyword search",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/merkx21_interspeech.html": {
    "title": "Semantic Sentence Similarity: Size does not Always Matter",
    "volume": "main",
    "abstract": "This study addresses the question whether visually grounded speech recognition (VGS) models learn to capture sentence semantics without access to any prior linguistic knowledge. We produce synthetic and natural spoken versions of a well known semantic textual similarity database and show that our VGS model produces embeddings that correlate well with human semantic similarity judgements. Our results show that a model trained on a small image-caption database outperforms two models trained on much larger databases, indicating that database size is not all that matters. We also investigate the importance of having multiple captions per image and find that this is indeed helpful even if the total number of images is lower, suggesting that paraphrasing is a valuable learning signal. While the general trend in the field is to create ever larger datasets to train models on, our findings indicate other characteristics of the database can just as important",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2acab95f3b364ad09ea9b0f5720bc32122fe3b94",
    "semantic_title": "semantic sentence similarity: size does not always matter",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/svec21_interspeech.html": {
    "title": "Spoken Term Detection and Relevance Score Estimation Using Dot-Product of Pronunciation Embeddings",
    "volume": "main",
    "abstract": "The paper describes a novel approach to Spoken Term Detection (STD) in large spoken archives using deep LSTM networks. The work is based on the previous approach of using Siamese neural networks for STD and naturally extends it to directly localize a spoken term and estimate its relevance score. The phoneme confusion network generated by a phoneme recognizer is processed by the deep LSTM network which projects each segment of the confusion network into an embedding space. The searched term is projected into the same embedding space using another deep LSTM network. The relevance score is then computed using a simple dot-product in the embedding space and calibrated using a sigmoid function to predict the probability of occurrence. The location of the searched term is then estimated from the sequence of output probabilities. The deep LSTM networks are trained in a self-supervised manner from paired recognition hypotheses on word and phoneme levels. The method is experimentally evaluated on MALACH data in English and Czech languages",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "62da8d9ffea80d267d61a5c06c8974c53cf83566",
    "semantic_title": "spoken term detection and relevance score estimation using dot-product of pronunciation embeddings",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/buet21_interspeech.html": {
    "title": "Toward Genre Adapted Closed Captioning",
    "volume": "main",
    "abstract": "This paper studies the generation of intralingual closed captions from automatic speech transcripts, with the aim to assess techniques for multi-genre captioning. Captions and subtitles greatly vary in form and content depending on the programs genres and subtitling styles, resulting for instance in significantly different compression rates and lexical content. Borrowing ideas from the multi-domain machine translation literature, we implement and contrast several adaptation methods on a diverse set of programs broadcast on the French public TV. Our results show that such multi-domain adaption techniques are effective and help to improve our automatic subtitling system",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "0fe6a4cec163b7f45f2c1b1d85169d2f085cf411",
    "semantic_title": "toward genre adapted closed captioning",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/korzekwa21b_interspeech.html": {
    "title": "Weakly-Supervised Word-Level Pronunciation Error Detection in Non-Native English Speech",
    "volume": "main",
    "abstract": "We propose a weakly-supervised model for word-level mispronunciation detection in non-native (L2) English speech. To train this model, phonetically transcribed L2 speech is not required and we only need to mark mispronounced words. The lack of phonetic transcriptions for L2 speech means that the model has to learn only from a weak signal of word-level mispronunciations. Because of that and due to the limited amount of mispronounced L2 speech, the model is more likely to overfit. To limit this risk, we train it in a multi-task setup. In the first task, we estimate the probabilities of word-level mispronunciation. For the second task, we use a phoneme recognizer trained on phonetically transcribed L1 speech that is easily accessible and can be automatically annotated. Compared to state-of-the-art approaches, we improve the accuracy of detecting word-level pronunciation errors in AUC metric by 30% on the GUT Isle Corpus of L2 Polish speakers, and by 21.5% on the Isle Corpus of L2 German and Italian speakers",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d381a2078ecd086fd4c7d6b3e9ca64919c61958a",
    "semantic_title": "weakly-supervised word-level pronunciation error detection in non-native english speech",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kanda21b_interspeech.html": {
    "title": "End-to-End Speaker-Attributed ASR with Transformer",
    "volume": "main",
    "abstract": "This paper presents our recent effort on end-to-end speaker-attributed automatic speech recognition, which jointly performs speaker counting, speech recognition and speaker identification for monaural multi-talker audio. Firstly, we thoroughly update the model architecture that was previously designed based on a long short-term memory (LSTM)-based attention encoder decoder by applying transformer architectures. Secondly, we propose a speaker deduplication mechanism to reduce speaker identification errors in highly overlapped regions. Experimental results on the LibriSpeechMix dataset shows that the transformer-based architecture is especially good at counting the speakers and that the proposed model reduces the speaker-attributed word error rate by 47% over the LSTM-based baseline. Furthermore, for the LibriCSS dataset, which consists of real recordings of overlapped speech, the proposed model achieves concatenated minimum-permutation word error rates of 11.9% and 16.3% with and without target speaker profiles, respectively, both of which are the state-of-the-art results for LibriCSS with the monaural setting",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "147eb3945a01378c1de19584ac4c48d6ef552366",
    "semantic_title": "end-to-end speaker-attributed asr with transformer",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/soltau21_interspeech.html": {
    "title": "Understanding Medical Conversations: Rich Transcription, Confidence Scores & Information Extraction",
    "volume": "main",
    "abstract": "In this paper, we describe novel components for extracting clinically relevant information from medical conversations which will be available as Google APIs. We describe a transformer-based Recurrent Neural Network Transducer (RNN-T) model tailored for long-form audio, which can produce rich transcriptions including speaker segmentation, speaker role labeling, punctuation and capitalization. On a representative test set, we compare performance of RNN-T models with different encoders, units and streaming constraints. Our transformer-based streaming model performs at about 20% WER on the ASR task, 6% WDER on the diarization task, 43% SER on periods, 52% SER on commas, 43% SER on question marks and 30% SER on capitalization. Our recognizer is paired with a confidence model that utilizes both acoustic and lexical features from the recognizer. The model performs at about 0.37 NCE. Finally, we describe a RNN-T based tagging model. The performance of the model depends on the ontologies, with F-scores of 0.90 for medications, 0.76 for symptoms, 0.75 for conditions, 0.76 for diagnosis, and 0.61 for treatments. While there is still room for improvement, our results suggest that these models are sufficiently accurate for practical applications",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2d43200201915e8be1921e964eb6ec484f1a9f78",
    "semantic_title": "understanding medical conversations: rich transcription, confidence scores & information extraction",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/vidal21_interspeech.html": {
    "title": "Phone-Level Pronunciation Scoring for Spanish Speakers Learning English Using a GOP-DNN System",
    "volume": "main",
    "abstract": "In today's globalized world being able to communicate in English is crucial to many people. Computer assisted pronunciation training (CAPT) systems can help students achieve English proficiency by providing an accessible way to practice, offering personalized feedback. However, phone-level pronunciation scoring is still a very challenging task, with performance far from that of human annotators. In this paper we compare and present results on the Spanish subset of the L2-ARCTIC corpus and the new Epa-DB database, both containing non-native English speech by native Spanish speakers and intended for the development of pronunciation scoring systems. We show the most frequent errors in each database and compare performance of a state-of-the-art goodness of pronunciation (GOP) system. Results show that both databases have similar error patterns and that performance is similar for most phones, despite differences in recording conditions. For the EpaDB database we also present an analysis of the errors per target phone. This study validates the EpaDB collection and annotations, providing initial results and contributing to the advancement of a challenging low-resource task",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "db8ceb0d5a7defc337a6d8ec337554f86c7b8c3e",
    "semantic_title": "phone-level pronunciation scoring for spanish speakers learning english using a gop-dnn system",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xu21k_interspeech.html": {
    "title": "Explore wav2vec 2.0 for Mispronunciation Detection",
    "volume": "main",
    "abstract": "This paper presents an initial attempt to use self-supervised learning for Mispronunciation Detection. Unlike existing methods that use speech recognition corpus to train models, we exploit unlabeled data and utilize a self-supervised learning technique, Wav2vec 2.0, for pretraining. After the pretraining process, the training process only requires a little pronunciation-labeled data for finetuning. Formulating Mispronunciation Detection as a binary classification task, we add convolutional and pooling layers on the top of the pretrained model to detect mispronunciations of the given prompted texts within the alignment segmentations. The training process is simple and effective. Several experiments are conducted to validate the effectiveness of the pretrained method. Our approach outperforms existing methods on a public dataset L2-ARCTIC with a F1 value of 0.610",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "035540bc396061f82f751d367d187f06113797c2",
    "semantic_title": "explore wav2vec 2.0 for mispronunciation detection",
    "citation_count": 35,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ando21_interspeech.html": {
    "title": "Lexical Density Analysis of Word Productions in Japanese English Using Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "In L2 pronunciation, what kind of phonetic errors are more influential to intelligibility reduction? Teachers say that learners' utterances become unintelligible when words are pronounced with such errors that make the words misidentified as others. In this paper, we focus on Japanese English (JE), where the number of phonemes of the L1 (Japanese) is much smaller than that of the L2 (American English, AE). Since learners often substitute L1 phonemes when speaking in L2, some words are expected to be pronounced not distinctively enough in JE, which may result in word misidentification. This implies that words of JE will exist phonetically closer to each other in a space where words are distributed. In this paper, lexical density analysis of JE and AE is carried out using acoustic word embeddings. Word productions in JE and AE, extracted from the ERJ corpus, are mapped as points in an acoustic word embedding space obtained by network training with the WSJ corpus. Experiments show that significantly higher density is found in JE than in AE and it is also found in poor learners than in good learners",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f7a2695f5e687783f2d6d1c114056c5009ba1715",
    "semantic_title": "lexical density analysis of word productions in japanese english using acoustic word embeddings",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21j_interspeech.html": {
    "title": "Deep Feature Transfer Learning for Automatic Pronunciation Assessment",
    "volume": "main",
    "abstract": "Automatic pronunciation assessment is commonly developed to evaluate pronunciation quality of second language (L2) learners. Traditional methods for automatic pronunciation assessment normally utilize speech features such as Goodness of pronunciation (GOP), which may not provide sufficient information for the pronunciation proficiency assessment [1]. In this paper, we propose a transfer learning method for automatic pronunciation assessment. We directly utilize the deep features from the acoustic model instead of traditional features such as GOP, and transfer the acoustic knowledge from ASR to a specific scoring module. The scoring module is designed to consider the relationship among different granularities in an utterance based on an attention mechanism. Only this module is updated for faster transfer and adaptation of various pronunciation assessment tasks. Experimental results based on the dataset recorded by Chinese English-as-second-language (ESL) learners and the Speechocean762 dataset demonstrate that the proposed method outperforms the traditional GOP-based baselines in Pearson correlation coefficient (PCC) and yields parameter-efficient transfer for different pronunciation assessment tasks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "9e2dec25de4045457e0cf8b1caa3c6eefe2354b0",
    "semantic_title": "deep feature transfer learning for automatic pronunciation assessment",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21fa_interspeech.html": {
    "title": "Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil",
    "volume": "main",
    "abstract": "Speech evaluation is an essential component in computer-assisted language learning (CALL). While speech evaluation on English has been popular, automatic speech scoring on low resource languages remains challenging. Work in this area has focused on monolingual specific designs and handcrafted features stemming from resource-rich languages like English. Such approaches are often difficult to generalize to other languages, especially if we also want to consider suprasegmental qualities such as rhythm. In this work, we examine three different languages that possess distinct rhythm patterns: English (stress-timed), Malay (syllable-timed), and Tamil (mora-timed). We exploit robust feature representations inspired by music processing and vector representation learning. Empirical validations show consistent gains for all three languages when predicting pronunciation, rhythm and intonation performance",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "0d1b5315ed2960dc427a8c36a5405d96dfa382c2",
    "semantic_title": "multilingual speech evaluation: case studies on english, malay and tamil",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21e_interspeech.html": {
    "title": "A Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "Mispronunciation detection and diagnosis (MDD) technology is a key component of computer-assisted pronunciation training system (CAPT). The mainstream method is based on deep neural network automatic speech recognition. Unfortunately, the technique requires massive human-annotated speech recordings for training. Due to the huge variations in mother tongue, age, and proficiency level among second language learners, it is difficult to gather a large amount of matching data for acoustic model training, which greatly limits the model performance. In this paper, we explore the use of Self-Supervised Pretraining (SSP) model wav2vec2.0 for MDD tasks. SSP utilizes a large unlabelled dataset to learn general representation and can be applied in downstream tasks. We conduct experiments using two publicly available datasets (TIMIT, L2-arctic) and our best system achieves 60.44% f1-score. Moreover, our method is able to achieve 55.52% f1-score with 3 times less data, which demonstrates the effectiveness of SSP on MDD",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "339d839cb8cb65a58dc8ec9cc0394af5a5922f17",
    "semantic_title": "a study on fine-tuning wav2vec2.0 model for the task of mispronunciation detection and diagnosis",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/qiao21b_interspeech.html": {
    "title": "The Impact of ASR on the Automatic Analysis of Linguistic Complexity and Sophistication in Spontaneous L2 Speech",
    "volume": "main",
    "abstract": "In recent years, automated approaches to assessing linguistic complexity in second language (L2) writing have made significant progress in gauging learner performance, predicting human ratings of the quality of learner productions, and benchmarking L2 development. In contrast, there is comparatively little work in the area of speaking, particularly with respect to fully automated approaches to assessing L2 spontaneous speech. While the importance of a well-performing ASR system is widely recognized, little research has been conducted to investigate the impact of its performance on subsequent automatic text analysis. In this paper, we focus on this issue and examine the impact of using a state-of-the-art ASR system for subsequent automatic analysis of linguistic complexity in spontaneously produced L2 speech. A set of 30 selected measures were considered, falling into four categories: syntactic, lexical, n-gram frequency, and information-theoretic measures. The agreement between the scores for these measures obtained on the basis of ASR-generated vs. manual transcriptions was determined through correlation analysis. A more differential effect of ASR performance on specific types of complexity measures when controlling for task type effects is also presented",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "10003487ecd145988d2ffd2cb9d5acf4bc89dfa2",
    "semantic_title": "the impact of asr on the automatic analysis of linguistic complexity and sophistication in spontaneous l2 speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tanaka21c_interspeech.html": {
    "title": "End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning",
    "volume": "main",
    "abstract": "We propose a semi-supervised learning method for building end-to-end rich transcription-style automatic speech recognition (RT-ASR) systems from small-scale rich transcription-style and large-scale common transcription-style datasets. In spontaneous speech tasks, various speech phenomena such as fillers, word fragments, laughter and coughs, etc. are often included. While common transcriptions do not give special awareness to these phenomena, rich transcriptions explicitly convert them into special phenomenon tokens as well as textual tokens. In previous studies, the textual and phenomenon tokens were simultaneously estimated in an end-to-end manner. However, it is difficult to build accurate RT-ASR systems because large-scale rich transcription-style datasets are often unavailable. To solve this problem, our training method uses a limited rich transcription-style dataset and common transcription-style dataset simultaneously. The Key process in our semi-supervised learning is to convert the common transcription-style dataset into a pseudo-rich transcription-style dataset. To this end, we introduce style tokens which control phenomenon tokens are generated or not into transformer-based autoregressive modeling. We use this modeling for generating the pseudo-rich transcription-style datasets and for building RT-ASR system from the pseudo and original datasets. Our experiments on spontaneous ASR tasks showed the effectiveness of the proposed method",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "58a05c3c535100779536313dd398b819e156959f",
    "semantic_title": "end-to-end rich transcription-style automatic speech recognition with semi-supervised learning",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cumbal21_interspeech.html": {
    "title": "You don't understand me!\": Comparing ASR Results for L1 and L2 Speakers of Swedish",
    "volume": "main",
    "abstract": "The performance of Automatic Speech Recognition (ASR) systems has constantly increased in state-of-the-art development. However, performance tends to decrease considerably in more challenging conditions (e.g., background noise, multiple speaker social conversations) and with more atypical speakers (e.g., children, non-native speakers or people with speech disorders), which signifies that general improvements do not necessarily transfer to applications that rely on ASR, e.g., educational software for younger students or language learners. In this study, we focus on the gap in performance between recognition results for native and non-native, read and spontaneous, Swedish utterances transcribed by different ASR services. We compare the recognition results using Word Error Rate and analyze the linguistic factors that may generate the observed transcription errors",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b9a1bbf59d0fce3511b0cdc5308714e6d6808676",
    "semantic_title": "you don't understand me!\": comparing asr results for l1 and l2 speakers of swedish",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ga_interspeech.html": {
    "title": "NeMo Inverse Text Normalization: From Development to Production",
    "volume": "main",
    "abstract": "Inverse text normalization (ITN) converts spoken-domain automatic speech recognition (ASR) output into written-domain text to improve the readability of the ASR output. Many state-of-the-art ITN systems use hand-written weighted finite-state transducer (WFST) grammars since this task has extremely low tolerance to unrecoverable errors. We introduce an open-source Python WFST-based library for ITN which enables a seamless path from development to production. We describe the specification of ITN grammar rules for English, but the library can be adapted for other languages. It can also be used for written-to-spoken text normalization. We evaluate the NeMo ITN library using a modified version of the Google Text normalization dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2f0c243fc11487f524f0d16ce512a4b87599e496",
    "semantic_title": "nemo inverse text normalization: from development to production",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/naijo21_interspeech.html": {
    "title": "Improvement of Automatic English Pronunciation Assessment with Small Number of Utterances Using Sentence Speakability",
    "volume": "main",
    "abstract": "The current Computer-Assisted Pronunciation Training (CAPT) system uses DNN-based speech recognition results to evaluate learner's pronunciation with high accuracy when using many utterances for the evaluation. However, when we use only a few utterances, the accuracy of the CAPT system deteriorates. One reason for the deterioration is that the score calculated by a CAPT system is biased depending on the pronunciation difficulty of the sentences when using a small number of utterances. In this study, we developed a CAPT system that takes the sentence speakability (pronunciation difficulty of sentences) into account. As a result, the correlation coefficient between the human evaluation and the machine score was 0.46 in the conventional method, while it improved to 0.57 with the proposed method",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "21f0dd4b738b9a974bf857e43f2377c434346ae2",
    "semantic_title": "improvement of automatic english pronunciation assessment with small number of utterances using sentence speakability",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/haider21_interspeech.html": {
    "title": "Affect Recognition Through Scalogram and Multi-Resolution Cochleagram Features",
    "volume": "main",
    "abstract": "An approach to the categorization of voice samples according to emotions expressed by the speaker is proposed which uses Multi-Resolution Cochleagram (MRCG) and scalogram features in a novel way. Audio recordings from the EmoDB, EMOVO and Savee Data-sets are employed in training and testing of predictive models consisting of different sets of speech features. This study systematically evaluates the performance of the feature sets most commonly used in computational paralinguistic tasks (i.e and ) in addition to MRCG- and scalogram-derived features and their fusion, across five different classifiers. The datasets used in this evaluation include speech in three different languages (German, Italian and English). MRCG features outperform the feature sets most commonly used in computational paralinguistic tasks, including and , for the EmoDB (unweighted average recall, UAR = 59.15%) and SAVEE (UAR = 36.12%) datasets, while provides the best overall UAR (33.84%) for the EMOVO dataset. A support vector machine (SVM) classifier yields the best UAR for EmoDB (80.05%) through fusion of and MRCG, and for EMOVO (40.31%), through fusion of , and For SAVEE, random forests provide the best result (46.55%) using the feature set",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2fd1cc96a04c83119ad95ecbb04781019f1101ea",
    "semantic_title": "affect recognition through scalogram and multi-resolution cochleagram features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21n_interspeech.html": {
    "title": "A Speech Emotion Recognition Framework for Better Discrimination of Confusions",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) plays an important role in human-machine interaction (HMI). Various methods have been proposed for the SER task. However, a common problem in most of the previous studies is some specific emotions are grossly misclassified. In this paper, we propose a novel SER framework aiming at discriminating the confusions by utilizing triplet loss and data augmentation to enforce a CNN-LSTM model to emphasize more on these emotions which are hard to be correctly classified. Ablation experiments demonstrate the effectiveness of the proposed framework. On Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset, our framework can achieve 79.52% of Weighted Accuracy (WA) and 78.30% of Unweighted Accuracy (UA). Compared to the other state-of-the-art models, our framework obtains more than 3.34% and 1.94% improvement on WA and UA respectively",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "070ab385cda2854fa82de33b0fd584d5bba7b5dd",
    "semantic_title": "a speech emotion recognition framework for better discrimination of confusions",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21p_interspeech.html": {
    "title": "Speech Emotion Recognition via Multi-Level Cross-Modal Distillation",
    "volume": "main",
    "abstract": "Speech emotion recognition faces the problem that most of the existing speech corpora are limited in scale and diversity due to the high annotation cost and label ambiguity. In this work, we explore the task of learning robust speech emotion representations based on large unlabeled speech data. Under a simple assumption that the internal emotional states across different modalities are similar, we propose a method called Multi-level Cross-modal Emotion Distillation (MCED), which trains the speech emotion model without any labeled speech emotion data by transferring emotion knowledge from a pretrained text emotion model. Extensive experiments on two benchmark datasets, IEMOCAP and MELD, show that our proposed MCED can help learn effective speech emotion representations which generalize well on downstream speech emotion recognition tasks",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2dea0b5d70a59ae6dc0bd13016e72166aea756e8",
    "semantic_title": "speech emotion recognition via multi-level cross-modal distillation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ito21_interspeech.html": {
    "title": "Audio-Visual Speech Emotion Recognition by Disentangling Emotion and Identity Attributes",
    "volume": "main",
    "abstract": "In this paper, we propose an audio-visual speech emotion recognition (AV-SER) that can suppress the disturbance from an identity attribute by disentangling an emotion attribute and an identity one. We developed a model that first disentangles both attributes for each modality. In order to achieve the disentanglement, we introduce a co-attention module to our model. Our model disentangles the emotion attribute by giving the identity attribute as conditional features to the module. Conversely, the identity attribute is also obtained with the emotion attribute as a condition. Our model then makes a prediction for each attribute from these disentangled features by considering both modalities. In addition, to ensure the disentanglement capacity of our model, we train the model with an identification task as the auxiliary task and an SER task as the primary task alternately, and we update only the part of parameters responsible for each task. The experimental result shows the effectiveness of our method with the wild CMU-MOSEI dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f5d3d3713604de167eb70b4e26fefb26bfe4efe1",
    "semantic_title": "audio-visual speech emotion recognition by disentangling emotion and identity attributes",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/bose21_interspeech.html": {
    "title": "Parametric Distributions to Model Numerical Emotion Labels",
    "volume": "main",
    "abstract": "It is common to represent emotional states as values on a set of numerical scales corresponding to attributes such as arousal and valence. Often these labels are obtained from multiple annotators who record their perception of emotion in terms of these attributes. Combining these multiple annotations by taking the mean, as is typical in affective computing systems ignores the inherent ambiguity in the labels. Recently it has been recognised that this ambiguity carries useful information and systems that employ distributions over the numerical scales to represent emotional states have been proposed. In this paper we show that the common and widespread assumption that this distribution is Gaussian may not be suitable since the underlying numerical scales are bounded. We then compare a range of well-known distributions defined on bounded domains to ascertain which of them would be the most suitable alternative. Statistical measures are proposed to enable quantifiable comparisons and the results are reported. All comparisons reported in the paper were carried out on the RECOLA dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5c46020a5fc2270668f6f1f670a3524f9a7fda47",
    "semantic_title": "parametric distributions to model numerical emotion labels",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21e_interspeech.html": {
    "title": "Metric Learning Based Feature Representation with Gated Fusion Model for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Due to the lack of sufficient speech emotional data, the recognition performance of existing speech emotion recognition (SER) approaches is relatively low and requires further improvement to meet the needs of real-life applications. For the problem of data scarcity, an increasingly popular solution is to transfer emotional information through pre-training models and extract additional features. However, the feature representation needs further compression because the training object of unsupervised learning is to reconstruct input, making the latent representation contain non-affective information. In this paper, we introduce deep metric learning to constrain the feature distribution of the pre-training model. Specifically, we propose a triplet loss to modify the representation extraction model as a pseudo-siamese network and achieve more efficient knowledge transfer for emotion recognition. Furthermore, we propose a gated fusion method to learn the connection of features extracted from the pre-training model and supervised feature extraction model. We conduct experiments on the common benchmarking dataset IEMOCAP to verify the performance of the proposed model. The experimental results demonstrate the advantages of our model, outperforming the unsupervised transfer learning system by 3.7% and 3.88% in weighted accuracy and unweighted accuracy, respectively",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "e92936f5bf1a8041ac94eae5cdd8717e79f9e885",
    "semantic_title": "metric learning based feature representation with gated fusion model for speech emotion recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cai21b_interspeech.html": {
    "title": "Speech Emotion Recognition with Multi-Task Learning",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) classifies speech into emotion categories such as: and Recently, deep learning has been applied to the SER task. This paper proposes a multi-task learning (MTL) framework to simultaneously perform speech-to-text recognition and emotion classification, with an end-to-end deep neural model based on wav2vec-2.0. Experiments on the IEMOCAP benchmark show that the proposed method achieves the state-of-the-art performance on the SER task. In addition, an ablation study establishes the effectiveness of the proposed MTL framework",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b94262a62cd2af728550a99f263a3084d1b4e0f9",
    "semantic_title": "speech emotion recognition with multi-task learning",
    "citation_count": 43,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seneviratne21b_interspeech.html": {
    "title": "Generalized Dilated CNN Models for Depression Detection Using Inverted Vocal Tract Variables",
    "volume": "main",
    "abstract": "Depression detection using vocal biomarkers is a highly researched area. Articulatory coordination features (ACFs) are developed based on the changes in neuromotor coordination due to psychomotor slowing, a key feature of Major Depressive Disorder. However findings of existing studies are mostly validated on a single database which limits the generalizability of results. Variability across different depression databases adversely affects the results in cross corpus evaluations (CCEs). We propose to develop a generalized classifier for depression detection using a dilated Convolutional Neural Network which is trained on ACFs extracted from two depression databases. We show that ACFs derived from Vocal Tract Variables (TVs) show promise as a robust set of features for depression detection. Our model achieves relative accuracy improvements of ~10% compared to CCEs performed on models trained on a single database. We extend the study to show that fusing TVs and Mel-Frequency Cepstral Coefficients can further improve the performance of this classifier",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "31ed1854b90647e8145af842f97a35bbae294a20",
    "semantic_title": "generalized dilated cnn models for depression detection using inverted vocal tract variables",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ga_interspeech.html": {
    "title": "Learning Mutual Correlation in Multimodal Transformer for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Various studies have confirmed the necessity and benefits of leveraging multimodal features for SER, and the latest research results show that the temporal information captured by the transformer is very useful for improving multimodal speech emotion recognition. However, the dependency between different modalities and high-level temporal-feature learning using a deeper transformer is yet to be investigated. Thus, we propose a multimodal transformer with sharing weights for speech emotion recognition. The proposed network shares the weights across the modalities in each transformer layer to learn the correlation among multiple modalities. In addition, since the emotion contained in a speech generally include audio and text features, both of which have not only internal dependence but also mutual dependence, we design a deep multimodal attention mechanism to capture these two kinds of emotional dependence. We evaluated our model on the publicly available IEMOCAP dataset. The experimental results demonstrate that the proposed model yielded a promising result",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fae8caa28f806bfc70969a7d2444889f4ecda024",
    "semantic_title": "learning mutual correlation in multimodal transformer for speech emotion recognition",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21o_interspeech.html": {
    "title": "Time-Frequency Representation Learning with Graph Convolutional Network for Dialogue-Level Speech Emotion Recognition",
    "volume": "main",
    "abstract": "With the development of speech emotion recognition (SER), dialogue-level SER (DSER) is more aligned with actual scenarios. In this paper, we propose a DSER approach that includes two stages of representation learning: intra-utterance representation learning and inter-utterance representation learning. In the intra-utterance representation learning stage, traditional convolutional neural network (CNN) has demonstrated great success. However, the basic design of a CNN restricts its ability to model the local and global information in the spectrogram. Therefore, we propose a novel local-global representation learning method for the intra-utterance stage. The local information is learned by a time-frequency convolutional neural network (TFCNN), which we published previously. Here, we propose a time-frequency capsule neural network (TFCap) to model global information that can extract more stable global time-frequency information directly from spectrograms. In the inter-utterance stage, a graph convolutional network (GCN) is introduced to explore the relations between utterances in a dialog. Our proposed methods were evaluated on the IEMOCAP database. The proposed time-frequency based method in the intra-utterance stage achieves an absolute increase of 9.35% compared to CNN. By integrating GCN in the inter-utterance stage, the proposed approach achieves an absolute increase of 4.05% compared to the model in the previous stage",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b54e1a691cb9754fe93e4612db5aa85b203dd017",
    "semantic_title": "time-frequency representation learning with graph convolutional network for dialogue-level speech emotion recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mordido21_interspeech.html": {
    "title": "Compressing 1D Time-Channel Separable Convolutions Using Sparse Random Ternary Matrices",
    "volume": "main",
    "abstract": "We demonstrate that 1×1-convolutions in 1D time-channel separable convolutions may be replaced by constant, sparse random ternary matrices with weights in -1, 0, +1. Such layers do not perform any multiplications and do not require training. Moreover, the matrices may be generated on the chip during computation and therefore do not require any memory access. With the same parameter budget, we can afford deeper and more expressive models, improving the Pareto frontiers of existing models on several tasks. For command recognition on Google Speech Commands v1, we improve the state-of-the-art accuracy from 97.21% to 97.41% at the same network size. Alternatively, we can lower the cost of existing models. For speech recognition on Librispeech, we halve the number of weights to be trained while only sacrificing about 1% of the floating-point baseline's word error rate",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "432277645898ff2cc2fb55a1ca64b9f63ffe63a5",
    "semantic_title": "compressing 1d time-channel separable convolutions using sparse random ternary matrices",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cheng21c_interspeech.html": {
    "title": "Weakly Supervised Construction of ASR Systems from Massive Video Data",
    "volume": "main",
    "abstract": "Despite the rapid development of deep learning models, for real-world applications, building large-scale Automatic Speech Recognition (ASR) systems from scratch is still significantly challenging, mostly due to the time-consuming and financially-expensive process of annotating a large amount of audio data with transcripts. Although several self-supervised pre-training models have been proposed to learn speech representations, applying such models directly might be sub-optimal if more labeled, training data could be obtained without a large cost In this paper, we present VideoASR, a weakly supervised framework for constructing ASR systems from massive video data. As user-generated videos often contain human-speech audio roughly aligned with subtitles, we consider videos as an important knowledge source, and propose an effective approach to extract high-quality audio aligned with transcripts from videos based on text detection and Optical Character Recognition. The underlying ASR models can be fine-tuned to fit any domain-specific target training datasets after weakly supervised pre-training on automatically generated datasets. Extensive experiments show that VideoASR can easily produce state-of-the-art results on six public datasets for Mandarin speech recognition. In addition, the VideoASR framework has been deployed on the cloud to support various industrial-scale applications",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2709104e2409e2a69b5b45a323e3c032667bff3d",
    "semantic_title": "weakly supervised construction of asr systems from massive video data",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21l_interspeech.html": {
    "title": "Broadcasted Residual Learning for Efficient Keyword Spotting",
    "volume": "main",
    "abstract": "Keyword spotting is an important research field because it plays a key role in device wake-up and user interaction on smart devices. However, it is challenging to minimize errors while operating efficiently in devices with limited resources such as mobile phones. We present a method to achieve high accuracy with small model size and computational load. Our method configures most of the residual functions as 1D temporal convolution while still allows 2D convolution together using a broadcasted-residual connection that expands temporal output to frequency-temporal dimension. This residual mapping enables the network to effectively represent useful audio features with much less computation than conventional convolutional neural networks. We also propose a novel network architecture, Broadcasting-residual network (BC-ResNet), based on broadcasted residual learning and describe how to scale up the model according to the target device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7% top-1 accuracy on Google speech command datasets v1 and v2, respectively, and consistently outperform previous approaches, using fewer computations and parameters",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "cc672a56e01c714aacdc72c6bc8a391e25015dcd",
    "semantic_title": "broadcasted residual learning for efficient keyword spotting",
    "citation_count": 63,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/swaminathan21_interspeech.html": {
    "title": "CoDERT: Distilling Encoder Representations with Co-Learning for Transducer-Based Speech Recognition",
    "volume": "main",
    "abstract": "We propose a simple yet effective method to compress an RNN-Transducer (RNN-T) through the well-known knowledge distillation paradigm. We show that the transducer's encoder outputs naturally have a high entropy and contain rich information about acoustically similar word-piece confusions. This rich information is suppressed when combined with the lower entropy decoder outputs to produce the joint network logits. Consequently, we introduce an auxiliary loss to distill the encoder logits from a teacher transducer's encoder, and explore training strategies where this encoder distillation works effectively. We find that tandem training of teacher and student encoders with an inplace encoder distillation outperforms the use of a pre-trained and static teacher transducer. We also report an interesting phenomenon we refer to as implicit distillation, that occurs when the teacher and student encoders share the same decoder. Our experiments show 5.37–8.4% relative word error rate reductions (WERR) on in-house test sets, and 5.05–6.18% relative WERRs on LibriSpeech test sets",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7a656eece824b989e7f0d3c1fcb42d985d4a44a8",
    "semantic_title": "codert: distilling encoder representations with co-learning for transducer-based speech recognition",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gao21f_interspeech.html": {
    "title": "Extremely Low Footprint End-to-End ASR System for Smart Device",
    "volume": "main",
    "abstract": "Recently, end-to-end (E2E) speech recognition has become popular, since it can integrate the acoustic, pronunciation and language models into a single neural network, which outperforms conventional models. Among E2E approaches, attention-based models, e.g. Transformer, have emerged as being superior. Such models have opened the door to deployment of ASR on smart devices, however they still suffer from requiring a large number of model parameters. We propose an extremely low footprint E2E ASR system for smart devices, to achieve the goal of satisfying resource constraints without sacrificing recognition accuracy. We design cross-layer weight sharing to improve parameter efficiency and further exploit model compression methods including sparsification and quantization, to reduce memory storage and boost decoding efficiency. We evaluate our approaches on the public AISHELL-1 and AISHELL-2 benchmarks. On the AISHELL-2 task, the proposed method achieves more than 10× compression (model size reduces from 248 to 24MB), at the cost of only minor performance loss (CER reduces from 6.49% to 6.92%)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "a738c792cc9f3b39a5635caf33065e7256dbee29",
    "semantic_title": "extremely low footprint end-to-end asr system for smart device",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shangguan21_interspeech.html": {
    "title": "Dissecting User-Perceived Latency of On-Device E2E Speech Recognition",
    "volume": "main",
    "abstract": "As speech-enabled devices such as smartphones and smart speakers become increasingly ubiquitous, there is growing interest in building automatic speech recognition (ASR) systems that can run directly on-device; end-to-end (E2E) speech recognition models such as recurrent neural network transducers and their variants have recently emerged as prime candidates for this task. Apart from being accurate and compact, such systems need to decode speech with low user-perceived latency (UPL), producing words as soon as they are spoken. This work examines the impact of various techniques — model architectures, training criteria, decoding hyperparameters, and endpointer parameters — on UPL. Our analyses suggest that measures of model size (parameters, input chunk sizes), or measures of computation (e.g., FLOPS, RTF) that reflect the model's ability to process input frames are not always strongly correlated with observed UPL. Thus, conventional algorithmic latency measurements might be inadequate in accurately capturing latency observed when models are deployed on embedded devices. Instead, we find that factors affecting token emission latency, and endpointing behavior have a larger impact on UPL. We achieve the best trade-off between latency and word error rate when performing ASR jointly with endpointing, while utilizing the recently proposed alignment regularization mechanism",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3f3d53d33f91b91c87738fc350f1892e967d4d94",
    "semantic_title": "dissecting user-perceived latency of on-device e2e speech recognition",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/macoskey21b_interspeech.html": {
    "title": "Amortized Neural Networks for Low-Latency Speech Recognition",
    "volume": "main",
    "abstract": "We introduce Amortized Neural Networks (AmNets), a compute cost- and latency-aware network architecture particularly well-suited for sequence modeling tasks. We apply AmNets to the Recurrent Neural Network Transducer (RNN-T) to reduce compute cost and latency for an automatic speech recognition (ASR) task. The AmNets RNN-T architecture enables the network to dynamically switch between encoder branches on a frame-by-frame basis. Branches are constructed with variable levels of compute cost and model capacity. Here, we achieve variable compute for two well-known candidate techniques: one using sparse pruning and the other using matrix factorization. Frame-by-frame switching is determined by an arbitrator network that requires negligible compute overhead. We present results using both architectures on LibriSpeech data and show that our proposed architecture can reduce inference cost by up to 45% and latency to nearly real-time without incurring a loss in accuracy",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "211c44afff96be36538e6d454286f95579d883e4",
    "semantic_title": "amortized neural networks for low-latency speech recognition",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/botros21_interspeech.html": {
    "title": "Tied & Reduced RNN-T Decoder",
    "volume": "main",
    "abstract": "Previous works on the Recurrent Neural Network-Transducer (RNN-T) models have shown that, under some conditions, it is possible to simplify its prediction network with little or no loss in recognition accuracy [1, 2, 3]. This is done by limiting the context size of previous labels and/or using a simpler architecture for its layers instead of LSTMs. The benefits of such changes include reduction in model size, faster inference and power savings, which are all useful for on-device applications In this work, we study ways to make the RNN-T decoder (prediction network + joint network) smaller and faster without degradation in recognition performance. Our prediction network performs a simple weighted averaging of the input embeddings, and shares its embedding matrix weights with the joint network's output layer (a.k.a. weight tying, commonly used in language modeling [4]). This simple design, when used in conjunction with additional Edit-based Minimum Bayes Risk (EMBR) training, reduces the RNN-T Decoder from 23M parameters to just 2M, without affecting word-error rate (WER)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7f31856eb04f6ea49aff276621de66ca85a1a554",
    "semantic_title": "tied & reduced rnn-t decoder",
    "citation_count": 31,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21m_interspeech.html": {
    "title": "PQK: Model Compression via Pruning, Quantization, and Knowledge Distillation",
    "volume": "main",
    "abstract": "As edge devices become prevalent, deploying Deep Neural Networks (DNN) on edge devices has become a critical issue. However, DNN requires a high computational resource which is rarely available for edge devices. To handle this, we propose a novel model compression method for the devices with limited computational resources, called consisting of pruning, quantization, and knowledge distillation (KD) processes. Unlike traditional pruning and KD, PQK makes use of unimportant weights pruned in the pruning process to make a teacher network for training a better student network without pre-training the teacher model. PQK has two phases. Phase 1 exploits iterative pruning and quantization-aware training to make a lightweight and power-efficient model. In phase 2, we make a teacher network by adding unimportant weights unused in phase 1 to a pruned network. By using this teacher network, we train the pruned network as a student network. In doing so, we do not need a pre-trained teacher network for the KD framework because the teacher and the student networks coexist within the same network (See Fig. 1). We apply our method to the recognition model and verify the effectiveness of PQK on keyword spotting (KWS) and image recognition",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fd504f20b3ce6cb4e09633941cefa388311d30ce",
    "semantic_title": "pqk: model compression via pruning, quantization, and knowledge distillation",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nagaraja21_interspeech.html": {
    "title": "Collaborative Training of Acoustic Encoders for Speech Recognition",
    "volume": "main",
    "abstract": "On-device speech recognition requires training models of different sizes for deploying on devices with various computational budgets. When building such different models, we can benefit from training them jointly to take advantage of the knowledge shared between them. Joint training is also efficient since it reduces the redundancy in the training procedure's data handling operations. We propose a method for collaboratively training acoustic encoders of different sizes for speech recognition. We use a sequence transducer setup where different acoustic encoders share a common predictor and joiner modules. The acoustic encoders are also trained using co-distillation through an auxiliary task for frame level chenone prediction, along with the transducer loss. We perform experiments using the LibriSpeech corpus and demonstrate that the collaboratively trained acoustic encoders can provide up to a 11% relative improvement in the word error rate on both the test partitions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f6ea4b191e8b4ab2efe2f772f4947f84b44bcf04",
    "semantic_title": "collaborative training of acoustic encoders for speech recognition",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ha_interspeech.html": {
    "title": "Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end models are favored in automatic speech recognition (ASR) because of their simplified system structure and superior performance. Among these models, Transformer and Conformer have achieved state-of-the-art recognition accuracy in which self-attention plays a vital role in capturing important global information. However, the time and memory complexity of self-attention increases squarely with the length of the sentence. In this paper, a prob-sparse self-attention mechanism is introduced into Conformer to sparse the computing process of self-attention in order to accelerate inference speed and reduce space consumption. Specifically, we adopt a Kullback-Leibler divergence based sparsity measurement for each query to decide whether we compute the attention function on this query. By using the prob-sparse attention mechanism, we achieve impressively 8% to 45% inference speed-up and 15% to 45% memory usage reduction of the self-attention module of Conformer Transducer while maintaining the same level of error rate",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b8654c3c10a8d569a7410ffc6fffcb6fa5b5a434",
    "semantic_title": "efficient conformer with prob-sparse attention mechanism for end-to-endspeech recognition",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/parcollet21_interspeech.html": {
    "title": "The Energy and Carbon Footprint of Training End-to-End Speech Recognizers",
    "volume": "main",
    "abstract": "Deep learning contributes to reaching higher levels of artificial intelligence. Due to its pervasive adoption, however, growing concerns on the environmental impact of this technology have been raised. In particular, the energy consumed at training and inference time by modern neural networks is far from being negligible and will increase even further due to the deployment of ever larger models This work investigates for the first time the carbon cost of end-to-end automatic speech recognition (ASR). First, it quantifies the amount of CO emitted while training state-of-the-art (SOTA) ASR systems on a university-scale cluster. Then, it shows that a tiny performance improvement comes at an extremely high carbon cost. For instance, the conducted experiments reveal that a SOTA Transformer emits 50% of its total training released CO solely to achieve a final decrease of 0.3 of the word error rate. With this study, we hope to raise awareness on this crucial topic and we provide guidelines, insights, and estimates enabling researchers to better assess the environmental impact of training speech technologies",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3f6ac5e9d67484e9c64bfb1ec546400070073e63",
    "semantic_title": "the energy and carbon footprint of training end-to-end speech recognizers",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21v_interspeech.html": {
    "title": "Graph-Based Label Propagation for Semi-Supervised Speaker Identification",
    "volume": "main",
    "abstract": "Speaker identification in the household scenario (e.g., for smart speakers) is typically based on only a few enrollment utterances but a much larger set of unlabeled data, suggesting semi-supervised learning to improve speaker profiles. We propose a graph-based semi-supervised learning approach for speaker identification in the household scenario, to leverage the unlabeled speech samples. In contrast to most of the works in speaker recognition that focus on speaker-discriminative embeddings, this work focuses on speaker label inference (scoring). Given a pre-trained embedding extractor, graph-based learning allows us to integrate information about both labeled and unlabeled utterances. Considering each utterance as a graph node, we represent pairwise utterance similarity scores as edge weights. Graphs are constructed per household, and speaker identities are propagated to unlabeled nodes to optimize a global consistency criterion. We show in experiments on the VoxCeleb dataset that this approach makes effective use of unlabeled data and improves speaker identification accuracy compared to two state-of-the-art scoring methods as well as their semi-supervised variants based on pseudo-labels",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "e197dce74eb850428b98c1b5c5820464a7f8fd1f",
    "semantic_title": "graph-based label propagation for semi-supervised speaker identification",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21q_interspeech.html": {
    "title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition",
    "volume": "main",
    "abstract": "By implicitly recognizing a user based on his/her speech input, speaker identification enables many downstream applications, such as personalized system behavior and expedited shopping checkouts. Based on whether the speech content is constrained or not, both text-dependent (TD) and text-independent (TI) speaker recognition models may be used. We wish to combine the advantages of both types of models through an ensemble system to make more reliable predictions. However, any such combined approach has to be robust to incomplete inputs, i.e., when either TD or TI input is missing. As a solution we propose a fusion of embeddings network ( foenet) architecture, combining joint learning with neural attention. We compare foenet with four competitive baseline methods on a dataset of voice assistant inputs, and show that it achieves higher accuracy than the baseline and score fusion methods, especially in the presence of incomplete inputs",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "abab1958a7421a225cac719041c7e953ec7517c7",
    "semantic_title": "fusion of embeddings networks for robust combination of text dependent and independent speaker recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cumani21_interspeech.html": {
    "title": "A Generative Model for Duration-Dependent Score Calibration",
    "volume": "main",
    "abstract": "In this work we introduce a generative score calibration model for speaker verification systems able to explicitly account for utterance-dependent miscalibration sources, with a focus on segment duration. The model is theoretically motivated by an analysis of the effects of distribution mismatch on the scores produced by Probabilistic Linear Discriminant Analysis (PLDA), and extends our previous investigation on the distribution of well-calibrated PLDA log-likelihood ratios. We characterize target and non-target scores by means of Variance-Gamma densities, whose parameters represent effective between and within-class variabilities. Experimental results on SRE 2019 show that the proposed method improves both calibration and verification accuracy with respect to duration-agnostic models and to duration-aware discriminative methods",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "389ae28d528e24194ed733785ff6828e74342dbd",
    "semantic_title": "a generative model for duration-dependent score calibration",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pelecanos21_interspeech.html": {
    "title": "Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition",
    "volume": "main",
    "abstract": "Many neural network speaker recognition systems model each speaker using a fixed-dimensional embedding vector. These embeddings are generally compared using either linear or 2nd-order scoring and, until recently, do not handle utterance-specific uncertainty. In this work we propose scoring these representations in a way that can capture uncertainty, enroll/test asymmetry and additional non-linear information. This is achieved by incorporating a 2nd-stage neural network (known as a decision network) as part of an end-to-end training regimen. In particular, we propose the concept of decision residual networks which involves the use of a compact decision network to leverage cosine scores and to model the residual signal that's needed. Additionally, we present a modification to the generalized end-to-end softmax loss function to target the separation of same/different speaker scores. We observed significant performance gains for the two techniques",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3127fbc4d019ef80f2afda57634e0b0aca61ecf8",
    "semantic_title": "dr-vectors: decision residual networks and an improved loss for speaker recognition",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kataria21b_interspeech.html": {
    "title": "Multi-Channel Speaker Verification for Single and Multi-Talker Speech",
    "volume": "main",
    "abstract": "To improve speaker verification in real scenarios with interference speakers, noise, and reverberation, we propose to bring together advancements made in multi-channel speech features. Specifically, we combine , , and features, which includes inter-channel phase difference, multichannel convolutions, directional power ratio features, and angle features. To maximally leverage supervised learning, our framework is also equipped with multi-channel speech enhancement and voice activity detection. On all simulated, replayed, and real recordings, we observe large and consistent improvements at various degradation levels. On real recordings of multi-talker speech, we achieve a 36% relative reduction in equal error rate w.r.t. single-channel baseline. We find the improvements from speaker-dependent features more consistent in multi-talker conditions than clean. Lastly, we investigate if the learned multi-channel speaker embedding space can be made more discriminative through a contrastive loss-based fine-tuning. With a simple choice of Triplet loss, we observe a further 8.3% relative reduction in EER",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "49ef93a4d78b5c73f1ce9c075a5cc73c8fd06502",
    "semantic_title": "multi-channel speaker verification for single and multi-talker speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/padfield21_interspeech.html": {
    "title": "Chronological Self-Training for Real-Time Speaker Diarization",
    "volume": "main",
    "abstract": "Diarization partitions an audio stream into segments based on the voices of the speakers. Real-time diarization systems that include an enrollment step should limit enrollment training samples to reduce user interaction time. Although training on a small number of samples yields poor performance, we show that the accuracy can be improved dramatically using a chronological self-training approach. We studied the tradeoff between training time and classification performance and found that 1 second is sufficient to reach over 95% accuracy. We evaluated on 700 audio conversation files of about 10 minutes each from 6 different languages and demonstrated average diarization error rates as low as 10%",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "984cba97eaf27ff2c9c931e6c0c509abbefab195",
    "semantic_title": "chronological self-training for real-time speaker diarization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xiao21b_interspeech.html": {
    "title": "Adaptive Margin Circle Loss for Speaker Verification",
    "volume": "main",
    "abstract": "Deep-Neural-Network (DNN) based speaker verification systems use the angular softmax loss with margin penalties to enhance the intra-class compactness of speaker embeddings, which achieved remarkable performance. In this paper, we propose a novel angular loss function called adaptive margin circle loss for speaker verification. The stage-based margin and chunk-based margin are applied to improve the angular discrimination of circle loss on the training set. The analysis on gradients shows that, compared with the previous angular loss like Additive Margin Softmax(Am-Softmax), circle loss has flexible optimization and definite convergence status. Experiments are carried out on the Voxceleb and SITW. By applying adaptive margin circle loss, our best system achieves 1.31%EER on Voxceleb1 and 2.13% on SITW core-core",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6a5166689b2a317ce18569ba2a47a12d491c6e30",
    "semantic_title": "adaptive margin circle loss for speaker verification",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/obrien21b_interspeech.html": {
    "title": "Presentation Matters: Evaluating Speaker Identification Tasks",
    "volume": "main",
    "abstract": "This paper details our evaluations and comparisons of speaker identification (SID) performance by listeners across different tasks. Experiment 1 participants completed traditional target-lineup (1-out-of-N speakers or out-of-set speaker) and binary (speaker verification) tasks. Experiment 2 participants completed trials online by using a method by grouping speech recordings into speaker-specific clusters. Both studies employed similar speech recordings from the PTSVOX corpus. Our results showed participants who completed the binary and clustering tasks had higher accuracy than those who completed the target-lineup task. We also observed that independent of the tasks participants found some speakers significantly more difficult to identify relative to their foils. Pearson correlation procedures showed significant negative correlations between accuracy and task-dependent temporal-based metrics across tasks, where an increase in time required to make determinations yielded a decrease in perceptual SID performance. These findings underscored the important role of SID task design and the process of selecting speech recordings. Future work aims to examine the relationship between different perceptual SID task performances and scores generated by automatic speaker verification systems",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6c22ff5d1437636b7314074b621a30883f3d15f5",
    "semantic_title": "presentation matters: evaluating speaker identification tasks",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tong21_interspeech.html": {
    "title": "Automatic Error Correction for Speaker Embedding Learning with Noisy Labels",
    "volume": "main",
    "abstract": "Despite the superior performance deep neural networks have achieved in speaker verification tasks, much of their success benefits from the availability of large-scale and carefully labeled datasets. However, noisy labels often occur during data collection. In this paper, we propose an automatic error correction method for deep speaker embedding learning with noisy labels. Specifically, a label noise correction loss is proposed that leverages a model's generalization capability to correct noisy labels during training. In addition, we improve the vanilla AM-Softmax to estimate a more robust speaker posterior by introducing sub-centers. When applied on the VoxCeleb dataset, the proposed method performs gracefully when noisy labels are introduced. Moreover, when combining with the Bayesian estimation of PLDA with noisy training labels at the back-end, the whole system performs better under conditions in which noisy labels are present",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "03b7a9f6001021a65c65f9b8b30c2dd26f3fd8fc",
    "semantic_title": "automatic error correction for speaker embedding learning with noisy labels",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liao21_interspeech.html": {
    "title": "An Integrated Framework for Two-Pass Personalized Voice Trigger",
    "volume": "main",
    "abstract": "In this paper, we present the XMUSPEECH system for Task 1 of 2020 Personalized Voice Trigger Challenge (PVTC2020). Task 1 is a joint wake-up word detection with speaker verification on close talking data. The whole system consists of a keyword spotting (KWS) sub-system and a speaker verification (SV) sub-system. For the KWS system, we applied a Temporal Depthwise Separable Convolution Residual Network (TDSC-ResNet) to improve the system's performance. For the SV system, we proposed a multi-task learning network, where phonetic branch is trained with the character label of the utterance, and speaker branch is trained with the label of the speaker. Phonetic branch is optimized with connectionist temporal classification (CTC) loss, which is treated as an auxiliary module for speaker branch. Experiments show that our system gets significant improvements compared with baseline system",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "bee2962e881301dfaaf7bb8e02f31001c91a838c",
    "semantic_title": "an integrated framework for two-pass personalized voice trigger",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lian21_interspeech.html": {
    "title": "Masked Proxy Loss for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Open-set speaker recognition can be regarded as a metric learning problem, which is to maximize inter-class variance and minimize intra-class variance. Supervised metric learning can be categorized into pair-based learning and proxy-based learning [1]. Most of the existing metric learning objectives belong to the former division, the performance of which is either highly dependent on sample mining strategy or restricted by insufficient label information in the mini-batch. Proxy-based losses mitigate both shortcomings, however, fine-grained connections among entities are either not or indirectly leveraged. This paper proposes a Masked Proxy (MP) loss which directly incorporates both proxy-based relationship and pair-based relationship. We further propose Multinomial Masked Proxy (MMP) loss to leverage the hardness of speaker pairs. These methods have been applied to evaluate on VoxCeleb test set and reach state-of-the-art Equal Error Rate (EER)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2c0473108d1c0b13b65107970ae76de7ef595e99",
    "semantic_title": "masked proxy loss for text-independent speaker verification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lee21h_interspeech.html": {
    "title": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech",
    "volume": "main",
    "abstract": "Previous works on neural text-to-speech (TTS) have been addressed on limited speed in training and inference time, robustness for difficult synthesis conditions, expressiveness, and controllability. Although several approaches resolve some limitations, there has been no attempt to solve all weaknesses at once. In this paper, we propose STYLER, an expressive and controllable TTS framework with high-speed and robust synthesis. Our novel audio-text aligning method called Mel Calibrator and excluding autoregressive decoding enable rapid training and inference and robust synthesis on unseen data. Also, disentangled style factor modeling under supervision enlarges the controllability in synthesizing process leading to expressive TTS. On top of it, a novel noise modeling pipeline using domain adversarial training and Residual Decoding empowers noise-robust style transfer, decomposing the noise without any additional label. Various experiments demonstrate that STYLER is more effective in speed and robustness than expressive TTS with autoregressive decoding and more expressive and controllable than reading style non-autoregressive TTS. Synthesis samples and experiment results are provided via our demo page, and code is available publicly",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3adc874c506c4a9e848f5ef49696452fa1cdddfd",
    "semantic_title": "styler: style factor modeling with rapidity and robustness via speech decomposition for expressive and controllable neural text to speech",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/liu21p_interspeech.html": {
    "title": "Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability",
    "volume": "main",
    "abstract": "Emotional text-to-speech synthesis (ETTS) has seen much progress in recent years. However, the generated voice is often not perceptually identifiable by its intended emotion category. To address this problem, we propose a new interactive training paradigm for ETTS, denoted as , which seeks to directly improve the emotion discriminability by interacting with a speech emotion recognition (SER) model. Moreover, we formulate an iterative training strategy with reinforcement learning to ensure the quality of optimization. Experimental results demonstrate that the proposed outperforms the state-of-the-art baselines by rendering speech with more accurate emotion style. To our best knowledge, this is the first study of reinforcement learning in emotional text-to-speech synthesis",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b6e836b131b2abe9aa82668d65c589aa8c56ea22",
    "semantic_title": "reinforcement learning for emotional text-to-speech synthesis with improved emotion discriminability",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/sivaprasad21_interspeech.html": {
    "title": "Emotional Prosody Control for Speech Generation",
    "volume": "main",
    "abstract": "Machine-generated speech is characterized by its limited or unnatural emotional variation. Current text to speech systems generates speech with either a flat emotion, emotion selected from a predefined set, average variation learned from prosody sequences in training data or transferred from a source style. We propose a text to speech (TTS) system, where a user can choose the emotion of generated speech from a continuous and meaningful emotion space (Arousal-Valence space). The proposed TTS system can generate speech from the text in any speaker's style, with fine control of emotion. We show that the system works on emotion unseen during training and can scale to previously unseen speakers given his/her speech sample. Our work expands the horizon of the state-of-the-art FastSpeech2 backbone to a multi-speaker setting and gives it much-coveted continuous (and interpretable) affective control, without any observable degradation in the quality of the synthesized speech. Audio samples are publicly available",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "794ade10864b198f5b007f05eeb9220b219b72f8",
    "semantic_title": "emotional prosody control for speech generation",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cong21b_interspeech.html": {
    "title": "Controllable Context-Aware Conversational Speech Synthesis",
    "volume": "main",
    "abstract": "In spoken conversations, spontaneous behaviors like filled pause and prolongations always happen. Conversational partner tends to align features of their speech with their interlocutor which is known as entrainment. To produce human-like conversations, we propose a unified controllable spontaneous conversational speech synthesis framework to model the above two phenomena. Specifically, we use explicit labels to represent two typical spontaneous behaviors and in the acoustic model and develop a neural network based predictor to predict the occurrences of the two behaviors from text. We subsequently develop an algorithm based on the predictor to control the occurrence frequency of the behaviors, making the synthesized speech vary from less disfluent to more disfluent. To model the speech entrainment at acoustic level, we utilize a context acoustic encoder to extract a global style embedding from the previous speech conditioning on the synthesizing of current speech. Furthermore, since the current and previous utterances belong to the different speakers in a conversation, we add a domain adversarial training module to eliminate the speaker-related information in the acoustic encoder while maintaining the style-related information. Experiments show that our proposed approach can synthesize realistic conversations and control the occurrences of the spontaneous behaviors naturally",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6bb9cc7a469e68441f52c6aee91c34a39c58c580",
    "semantic_title": "controllable context-aware conversational speech synthesis",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kim21n_interspeech.html": {
    "title": "Expressive Text-to-Speech Using Style Tag",
    "volume": "main",
    "abstract": "As recent text-to-speech (TTS) systems have been rapidly improved in speech quality and generation speed, many researchers now focus on a more challenging issue: expressive TTS. To control speaking styles, existing expressive TTS models use categorical style index or reference speech as style input. In this work, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that utilizes a style tag written in natural language. Using a style-tagged TTS dataset and a pre-trained language model, we modeled the relationship between linguistic embedding and speaking style domain, which enables our model to work even with style tags unseen during training. As style tag is written in natural language, it can control speaking style in a more intuitive, interpretable, and scalable way compared with style index or reference speech. In addition, in terms of model architecture, we propose an efficient non-autoregressive (NAR) TTS architecture with single-stage training. The experimental result shows that ST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech quality and expressiveness",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "1fd65f9dbf4d75086c757c215bd0015d576ea862",
    "semantic_title": "expressive text-to-speech using style tag",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yan21d_interspeech.html": {
    "title": "Adaptive Text to Speech for Spontaneous Style",
    "volume": "main",
    "abstract": "While recent text to speech (TTS) models perform very well in synthesizing reading-style (e.g., audiobook) speech, it is still challenging to synthesize spontaneous-style speech (e.g., podcast or conversation), mainly because of two reasons: 1) the lack of training data for spontaneous speech; 2) the difficulty in modeling the filled pauses ( and ) and diverse rhythms in spontaneous speech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that fine-tunes a well-trained reading-style TTS model for spontaneous-style speech. Specifically, 1) to insert filled pauses (FP) in the text sequence appropriately, we introduce an FP predictor to the TTS model; 2) to model the varying rhythms, we introduce a duration predictor based on mixture of experts (MoE), which contains three experts responsible for the generation of fast, medium and slow speech respectively, and fine-tune it as well as the pitch predictor for rhythm adaptation; 3) to adapt to other speaker timbre, we fine-tune some parameters in the decoder with few speech data. To address the challenge of lack of training data, we mine a spontaneous speech dataset to support our research this work and facilitate future research on spontaneous TTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and rhythms in spontaneous styles, and achieves much better MOS and SMOS scores than previous adaptive TTS systems",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "88dfd9578a659902e061c3937eaceb44c0e18044",
    "semantic_title": "adaptive text to speech for spontaneous style",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/li21r_interspeech.html": {
    "title": "Towards Multi-Scale Style Control for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "This paper introduces a multi-scale speech style modeling method for end-to-end expressive speech synthesis. The proposed method employs a multi-scale reference encoder to extract both the global-scale utterance-level and the local-scale quasi-phoneme-level style features of the target speech, which are then fed into the speech synthesis model as an extension to the input phoneme sequence. During training time, the multi-scale style model could be jointly trained with the speech synthesis model in an end-to-end fashion. By applying the proposed method to style transfer task, experimental results indicate that the controllability of the multi-scale speech style model and the expressiveness of the synthesized speech are greatly improved. Moreover, by assigning different reference speeches to extraction of style on each scale, the flexibility of the proposed method is further revealed",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b23bc1e8732521cf83586e1a0ba924d7a89cdb17",
    "semantic_title": "towards multi-scale style control for expressive speech synthesis",
    "citation_count": 28,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pan21d_interspeech.html": {
    "title": "Cross-Speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis",
    "volume": "main",
    "abstract": "Cross-speaker style transfer is crucial to the applications of multi-style and expressive speech synthesis at scale. It does not require the target speakers to be experts in expressing all styles and to collect corresponding recordings for model training. However, the performances of existing style transfer methods are still far behind real application needs. The root causes are mainly twofold. Firstly, the style embedding extracted from single reference speech can hardly provide fine-grained and appropriate prosody information for arbitrary text to synthesize. Secondly, in these models the content/text, prosody, and speaker timbre are usually highly entangled, it's therefore not realistic to expect a satisfied result when freely combining these components, such as to transfer speaking style between speakers. In this paper, we propose a cross-speaker style transfer text-to-speech (TTS) model with explicit prosody bottleneck. The prosody bottleneck builds up the kernels accounting for speaking style robustly, and disentangles the prosody from content and speaker timbre, therefore guarantees high quality cross-speaker style transfer. Evaluation result shows the proposed method even achieves on-par performance with source speaker's speaker-dependent (SD) model in objective measurement of prosody, and significantly outperforms the cycle consistency and GMVAE-based baselines in objective and subjective evaluations",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "02cfdb9ba150c4f31ec8727a39947d2a39ee3c77",
    "semantic_title": "cross-speaker style transfer with prosody bottleneck in neural speech synthesis",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tan21_interspeech.html": {
    "title": "Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement",
    "volume": "main",
    "abstract": "This paper presents a novel design of neural network system for fine-grained style modeling, transfer and prediction in expressive text-to-speech (TTS) synthesis. Fine-grained modeling is realized by extracting style embeddings from the mel-spectrograms of phone-level speech segments. Collaborative learning and adversarial learning strategies are applied in order to achieve effective disentanglement of content and style factors in speech and alleviate the \"content leakage\" problem in style modeling. The proposed system can be used for varying-content speech style transfer in the single-speaker scenario. The results of objective and subjective evaluation show that our system performs better than other fine-grained speech style transfer models, especially in the aspect of content preservation. By incorporating a style predictor, the proposed system can also be used for text-to-speech synthesis. Audio samples are provided for system demonstration",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "d44cb6100b227947d18c3a5ec76c7d9264e085f6",
    "semantic_title": "fine-grained style modeling, transfer and prediction in text-to-speech synthesis via phone-level content-style disentanglement",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/an21b_interspeech.html": {
    "title": "Improving Performance of Seen and Unseen Speech Style Transfer in End-to-End Neural TTS",
    "volume": "main",
    "abstract": "End-to-end neural TTS training has shown improved performance in speech style transfer. However, the improvement is still limited by the training data in both target styles and speakers. Inadequate style transfer performance occurs when the trained TTS tries to transfer the speech to a target style from a new speaker with an unknown, arbitrary style. In this paper, we propose a new approach to style transfer for both seen and unseen styles, with disjoint, multi-style datasets, i.e., datasets of different styles are recorded, each individual style is by one speaker with multiple utterances. To encode the style information, we adopt an inverse autoregressive flow (IAF) structure to improve the variational inference. The whole system is optimized to minimize a weighed sum of four different loss functions: 1) a reconstruction loss to measure the distortions in both source and target reconstructions; 2) an adversarial loss to \"fool\" a well-trained discriminator; 3) a style distortion loss to measure the expected style loss after the transfer; 4) a cycle consistency loss to preserve the speaker identity of the source after the transfer. Experiments demonstrate, both objectively and subjectively, the effectiveness of the proposed approach for seen and unseen style transfer tasks. The performance of the new approach is better and more robust than those of four baseline systems of the prior art",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "218a418cb605274b9db1b5ffd46053349b074167",
    "semantic_title": "improving performance of seen and unseen speech style transfer in end-to-end neural tts",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shechtman21_interspeech.html": {
    "title": "Synthesis of Expressive Speaking Styles with Limited Training Data in a Multi-Speaker, Prosody-Controllable Sequence-to-Sequence Architecture",
    "volume": "main",
    "abstract": "Although Sequence-to-Sequence (S2S) architectures have become state-of-the-art in speech synthesis, the best models benefit from access to moderate-to-large amounts of training data, posing a resource bottleneck when we are interested in generating speech in a variety of expressive styles. In this work we explore a S2S architecture variant that is capable of generating a variety of stylistic expressive variations observed in a limited amount of training data, and of transplanting that style to a neutral target speaker for whom no labeled expressive resources exist. The architecture is furthermore controllable, allowing the user to select an operating point that conveys a desired level of expressiveness. We evaluate this proposal against a classically supervised baseline via perceptual listening tests, and demonstrate that i) it is able to outperform the baseline in terms of its generalizability to neutral speakers, ii) it is strongly preferred in terms of its ability to convey expressiveness, and iii) it provides a reasonable trade-off between expressiveness and naturalness, allowing the user to tune it to the particular demands of a given application",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "98331b5cedf38e5513fd3306fb5a1e5f87b8f866",
    "semantic_title": "synthesis of expressive speaking styles with limited training data in a multi-speaker, prosody-controllable sequence-to-sequence architecture",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/dao21_interspeech.html": {
    "title": "Intent Detection and Slot Filling for Vietnamese",
    "volume": "main",
    "abstract": "Intent detection and slot filling are important tasks in spoken and natural language understanding. However, Vietnamese is a low-resource language in these research topics. In this paper, we present the public intent detection and slot filling dataset for Vietnamese. In addition, we also propose a joint model for intent detection and slot filling, that extends the recent state-of-the-art JointBERT+CRF model [1] with an intent-slot attention layer to explicitly incorporate intent context information into slot filling via \"soft\" intent label embedding. Experimental results on our Vietnamese dataset show that our proposed model significantly outperforms JointBERT+CRF. We publicly release our dataset and the implementation of our model",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4ca2cf5671d16e49d36f20d8416f6ed3596da459",
    "semantic_title": "intent detection and slot filling for vietnamese",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/lin21k_interspeech.html": {
    "title": "Augmenting Slot Values and Contexts for Spoken Language Understanding with Pretrained Models",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) is one essential step in building a dialogue system. Due to the expensive cost of obtaining the labeled data, SLU suffers from the data scarcity problem. Therefore, in this paper, we focus on data augmentation for slot filling task in SLU. To achieve that, we aim at generating more diverse data based on existing data. Specifically, we try to exploit the latent language knowledge from pretrained language models by finetuning them. We propose two strategies for finetuning process: value-based and context-based augmentation. Experimental results on two public SLU datasets have shown that compared with existing data augmentation methods, our proposed method can generate more diverse sentences and significantly improve the performance on SLU",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5a36dada91c36a7cb5e422443508b35d1b535b56",
    "semantic_title": "augmenting slot values and contexts for spoken language understanding with pretrained models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gaspers21_interspeech.html": {
    "title": "The Impact of Intent Distribution Mismatch on Semi-Supervised Spoken Language Understanding",
    "volume": "main",
    "abstract": "With the expanding role of voice-controlled devices, bootstrapping spoken language understanding models from little labeled data becomes essential. Semi-supervised learning is a common technique to improve model performance when labeled data is scarce. In a real-world production system, the labeled data and the online test data often may come from different distributions. In this work, we use semi-supervised learning based on pseudo-labeling with an auxiliary task on incoming unlabeled noisy data, which is closer to the test distribution. We demonstrate empirically that our approach can mitigate negative effects arising from training with non-representative labeled data as well as the negative impacts of noises in the data, which are introduced by pseudo-labeling and automatic speech recognition",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4c972f1f7f8dec4811031b20af511d9ca1b6062a",
    "semantic_title": "the impact of intent distribution mismatch on semi-supervised spoken language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jiang21c_interspeech.html": {
    "title": "Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification",
    "volume": "main",
    "abstract": "End-to-end intent classification using speech has numerous advantages compared to the conventional pipeline approach using automatic speech recognition (ASR), followed by natural language processing modules. It attempts to predict intent from speech without using an intermediate ASR module. However, such end-to-end framework suffers from the unavailability of large speech resources with higher acoustic variation in spoken language understanding. In this work, we exploit the scope of the transformer distillation method that is specifically designed for knowledge distillation from a transformer based language model to a transformer based speech model. In this regard, we leverage the reliable and widely used bidirectional encoder representations from transformers (BERT) model as a language model and transfer the knowledge to build an acoustic model for intent classification using the speech. In particular, a multilevel transformer based teacher-student model is designed, and knowledge distillation is performed across attention and hidden sub-layers of different transformer layers of the student and teacher models. We achieve an intent classification accuracy of 99.10% and 88.79% for Fluent speech corpus and ATIS database, respectively. Further, the proposed method demonstrates better performance and robustness in acoustically degraded condition compared to the baseline method",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "19f03f571cf84559e8844de5e9dcc5b82fcdbed5",
    "semantic_title": "knowledge distillation from bert transformer to speech transformer for intent classification",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ia_interspeech.html": {
    "title": "Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-Trained DNN-HMM-Based Acoustic-Phonetic Model",
    "volume": "main",
    "abstract": "In spoken language understanding (SLU), what the user says is converted to his/her intent. Recent work on end-to-end SLU has shown that accuracy can be improved via pre-training approaches. We revisit ideas presented by Lugosch et al. using speech pre-training and three-module modeling; however, to ease construction of the end-to-end SLU model, we use as our phoneme module an open-source acoustic-phonetic model from a DNN-HMM hybrid automatic speech recognition (ASR) system instead of training one from scratch. Hence we fine-tune on speech only for the word module, and we apply multi-target learning (MTL) on the word and intent modules to jointly optimize SLU performance. MTL yields a relative reduction of 40% in intent-classification error rates (from 1.0% to 0.6%). Note that our three-module model is a streaming method. The final outcome of the proposed three-module modeling approach yields an intent accuracy of 99.4% on FluentSpeech, an intent error rate reduction of 50% compared to that of Lugosch et al. Although we focus on real-time streaming methods, we also list non-streaming methods for comparison",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2b40acf8f05ce3d0a698b2f41b5a2bfed0ed864a",
    "semantic_title": "three-module modeling for end-to-end spoken language understanding using pre-trained dnn-hmm-based acoustic-phonetic model",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cha21_interspeech.html": {
    "title": "Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs",
    "volume": "main",
    "abstract": "A major focus of recent research in spoken language understanding (SLU) has been on the end-to-end approach where a single model can predict intents directly from speech inputs without intermediate transcripts. However, this approach presents some challenges. First, since speech can be considered as personally identifiable information, in some cases only automatic speech recognition (ASR) transcripts are accessible. Second, intent-labeled speech data is scarce. To address the first challenge, we propose a novel system that can predict intents from flexible types of inputs: speech, ASR transcripts, or both. We demonstrate strong performance for either modality separately, and when both speech and ASR transcripts are available, through system combination, we achieve better results than using a single input modality. To address the second challenge, we leverage a semantically robust pre-trained BERT model and adopt a cross-modal system that co-trains text embeddings and acoustic embeddings in a shared latent space. We further enhance this system by utilizing an acoustic module pre-trained on LibriSpeech and domain-adapting the text module on our target datasets. Our experiments show significant advantages for these pre-training and fine-tuning strategies, resulting in a system that achieves competitive intent-classification performance on Snips SLU and Fluent Speech Commands datasets",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "0875dd82807997e336f11cacb817c6536ef6b7a1",
    "semantic_title": "speak or chat with me: end-to-end spoken language understanding system with flexible inputs",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ha_interspeech.html": {
    "title": "End-to-End Cross-Lingual Spoken Language Understanding Model with Multilingual Pretraining",
    "volume": "main",
    "abstract": "The spoken language understanding (SLU) plays an essential role in the field of human-computer interaction. Most of the current SLU systems are cascade systems of automatic speech recognition (ASR) and natural language understanding (NLU). Error propagation and scarcity of annotated speech data are two common difficulties for resource-poor languages. To solve them, we propose a simple but effective end-to-end cross-lingual spoken language understanding model based on XLSR-53, which is a pretrained model in 53 languages by the Facebook research team. The end-to-end approach avoids error propagation and the multilingual pretraining reduces data annotation requirements. Our proposed method achieves 99.71% on the Fluent Speech Commands (FSC) English database and 79.89% on the CATSLU-MAP Chinese database, in intent classification accuracy. To the best of our knowledge, the former is the reported best result on the FSC database",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "384609e86ad41cb5fafb276a6bec9439d39acc78",
    "semantic_title": "end-to-end cross-lingual spoken language understanding model with multilingual pretraining",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saghir21_interspeech.html": {
    "title": "Factorization-Aware Training of Transformers for Natural Language Understanding on the Edge",
    "volume": "main",
    "abstract": "Fine-tuning transformer-based models have shown to outperform other methods for many Natural Language Understanding (NLU) tasks. Recent studies to reduce the size of transformer models have achieved reductions of > 80%, making on-device inference on powerful devices possible. However, other resource-constrained devices, like those enabling voice assistants (VAs), require much further reductions. In this work, we propose factorization-aware training (FAT), wherein we factorize the linear mappings of an already compressed transformer model (DistilBERT) and train jointly on NLU tasks. We test this method on three different NLU datasets and show our method outperforms naive application of factorization after training by 10%–440% across various compression rates. Additionally, We introduce a new metric called factorization gap and use it to analyze the need for FAT across various model components. We also present results for training subsets of factorized components to enable faster training, re-usability and maintainability for multiple on-device models. We further demonstrate the trade-off between memory, inference speed and performance at a given compression-rate for a on-device implementation of a factorized model. Our best performing factorized model, achieves a relative size reduction of 84% with ≈10% relative degradation in NLU error rate compared to a non-factorized model on our internal dataset",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4a7b838fc0f759fe539aea8460497aaa5a25ceec",
    "semantic_title": "factorization-aware training of transformers for natural language understanding on the edge",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/saxon21_interspeech.html": {
    "title": "End-to-End Spoken Language Understanding for Generalized Voice Assistants",
    "volume": "main",
    "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems predict utterance semantics directly from speech using a single model. Previous work in this area has focused on targeted tasks in fixed domains, where the output semantic structure is assumed a priori and the input speech is of limited complexity. In this work we present our approach to developing an E2E model for generalized SLU in commercial voice assistants (VAs). We propose a fully differentiable, transformer-based, hierarchical system that can be pretrained at both the ASR and NLU levels. This is then fine-tuned on both transcription and semantic classification losses to handle a diverse set of intent and argument combinations. This leads to an SLU system that achieves significant improvements over baselines on a complex internal generalized VA dataset with a 43% improvement in accuracy, while still meeting the 99% accuracy benchmark on the popular Fluent Speech Commands dataset. We further evaluate our model on a hard test set, exclusively containing slot arguments unseen in training, and demonstrate a nearly 20% improvement, showing the efficacy of our approach in truly demanding VA scenarios",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3cc9585efa86127d2c981d5547eb8765c1250372",
    "semantic_title": "end-to-end spoken language understanding for generalized voice assistants",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/han21f_interspeech.html": {
    "title": "Bi-Directional Joint Neural Networks for Intent Classification and Slot Filling",
    "volume": "main",
    "abstract": "Intent classification and slot filling are two critical tasks for natural language understanding. Traditionally the two tasks proceeded independently. However, more recently joint models for intent classification and slot filling have achieved state-of-the-art performance, and have proved that there exists a strong relationship between the two tasks. In this paper, we propose a bi-directional joint model for intent classification and slot filling, which includes a multi-stage hierarchical process via BERT and bi-directional joint natural language understanding mechanisms, including intent2slot and slot2intent, to obtain mutual performance enhancement between intent classification and slot filling. The evaluations show that our model achieves state-of-the-art results on intent classification accuracy, slot filling F1, and significantly improves sentence-level semantic frame accuracy when applied to publicly available benchmark datasets, ATIS (88.6%) and SNIPS (92.8%)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3bf21376726896f73554288121b8263382d3984f",
    "semantic_title": "bi-directional joint neural networks for intent classification and slot filling",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cutler21_interspeech.html": {
    "title": "INTERSPEECH 2021 Acoustic Echo Cancellation Challenge",
    "volume": "main",
    "abstract": "The INTERSPEECH 2021 Acoustic Echo Cancellation Challenge is intended to stimulate research in the area of acoustic echo cancellation (AEC), which is an important part of speech enhancement and still a top issue in audio communication. Many recent AEC studies report good performance on synthetic datasets where the training and testing data may come from the same underlying distribution. However, AEC performance often degrades significantly on real recordings. Also, most of the conventional objective metrics such as echo return loss enhancement and perceptual evaluation of speech quality do not correlate well with subjective speech quality tests in the presence of background noise and reverberation found in realistic environments. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 5,000 real audio devices and human speakers in real environments, as well as a synthetic dataset. We also open source an online subjective test framework and provide an online objective metric service for researchers to quickly test their results. The winners of this challenge are selected based on the average Mean Opinion Score achieved across all different single talk and double talk scenarios",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8888933a55e1163b73099d322144f2a34726e5c3",
    "semantic_title": "interspeech 2021 acoustic echo cancellation challenge",
    "citation_count": 38,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/pfeifenberger21_interspeech.html": {
    "title": "Acoustic Echo Cancellation with Cross-Domain Learning",
    "volume": "main",
    "abstract": "This paper proposes the Cross-Domain Echo-Controller (CDEC), submitted to the Interspeech 2021 AEC-Challenge. The algorithm consists of three building blocks: (i) a Time-Delay Compensation (TDC) module, (ii) a frequency-domain block-based Acoustic Echo Canceler (AEC), and (iii) a Time-Domain Neural-Network (TD-NN) used as a post-processor. Our system achieves an overall MOS score of 3.80, while only using 2.1 million parameters at a system latency of 32ms",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "45904f436642372481d96ff80459fac15443dd45",
    "semantic_title": "acoustic echo cancellation with cross-domain learning",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ia_interspeech.html": {
    "title": "F-T-LSTM Based Complex Network for Joint Acoustic Echo Cancellation and Speech Enhancement",
    "volume": "main",
    "abstract": "With the increasing demand for audio communication and online conference, ensuring the robustness of Acoustic Echo Cancellation (AEC) under the complicated acoustic scenario including noise, reverberation and nonlinear distortion has become a top issue. Although there have been some traditional methods that consider nonlinear distortion, they are still inefficient for echo suppression and the performance will be attenuated when noise is present. In this paper, we present a real-time AEC approach using complex neural network to better modeling the important phase information and frequency-time-LSTMs (F-T-LSTM), which scan both frequency and time axis, for better temporal modeling. Moreover, we utilize modified SI-SNR as cost function to make the model to have better echo cancellation and noise suppression (NS) performance. With only 1.4M parameters, the proposed approach outperforms the AEC-challenge baseline by 0.27 in terms of Mean Opinion Score (MOS)",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2441425c9c9aa4a6e562c3f9c836f5f91c43986f",
    "semantic_title": "f-t-lstm based complex network for joint acoustic echo cancellation and speech enhancement",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/seidel21_interspeech.html": {
    "title": "Y2-Net FCRN for Acoustic Echo and Noise Suppression",
    "volume": "main",
    "abstract": "In recent years, deep neural networks (DNNs) were studied as an alternative to traditional acoustic echo cancellation (AEC) algorithms. The proposed models achieved remarkable performance for the separate tasks of AEC and residual echo suppression (RES). A promising network topology is a fully convolutional recurrent network (FCRN) structure, which has already proven its performance on both noise suppression and AEC tasks, individually. However, the combination of AEC, postfiltering, and noise suppression to a single network typically leads to a noticeable decline in the quality of the near-end speech component due to the lack of a separate loss for echo estimation. In this paper, we propose a two-stage model (Y -Net) which consists of two FCRNs, each with two inputs and one output (Y-Net). The first stage (AEC) yields an echo estimate, which — as a novelty for a DNN AEC model — is further used by the second stage to perform RES and noise suppression. While the subjective listening test of the Interspeech 2021 AEC Challenge mostly yielded results close to the baseline, the proposed method scored an average improvement of 0.46 points over the baseline on the blind testset in double-talk on the instrumental metric DECMOS, provided by the challenge organizers",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4ce4ac2368ed9ed84dba3eca11887ff280afc19b",
    "semantic_title": "Y$^2$-Net FCRN for Acoustic Echo and Noise Suppression",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/peng21f_interspeech.html": {
    "title": "Acoustic Echo Cancellation Using Deep Complex Neural Network with Nonlinear Magnitude Compression and Phase Information",
    "volume": "main",
    "abstract": "This paper describes a two-stage acoustic echo cancellation (AEC) and suppression framework for the INTERSPEECH2021 AEC Challenge. In the first stage, four parallel partitioned block frequency domain adaptive filters are used to cancel the linear echo components, where the far-end signal is delayed 0ms, 320ms, 640ms and 960ms for these four adaptive filters, respectively, thus a maximum 1280 ms time delay can be well handled in the blind test dataset. The error signal with minimum energy and its corresponding reference signal are chosen as the input for the second stage, where a gate complex convolutional recurrent neural network (GCCRN) is trained to further suppress the residual echo, late reverberation and environmental noise simultaneously. To improve the performance of GCCRN, we compress both the magnitude of the error signal and that of the far-end reference signal, and then the two compressed magnitudes are combined with the phase of the error signal to regenerate the complex spectra as the input features of GCCRN. Numerous experimental results show that the proposed framework is robust to the blind test dataset, and achieves a promising result with the P.808 evaluation",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7737769f0301f9a4a2e1d0ca870b7c3ad8bd3166",
    "semantic_title": "acoustic echo cancellation using deep complex neural network with nonlinear magnitude compression and phase information",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/ivry21_interspeech.html": {
    "title": "Nonlinear Acoustic Echo Cancellation with Deep Learning",
    "volume": "main",
    "abstract": "We propose a nonlinear acoustic echo cancellation system, which aims to model the echo path from the far-end signal to the near-end microphone in two parts. Inspired by the physical behavior of modern hands-free devices, we first introduce a novel neural network architecture that is specifically designed to model the nonlinear distortions these devices induce between receiving and playing the far-end signal. To account for variations between devices, we construct this network with trainable memory length and nonlinear activation functions that are not parameterized in advance, but are rather optimized during the training stage using the training data. Second, the network is succeeded by a standard adaptive linear filter that constantly tracks the echo path between the loudspeaker output and the microphone. During training, the network and filter are jointly optimized to learn the network parameters. This system requires 17 thousand parameters that consume 500 Million floating-point operations per second and 40 Kilo-bytes of memory. It also satisfies hands-free communication timing requirements on a standard neural processor, which renders it adequate for embedding on hands-free communication devices. Using 280 hours of real and synthetic data, experiments show advantageous performance compared to competing methods",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "5b60403d4b9fa0616414fa4a39ce03176198be40",
    "semantic_title": "nonlinear acoustic echo cancellation with deep learning",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/green21_interspeech.html": {
    "title": "Automatic Speech Recognition of Disordered Speech: Personalized Models Outperforming Human Listeners on Short Phrases",
    "volume": "main",
    "abstract": "This study evaluated the accuracy of personalized automatic speech recognition (ASR) for recognizing disordered speech from a large cohort of individuals with a wide range of underlying etiologies using an open vocabulary. The performance of these models was benchmarked relative to that of expert human transcribers and two different speaker-independent ASR models trained on typical speech. 432 individuals with self-reported disordered speech recorded at least 300 short phrases using a web-based application. Word error rates (WERs) were estimated for three different ASR models and for human transcribers. Metadata were collected to evaluate the potential impact of participants, atypical speech characteristics, and technical factors on recognition accuracy. Personalized models outperformed human transcribers with median and max recognition accuracy gains of 9% and 80%, respectively. The accuracies of personalized models were high (median WER: 4.6%) and better than those of speaker-independent models (median WER: 31%). The most significant improvements were for the most severely affected speakers. Low signal-to-noise ratio and fewer training utterances were associated with poor word recognition, even for speakers with mild speech impairments. Our results demonstrate the efficacy of personalized ASR models in recognizing a wide range of speech impairments and severities and using an open vocabulary",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b6d5efdb04e49beb2d60229fa3576ccb98e28d77",
    "semantic_title": "automatic speech recognition of disordered speech: personalized models outperforming human listeners on short phrases",
    "citation_count": 34,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/neumann21b_interspeech.html": {
    "title": "Investigating the Utility of Multimodal Conversational Technology and Audiovisual Analytic Measures for the Assessment and Monitoring of Amyotrophic Lateral Sclerosis at Scale",
    "volume": "main",
    "abstract": "We propose a cloud-based multimodal dialog platform for the remote assessment and monitoring of Amyotrophic Lateral Sclerosis (ALS) at scale. This paper presents our vision, technology setup, and an initial investigation of the efficacy of the various acoustic and visual speech metrics automatically extracted by the platform. 82 healthy controls and 54 people with ALS (pALS) were instructed to interact with the platform and completed a battery of speaking tasks designed to probe the acoustic, articulatory, phonatory, and respiratory aspects of their speech. We find that multiple acoustic (rate, duration, voicing) and visual (higher order statistics of the jaw and lip) speech metrics show statistically significant differences between controls, bulbar symptomatic and bulbar pre-symptomatic patients. We report on the sensitivity and specificity of these metrics using five-fold cross-validation. We further conducted a LASSO-LARS regression analysis to uncover the relative contributions of various acoustic and visual features in predicting the severity of patients' ALS (as measured by their self-reported ALSFRSR scores). Our results provide encouraging evidence of the utility of automatically extracted audiovisual analytics for scalable remote patient assessment and monitoring in ALS",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "4d87e911c6dca6eaed925d4c6f82e9ae3e17d9cd",
    "semantic_title": "investigating the utility of multimodal conversational technology and audiovisual analytic measures for the assessment and monitoring of amyotrophic lateral sclerosis at scale",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hermann21_interspeech.html": {
    "title": "Handling Acoustic Variation in Dysarthric Speech Recognition Systems Through Model Combination",
    "volume": "main",
    "abstract": "Developing automatic speech recognition (ASR) systems that recognise dysarthric speech as well as control speech from unimpaired speakers remains challenging. Including more highly variable dysarthric speech during training can also negatively affect the performance on control speakers, which is not desirable when developing speech recognisers for a wider audience. In this work, we analyse how the acoustic variability of dysarthric speech affects ASR systems and propose the combination of multiple acoustic models trained on different subsets of speakers to mitigate this effect. This approach shows improvements for both dysarthric and control speakers on the Torgo and UA-Speech corpora",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "38e707a0f1cba07909fc2a8dbd3b9b4934583c52",
    "semantic_title": "handling acoustic variation in dysarthric speech recognition systems through model combination",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/geng21b_interspeech.html": {
    "title": "Spectro-Temporal Deep Features for Disordered Speech Assessment and Recognition",
    "volume": "main",
    "abstract": "Automatic recognition of disordered speech remains a highly challenging task to date. Sources of variability commonly found in normal speech including accent, age or gender, when further compounded with the underlying causes of speech impairment and varying severity levels, create large diversity among speakers. To this end, speaker adaptation techniques play a vital role in current speech recognition systems. Motivated by the spectro-temporal level differences between disordered and normal speech that systematically manifest in articulatory imprecision, decreased volume and clarity, slower speaking rates and increased dysfluencies, novel spectro-temporal subspace basis embedding deep features derived by SVD decomposition of speech spectrum are proposed to facilitate both accurate speech intelligibility assessment and auxiliary feature based speaker adaptation of state-of-the-art hybrid DNN and end-to-end disordered speech recognition systems. Experiments conducted on the UASpeech corpus suggest the proposed spectro-temporal deep feature adapted systems consistently outperformed baseline i-Vector adaptation by up to 2.63% absolute (8.6% relative) reduction in word error rate (WER) with or without data augmentation. Learning hidden unit contribution (LHUC) based speaker adaptation was further applied. The final speaker adapted system using the proposed spectral basis embedding features gave an overall WER of 25.6% on the UASpeech test set of 16 dysarthric speakers",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "e04fe197bfe24d8cc60b9e997692b08341d729c0",
    "semantic_title": "spectro-temporal deep features for disordered speech assessment and recognition",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/gutz21_interspeech.html": {
    "title": "Speaking with a KN95 Face Mask: ASR Performance and Speaker Compensation",
    "volume": "main",
    "abstract": "The increasing prevalence of face masks in the United States due to the COVID-19 pandemic necessitates serious consideration of the functional impact of wearing a mask on speech. This study considers how the presence of a KN95 mask affects the performance of a commercial ASR system, Google Cloud Speech. We present evidence that wearing a mask does not impact ASR performance at the sentence level. Moreover, speakers may be naturally adapting to the mask by increasing their vowel space area. However, when speakers intentionally altered their speech by speaking clearly or loudly (though not slowly), ASR performance improved. These findings suggest that ASR users can employ speech strategies to achieve better ASR results when wearing a mask. Beyond healthy speakers, our study has implications for mask-wearing ASR users with otherwise reduced speech intelligibility",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b36e186126eb44ce8e122dcc136369ad4def75cd",
    "semantic_title": "speaking with a kn95 face mask: asr performance and speaker compensation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jin21_interspeech.html": {
    "title": "Adversarial Data Augmentation for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "Automatic recognition of disordered speech remains a highly challenging task to date. The underlying neuro-motor conditions, often compounded with co-occurring physical disabilities, lead to the difficulty in collecting large quantities of impaired speech required for ASR system development. To this end, data augmentation techniques play a vital role in current disordered speech recognition systems. In contrast to existing data augmentation techniques only modifying the speaking rate or overall shape of spectral contour, fine-grained spectro-temporal differences between disordered and normal speech are modelled using deep convolutional generative adversarial networks (DCGAN) during data augmentation to modify normal speech spectra into those closer to disordered speech. Experiments conducted on the UASpeech corpus suggest the proposed adversarial data augmentation approach consistently outperformed the baseline augmentation methods using tempo or speed perturbation on a state-of-the-art hybrid DNN system. An overall word error rate (WER) reduction up to 3.05% (9.7% relative) was obtained over the baseline system using no data augmentation. The final learning hidden unit contribution (LHUC) speaker adapted system using the best adversarial augmentation approach gives an overall WER of 25.89% on the UASpeech test set of 16 dysarthric speakers",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "7f102b3b38d24fe2c643130116a71e56b01ec5da",
    "semantic_title": "adversarial data augmentation for disordered speech recognition",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/xie21b_interspeech.html": {
    "title": "Variational Auto-Encoder Based Variability Encoding for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "Dysarthric speech recognition is a challenging task due to acoustic variability and limited amount of available data. Diverse conditions of dysarthric speakers account for the acoustic variability, which make the variability difficult to be modeled precisely. This paper presents a variational auto-encoder based variability encoder (VAEVE) to explicitly encode such variability for dysarthric speech. The VAEVE makes use of both phoneme information and low-dimensional latent variable to reconstruct the input acoustic features, thereby the latent variable is forced to encode the phoneme-independent variability. Stochastic gradient variational Bayes algorithm is applied to model the distribution for generating variability encodings, which are further used as auxiliary features for DNN acoustic modeling. Experiment results conducted on the UASpeech corpus show that the VAEVE based variability encodings have complementary effect to the learning hidden unit contributions (LHUC) speaker adaptation. The systems using variability encodings consistently outperform the comparable baseline systems without using them, and obtain absolute word error rate (WER) reduction by up to 2.2% on dysarthric speech with \"Very low\" intelligibility level, and up to 2% on the \"Mixed\" type of dysarthric speech with diverse or uncertain conditions",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "3c9d3a3e70ab538b9f52dbe601c5cfffd09c86f6",
    "semantic_title": "variational auto-encoder based variability encoding for dysarthric speech recognition",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/wang21ja_interspeech.html": {
    "title": "Learning Explicit Prosody Models and Deep Speaker Embeddings for Atypical Voice Conversion",
    "volume": "main",
    "abstract": "Though significant progress has been made for the voice conversion (VC) of typical speech, VC for atypical speech, e.g., dysarthric and second-language (L2) speech, remains a challenge, since it involves correcting for atypical prosody while maintaining speaker identity. To address this issue, we propose a VC system with explicit prosodic modelling and deep speaker embedding (DSE) learning. First, a speech-encoder strives to extract robust phoneme embeddings from atypical speech. Second, a prosody corrector takes in phoneme embeddings to infer typical phoneme duration and pitch values. Third, a conversion model takes phoneme embeddings and typical prosody features as inputs to generate the converted speech, conditioned on the target DSE that is learned via speaker encoder or speaker adaptation. Extensive experiments demonstrate that speaker adaptation can achieve higher speaker similarity, and the speaker encoder based conversion model can greatly reduce dysarthric and non-native pronunciation patterns with improved speech intelligibility. A comparison of speech recognition results between the original dysarthric speech and converted speech show that absolute reduction of 47.6% character error rate (CER) and 29.3% word error rate (WER) can be achieved",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "e85e6f093b6bec9d73bb537bed04830b3c0f8a46",
    "semantic_title": "learning explicit prosody models and deep speaker embeddings for atypical voice conversion",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/deng21d_interspeech.html": {
    "title": "Bayesian Parametric and Architectural Domain Adaptation of LF-MMI Trained TDNNs for Elderly and Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "Automatic recognition of elderly and disordered speech remains a highly challenging task to date. Such data is not only difficult to collect in large quantities, but also exhibits a significant mismatch against normal speech trained ASR systems. To this end, conventional deep neural network model adaptation approaches only consider parameter fine-tuning on limited target domain data. In this paper, a novel Bayesian parametric and neural architectural domain adaptation approach is proposed. Both the standard model parameters and architectural hyper-parameters (hidden layer L/R context offsets) of two lattice-free MMI (LF-MMI) factored TDNN systems separately trained using large quantities of normal speech from the English LibriSpeech and Cantonese SpeechOcean corpora were domain adapted to two tasks: a) 16-hour DementiaBank elderly speech corpus; and b) 14-hour CUDYS dysarthric speech database. A Bayesian differentiable architectural search (DARTS) super-network was designed to allow both efficient search over up to 7 different TDNN structures during domain adaptation, and robust modelling of parameter uncertainty given limited target domain data. Absolute recognition error rate reductions of 1.82% and 2.93% (13.2% and 8.3% relative) were obtained over the baseline systems performing model parameter fine-tuning only. Consistent performance improvements were retained after data augmentation and learning hidden unit contribution (LHUC) based speaker adaptation was performed",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "f9b49b12d4921136c4d3bb851ad5e0433d8b837b",
    "semantic_title": "bayesian parametric and architectural domain adaptation of lf-mmi trained tdnns for elderly and dysarthric speech recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/cai21c_interspeech.html": {
    "title": "A Voice-Activated Switch for Persons with Motor and Speech Impairments: Isolated-Vowel Spotting Using Neural Networks",
    "volume": "main",
    "abstract": "Severe speech impairments limit the precision and range of producible speech sounds. As a result, generic automatic speech recognition (ASR) and keyword spotting (KWS) systems fail to accurately recognize the utterances produced by individuals with severe speech impairments. This paper describes an approach in a simple speech sound, namely isolated open vowel (/a/), is used in lieu of more motorically-demanding utterances. A neural network (NN) is trained to detect the isolated open vowel uttered by impaired speakers. The NN is trained with a two-phase approach. The pre-training phase uses samples from unimpaired speakers along with samples of background noises and unrelated speech; then the fine-tuning phase uses samples of vowel samples collected from individuals with speech impairments. This model can be built into an experimental mobile app to act as a switch that allows users to activate preconfigured actions such as alerting caregivers. Preliminary user testing indicates the vowel spotter has the potential to be a useful and flexible emergency communication channel for motor- and speech-impaired individuals",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6892087c77977ab11fdc7df647d0456edcaff0c8",
    "semantic_title": "a voice-activated switch for persons with motor and speech impairments: isolated-vowel spotting using neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/chen21w_interspeech.html": {
    "title": "Conformer Parrotron: A Faster and Stronger End-to-End Speech Conversion and Recognition Model for Atypical Speech",
    "volume": "main",
    "abstract": "Parrotron is an end-to-end personalizable model that enables many-to-one voice conversion (VC) and automated speech recognition (ASR) simultaneously for atypical speech. In this work, we present the next-generation Parrotron model with improvements in overall accuracy, training and inference speeds. The proposed architecture builds on the recent Conformer encoder comprising of convolution and attention layer based blocks used in ASR. We introduce architectural modifications that subsamples encoder activations to achieve speed-ups in training and inference. In order to jointly improve ASR and voice conversion quality, we show that this requires a corresponding upsampling after the Conformer encoder blocks. We provide an in-depth analysis on how the proposed approach can maximize the efficiency of a speech-to-speech conversion model in the context of atypical speech. Experiments on both many-to-one and one-to-one dysarthric speech conversion tasks show that we can achieve up to 7× speedup and 35% relative reduction in WER over the previous best Transformer Parrotron",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "973a8ff6db0b827c02e7613d53135be39d73e165",
    "semantic_title": "conformer parrotron: a faster and stronger end-to-end speech conversion and recognition model for atypical speech",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/macdonald21_interspeech.html": {
    "title": "Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project Euphonia",
    "volume": "main",
    "abstract": "Speech samples from over 1000 individuals with impaired speech have been submitted for Project Euphonia, aimed at improving automated speech recognition systems for disordered speech. We provide an overview of the corpus, which recently passed 1 million utterances (>1300 hours), and review key lessons learned from this project. The reasoning behind decisions such as phrase set composition, prompted vs extemporaneous speech, metadata and data quality efforts are explained based on findings from both technical and user-facing research",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "e9f715b0c67a8dd43a6bbc420e3809dce16fa35a",
    "semantic_title": "disordered speech data collection: lessons learned at 1 million utterances from project euphonia",
    "citation_count": 31,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/yeo21_interspeech.html": {
    "title": "Automatic Severity Classification of Korean Dysarthric Speech Using Phoneme-Level Pronunciation Features",
    "volume": "main",
    "abstract": "This paper proposes an automatic severity classification method for Korean dysarthric speech by using two types of phoneme-level pronunciation features. The first type is the percentage of correct phonemes, which consists of percentage of correct consonants, percentage of correct vowels, and percentage of total correct phonemes. The second type is related to the degree of vowel distortion, such as vowel space area, formant centralized ratio, vowel articulatory index, and F2-ratio. The baseline experiments use features from our previous study, consisting of MFCCs, voice quality features, and prosody features. Compared to the baseline, experiments including phoneme-level pronunciation features achieve a relative percentage increase of 32.55% and 33.84% in F1-score for support vector machine and feed-forward neural network classifiers, respectively. Our best performance reaches an F1-score of 77.38%, which is a relative percentage increase of 10.39% compared to the best previous results conducted on the Korean dysarthric QoLT corpus. Furthermore, with feature selection applied, all seven phoneme-level pronunciation features are chosen, accounting for the highest percentage of the selected feature set by both recursive feature elimination and extra trees classifier feature selection algorithms. Results indicate that phoneme-level pronunciation features are useful in enhancing the performance for automatic severity classification of dysarthric speech",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "079300e3de20e9fa9b241a646ca0d2ce307399aa",
    "semantic_title": "automatic severity classification of korean dysarthric speech using phoneme-level pronunciation features",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/venugopalan21_interspeech.html": {
    "title": "Comparing Supervised Models and Learned Speech Representations for Classifying Intelligibility of Disordered Speech on Selected Phrases",
    "volume": "main",
    "abstract": "Automatic classification of disordered speech can provide an objective tool for identifying the presence and severity of a speech impairment. Classification approaches can also help identify hard-to-recognize speech samples to teach ASR systems about the variable manifestations of impaired speech. Here, we develop and compare different deep learning techniques to classify the intelligibility of disordered speech on selected phrases. We collected samples from a diverse set of 661 speakers with a variety of self-reported disorders speaking 29 words or phrases, which were rated by speech-language pathologists for their overall intelligibility using a five-point Likert scale. We then evaluated classifiers developed using 3 approaches: (1) a convolutional neural network (CNN) trained for the task, (2) classifiers trained on non-semantic speech representations from CNNs that used an unsupervised objective [1], and (3) classifiers trained on the acoustic (encoder) embeddings from an ASR system trained on typical speech [2]. We found that the ASR encoder's embeddings considerably outperform the other two on detecting and classifying disordered speech. Further analysis shows that the ASR embeddings cluster speech by the spoken phrase, while the non-semantic embeddings cluster speech by speaker. Also, longer phrases are more indicative of intelligibility deficits than single words",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "b54bed99035297b64bbd72a35de07e4b72c49c88",
    "semantic_title": "comparing supervised models and learned speech representations for classifying intelligibility of disordered speech on selected phrases",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/mitra21_interspeech.html": {
    "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech",
    "volume": "main",
    "abstract": "Dysfluencies and variations in speech pronunciation can severely degrade speech recognition performance, and for many individuals with moderate-to-severe speech disorders, voice operated systems do not work. Current speech recognition systems are trained primarily with data from fluent speakers and as a consequence do not generalize well to speech with dysfluencies such as sound or word repetitions, sound prolongations, or audible blocks. The focus of this work is on quantitative analysis of a consumer speech recognition system on individuals who stutter and production-oriented approaches for improving performance for common voice assistant tasks (i.e., \"what is the weather?\"). At baseline, this system introduces a significant number of insertion and substitution errors resulting in intended speech Word Error Rates (isWER) that are 13.64% worse (absolute) for individuals with fluency disorders. We show that by simply tuning the decoding parameters in an existing hybrid speech recognition system one can improve isWER by 24% (relative) for individuals with fluency disorders. Tuning these parameters translates to 3.6% better domain recognition and 1.7% better intent recognition relative to the default setup for the 18 study participants across all stuttering severities",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "91f8ae52896efe45d017357d297975ebc012a471",
    "semantic_title": "analysis and tuning of a voice assistant system for dysfluent speech",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/kawahara21b_interspeech.html": {
    "title": "Interactive and Real-Time Acoustic Measurement Tools for Speech Data Acquisition and Presentation: Application of an Extended Member of Time Stretched Pulses",
    "volume": "main",
    "abstract": "Objective measurements of speech data acquisition and presentation processes are crucial for assuring reproducibility and reusability of experimental results and acquired materials. We introduce setting and measurement examples of those conditions using an interactive and real-time acoustic measurement tool based on an extended time-stretched pulse. We also introduce supporting tools",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "6dc6334d9c936c2779331fec15b9f66c06875dc2",
    "semantic_title": "interactive and real-time acoustic measurement tools for speech data acquisition and presentation: application of an extended member of time stretched pulses",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/tihelka21_interspeech.html": {
    "title": "Save Your Voice: Voice Banking and TTS for Anyone",
    "volume": "main",
    "abstract": "The paper describes the process of automatic building of a personalized TTS system. The system was primarily developed for people facing the threat of voice loss; however, it can be used by anyone who wants to save his/her voice for any reason. Regarding the target group of users, the whole system is designed to be as simple to use as possible while still being fully autonomous",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c05e561f946ac155c5c3d57dbd1a514918db4bab",
    "semantic_title": "save your voice: voice banking and tts for anyone",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/zhang21ja_interspeech.html": {
    "title": "NeMo (Inverse) Text Normalization: From Development to Production",
    "volume": "main",
    "abstract": "We introduce the NeMo Text Processing (NTP) toolkit — an open-source Python library for text normalization (TN) and inverse text normalization (ITN) based on weighted finite-state transducers (WFSTs). The English grammars provided within NTP can be seamlessly deployed to the C++ Sparrowhawk framework for production",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "2f0c243fc11487f524f0d16ce512a4b87599e496",
    "semantic_title": "nemo inverse text normalization: from development to production",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/hembise21_interspeech.html": {
    "title": "Lalilo: A Reading Assistant for Children Featuring Speech Recognition-Based Reading Mistake Detection",
    "volume": "main",
    "abstract": "Lalilo is a reading assistant intended to help kindergarten to second grade students to master their reading skills. Students progress at their own pace thanks to an adaptive learning system that differentiates instructions. Teachers can access data on their students' progression. Among other exercises, a read-aloud exercise is provided for students to practice their reading. This exercise uses a reading mistake detection system based on speech recognition to offer automatic feedback on the child's reading. Since speech recognition on children learning to read is highly challenging, we overcome potential inaccurate thus damageable feedback with an uncertainty estimation leading to a neutral feedback",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "33e24538132d3c4e5ab62ec31c991869b3133454",
    "semantic_title": "lalilo: a reading assistant for children featuring speech recognition-based reading mistake detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/nguyen21f_interspeech.html": {
    "title": "Automatic Radiology Report Editing Through Voice",
    "volume": "main",
    "abstract": "We present a system that allows radiologists to edit the radiology report through their voices. This is a function in our bigger system at VinBrain LLC that uses AI algorithms to assist radiologists with chest x-ray diagnosis, the system can suggest the abnormalities, then bases on the radiologist's confirmations or conclusions to automatically generate the report using predefined templates. We then allow the radiologist to freely edit the report using voice. The system combines two components, the first is the Speech Recognition System (SRS), and the second is the Natural Language Understanding System (NLUS) that executes the user's command. The user can delete, modify or add an arbitrary whole sentence. In addition, we successfully developed an SRS for such a non-mainstream language as Vietnamese and adapted it for the radiology domain",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "fff9af455217771ef93854ff0aa45824b4e073e8",
    "semantic_title": "automatic radiology report editing through voice",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/shi21e_interspeech.html": {
    "title": "WittyKiddy: Multilingual Spoken Language Learning for Kids",
    "volume": "main",
    "abstract": "We present , a spoken language learning system for children, developed at the Institute for Infocomm Research (I2R), A*STAR, Singapore. Our system automatically evaluates a student's oral proficiency by scoring pronunciation, fluency and intonation of a spoken utterance. We demonstrate the technical capabilities of the system via reading aloud exercises and oral cloze tests in English and Malay. Both quantitative and qualitative feedback are given to the student. Our work helps support multilingual education for children",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "c09fce8e418c3a195ba5bf9a90c1ca5abc776ccb",
    "semantic_title": "wittykiddy: multilingual spoken language learning for kids",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/jin21b_interspeech.html": {
    "title": "Duplex Conversation in Outbound Agent System",
    "volume": "main",
    "abstract": "Intelligent outbound is a popular way to contact customers. The traditional outbound agents communicate with users in a simplex way. The user and the agent cannot speak at the same time, and the user cannot actively interrupt the conversation while the agent is playing audio generated by TTS. The traditional solution is based on the output of the VAD module, once the user voice is detected, the agent will immediately stop talking. However, the user sometimes expresses the short answer at will, not to interrupt the agent, and it will cause the agent to be frequently interrupted. In addition, when users say named entity nouns(numbers, locations, company names, etc), their speech speed is slow and the pause time between words is longer, and they may be interrupted by the agent unreasonably. We propose a method to identify user's interruption requests and discontinuous expressions by analyzing the semantic information of the user's utterance. As a result, fluency of the dialogue is improved",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "27011097d8c93b1e6c1dcbfad7e412df41fdab3f",
    "semantic_title": "duplex conversation in outbound agent system",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2021/udupa21b_interspeech.html": {
    "title": "Web Interface for Estimating Articulatory Movements in Speech Production from Acoustics and Text",
    "volume": "main",
    "abstract": "We release a web interface to visualise estimated articulatory movements in speech production from different modalities — acoustics and text. We allow the use of various trained models for this purpose. This tool also serves the purpose of comparing the predicted articulatory movements from different modalities and visually understanding the effect of noise in speech",
    "keywords": [
      []
    ],
    "checked": true,
    "id": "8edef8eb2f64b1819ee8189b6f6918581f013ed3",
    "semantic_title": "web interface for estimating articulatory movements in speech production from acoustics and text",
    "citation_count": 0,
    "authors": []
  }
}